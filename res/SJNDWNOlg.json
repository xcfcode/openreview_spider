{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396329216, "tcdate": 1486396329216, "number": 1, "id": "BJZnoGI_l", "invitation": "ICLR.cc/2017/conference/-/paper55/acceptance", "forum": "SJNDWNOlg", "replyto": "SJNDWNOlg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The paper conducts a detailed evaluation of different CNN architectures applied to visual instance retrieval. The authors consider various deep neural network architectures, with a focus on architectures pre-trained for image classification. \n \n An important concern of the reviewers is the relevance of the evaluation given the recent impressive experimental results of deep neural networks trained end-to-end for visual instance retrieval by Gordo et al. \"End-to-end Learning of Deep Visual Representations for Image Retrieval\". Another concern is the novelty of the proposed evaluation given the evaluation of the performance for visual instance retrieval of deep neural network pre-trained for image classification performed in Paulin et al. \"Convolutional Patch Representations for Image Retrieval: An Unsupervised Approach\". \n \n A revision of the paper, following the reviewers' suggestions, will generate a stronger submission to a future venue."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "What Is the Best Practice for CNNs Applied to Visual Instance Retrieval?", "abstract": "Previous work has shown that feature maps of deep convolutional neural networks (CNNs)\ncan be interpreted as feature representation of a particular image region. Features aggregated from\nthese feature maps have been exploited for image retrieval tasks and achieved state-of-the-art performances in\nrecent years. The key to the success of such methods is the feature representation. However, the different\nfactors that impact the effectiveness of features are still not explored thoroughly. There are much less\ndiscussion about the best combination of them.\n\nThe main contribution of our paper is the thorough evaluations of the various factors that affect the\ndiscriminative ability of the features extracted from CNNs. Based on the evaluation results, we also identify \nthe best choices for different factors and propose a new multi-scale image feature representation method to \nencode the image effectively. Finally, we show that the proposed method generalises well and outperforms \nthe state-of-the-art methods on four typical datasets used for visual instance retrieval.", "pdf": "/pdf/a7ed76c871eabe3940c81677382ff3c612068f5b.pdf", "paperhash": "hao|what_is_the_best_practice_for_cnns_applied_to_visual_instance_retrieval", "conflicts": ["ia.ac.cn"], "keywords": ["Computer vision", "Deep learning"], "authors": ["Jiedong Hao", "Jing Dong", "Wei Wang", "Tieniu Tan"], "authorids": ["jiedong.hao@cripac.ia.ac.cn", "jdong@nlpr.ia.ac.cn", "wwang@nlpr.ia.ac.cn", "tnt@nlpr.ia.ac.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396329695, "id": "ICLR.cc/2017/conference/-/paper55/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "SJNDWNOlg", "replyto": "SJNDWNOlg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396329695}}}, {"tddate": null, "tmdate": 1484949025115, "tcdate": 1481896079280, "number": 2, "id": "SkvcxOZVg", "invitation": "ICLR.cc/2017/conference/-/paper55/official/review", "forum": "SJNDWNOlg", "replyto": "SJNDWNOlg", "signatures": ["ICLR.cc/2017/conference/paper55/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper55/AnonReviewer2"], "content": {"title": "Not much utility in the paper", "rating": "3: Clear rejection", "review": "Authors investigate how to use pretrained CNNs for retrieval and perform an extensive evaluation of the influence of various parameters. For detailed comments on everything see the questions I posted earlier. The summary is here:\n\nI don't think we learn much from this paper: we already knew that we should use the last conv layer, we knew we should use PCA with whitening, we knew we should use original size images (authors say Tolias didn't do this as they resized the images, but they did this exactly for the same reason as authors didn't evaluate on Holidays - the images are too big. So they basically used \"as large as possible\" image sizes, which is what this paper effectively suggests as well), etc. This paper essentially concatenates methods that people have already used, and performs some more parameter tweaking to achieve the state-of-the-art (while the tweaking is actually performed on the test set of some of the tests).\n\nThe setting of the state-of-the-art results is quite misleading as it doesn't really come from the good choice of parameters, but mainly due to the usage of the deeper VGG-19 network. \n\nFurthermore, I don't think it's sufficient to just try one network and claim these are the best practices for using CNNs for instance retrieval - what about ResNet, what about Inception, I don't know how to apply any of these conclusions for those networks, and would these conclusions even hold for them. Furthermore the parameter tweaking was done on Oxford, I really can't tell what conclusions would we get if we tuned on UKB for example. So a more appropriate paper title would be \"What are the best parameter values for VGG-19 on Oxford/Paris benchmarks?\" - I don't think this is sufficiently novel nor interesting for the community.\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "What Is the Best Practice for CNNs Applied to Visual Instance Retrieval?", "abstract": "Previous work has shown that feature maps of deep convolutional neural networks (CNNs)\ncan be interpreted as feature representation of a particular image region. Features aggregated from\nthese feature maps have been exploited for image retrieval tasks and achieved state-of-the-art performances in\nrecent years. The key to the success of such methods is the feature representation. However, the different\nfactors that impact the effectiveness of features are still not explored thoroughly. There are much less\ndiscussion about the best combination of them.\n\nThe main contribution of our paper is the thorough evaluations of the various factors that affect the\ndiscriminative ability of the features extracted from CNNs. Based on the evaluation results, we also identify \nthe best choices for different factors and propose a new multi-scale image feature representation method to \nencode the image effectively. Finally, we show that the proposed method generalises well and outperforms \nthe state-of-the-art methods on four typical datasets used for visual instance retrieval.", "pdf": "/pdf/a7ed76c871eabe3940c81677382ff3c612068f5b.pdf", "paperhash": "hao|what_is_the_best_practice_for_cnns_applied_to_visual_instance_retrieval", "conflicts": ["ia.ac.cn"], "keywords": ["Computer vision", "Deep learning"], "authors": ["Jiedong Hao", "Jing Dong", "Wei Wang", "Tieniu Tan"], "authorids": ["jiedong.hao@cripac.ia.ac.cn", "jdong@nlpr.ia.ac.cn", "wwang@nlpr.ia.ac.cn", "tnt@nlpr.ia.ac.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512713629, "id": "ICLR.cc/2017/conference/-/paper55/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper55/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper55/AnonReviewer1", "ICLR.cc/2017/conference/paper55/AnonReviewer2", "ICLR.cc/2017/conference/paper55/AnonReviewer3"], "reply": {"forum": "SJNDWNOlg", "replyto": "SJNDWNOlg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper55/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper55/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512713629}}}, {"tddate": null, "tmdate": 1481900590873, "tcdate": 1481900590873, "number": 3, "id": "SJv4GtW4g", "invitation": "ICLR.cc/2017/conference/-/paper55/official/review", "forum": "SJNDWNOlg", "replyto": "SJNDWNOlg", "signatures": ["ICLR.cc/2017/conference/paper55/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper55/AnonReviewer3"], "content": {"title": "A paper with some good but limited and possibly slightly outdated experiments on object retrieval with CNNs", "rating": "6: Marginally above acceptance threshold", "review": "The paper conducts a detailed evaluation of different CNN architectures applied to image retrieval. The authors focus on testing various architectural choices, but do not propose or compare to end-to-end learning frameworks.\n\nTechnically, the contribution is clear, particularly with the promised clarifications on how multiple scales are handled in the representation. However, I am still not entirely clear whether there would be a difference in the multi-scale settting for full and cropped queries.\n\nWhile the paper focuses on comparing different baseline architectures for CNN-based image retrieval, several recent papers have proposed to learn end-to-end representations specific for this task, with very good result (see for instance the recent work by Gordo et al. \"End-to-end Learning of Deep Visual Representations for Image Retrieval\"). The authors clarify that their work is orthogonal to papers such as Gordo et al. as they assess instead the performance of networks pre-trained from image classification. In fact, they also indicate that image retrieval is more difficult than image classification -- this is because it is performed by using features originally trained for classification. I can partially accept this argument. However, given the results in recent papers, it is clear than end-to-end training is far superior in practice and it is not clear the analysis developed by the authors in this work would transfer or be useful for that case as well.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "What Is the Best Practice for CNNs Applied to Visual Instance Retrieval?", "abstract": "Previous work has shown that feature maps of deep convolutional neural networks (CNNs)\ncan be interpreted as feature representation of a particular image region. Features aggregated from\nthese feature maps have been exploited for image retrieval tasks and achieved state-of-the-art performances in\nrecent years. The key to the success of such methods is the feature representation. However, the different\nfactors that impact the effectiveness of features are still not explored thoroughly. There are much less\ndiscussion about the best combination of them.\n\nThe main contribution of our paper is the thorough evaluations of the various factors that affect the\ndiscriminative ability of the features extracted from CNNs. Based on the evaluation results, we also identify \nthe best choices for different factors and propose a new multi-scale image feature representation method to \nencode the image effectively. Finally, we show that the proposed method generalises well and outperforms \nthe state-of-the-art methods on four typical datasets used for visual instance retrieval.", "pdf": "/pdf/a7ed76c871eabe3940c81677382ff3c612068f5b.pdf", "paperhash": "hao|what_is_the_best_practice_for_cnns_applied_to_visual_instance_retrieval", "conflicts": ["ia.ac.cn"], "keywords": ["Computer vision", "Deep learning"], "authors": ["Jiedong Hao", "Jing Dong", "Wei Wang", "Tieniu Tan"], "authorids": ["jiedong.hao@cripac.ia.ac.cn", "jdong@nlpr.ia.ac.cn", "wwang@nlpr.ia.ac.cn", "tnt@nlpr.ia.ac.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512713629, "id": "ICLR.cc/2017/conference/-/paper55/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper55/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper55/AnonReviewer1", "ICLR.cc/2017/conference/paper55/AnonReviewer2", "ICLR.cc/2017/conference/paper55/AnonReviewer3"], "reply": {"forum": "SJNDWNOlg", "replyto": "SJNDWNOlg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper55/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper55/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512713629}}}, {"tddate": null, "tmdate": 1481798129984, "tcdate": 1481798129973, "number": 1, "id": "By5gfegEg", "invitation": "ICLR.cc/2017/conference/-/paper55/official/review", "forum": "SJNDWNOlg", "replyto": "SJNDWNOlg", "signatures": ["ICLR.cc/2017/conference/paper55/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper55/AnonReviewer1"], "content": {"title": "An outdated method with misleading claims.", "rating": "3: Clear rejection", "review": "This paper explores different strategies for instance-level image retrieval with deep CNNs. The approach consists of extracting features from a network pre-trained for image classification (e.g. VGG), and post-process them for image retrieval. In other words, the network is off-the-shelf and solely acts as a feature extractor. The post-processing strategies are borrowed from traditional retrieval pipelines relying on hand-crafted features (e.g. SIFT + Fisher Vectors), denoted by the authors as \"traditional wisdom\".\n\nSpecifically, the authors examine where to extract features in the network (i.e. features are neurons activations of a convolution layer), which type of feature aggregation and normalization performs best, whether resizing images helps, whether combining multiple scales helps, and so on. \n\nWhile this type of experimental study is reasonable and well motivated, it suffers from a huge problem. Namely it \"ignores\" 2 major recent works that are in direct contradictions with many claims of the paper ([a] \"End-to-end Learning of Deep Visual Representations for Image Retrieval\" by  Gordo et al. and [b] \"CNN Image Retrieval Learns from BoW: Unsupervised Fine-Tuning with Hard Examples\" by Radenovi\u0107 et al., both ECCV'16 papers). These works have shown that training for retrieval can be achieved with a siamese architectures and have demonstrated outstanding performance. As a result, many claims and findings of the paper are either outdated, questionable or just wrong.\n\nHere are some of the misleading claims: \n\n  - \"Features aggregated from these feature maps have been exploited for image retrieval tasks and achieved state-of-the-art performances in recent years.\"\n  Until [a] (not cited), the state-of-the-art was still largely dominated by sparse invariant features based methods (see last Table in [a]).\n  \n  - \"the proposed method [...] outperforms the state-of-the-art methods on four typical datasets\"\n  That is not true, for the same reasons than above, and also because the state-of-the-art is now dominated by [a] and [b].\n  \n  - \"Also in situations where a large numbers of training samples are not available, instance retrieval using unsupervised method is still preferable and may be the only option.\".\n  This is a questionable opinion. The method exposed in \"End-to-end Learning of Deep Visual Representations for Image Retrieval\" by Gordo et al. outperforms the state-of-the-art on the UKB dataset (3.84 without QE or DBA) whereas it was trained for landmarks retrieval and not objects, i.e. in a different retrieval context. This demonstrates that in spite of insufficient training data, training is still possible and beneficial.\n\n  - Finally, most findings are not even new or surprising (e.g. aggregate several regions in a multi-scale manner was already achieved by Tolias at al, etc.). So the interest of the paper is limited overall.\n\nIn addition, there are some problems in the experiments. For instance, the tuning experiments are only conducted on the Oxford dataset and using a single network (VGG-19), whereas it is not clear whether these conditions are well representative of all datasets and all networks (it is well known that the Oxford dataset behaves very differently than the Holidays dataset, for instance). In addition, tuning is performed very aggressively, making it look like the authors are tuning on the test set (e.g. see Table 3). \n\nTo conclude, the paper is one year too late with respect to recent developments in the state of the art.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "What Is the Best Practice for CNNs Applied to Visual Instance Retrieval?", "abstract": "Previous work has shown that feature maps of deep convolutional neural networks (CNNs)\ncan be interpreted as feature representation of a particular image region. Features aggregated from\nthese feature maps have been exploited for image retrieval tasks and achieved state-of-the-art performances in\nrecent years. The key to the success of such methods is the feature representation. However, the different\nfactors that impact the effectiveness of features are still not explored thoroughly. There are much less\ndiscussion about the best combination of them.\n\nThe main contribution of our paper is the thorough evaluations of the various factors that affect the\ndiscriminative ability of the features extracted from CNNs. Based on the evaluation results, we also identify \nthe best choices for different factors and propose a new multi-scale image feature representation method to \nencode the image effectively. Finally, we show that the proposed method generalises well and outperforms \nthe state-of-the-art methods on four typical datasets used for visual instance retrieval.", "pdf": "/pdf/a7ed76c871eabe3940c81677382ff3c612068f5b.pdf", "paperhash": "hao|what_is_the_best_practice_for_cnns_applied_to_visual_instance_retrieval", "conflicts": ["ia.ac.cn"], "keywords": ["Computer vision", "Deep learning"], "authors": ["Jiedong Hao", "Jing Dong", "Wei Wang", "Tieniu Tan"], "authorids": ["jiedong.hao@cripac.ia.ac.cn", "jdong@nlpr.ia.ac.cn", "wwang@nlpr.ia.ac.cn", "tnt@nlpr.ia.ac.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512713629, "id": "ICLR.cc/2017/conference/-/paper55/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper55/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper55/AnonReviewer1", "ICLR.cc/2017/conference/paper55/AnonReviewer2", "ICLR.cc/2017/conference/paper55/AnonReviewer3"], "reply": {"forum": "SJNDWNOlg", "replyto": "SJNDWNOlg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper55/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper55/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512713629}}}, {"tddate": null, "tmdate": 1481719715112, "tcdate": 1481719715105, "number": 5, "id": "Syoo1TCmx", "invitation": "ICLR.cc/2017/conference/-/paper55/public/comment", "forum": "SJNDWNOlg", "replyto": "H13U4Sofe", "signatures": ["~Jiedong_Hao1"], "readers": ["everyone"], "writers": ["~Jiedong_Hao1"], "content": {"title": "Reply to reviewer's questions (continued)", "comment": "5. This paper focuses on studying the impact of different factors on retrieval performances using the existing CNNs without further training. The other line of research is to fine-tuning the CNNs for retrieval. These are two different research directions. The grid search is done only to find the best image size for fixed-two and fixed-one strategy in order to show that even the best result of these two strategy can not outperform the free strategy, which, to the best of our knowledge, have been mentioned by previous papers. \n\n6. As I have mentioned, this paper focuses on studying the impact of various factors. Although some of the factors may have been mentioned previously, but they are not explicitly and fully explored. For feature aggregation and normalization, Babenko&Lempitsky use sum-l2 and Tolias et al. use max-l2. We want to find if L1 normalization works because it outperforms the L2 normalizaiton in the context of BOF using SIFT. We want to explore if the traditional wisdom apply here. We think it is still meaningful even if sum-l1 actually performs worse than max-l2 in the multiscale approach. For image size, Tolias et al. actually resized the original images and keep their aspect ratio (it is not the \"free\" way). The resized images are fed into the network. They have not explored the relationship between image resizing strategy and performances. And for PCA with whitening, we point out that it is not always beneficial to learn the PCA matrix on other datasets. And for the multiscale representation, we explore the impact of the number of scales and the impact of region overlapping on the performances. As for figure 3, it shows that if we want to do PCA and whitening, learning the PCA and whitening matrix \non a similar dataset is better than learning the matrix on the same dataset.\n\n8. The CNNs learned on the ImageNet already have pretty good generalization abilities. Under situation where the dataset is very small, unsupervised learning of features using the existing CNNs is a efficient and time-saving solution. So We want to improve previous works on this direction. The results of our experiment shows that even with an off-the-shelf CNN, we can achieve state-of-the-art performances. Also, experiences learned from this work can be applied to train the networks directly for the instance retrieval task, such as the work of Gordo et al. .These two directions are not contradictory. In the future, we will focus on exploring how to train the networks using the experiences gained in this paper to better advance the performances of instance retrieval.\n\nAs for the hardness of instance and category retrieval, in the context of using off-the-shelf CNNs for instance retrieval, we think instance retrieval is harder. Because the CNNs we used are trained for the classification tasks, they inherently tend to encode the general information of a class and ignore the finer differences which differentiate two different instances belonging to the same class(eg. two different cars may be considered the same despite their differences). But the ability to differentiate two similar but different instances is what we really need in instance retrieval. So when such a model is used for instance retrieval, images containing instances which are in the same class as the query but are different in details are ranked higher in the returned results. The false positive rate will be high for instance retrieval and the performances will not be good enough. But for category retrieval, that will not hurt the performances. In that sense, instance retrieval is harder. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "What Is the Best Practice for CNNs Applied to Visual Instance Retrieval?", "abstract": "Previous work has shown that feature maps of deep convolutional neural networks (CNNs)\ncan be interpreted as feature representation of a particular image region. Features aggregated from\nthese feature maps have been exploited for image retrieval tasks and achieved state-of-the-art performances in\nrecent years. The key to the success of such methods is the feature representation. However, the different\nfactors that impact the effectiveness of features are still not explored thoroughly. There are much less\ndiscussion about the best combination of them.\n\nThe main contribution of our paper is the thorough evaluations of the various factors that affect the\ndiscriminative ability of the features extracted from CNNs. Based on the evaluation results, we also identify \nthe best choices for different factors and propose a new multi-scale image feature representation method to \nencode the image effectively. Finally, we show that the proposed method generalises well and outperforms \nthe state-of-the-art methods on four typical datasets used for visual instance retrieval.", "pdf": "/pdf/a7ed76c871eabe3940c81677382ff3c612068f5b.pdf", "paperhash": "hao|what_is_the_best_practice_for_cnns_applied_to_visual_instance_retrieval", "conflicts": ["ia.ac.cn"], "keywords": ["Computer vision", "Deep learning"], "authors": ["Jiedong Hao", "Jing Dong", "Wei Wang", "Tieniu Tan"], "authorids": ["jiedong.hao@cripac.ia.ac.cn", "jdong@nlpr.ia.ac.cn", "wwang@nlpr.ia.ac.cn", "tnt@nlpr.ia.ac.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287747887, "id": "ICLR.cc/2017/conference/-/paper55/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJNDWNOlg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper55/reviewers", "ICLR.cc/2017/conference/paper55/areachairs"], "cdate": 1485287747887}}}, {"tddate": null, "tmdate": 1481719260616, "tcdate": 1481719215402, "number": 4, "id": "rkDnahCXe", "invitation": "ICLR.cc/2017/conference/-/paper55/public/comment", "forum": "SJNDWNOlg", "replyto": "Byua7Hjfg", "signatures": ["~Jiedong_Hao1"], "readers": ["everyone"], "writers": ["~Jiedong_Hao1"], "content": {"title": "Reply to reviewer's questions", "comment": "Thanks for your detailed comments and feedbacks. I will answer the questions according to the points listed.\n1. The main goal of this paper is not in surpassing the others' work, but to explore the various factors and how we can combine them in a complementary way. \nFor example, we explored several ways to compute the multi-scale feature vectors to see if they are useful for image retrieval. We think it is import to know what works \nand  what does not work.\u3000Without the systematic exploration of various factors, even using a deeper network will not necessarily lead to better results. For example,\nif we use the \"two-fixed\" way, the best result for full and cropped query with feature dim 512 are 55.5 and 38.7, which is far below the proposed methods (73.0 and 70.0\nfor full and cropped query respectively). If we use the VGG-16 network, the conclusions about how to to exploit different factors is likely to be the same. We will update \nthe results on vgg16 later. \n\n2. The performance of 'ours' with 1024-D is provided to compare the performance with the traditional method which have 1024-D features. Also, the additional time cost\nis not too much, so it is preferable when we are more concerned with performances than time. If it is compressed to 512-D, I would expect the performances will drop but may \nstill exceed that of single layer(conv5_4). We will update the results of using 256-D features later.\n\n3. In order to find the good combination of different factors, we have to do some experiments on a dataset (Oxford5k). If overfitting happens, then the performance for Oxford5k should be good. But the performances on Paris6k and UKB is not guaranteed to be better than previous results. It is very likely that the results will be much\nworse due to the serious overfitting on Oxford5k. But actually, the results on Paris6k and UKB are on par with or exceed the previous results. So we think it is not\noverfitting.\n\n4. Yes, if we tuned the parameters on UKB, some of the conclusions may be slightly different. But we do not think they will change dramatically. The conclusions for which \nlayer to choose, the image resizing strategy, the feature aggregation & normalization methods and the PCA and whitening will likely be the same. Only the conclusion for multiscale feature representation may change a little, for example, about whether to use overlap between image regions of the same level. The holiday dataset contains \nimages of high resolutions, if we use the \"free\" way, the images will not fit into the memory of GPU. That is why we have not reported results on this dataset.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "What Is the Best Practice for CNNs Applied to Visual Instance Retrieval?", "abstract": "Previous work has shown that feature maps of deep convolutional neural networks (CNNs)\ncan be interpreted as feature representation of a particular image region. Features aggregated from\nthese feature maps have been exploited for image retrieval tasks and achieved state-of-the-art performances in\nrecent years. The key to the success of such methods is the feature representation. However, the different\nfactors that impact the effectiveness of features are still not explored thoroughly. There are much less\ndiscussion about the best combination of them.\n\nThe main contribution of our paper is the thorough evaluations of the various factors that affect the\ndiscriminative ability of the features extracted from CNNs. Based on the evaluation results, we also identify \nthe best choices for different factors and propose a new multi-scale image feature representation method to \nencode the image effectively. Finally, we show that the proposed method generalises well and outperforms \nthe state-of-the-art methods on four typical datasets used for visual instance retrieval.", "pdf": "/pdf/a7ed76c871eabe3940c81677382ff3c612068f5b.pdf", "paperhash": "hao|what_is_the_best_practice_for_cnns_applied_to_visual_instance_retrieval", "conflicts": ["ia.ac.cn"], "keywords": ["Computer vision", "Deep learning"], "authors": ["Jiedong Hao", "Jing Dong", "Wei Wang", "Tieniu Tan"], "authorids": ["jiedong.hao@cripac.ia.ac.cn", "jdong@nlpr.ia.ac.cn", "wwang@nlpr.ia.ac.cn", "tnt@nlpr.ia.ac.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287747887, "id": "ICLR.cc/2017/conference/-/paper55/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJNDWNOlg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper55/reviewers", "ICLR.cc/2017/conference/paper55/areachairs"], "cdate": 1485287747887}}}, {"tddate": null, "tmdate": 1481199377359, "tcdate": 1481199377354, "number": 2, "id": "HyKM1CIme", "invitation": "ICLR.cc/2017/conference/-/paper55/public/comment", "forum": "SJNDWNOlg", "replyto": "SJmNbRcfl", "signatures": ["~Jiedong_Hao1"], "readers": ["everyone"], "writers": ["~Jiedong_Hao1"], "content": {"title": "Different directions for CNNs applied to instance retrieval", "comment": "Thanks for taking time to read our paper and giving your comment.\n\nThe work of Gordo et al. and Radenovic et al. achieve great results. Their way is to fine-tune the existing CNNs for instance retrieval \nand also employ post-processing techniques such as query expansion and dataset side feature augmentation. In our work, we focus on how \nto best exploit the existing CNNs to achieve good performances without further training of CNNs and post-processing. These are different\ndirections of research. So we compare only with methods of the same direction for fair comparisons."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "What Is the Best Practice for CNNs Applied to Visual Instance Retrieval?", "abstract": "Previous work has shown that feature maps of deep convolutional neural networks (CNNs)\ncan be interpreted as feature representation of a particular image region. Features aggregated from\nthese feature maps have been exploited for image retrieval tasks and achieved state-of-the-art performances in\nrecent years. The key to the success of such methods is the feature representation. However, the different\nfactors that impact the effectiveness of features are still not explored thoroughly. There are much less\ndiscussion about the best combination of them.\n\nThe main contribution of our paper is the thorough evaluations of the various factors that affect the\ndiscriminative ability of the features extracted from CNNs. Based on the evaluation results, we also identify \nthe best choices for different factors and propose a new multi-scale image feature representation method to \nencode the image effectively. Finally, we show that the proposed method generalises well and outperforms \nthe state-of-the-art methods on four typical datasets used for visual instance retrieval.", "pdf": "/pdf/a7ed76c871eabe3940c81677382ff3c612068f5b.pdf", "paperhash": "hao|what_is_the_best_practice_for_cnns_applied_to_visual_instance_retrieval", "conflicts": ["ia.ac.cn"], "keywords": ["Computer vision", "Deep learning"], "authors": ["Jiedong Hao", "Jing Dong", "Wei Wang", "Tieniu Tan"], "authorids": ["jiedong.hao@cripac.ia.ac.cn", "jdong@nlpr.ia.ac.cn", "wwang@nlpr.ia.ac.cn", "tnt@nlpr.ia.ac.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287747887, "id": "ICLR.cc/2017/conference/-/paper55/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJNDWNOlg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper55/reviewers", "ICLR.cc/2017/conference/paper55/areachairs"], "cdate": 1485287747887}}}, {"tddate": null, "tmdate": 1481198946007, "tcdate": 1481198945997, "number": 1, "id": "rJcDTpUXe", "invitation": "ICLR.cc/2017/conference/-/paper55/public/comment", "forum": "SJNDWNOlg", "replyto": "ryJCz2rMx", "signatures": ["~Jiedong_Hao1"], "readers": ["everyone"], "writers": ["~Jiedong_Hao1"], "content": {"title": "Comment to questions", "comment": "Thanks for your comment and feedback.\nHow final feature is formed and why does it work better?\n1. First the regional vectors is computed, then regional vectors are summed to\nform the image features. \n2. compared to the single scale approach, the multi-scale approach can obtain finer\ndetailed information from various regions in the image and the sum between\ndifferent regions also suppress suspicious bursts of features in the Convolutional\nfeature channels which may arise in single scale case. So the final feature \nwill improve the performance.\n\nThe contribution of Gordo et al. \"End-to-end Learning of Deep Visual Representations \nfor Image Retrieval\" and relationship to this work.\n1. The contribution of the work of Gordo et al. are mainly:\n    a)the automatic cleaning process of the Landmark dataset to get cleaner data for train \n    a triplet network.\n    b)incorporation of the RPN network from object detection task and R-MAC encoding from [1]\n     to the triplet network pipeline.\n2. There are two stream of research, one is to explore how we can best exploit the existing \nCNNs trained for ImageNet(they already have pretty good generalization ability) to retrieval\ntask, such as the work of [1] and [2]. Our work belongs to this stream and we explored how\nwe can choose the different factors to achieve good performances, which have not been discussed\nthoroughly by previous papers.\nThe other stream is to explore how to fine-tune the existing CNNs for retrieval tasks to \nachieve better performances, such as the work of Gordo et al. and [3][4].\nThese two streams are not contradictory, eg., some of the experiences learned from the \nformer can also be applied to the latter, for example, the R-MAC encoding used by \nGordo et al.\n\nAbout the \"hardness\" of instance retrieval.\n\nIn the context of using existing CNNs for instance retrieval, we think instance retrieval\nis harder. Because the CNNs are trained for the classification tasks, they tend to encode\nthe general information of a class and ignore the finer differences which differentiate\ntwo different instances belonging to the same class(eg. two different cars may be \nconsidered the same despite their differences). So when such as model is used for\ninstance retrieval, instance which is in the same class as the query are \nranked higher in the returned result. For category retrieval, it will not hurt\nthe performances, but for instance retrieval, if the returned instance is not the \nsame as the query, the mAP will drop. In this sense, instance retrieval is harder. \nThat is also why we need to explore different factors (the subject of this paper) \nto increase the discriminative power of features extracted from CNNs.\n\nReference\n[1]Tolias et al.:Particular Object Retrieval with Integral Max-pooling of CNN Activations\n[2]Babenko&Lempitsky: Aggregating Deep Convolutional Features for Image Retrieval\n[3]Yu et al.: Sketch Me That Shoe\n[4] Babenko&Lempitsky: Neural Codes for Image retrieval"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "What Is the Best Practice for CNNs Applied to Visual Instance Retrieval?", "abstract": "Previous work has shown that feature maps of deep convolutional neural networks (CNNs)\ncan be interpreted as feature representation of a particular image region. Features aggregated from\nthese feature maps have been exploited for image retrieval tasks and achieved state-of-the-art performances in\nrecent years. The key to the success of such methods is the feature representation. However, the different\nfactors that impact the effectiveness of features are still not explored thoroughly. There are much less\ndiscussion about the best combination of them.\n\nThe main contribution of our paper is the thorough evaluations of the various factors that affect the\ndiscriminative ability of the features extracted from CNNs. Based on the evaluation results, we also identify \nthe best choices for different factors and propose a new multi-scale image feature representation method to \nencode the image effectively. Finally, we show that the proposed method generalises well and outperforms \nthe state-of-the-art methods on four typical datasets used for visual instance retrieval.", "pdf": "/pdf/a7ed76c871eabe3940c81677382ff3c612068f5b.pdf", "paperhash": "hao|what_is_the_best_practice_for_cnns_applied_to_visual_instance_retrieval", "conflicts": ["ia.ac.cn"], "keywords": ["Computer vision", "Deep learning"], "authors": ["Jiedong Hao", "Jing Dong", "Wei Wang", "Tieniu Tan"], "authorids": ["jiedong.hao@cripac.ia.ac.cn", "jdong@nlpr.ia.ac.cn", "wwang@nlpr.ia.ac.cn", "tnt@nlpr.ia.ac.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287747887, "id": "ICLR.cc/2017/conference/-/paper55/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJNDWNOlg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper55/reviewers", "ICLR.cc/2017/conference/paper55/areachairs"], "cdate": 1485287747887}}}, {"tddate": null, "tmdate": 1480442964428, "tcdate": 1480442964423, "number": 1, "id": "H13U4Sofe", "invitation": "ICLR.cc/2017/conference/-/paper55/official/comment", "forum": "SJNDWNOlg", "replyto": "SJNDWNOlg", "signatures": ["ICLR.cc/2017/conference/paper55/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper55/AnonReviewer2"], "content": {"title": "Various, chunk 2", "comment": "\n5)\nI agree with other reviewers on the lack of comparison with Gordo et al and Radenovic et al, though I understand that authors' arguments are that they do not want to train their networks (though then comparing with Arandjelovic et al. also doesn't make sense). It's still worth citing these papers and commenting on them. Also, with such an extensive set of experiments, it's a bit arguable if authors don't really do training - they don't do the canonical SGD, but they essentially perform grid search for parameters on the test (see question 3).\n\n6)\nI'm not sure what did we actually learn from this paper. To use the last conv? We knew that before as all recent papers do this (Arandjelovic et al, Tolias et al, Gordo et al, Radenovic et al, Babenko and Lempitsky, ..). That using original image sizes is important? We knew this as well, early works (Babenko et al 2014, etc) used smaller images while all recent works apply the networks convolutionally over original size images (e.g. Tolias et al have this experiment in table 1). That one should use PCA with whitening (and if possible learn whitening on the test set)? We knew this already as well. So the only two things that haven't been done in exactly the same way as people did it before is the multi-scale pooling (though obviously various other similar versions exist), and the exploration of max/sum pooling with l1 or l2 normalization (though the experiments in table 1 are basically ignored as sum-l1 works the best there, but authors then say that actually later they notice that for multiscale max-l2 works best). Actually the most interesting part for me, one that I can actually say I didn't know and don't think anyone knew, is figure 3.\n\n7)\nI think it's a bit of an overstatement to call this paper 'best practice for CNNs' when only a single CNN architecture, VGG-19, is considered. What is the best practice for other models, e.g. ResNet, Inception? Presumably the last conv is likely to be best though for ResNet it's not that clear, and I'm not sure if sum vs max pooling would change as those two networks were trained with sum pooling, and I'm not sure if any of the other conclusions hold either. This is more of a surgery of VGG-19 than best practices for CNNs in general.\n\n8)\nOn a more philosophical level, and not only aimed at authors but also at others who are potentially reading this - this conference is about learning representations, while no learning is being performed. Taking CNNs as black boxes and tweaking the inputs and outputs in different ways with different normalizations is much more like using hand-engineered features like SIFT (replace black-box SIFT extractor with black-box CNN) than actually doing Deep Learning. I'm not saying this type of paper shouldn't exist as it's good to know what works best, but my preference in terms of what papers I would like to see in the future is:\na) There have been too many papers for using CNNs as black-boxes, I hoped we are finally over with this\nb) For ICLR I think one should actually do some training, e.g. after we figure out the best image representation, now train the whole system end-to-end and see if you can improve the performance.\nc) Design architectures which are specifically aimed at image retrieval - maybe something different than CNNs for classification pops up?\nd) Figure out ways to train CNNs for retrieval, we know how to do it for classification by paying people to label millions of images, can we do something better for retrieval? (though this is to some extend addressed now by Arandjelovic et al, Gordo et al and Radenovic et al).\n\n\nOther minor comments:\n\n- I was also surprised by the \"harder than category retrieval\" statement, as reviewer 3. I wouldn't go as far as saying that the opposite is true either, the two just cannot be compared so easily.\n- Inconsistencies of references (e.g. \"Y. Lecun\" vs \"Ross Girshick\", \"CVPR\" versus \"Computer\nVision and Pattern Recognition\", ..\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "What Is the Best Practice for CNNs Applied to Visual Instance Retrieval?", "abstract": "Previous work has shown that feature maps of deep convolutional neural networks (CNNs)\ncan be interpreted as feature representation of a particular image region. Features aggregated from\nthese feature maps have been exploited for image retrieval tasks and achieved state-of-the-art performances in\nrecent years. The key to the success of such methods is the feature representation. However, the different\nfactors that impact the effectiveness of features are still not explored thoroughly. There are much less\ndiscussion about the best combination of them.\n\nThe main contribution of our paper is the thorough evaluations of the various factors that affect the\ndiscriminative ability of the features extracted from CNNs. Based on the evaluation results, we also identify \nthe best choices for different factors and propose a new multi-scale image feature representation method to \nencode the image effectively. Finally, we show that the proposed method generalises well and outperforms \nthe state-of-the-art methods on four typical datasets used for visual instance retrieval.", "pdf": "/pdf/a7ed76c871eabe3940c81677382ff3c612068f5b.pdf", "paperhash": "hao|what_is_the_best_practice_for_cnns_applied_to_visual_instance_retrieval", "conflicts": ["ia.ac.cn"], "keywords": ["Computer vision", "Deep learning"], "authors": ["Jiedong Hao", "Jing Dong", "Wei Wang", "Tieniu Tan"], "authorids": ["jiedong.hao@cripac.ia.ac.cn", "jdong@nlpr.ia.ac.cn", "wwang@nlpr.ia.ac.cn", "tnt@nlpr.ia.ac.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287747760, "id": "ICLR.cc/2017/conference/-/paper55/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "SJNDWNOlg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper55/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper55/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper55/reviewers", "ICLR.cc/2017/conference/paper55/areachairs"], "cdate": 1485287747760}}}, {"tddate": null, "ddate": null, "writable": true, "revisions": false, "tmdate": 1480442902021, "tcdate": 1480442815995, "number": 3, "replyCount": 0, "id": "Byua7Hjfg", "invitation": "ICLR.cc/2017/conference/-/paper55/pre-review/question", "forum": "SJNDWNOlg", "replyto": "SJNDWNOlg", "signatures": ["ICLR.cc/2017/conference/paper55/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper55/AnonReviewer2"], "content": {"title": "Various", "question": "It seems my questions are too long so I have to split it into chunks. Sorry about the length, and obviously authors don't have to answer it all but pick on important points. However, I think it's important for all these comments to be made, so I'm just submitting the questions in chunks:\n\n1)\nWhy do the authors chose to use VGG-19 when previous papers use AlexNet or VGG-16 (e.g. all other results in table 5)? This looks very important as figure 2 shows that going deeper provides a huge boost, e.g. conv5_3 gets less than 48% while conv5_4 gets more than 58%. Thus, it is not clear to me whether authors get the state-of-the-art due to their systematic exploration of various factor, or simply due to using a deeper network. In the light of figure 2, I would bet the latter is the key. To be more convincing, performance with VGG-16 would be needed, does it outperform Tolias et al?\n\n2)\nIn image retrieval it is very important to compare oranges with oranges, i.e. compare performances for a fixed memory usage (fixed image representation dimensionality). Table 5 roughly does it by comparing proposed 512-D versus Tolias et al. 512-D, but I have a few comments:\na) The performance of 'ours' with 1024-D isn't really that relevant as it's clear that more dimensions (in retrieval) mean better performance. How well does the ensemble work if you compress it down to 512-D, is it worth it?\nb) Arandjelovic et al. 512-D in some cases outperforms Tolias et al. (65.6 / 67.6 for Oxford 5k full / crop) thus making the gap between existing and proposed method smaller\nc) How well would 'ours' work when compressed down to 256-D?\n\n3)\nAuthors performed many experiments on Oxford 5k to find the best possible combination of parameters. I am concerned that there has been a lot of overfitting on the test data. Note that Oxford also only has 55 queries, so overfitting is even easier. Oxford 5k results are extremely tuned on the test set, Oxford 105k is thus also directly impacted, so the only fair results are on Paris6k and UKB. We can already see that on Paris6k the improvements are much smaller than on Oxford5k (from 80.0 to 83.3, while Oxford is from 66.8 to 70.6), while for UKB we don't really know as the two best baselines did not report results on UKB.\nSo, are we just seeing overfitting?\n\n4)\nRelated to 3 - how different would the conclusions be if you tuned everything on UKB? I would bet quite different - the dataset statistics are quite different (Oxford and Paris are fairly similar). In order to claim 'best practice' for CNNs on visual instance retrieval, one should really consider this question, are these practices really the best across datasets? Testing on Holidays would also be very beneficial (and at least one of the two best baselines reports results on this)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "What Is the Best Practice for CNNs Applied to Visual Instance Retrieval?", "abstract": "Previous work has shown that feature maps of deep convolutional neural networks (CNNs)\ncan be interpreted as feature representation of a particular image region. Features aggregated from\nthese feature maps have been exploited for image retrieval tasks and achieved state-of-the-art performances in\nrecent years. The key to the success of such methods is the feature representation. However, the different\nfactors that impact the effectiveness of features are still not explored thoroughly. There are much less\ndiscussion about the best combination of them.\n\nThe main contribution of our paper is the thorough evaluations of the various factors that affect the\ndiscriminative ability of the features extracted from CNNs. Based on the evaluation results, we also identify \nthe best choices for different factors and propose a new multi-scale image feature representation method to \nencode the image effectively. Finally, we show that the proposed method generalises well and outperforms \nthe state-of-the-art methods on four typical datasets used for visual instance retrieval.", "pdf": "/pdf/a7ed76c871eabe3940c81677382ff3c612068f5b.pdf", "paperhash": "hao|what_is_the_best_practice_for_cnns_applied_to_visual_instance_retrieval", "conflicts": ["ia.ac.cn"], "keywords": ["Computer vision", "Deep learning"], "authors": ["Jiedong Hao", "Jing Dong", "Wei Wang", "Tieniu Tan"], "authorids": ["jiedong.hao@cripac.ia.ac.cn", "jdong@nlpr.ia.ac.cn", "wwang@nlpr.ia.ac.cn", "tnt@nlpr.ia.ac.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959489120, "id": "ICLR.cc/2017/conference/-/paper55/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper55/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper55/AnonReviewer3", "ICLR.cc/2017/conference/paper55/AnonReviewer1", "ICLR.cc/2017/conference/paper55/AnonReviewer2"], "reply": {"forum": "SJNDWNOlg", "replyto": "SJNDWNOlg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper55/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper55/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959489120}}}, {"tddate": null, "tmdate": 1480413506503, "tcdate": 1480413482785, "number": 2, "id": "SJmNbRcfl", "invitation": "ICLR.cc/2017/conference/-/paper55/pre-review/question", "forum": "SJNDWNOlg", "replyto": "SJNDWNOlg", "signatures": ["ICLR.cc/2017/conference/paper55/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper55/AnonReviewer1"], "content": {"title": "Comparison and discussion with respect to relevant works", "question": "As already raised by another reviewer, why did you not compare with the recent deep approaches for retrieval \"End-to-end Learning of Deep Visual Representations for Image Retrieval\" by  Gordo et al. and \"CNN Image Retrieval Learns from BoW: Unsupervised Fine-Tuning with Hard Examples\" by Radenovi\u0107 et al.?\n\nThese papers seem extremely relevant, and obtain state-of-the-art results (beyond any published results so far for the first one)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "What Is the Best Practice for CNNs Applied to Visual Instance Retrieval?", "abstract": "Previous work has shown that feature maps of deep convolutional neural networks (CNNs)\ncan be interpreted as feature representation of a particular image region. Features aggregated from\nthese feature maps have been exploited for image retrieval tasks and achieved state-of-the-art performances in\nrecent years. The key to the success of such methods is the feature representation. However, the different\nfactors that impact the effectiveness of features are still not explored thoroughly. There are much less\ndiscussion about the best combination of them.\n\nThe main contribution of our paper is the thorough evaluations of the various factors that affect the\ndiscriminative ability of the features extracted from CNNs. Based on the evaluation results, we also identify \nthe best choices for different factors and propose a new multi-scale image feature representation method to \nencode the image effectively. Finally, we show that the proposed method generalises well and outperforms \nthe state-of-the-art methods on four typical datasets used for visual instance retrieval.", "pdf": "/pdf/a7ed76c871eabe3940c81677382ff3c612068f5b.pdf", "paperhash": "hao|what_is_the_best_practice_for_cnns_applied_to_visual_instance_retrieval", "conflicts": ["ia.ac.cn"], "keywords": ["Computer vision", "Deep learning"], "authors": ["Jiedong Hao", "Jing Dong", "Wei Wang", "Tieniu Tan"], "authorids": ["jiedong.hao@cripac.ia.ac.cn", "jdong@nlpr.ia.ac.cn", "wwang@nlpr.ia.ac.cn", "tnt@nlpr.ia.ac.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959489120, "id": "ICLR.cc/2017/conference/-/paper55/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper55/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper55/AnonReviewer3", "ICLR.cc/2017/conference/paper55/AnonReviewer1", "ICLR.cc/2017/conference/paper55/AnonReviewer2"], "reply": {"forum": "SJNDWNOlg", "replyto": "SJNDWNOlg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper55/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper55/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959489120}}}, {"tddate": null, "tmdate": 1480078023273, "tcdate": 1480078023267, "number": 1, "id": "ryJCz2rMx", "invitation": "ICLR.cc/2017/conference/-/paper55/pre-review/question", "forum": "SJNDWNOlg", "replyto": "SJNDWNOlg", "signatures": ["ICLR.cc/2017/conference/paper55/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper55/AnonReviewer3"], "content": {"title": "Technical clarification and positioning of manuscript w.r.t. literature", "question": "The paper conduct a solid and detailed evaluation of different CNN architectures applied to retrieval. The authors focus on testing various architectural choices, but do not propose and end-to-end learning framework. \n\nTechnically, the contribution is mostly clear. However, I would like the authors to discuss more explicitly their multi-scale representation. Here multi scale seems to refer to splitting the images in grids of different resolution before feature pooling. This should be contrasted to multi scale as in evaluating features on an image pyramid. I would also like to understand better how the final feature vector is formed. After pooling in two disjoint regions, the feature vectors seem to be simply summed together. Is that so? Can the authors explain why this work so much better than directly pooling the features in the larger region?\n\nIntuitively, one-side scale normalisation should help, and in fact it does for the full-query case. I am not surprised that it hurts in the cropped-query case, as in this case normalisation completely changes the size of the query image. What happens if, in the cropped-query case, the scaling factor of the fully-query is used instead? My guess its that this normalisation would help.\n\nWhile the paper focuses on comparing different baseline architectures for CNN-based image retrieval, several recent papers have proposed to fine-tune the representation for this task, with great result. See for instance the recent work by Gordo et al. \"End-to-end Learning of Deep Visual Representations for Image Retrieval\". Can the authors comment on the relationship between their contributions and these works?\n\nOther remarks:\n\nIn my opinion, instance retrieval is hardly \"harder than category retrieval\" (see introduction).  While it is true that instance retrieval need to remember very precise details about the object, this is generally considered much easier to achieve in a computer than the generalisation capabilities required to understand object classes.\n\n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "What Is the Best Practice for CNNs Applied to Visual Instance Retrieval?", "abstract": "Previous work has shown that feature maps of deep convolutional neural networks (CNNs)\ncan be interpreted as feature representation of a particular image region. Features aggregated from\nthese feature maps have been exploited for image retrieval tasks and achieved state-of-the-art performances in\nrecent years. The key to the success of such methods is the feature representation. However, the different\nfactors that impact the effectiveness of features are still not explored thoroughly. There are much less\ndiscussion about the best combination of them.\n\nThe main contribution of our paper is the thorough evaluations of the various factors that affect the\ndiscriminative ability of the features extracted from CNNs. Based on the evaluation results, we also identify \nthe best choices for different factors and propose a new multi-scale image feature representation method to \nencode the image effectively. Finally, we show that the proposed method generalises well and outperforms \nthe state-of-the-art methods on four typical datasets used for visual instance retrieval.", "pdf": "/pdf/a7ed76c871eabe3940c81677382ff3c612068f5b.pdf", "paperhash": "hao|what_is_the_best_practice_for_cnns_applied_to_visual_instance_retrieval", "conflicts": ["ia.ac.cn"], "keywords": ["Computer vision", "Deep learning"], "authors": ["Jiedong Hao", "Jing Dong", "Wei Wang", "Tieniu Tan"], "authorids": ["jiedong.hao@cripac.ia.ac.cn", "jdong@nlpr.ia.ac.cn", "wwang@nlpr.ia.ac.cn", "tnt@nlpr.ia.ac.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959489120, "id": "ICLR.cc/2017/conference/-/paper55/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper55/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper55/AnonReviewer3", "ICLR.cc/2017/conference/paper55/AnonReviewer1", "ICLR.cc/2017/conference/paper55/AnonReviewer2"], "reply": {"forum": "SJNDWNOlg", "replyto": "SJNDWNOlg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper55/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper55/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959489120}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1478144348047, "tcdate": 1478144348038, "number": 55, "id": "SJNDWNOlg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "SJNDWNOlg", "signatures": ["~Jiedong_Hao1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "What Is the Best Practice for CNNs Applied to Visual Instance Retrieval?", "abstract": "Previous work has shown that feature maps of deep convolutional neural networks (CNNs)\ncan be interpreted as feature representation of a particular image region. Features aggregated from\nthese feature maps have been exploited for image retrieval tasks and achieved state-of-the-art performances in\nrecent years. The key to the success of such methods is the feature representation. However, the different\nfactors that impact the effectiveness of features are still not explored thoroughly. There are much less\ndiscussion about the best combination of them.\n\nThe main contribution of our paper is the thorough evaluations of the various factors that affect the\ndiscriminative ability of the features extracted from CNNs. Based on the evaluation results, we also identify \nthe best choices for different factors and propose a new multi-scale image feature representation method to \nencode the image effectively. Finally, we show that the proposed method generalises well and outperforms \nthe state-of-the-art methods on four typical datasets used for visual instance retrieval.", "pdf": "/pdf/a7ed76c871eabe3940c81677382ff3c612068f5b.pdf", "paperhash": "hao|what_is_the_best_practice_for_cnns_applied_to_visual_instance_retrieval", "conflicts": ["ia.ac.cn"], "keywords": ["Computer vision", "Deep learning"], "authors": ["Jiedong Hao", "Jing Dong", "Wei Wang", "Tieniu Tan"], "authorids": ["jiedong.hao@cripac.ia.ac.cn", "jdong@nlpr.ia.ac.cn", "wwang@nlpr.ia.ac.cn", "tnt@nlpr.ia.ac.cn"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}], "count": 13}