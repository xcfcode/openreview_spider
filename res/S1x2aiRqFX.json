{"notes": [{"id": "S1x2aiRqFX", "original": "BkeMaOi5t7", "number": 837, "cdate": 1538087875546, "ddate": null, "tcdate": 1538087875546, "tmdate": 1545355390518, "tddate": null, "forum": "S1x2aiRqFX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Differentiable Expected BLEU for Text Generation", "abstract": "Neural text generation models such as recurrent networks are typically trained by maximizing data log-likelihood based on cross entropy. Such training objective shows a discrepancy from test criteria like the BLEU metric. Recent work optimizes expected BLEU under the model distribution using policy gradient, while such algorithm can suffer from high variance and become impractical. In this paper, we propose a new Differentiable Expected BLEU (DEBLEU) objective that permits direct optimization of neural generation models with gradient descent. We leverage the decomposability and sparsity of BLEU, and reformulate it with moderate approximations, making the evaluation of the objective and its gradient efficient, comparable to common cross-entropy loss. We further devise a simple training procedure with ground-truth masking and annealing for stable optimization. Experiments on neural machine translation and image captioning show our method significantly improves over both cross-entropy and policy gradient training.", "keywords": ["text generation", "BLEU", "differentiable", "gradient descent", "maximum likelihood learning", "policy gradient", "machine translation"], "authorids": ["wwt10@pku.edu.cn", "zhitinghu@gmail.com", "yangtze2301@gmail.com", "shr970423@gmail.com", "epxing@cs.cmu.edu"], "authors": ["Wentao Wang", "Zhiting Hu", "Zichao Yang", "Haoran Shi", "Eric P. Xing"], "TL;DR": "A new differentiable expected BLEU objective that is end-to-end trainable with gradient descent for neural text generation models", "pdf": "/pdf/3f04f582cf9897abacd3f6adf8a61179dcb14c52.pdf", "paperhash": "wang|differentiable_expected_bleu_for_text_generation", "_bibtex": "@misc{\nwang2019differentiable,\ntitle={Differentiable Expected {BLEU} for Text Generation},\nauthor={Wentao Wang and Zhiting Hu and Zichao Yang and Haoran Shi and Eric P. Xing},\nyear={2019},\nurl={https://openreview.net/forum?id=S1x2aiRqFX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "B1x0O4pelV", "original": null, "number": 1, "cdate": 1544766581561, "ddate": null, "tcdate": 1544766581561, "tmdate": 1545354520403, "tddate": null, "forum": "S1x2aiRqFX", "replyto": "S1x2aiRqFX", "invitation": "ICLR.cc/2019/Conference/-/Paper837/Meta_Review", "content": {"metareview": "The paper presents a differentiable approximation of BLEU score, which can be directly optimized using SGD. The reviewers raised concerns about (1) direct evaluation of the quality of the approximation and (2) the significance of the experimental results. There is also a concern (3) regarding the significance of BLEU score in the first place, and whether BLEU is the right metric that one needs to directly optimize. The authors did not provide a response, and based on the concerns above (especially 1-2) I believe that the paper does not pass the bar for acceptance at ICLR.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "The paper needs improvement"}, "signatures": ["ICLR.cc/2019/Conference/Paper837/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper837/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentiable Expected BLEU for Text Generation", "abstract": "Neural text generation models such as recurrent networks are typically trained by maximizing data log-likelihood based on cross entropy. Such training objective shows a discrepancy from test criteria like the BLEU metric. Recent work optimizes expected BLEU under the model distribution using policy gradient, while such algorithm can suffer from high variance and become impractical. In this paper, we propose a new Differentiable Expected BLEU (DEBLEU) objective that permits direct optimization of neural generation models with gradient descent. We leverage the decomposability and sparsity of BLEU, and reformulate it with moderate approximations, making the evaluation of the objective and its gradient efficient, comparable to common cross-entropy loss. We further devise a simple training procedure with ground-truth masking and annealing for stable optimization. Experiments on neural machine translation and image captioning show our method significantly improves over both cross-entropy and policy gradient training.", "keywords": ["text generation", "BLEU", "differentiable", "gradient descent", "maximum likelihood learning", "policy gradient", "machine translation"], "authorids": ["wwt10@pku.edu.cn", "zhitinghu@gmail.com", "yangtze2301@gmail.com", "shr970423@gmail.com", "epxing@cs.cmu.edu"], "authors": ["Wentao Wang", "Zhiting Hu", "Zichao Yang", "Haoran Shi", "Eric P. Xing"], "TL;DR": "A new differentiable expected BLEU objective that is end-to-end trainable with gradient descent for neural text generation models", "pdf": "/pdf/3f04f582cf9897abacd3f6adf8a61179dcb14c52.pdf", "paperhash": "wang|differentiable_expected_bleu_for_text_generation", "_bibtex": "@misc{\nwang2019differentiable,\ntitle={Differentiable Expected {BLEU} for Text Generation},\nauthor={Wentao Wang and Zhiting Hu and Zichao Yang and Haoran Shi and Eric P. Xing},\nyear={2019},\nurl={https://openreview.net/forum?id=S1x2aiRqFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper837/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353067483, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1x2aiRqFX", "replyto": "S1x2aiRqFX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper837/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper837/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper837/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353067483}}}, {"id": "SyxopJ_537", "original": null, "number": 3, "cdate": 1541205955081, "ddate": null, "tcdate": 1541205955081, "tmdate": 1541533648970, "tddate": null, "forum": "S1x2aiRqFX", "replyto": "S1x2aiRqFX", "invitation": "ICLR.cc/2019/Conference/-/Paper837/Official_Review", "content": {"title": "Too many approximations in formulation, few experiments and discussion", "review": "This paper proposed a differentiable metric for text generation tasks inspired by BLEU and a random training method by the Gumbel-softmax trick to utilize the proposed metric. Experiments showed that the proposed method improves BLEU compared with simple cross entropy training and policy gradient training.\n\nPros:\n* The new metric provides a direct perspective on how good the conjunction of a generated sentence is, which has not been provided other metric historically used on language generation tasks, such as cross-entropy. \n\nCons:\n* Too many approximations that blur the relationship between the original metric (BLEU) and the derived metric.\n* Not enough experiments and poor discussion. Authors should consume more space in the paper for experiments.\n\nThe formulation of the metric consists of many approximations and it looks no longer BLEU, although the new metric shares the same motivation: \"introducing accuracy of n-gram conjunction\" to evaluate outputs. Selecting BLEU as the starting point of this study seems not a reasonable idea. Most approximations look gratuitously introduced to force to modify BLEU to the final metric, but choosing an appropriate motivation first may conduct more straightforward metric for this purpose.\n\nIn experiments on machine translation, its setting looks problematic. The corpus size is relatively smaller than other standard tasks (e.g., WMT) but the size of the network layers is large. This may result in an over-fitting of the model easily, as shown in the results of cross-entropy training in Figure 3. Authors mentioned that this tendency is caused by the \"misalignment between cross entropy and BLEU,\" however they should first remove other trivial reasons before referring an additional hypothesis.\nIn addition, the paper proposed a training method based on Gumbel softmax and annealing which affect the training stability through additional hyperparameters and annealing settings. Since the paper provided only one training case of the proposed method, we couldn't discuss if the result can be generalized or just a lucky.\n\nIf the lengths of source and target are assumed as same, the BP factor becomes always 1. Why the final metric (Eq. 17) maintains this factor?", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper837/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentiable Expected BLEU for Text Generation", "abstract": "Neural text generation models such as recurrent networks are typically trained by maximizing data log-likelihood based on cross entropy. Such training objective shows a discrepancy from test criteria like the BLEU metric. Recent work optimizes expected BLEU under the model distribution using policy gradient, while such algorithm can suffer from high variance and become impractical. In this paper, we propose a new Differentiable Expected BLEU (DEBLEU) objective that permits direct optimization of neural generation models with gradient descent. We leverage the decomposability and sparsity of BLEU, and reformulate it with moderate approximations, making the evaluation of the objective and its gradient efficient, comparable to common cross-entropy loss. We further devise a simple training procedure with ground-truth masking and annealing for stable optimization. Experiments on neural machine translation and image captioning show our method significantly improves over both cross-entropy and policy gradient training.", "keywords": ["text generation", "BLEU", "differentiable", "gradient descent", "maximum likelihood learning", "policy gradient", "machine translation"], "authorids": ["wwt10@pku.edu.cn", "zhitinghu@gmail.com", "yangtze2301@gmail.com", "shr970423@gmail.com", "epxing@cs.cmu.edu"], "authors": ["Wentao Wang", "Zhiting Hu", "Zichao Yang", "Haoran Shi", "Eric P. Xing"], "TL;DR": "A new differentiable expected BLEU objective that is end-to-end trainable with gradient descent for neural text generation models", "pdf": "/pdf/3f04f582cf9897abacd3f6adf8a61179dcb14c52.pdf", "paperhash": "wang|differentiable_expected_bleu_for_text_generation", "_bibtex": "@misc{\nwang2019differentiable,\ntitle={Differentiable Expected {BLEU} for Text Generation},\nauthor={Wentao Wang and Zhiting Hu and Zichao Yang and Haoran Shi and Eric P. Xing},\nyear={2019},\nurl={https://openreview.net/forum?id=S1x2aiRqFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper837/Official_Review", "cdate": 1542234365576, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1x2aiRqFX", "replyto": "S1x2aiRqFX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper837/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335813761, "tmdate": 1552335813761, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper837/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJluy5-qn7", "original": null, "number": 2, "cdate": 1541179872430, "ddate": null, "tcdate": 1541179872430, "tmdate": 1541533648774, "tddate": null, "forum": "S1x2aiRqFX", "replyto": "S1x2aiRqFX", "invitation": "ICLR.cc/2019/Conference/-/Paper837/Official_Review", "content": {"title": "The paper misses important references. It chooses an empirical setup which prevents comparison with related work, and the report results on de-en seem weak. The proposed approach does not bound or estimate how far from BLEU is the proposed approximation. This means that the authors need to justify empirically that it preserves correlation with BLEU, which is not shown in the paper.", "review": "Differentiable Expected BLEU for Text Generation\n\nPaper Summary:\n\nNeural translation systems optimizes training data likelihood, not the end metric of interest BLEU. This work proposes to approximate BLEU with a continuous, differentiable function that can be optimized during training.\n\nReview:\n\nThe paper reads well. It has a few but crucial missing references. The motivation is easy to understand and a relevant problem to work on. The main weaknesses of the work lies in its very loose derivations, and its weak empirical results.\n\nFirst on context/missing references: the author ignores approaches optimizing BLEU with log linear models (Franz Och 2003), and the structured prediction literature in general, both for exact (Tsochantaridis et al 2004) and approximate search (Daume and Marcu 2005). This type of approach has been applied to NMT recently (Edunov et al 2018). Your paper also misses important references addressing BLEU optimization with reinforcement strategies (Norouzi et al 2016) or (Bahdanau et al 2017). Although not targeting BLEU directly (Wiseman and Rush 16) is also a reference to cite wrt optimizing search quality directly. \n\nOn empirical results, you chose to work IWSLT in the de-en direction while most of the literature worked on en-de. It prevents comparing your results to other papers. I would suggest to switch directions and/or to report results from other methods (Ranzato et al 2015; Wiseman and Rush 2016; Norouzi et al 2016; Edunov et al 2018). De-en is generally easier than en-de (generating German) and your BLEU scores are particularly low < 25 for de-en while other methods ranges in 26-33 BLEU for en-de (Edunov et al 2018).\n\nOn the method itself, approximating BLEU with a continuous function is not easy and the approach you take involves swapping function composition and expectation multiple times in a loose way. You acknowledge that (7) is unprincipled but (10) is also problematic since this equation does not acknowledge that successive ngrams overlap and cannot be considered independent. Also, the dependence of successive words is core to NMT/conditional language models and the independence hypothesis from the footnote on page 4 can be true only for a bag of word model. Overall, I feel that given the shortcuts you take, you need to justify that your approximation of BLEU is still correlated with BLEU. I would suggest to sample from a well trained NMT system to collect several hypotheses and to measure how well your BLEU approximation correlate with BLEU. How many times BLEU decides that hypA > hypB but your approximation invert this relation? is it true for large difference, small difference of BLEU score? at low BLEU score, high BLEU score?\n\nFinally, you do not mention the distinction between expected BLEU  \\sum_y P(y|x) BLEU(y, ref) and the BLEU obtained by beam search which only look at (an estimate of) the most likely sequence y* = argmax P(y|x) . Your approach and most reinforcement strategy targets optimizing expected BLEU, but this has no guarantee to make BLEU(y*, ref) any better. Could you report both an estimate of expected BLEU and beam BLEU for different methods? In particular, MERT (), beam optimization (Wiseman and Rush 2016) and  structured prediction (Edunov et al 2018) explicitly make this distinction. This is not a side issue as this discussion is in tension with your motivations.\n\nReview Summary:\n\nThe paper misses important references. It chooses an empirical setup which prevents comparison with related work, and the report results on de-en seem weak. The proposed approach does not bound or estimate how far from BLEU is the proposed approximation. This means that the authors need to justify empirically that it preserves correlation with BLEU, which is not shown in the paper.\n\nMissing references\n\nAn Actor-Critic Algorithm for Sequence Prediction (ICLR 2017)  Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, Yoshua Bengio\n\nHal Daume III and Daniel Marcu. Learning as search optimization: Approximate large margin methods for structured prediction. ICML 2005.\n\nSergey Edunov, Myle Ott, Michael Auli, David Grangier, Marc'Aurelio Ranzato\nClassical Structured Prediction Losses for Sequence to Sequence Learning, NAACL 18\n\nMinimum Error Rate Training in Statistical Machine Translation Franz Josef Och. 2003 ACL\n\nI. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun, Support Vector Machine Learning for Interdependent and Structured Output Spaces, ICML 2004.\n\nMohammad Norouzi, Samy Bengio, Zhifeng Chen, Navdeep Jaitly, Mike Schuster, Yonghui Wu, Dale Schuurmans, Reward Augmented Maximum Likelihood for Neural Structured Prediction, 2016\n\nSequence-to-Sequence Learning as Beam-Search Optimization, Sam Wiseman and Alexander M. Rush., EMNLP 2016\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper837/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentiable Expected BLEU for Text Generation", "abstract": "Neural text generation models such as recurrent networks are typically trained by maximizing data log-likelihood based on cross entropy. Such training objective shows a discrepancy from test criteria like the BLEU metric. Recent work optimizes expected BLEU under the model distribution using policy gradient, while such algorithm can suffer from high variance and become impractical. In this paper, we propose a new Differentiable Expected BLEU (DEBLEU) objective that permits direct optimization of neural generation models with gradient descent. We leverage the decomposability and sparsity of BLEU, and reformulate it with moderate approximations, making the evaluation of the objective and its gradient efficient, comparable to common cross-entropy loss. We further devise a simple training procedure with ground-truth masking and annealing for stable optimization. Experiments on neural machine translation and image captioning show our method significantly improves over both cross-entropy and policy gradient training.", "keywords": ["text generation", "BLEU", "differentiable", "gradient descent", "maximum likelihood learning", "policy gradient", "machine translation"], "authorids": ["wwt10@pku.edu.cn", "zhitinghu@gmail.com", "yangtze2301@gmail.com", "shr970423@gmail.com", "epxing@cs.cmu.edu"], "authors": ["Wentao Wang", "Zhiting Hu", "Zichao Yang", "Haoran Shi", "Eric P. Xing"], "TL;DR": "A new differentiable expected BLEU objective that is end-to-end trainable with gradient descent for neural text generation models", "pdf": "/pdf/3f04f582cf9897abacd3f6adf8a61179dcb14c52.pdf", "paperhash": "wang|differentiable_expected_bleu_for_text_generation", "_bibtex": "@misc{\nwang2019differentiable,\ntitle={Differentiable Expected {BLEU} for Text Generation},\nauthor={Wentao Wang and Zhiting Hu and Zichao Yang and Haoran Shi and Eric P. Xing},\nyear={2019},\nurl={https://openreview.net/forum?id=S1x2aiRqFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper837/Official_Review", "cdate": 1542234365576, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1x2aiRqFX", "replyto": "S1x2aiRqFX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper837/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335813761, "tmdate": 1552335813761, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper837/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1lpbM__nX", "original": null, "number": 1, "cdate": 1541075461174, "ddate": null, "tcdate": 1541075461174, "tmdate": 1541533648537, "tddate": null, "forum": "S1x2aiRqFX", "replyto": "S1x2aiRqFX", "invitation": "ICLR.cc/2019/Conference/-/Paper837/Official_Review", "content": {"title": "official review", "review": "The paper describes a differentiable expected BLEU objective which computes expected n-gram precision values by ignoring the brevity penalty. \n\nClarity: \nSection 3 of the paper is very technical and hard to follow. Please rewrite this section to be more accessible to a wider audience by including diagrams and more explanation.\n\nOriginality/signifiance: the idea of making BLEU differentiable is a much researched topic and this paper provides a nice idea on how to make this work.\n\nEvaluation: \nThe evaluation is not very strong for the following reasons:\n\n1) The IWSLT baselines are very weak. For example, current ICLR submissions, report cross-entropy baselines of >33 BLEU, whereas this paper starts from 23 BLEU on IWSTL14 de-en (e.g., https://openreview.net/pdf?id=r1gGpjActQ), even two years ago baselines were stronger: https://arxiv.org/abs/1606.02960\n\n2) Why is policy gradient not better? You report a 0.26 BLEU improvement on IWSLT de-en, which is tiny compared to what other papers achieved, e.g., https://arxiv.org/abs/1606.02960, https://arxiv.org/abs/1711.04956\n\n3) The experiments are on some of the smallest translation tasks. IWSLT is very small and given that the method is supposed to be lightweight, i.e., not much more costly than cross-entropy, it should be feasibile to run experiments on larger datasets.\n\nThis makes me wonder how significant any improvements would be with a good baseline and on a larger datasets.\n\nAlso, which test set are you using?\n\nFinally, in Figure 3, why is cross-entropy getting worse after only ~2-4K updates? Are you overfitting? \nPlease reference this figure in the text.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper837/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentiable Expected BLEU for Text Generation", "abstract": "Neural text generation models such as recurrent networks are typically trained by maximizing data log-likelihood based on cross entropy. Such training objective shows a discrepancy from test criteria like the BLEU metric. Recent work optimizes expected BLEU under the model distribution using policy gradient, while such algorithm can suffer from high variance and become impractical. In this paper, we propose a new Differentiable Expected BLEU (DEBLEU) objective that permits direct optimization of neural generation models with gradient descent. We leverage the decomposability and sparsity of BLEU, and reformulate it with moderate approximations, making the evaluation of the objective and its gradient efficient, comparable to common cross-entropy loss. We further devise a simple training procedure with ground-truth masking and annealing for stable optimization. Experiments on neural machine translation and image captioning show our method significantly improves over both cross-entropy and policy gradient training.", "keywords": ["text generation", "BLEU", "differentiable", "gradient descent", "maximum likelihood learning", "policy gradient", "machine translation"], "authorids": ["wwt10@pku.edu.cn", "zhitinghu@gmail.com", "yangtze2301@gmail.com", "shr970423@gmail.com", "epxing@cs.cmu.edu"], "authors": ["Wentao Wang", "Zhiting Hu", "Zichao Yang", "Haoran Shi", "Eric P. Xing"], "TL;DR": "A new differentiable expected BLEU objective that is end-to-end trainable with gradient descent for neural text generation models", "pdf": "/pdf/3f04f582cf9897abacd3f6adf8a61179dcb14c52.pdf", "paperhash": "wang|differentiable_expected_bleu_for_text_generation", "_bibtex": "@misc{\nwang2019differentiable,\ntitle={Differentiable Expected {BLEU} for Text Generation},\nauthor={Wentao Wang and Zhiting Hu and Zichao Yang and Haoran Shi and Eric P. Xing},\nyear={2019},\nurl={https://openreview.net/forum?id=S1x2aiRqFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper837/Official_Review", "cdate": 1542234365576, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1x2aiRqFX", "replyto": "S1x2aiRqFX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper837/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335813761, "tmdate": 1552335813761, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper837/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rygg9AB-hm", "original": null, "number": 1, "cdate": 1540607624052, "ddate": null, "tcdate": 1540607624052, "tmdate": 1540607624052, "tddate": null, "forum": "S1x2aiRqFX", "replyto": "BJe1ZVt3iQ", "invitation": "ICLR.cc/2019/Conference/-/Paper837/Official_Comment", "content": {"title": "Thanks and response", "comment": "Thanks for your valuable comment. We didn\u2019t notice the latest update of [1] which was released on arXiv only around one month prior to this submission. With the new results in [1], we will update our statement of \u201ctoy tasks\u201d accordingly. We appreciate for pointing this out! As in the paper, we\u2019ve said \u201cour formulation uses a couple of similar approximations or assumptions\u201d with [1] and (Casas et al., 2018). Here we emphasize the difference of our work with [1] as below:\n    * Our formulation stems from a different and clear intuition of leveraging the sparsity of BLEU score, and decomposes the goal into multiple derivation steps with clear motivations.\n    * We\u2019ve developed a mask-and-anneal training process to stabilize the training. We also describe key implementation and analyze the computational complexity which is comparable to common cross-entropy training. \n    * Our formulation naturally leads to Gumbel-softmax decoding for differentiable BLEU training and gradient backpropagation along time steps, while in [1] it\u2019s unclear to us what decoding strategy is used.\n    * We believe the claim of \u201clower bound\u201d in [1] could be problematic. For example, in Eq.(20) in [1], the inequality does not necessarily hold since `min(1, c/x)` is not a convex function of x.\n\nThe difference of experimental results on IWSLT\u201914 can be attributed to different data preprocessing procedures and model configurations (e.g., input to each decoding step, #layers in encoder, etc). We will release the code.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper837/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper837/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper837/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentiable Expected BLEU for Text Generation", "abstract": "Neural text generation models such as recurrent networks are typically trained by maximizing data log-likelihood based on cross entropy. Such training objective shows a discrepancy from test criteria like the BLEU metric. Recent work optimizes expected BLEU under the model distribution using policy gradient, while such algorithm can suffer from high variance and become impractical. In this paper, we propose a new Differentiable Expected BLEU (DEBLEU) objective that permits direct optimization of neural generation models with gradient descent. We leverage the decomposability and sparsity of BLEU, and reformulate it with moderate approximations, making the evaluation of the objective and its gradient efficient, comparable to common cross-entropy loss. We further devise a simple training procedure with ground-truth masking and annealing for stable optimization. Experiments on neural machine translation and image captioning show our method significantly improves over both cross-entropy and policy gradient training.", "keywords": ["text generation", "BLEU", "differentiable", "gradient descent", "maximum likelihood learning", "policy gradient", "machine translation"], "authorids": ["wwt10@pku.edu.cn", "zhitinghu@gmail.com", "yangtze2301@gmail.com", "shr970423@gmail.com", "epxing@cs.cmu.edu"], "authors": ["Wentao Wang", "Zhiting Hu", "Zichao Yang", "Haoran Shi", "Eric P. Xing"], "TL;DR": "A new differentiable expected BLEU objective that is end-to-end trainable with gradient descent for neural text generation models", "pdf": "/pdf/3f04f582cf9897abacd3f6adf8a61179dcb14c52.pdf", "paperhash": "wang|differentiable_expected_bleu_for_text_generation", "_bibtex": "@misc{\nwang2019differentiable,\ntitle={Differentiable Expected {BLEU} for Text Generation},\nauthor={Wentao Wang and Zhiting Hu and Zichao Yang and Haoran Shi and Eric P. Xing},\nyear={2019},\nurl={https://openreview.net/forum?id=S1x2aiRqFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper837/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621624745, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1x2aiRqFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper837/Authors", "ICLR.cc/2019/Conference/Paper837/Reviewers", "ICLR.cc/2019/Conference/Paper837/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper837/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper837/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper837/Authors|ICLR.cc/2019/Conference/Paper837/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper837/Reviewers", "ICLR.cc/2019/Conference/Paper837/Authors", "ICLR.cc/2019/Conference/Paper837/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621624745}}}, {"id": "BJe1ZVt3iQ", "original": null, "number": 1, "cdate": 1540293622590, "ddate": null, "tcdate": 1540293622590, "tmdate": 1540293622590, "tddate": null, "forum": "S1x2aiRqFX", "replyto": "S1x2aiRqFX", "invitation": "ICLR.cc/2019/Conference/-/Paper837/Public_Comment", "content": {"comment": "Hi!\n\nComparing with [1], derivation of DEBLEU objective looks very similar to \"lower bound\" (LB) in [1] (it should be noted however, that your derivation is much easier to follow). What is a general difference between DEBLEU and LB of [1]?\n\nYou refer [1] as \"made preliminary attempts to develop differentiable approximations of BLEU for neural model training, but only studied on toy tasks\", however, the latest version of [1] in arXiv (dated August 23) includes experiments on IWSLT'14 and WMT'14 datasets, which show improvement over both cross-entropy and direct BLEU objectives. Moreover, [1] reports significantly higher BLEU scores with a smaller network for all objectives.\n\n[1] Vlad Zhukov, Eugene Golikov, Maksim Kretov - Differentiable lower bound for expected BLEU score. arXiv:1712.04708v4", "title": "Difference between DEBLEU and LB of [1]? Novelty and results of experiments on IWSLT'14?"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper837/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentiable Expected BLEU for Text Generation", "abstract": "Neural text generation models such as recurrent networks are typically trained by maximizing data log-likelihood based on cross entropy. Such training objective shows a discrepancy from test criteria like the BLEU metric. Recent work optimizes expected BLEU under the model distribution using policy gradient, while such algorithm can suffer from high variance and become impractical. In this paper, we propose a new Differentiable Expected BLEU (DEBLEU) objective that permits direct optimization of neural generation models with gradient descent. We leverage the decomposability and sparsity of BLEU, and reformulate it with moderate approximations, making the evaluation of the objective and its gradient efficient, comparable to common cross-entropy loss. We further devise a simple training procedure with ground-truth masking and annealing for stable optimization. Experiments on neural machine translation and image captioning show our method significantly improves over both cross-entropy and policy gradient training.", "keywords": ["text generation", "BLEU", "differentiable", "gradient descent", "maximum likelihood learning", "policy gradient", "machine translation"], "authorids": ["wwt10@pku.edu.cn", "zhitinghu@gmail.com", "yangtze2301@gmail.com", "shr970423@gmail.com", "epxing@cs.cmu.edu"], "authors": ["Wentao Wang", "Zhiting Hu", "Zichao Yang", "Haoran Shi", "Eric P. Xing"], "TL;DR": "A new differentiable expected BLEU objective that is end-to-end trainable with gradient descent for neural text generation models", "pdf": "/pdf/3f04f582cf9897abacd3f6adf8a61179dcb14c52.pdf", "paperhash": "wang|differentiable_expected_bleu_for_text_generation", "_bibtex": "@misc{\nwang2019differentiable,\ntitle={Differentiable Expected {BLEU} for Text Generation},\nauthor={Wentao Wang and Zhiting Hu and Zichao Yang and Haoran Shi and Eric P. Xing},\nyear={2019},\nurl={https://openreview.net/forum?id=S1x2aiRqFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper837/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311741101, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1x2aiRqFX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper837/Authors", "ICLR.cc/2019/Conference/Paper837/Reviewers", "ICLR.cc/2019/Conference/Paper837/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper837/Authors", "ICLR.cc/2019/Conference/Paper837/Reviewers", "ICLR.cc/2019/Conference/Paper837/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311741101}}}], "count": 7}