{"notes": [{"id": "SJgaRA4FPH", "original": "rJldYqcODH", "number": 1448, "cdate": 1569439445081, "ddate": null, "tcdate": 1569439445081, "tmdate": 1583912043873, "tddate": null, "forum": "SJgaRA4FPH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["saugenst@google.com", "mcmahan@google.com", "dramage@google.com", "swaroopram@google.com", "kairouz@google.com", "mingqing@google.com", "mathews@google.com", "blaisea@google.com"], "title": "Generative Models for Effective ML on Private, Decentralized Datasets", "authors": ["Sean Augenstein", "H. Brendan McMahan", "Daniel Ramage", "Swaroop Ramaswamy", "Peter Kairouz", "Mingqing Chen", "Rajiv Mathews", "Blaise Aguera y Arcas"], "pdf": "/pdf/ec8b5bcb57ec7ce967ce19be3e0aa3389526aca0.pdf", "TL;DR": "Generative Models + Federated Learning + Differential Privacy gives data scientists a way to analyze private, decentralized data (e.g., on mobile devices) where direct inspection is prohibited.", "abstract": "To improve real-world applications of machine learning, experienced modelers develop intuition about their datasets, their models, and how the two interact. Manual inspection of raw data\u2014of representative samples, of outliers, of misclassifications\u2014is an essential tool in a) identifying and fixing problems in the data, b) generating new modeling hypotheses,\nand c) assigning or refining human-provided labels. However, manual data inspection is risky for privacy-sensitive datasets, such as those representing the behavior of real-world individuals. Furthermore, manual data inspection is impossible in the increasingly important setting of federated learning, where raw examples are stored at the edge and the modeler may only access aggregated outputs such as metrics or model parameters. This paper demonstrates that generative models\u2014trained using federated methods and with formal differential privacy guarantees\u2014can be used effectively to debug data issues even\nwhen the data cannot be directly inspected. We explore these methods in applications to text with differentially private federated RNNs and to images using a novel algorithm for differentially private federated GANs.", "code": "https://github.com/tensorflow/federated/tree/master/tensorflow_federated/python/research/gans", "keywords": ["generative models", "federated learning", "decentralized learning", "differential privacy", "privacy", "security", "GAN"], "paperhash": "augenstein|generative_models_for_effective_ml_on_private_decentralized_datasets", "_bibtex": "@inproceedings{\nAugenstein2020Generative,\ntitle={Generative Models for Effective ML on Private, Decentralized Datasets},\nauthor={Sean Augenstein and H. Brendan McMahan and Daniel Ramage and Swaroop Ramaswamy and Peter Kairouz and Mingqing Chen and Rajiv Mathews and Blaise Aguera y Arcas},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgaRA4FPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/408514603ac0f384d775eaeb0abdefa2c1d6fe87.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "vAFRmcIOJI", "original": null, "number": 1, "cdate": 1576798723557, "ddate": null, "tcdate": 1576798723557, "tmdate": 1576800912973, "tddate": null, "forum": "SJgaRA4FPH", "replyto": "SJgaRA4FPH", "invitation": "ICLR.cc/2020/Conference/Paper1448/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "The paper provides methods for training generative models by combining federated learning techniques with differentiable privacy. The paper also provides two concrete applications for the problem of debugging models. Even though the method in the paper seems to be a standard combination of DP deep learning and federated learning, the paper is well-written and presents interesting use cases.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["saugenst@google.com", "mcmahan@google.com", "dramage@google.com", "swaroopram@google.com", "kairouz@google.com", "mingqing@google.com", "mathews@google.com", "blaisea@google.com"], "title": "Generative Models for Effective ML on Private, Decentralized Datasets", "authors": ["Sean Augenstein", "H. Brendan McMahan", "Daniel Ramage", "Swaroop Ramaswamy", "Peter Kairouz", "Mingqing Chen", "Rajiv Mathews", "Blaise Aguera y Arcas"], "pdf": "/pdf/ec8b5bcb57ec7ce967ce19be3e0aa3389526aca0.pdf", "TL;DR": "Generative Models + Federated Learning + Differential Privacy gives data scientists a way to analyze private, decentralized data (e.g., on mobile devices) where direct inspection is prohibited.", "abstract": "To improve real-world applications of machine learning, experienced modelers develop intuition about their datasets, their models, and how the two interact. Manual inspection of raw data\u2014of representative samples, of outliers, of misclassifications\u2014is an essential tool in a) identifying and fixing problems in the data, b) generating new modeling hypotheses,\nand c) assigning or refining human-provided labels. However, manual data inspection is risky for privacy-sensitive datasets, such as those representing the behavior of real-world individuals. Furthermore, manual data inspection is impossible in the increasingly important setting of federated learning, where raw examples are stored at the edge and the modeler may only access aggregated outputs such as metrics or model parameters. This paper demonstrates that generative models\u2014trained using federated methods and with formal differential privacy guarantees\u2014can be used effectively to debug data issues even\nwhen the data cannot be directly inspected. We explore these methods in applications to text with differentially private federated RNNs and to images using a novel algorithm for differentially private federated GANs.", "code": "https://github.com/tensorflow/federated/tree/master/tensorflow_federated/python/research/gans", "keywords": ["generative models", "federated learning", "decentralized learning", "differential privacy", "privacy", "security", "GAN"], "paperhash": "augenstein|generative_models_for_effective_ml_on_private_decentralized_datasets", "_bibtex": "@inproceedings{\nAugenstein2020Generative,\ntitle={Generative Models for Effective ML on Private, Decentralized Datasets},\nauthor={Sean Augenstein and H. Brendan McMahan and Daniel Ramage and Swaroop Ramaswamy and Peter Kairouz and Mingqing Chen and Rajiv Mathews and Blaise Aguera y Arcas},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgaRA4FPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/408514603ac0f384d775eaeb0abdefa2c1d6fe87.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SJgaRA4FPH", "replyto": "SJgaRA4FPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795711094, "tmdate": 1576800260222, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1448/-/Decision"}}}, {"id": "Bkl4cLB15S", "original": null, "number": 2, "cdate": 1571931788194, "ddate": null, "tcdate": 1571931788194, "tmdate": 1573839213274, "tddate": null, "forum": "SJgaRA4FPH", "replyto": "SJgaRA4FPH", "invitation": "ICLR.cc/2020/Conference/Paper1448/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "This work presents a method for using generative models to gain insight into sensitive user data, while maintaining guarantees about the privacy of that data via differential privacy (DP) techniques. This scheme takes place in the federated learning (FL) setting, where the data in question remains on a local device and only aggregate updates are sent to a centralized server. The intended application here is to use the trained generative models as a substitute for direct inspection of user data, thus providing more tools for debugging and troubleshooting deployed models in a privacy conscious manner.\n\nPros:\nGiven the growing computational power of mobile devices and the importance of privacy for large-scale deployment of machine learning, this work is a timely contribution that could augment the ML pipeline for at-scale applications dealing with sensitive data. The authors do a good job of fleshing out the intended use cases of their training scheme, and present a pair of experiments that are well-chosen for illustrating the utility of generative models when dealing with private data.\n\nCons:\nAlthough likely of practical use, the work seems to be lacking in novelty in several respects. First, the techniques developed here represent a fairly straightforward merger of DP and FL tools without much in the way of qualitatively new offerings. While the authors do develop a new GAN training scheme that works in the FL setting, this adaptation is also pretty straightforward, and mostly follows the approach laid out in [1] for training recurrent neural nets.\n\nSecondly, this paper comes in the midst of many other works aiming to integrate different combinations of generative models, privacy, and distributed training (as pointed out in the related work section). While the particular combination of techniques here differ from those in previous work, the authors don't attempt to justify why their training scheme should be preferred over these prior methods. And although their experiments are useful for understanding the general utility of generative models trained in a private and decentralized setting, they unfortunately don't permit any direct comparison with the experiments used in these previous papers.\n\nVerdict:\nFor the reasons given above, I cannot recommend acceptance of this work.\n\n[1] H Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang, Learning differentially private recurrent language models, ICLR 2018\n\n*** Follow-up after authors' rebuttal ***\n\nI'd like to thank the authors for their rebuttal, and for the significant addition to the paper in the form of an expanded Section 2. This material has helped me gain a bit better perspective on the use cases for their work, and convinced me of the potential for their methodology within real-world development of deep learning tools and services. In addition, this additional context helps to motivate the two experiments described here as fair representatives of actual debugging problems, and not simply issues that were hand-chosen to prove the authors' point.\n\nI still hold that the paper offers very little in the way of new conceptual or technical contributions, but in light of the potential utility of this privacy-conscious generative pipeline for the broader deep learning community, I have changed my score from a weak reject to a weak accept.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper1448/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1448/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["saugenst@google.com", "mcmahan@google.com", "dramage@google.com", "swaroopram@google.com", "kairouz@google.com", "mingqing@google.com", "mathews@google.com", "blaisea@google.com"], "title": "Generative Models for Effective ML on Private, Decentralized Datasets", "authors": ["Sean Augenstein", "H. Brendan McMahan", "Daniel Ramage", "Swaroop Ramaswamy", "Peter Kairouz", "Mingqing Chen", "Rajiv Mathews", "Blaise Aguera y Arcas"], "pdf": "/pdf/ec8b5bcb57ec7ce967ce19be3e0aa3389526aca0.pdf", "TL;DR": "Generative Models + Federated Learning + Differential Privacy gives data scientists a way to analyze private, decentralized data (e.g., on mobile devices) where direct inspection is prohibited.", "abstract": "To improve real-world applications of machine learning, experienced modelers develop intuition about their datasets, their models, and how the two interact. Manual inspection of raw data\u2014of representative samples, of outliers, of misclassifications\u2014is an essential tool in a) identifying and fixing problems in the data, b) generating new modeling hypotheses,\nand c) assigning or refining human-provided labels. However, manual data inspection is risky for privacy-sensitive datasets, such as those representing the behavior of real-world individuals. Furthermore, manual data inspection is impossible in the increasingly important setting of federated learning, where raw examples are stored at the edge and the modeler may only access aggregated outputs such as metrics or model parameters. This paper demonstrates that generative models\u2014trained using federated methods and with formal differential privacy guarantees\u2014can be used effectively to debug data issues even\nwhen the data cannot be directly inspected. We explore these methods in applications to text with differentially private federated RNNs and to images using a novel algorithm for differentially private federated GANs.", "code": "https://github.com/tensorflow/federated/tree/master/tensorflow_federated/python/research/gans", "keywords": ["generative models", "federated learning", "decentralized learning", "differential privacy", "privacy", "security", "GAN"], "paperhash": "augenstein|generative_models_for_effective_ml_on_private_decentralized_datasets", "_bibtex": "@inproceedings{\nAugenstein2020Generative,\ntitle={Generative Models for Effective ML on Private, Decentralized Datasets},\nauthor={Sean Augenstein and H. Brendan McMahan and Daniel Ramage and Swaroop Ramaswamy and Peter Kairouz and Mingqing Chen and Rajiv Mathews and Blaise Aguera y Arcas},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgaRA4FPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/408514603ac0f384d775eaeb0abdefa2c1d6fe87.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJgaRA4FPH", "replyto": "SJgaRA4FPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1448/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1448/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576116850574, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1448/Reviewers"], "noninvitees": [], "tcdate": 1570237737244, "tmdate": 1576116850585, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1448/-/Official_Review"}}}, {"id": "SkeZlzEDqr", "original": null, "number": 3, "cdate": 1572450793099, "ddate": null, "tcdate": 1572450793099, "tmdate": 1573430009356, "tddate": null, "forum": "SJgaRA4FPH", "replyto": "SJgaRA4FPH", "invitation": "ICLR.cc/2020/Conference/Paper1448/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #1", "review": "Goals\nThe paper identifies a key challenge in a large class of real world federate learning problems where we also have to ensure user level data privacy. In these settings the modeler can not inspect the raw data samples from the user (due to privacy concerns) and hence all modeling tasks (from data wrangling to hypothesis generation to labeling to model class selection to validation) become far more challenging. The paper proposes that in these circumstances one may use a generative model that learns the data distribution using federated learning methods with provable differentiable privacy guarantees. The generative model can then produce data (unconditional, or conditional on some features or class labels) which can be inspected by the modeler without compromising user privacy. \n\nExperiments\nThe authors illustrate the approach using existing federated DP RNN learning methods, and using a slightly novel GAN learning algorithm for images (largely similar to other algorithms). They use these methods to provide two examples: 1) learning a language model from text (word sequences) where there is a bug in pre-processing steps (tokenization); 2) learning a GAN for images of handwriting on checks where there is a pre-processing bug that inverts the grayscale of images. These examples illustrate the potential for such methods to possibly be useful to modelers. While one may quibble about some details (see section below) the experimental set up is reasonable to illustrate the need and some of the challenges modelers are likely to face in the real world (\n\nEvaluation & Questions\n\nI'm really torn because I really enjoyed the paper very much overall but I have some strong concerns as well. \n\nPositives: the paper is well motivated and very well written (it is really a pleasure to read, and it is very clear about the details -- especially after they release the code it should be possible to reproduce the results too). The authors shine a spot light on a problem that is very important & widespread (eg while learning from condifential data on cell phones). The proposed solution is fairly simple, intuitive, and quite high level (lets use a generative model that creates phantom data that can be inspected)\n\nNegatives: I am not entirely sold on this being a realistic approach in the long term -- ie that some of the key problems will ever be solvable (I'm quite ok even if they are not solved now in the first paperr). The authors do a very good job of being transparent about several potential issues (see eg last paragraphs of main paper and appendix D). My biggest concerns are below:\n* the phantom samples generated from the model need to be very realistic in order to be useful. In other words, we need to have excellent, high fidelity generate models. Even to create proper hypothesis, create proper model classes, assess convergence, or assess whether the generative model is good enough one needs to be able to inspect the raw data -- which can not be done in the first place. This can not be entirely automated eliminating need for human inspection -- and the problem is much worse in generative models (which need to encode more information) than in discriminative models which need to encode less information (bits) almost by definition. Thus one has simply traded the problem of needing to inspect data to model the final algorithm (whcih could be discriminative) and has to deal with the problem of needing data to inspect the intermediate, generative model (which is also learned using federates, DP guaranteeing ways). It is not at all clear what one accomplished by doing this. \n* GANs are notoriously hard to train with mode collapse etc.  Setting hyper parameters of any generative model also needs access to original data and impacts the privacy guarantees. \n\n***NOTE added after author response***\nThe rebuttal has sufficiently address the quibbles I raised below. I'm leaving it here to allow traceability. I'm not fully convinced about the response to the main issue I raised above (ie if the generative model is not very representative, high-fidelity, then one cant know whether a potential bug discerned by inspecting its samples is an artefact of the generative model or whether it is truly a fundamental bug upstream -- and training a high quality generative model also requires one to inspect the raw data in the first place so the problem has simply been swept under the carpet). Nevertheless for a first paper on the topic I think the contributions and intuition provided here are quite valuable so I am ok leaving this for for future work. \n\n* quibble#1: theoretical DP bounds are not very tight. For example, in table 2 they may want to use realistic estimates instead of epsilon even to prove their high level point. I'm not sure I can buy their argument even on this illustrative problem as it stands. \n* quibble#2: You may want to at least make an effort to compare against the nearest possible methods in your experimental setup even if they are not a great match to the problem. I'm not intimately familiar with the recent literature but you mention Triascyn & Faltings (2019) so perhaps you could also use that and expand a bit more on the novelty here", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1448/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1448/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["saugenst@google.com", "mcmahan@google.com", "dramage@google.com", "swaroopram@google.com", "kairouz@google.com", "mingqing@google.com", "mathews@google.com", "blaisea@google.com"], "title": "Generative Models for Effective ML on Private, Decentralized Datasets", "authors": ["Sean Augenstein", "H. Brendan McMahan", "Daniel Ramage", "Swaroop Ramaswamy", "Peter Kairouz", "Mingqing Chen", "Rajiv Mathews", "Blaise Aguera y Arcas"], "pdf": "/pdf/ec8b5bcb57ec7ce967ce19be3e0aa3389526aca0.pdf", "TL;DR": "Generative Models + Federated Learning + Differential Privacy gives data scientists a way to analyze private, decentralized data (e.g., on mobile devices) where direct inspection is prohibited.", "abstract": "To improve real-world applications of machine learning, experienced modelers develop intuition about their datasets, their models, and how the two interact. Manual inspection of raw data\u2014of representative samples, of outliers, of misclassifications\u2014is an essential tool in a) identifying and fixing problems in the data, b) generating new modeling hypotheses,\nand c) assigning or refining human-provided labels. However, manual data inspection is risky for privacy-sensitive datasets, such as those representing the behavior of real-world individuals. Furthermore, manual data inspection is impossible in the increasingly important setting of federated learning, where raw examples are stored at the edge and the modeler may only access aggregated outputs such as metrics or model parameters. This paper demonstrates that generative models\u2014trained using federated methods and with formal differential privacy guarantees\u2014can be used effectively to debug data issues even\nwhen the data cannot be directly inspected. We explore these methods in applications to text with differentially private federated RNNs and to images using a novel algorithm for differentially private federated GANs.", "code": "https://github.com/tensorflow/federated/tree/master/tensorflow_federated/python/research/gans", "keywords": ["generative models", "federated learning", "decentralized learning", "differential privacy", "privacy", "security", "GAN"], "paperhash": "augenstein|generative_models_for_effective_ml_on_private_decentralized_datasets", "_bibtex": "@inproceedings{\nAugenstein2020Generative,\ntitle={Generative Models for Effective ML on Private, Decentralized Datasets},\nauthor={Sean Augenstein and H. Brendan McMahan and Daniel Ramage and Swaroop Ramaswamy and Peter Kairouz and Mingqing Chen and Rajiv Mathews and Blaise Aguera y Arcas},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgaRA4FPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/408514603ac0f384d775eaeb0abdefa2c1d6fe87.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJgaRA4FPH", "replyto": "SJgaRA4FPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1448/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1448/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576116850574, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1448/Reviewers"], "noninvitees": [], "tcdate": 1570237737244, "tmdate": 1576116850585, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1448/-/Official_Review"}}}, {"id": "rJgi-kwNoH", "original": null, "number": 3, "cdate": 1573314306620, "ddate": null, "tcdate": 1573314306620, "tmdate": 1573314306620, "tddate": null, "forum": "SJgaRA4FPH", "replyto": "Byg96ATCtS", "invitation": "ICLR.cc/2020/Conference/Paper1448/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "We thank the reviewer for their comments.  We answer their specific questions in turn.\n\nQuestion #1 (Re: generality of approach to bugs, choice of bugs for paper)\n\nWe respectfully disagree with the reviewer\u2019s conclusions on the generality of this approach (e.g., \u201cThe two debugging illustrations are very specific in term of the errors introduced and the ways to achieve the debugging goal. It is not sure how they can be further generalized to other types of bugs.\u201d), though we acknowledge this generality was not sufficiently described in our initial submission. We have significantly revised Section 2, including adding a new Section 2.1, which we hope resolves these concerns.  In particular, apart from selecting a type of generative network that best applies to the problem domain (e.g., choosing RNNs to debug a language modeling task, choosing GANs to debug an image modeling task), no further assumptions need be made by the user about the nature of the data (or any bugs or biases therein).\n\nIndeed, it is precisely because the signal we are trying to detect is unknown that we recommend a generative model.  Were the modeler to possess additional evidence that strongly indicated a particular type of bug, a simpler data analysis may be enough to detect it.  (E.g., if the modeler of the image pipeline had reason to strongly suspect a priori that some user\u2019s images were black/white inverted, they could instead compute per-user device average pixel values and use federated computation to aggregate into a histogram.  The histogram would reveal that many devices had predominantly very white images.)  But because we assume no such a priori knowledge, we desire an approach that is as general as possible.\n\nNote that in Section 2 we now reference a recently published survey paper providing a taxonomy of faults in deep learning systems (https://arxiv.org/abs/1910.11015).  We feel this paper confirms our choice of bugs as being representative examples, as they are listed prominently (e.g., \u2018text segmentation\u2019, \u2018pixel encoding\u2019) under one of the largest subcategories of faults, \u2018Preprocessing of Training Data\u2019  (https://arxiv.org/abs/1910.11015, Figure 1).\n\n\nQuestion #2 (Re: can approach be generalized into some methodologies, what are the limits)\n\nAs mentioned, we have attempted to make the general methodology prominent in Section 2. \n\nAssessing the limits of this approach is one of the most interesting questions to explore in future work.  To make an analogy: In this paper we present the use of a new \u2018sensor\u2019 and show its promise; we hope to see the community take up research on this sensor so we can all work together to characterize its \u2018signal-to-noise\u2019 ratio. \n\nSome different limits that can be thought of are sensitivity (i.e., how much presence in the underlying data distribution is required before representative examples of a characteristic start to be synthesized) and fidelity (i.e., how realistic do synthesized examples need to be to detect a characteristic).  We\u2019ve attempted to discuss each in the paper; we welcome the reviewer\u2019s feedback if the current discussion in the paper could be improved.\n\nSensitivity : We empirically characterize the sensitivity limits of the approach in the paper, e.g., in Figure 4 and Table 4.  There we show, for RNN language models trained with varying degrees of presence of the concatenation bug, the varying levels of UNKs noticed in the generated content.  E.g., Figure 4 shows that when the bug is only present in 1% of sentences, the distribution of generated content is close to unchanged vs. the no-bug case; however, when the bug is present in 10% of sentences, a clear change in distribution of UNKs is noticeable.  Does the reviewer feel this a useful empirical analysis of sensitivity limits?\n\nFidelity : Section 2.1 now contains a discussion of the types of problems where lower-fidelity synthesis is ok and the problems where high-fidelity will likely be necessary. Please let us know if this addresses the reviewer\u2019s concerns.  (Thanks to this reviewer and other reviewers\u2019 feedback, as we realized this matter was less clearly discussed in the initial draft.)\n\nAgain, we hope this paper encourages new work in the generative modeling community, in particular to both assess current limits and hopefully push them further.  (Towards this end, we call attention to open questions about fidelity and sensitivity in the Conclusion and Open Problems sections, respectively.)  But we feel this initial paper demonstrates that there are realistic data inspection problems that exist today that can already be addressed with an approach like we describe, i.e., are useful within current limits of sensitivity and fidelity.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1448/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1448/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["saugenst@google.com", "mcmahan@google.com", "dramage@google.com", "swaroopram@google.com", "kairouz@google.com", "mingqing@google.com", "mathews@google.com", "blaisea@google.com"], "title": "Generative Models for Effective ML on Private, Decentralized Datasets", "authors": ["Sean Augenstein", "H. Brendan McMahan", "Daniel Ramage", "Swaroop Ramaswamy", "Peter Kairouz", "Mingqing Chen", "Rajiv Mathews", "Blaise Aguera y Arcas"], "pdf": "/pdf/ec8b5bcb57ec7ce967ce19be3e0aa3389526aca0.pdf", "TL;DR": "Generative Models + Federated Learning + Differential Privacy gives data scientists a way to analyze private, decentralized data (e.g., on mobile devices) where direct inspection is prohibited.", "abstract": "To improve real-world applications of machine learning, experienced modelers develop intuition about their datasets, their models, and how the two interact. Manual inspection of raw data\u2014of representative samples, of outliers, of misclassifications\u2014is an essential tool in a) identifying and fixing problems in the data, b) generating new modeling hypotheses,\nand c) assigning or refining human-provided labels. However, manual data inspection is risky for privacy-sensitive datasets, such as those representing the behavior of real-world individuals. Furthermore, manual data inspection is impossible in the increasingly important setting of federated learning, where raw examples are stored at the edge and the modeler may only access aggregated outputs such as metrics or model parameters. This paper demonstrates that generative models\u2014trained using federated methods and with formal differential privacy guarantees\u2014can be used effectively to debug data issues even\nwhen the data cannot be directly inspected. We explore these methods in applications to text with differentially private federated RNNs and to images using a novel algorithm for differentially private federated GANs.", "code": "https://github.com/tensorflow/federated/tree/master/tensorflow_federated/python/research/gans", "keywords": ["generative models", "federated learning", "decentralized learning", "differential privacy", "privacy", "security", "GAN"], "paperhash": "augenstein|generative_models_for_effective_ml_on_private_decentralized_datasets", "_bibtex": "@inproceedings{\nAugenstein2020Generative,\ntitle={Generative Models for Effective ML on Private, Decentralized Datasets},\nauthor={Sean Augenstein and H. Brendan McMahan and Daniel Ramage and Swaroop Ramaswamy and Peter Kairouz and Mingqing Chen and Rajiv Mathews and Blaise Aguera y Arcas},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgaRA4FPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/408514603ac0f384d775eaeb0abdefa2c1d6fe87.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJgaRA4FPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1448/Authors", "ICLR.cc/2020/Conference/Paper1448/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1448/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1448/Reviewers", "ICLR.cc/2020/Conference/Paper1448/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1448/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1448/Authors|ICLR.cc/2020/Conference/Paper1448/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155863, "tmdate": 1576860560165, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1448/Authors", "ICLR.cc/2020/Conference/Paper1448/Reviewers", "ICLR.cc/2020/Conference/Paper1448/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1448/-/Official_Comment"}}}, {"id": "rkeTWC8EiB", "original": null, "number": 2, "cdate": 1573314052972, "ddate": null, "tcdate": 1573314052972, "tmdate": 1573314052972, "tddate": null, "forum": "SJgaRA4FPH", "replyto": "Bkl4cLB15S", "invitation": "ICLR.cc/2020/Conference/Paper1448/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "We appreciate the reviewer\u2019s comments.\n\nFirst, we wish to clarify our view of the contributions of the paper. The principle contribution is not the introduction of new algorithms, but of a methodology for combining existing techniques together with a careful selection procedure in order to solve a large set of ML modeling challenges when working with decentralized data. While this observation may seem straightforward in hindsight, we do not believe it has been presented in any previous works. We have revised Section 2 and added a new Section 2.1 which hopefully makes this contribution more clear. While indeed we did need to make some algorithmic contributions (training user-level DP GANs on decentralized data for the first time), this is a secondary contribution.\n\nWe think something that was missed in the initial review of our paper was the uniqueness of combining federated learning, generative models, and user-level differential privacy.  We respectfully disagree with the reviewer\u2019s assessment of the level of previous work that\u2019s been done at the intersection of these 3 areas.  (E.g., the reviewer states our paper \u201ccomes in the midst of many other works\u201d; we feel this is erroneous, and revised the paper to make things more clear.)\n\nWe have significantly edited the related work section to explain how none of the existing methods directly apply to our setting (e.g., how Triascyn & Faltings 2019 uses a much weaker, empirical measure of privacy than our setting with user-level differential privacy).  In the cases where existing results are applicable, we have in fact used them directly, e.g. adopting techniques from McMahan et al. 2018 and Chen et al. 2019. \n\nDoes our revised comparison in the related work section resolve the reviewer\u2019s concerns?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1448/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1448/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["saugenst@google.com", "mcmahan@google.com", "dramage@google.com", "swaroopram@google.com", "kairouz@google.com", "mingqing@google.com", "mathews@google.com", "blaisea@google.com"], "title": "Generative Models for Effective ML on Private, Decentralized Datasets", "authors": ["Sean Augenstein", "H. Brendan McMahan", "Daniel Ramage", "Swaroop Ramaswamy", "Peter Kairouz", "Mingqing Chen", "Rajiv Mathews", "Blaise Aguera y Arcas"], "pdf": "/pdf/ec8b5bcb57ec7ce967ce19be3e0aa3389526aca0.pdf", "TL;DR": "Generative Models + Federated Learning + Differential Privacy gives data scientists a way to analyze private, decentralized data (e.g., on mobile devices) where direct inspection is prohibited.", "abstract": "To improve real-world applications of machine learning, experienced modelers develop intuition about their datasets, their models, and how the two interact. Manual inspection of raw data\u2014of representative samples, of outliers, of misclassifications\u2014is an essential tool in a) identifying and fixing problems in the data, b) generating new modeling hypotheses,\nand c) assigning or refining human-provided labels. However, manual data inspection is risky for privacy-sensitive datasets, such as those representing the behavior of real-world individuals. Furthermore, manual data inspection is impossible in the increasingly important setting of federated learning, where raw examples are stored at the edge and the modeler may only access aggregated outputs such as metrics or model parameters. This paper demonstrates that generative models\u2014trained using federated methods and with formal differential privacy guarantees\u2014can be used effectively to debug data issues even\nwhen the data cannot be directly inspected. We explore these methods in applications to text with differentially private federated RNNs and to images using a novel algorithm for differentially private federated GANs.", "code": "https://github.com/tensorflow/federated/tree/master/tensorflow_federated/python/research/gans", "keywords": ["generative models", "federated learning", "decentralized learning", "differential privacy", "privacy", "security", "GAN"], "paperhash": "augenstein|generative_models_for_effective_ml_on_private_decentralized_datasets", "_bibtex": "@inproceedings{\nAugenstein2020Generative,\ntitle={Generative Models for Effective ML on Private, Decentralized Datasets},\nauthor={Sean Augenstein and H. Brendan McMahan and Daniel Ramage and Swaroop Ramaswamy and Peter Kairouz and Mingqing Chen and Rajiv Mathews and Blaise Aguera y Arcas},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgaRA4FPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/408514603ac0f384d775eaeb0abdefa2c1d6fe87.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJgaRA4FPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1448/Authors", "ICLR.cc/2020/Conference/Paper1448/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1448/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1448/Reviewers", "ICLR.cc/2020/Conference/Paper1448/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1448/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1448/Authors|ICLR.cc/2020/Conference/Paper1448/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155863, "tmdate": 1576860560165, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1448/Authors", "ICLR.cc/2020/Conference/Paper1448/Reviewers", "ICLR.cc/2020/Conference/Paper1448/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1448/-/Official_Comment"}}}, {"id": "Bkl15TIEjB", "original": null, "number": 1, "cdate": 1573313926933, "ddate": null, "tcdate": 1573313926933, "tmdate": 1573313926933, "tddate": null, "forum": "SJgaRA4FPH", "replyto": "SkeZlzEDqr", "invitation": "ICLR.cc/2020/Conference/Paper1448/-/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "We thank the reviewer for their comments and observations, and are thrilled they enjoyed the paper and found the motivating problem and proposed solution compelling.  We now address their list of \u2018negatives\u2019 in turn.\n\nNeed for Realism:\n\nWith regards to the comment that generative models \u201cneed to be very realistic in order to be useful\u201d, it is our experience that this is not the case for many real-life problem examples, such as the pixel inversion and concatenation bugs we consider.  The measure of utility for the applications described in our paper is not realism, but rather the ability to detect the presence of distinguishing characteristics in the mimicked distribution.  We agree it is certainly the case that one could not distinguish all characteristics unless generating content to full realism, but it definitely the case that there are a broad set of characteristics that are distinguishable at well short of full realism.  We feel the two problem examples demonstrate this characteristic: while not generating extremely realistic samples, they nevertheless convey a clear \u2018signal\u2019 that is useful to the modeler, e.g., the presence of bugs. But our work far from solves the problem we address, and we hope this encourages new work in the generative modeling community.  \n\nThanks for the reviewer\u2019s comment as we\u2019ve updated the paper (e.g., Section 2.1) to better describe the types of problems where lower-fidelity synthesis is ok and the problems where high-fidelity will likely be necessary. Please let us know if this addresses the reviewer\u2019s concerns.\n\nPrivacy budgets for hyperparameter sweeps (\u201cSetting hyper parameters of any generative model also needs access to original data and impacts the privacy guarantees. \u201d):\n\nWe believe what you are referring to is that identifying the correct hyperparameters typically requires a \u2018sweep\u2019 of values, each of which involves data queries against the private data; the privacy budget must account for all these queries, not simply the final training run.  (If we have mistakenly interpreted your comment, we apologize, and would benefit from a clarification.)\n\nThis is absolutely true, and we made sure this paper raises this issue prominently.  In Section 3 on DP Federated Generative Models, we conclude the discussion of DP by noting \u201c....  that since the modeler has access to not only the auxiliary generative models, but also potentially other models trained on the same data, if an (eps, delta) guarantee is desired, the total privacy loss should be quantified across these models, e.g. ...\u201d  We also discuss the need for algorithms requiring minimal tuning as an import step for future work. Again, our contribution is primarily in highlighting this important problem, rather than solving it. Please let us know if you feel our current wording doesn\u2019t properly convey this matter prominently enough; we certainly wish to call attention to it as we hope to see further research in this area.\n\nIt\u2019s also true that this is a broader concern that impacts not just the generative models of our paper, but any ML (or other query-based) process that repeatedly samples from private data.  There are some mitigations typically proposed (i.e., using a different, proxy dataset to work out the hyperparameter values before training on the actual private data), but this continues to be an active research area in the larger DP community, which we applaud.  Along with benefiting everyone else working in DP ML (generative models or not, federated or not), it will definitely benefit those of us working with federated generative models.\n\nFinally, as the paper shows, GAN convergence did not take an exorbitant # of rounds (the generated image results we show are after 1000 rounds of federated training).  So the volume of data queries being performed when training federated generative models is in-line with the typical volume of data queries performed when doing any type of federated learning.\n\nQuibble #1 - DP bounds:\n\nWould the reviewer be able to clarify this comment further for our benefit?  We regretfully have had trouble parsing their meaning the first time around.  As DP gives us an upper bound on privacy loss, and we\u2019re achieving a DP $(\\epsilon, \\delta)$ at population scale that are indicative of a tight bound, we feel we\u2019ve shown that privacy loss is minimal?  We must be misunderstanding something in the reviewer\u2019s comment/critique.\n\nQuibble #2 - Compare with other methods:\n\nWe have significantly edited the related work section to explain how none of the existing methods directly apply to our setting; in the cases where existing results are applicable, we have in fact used them directly, e.g. adopting techniques from McMahan et al. (2018) and Chen et al. (2019). Please let us know if this does not address these concerns."}, "signatures": ["ICLR.cc/2020/Conference/Paper1448/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1448/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["saugenst@google.com", "mcmahan@google.com", "dramage@google.com", "swaroopram@google.com", "kairouz@google.com", "mingqing@google.com", "mathews@google.com", "blaisea@google.com"], "title": "Generative Models for Effective ML on Private, Decentralized Datasets", "authors": ["Sean Augenstein", "H. Brendan McMahan", "Daniel Ramage", "Swaroop Ramaswamy", "Peter Kairouz", "Mingqing Chen", "Rajiv Mathews", "Blaise Aguera y Arcas"], "pdf": "/pdf/ec8b5bcb57ec7ce967ce19be3e0aa3389526aca0.pdf", "TL;DR": "Generative Models + Federated Learning + Differential Privacy gives data scientists a way to analyze private, decentralized data (e.g., on mobile devices) where direct inspection is prohibited.", "abstract": "To improve real-world applications of machine learning, experienced modelers develop intuition about their datasets, their models, and how the two interact. Manual inspection of raw data\u2014of representative samples, of outliers, of misclassifications\u2014is an essential tool in a) identifying and fixing problems in the data, b) generating new modeling hypotheses,\nand c) assigning or refining human-provided labels. However, manual data inspection is risky for privacy-sensitive datasets, such as those representing the behavior of real-world individuals. Furthermore, manual data inspection is impossible in the increasingly important setting of federated learning, where raw examples are stored at the edge and the modeler may only access aggregated outputs such as metrics or model parameters. This paper demonstrates that generative models\u2014trained using federated methods and with formal differential privacy guarantees\u2014can be used effectively to debug data issues even\nwhen the data cannot be directly inspected. We explore these methods in applications to text with differentially private federated RNNs and to images using a novel algorithm for differentially private federated GANs.", "code": "https://github.com/tensorflow/federated/tree/master/tensorflow_federated/python/research/gans", "keywords": ["generative models", "federated learning", "decentralized learning", "differential privacy", "privacy", "security", "GAN"], "paperhash": "augenstein|generative_models_for_effective_ml_on_private_decentralized_datasets", "_bibtex": "@inproceedings{\nAugenstein2020Generative,\ntitle={Generative Models for Effective ML on Private, Decentralized Datasets},\nauthor={Sean Augenstein and H. Brendan McMahan and Daniel Ramage and Swaroop Ramaswamy and Peter Kairouz and Mingqing Chen and Rajiv Mathews and Blaise Aguera y Arcas},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgaRA4FPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/408514603ac0f384d775eaeb0abdefa2c1d6fe87.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJgaRA4FPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1448/Authors", "ICLR.cc/2020/Conference/Paper1448/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1448/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1448/Reviewers", "ICLR.cc/2020/Conference/Paper1448/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1448/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1448/Authors|ICLR.cc/2020/Conference/Paper1448/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155863, "tmdate": 1576860560165, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1448/Authors", "ICLR.cc/2020/Conference/Paper1448/Reviewers", "ICLR.cc/2020/Conference/Paper1448/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1448/-/Official_Comment"}}}, {"id": "Byg96ATCtS", "original": null, "number": 1, "cdate": 1571901121835, "ddate": null, "tcdate": 1571901121835, "tmdate": 1572972467696, "tddate": null, "forum": "SJgaRA4FPH", "replyto": "SJgaRA4FPH", "invitation": "ICLR.cc/2020/Conference/Paper1448/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a differentially private federated learning method to learn GAN with application to data bugging situations where privacy protection is needed. The proposed method tries to leave the data at the user-end to train the discriminators, and learn the generator at the centralised server. To support the debugging data related issues as claimed, two specific examples related to text and image modeling were presented. It is the generator which is DP-protected (as the discriminators are DP-protected) makes it possible where the generated data can hint the potential bugs. \n\nThe scenario being considered is interesting and two real examples have used to illustrate the idea. However, this paper falls short in the following ways: \n- It adopts what being proposed in McMahan et al. (2018) with some modifications to achieve the goal. The novelty is more related to the proposed application which allows debugging data issues to be possible when the data is private and decentralised.\n- The two debugging illustrations are very specific in term of the errors introduced and the ways to achieve the debugging goal. It is not sure how they can be further generalized to other types of bugs.\n- The paper is well written. However, the readers should have reasonable background on DP, GAN, federated learning, and generative models, or it will be hard to read through. Having said that, the authors do provide quite comprehensive literature review on related topics. But, then not much space is left for providing the necessary background and details for  the proposed federated learning for GAN with DP (other than referring to Algorithm 1). The experiment section is good.\n\nSpecific questions:\n- Other than the tokenisation bug and the image insertion bug, can more possible examples be described?\n- Can the examples be generalised into some methodologies? And, what are the limits? Will there be data inspection needs which cannot be achieved by this approach? What are they?"}, "signatures": ["ICLR.cc/2020/Conference/Paper1448/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1448/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["saugenst@google.com", "mcmahan@google.com", "dramage@google.com", "swaroopram@google.com", "kairouz@google.com", "mingqing@google.com", "mathews@google.com", "blaisea@google.com"], "title": "Generative Models for Effective ML on Private, Decentralized Datasets", "authors": ["Sean Augenstein", "H. Brendan McMahan", "Daniel Ramage", "Swaroop Ramaswamy", "Peter Kairouz", "Mingqing Chen", "Rajiv Mathews", "Blaise Aguera y Arcas"], "pdf": "/pdf/ec8b5bcb57ec7ce967ce19be3e0aa3389526aca0.pdf", "TL;DR": "Generative Models + Federated Learning + Differential Privacy gives data scientists a way to analyze private, decentralized data (e.g., on mobile devices) where direct inspection is prohibited.", "abstract": "To improve real-world applications of machine learning, experienced modelers develop intuition about their datasets, their models, and how the two interact. Manual inspection of raw data\u2014of representative samples, of outliers, of misclassifications\u2014is an essential tool in a) identifying and fixing problems in the data, b) generating new modeling hypotheses,\nand c) assigning or refining human-provided labels. However, manual data inspection is risky for privacy-sensitive datasets, such as those representing the behavior of real-world individuals. Furthermore, manual data inspection is impossible in the increasingly important setting of federated learning, where raw examples are stored at the edge and the modeler may only access aggregated outputs such as metrics or model parameters. This paper demonstrates that generative models\u2014trained using federated methods and with formal differential privacy guarantees\u2014can be used effectively to debug data issues even\nwhen the data cannot be directly inspected. We explore these methods in applications to text with differentially private federated RNNs and to images using a novel algorithm for differentially private federated GANs.", "code": "https://github.com/tensorflow/federated/tree/master/tensorflow_federated/python/research/gans", "keywords": ["generative models", "federated learning", "decentralized learning", "differential privacy", "privacy", "security", "GAN"], "paperhash": "augenstein|generative_models_for_effective_ml_on_private_decentralized_datasets", "_bibtex": "@inproceedings{\nAugenstein2020Generative,\ntitle={Generative Models for Effective ML on Private, Decentralized Datasets},\nauthor={Sean Augenstein and H. Brendan McMahan and Daniel Ramage and Swaroop Ramaswamy and Peter Kairouz and Mingqing Chen and Rajiv Mathews and Blaise Aguera y Arcas},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgaRA4FPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/408514603ac0f384d775eaeb0abdefa2c1d6fe87.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJgaRA4FPH", "replyto": "SJgaRA4FPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1448/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1448/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576116850574, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1448/Reviewers"], "noninvitees": [], "tcdate": 1570237737244, "tmdate": 1576116850585, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1448/-/Official_Review"}}}], "count": 8}