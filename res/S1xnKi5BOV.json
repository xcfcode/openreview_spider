{"notes": [{"id": "S1xnKi5BOV", "original": "HJxUko9HuV", "number": 68, "cdate": 1553472388032, "ddate": null, "tcdate": 1553472388032, "tmdate": 1562082111056, "tddate": null, "forum": "S1xnKi5BOV", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "content": {"title": "A Simple yet Effective Baseline for Robust Deep Learning with Noisy Labels", "authors": ["Anonymous"], "authorids": ["luoyc15@mails.tsinghua.edu.cn", "dcszj@mail.tsinghua.edu.cn", "tpfister@google.com"], "keywords": ["Learning with noisy labels", "generalization of deep neural networks", "robust deep learning"], "TL;DR": "The paper proposed a simple yet effective baseline for learning with noisy labels.", "abstract": "Recently deep neural networks have shown their capacity to memorize training data, even with noisy labels, which hurts generalization performance. To mitigate this issue, we propose a simple but effective method that is robust to noisy labels, even with severe noise.  Our objective involves a variance regularization term that implicitly penalizes the Jacobian norm of the neural network on the whole training set (including the noisy-labeled data), which encourages generalization and prevents overfitting to the corrupted labels. Experiments on noisy benchmarks demonstrate that our approach achieves state-of-the-art performance with a high tolerance to severe noise.", "pdf": "/pdf/4ba6db75005b77ffbe09fc3209bbfa31d6c8e346.pdf", "paperhash": "anonymous|a_simple_yet_effective_baseline_for_robust_deep_learning_with_noisy_labels"}, "signatures": ["ICLR.cc/2019/Workshop/LLD"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD"], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "cdate": 1548689671889, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "content": {"authors": {"values-regex": ".*"}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1548689671889, "tmdate": 1557933709646, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/LLD"], "details": {"writable": true}}}, "tauthor": "OpenReview.net"}, {"id": "r1lf1kprY4", "original": null, "number": 1, "cdate": 1554530009799, "ddate": null, "tcdate": 1554530009799, "tmdate": 1555512025268, "tddate": null, "forum": "S1xnKi5BOV", "replyto": "S1xnKi5BOV", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper68/Official_Review", "content": {"title": "Interesting method with excitingly good empirical results;  Paper presentation can be improved further", "review": "This paper presents a method to train deep NN with noisy labels by adding a variance regularization term.  The authors show/derive that such variance regularization term is an unbiased estimator of Jacobian norm of the neural network. As previous literature (Sokolic et al; Novak et al. 2018) show that this Jacobian norm is highly related to the generalization performance of NN, the authors conclude that minimizing their proposed variance regularization term can also help model generalization.  Finally, the authors conduct experiments on CIFAR-10 AND CIFAR-100 datasets and show their proposed learning objective can be used to train a NN robust to noise. The experimental results are strikingly good. \n\nOverall the paper is interesting, the major question is why EXACTLY minimizing the norm of Jacobian of NN can help improve the model robustness. I am not very familiar with the work (Sokolic et al; Novak et al. 2018) and thus I would appreciate a (high-level) description of those previous studies' major ideas. Furthermore, I am wondering why the authors treat good \"generalization\" of NN as the same as good \"robustness\" of NN. The authors describe label noise in Section 2.1 but I don't see how such label noise is modeled later in the regularization term design. Finally, I am a little bit confused about the derivation of the equation between equation (3) and (4), from lim_{\\tau -> 0} ... to \\frac{1}{N} \\sum_{i=1}^{N} Tr(...). More explanation on this part is appreciated. \n\nThe structure of the paper is good and writing is overall clear. There are still some places can be improved. First, at the begining of section 2, the authors write a K-class classifier f from ..., it's better to explicitly state the function f is from R^{d} to R^{K}, instead of being a scalar-valued function. This can make later discussion on Jacobian matrix more clear. Second, the connection of section 2.1 with later parts is not clear. Finally, there are some typos listed below:\n1. in the third line of section 2.1, there are two contiguous \",\"s .\n2. in the last line of page 2, there are two contiguous \",\"s .\n3. in the second line of page 3, there are two contiguous \",\"s .\n4. in the second line of section 4.1, what are the \"Sec. 5.5 and Sec. 5.6\"?\n\n\n", "rating": "3: Marginally above acceptance threshold", "confidence": "1: The reviewer's evaluation is an educated guess"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper68/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper68/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Simple yet Effective Baseline for Robust Deep Learning with Noisy Labels", "authors": ["Anonymous"], "authorids": ["luoyc15@mails.tsinghua.edu.cn", "dcszj@mail.tsinghua.edu.cn", "tpfister@google.com"], "keywords": ["Learning with noisy labels", "generalization of deep neural networks", "robust deep learning"], "TL;DR": "The paper proposed a simple yet effective baseline for learning with noisy labels.", "abstract": "Recently deep neural networks have shown their capacity to memorize training data, even with noisy labels, which hurts generalization performance. To mitigate this issue, we propose a simple but effective method that is robust to noisy labels, even with severe noise.  Our objective involves a variance regularization term that implicitly penalizes the Jacobian norm of the neural network on the whole training set (including the noisy-labeled data), which encourages generalization and prevents overfitting to the corrupted labels. Experiments on noisy benchmarks demonstrate that our approach achieves state-of-the-art performance with a high tolerance to severe noise.", "pdf": "/pdf/4ba6db75005b77ffbe09fc3209bbfa31d6c8e346.pdf", "paperhash": "anonymous|a_simple_yet_effective_baseline_for_robust_deep_learning_with_noisy_labels"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper68/Official_Review", "cdate": 1553713409983, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "S1xnKi5BOV", "replyto": "S1xnKi5BOV", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper68/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper68/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713409983, "tmdate": 1555511820341, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper68/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "S1grdN49KV", "original": null, "number": 2, "cdate": 1554822253021, "ddate": null, "tcdate": 1554822253021, "tmdate": 1555511880358, "tddate": null, "forum": "S1xnKi5BOV", "replyto": "S1xnKi5BOV", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper68/Official_Review", "content": {"title": "Impressive performance but contributions unclear", "review": "The authors propose a method for learning deep neural networks under label noise by regularizing the Jacobian of the network as a first-order approximation of the variance between perturbed examples. They argue that this will mitigate overfitting to mislabeled examples and produce smoother decision boundaries. They evaluate their methods by comparing to other noise-robust techniques on CIFAR-10 and CIFAR-100.\n\nThe authors present a complete piece of work, from a summary of noise issues in deep learning, to a description of their variance regularization technique, to their experiments. The paper follows a logical structure, and is generally well written. It would benefit from a clearer discussion of related works in the main body.\n\nThe regularization term they use is sensible, and produces strong results on the datasets in comparison to the selected methods. The results are continued in the Appendix in the data-dependent noise case. Had the authors focused on empirical variance regularization results for image classification, this would have been a much stronger paper.\n\nHowever, the authors don't deliver on their two main stated contributions. The first contribution - analysis of solution smoothness and subspace dimensionality for noisily trained models - is not presented. They reference past works (e.g. Sokolic et al., 2016) but do not provide an original analysis or evaluation as implied (\"we show empirically that the objective can learn a model with low subspace dimensionality and low hypothesis complexity\"); their experiments solely measure classification performance. The authors' second contribution is to \"propose a novel approach for training with noisy labels. \". However, variance regularization via data augmentation has been previously proposed (\"Regularization With Stochastic Transformations and Perturbations for Deep Semi-Supervised Learning\", Sajjadi et al., 2016), as has Jacobian regularization (\"Robust Large Margin Deep Neural Networks\", Sokolic et al., 2016). Jacobian regularization and related methods have also been previously analyzed in the noisy label setting (\"Gradient Regularization Improves Accuracy of Discriminative Models\", Varga et al., 2017). If the authors' proposed method is significantly different from these approaches, it is not clear from the paper.\n\nAgain, had this paper focused on empirical evaluation of Jacobian regularization for deep image recognition as compared to other methods, it would have been stronger. In its current form, the contributions are unclear.", "rating": "2: Marginally below acceptance threshold", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper68/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper68/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Simple yet Effective Baseline for Robust Deep Learning with Noisy Labels", "authors": ["Anonymous"], "authorids": ["luoyc15@mails.tsinghua.edu.cn", "dcszj@mail.tsinghua.edu.cn", "tpfister@google.com"], "keywords": ["Learning with noisy labels", "generalization of deep neural networks", "robust deep learning"], "TL;DR": "The paper proposed a simple yet effective baseline for learning with noisy labels.", "abstract": "Recently deep neural networks have shown their capacity to memorize training data, even with noisy labels, which hurts generalization performance. To mitigate this issue, we propose a simple but effective method that is robust to noisy labels, even with severe noise.  Our objective involves a variance regularization term that implicitly penalizes the Jacobian norm of the neural network on the whole training set (including the noisy-labeled data), which encourages generalization and prevents overfitting to the corrupted labels. Experiments on noisy benchmarks demonstrate that our approach achieves state-of-the-art performance with a high tolerance to severe noise.", "pdf": "/pdf/4ba6db75005b77ffbe09fc3209bbfa31d6c8e346.pdf", "paperhash": "anonymous|a_simple_yet_effective_baseline_for_robust_deep_learning_with_noisy_labels"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper68/Official_Review", "cdate": 1553713409983, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "S1xnKi5BOV", "replyto": "S1xnKi5BOV", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper68/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper68/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713409983, "tmdate": 1555511820341, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper68/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "Byx_W62zcV", "original": null, "number": 1, "cdate": 1555381504211, "ddate": null, "tcdate": 1555381504211, "tmdate": 1555510977441, "tddate": null, "forum": "S1xnKi5BOV", "replyto": "S1xnKi5BOV", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper68/Decision", "content": {"title": "Acceptance Decision", "decision": "Reject", "comment": "Reviewers found issue with the novelty, further clarification of novelty is needed"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Simple yet Effective Baseline for Robust Deep Learning with Noisy Labels", "authors": ["Anonymous"], "authorids": ["luoyc15@mails.tsinghua.edu.cn", "dcszj@mail.tsinghua.edu.cn", "tpfister@google.com"], "keywords": ["Learning with noisy labels", "generalization of deep neural networks", "robust deep learning"], "TL;DR": "The paper proposed a simple yet effective baseline for learning with noisy labels.", "abstract": "Recently deep neural networks have shown their capacity to memorize training data, even with noisy labels, which hurts generalization performance. To mitigate this issue, we propose a simple but effective method that is robust to noisy labels, even with severe noise.  Our objective involves a variance regularization term that implicitly penalizes the Jacobian norm of the neural network on the whole training set (including the noisy-labeled data), which encourages generalization and prevents overfitting to the corrupted labels. Experiments on noisy benchmarks demonstrate that our approach achieves state-of-the-art performance with a high tolerance to severe noise.", "pdf": "/pdf/4ba6db75005b77ffbe09fc3209bbfa31d6c8e346.pdf", "paperhash": "anonymous|a_simple_yet_effective_baseline_for_robust_deep_learning_with_noisy_labels"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper68/Decision", "cdate": 1554736071631, "reply": {"forum": "S1xnKi5BOV", "replyto": "S1xnKi5BOV", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1554736071631, "tmdate": 1555510967181, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}], "count": 4}