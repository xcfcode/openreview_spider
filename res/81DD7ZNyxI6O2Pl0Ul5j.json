{"notes": [{"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1459133309395, "tcdate": 1459133309395, "id": "4QnROAyoxIBYD9yOFqN7", "invitation": "ICLR.cc/2016/workshop/-/paper/7/comment", "forum": "81DD7ZNyxI6O2Pl0Ul5j", "replyto": "YW94w2X41HLknpQqIK3Q", "signatures": ["~Colin_Raffel1"], "readers": ["everyone"], "writers": ["~Colin_Raffel1"], "content": {"title": "Paper updated", "comment": "We just uploaded the camera-ready version to this submission page.  We took care to not compare it directly with an RNN, because as you said it is not an entirely valid comparison.  We also made our wording more clear about what we claim to be the capabilities and potential applications of our model.  Thank you again for your feedback!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems", "abstract": "We propose a simplified model of attention which is applicable to feed-forward neural networks and demonstrate that the resulting model can solve the synthetic \"addition\" and \"multiplication\" long-term memory problems for sequence lengths which are both longer and more widely varying than the best published results for these tasks.", "pdf": "/pdf/81DD7ZNyxI6O2Pl0Ul5j.pdf", "paperhash": "raffel|feedforward_networks_with_attention_can_solve_some_longterm_memory_problems", "conflicts": ["columbia.edu", "google.com"], "authors": ["Colin Raffel", "Daniel P. W. Ellis"], "authorids": ["craffel@gmail.com", "dpwe@ee.columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455294575995, "ddate": null, "super": null, "final": null, "tcdate": 1455294575995, "id": "ICLR.cc/2016/workshop/-/paper/7/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "81DD7ZNyxI6O2Pl0Ul5j", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/7/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1459133212702, "tcdate": 1459133212702, "id": "YWDv81xLRcLknpQqIKjB", "invitation": "ICLR.cc/2016/workshop/-/paper/7/comment", "forum": "81DD7ZNyxI6O2Pl0Ul5j", "replyto": "xnrQAOKVPc1m7RyVi320", "signatures": ["~Colin_Raffel1"], "readers": ["everyone"], "writers": ["~Colin_Raffel1"], "content": {"title": "Paper updated", "comment": "We just uploaded the camera-ready version to this submission page.  We made our wording more clear about what our claims were (e.g. not temporal dependencies, but rather the ability to refer to different entries in very long sequences when computing the output).  We also added a more explicit statement about how the toy problems are, in fact, easier with our model which is unaware of temporal order.  Thank you again for your feedback!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems", "abstract": "We propose a simplified model of attention which is applicable to feed-forward neural networks and demonstrate that the resulting model can solve the synthetic \"addition\" and \"multiplication\" long-term memory problems for sequence lengths which are both longer and more widely varying than the best published results for these tasks.", "pdf": "/pdf/81DD7ZNyxI6O2Pl0Ul5j.pdf", "paperhash": "raffel|feedforward_networks_with_attention_can_solve_some_longterm_memory_problems", "conflicts": ["columbia.edu", "google.com"], "authors": ["Colin Raffel", "Daniel P. W. Ellis"], "authorids": ["craffel@gmail.com", "dpwe@ee.columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455294575995, "ddate": null, "super": null, "final": null, "tcdate": 1455294575995, "id": "ICLR.cc/2016/workshop/-/paper/7/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "81DD7ZNyxI6O2Pl0Ul5j", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/7/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1459133035869, "tcdate": 1459133035869, "id": "vl6o1GJJpF7OYLG5in8M", "invitation": "ICLR.cc/2016/workshop/-/paper/7/comment", "forum": "81DD7ZNyxI6O2Pl0Ul5j", "replyto": "D1VBLkynVC5jEJ1zfER6", "signatures": ["~Colin_Raffel1"], "readers": ["everyone"], "writers": ["~Colin_Raffel1"], "content": {"title": "Paper updated", "comment": "Hi, just letting you know that we have integrated your suggestions and corrections into the camera-ready version, which I have just uploaded on this submission page.  Thanks again!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems", "abstract": "We propose a simplified model of attention which is applicable to feed-forward neural networks and demonstrate that the resulting model can solve the synthetic \"addition\" and \"multiplication\" long-term memory problems for sequence lengths which are both longer and more widely varying than the best published results for these tasks.", "pdf": "/pdf/81DD7ZNyxI6O2Pl0Ul5j.pdf", "paperhash": "raffel|feedforward_networks_with_attention_can_solve_some_longterm_memory_problems", "conflicts": ["columbia.edu", "google.com"], "authors": ["Colin Raffel", "Daniel P. W. Ellis"], "authorids": ["craffel@gmail.com", "dpwe@ee.columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455294575995, "ddate": null, "super": null, "final": null, "tcdate": 1455294575995, "id": "ICLR.cc/2016/workshop/-/paper/7/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "81DD7ZNyxI6O2Pl0Ul5j", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/7/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458129441190, "tcdate": 1458129441190, "id": "D1VBLkynVC5jEJ1zfER6", "invitation": "ICLR.cc/2016/workshop/-/paper/7/comment", "forum": "81DD7ZNyxI6O2Pl0Ul5j", "replyto": "BNYzYZN7EC7PwR1riXv7", "signatures": ["~Colin_Raffel1"], "readers": ["everyone"], "writers": ["~Colin_Raffel1"], "content": {"title": "Thanks! We will make your proposed updates.", "comment": "Hello, thanks for your thorough review!  Addressing your comments directly - \n\n\"I would have liked a more elaborate discussion of the weaknesses of the approach.\" - That makes sense, we will do that.  Are you referring to the fact that we should be more explicit about the drawbacks of ignoring temporal order completely?\n\n\"The only other major flaw, the lack of experiments that show that the method generalises to more complicated problems is not an issue for a workshop track IMHO.\"  Yes, we agree - we have some parallel work on real-world problems, but we decided that for a short workshop track paper it was best to keep it as simple and short as possible!\n\n\"Overall, I like the idea very much! I believe that this assumption holds for a huge variety of data sets, and that variations of the model can overcome it to some degree, e.g. by using a model that has a limited \"attention window\" spanning more than a single time step\u2013e.g. using smaller RNNs or CNNs.\"  Thank you!  In fact, we have also explored your idea of using a limited \"attention window\" in our real-world problem work; of course, we omit that here because it is not needed for the toy problems."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems", "abstract": "We propose a simplified model of attention which is applicable to feed-forward neural networks and demonstrate that the resulting model can solve the synthetic \"addition\" and \"multiplication\" long-term memory problems for sequence lengths which are both longer and more widely varying than the best published results for these tasks.", "pdf": "/pdf/81DD7ZNyxI6O2Pl0Ul5j.pdf", "paperhash": "raffel|feedforward_networks_with_attention_can_solve_some_longterm_memory_problems", "conflicts": ["columbia.edu", "google.com"], "authors": ["Colin Raffel", "Daniel P. W. Ellis"], "authorids": ["craffel@gmail.com", "dpwe@ee.columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455294575995, "ddate": null, "super": null, "final": null, "tcdate": 1455294575995, "id": "ICLR.cc/2016/workshop/-/paper/7/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "81DD7ZNyxI6O2Pl0Ul5j", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/7/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458129266086, "tcdate": 1458129266086, "id": "xnrQAOKVPc1m7RyVi320", "invitation": "ICLR.cc/2016/workshop/-/paper/7/comment", "forum": "81DD7ZNyxI6O2Pl0Ul5j", "replyto": "4Qy5VNKgBCBYD9yOFqMy", "signatures": ["~Colin_Raffel1"], "readers": ["everyone"], "writers": ["~Colin_Raffel1"], "content": {"title": "How can we better frame our approach?", "comment": "Thanks for your review!  I appreciate your point that due to the fact that independence is assumed, it is not clear to say that our model handles long-term dependencies.  What we meant by this statement was that the attention mechanism can treat the hidden state sequence h_t as a memory and address any locations in that memory when computing its output.  Do you have any input as to how we could better communicate that?  We agree that the toy problems are \"in fact easier to model if a network is a priori oblivious to any temporal ordering of the input sequence\", but that is in some sense exactly the point of the model - that certain problems (such as the toy problems, but also some real world problems as we have shown in parallel work) are easier to solve when the model ignores temporal order.  How can we better frame this argument?   Thank you again!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems", "abstract": "We propose a simplified model of attention which is applicable to feed-forward neural networks and demonstrate that the resulting model can solve the synthetic \"addition\" and \"multiplication\" long-term memory problems for sequence lengths which are both longer and more widely varying than the best published results for these tasks.", "pdf": "/pdf/81DD7ZNyxI6O2Pl0Ul5j.pdf", "paperhash": "raffel|feedforward_networks_with_attention_can_solve_some_longterm_memory_problems", "conflicts": ["columbia.edu", "google.com"], "authors": ["Colin Raffel", "Daniel P. W. Ellis"], "authorids": ["craffel@gmail.com", "dpwe@ee.columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455294575995, "ddate": null, "super": null, "final": null, "tcdate": 1455294575995, "id": "ICLR.cc/2016/workshop/-/paper/7/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "81DD7ZNyxI6O2Pl0Ul5j", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/7/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458128973218, "tcdate": 1458128973218, "id": "YW94w2X41HLknpQqIK3Q", "invitation": "ICLR.cc/2016/workshop/-/paper/7/comment", "forum": "81DD7ZNyxI6O2Pl0Ul5j", "replyto": "K1VgppYmZc28XMlNCVpx", "signatures": ["~Colin_Raffel1"], "readers": ["everyone"], "writers": ["~Colin_Raffel1"], "content": {"title": "Thanks! Help clarifying?", "comment": "Hi, thank you for your review!  I appreciate your point that saying that our approach handles long term dependencies may not be the best way to describe its behavior and capabilities.  On the one hand, our approach has no notion of temporal order, but on the other hand we do consider the hidden state sequence h_t as a memory which our attention mechanism can address.  This allows the output to consider different points in the memory when it is computing its output.  As you suggest, the RNN is much more generic, but the idea behind this paper is to argue that for some tasks involving sequential data (such as these toy problems), order is less important than being able to refer to vastly different time instances in the input sequence.  Do you have any suggestions for us to clarify this point?  Based on your comments, we will take much more care to not compare it to an RNN directly, or without explicitly reiterating its drawbacks.  Regarding your comment that \"it performs almost as well as a simple baseline that just sums all the inputs.\", I think there may be a misunderstanding here - the unweighted average approach does not perform nearly as well, because it was not able to solve the toy problems for all sequence lengths and was unable to solve the problems for varying sequence lengths.  How can we better communicate this?  Thank you again!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems", "abstract": "We propose a simplified model of attention which is applicable to feed-forward neural networks and demonstrate that the resulting model can solve the synthetic \"addition\" and \"multiplication\" long-term memory problems for sequence lengths which are both longer and more widely varying than the best published results for these tasks.", "pdf": "/pdf/81DD7ZNyxI6O2Pl0Ul5j.pdf", "paperhash": "raffel|feedforward_networks_with_attention_can_solve_some_longterm_memory_problems", "conflicts": ["columbia.edu", "google.com"], "authors": ["Colin Raffel", "Daniel P. W. Ellis"], "authorids": ["craffel@gmail.com", "dpwe@ee.columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455294575995, "ddate": null, "super": null, "final": null, "tcdate": 1455294575995, "id": "ICLR.cc/2016/workshop/-/paper/7/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "81DD7ZNyxI6O2Pl0Ul5j", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/7/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458116854804, "tcdate": 1458116854804, "id": "BNYzYZN7EC7PwR1riXv7", "invitation": "ICLR.cc/2016/workshop/-/paper/7/review/10", "forum": "81DD7ZNyxI6O2Pl0Ul5j", "replyto": "81DD7ZNyxI6O2Pl0Ul5j", "signatures": ["ICLR.cc/2016/workshop/paper/7/reviewer/10"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/7/reviewer/10"], "content": {"title": "Nice workshop paper with an interesting direction", "rating": "7: Good paper, accept", "review": "The paper introduces a simplified attention mechanism by replacing the recurrent attention model with a feed forward one. It is shown that this is sufficient for some of the pathological long term problems used to evaluate the long term capabilities of RNNs. The results are convincing in the sense that performance is greatly improved and that problems targeted by the model can be solved.\n\nI would have liked a more elaborate discussion of the weaknesses of the approach. Clearly, cases where attention requires\n\nWhile this clearly limits the power of the model, as more some patterns cannot be detected. The feed forward attention assumes that an optimal attention value can be estimated by the model without the context: this is clearly not the case for a large set of data sets, such as in NLP.\n\nThe only other major flaw, the lack of experiments that show that the method generalises to more complicated problems is not an issue for a workshop track IMHO.\n\nOverall, I like the idea very much! I believe that this assumption holds for a huge variety of data sets, and that variations of the model can overcome it to some degree, e.g. by using a model that has a limited \"attention window\" spanning more than a single time step\u2013e.g. using smaller RNNs or CNNs.  Therefore I would like to see the paper accepted to the workshop track so that the authors get a chance to discuss this ongoing work with other researchers. I am confident that this work can result in a method that is generally applicable to a wide range of problems.\n\n\nMinors\n\n- Wrong citation way, use \\citep and \\cite correctly. (e.g. \"and exploding gradient problem Pascanu et al. (2012);\"\n- References are used as nouns\n- Use \u201cEquation (1)\u201d instead of \u201cEquation 1\"", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems", "abstract": "We propose a simplified model of attention which is applicable to feed-forward neural networks and demonstrate that the resulting model can solve the synthetic \"addition\" and \"multiplication\" long-term memory problems for sequence lengths which are both longer and more widely varying than the best published results for these tasks.", "pdf": "/pdf/81DD7ZNyxI6O2Pl0Ul5j.pdf", "paperhash": "raffel|feedforward_networks_with_attention_can_solve_some_longterm_memory_problems", "conflicts": ["columbia.edu", "google.com"], "authors": ["Colin Raffel", "Daniel P. W. Ellis"], "authorids": ["craffel@gmail.com", "dpwe@ee.columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456579935653, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456579935653, "id": "ICLR.cc/2016/workshop/-/paper/7/review/10", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "81DD7ZNyxI6O2Pl0Ul5j", "replyto": "81DD7ZNyxI6O2Pl0Ul5j", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/7/reviewer/10", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457911242909, "tcdate": 1457911242909, "id": "4Qy5VNKgBCBYD9yOFqMy", "invitation": "ICLR.cc/2016/workshop/-/paper/7/review/12", "forum": "81DD7ZNyxI6O2Pl0Ul5j", "replyto": "81DD7ZNyxI6O2Pl0Ul5j", "signatures": ["ICLR.cc/2016/workshop/paper/7/reviewer/12"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/7/reviewer/12"], "content": {"title": "There's an issue with how the authors have framed their approach and consequently evaluated their approach.", "rating": "4: Ok but not good enough - rejection", "review": "Capturing long-term dependencies is difficult when a problem requires temporal positions of input symbols to be exploited. From its definition, the proposed model assumes temporal independence among the input symbols (Eq. (1) and h_t = f(x_t)), and it's not supposed to capture well, if not at all, any interesting long-term dependencies in the input sequence. \n\nThis is not to say that the proposed approach will not be useful at all. Rather, it should be framed differently in a very different context. In this regard, the evaluation in this paper is quite meaningless, as both of the tasks (addition and multiplication) are commutative and in fact easier to model if a network is a priori oblivious to any temporal ordering of the input sequence. \n\nIn short, the argument for the proposed approach is wrong, and therefore, the evaluation is rather meaningless. \n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems", "abstract": "We propose a simplified model of attention which is applicable to feed-forward neural networks and demonstrate that the resulting model can solve the synthetic \"addition\" and \"multiplication\" long-term memory problems for sequence lengths which are both longer and more widely varying than the best published results for these tasks.", "pdf": "/pdf/81DD7ZNyxI6O2Pl0Ul5j.pdf", "paperhash": "raffel|feedforward_networks_with_attention_can_solve_some_longterm_memory_problems", "conflicts": ["columbia.edu", "google.com"], "authors": ["Colin Raffel", "Daniel P. W. Ellis"], "authorids": ["craffel@gmail.com", "dpwe@ee.columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456579934650, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456579934650, "id": "ICLR.cc/2016/workshop/-/paper/7/review/12", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "81DD7ZNyxI6O2Pl0Ul5j", "replyto": "81DD7ZNyxI6O2Pl0Ul5j", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/7/reviewer/12", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457652270628, "tcdate": 1457652270628, "id": "K1VgppYmZc28XMlNCVpx", "invitation": "ICLR.cc/2016/workshop/-/paper/7/review/11", "forum": "81DD7ZNyxI6O2Pl0Ul5j", "replyto": "81DD7ZNyxI6O2Pl0Ul5j", "signatures": ["ICLR.cc/2016/workshop/paper/7/reviewer/11"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/7/reviewer/11"], "content": {"title": "The Idea has very limited applicability and can not be considered a method for handling long-term dependencies", "rating": "4: Ok but not good enough - rejection", "review": "The paper proposes a simplified version of an attention mechanism. The proposed mechanism summarizes the input sequence into a context by taking a weighted sum of its elements, where the weights are computed by a feed-forward network. The resulting context is further used by the same feedforward network.\n\nI do not think it is correct to say that the proposed approach actually handles long-term dependencies. It does solve the considered toy tasks, but it performs almost as well as a simple baseline that just sums all the inputs. As mentioned in the paper,\nthe information about positions of the input elements relative to each other is completely discarded by the proposed method. Thus, I do not think that it is fair to compare it with RNN, as the latter are much more generic.\n\nWhile the approach may be suitable for some applications, I think that the presenting it as a method to handle long-term dependencies is wrong.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems", "abstract": "We propose a simplified model of attention which is applicable to feed-forward neural networks and demonstrate that the resulting model can solve the synthetic \"addition\" and \"multiplication\" long-term memory problems for sequence lengths which are both longer and more widely varying than the best published results for these tasks.", "pdf": "/pdf/81DD7ZNyxI6O2Pl0Ul5j.pdf", "paperhash": "raffel|feedforward_networks_with_attention_can_solve_some_longterm_memory_problems", "conflicts": ["columbia.edu", "google.com"], "authors": ["Colin Raffel", "Daniel P. W. Ellis"], "authorids": ["craffel@gmail.com", "dpwe@ee.columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456579935066, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456579935066, "id": "ICLR.cc/2016/workshop/-/paper/7/review/11", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "81DD7ZNyxI6O2Pl0Ul5j", "replyto": "81DD7ZNyxI6O2Pl0Ul5j", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/7/reviewer/11", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1455294575613, "tcdate": 1455294575613, "id": "81DD7ZNyxI6O2Pl0Ul5j", "invitation": "ICLR.cc/2016/workshop/-/submission", "forum": "81DD7ZNyxI6O2Pl0Ul5j", "signatures": ["~Colin_Raffel1"], "readers": ["everyone"], "writers": ["~Colin_Raffel1"], "content": {"CMT_id": "", "title": "Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems", "abstract": "We propose a simplified model of attention which is applicable to feed-forward neural networks and demonstrate that the resulting model can solve the synthetic \"addition\" and \"multiplication\" long-term memory problems for sequence lengths which are both longer and more widely varying than the best published results for these tasks.", "pdf": "/pdf/81DD7ZNyxI6O2Pl0Ul5j.pdf", "paperhash": "raffel|feedforward_networks_with_attention_can_solve_some_longterm_memory_problems", "conflicts": ["columbia.edu", "google.com"], "authors": ["Colin Raffel", "Daniel P. W. Ellis"], "authorids": ["craffel@gmail.com", "dpwe@ee.columbia.edu"]}, "nonreaders": [], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1454464564200, "ddate": null, "super": null, "final": null, "duedate": 1455833700000, "tcdate": 1454464564200, "id": "ICLR.cc/2016/workshop/-/submission", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"order": 4, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv.", "value-regex": "upload|http://arxiv.org/pdf/.+"}, "title": {"order": 3, "description": "Title of paper.", "value-regex": ".{0,500}"}, "abstract": {"order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"order": 1, "description": "Comma separated list of author names, as they appear in the paper.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "author_emails": {"order": 2, "description": "Comma separated list of author email addresses, in the same order as above.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "conflicts": {"order": 100, "description": "Semi-colon separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.).", "value-regex": "^([a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))+(;[a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))*$"}, "CMT_id": {"order": 5, "value-regex": ".*", "description": "If the paper is a resubmission from the ICLR 2016 Conference Track, enter its CMT ID; otherwise, leave blank."}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "expdate": 1463609700000}}}], "count": 10}