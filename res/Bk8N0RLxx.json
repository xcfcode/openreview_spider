{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396320308, "tcdate": 1486396320308, "number": 1, "id": "SkuiiGIOg", "invitation": "ICLR.cc/2017/conference/-/paper40/acceptance", "forum": "Bk8N0RLxx", "replyto": "Bk8N0RLxx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The reviewers agree that the method is exciting as practical contributions go, but the case for originality is not strong enough."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Vocabulary Selection Strategies for Neural Machine Translation", "abstract": "Classical translation models constrain the space of possible outputs by selecting a subset of translation rules based on the input sentence. Recent work on improving the efficiency of neural translation models adopted a similar strategy by restricting the output vocabulary to a subset of likely candidates given the source. In this paper we experiment with context and embedding-based selection methods and extend previous work by examining speed and accuracy trade-offs in more detail. We show that decoding time on CPUs can be reduced by up to 90% and training time by 25% on the WMT15 English-German and WMT16 English-Romanian tasks at the same or only negligible change in accuracy. This brings the time to decode with a state of the art neural translation system to just over 140 words per seconds on a single CPU core for English-German.", "pdf": "/pdf/7227c1343250912077aa3080f3198bfc6e10d3ba.pdf", "TL;DR": "Neural machine translation can reach same accuracy with a 10x speedup by pruning the vocabulary prior to decoding.", "paperhash": "lhostis|vocabulary_selection_strategies_for_neural_machine_translation", "keywords": ["Natural language processing"], "conflicts": ["facebook.com", "microsoft.com"], "authors": ["Gurvan L'Hostis", "David Grangier", "Michael Auli"], "authorids": ["gurvan.lhostis@polytechnique.edu", "grangier@fb.com", "michaelauli@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396320869, "id": "ICLR.cc/2017/conference/-/paper40/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "Bk8N0RLxx", "replyto": "Bk8N0RLxx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396320869}}}, {"tddate": null, "tmdate": 1483630491591, "tcdate": 1483630491591, "number": 4, "id": "By4jvk3re", "invitation": "ICLR.cc/2017/conference/-/paper40/official/review", "forum": "Bk8N0RLxx", "replyto": "Bk8N0RLxx", "signatures": ["ICLR.cc/2017/conference/paper40/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper40/AnonReviewer3"], "content": {"title": "A solid, practical paper - but not very innovative", "rating": "5: Marginally below acceptance threshold", "review": "This paper conducts a comprehensive series of experiments on vocabulary selection strategies to reduce the computational cost of neural machine translation.\n\nA range of techniques are investigated, ranging from very simple methods such as word co-occurences, to the relatively complex use of SVMs.\n\nThe experiments are solid, comprehensive and very useful in practical terms.  It is good to see that the best vocabulary selection method is very effective at achieving a very high proportion of the coverage of the full-vocabulary model (fig 3).  However, I feel that the experiments in section 4.3 (vocabulary selection during training) was rather limited in their scope - I would have liked to see more experiments here.\n\nA major criticism I have with this paper is that there is little novelty here.  The techniques are mostly standard methods and rather simple, and in particular, there it seems that there is not much additional material beyond the work of Mi et al (2016).  So although the work is solid, the lack of originality lets it down.\n\nMinor comments: in 2.1, the word co-occurence measure - was any smoothing used to make this measure more robust to low counts?\n\n\n\n\n\n\n\n\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Vocabulary Selection Strategies for Neural Machine Translation", "abstract": "Classical translation models constrain the space of possible outputs by selecting a subset of translation rules based on the input sentence. Recent work on improving the efficiency of neural translation models adopted a similar strategy by restricting the output vocabulary to a subset of likely candidates given the source. In this paper we experiment with context and embedding-based selection methods and extend previous work by examining speed and accuracy trade-offs in more detail. We show that decoding time on CPUs can be reduced by up to 90% and training time by 25% on the WMT15 English-German and WMT16 English-Romanian tasks at the same or only negligible change in accuracy. This brings the time to decode with a state of the art neural translation system to just over 140 words per seconds on a single CPU core for English-German.", "pdf": "/pdf/7227c1343250912077aa3080f3198bfc6e10d3ba.pdf", "TL;DR": "Neural machine translation can reach same accuracy with a 10x speedup by pruning the vocabulary prior to decoding.", "paperhash": "lhostis|vocabulary_selection_strategies_for_neural_machine_translation", "keywords": ["Natural language processing"], "conflicts": ["facebook.com", "microsoft.com"], "authors": ["Gurvan L'Hostis", "David Grangier", "Michael Auli"], "authorids": ["gurvan.lhostis@polytechnique.edu", "grangier@fb.com", "michaelauli@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483630492275, "id": "ICLR.cc/2017/conference/-/paper40/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper40/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper40/AnonReviewer4", "ICLR.cc/2017/conference/paper40/AnonReviewer2", "ICLR.cc/2017/conference/paper40/AnonReviewer5", "ICLR.cc/2017/conference/paper40/AnonReviewer3"], "reply": {"forum": "Bk8N0RLxx", "replyto": "Bk8N0RLxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper40/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper40/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483630492275}}}, {"tddate": null, "tmdate": 1483073761555, "tcdate": 1483073761555, "number": 3, "id": "Sy5ytDQBx", "invitation": "ICLR.cc/2017/conference/-/paper40/official/review", "forum": "Bk8N0RLxx", "replyto": "Bk8N0RLxx", "signatures": ["ICLR.cc/2017/conference/paper40/AnonReviewer5"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper40/AnonReviewer5"], "content": {"title": "vocabulary selection is a very promising technique", "rating": "4: Ok but not good enough - rejection", "review": "This paper compares several strategies for guessing a short list of vocabulary for the target language in neural machine translation. The primary findings are that word alignment dictionaries work better than a variety of other techniques.\n\nMy take on this paper is that to have a significant impact, it needs to make the case for why one might want vocabulary rather than characters or sub word units like BPE. I think there are likely many very good reasons to do this that could be argued for (synthesize morphology, deal with transliteration, etc), but most of these would suggest some particular models and experiments, which are of course not in this paper. As it is, I think this paper is a useful but minor contribution that shows that word alignment is a good way of getting short lists, but it does not strongly make the case that we should abandon work in other directions.\n\nMinor comments:\nIn addition to the SVM approach for modeling vocabulary, the discriminative word lexicon of Mauser et al. (2009) and the neural version of Ha et al. (2014) are also worth mentioning.\n\nIt would be useful to know what the coverage rate of the actual full vocabulary would be (rather than the 100k \u201cfull vocabulary\u201d). Since presumably this technique could be used to work with much larger vocabularies.\n\nWhen reducing the vocabulary size for training, the Mi et al. (2016) technique of taking the union of all the vocabularies in a mini batch seems like a rather strange objective. If the vocabulary of a single sentence is used, the probabilistic semantics of the translation model can still be preserved since p(e | f, vocab(f)) = p(e | f) if p(vocab(f) | f) = 1, i.e., is deterministic, which it is here. Whereas the objective is no longer a sensible probability model in the mini batch vocabulary case. Thus, while it may be a bit more difficult to implement, it seems like it would at least be a sensible comparison to make.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Vocabulary Selection Strategies for Neural Machine Translation", "abstract": "Classical translation models constrain the space of possible outputs by selecting a subset of translation rules based on the input sentence. Recent work on improving the efficiency of neural translation models adopted a similar strategy by restricting the output vocabulary to a subset of likely candidates given the source. In this paper we experiment with context and embedding-based selection methods and extend previous work by examining speed and accuracy trade-offs in more detail. We show that decoding time on CPUs can be reduced by up to 90% and training time by 25% on the WMT15 English-German and WMT16 English-Romanian tasks at the same or only negligible change in accuracy. This brings the time to decode with a state of the art neural translation system to just over 140 words per seconds on a single CPU core for English-German.", "pdf": "/pdf/7227c1343250912077aa3080f3198bfc6e10d3ba.pdf", "TL;DR": "Neural machine translation can reach same accuracy with a 10x speedup by pruning the vocabulary prior to decoding.", "paperhash": "lhostis|vocabulary_selection_strategies_for_neural_machine_translation", "keywords": ["Natural language processing"], "conflicts": ["facebook.com", "microsoft.com"], "authors": ["Gurvan L'Hostis", "David Grangier", "Michael Auli"], "authorids": ["gurvan.lhostis@polytechnique.edu", "grangier@fb.com", "michaelauli@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483630492275, "id": "ICLR.cc/2017/conference/-/paper40/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper40/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper40/AnonReviewer4", "ICLR.cc/2017/conference/paper40/AnonReviewer2", "ICLR.cc/2017/conference/paper40/AnonReviewer5", "ICLR.cc/2017/conference/paper40/AnonReviewer3"], "reply": {"forum": "Bk8N0RLxx", "replyto": "Bk8N0RLxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper40/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper40/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483630492275}}}, {"tddate": null, "tmdate": 1481961910618, "tcdate": 1481961858915, "number": 2, "id": "HyiYbufVl", "invitation": "ICLR.cc/2017/conference/-/paper40/official/review", "forum": "Bk8N0RLxx", "replyto": "Bk8N0RLxx", "signatures": ["ICLR.cc/2017/conference/paper40/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper40/AnonReviewer2"], "content": {"title": "A well-executed NLP paper but with little novelty", "rating": "4: Ok but not good enough - rejection", "review": "In this paper, the authors present several strategies to select a small subset of target vocabulary to work with per source sentence, which results in significant speedup. The results are convincing and I think this paper offers practical values to general seq2seq approaches to language tasks. However, there is little novelty in this work: the authors further mostly extend the work of (Mi et al., 2016) with more vocabulary selection strategies and thorough experiments. This paper will fit better in an NLP venue.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Vocabulary Selection Strategies for Neural Machine Translation", "abstract": "Classical translation models constrain the space of possible outputs by selecting a subset of translation rules based on the input sentence. Recent work on improving the efficiency of neural translation models adopted a similar strategy by restricting the output vocabulary to a subset of likely candidates given the source. In this paper we experiment with context and embedding-based selection methods and extend previous work by examining speed and accuracy trade-offs in more detail. We show that decoding time on CPUs can be reduced by up to 90% and training time by 25% on the WMT15 English-German and WMT16 English-Romanian tasks at the same or only negligible change in accuracy. This brings the time to decode with a state of the art neural translation system to just over 140 words per seconds on a single CPU core for English-German.", "pdf": "/pdf/7227c1343250912077aa3080f3198bfc6e10d3ba.pdf", "TL;DR": "Neural machine translation can reach same accuracy with a 10x speedup by pruning the vocabulary prior to decoding.", "paperhash": "lhostis|vocabulary_selection_strategies_for_neural_machine_translation", "keywords": ["Natural language processing"], "conflicts": ["facebook.com", "microsoft.com"], "authors": ["Gurvan L'Hostis", "David Grangier", "Michael Auli"], "authorids": ["gurvan.lhostis@polytechnique.edu", "grangier@fb.com", "michaelauli@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483630492275, "id": "ICLR.cc/2017/conference/-/paper40/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper40/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper40/AnonReviewer4", "ICLR.cc/2017/conference/paper40/AnonReviewer2", "ICLR.cc/2017/conference/paper40/AnonReviewer5", "ICLR.cc/2017/conference/paper40/AnonReviewer3"], "reply": {"forum": "Bk8N0RLxx", "replyto": "Bk8N0RLxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper40/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper40/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483630492275}}}, {"tddate": null, "tmdate": 1481909302954, "tcdate": 1481909302954, "number": 1, "id": "SkJBNo-Ve", "invitation": "ICLR.cc/2017/conference/-/paper40/official/review", "forum": "Bk8N0RLxx", "replyto": "Bk8N0RLxx", "signatures": ["ICLR.cc/2017/conference/paper40/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper40/AnonReviewer4"], "content": {"title": "Useful tricks for faster decoding and training of NMT", "rating": "5: Marginally below acceptance threshold", "review": "This paper evaluates several strategies to reduce output vocabulary size in order to speed up NMT decoding and training. It could be quite useful to practitioners, although the main contributions of the paper seem somewhat orthogonal to representation learning and neural networks, and I am not sure ICLR is the ideal venue for this work.\n\n- Do the reported decoding times take into account the vocabulary reduction step?\n- Aside from machine translation, might there be applications to other settings such as language modeling, where large vocabulary is also a scalability challenge?\n- The proposed methods are helpful because of the difficulties induced by using a word-level model. But (at least in my opinion) starting from a character or even lower-level abstraction seems to be the obvious solution to the huge vocabulary problem.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Vocabulary Selection Strategies for Neural Machine Translation", "abstract": "Classical translation models constrain the space of possible outputs by selecting a subset of translation rules based on the input sentence. Recent work on improving the efficiency of neural translation models adopted a similar strategy by restricting the output vocabulary to a subset of likely candidates given the source. In this paper we experiment with context and embedding-based selection methods and extend previous work by examining speed and accuracy trade-offs in more detail. We show that decoding time on CPUs can be reduced by up to 90% and training time by 25% on the WMT15 English-German and WMT16 English-Romanian tasks at the same or only negligible change in accuracy. This brings the time to decode with a state of the art neural translation system to just over 140 words per seconds on a single CPU core for English-German.", "pdf": "/pdf/7227c1343250912077aa3080f3198bfc6e10d3ba.pdf", "TL;DR": "Neural machine translation can reach same accuracy with a 10x speedup by pruning the vocabulary prior to decoding.", "paperhash": "lhostis|vocabulary_selection_strategies_for_neural_machine_translation", "keywords": ["Natural language processing"], "conflicts": ["facebook.com", "microsoft.com"], "authors": ["Gurvan L'Hostis", "David Grangier", "Michael Auli"], "authorids": ["gurvan.lhostis@polytechnique.edu", "grangier@fb.com", "michaelauli@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483630492275, "id": "ICLR.cc/2017/conference/-/paper40/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper40/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper40/AnonReviewer4", "ICLR.cc/2017/conference/paper40/AnonReviewer2", "ICLR.cc/2017/conference/paper40/AnonReviewer5", "ICLR.cc/2017/conference/paper40/AnonReviewer3"], "reply": {"forum": "Bk8N0RLxx", "replyto": "Bk8N0RLxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper40/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper40/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483630492275}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1478057518108, "tcdate": 1478057518099, "number": 40, "id": "Bk8N0RLxx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "Bk8N0RLxx", "signatures": ["~David_Grangier1"], "readers": ["everyone"], "content": {"title": "Vocabulary Selection Strategies for Neural Machine Translation", "abstract": "Classical translation models constrain the space of possible outputs by selecting a subset of translation rules based on the input sentence. Recent work on improving the efficiency of neural translation models adopted a similar strategy by restricting the output vocabulary to a subset of likely candidates given the source. In this paper we experiment with context and embedding-based selection methods and extend previous work by examining speed and accuracy trade-offs in more detail. We show that decoding time on CPUs can be reduced by up to 90% and training time by 25% on the WMT15 English-German and WMT16 English-Romanian tasks at the same or only negligible change in accuracy. This brings the time to decode with a state of the art neural translation system to just over 140 words per seconds on a single CPU core for English-German.", "pdf": "/pdf/7227c1343250912077aa3080f3198bfc6e10d3ba.pdf", "TL;DR": "Neural machine translation can reach same accuracy with a 10x speedup by pruning the vocabulary prior to decoding.", "paperhash": "lhostis|vocabulary_selection_strategies_for_neural_machine_translation", "keywords": ["Natural language processing"], "conflicts": ["facebook.com", "microsoft.com"], "authors": ["Gurvan L'Hostis", "David Grangier", "Michael Auli"], "authorids": ["gurvan.lhostis@polytechnique.edu", "grangier@fb.com", "michaelauli@fb.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}], "count": 6}