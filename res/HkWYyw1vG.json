{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124464575, "tcdate": 1518460793278, "number": 200, "cdate": 1518460793278, "id": "HkWYyw1vG", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "HkWYyw1vG", "signatures": ["~Yongxi_Tan1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "SIM-TO-REAL OPTIMIZATION OF COMPLEX REAL WORLD MOBILE NETWORK BY DEEP REINFORCEMENT LEARNING FROM SELF-PLAY", "abstract": "Mobile network that millions of people use every day is one of the most complex systems in real world. Optimization of mobile network to meet exploding customer demand and reduce CAPEX/OPEX poses greater challenges than in prior works. Actually, learning to solve complex problems in real world to benefit everyone and make the world better has long been ultimate goal of AI. However, it still remains an unsolved problem for deep reinforcement learning (DRL), given incomplete/imperfect information in real world, huge state/action space, lots of data needed for training, associated time/cost, interactions among multi-agents, potential negative impact to real world, etc. To bridge this reality gap, we proposed a DRL framework to direct transfer optimal policy learned from multi-tasks in source domain to unseen similar tasks in target domain without any further training in both domains. First, we distilled temporal-spatial relationships between cells and mobile users to scalable 3D image-like tensor to best characterize partially observed mobile network. Second, inspired by AlphaGo, we used a novel self-play mechanism to empower DRL agent to gradually improve its intelligence by competing for best record on multiple tasks. Third, a decentralized DRL method is proposed to coordinate multi-agents to compete and cooperate as a team to maximize global reward and minimize potential negative impact. Using 7693 unseen test tasks over 160 unseen simulated mobile networks and 6 field trials over 4 commercial mobile networks in real world, we demonstrated the capability of our approach to direct transfer the learning from one simulator to another simulator, and from simulation to real world. This is the first time that a DRL agent successfully transfers its learning directly from simulation to very complex real world problems with incomplete and imperfect information, huge state/action space and multi-agent interactions.", "paperhash": "tan|simtoreal_optimization_of_complex_real_world_mobile_network_by_deep_reinforcement_learning_from_selfplay", "keywords": ["deep reinforcement learning", "deep learning", "transfer learning", "reality gap", "artificial intelligence", "mobile network", "optimization", "real world", "sim-to-real", "DRL", "neural network", "multi-agent", "multi-task", "imperfect information", "simulation", "optimization", "CCO", "coverage and capacity optimization", "self play", "competitive", "cooperative", "competition", "cooperation", "coordination", "self organizing network", "network optimization"], "_bibtex": "@misc{\n  tan2018sim-to-real,\n  title={SIM-TO-REAL OPTIMIZATION OF COMPLEX REAL WORLD MOBILE NETWORK BY DEEP REINFORCEMENT LEARNING FROM SELF-PLAY},\n  author={Yongxi Tan and Jin Yang and Xin Chen and Qitao Song and Yunjun Chen and Zhangxiang Ye and Zhenqiang Su},\n  year={2018},\n  url={https://openreview.net/forum?id=HkWYyw1vG}\n}", "authorids": ["yongxi.tan@gmail.com", "jin.yang@huawei.com", "yongxi.tan@huawei.com"], "authors": ["Yongxi Tan", "Jin Yang", "Xin Chen", "Qitao Song", "Yunjun Chen", "Zhangxiang Ye", "Zhenqiang Su"], "TL;DR": "Sim-to-real optimization of complex real world mobile network with imperfect information via multi-agent multi-task deep reinforcement learning from self-play", "pdf": "/pdf/ab6df39113d2d550de4848319e9ef1ad4dbe5773.pdf"}, "nonreaders": [], "details": {"replyCount": 2, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "tmdate": 1518866357906, "tcdate": 1518866357906, "number": 9, "cdate": 1518866357906, "id": "B1a2k9rwG", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "HkWYyw1vG", "replyto": "HkWYyw1vG", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "This paper does not follow the formatting guidelines in https://iclr.cc/Conferences/2018/CallForWorkshops and is, thus, rejected. In particular, the authors altered the margins to fit in 3 pages."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SIM-TO-REAL OPTIMIZATION OF COMPLEX REAL WORLD MOBILE NETWORK BY DEEP REINFORCEMENT LEARNING FROM SELF-PLAY", "abstract": "Mobile network that millions of people use every day is one of the most complex systems in real world. Optimization of mobile network to meet exploding customer demand and reduce CAPEX/OPEX poses greater challenges than in prior works. Actually, learning to solve complex problems in real world to benefit everyone and make the world better has long been ultimate goal of AI. However, it still remains an unsolved problem for deep reinforcement learning (DRL), given incomplete/imperfect information in real world, huge state/action space, lots of data needed for training, associated time/cost, interactions among multi-agents, potential negative impact to real world, etc. To bridge this reality gap, we proposed a DRL framework to direct transfer optimal policy learned from multi-tasks in source domain to unseen similar tasks in target domain without any further training in both domains. First, we distilled temporal-spatial relationships between cells and mobile users to scalable 3D image-like tensor to best characterize partially observed mobile network. Second, inspired by AlphaGo, we used a novel self-play mechanism to empower DRL agent to gradually improve its intelligence by competing for best record on multiple tasks. Third, a decentralized DRL method is proposed to coordinate multi-agents to compete and cooperate as a team to maximize global reward and minimize potential negative impact. Using 7693 unseen test tasks over 160 unseen simulated mobile networks and 6 field trials over 4 commercial mobile networks in real world, we demonstrated the capability of our approach to direct transfer the learning from one simulator to another simulator, and from simulation to real world. This is the first time that a DRL agent successfully transfers its learning directly from simulation to very complex real world problems with incomplete and imperfect information, huge state/action space and multi-agent interactions.", "paperhash": "tan|simtoreal_optimization_of_complex_real_world_mobile_network_by_deep_reinforcement_learning_from_selfplay", "keywords": ["deep reinforcement learning", "deep learning", "transfer learning", "reality gap", "artificial intelligence", "mobile network", "optimization", "real world", "sim-to-real", "DRL", "neural network", "multi-agent", "multi-task", "imperfect information", "simulation", "optimization", "CCO", "coverage and capacity optimization", "self play", "competitive", "cooperative", "competition", "cooperation", "coordination", "self organizing network", "network optimization"], "_bibtex": "@misc{\n  tan2018sim-to-real,\n  title={SIM-TO-REAL OPTIMIZATION OF COMPLEX REAL WORLD MOBILE NETWORK BY DEEP REINFORCEMENT LEARNING FROM SELF-PLAY},\n  author={Yongxi Tan and Jin Yang and Xin Chen and Qitao Song and Yunjun Chen and Zhangxiang Ye and Zhenqiang Su},\n  year={2018},\n  url={https://openreview.net/forum?id=HkWYyw1vG}\n}", "authorids": ["yongxi.tan@gmail.com", "jin.yang@huawei.com", "yongxi.tan@huawei.com"], "authors": ["Yongxi Tan", "Jin Yang", "Xin Chen", "Qitao Song", "Yunjun Chen", "Zhangxiang Ye", "Zhenqiang Su"], "TL;DR": "Sim-to-real optimization of complex real world mobile network with imperfect information via multi-agent multi-task deep reinforcement learning from self-play", "pdf": "/pdf/ab6df39113d2d550de4848319e9ef1ad4dbe5773.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "ddate": null, "tmdate": 1518717533243, "tcdate": 1518717533243, "number": 1, "cdate": 1518717533243, "id": "HkSD5BQDf", "invitation": "ICLR.cc/2018/Workshop/-/Paper200/Public_Comment", "forum": "HkWYyw1vG", "replyto": "HkWYyw1vG", "signatures": ["~Yongxi_Tan1"], "readers": ["everyone"], "writers": ["~Yongxi_Tan1"], "content": {"title": "revision for citation \"Poker (Brown & Sandholm 2018)\" in the abstract", "comment": "On Tuesday, we found that citation \"Poker (Brown & Sandholm 2018)\" was mistakenly referred in the context of deep reinforcement learning as below: \n\n\"Using deep neural network (LeCun et al., 2015) for a rich representation of high-dimensional visual input and as an universal function approximator, deep reinforcement learning (DRL) have achieved unprecedented success in some challenging domains, such as Atari game (Mnih et al., 2015), Go (Silver et al., 2016; Silver et al., 2017), Poker (Brown & Sandholm 2018).\" \n\nHowever, it is not, and should be referred in the context of imperfect information game. This might be occurred during the copy, paste and rearrange from our original full paper (both Libratus and Deepstack are cited for imperfect information game in our original full paper). \n\nAfter contacted ICLR 2018 workshop program chairs (thank you very much for the very quick response and kind help!), we make the revision here as comment, by moving \"Poker (Brown & Sandholm 2018)\" to the context of imperfect information, and adding another paper Deepstack here and in the REFERENCE:\n\n \"Using deep neural network (LeCun et al., 2015) for a rich representation of high-dimensional visual input and as an universal function approximator, deep reinforcement learning (DRL) have achieved unprecedented success in some challenging domains, such as Atari game (Mnih et al., 2015), Go (Silver et al., 2016; Silver et al., 2017). The ultimate goal of AI is creating agent that can not only learn like human, but also make the world better by solving complex problems in real world. However, application of DRL in complex real world problems still remains an unsolved problem due to imperfect information (Morav\u010d\u00edk et al., 2017; Brown & Sandholm 2018), huge state/action space, big gap between simulation and real world (Rusu et al., 2016; Tobin et al., 2017; Bousmalis & Levine 2017), multi-agent interactions (Vinyals et al., 2017), time/cost, etc.\"\n\nREFERENCE \n\nMatej Morav\u010d\u00edk, Martin Schmid, Neil Burch, Viliam Lis\u00fd, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin Waugh, Michael Johanson, Michael Bowling. DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker. Science, 356:508-513, 2017\n\nSorry for any inconvenience brought to the reviewer and readers!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SIM-TO-REAL OPTIMIZATION OF COMPLEX REAL WORLD MOBILE NETWORK BY DEEP REINFORCEMENT LEARNING FROM SELF-PLAY", "abstract": "Mobile network that millions of people use every day is one of the most complex systems in real world. Optimization of mobile network to meet exploding customer demand and reduce CAPEX/OPEX poses greater challenges than in prior works. Actually, learning to solve complex problems in real world to benefit everyone and make the world better has long been ultimate goal of AI. However, it still remains an unsolved problem for deep reinforcement learning (DRL), given incomplete/imperfect information in real world, huge state/action space, lots of data needed for training, associated time/cost, interactions among multi-agents, potential negative impact to real world, etc. To bridge this reality gap, we proposed a DRL framework to direct transfer optimal policy learned from multi-tasks in source domain to unseen similar tasks in target domain without any further training in both domains. First, we distilled temporal-spatial relationships between cells and mobile users to scalable 3D image-like tensor to best characterize partially observed mobile network. Second, inspired by AlphaGo, we used a novel self-play mechanism to empower DRL agent to gradually improve its intelligence by competing for best record on multiple tasks. Third, a decentralized DRL method is proposed to coordinate multi-agents to compete and cooperate as a team to maximize global reward and minimize potential negative impact. Using 7693 unseen test tasks over 160 unseen simulated mobile networks and 6 field trials over 4 commercial mobile networks in real world, we demonstrated the capability of our approach to direct transfer the learning from one simulator to another simulator, and from simulation to real world. This is the first time that a DRL agent successfully transfers its learning directly from simulation to very complex real world problems with incomplete and imperfect information, huge state/action space and multi-agent interactions.", "paperhash": "tan|simtoreal_optimization_of_complex_real_world_mobile_network_by_deep_reinforcement_learning_from_selfplay", "keywords": ["deep reinforcement learning", "deep learning", "transfer learning", "reality gap", "artificial intelligence", "mobile network", "optimization", "real world", "sim-to-real", "DRL", "neural network", "multi-agent", "multi-task", "imperfect information", "simulation", "optimization", "CCO", "coverage and capacity optimization", "self play", "competitive", "cooperative", "competition", "cooperation", "coordination", "self organizing network", "network optimization"], "_bibtex": "@misc{\n  tan2018sim-to-real,\n  title={SIM-TO-REAL OPTIMIZATION OF COMPLEX REAL WORLD MOBILE NETWORK BY DEEP REINFORCEMENT LEARNING FROM SELF-PLAY},\n  author={Yongxi Tan and Jin Yang and Xin Chen and Qitao Song and Yunjun Chen and Zhangxiang Ye and Zhenqiang Su},\n  year={2018},\n  url={https://openreview.net/forum?id=HkWYyw1vG}\n}", "authorids": ["yongxi.tan@gmail.com", "jin.yang@huawei.com", "yongxi.tan@huawei.com"], "authors": ["Yongxi Tan", "Jin Yang", "Xin Chen", "Qitao Song", "Yunjun Chen", "Zhangxiang Ye", "Zhenqiang Su"], "TL;DR": "Sim-to-real optimization of complex real world mobile network with imperfect information via multi-agent multi-task deep reinforcement learning from self-play", "pdf": "/pdf/ab6df39113d2d550de4848319e9ef1ad4dbe5773.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518712625035, "id": "ICLR.cc/2018/Workshop/-/Paper200/Public_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper200/Reviewers"], "reply": {"replyto": null, "forum": "HkWYyw1vG", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1518712625035}}}], "count": 3}