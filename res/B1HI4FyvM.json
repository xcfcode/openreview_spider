{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124451849, "tcdate": 1518470220969, "number": 276, "cdate": 1518470220969, "id": "B1HI4FyvM", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "B1HI4FyvM", "signatures": ["~Maxwell_Nye1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Are Efficient Deep Representations Learnable?", "abstract": "Many theories of deep learning have shown that a deep network can require dramatically\nfewer resources to represent a given function compared to a shallow\nnetwork. But a question remains: can these efficient representations be learned\nusing current deep learning techniques? In this work, we test whether standard\ndeep learning methods can in fact find the efficient representations posited by several\ntheories of deep representation. Specifically, we train deep neural networks\nto learn two simple functions with known efficient solutions: the parity function\nand the fast Fourier transform. We find that using gradient-based optimization, a\ndeep network does not learn the parity function, unless initialized very close to a\nhand-coded exact solution. We also find that a deep linear neural network does not\nlearn the fast Fourier transform, even in the best-case scenario of infinite training\ndata, unless the weights are initialized very close to the exact hand-coded solution.\nOur results suggest that not every element of the class of compositional functions\ncan be learned efficiently by a deep network, and further restrictions are necessary\nto understand what functions are both efficiently representable and learnable.", "paperhash": "nye|are_efficient_deep_representations_learnable", "keywords": ["deep learning", "sparse representations"], "_bibtex": "@misc{\n  nye2018are,\n  title={Are Efficient Deep Representations Learnable?},\n  author={Maxwell Nye and Andrew Saxe},\n  year={2018},\n  url={https://openreview.net/forum?id=B1HI4FyvM}\n}", "authorids": ["mnye@mit.edu", "asaxe@fas.harvard.edu"], "authors": ["Maxwell Nye", "Andrew Saxe"], "TL;DR": "Examining theory by testing if deep networks can learn to represent simple functions", "pdf": "/pdf/a147b6a267ba787a47835a353e543ae14e47bc95.pdf"}, "nonreaders": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "tmdate": 1524081248026, "tcdate": 1524081248026, "number": 3, "cdate": 1524081248026, "id": "SkuvGmS2G", "invitation": "ICLR.cc/2018/Workshop/-/Paper276/Official_Comment", "forum": "B1HI4FyvM", "replyto": "Byvls9ltz", "signatures": ["ICLR.cc/2018/Workshop/Paper276/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper276/Authors"], "content": {"title": "Reply to Reviewer 2", "comment": "Thank you for your helpful comments and feedback! We have compiled responses to your comments below. \n\n- In order to express the complex-valued fast Fourier transform with only real numbers, we simply concatenated the real and imaginary parts of each input and output vector. The hand-coded FFT weight matrices were correspondingly transformed. Thus the network weights and input were entirely real valued, but represent complex values. \n\n- The fast Fourier transform has a complexity of O(n log n), so any network with a single layer, while it may be able to implement the DFT in general, cannot implement the FFT. \n\n- The FFT networks were fully connected.\n\n- The reviewer notes that \"why deep learning works in computer vision is due to the use of inductive biases such as CNN.\u201d While we certainly agree that CNNs are extraordinarily useful for speeding up training, and are essentially necessary in practice, extensive experiments with MNIST seem to suggest that CNNs and fully connected nets don\u2019t necessarily perform differently in the limit of large data and long training times (http://yann.lecun.com/exdb/mnist/, see \u201c6-layer NN 784-2500-2000-1500-1000-500-10 (on GPU) [elastic distortions]\u201d and \u201clarge/deep conv. net, 1-20-40-60-80-100-120-120-10 [elastic distortions]\u201d). Because our data is synthesized and we trained many networks for millions of epochs, our experiments occur in this large data, long training regime. Thus, it is unclear to us whether inductive biases such as CNNs would have made a difference for our FFT experiments. Additionally, we showed that enforcing the correct sparsity pattern for the parity function does not appear to increase the ability of the network to converge."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Are Efficient Deep Representations Learnable?", "abstract": "Many theories of deep learning have shown that a deep network can require dramatically\nfewer resources to represent a given function compared to a shallow\nnetwork. But a question remains: can these efficient representations be learned\nusing current deep learning techniques? In this work, we test whether standard\ndeep learning methods can in fact find the efficient representations posited by several\ntheories of deep representation. Specifically, we train deep neural networks\nto learn two simple functions with known efficient solutions: the parity function\nand the fast Fourier transform. We find that using gradient-based optimization, a\ndeep network does not learn the parity function, unless initialized very close to a\nhand-coded exact solution. We also find that a deep linear neural network does not\nlearn the fast Fourier transform, even in the best-case scenario of infinite training\ndata, unless the weights are initialized very close to the exact hand-coded solution.\nOur results suggest that not every element of the class of compositional functions\ncan be learned efficiently by a deep network, and further restrictions are necessary\nto understand what functions are both efficiently representable and learnable.", "paperhash": "nye|are_efficient_deep_representations_learnable", "keywords": ["deep learning", "sparse representations"], "_bibtex": "@misc{\n  nye2018are,\n  title={Are Efficient Deep Representations Learnable?},\n  author={Maxwell Nye and Andrew Saxe},\n  year={2018},\n  url={https://openreview.net/forum?id=B1HI4FyvM}\n}", "authorids": ["mnye@mit.edu", "asaxe@fas.harvard.edu"], "authors": ["Maxwell Nye", "Andrew Saxe"], "TL;DR": "Examining theory by testing if deep networks can learn to represent simple functions", "pdf": "/pdf/a147b6a267ba787a47835a353e543ae14e47bc95.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1519222446406, "id": "ICLR.cc/2018/Workshop/-/Paper276/Official_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "B1HI4FyvM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper276/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper276/Authors|ICLR.cc/2018/Workshop/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper276/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper276/Authors|ICLR.cc/2018/Workshop/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Workshop/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Workshop/Paper276/Reviewers", "ICLR.cc/2018/Workshop/Paper276/Authors", "ICLR.cc/2018/Workshop/Program_Chairs"], "cdate": 1519222446406}}, "tauthor": "maxnye@comcast.net"}, {"tddate": null, "ddate": null, "tmdate": 1524080736446, "tcdate": 1524080736446, "number": 2, "cdate": 1524080736446, "id": "r1OvxmS3M", "invitation": "ICLR.cc/2018/Workshop/-/Paper276/Official_Comment", "forum": "B1HI4FyvM", "replyto": "B14vbd7tM", "signatures": ["ICLR.cc/2018/Workshop/Paper276/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper276/Authors"], "content": {"title": "Reply to Reviewer 1", "comment": "Thank you for your very thoughtful comments! We have compiled responses below. \nMany of the implementation details were present in earlier drafts of the paper, but were removed due to space constraints. \n\n- The Fourier transform networks had log(n/2) layers of size n by n, where n = {32, 64, 128, 256}. The parity function networks had input sizes n = {16, 32, 64, 128}, with 2 log(n) layers and the number of hidden units halved every other layer, such that the final output layer had a single neuron.\n\n- For the fast Fourier transform experiments, we tried initializing our networks with noisy weights around zero and the identity matrix, in addition to noisy versions of a hand-coded solution. We found that noisy versions of the hand-coded solution had the best convergence properties, while still not converging to a sparse solution if the initialization noise scale was too large. We thought this initialization to the hand-coded solution would represent a \u201cbest case\u201d scenario, with other initializations likely to be less effective and this was empirically verified.\n\n- We used deep sigmoid networks for the parity function experiments simply because they are often used as the activation of choice when attempting to learn XOR gates. Since our hand-coded parity function baseline was made out of a tree of XOR gates, we thought sigmoid activations were the most natural choice. Future work could certainly conduct experiments with other activation functions. \n\n- For the fast Fourier transform experiments, we trained networks with hidden layers with up to 1.5x the number of units as the input and output layers. We observed no additional benefit, but it is possible that even greater overparametrization could help.\n\n- The DFT can be expressed as a single matrix multiplication, and can therefore be expressed by a linear neural network with no hidden layers. We chose to examine the FFT because it can be expressed by a deep network, but not by a shallow network, and our aim is to examine the learning properties of deep neural networks. Over the course of our experiments, we did indeed observe that a deep network with no sparsity constraint can learn to perform the DFT, but its weights will not generally have the efficient sparsity pattern of the FFT.\n\n- Sparsity was calculated in the following way: for each trained network, we found the maximum value \\epsilon, such that if all weights with magnitude less than \\epsilon were set to zero, the network error decreased. We defined the sparsity of the network as the L_0 norm of the network after rectification by this maximizing value of \\epsilon. As reported in the paper, we found that there was a clear basin of attraction around the hand-coded solution, within which the network converged to a sparsity pattern consistent with O(n log n) scaling, and outside of which the network did not converge to a sparse pattern. \n\n- For the parity function experiments, we tried batch sizes of 250, 500 and 1000. We saw no significant differences between different batch sizes. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Are Efficient Deep Representations Learnable?", "abstract": "Many theories of deep learning have shown that a deep network can require dramatically\nfewer resources to represent a given function compared to a shallow\nnetwork. But a question remains: can these efficient representations be learned\nusing current deep learning techniques? In this work, we test whether standard\ndeep learning methods can in fact find the efficient representations posited by several\ntheories of deep representation. Specifically, we train deep neural networks\nto learn two simple functions with known efficient solutions: the parity function\nand the fast Fourier transform. We find that using gradient-based optimization, a\ndeep network does not learn the parity function, unless initialized very close to a\nhand-coded exact solution. We also find that a deep linear neural network does not\nlearn the fast Fourier transform, even in the best-case scenario of infinite training\ndata, unless the weights are initialized very close to the exact hand-coded solution.\nOur results suggest that not every element of the class of compositional functions\ncan be learned efficiently by a deep network, and further restrictions are necessary\nto understand what functions are both efficiently representable and learnable.", "paperhash": "nye|are_efficient_deep_representations_learnable", "keywords": ["deep learning", "sparse representations"], "_bibtex": "@misc{\n  nye2018are,\n  title={Are Efficient Deep Representations Learnable?},\n  author={Maxwell Nye and Andrew Saxe},\n  year={2018},\n  url={https://openreview.net/forum?id=B1HI4FyvM}\n}", "authorids": ["mnye@mit.edu", "asaxe@fas.harvard.edu"], "authors": ["Maxwell Nye", "Andrew Saxe"], "TL;DR": "Examining theory by testing if deep networks can learn to represent simple functions", "pdf": "/pdf/a147b6a267ba787a47835a353e543ae14e47bc95.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1519222446406, "id": "ICLR.cc/2018/Workshop/-/Paper276/Official_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "B1HI4FyvM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper276/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper276/Authors|ICLR.cc/2018/Workshop/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper276/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper276/Authors|ICLR.cc/2018/Workshop/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Workshop/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Workshop/Paper276/Reviewers", "ICLR.cc/2018/Workshop/Paper276/Authors", "ICLR.cc/2018/Workshop/Program_Chairs"], "cdate": 1519222446406}}, "tauthor": "maxnye@comcast.net"}, {"tddate": null, "ddate": null, "tmdate": 1524079857061, "tcdate": 1524079857061, "number": 1, "cdate": 1524079857061, "id": "SyKl6zShM", "invitation": "ICLR.cc/2018/Workshop/-/Paper276/Official_Comment", "forum": "B1HI4FyvM", "replyto": "SJy5EQMuM", "signatures": ["ICLR.cc/2018/Workshop/Paper276/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper276/Authors"], "content": {"title": "Reply to Reviewer 3", "comment": "We very much appreciate the positive feedback, thank you!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Are Efficient Deep Representations Learnable?", "abstract": "Many theories of deep learning have shown that a deep network can require dramatically\nfewer resources to represent a given function compared to a shallow\nnetwork. But a question remains: can these efficient representations be learned\nusing current deep learning techniques? In this work, we test whether standard\ndeep learning methods can in fact find the efficient representations posited by several\ntheories of deep representation. Specifically, we train deep neural networks\nto learn two simple functions with known efficient solutions: the parity function\nand the fast Fourier transform. We find that using gradient-based optimization, a\ndeep network does not learn the parity function, unless initialized very close to a\nhand-coded exact solution. We also find that a deep linear neural network does not\nlearn the fast Fourier transform, even in the best-case scenario of infinite training\ndata, unless the weights are initialized very close to the exact hand-coded solution.\nOur results suggest that not every element of the class of compositional functions\ncan be learned efficiently by a deep network, and further restrictions are necessary\nto understand what functions are both efficiently representable and learnable.", "paperhash": "nye|are_efficient_deep_representations_learnable", "keywords": ["deep learning", "sparse representations"], "_bibtex": "@misc{\n  nye2018are,\n  title={Are Efficient Deep Representations Learnable?},\n  author={Maxwell Nye and Andrew Saxe},\n  year={2018},\n  url={https://openreview.net/forum?id=B1HI4FyvM}\n}", "authorids": ["mnye@mit.edu", "asaxe@fas.harvard.edu"], "authors": ["Maxwell Nye", "Andrew Saxe"], "TL;DR": "Examining theory by testing if deep networks can learn to represent simple functions", "pdf": "/pdf/a147b6a267ba787a47835a353e543ae14e47bc95.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1519222446406, "id": "ICLR.cc/2018/Workshop/-/Paper276/Official_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "B1HI4FyvM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper276/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper276/Authors|ICLR.cc/2018/Workshop/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper276/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper276/Authors|ICLR.cc/2018/Workshop/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Workshop/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Workshop/Paper276/Reviewers", "ICLR.cc/2018/Workshop/Paper276/Authors", "ICLR.cc/2018/Workshop/Program_Chairs"], "cdate": 1519222446406}}, "tauthor": "maxnye@comcast.net"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582984720, "tcdate": 1519690887330, "number": 1, "cdate": 1519690887330, "id": "SJy5EQMuM", "invitation": "ICLR.cc/2018/Workshop/-/Paper276/Official_Review", "forum": "B1HI4FyvM", "replyto": "B1HI4FyvM", "signatures": ["ICLR.cc/2018/Workshop/Paper276/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper276/AnonReviewer3"], "content": {"title": "Very good analysis", "rating": "8: Top 50% of accepted papers, clear accept", "review": "With simple examples, the authors clearly demonstrate the difference between what functions a deep neural network model can represent and what it can learn. This is important because the ability of deep neural networks to more efficiently represent complex functions is frequently cited as their primary advantage.\n\nThe presentation, experiments, and conclusions are very good.  ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Are Efficient Deep Representations Learnable?", "abstract": "Many theories of deep learning have shown that a deep network can require dramatically\nfewer resources to represent a given function compared to a shallow\nnetwork. But a question remains: can these efficient representations be learned\nusing current deep learning techniques? In this work, we test whether standard\ndeep learning methods can in fact find the efficient representations posited by several\ntheories of deep representation. Specifically, we train deep neural networks\nto learn two simple functions with known efficient solutions: the parity function\nand the fast Fourier transform. We find that using gradient-based optimization, a\ndeep network does not learn the parity function, unless initialized very close to a\nhand-coded exact solution. We also find that a deep linear neural network does not\nlearn the fast Fourier transform, even in the best-case scenario of infinite training\ndata, unless the weights are initialized very close to the exact hand-coded solution.\nOur results suggest that not every element of the class of compositional functions\ncan be learned efficiently by a deep network, and further restrictions are necessary\nto understand what functions are both efficiently representable and learnable.", "paperhash": "nye|are_efficient_deep_representations_learnable", "keywords": ["deep learning", "sparse representations"], "_bibtex": "@misc{\n  nye2018are,\n  title={Are Efficient Deep Representations Learnable?},\n  author={Maxwell Nye and Andrew Saxe},\n  year={2018},\n  url={https://openreview.net/forum?id=B1HI4FyvM}\n}", "authorids": ["mnye@mit.edu", "asaxe@fas.harvard.edu"], "authors": ["Maxwell Nye", "Andrew Saxe"], "TL;DR": "Examining theory by testing if deep networks can learn to represent simple functions", "pdf": "/pdf/a147b6a267ba787a47835a353e543ae14e47bc95.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582984534, "id": "ICLR.cc/2018/Workshop/-/Paper276/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper276/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper276/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper276/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper276/AnonReviewer1"], "reply": {"forum": "B1HI4FyvM", "replyto": "B1HI4FyvM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper276/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper276/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582984534}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582767754, "tcdate": 1520638702846, "number": 2, "cdate": 1520638702846, "id": "Byvls9ltz", "invitation": "ICLR.cc/2018/Workshop/-/Paper276/Official_Review", "forum": "B1HI4FyvM", "replyto": "B1HI4FyvM", "signatures": ["ICLR.cc/2018/Workshop/Paper276/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper276/AnonReviewer2"], "content": {"title": "testing theory of deep networks on learning simple functions", "rating": "6: Marginally above acceptance threshold", "review": "The paper does an empirical study of how well neural network optimized by back propagation learns simple functions: parity function/ fast fourier transform. The paper concludes that the deep network can not learn those simple functions unless initialized near the optimal solution.  \n\nThe paper would benefit from explaining by what is meant by efficient representations (i.e number of neurons used vs number of examples etc).\n\nQuestion regarding learning the  Fourier transform:\n-  was the complex matrix learned by back propagation ? or you used the discrete cosine transform?\n-  what if your signals in the training were translated version one of another? one would expect the network to learn the Fourier transform?\n- In the Fourier case you use a deep linear network ,how many layers? what if one uses only a linear layer?\n- was the network fully connected ? what if it was convolutional network? would it learn the fourier transform? \n\nStudying those simple functions is insightful, but it is hard to draw conclusions out of this simple set of experiments. For instance why deep learning works in computer vision is due to the use of inductive biases such as CNN, for learning the Fourier transform one would expect that data augmentation with translation might help the learning.  \n\n\n\n\n\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Are Efficient Deep Representations Learnable?", "abstract": "Many theories of deep learning have shown that a deep network can require dramatically\nfewer resources to represent a given function compared to a shallow\nnetwork. But a question remains: can these efficient representations be learned\nusing current deep learning techniques? In this work, we test whether standard\ndeep learning methods can in fact find the efficient representations posited by several\ntheories of deep representation. Specifically, we train deep neural networks\nto learn two simple functions with known efficient solutions: the parity function\nand the fast Fourier transform. We find that using gradient-based optimization, a\ndeep network does not learn the parity function, unless initialized very close to a\nhand-coded exact solution. We also find that a deep linear neural network does not\nlearn the fast Fourier transform, even in the best-case scenario of infinite training\ndata, unless the weights are initialized very close to the exact hand-coded solution.\nOur results suggest that not every element of the class of compositional functions\ncan be learned efficiently by a deep network, and further restrictions are necessary\nto understand what functions are both efficiently representable and learnable.", "paperhash": "nye|are_efficient_deep_representations_learnable", "keywords": ["deep learning", "sparse representations"], "_bibtex": "@misc{\n  nye2018are,\n  title={Are Efficient Deep Representations Learnable?},\n  author={Maxwell Nye and Andrew Saxe},\n  year={2018},\n  url={https://openreview.net/forum?id=B1HI4FyvM}\n}", "authorids": ["mnye@mit.edu", "asaxe@fas.harvard.edu"], "authors": ["Maxwell Nye", "Andrew Saxe"], "TL;DR": "Examining theory by testing if deep networks can learn to represent simple functions", "pdf": "/pdf/a147b6a267ba787a47835a353e543ae14e47bc95.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582984534, "id": "ICLR.cc/2018/Workshop/-/Paper276/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper276/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper276/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper276/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper276/AnonReviewer1"], "reply": {"forum": "B1HI4FyvM", "replyto": "B1HI4FyvM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper276/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper276/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582984534}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582627615, "tcdate": 1520824668461, "number": 3, "cdate": 1520824668461, "id": "B14vbd7tM", "invitation": "ICLR.cc/2018/Workshop/-/Paper276/Official_Review", "forum": "B1HI4FyvM", "replyto": "B1HI4FyvM", "signatures": ["ICLR.cc/2018/Workshop/Paper276/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper276/AnonReviewer1"], "content": {"title": "Interesting work benchmarking learnability of functions by deep networks against a ground truth", "rating": "5: Marginally below acceptance threshold", "review": "The premise of this paper is very interesting: the authors test the ability of deep networks to converge to the parity/FFT as a function of noise magnitude added to a ground truth initialization. They show that in both cases, the deep model only converges when initialized close enough.\n\nMy concern is with the lack of detail in some of the experimental settings: while I can completely believe that deep models do indeed behave in this way, I'd like to have an enumeration of the range of things tried before being convinced that this is the case.  I've mentioned some things that would be nice to see in the comments.\n\nMajor Comments\nIf the network architectures are as pictured in the figures, they look extremely small (in number of neurons), and it would be very interesting to have overparametrized networks as a comparison. \n\nIt seems important to also try other initialization schemes and see if models converge from there, particularly with overparametrized networks. My suspicion would be that for large enough networks, and small enough n, we would see the models correctly learn these functions.\n\nMore generally, it would be nice to see the range of architectures and methods tried described explicitly in the text (perhaps in the Appendix.)\n\nMinor Comments:\nWhy use deep sigmoid networks for parity?\nTest out on larger architectures?\nDoesn't the DFT have very poor scaling (exponential variations in size?) This would be a reason why this might be hard for a deep network to learn? \nAlso, why skip to learning the FFT before seeing if the DFT can be learned?\nWould like actual details of network architectures -- how big are they aside from the input size? (Width/depth?) Were different batch sizes tried? (1000 is a large-ish batch size.) \n\"Without encouragement towards a sparse solution, a deep linear network will learn dense solutions in general\" -- even if many weights are nonzero, deep models typically have only a small set of weights that are very large in magnitude after training, so they are \"almost\" sparse?\n\nI think this is very important and interesting work, but would like more details to paint a clearer picture before acceptance. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Are Efficient Deep Representations Learnable?", "abstract": "Many theories of deep learning have shown that a deep network can require dramatically\nfewer resources to represent a given function compared to a shallow\nnetwork. But a question remains: can these efficient representations be learned\nusing current deep learning techniques? In this work, we test whether standard\ndeep learning methods can in fact find the efficient representations posited by several\ntheories of deep representation. Specifically, we train deep neural networks\nto learn two simple functions with known efficient solutions: the parity function\nand the fast Fourier transform. We find that using gradient-based optimization, a\ndeep network does not learn the parity function, unless initialized very close to a\nhand-coded exact solution. We also find that a deep linear neural network does not\nlearn the fast Fourier transform, even in the best-case scenario of infinite training\ndata, unless the weights are initialized very close to the exact hand-coded solution.\nOur results suggest that not every element of the class of compositional functions\ncan be learned efficiently by a deep network, and further restrictions are necessary\nto understand what functions are both efficiently representable and learnable.", "paperhash": "nye|are_efficient_deep_representations_learnable", "keywords": ["deep learning", "sparse representations"], "_bibtex": "@misc{\n  nye2018are,\n  title={Are Efficient Deep Representations Learnable?},\n  author={Maxwell Nye and Andrew Saxe},\n  year={2018},\n  url={https://openreview.net/forum?id=B1HI4FyvM}\n}", "authorids": ["mnye@mit.edu", "asaxe@fas.harvard.edu"], "authors": ["Maxwell Nye", "Andrew Saxe"], "TL;DR": "Examining theory by testing if deep networks can learn to represent simple functions", "pdf": "/pdf/a147b6a267ba787a47835a353e543ae14e47bc95.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582984534, "id": "ICLR.cc/2018/Workshop/-/Paper276/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper276/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper276/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper276/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper276/AnonReviewer1"], "reply": {"forum": "B1HI4FyvM", "replyto": "B1HI4FyvM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper276/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper276/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582984534}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573565548, "tcdate": 1521573565548, "number": 99, "cdate": 1521573565210, "id": "B1U6R0CtG", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "B1HI4FyvM", "replyto": "B1HI4FyvM", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Are Efficient Deep Representations Learnable?", "abstract": "Many theories of deep learning have shown that a deep network can require dramatically\nfewer resources to represent a given function compared to a shallow\nnetwork. But a question remains: can these efficient representations be learned\nusing current deep learning techniques? In this work, we test whether standard\ndeep learning methods can in fact find the efficient representations posited by several\ntheories of deep representation. Specifically, we train deep neural networks\nto learn two simple functions with known efficient solutions: the parity function\nand the fast Fourier transform. We find that using gradient-based optimization, a\ndeep network does not learn the parity function, unless initialized very close to a\nhand-coded exact solution. We also find that a deep linear neural network does not\nlearn the fast Fourier transform, even in the best-case scenario of infinite training\ndata, unless the weights are initialized very close to the exact hand-coded solution.\nOur results suggest that not every element of the class of compositional functions\ncan be learned efficiently by a deep network, and further restrictions are necessary\nto understand what functions are both efficiently representable and learnable.", "paperhash": "nye|are_efficient_deep_representations_learnable", "keywords": ["deep learning", "sparse representations"], "_bibtex": "@misc{\n  nye2018are,\n  title={Are Efficient Deep Representations Learnable?},\n  author={Maxwell Nye and Andrew Saxe},\n  year={2018},\n  url={https://openreview.net/forum?id=B1HI4FyvM}\n}", "authorids": ["mnye@mit.edu", "asaxe@fas.harvard.edu"], "authors": ["Maxwell Nye", "Andrew Saxe"], "TL;DR": "Examining theory by testing if deep networks can learn to represent simple functions", "pdf": "/pdf/a147b6a267ba787a47835a353e543ae14e47bc95.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 8}