{"notes": [{"id": "kKwFlM32HV5", "original": "SwMCcWf-c-z", "number": 1442, "cdate": 1601308160621, "ddate": null, "tcdate": 1601308160621, "tmdate": 1614985633279, "tddate": null, "forum": "kKwFlM32HV5", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Natural World Distribution via Adaptive Confusion Energy Regularization", "authorids": ["~Yen-Chi_Hsu1", "~Cheng-Yao_Hong2", "~Wan-Cyuan_Fan1", "~Ding-Jie_Chen1", "~Ming-Sui_Lee1", "~davi_geiger1", "~Tyng-Luh_Liu1"], "authors": ["Yen-Chi Hsu", "Cheng-Yao Hong", "Wan-Cyuan Fan", "Ding-Jie Chen", "Ming-Sui Lee", "davi geiger", "Tyng-Luh Liu"], "keywords": ["Fine-Grained Visual Classification", "long-tailed distribution", "confusion energy"], "abstract": "We introduce a novel and adaptive batch-wise regularization based on the proposed Batch Confusion Norm (BCN) to flexibly address the natural world distribution which usually involves fine-grained and long-tailed properties at the same time. The Fine-Grained Visual Classification (FGVC) problem is notably characterized by two intriguing properties, significant inter-class similarity and intra-class variations, which cause learning an effective FGVC classifier a challenging task. Existing techniques attempt to capture the discriminative parts by their modified attention mechanism. The long-tailed distribution of visual classification poses a great challenge for handling the class imbalance problem. Most of existing solutions usually focus on the class-balancing strategies, classifier normalization, or alleviating the negative gradient of tailed categories. Depart from the conventional approaches, we propose to tackle both problems simultaneously with the adaptive confusion concept. When inter-class similarity prevails in a batch, the BCN term can alleviate possible overfitting due to exploring image features of fine details. On the other hand, when inter-class similarity is not an issue, the class predictions from different samples would unavoidably yield a substantial BCN loss, and prompt the network learning to further reduce the cross-entropy loss. More importantly, extending the existing confusion energy-based framework to account for long-tailed scenario, BCN can learn to exert proper distribution of confusion strength over tailed and head categories to improve classification performance. While the resulting FGVC model by the BCN technique is effective, the performance can be consistently boosted by incorporating extra attention mechanism. In our experiments, we have obtained state-of-the-art results on several benchmark FGVC datasets, and also demonstrated that our approach is competitive on the popular natural world distribution dataset, iNaturalist2018. ", "one-sentence_summary": "We propose a novel confusion term which can adaptively addresses the fine-grained and long-tailed distribution simultaneously.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hsu|natural_world_distribution_via_adaptive_confusion_energy_regularization", "pdf": "/pdf/584bf002b19b20eb8ae94f1f724a508a910c02d9.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=GEFTdiT8I", "_bibtex": "@misc{\nhsu2021natural,\ntitle={Natural World Distribution via Adaptive Confusion Energy Regularization},\nauthor={Yen-Chi Hsu and Cheng-Yao Hong and Wan-Cyuan Fan and Ding-Jie Chen and Ming-Sui Lee and davi geiger and Tyng-Luh Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=kKwFlM32HV5}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "tGM0QjoTrY", "original": null, "number": 1, "cdate": 1610040527768, "ddate": null, "tcdate": 1610040527768, "tmdate": 1610474137000, "tddate": null, "forum": "kKwFlM32HV5", "replyto": "kKwFlM32HV5", "invitation": "ICLR.cc/2021/Conference/Paper1442/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The authors address the problem of fine-grained image classification. They propose a batch based regularizer, called the batch confusion norm (BCN), to encourage less over confident predictions. They also tackle the problem of class imbalance during training by adaptively weighting the BCN loss at the class level to take the imbalances in the underlying label distributions into account. Results are presented on four different fine-grained datasets.  \n\nOverall, while the reviewers had some positive comments, there was not broad support for the paper. There are questions that need to be resolved related to the evaluation e.g. the best performing model uses GASPP, however there is no reported GASPP variant for the PC baseline. Similarly, it would be valuable to know how much PC would benefit from an additional class imbalance term in the iNaturalist2018 results. Given that the proposed regularizer builds on PC (Dubey et al.), it is very important that the authors provide a like-for-like comparison so that readers can better understand the merits of the proposed method.  \n\nThere were also concerns with the presentation of the paper e.g. several typos (which can be easily fixed), issues with the clarity of the text (which require more work), and uninformative figures (e.g. Fig 2 should be revised to more clearly illustrate the differences between the three methods shown). The authors are encouraged to revise the text to resolve these problems. \n\nWhile the paper has some strengths (e.g. the empirical performance on some of the tasks is promising and the method is conceptually simple), there are still a number of concerns from the reviewers e.g. a lack of a clear motivation as to why the proposed method works, and why it is conceptually better than existing alternatives (e.g. PC). Given this lack of support, it is not possible to recommend the paper in its current form. \n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Natural World Distribution via Adaptive Confusion Energy Regularization", "authorids": ["~Yen-Chi_Hsu1", "~Cheng-Yao_Hong2", "~Wan-Cyuan_Fan1", "~Ding-Jie_Chen1", "~Ming-Sui_Lee1", "~davi_geiger1", "~Tyng-Luh_Liu1"], "authors": ["Yen-Chi Hsu", "Cheng-Yao Hong", "Wan-Cyuan Fan", "Ding-Jie Chen", "Ming-Sui Lee", "davi geiger", "Tyng-Luh Liu"], "keywords": ["Fine-Grained Visual Classification", "long-tailed distribution", "confusion energy"], "abstract": "We introduce a novel and adaptive batch-wise regularization based on the proposed Batch Confusion Norm (BCN) to flexibly address the natural world distribution which usually involves fine-grained and long-tailed properties at the same time. The Fine-Grained Visual Classification (FGVC) problem is notably characterized by two intriguing properties, significant inter-class similarity and intra-class variations, which cause learning an effective FGVC classifier a challenging task. Existing techniques attempt to capture the discriminative parts by their modified attention mechanism. The long-tailed distribution of visual classification poses a great challenge for handling the class imbalance problem. Most of existing solutions usually focus on the class-balancing strategies, classifier normalization, or alleviating the negative gradient of tailed categories. Depart from the conventional approaches, we propose to tackle both problems simultaneously with the adaptive confusion concept. When inter-class similarity prevails in a batch, the BCN term can alleviate possible overfitting due to exploring image features of fine details. On the other hand, when inter-class similarity is not an issue, the class predictions from different samples would unavoidably yield a substantial BCN loss, and prompt the network learning to further reduce the cross-entropy loss. More importantly, extending the existing confusion energy-based framework to account for long-tailed scenario, BCN can learn to exert proper distribution of confusion strength over tailed and head categories to improve classification performance. While the resulting FGVC model by the BCN technique is effective, the performance can be consistently boosted by incorporating extra attention mechanism. In our experiments, we have obtained state-of-the-art results on several benchmark FGVC datasets, and also demonstrated that our approach is competitive on the popular natural world distribution dataset, iNaturalist2018. ", "one-sentence_summary": "We propose a novel confusion term which can adaptively addresses the fine-grained and long-tailed distribution simultaneously.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hsu|natural_world_distribution_via_adaptive_confusion_energy_regularization", "pdf": "/pdf/584bf002b19b20eb8ae94f1f724a508a910c02d9.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=GEFTdiT8I", "_bibtex": "@misc{\nhsu2021natural,\ntitle={Natural World Distribution via Adaptive Confusion Energy Regularization},\nauthor={Yen-Chi Hsu and Cheng-Yao Hong and Wan-Cyuan Fan and Ding-Jie Chen and Ming-Sui Lee and davi geiger and Tyng-Luh Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=kKwFlM32HV5}\n}"}, "tags": [], "invitation": {"reply": {"forum": "kKwFlM32HV5", "replyto": "kKwFlM32HV5", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040527752, "tmdate": 1610474136985, "id": "ICLR.cc/2021/Conference/Paper1442/-/Decision"}}}, {"id": "hdf6afKr37C", "original": null, "number": 2, "cdate": 1606298152908, "ddate": null, "tcdate": 1606298152908, "tmdate": 1606305050335, "tddate": null, "forum": "kKwFlM32HV5", "replyto": "NUSZc-uBoOC", "invitation": "ICLR.cc/2021/Conference/Paper1442/-/Official_Comment", "content": {"title": "Our general and simple approach solves the fine-grained and long-tailed distribution at the same time with competitive results.", "comment": "[Q1]. Although the proposed method is reasonable, some specific model designs are not quite clear. 1) Regarding Eq. (2), the reason why it requires to optimize the ranking should be further explained and its motivation needs to state. 2) Regarding Eq. (5), what the intuition of the adaptive matrix (i.e., ($log_{\\mu+1} (N_i+1))^{\\delta^{\\tau}}$) when i = j should be provided to the authors.\n\n[A1]. 1) In our paper, we try to address the fine-grained and long-tailed problems at the same time. The point of view we choose is \u201cconfusion energy\u201d. The main idea of confusion energy is making the model learn the inter-class similarity and prevent the overfitting issue. Moreover, different from pairwise confusion (PC), we make the confusion efficiently from the prediction of each batch. Hence, our method starts by minimizing the rank of the prediction matrix. 2) The adaptive matrix A tries to control the confusion magnitude for each category. Hence, when $i = j$, it means that we only control the confusion magnitude on each class individually.\n\n[Q2]. The major issue of this paper is the experimental evaluations. 1) The classification accuracy on these fine-grained benchmark datasets and iNat18 are not significantly better than the accuracy of previous work. Thus, the effectiveness of the proposed method is problematic. 2) Some state-of-the-art methods are not involved in the experimental comparisons, such as [ref1-ref5]. Moreover, the accuracy of the proposed method cannot outperform these methods.\n\n[A2].  1) Firstly, we straddle two different domains at the same time, and each of them is the mainstream research direction recently. Second, we achieve a competitive performance without any data sampling methods or any additional model parameters. Third, we solve the problems by a very special point, confusion energy. Our significant improvement is against PC or baseline. In conclusion, this paper tries to introduce to everyone that we can address fine-grained and long-tailed problems by the confusion energy way. Furthermore, the training process shows that it is practical and the experimental results present the competitive performance. 2) We solve the problems from a very different concept. [ref1-ref3] try to capture the discriminative parts by the network design. All of them need a larger model size and a complex training process. [ref4-ref5] only focus on the long-tailed problem. And the performance in ref4, as we all know, is hard to reproduce.\n\n[Q3]. Minor issues.\n\n[A3]. Thanks for the mistakes picked out by the reviewer, we will correct them."}, "signatures": ["ICLR.cc/2021/Conference/Paper1442/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1442/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Natural World Distribution via Adaptive Confusion Energy Regularization", "authorids": ["~Yen-Chi_Hsu1", "~Cheng-Yao_Hong2", "~Wan-Cyuan_Fan1", "~Ding-Jie_Chen1", "~Ming-Sui_Lee1", "~davi_geiger1", "~Tyng-Luh_Liu1"], "authors": ["Yen-Chi Hsu", "Cheng-Yao Hong", "Wan-Cyuan Fan", "Ding-Jie Chen", "Ming-Sui Lee", "davi geiger", "Tyng-Luh Liu"], "keywords": ["Fine-Grained Visual Classification", "long-tailed distribution", "confusion energy"], "abstract": "We introduce a novel and adaptive batch-wise regularization based on the proposed Batch Confusion Norm (BCN) to flexibly address the natural world distribution which usually involves fine-grained and long-tailed properties at the same time. The Fine-Grained Visual Classification (FGVC) problem is notably characterized by two intriguing properties, significant inter-class similarity and intra-class variations, which cause learning an effective FGVC classifier a challenging task. Existing techniques attempt to capture the discriminative parts by their modified attention mechanism. The long-tailed distribution of visual classification poses a great challenge for handling the class imbalance problem. Most of existing solutions usually focus on the class-balancing strategies, classifier normalization, or alleviating the negative gradient of tailed categories. Depart from the conventional approaches, we propose to tackle both problems simultaneously with the adaptive confusion concept. When inter-class similarity prevails in a batch, the BCN term can alleviate possible overfitting due to exploring image features of fine details. On the other hand, when inter-class similarity is not an issue, the class predictions from different samples would unavoidably yield a substantial BCN loss, and prompt the network learning to further reduce the cross-entropy loss. More importantly, extending the existing confusion energy-based framework to account for long-tailed scenario, BCN can learn to exert proper distribution of confusion strength over tailed and head categories to improve classification performance. While the resulting FGVC model by the BCN technique is effective, the performance can be consistently boosted by incorporating extra attention mechanism. In our experiments, we have obtained state-of-the-art results on several benchmark FGVC datasets, and also demonstrated that our approach is competitive on the popular natural world distribution dataset, iNaturalist2018. ", "one-sentence_summary": "We propose a novel confusion term which can adaptively addresses the fine-grained and long-tailed distribution simultaneously.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hsu|natural_world_distribution_via_adaptive_confusion_energy_regularization", "pdf": "/pdf/584bf002b19b20eb8ae94f1f724a508a910c02d9.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=GEFTdiT8I", "_bibtex": "@misc{\nhsu2021natural,\ntitle={Natural World Distribution via Adaptive Confusion Energy Regularization},\nauthor={Yen-Chi Hsu and Cheng-Yao Hong and Wan-Cyuan Fan and Ding-Jie Chen and Ming-Sui Lee and davi geiger and Tyng-Luh Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=kKwFlM32HV5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "kKwFlM32HV5", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1442/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1442/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1442/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1442/Authors|ICLR.cc/2021/Conference/Paper1442/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1442/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923859637, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1442/-/Official_Comment"}}}, {"id": "sZSAJK-70iM", "original": null, "number": 3, "cdate": 1606298936849, "ddate": null, "tcdate": 1606298936849, "tmdate": 1606304869835, "tddate": null, "forum": "kKwFlM32HV5", "replyto": "evX4kdIRf8r", "invitation": "ICLR.cc/2021/Conference/Paper1442/-/Official_Comment", "content": {"title": "The novelty is we propose a confusion energy called ''BCN'' that can adaptively address the fine-grained and long-tailed distribution.", "comment": "[Q1]. I am not sure if BCN is a proper regularizer. While I understand the geometry behind minimizing the rank of the matrix P, I don't think this is a proper way of processing the columns of matrix P which are posterior distributions. I think the paper lacks a clear justification about using BCN as a way to treat posterior distributions without using statistical or probabilistic methods.\n\n[A1]. Inter-class similarity between each category is the main challenging term in FGVC. Therefore, the proposed BCN is a confusion regularization that tries to equip the model with the ability to capture the inter-class similarity automatically. Hence, it is not a posterior distribution.\n\n[Q2]. BCN is conditioned to operate if the classes in the batch are unique. However, satisfying this condition in practice can be challenging. This becomes challenging when dealing with long-tailed datasets. What is the optimal/efficient way to guarantee a batch that satisfies this constraint? If the constraint is not satisfied, can BCN provide bad gradients because repeated classes with different posteriors in different columns can double its contribution?\n\n[A2]. The classes in a batch are unique is an assumption in the most extreme cases. On the other hand, few of classes in a batch have the same label is a normal condition. For instance, if two predictions have a similar probability distribution, the BCN loss between them becomes very small. In other words, same labels with small loss is intuitive. Hence, we do not make any restriction for generating each batch at the training time.\n\n[Q3]. The extension of adding a matrix A is simple and feels a bit ad hoc. What is the intuition behind the matrix A in terms of the norm? Can the matrix A be considered a way to ` weight a posterior as a function of the statistics?\n\n[A3]. The matrix A tries to control the confusion magnitude. For example, frequent categories has more instances to prevent a stronger confusion energy, and rare categories is opposite. Intuitively, we formulate the adaptive matrix A by the training data statistics. Hence, the matrix A is an adaptive confusion term, not a posterior mechanism.\n\n[Q4]. While I am pleased to see that the paper uses datasets depicting real scenes (e.g., iNaturalist, CUB, CAR, etc.), the experiments only focus on using one family of networks: ResNet. Does this method operate well on other more modern architectures, e.g., EfficientNet or MobileNetV2?\n\n[A4]. Yes, it still operates well on not only the ResNet series but also the EfficientNet, DenseNet, VGG, etc. In the powerful model, the results of EfficientNet-B7 from baseline, PC, and BCN approach are 70.9, 69.8, and 74.1, respectively.  The improvement of our approach is consistent with other networks. For the sake of the fair comparison to the other approaches, our main paper follows the most commonly used network, ResNet series, to report the experimental results.\n\n[Q5]. In terms of baselines, I think the paper is missing recent long-tail methods: a) Liu, et al. Large-Scale Long-Tailed Recognition in an Open World. CVPR 2019. b) Cao, et al. Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss. NeurIPS 2019.\n\n[A5]. We straddle two different domains at the same time, and each of them is the mainstream research direction recently. Hence, due to the limited space, we can only choose a few representative articles in their respective fields. a) has missed the experiments about iNaturalist which contains fine-grained and long-tailed properties both. We have the comparison with b) at Table 2 (b) LDAM and the citation is located in the second.\n\n[Q6]. The results for the long-tail methods are lacking more details. As it is common in various long-tail recognition papers, the paper is not showing performance on head or tail classes. It only shows an overall classification performance. This is misleading as the method can helping more head classes and thus improving the overall classification performance.\n\n[A6]. In Figure 3 (b), we have shown the performance on head or tail classes. More details can be found in the captions.\n\n[Q7]. Minor concern.\n\n[A7]. We can only say that this article is entirely our own work, without any plagiarism."}, "signatures": ["ICLR.cc/2021/Conference/Paper1442/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1442/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Natural World Distribution via Adaptive Confusion Energy Regularization", "authorids": ["~Yen-Chi_Hsu1", "~Cheng-Yao_Hong2", "~Wan-Cyuan_Fan1", "~Ding-Jie_Chen1", "~Ming-Sui_Lee1", "~davi_geiger1", "~Tyng-Luh_Liu1"], "authors": ["Yen-Chi Hsu", "Cheng-Yao Hong", "Wan-Cyuan Fan", "Ding-Jie Chen", "Ming-Sui Lee", "davi geiger", "Tyng-Luh Liu"], "keywords": ["Fine-Grained Visual Classification", "long-tailed distribution", "confusion energy"], "abstract": "We introduce a novel and adaptive batch-wise regularization based on the proposed Batch Confusion Norm (BCN) to flexibly address the natural world distribution which usually involves fine-grained and long-tailed properties at the same time. The Fine-Grained Visual Classification (FGVC) problem is notably characterized by two intriguing properties, significant inter-class similarity and intra-class variations, which cause learning an effective FGVC classifier a challenging task. Existing techniques attempt to capture the discriminative parts by their modified attention mechanism. The long-tailed distribution of visual classification poses a great challenge for handling the class imbalance problem. Most of existing solutions usually focus on the class-balancing strategies, classifier normalization, or alleviating the negative gradient of tailed categories. Depart from the conventional approaches, we propose to tackle both problems simultaneously with the adaptive confusion concept. When inter-class similarity prevails in a batch, the BCN term can alleviate possible overfitting due to exploring image features of fine details. On the other hand, when inter-class similarity is not an issue, the class predictions from different samples would unavoidably yield a substantial BCN loss, and prompt the network learning to further reduce the cross-entropy loss. More importantly, extending the existing confusion energy-based framework to account for long-tailed scenario, BCN can learn to exert proper distribution of confusion strength over tailed and head categories to improve classification performance. While the resulting FGVC model by the BCN technique is effective, the performance can be consistently boosted by incorporating extra attention mechanism. In our experiments, we have obtained state-of-the-art results on several benchmark FGVC datasets, and also demonstrated that our approach is competitive on the popular natural world distribution dataset, iNaturalist2018. ", "one-sentence_summary": "We propose a novel confusion term which can adaptively addresses the fine-grained and long-tailed distribution simultaneously.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hsu|natural_world_distribution_via_adaptive_confusion_energy_regularization", "pdf": "/pdf/584bf002b19b20eb8ae94f1f724a508a910c02d9.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=GEFTdiT8I", "_bibtex": "@misc{\nhsu2021natural,\ntitle={Natural World Distribution via Adaptive Confusion Energy Regularization},\nauthor={Yen-Chi Hsu and Cheng-Yao Hong and Wan-Cyuan Fan and Ding-Jie Chen and Ming-Sui Lee and davi geiger and Tyng-Luh Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=kKwFlM32HV5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "kKwFlM32HV5", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1442/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1442/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1442/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1442/Authors|ICLR.cc/2021/Conference/Paper1442/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1442/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923859637, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1442/-/Official_Comment"}}}, {"id": "pjmhng2rBNg", "original": null, "number": 5, "cdate": 1606300092005, "ddate": null, "tcdate": 1606300092005, "tmdate": 1606300092005, "tddate": null, "forum": "kKwFlM32HV5", "replyto": "7O5zRWYL6rD", "invitation": "ICLR.cc/2021/Conference/Paper1442/-/Official_Comment", "content": {"title": "We will revise some sentences to make it easier to follow.", "comment": "[Q1]. The writing made this paper really hard to understand. The formulations, particularly in abstract and introduction, are inaccurate and vague. They should rather be specific and concrete. For instance, the statement \"When inter-class similarity prevails in a batch, the BCN term can alleviate possible overfitting due to exploring image features of fine details\" is hard to understand since it was totally unclear what the BCN term actually is at that point of reading. In both abstract and introduction, I had a very hard time imagining what certain statements mean in terms of who the method would look like, without having read the full paper.\n\n[A1]. We are sorry for the problem. But here is the reason. We straddle two different domains at the same time, and each of them is the mainstream research direction recently. Hence, due to the limited space, we should try to write the article concisely. We will review and revise the structure we write.\n\n[Q2]. The results barely improve over SOTA, particularly for the three FGVC data sets. So the bigger advantage I see was for class imbalanced data sets, like iNaturalist. Although the performance is also not significantly better than prior works, the direct competitor (PC) seems to under-perform clearly. The proposed solution alleviates the problem, which is good. In light of this, I think it would make sense to build such data sets synthetically from the existing ones (like Cub, Car, Air) by removing samples to increase class imbalance. This would allow additional experiments on class imbalance in a controlled setup.\n\n[A2]. Thanks to the reviewer for the advice. The suggestion will make the article more convincing. We will make further improvements in this direction.\n\n[Q3]. Minor notes.\n\n[A3]. Thanks for the reviewer's comments. We will correct them and revise the sentences."}, "signatures": ["ICLR.cc/2021/Conference/Paper1442/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1442/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Natural World Distribution via Adaptive Confusion Energy Regularization", "authorids": ["~Yen-Chi_Hsu1", "~Cheng-Yao_Hong2", "~Wan-Cyuan_Fan1", "~Ding-Jie_Chen1", "~Ming-Sui_Lee1", "~davi_geiger1", "~Tyng-Luh_Liu1"], "authors": ["Yen-Chi Hsu", "Cheng-Yao Hong", "Wan-Cyuan Fan", "Ding-Jie Chen", "Ming-Sui Lee", "davi geiger", "Tyng-Luh Liu"], "keywords": ["Fine-Grained Visual Classification", "long-tailed distribution", "confusion energy"], "abstract": "We introduce a novel and adaptive batch-wise regularization based on the proposed Batch Confusion Norm (BCN) to flexibly address the natural world distribution which usually involves fine-grained and long-tailed properties at the same time. The Fine-Grained Visual Classification (FGVC) problem is notably characterized by two intriguing properties, significant inter-class similarity and intra-class variations, which cause learning an effective FGVC classifier a challenging task. Existing techniques attempt to capture the discriminative parts by their modified attention mechanism. The long-tailed distribution of visual classification poses a great challenge for handling the class imbalance problem. Most of existing solutions usually focus on the class-balancing strategies, classifier normalization, or alleviating the negative gradient of tailed categories. Depart from the conventional approaches, we propose to tackle both problems simultaneously with the adaptive confusion concept. When inter-class similarity prevails in a batch, the BCN term can alleviate possible overfitting due to exploring image features of fine details. On the other hand, when inter-class similarity is not an issue, the class predictions from different samples would unavoidably yield a substantial BCN loss, and prompt the network learning to further reduce the cross-entropy loss. More importantly, extending the existing confusion energy-based framework to account for long-tailed scenario, BCN can learn to exert proper distribution of confusion strength over tailed and head categories to improve classification performance. While the resulting FGVC model by the BCN technique is effective, the performance can be consistently boosted by incorporating extra attention mechanism. In our experiments, we have obtained state-of-the-art results on several benchmark FGVC datasets, and also demonstrated that our approach is competitive on the popular natural world distribution dataset, iNaturalist2018. ", "one-sentence_summary": "We propose a novel confusion term which can adaptively addresses the fine-grained and long-tailed distribution simultaneously.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hsu|natural_world_distribution_via_adaptive_confusion_energy_regularization", "pdf": "/pdf/584bf002b19b20eb8ae94f1f724a508a910c02d9.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=GEFTdiT8I", "_bibtex": "@misc{\nhsu2021natural,\ntitle={Natural World Distribution via Adaptive Confusion Energy Regularization},\nauthor={Yen-Chi Hsu and Cheng-Yao Hong and Wan-Cyuan Fan and Ding-Jie Chen and Ming-Sui Lee and davi geiger and Tyng-Luh Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=kKwFlM32HV5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "kKwFlM32HV5", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1442/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1442/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1442/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1442/Authors|ICLR.cc/2021/Conference/Paper1442/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1442/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923859637, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1442/-/Official_Comment"}}}, {"id": "YdlHNuI9-pm", "original": null, "number": 4, "cdate": 1606299741874, "ddate": null, "tcdate": 1606299741874, "tmdate": 1606299741874, "tddate": null, "forum": "kKwFlM32HV5", "replyto": "GkhzrehohLy", "invitation": "ICLR.cc/2021/Conference/Paper1442/-/Official_Comment", "content": {"title": "The novelty is that we propose a new confusion energy called BCN which can easily address the natural world data distribution problem with a competitive performance ", "comment": "[Q1]. I find this to be of minimal novelty, and I question the practicality of the method. Unless I have misunderstood, this sounds like it would be slower to train and I did not see any runtime analysis comparisons in the experiments section. Experiments showing how the new loss function effects training time would help alleviate this concern. The method does however improve performance on the tested benchmarks.\n\n[A1]. Firstly, we claim that the novelty of this paper is that our approach can solve the natural world data distribution with a general and reasonable form. Lots of previous works only focus on fine-grained or long-tailed problems. Second, we agree with the reviewer\u2019s argument, but it won\u2019t be the main issue. The runtime of our approach is only slower than the baseline but faster than all of the methods that are presented in Table 2. Hence, in a limited number of pages, we only claim that our training process is more simple and practical instead of presenting it as a table.\n\n[Q2]. Finally, I found that there were numerous grammatical and stylistic mistakes. The writing improves during the discussion of the mathematics of the technique, but the introduction, experiments, and discussion need work. I would like to see the writing improved for a publication.\n\n[A2]. We agree with the reviewer\u2019s comments. In this paper, we straddle two different domains at the same time, and each of them is the mainstream research direction recently. Whether in fine-grained or long-tailed problems, each of them could have written as a complete article. Hence, in a limited number of pages, it is a little bit challenging to explain more details about how our method affects both of these problems. Finally, we will review and revise the writing as the reviewer mentioned."}, "signatures": ["ICLR.cc/2021/Conference/Paper1442/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1442/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Natural World Distribution via Adaptive Confusion Energy Regularization", "authorids": ["~Yen-Chi_Hsu1", "~Cheng-Yao_Hong2", "~Wan-Cyuan_Fan1", "~Ding-Jie_Chen1", "~Ming-Sui_Lee1", "~davi_geiger1", "~Tyng-Luh_Liu1"], "authors": ["Yen-Chi Hsu", "Cheng-Yao Hong", "Wan-Cyuan Fan", "Ding-Jie Chen", "Ming-Sui Lee", "davi geiger", "Tyng-Luh Liu"], "keywords": ["Fine-Grained Visual Classification", "long-tailed distribution", "confusion energy"], "abstract": "We introduce a novel and adaptive batch-wise regularization based on the proposed Batch Confusion Norm (BCN) to flexibly address the natural world distribution which usually involves fine-grained and long-tailed properties at the same time. The Fine-Grained Visual Classification (FGVC) problem is notably characterized by two intriguing properties, significant inter-class similarity and intra-class variations, which cause learning an effective FGVC classifier a challenging task. Existing techniques attempt to capture the discriminative parts by their modified attention mechanism. The long-tailed distribution of visual classification poses a great challenge for handling the class imbalance problem. Most of existing solutions usually focus on the class-balancing strategies, classifier normalization, or alleviating the negative gradient of tailed categories. Depart from the conventional approaches, we propose to tackle both problems simultaneously with the adaptive confusion concept. When inter-class similarity prevails in a batch, the BCN term can alleviate possible overfitting due to exploring image features of fine details. On the other hand, when inter-class similarity is not an issue, the class predictions from different samples would unavoidably yield a substantial BCN loss, and prompt the network learning to further reduce the cross-entropy loss. More importantly, extending the existing confusion energy-based framework to account for long-tailed scenario, BCN can learn to exert proper distribution of confusion strength over tailed and head categories to improve classification performance. While the resulting FGVC model by the BCN technique is effective, the performance can be consistently boosted by incorporating extra attention mechanism. In our experiments, we have obtained state-of-the-art results on several benchmark FGVC datasets, and also demonstrated that our approach is competitive on the popular natural world distribution dataset, iNaturalist2018. ", "one-sentence_summary": "We propose a novel confusion term which can adaptively addresses the fine-grained and long-tailed distribution simultaneously.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hsu|natural_world_distribution_via_adaptive_confusion_energy_regularization", "pdf": "/pdf/584bf002b19b20eb8ae94f1f724a508a910c02d9.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=GEFTdiT8I", "_bibtex": "@misc{\nhsu2021natural,\ntitle={Natural World Distribution via Adaptive Confusion Energy Regularization},\nauthor={Yen-Chi Hsu and Cheng-Yao Hong and Wan-Cyuan Fan and Ding-Jie Chen and Ming-Sui Lee and davi geiger and Tyng-Luh Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=kKwFlM32HV5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "kKwFlM32HV5", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1442/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1442/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1442/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1442/Authors|ICLR.cc/2021/Conference/Paper1442/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1442/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923859637, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1442/-/Official_Comment"}}}, {"id": "7O5zRWYL6rD", "original": null, "number": 1, "cdate": 1603837380797, "ddate": null, "tcdate": 1603837380797, "tmdate": 1605024442896, "tddate": null, "forum": "kKwFlM32HV5", "replyto": "kKwFlM32HV5", "invitation": "ICLR.cc/2021/Conference/Paper1442/-/Official_Review", "content": {"title": "Interesting idea, poor presentation", "review": "**Summary:**\nThis paper is about fine-grained visual classification, which is challenging due to high inter-class similarity, high intra-class variation and potentially also class imbalance. The authors propose a regularization term that is added to a standard cross-entropy loss of a neural network trained in a supervised fashion. This regularization shares the motivation with prior art to confuse the network and reduce over-fitting by making all predictions within a mini-batch similar to each other. Different to prior art, they authors propose an approximation of a minimum rank objective. To also handle class imbalance, the authors extend the regularization with a learnable matrix that can automatically balance the importance of individual classes.\n\n**Pros:**\n- Overall, I think the contributions of the paper is interesting and useful, particularly the extension of the confusion-based regularization for class-imbalanced data sets. Unfortunately, the presentation of the idea needs significant improvement, see below.\n- The analysis and discussion in Sections 4.4 and 4.5 are great\n\n**Cons:**\n- The writing made this paper really hard to understand. The formulations, particularly in abstract and introduction, are inaccurate and vague. They should rather be specific and concrete. For instance, the statement \"When inter-class similarity prevails in a batch, the BCN term can alleviate possible overfitting due to exploring image features of fine details\" is hard to understand since it was totally unclear what the BCN term actually is at that point of reading. In both abstract and introduction, I had a very hard time imagining what certain statements mean in terms of who the method would look like, without having read the full paper.\n- The results barely improve over SOTA, particularly for the three FGVC data sets. So the bigger advantage I see was for class imbalanced data sets, like iNaturalist. Although the performance is also not significantly better than prior works, the direct competitor (PC) seems to under-perform clearly. The proposed solution alleviates the problem, which is good. In light of this, I think it would make sense to build such data sets synthetically from the existing ones (like Cub, Car, Air) by removing samples to increase class imbalance. This would allow additional experiments on class imbalance in a controlled setup.\n\n\n**Minor notes:**\n- The author names can be dropped with the ICLR citation format: \"... Dubey et al.Dubey et al. (2018) construct a Siamese\" on page 3.\n- Statements like \"The confusion-related formulation for dealing with intra-class variations and inter-class similarity in FGVC have two main implications\" on page 3 require the reader to be very familiar with these concepts. It would be better to re-formulate it with a brief introduction of the concept (just a sentence or two).\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1442/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1442/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Natural World Distribution via Adaptive Confusion Energy Regularization", "authorids": ["~Yen-Chi_Hsu1", "~Cheng-Yao_Hong2", "~Wan-Cyuan_Fan1", "~Ding-Jie_Chen1", "~Ming-Sui_Lee1", "~davi_geiger1", "~Tyng-Luh_Liu1"], "authors": ["Yen-Chi Hsu", "Cheng-Yao Hong", "Wan-Cyuan Fan", "Ding-Jie Chen", "Ming-Sui Lee", "davi geiger", "Tyng-Luh Liu"], "keywords": ["Fine-Grained Visual Classification", "long-tailed distribution", "confusion energy"], "abstract": "We introduce a novel and adaptive batch-wise regularization based on the proposed Batch Confusion Norm (BCN) to flexibly address the natural world distribution which usually involves fine-grained and long-tailed properties at the same time. The Fine-Grained Visual Classification (FGVC) problem is notably characterized by two intriguing properties, significant inter-class similarity and intra-class variations, which cause learning an effective FGVC classifier a challenging task. Existing techniques attempt to capture the discriminative parts by their modified attention mechanism. The long-tailed distribution of visual classification poses a great challenge for handling the class imbalance problem. Most of existing solutions usually focus on the class-balancing strategies, classifier normalization, or alleviating the negative gradient of tailed categories. Depart from the conventional approaches, we propose to tackle both problems simultaneously with the adaptive confusion concept. When inter-class similarity prevails in a batch, the BCN term can alleviate possible overfitting due to exploring image features of fine details. On the other hand, when inter-class similarity is not an issue, the class predictions from different samples would unavoidably yield a substantial BCN loss, and prompt the network learning to further reduce the cross-entropy loss. More importantly, extending the existing confusion energy-based framework to account for long-tailed scenario, BCN can learn to exert proper distribution of confusion strength over tailed and head categories to improve classification performance. While the resulting FGVC model by the BCN technique is effective, the performance can be consistently boosted by incorporating extra attention mechanism. In our experiments, we have obtained state-of-the-art results on several benchmark FGVC datasets, and also demonstrated that our approach is competitive on the popular natural world distribution dataset, iNaturalist2018. ", "one-sentence_summary": "We propose a novel confusion term which can adaptively addresses the fine-grained and long-tailed distribution simultaneously.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hsu|natural_world_distribution_via_adaptive_confusion_energy_regularization", "pdf": "/pdf/584bf002b19b20eb8ae94f1f724a508a910c02d9.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=GEFTdiT8I", "_bibtex": "@misc{\nhsu2021natural,\ntitle={Natural World Distribution via Adaptive Confusion Energy Regularization},\nauthor={Yen-Chi Hsu and Cheng-Yao Hong and Wan-Cyuan Fan and Ding-Jie Chen and Ming-Sui Lee and davi geiger and Tyng-Luh Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=kKwFlM32HV5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "kKwFlM32HV5", "replyto": "kKwFlM32HV5", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1442/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538118514, "tmdate": 1606915807876, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1442/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1442/-/Official_Review"}}}, {"id": "GkhzrehohLy", "original": null, "number": 2, "cdate": 1603930127421, "ddate": null, "tcdate": 1603930127421, "tmdate": 1605024442812, "tddate": null, "forum": "kKwFlM32HV5", "replyto": "kKwFlM32HV5", "invitation": "ICLR.cc/2021/Conference/Paper1442/-/Official_Review", "content": {"title": "Minimal Novelty but Good Results", "review": "This paper presents a novel technique for fine-grained visual classification. This technique addresses the classic issues in this task of inter-class similarity (coupled with intra-class variation) and the \u201clong tailed\u201d dataset problem, prevalent in datasets such as iNaturalist2018. The technique is an extension of Dubey et. al which extends their technique by incentivising the predictions for all samples in a mini-batch to be similar. This is in contrast to Dubey et. al which splits the mini-batch into two halves and incentivizes the aggregate predictions of the two halves to be similar. \n\nI find this to be of minimal novelty, and I question the practicality of the method. Unless I have misunderstood, this sounds like it would be slower to train and I did not see any runtime analysis comparisons in the experiments section. Experiments showing how the new loss function effects training time would help alleviate this concern. The method does however improve performance on the tested benchmarks. \n\nFinally, I found that there were numerous grammatical and stylistic mistakes. The writing improves during the discussion of the mathematics of the technique, but the introduction, experiments, and discussion need work. I would like to see the writing improved for a publication. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1442/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1442/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Natural World Distribution via Adaptive Confusion Energy Regularization", "authorids": ["~Yen-Chi_Hsu1", "~Cheng-Yao_Hong2", "~Wan-Cyuan_Fan1", "~Ding-Jie_Chen1", "~Ming-Sui_Lee1", "~davi_geiger1", "~Tyng-Luh_Liu1"], "authors": ["Yen-Chi Hsu", "Cheng-Yao Hong", "Wan-Cyuan Fan", "Ding-Jie Chen", "Ming-Sui Lee", "davi geiger", "Tyng-Luh Liu"], "keywords": ["Fine-Grained Visual Classification", "long-tailed distribution", "confusion energy"], "abstract": "We introduce a novel and adaptive batch-wise regularization based on the proposed Batch Confusion Norm (BCN) to flexibly address the natural world distribution which usually involves fine-grained and long-tailed properties at the same time. The Fine-Grained Visual Classification (FGVC) problem is notably characterized by two intriguing properties, significant inter-class similarity and intra-class variations, which cause learning an effective FGVC classifier a challenging task. Existing techniques attempt to capture the discriminative parts by their modified attention mechanism. The long-tailed distribution of visual classification poses a great challenge for handling the class imbalance problem. Most of existing solutions usually focus on the class-balancing strategies, classifier normalization, or alleviating the negative gradient of tailed categories. Depart from the conventional approaches, we propose to tackle both problems simultaneously with the adaptive confusion concept. When inter-class similarity prevails in a batch, the BCN term can alleviate possible overfitting due to exploring image features of fine details. On the other hand, when inter-class similarity is not an issue, the class predictions from different samples would unavoidably yield a substantial BCN loss, and prompt the network learning to further reduce the cross-entropy loss. More importantly, extending the existing confusion energy-based framework to account for long-tailed scenario, BCN can learn to exert proper distribution of confusion strength over tailed and head categories to improve classification performance. While the resulting FGVC model by the BCN technique is effective, the performance can be consistently boosted by incorporating extra attention mechanism. In our experiments, we have obtained state-of-the-art results on several benchmark FGVC datasets, and also demonstrated that our approach is competitive on the popular natural world distribution dataset, iNaturalist2018. ", "one-sentence_summary": "We propose a novel confusion term which can adaptively addresses the fine-grained and long-tailed distribution simultaneously.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hsu|natural_world_distribution_via_adaptive_confusion_energy_regularization", "pdf": "/pdf/584bf002b19b20eb8ae94f1f724a508a910c02d9.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=GEFTdiT8I", "_bibtex": "@misc{\nhsu2021natural,\ntitle={Natural World Distribution via Adaptive Confusion Energy Regularization},\nauthor={Yen-Chi Hsu and Cheng-Yao Hong and Wan-Cyuan Fan and Ding-Jie Chen and Ming-Sui Lee and davi geiger and Tyng-Luh Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=kKwFlM32HV5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "kKwFlM32HV5", "replyto": "kKwFlM32HV5", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1442/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538118514, "tmdate": 1606915807876, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1442/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1442/-/Official_Review"}}}, {"id": "evX4kdIRf8r", "original": null, "number": 3, "cdate": 1603944769054, "ddate": null, "tcdate": 1603944769054, "tmdate": 1605024442730, "tddate": null, "forum": "kKwFlM32HV5", "replyto": "kKwFlM32HV5", "invitation": "ICLR.cc/2021/Conference/Paper1442/-/Official_Review", "content": {"title": "Interesting Idea but Not Convinced About the Method nor by the Experiments", "review": "**Overview:** The paper presents an extension to the Batch Confusion Norm (BCN) regularization technique so that it can account for imbalanced datasets. The extension to BCN implies adding a matrix that determines its values as a function of class imbalanced statistics contained in a batch. The paper presents experiments showing the benefits of the proposed extension on Fine-Grained Visual Classification (FGVC) and long-tailed (LT) tasks. The experiments show modest improvements over the baselines.\n\n**Pros:**\n*Clarity of the paper is good.* The clarity of the paper is very good. The motivation behind the FGVC and LT learning problems as well as the proposed method is clear. Thanks to the clarity of the paper I believe the reproducibility should be good. Also, I believe that the paper addresses an important problem with practical value.\n\n**Cons:**\n*Novelty.* The novelty of the paper IMHO falls short. This is because the submission mainly extends the BCN paper by adding a matrix A whose entries are a function of the statistics of the statistics of the dataset.  The proposed extension lacks a more rigorous derivation and justification; IMHO, the proposed extension seems to be ad hoc.\n\n*Not convinced about BCN and proposed extension.* \n1) I am not sure if BCN is a proper regularizer. While I understand the geometry behind minimizing the rank of the matrix P, I don't think this is a proper way of processing the columns of matrix P which are *posterior distributions*. I think the paper lacks a clear justification about using BCN as a way to treat posterior distributions without using statistical or probabilistic methods. \n2) BCN is conditioned to operate if the classes in the batch are unique. However, satisfying this condition in practice can be challenging. This becomes challenging when dealing with long-tailed datasets. What is the optimal/efficient way to guarantee a batch that satisfies this constraint? If the constraint is not satisfied, can BCN provide bad gradients because repeated classes with different posteriors in different columns can double its contribution?\n3) The extension of adding a matrix A is simple and feels a bit ad hoc. What is the intuition behind the matrix A in terms of the norm? Can the matrix A be considered a way to ` *weight* a posterior as a function of the statistics?\n\n*Insufficient experiments and baselines.* \n1) While I am pleased to see that the paper uses datasets depicting real scenes (e.g., iNaturalist, CUB, CAR, etc.), the experiments only focus on using one family of networks: ResNet. Does this method operate well on other more modern architectures, e.g., EfficientNet or MobileNetV2? \n2) In terms of baselines, I think the paper is missing recent long-tail methods:\n  a) Liu, et al. Large-Scale Long-Tailed Recognition in an Open World. CVPR 2019. \n  b) Cao, et al. Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss. NeurIPS 2019.\n3) The results for the long-tail methods are lacking more details. As it is common in various long-tail recognition papers, the paper is not showing performance on head or tail classes. It only shows an overall classification performance. This is misleading as the method can helping more head classes and thus improving the overall classification performance.\n\n*Minor concern*: Most of the figures and diagrams look nearly identical to those presented in the BCM paper.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1442/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1442/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Natural World Distribution via Adaptive Confusion Energy Regularization", "authorids": ["~Yen-Chi_Hsu1", "~Cheng-Yao_Hong2", "~Wan-Cyuan_Fan1", "~Ding-Jie_Chen1", "~Ming-Sui_Lee1", "~davi_geiger1", "~Tyng-Luh_Liu1"], "authors": ["Yen-Chi Hsu", "Cheng-Yao Hong", "Wan-Cyuan Fan", "Ding-Jie Chen", "Ming-Sui Lee", "davi geiger", "Tyng-Luh Liu"], "keywords": ["Fine-Grained Visual Classification", "long-tailed distribution", "confusion energy"], "abstract": "We introduce a novel and adaptive batch-wise regularization based on the proposed Batch Confusion Norm (BCN) to flexibly address the natural world distribution which usually involves fine-grained and long-tailed properties at the same time. The Fine-Grained Visual Classification (FGVC) problem is notably characterized by two intriguing properties, significant inter-class similarity and intra-class variations, which cause learning an effective FGVC classifier a challenging task. Existing techniques attempt to capture the discriminative parts by their modified attention mechanism. The long-tailed distribution of visual classification poses a great challenge for handling the class imbalance problem. Most of existing solutions usually focus on the class-balancing strategies, classifier normalization, or alleviating the negative gradient of tailed categories. Depart from the conventional approaches, we propose to tackle both problems simultaneously with the adaptive confusion concept. When inter-class similarity prevails in a batch, the BCN term can alleviate possible overfitting due to exploring image features of fine details. On the other hand, when inter-class similarity is not an issue, the class predictions from different samples would unavoidably yield a substantial BCN loss, and prompt the network learning to further reduce the cross-entropy loss. More importantly, extending the existing confusion energy-based framework to account for long-tailed scenario, BCN can learn to exert proper distribution of confusion strength over tailed and head categories to improve classification performance. While the resulting FGVC model by the BCN technique is effective, the performance can be consistently boosted by incorporating extra attention mechanism. In our experiments, we have obtained state-of-the-art results on several benchmark FGVC datasets, and also demonstrated that our approach is competitive on the popular natural world distribution dataset, iNaturalist2018. ", "one-sentence_summary": "We propose a novel confusion term which can adaptively addresses the fine-grained and long-tailed distribution simultaneously.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hsu|natural_world_distribution_via_adaptive_confusion_energy_regularization", "pdf": "/pdf/584bf002b19b20eb8ae94f1f724a508a910c02d9.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=GEFTdiT8I", "_bibtex": "@misc{\nhsu2021natural,\ntitle={Natural World Distribution via Adaptive Confusion Energy Regularization},\nauthor={Yen-Chi Hsu and Cheng-Yao Hong and Wan-Cyuan Fan and Ding-Jie Chen and Ming-Sui Lee and davi geiger and Tyng-Luh Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=kKwFlM32HV5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "kKwFlM32HV5", "replyto": "kKwFlM32HV5", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1442/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538118514, "tmdate": 1606915807876, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1442/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1442/-/Official_Review"}}}, {"id": "NUSZc-uBoOC", "original": null, "number": 4, "cdate": 1603980574649, "ddate": null, "tcdate": 1603980574649, "tmdate": 1605024442648, "tddate": null, "forum": "kKwFlM32HV5", "replyto": "kKwFlM32HV5", "invitation": "ICLR.cc/2021/Conference/Paper1442/-/Official_Review", "content": {"title": "Official Blind Review #2", "review": "This paper proposes the batch confusion norm (BCN) for dealing with both fine-grained recognition and long-tailed recognition simultaneously. Specifically, BCN considers the confusion regularization within each training batch and an adaptive matrix term is designed for handling the long-tailed problem. Experiments are conducted on fine-grained benchmark datasets (e.g., CUB, CAR, AIR) as well as long-tailed recognition datasets (e.g., iNat18). \n\nPaper strengths:\n- The paper is well organized and easy to follow.\n- The problems studied in this paper, i.e., fine-grained recognition and long-tailed visual recognition, are both important, challenging and practical in computer vision, which deserves further studies.\n- The proposed method sounds reasonable.\n\nPaper weaknesses:\n- Although the proposed method is reasonable, some specific model designs are not quite clear. 1) Regarding Eq. (2), the reason why it requires to optimize the ranking should be further explained and its motivation needs to state. 2) Regarding Eq. (5), what the intuition of the adaptive matrix (i.e., (log_{\\mu+1} (N_i+1))^{\\delta^{\\tau}}) when i = j should be provided to the authors.\n- The major issue of this paper is the experimental evaluations. 1) The classification accuracy on these fine-grained benchmark datasets and iNat18 are not significantly better than the accuracy of previous work. Thus, the effectiveness of the proposed method is problematic. 2) Some state-of-the-art methods are not involved in the experimental comparisons, such as [ref1-ref5]. Moreover, the accuracy of the proposed method cannot outperform these methods.\n\nMinor issues:\n- There are several typos and writing problems in this paper. For example, on Page 3, \"Dubey et al.Dubey et al. (2018)\", and \"Chen et al.Chen et al. (2019)\". On Page 4, \"PC Dubey et al. (2018)\". On Page 8, \"And also solves the long-tailed problem by an adaptive matrix term.\"\n\n[ref1] Weakly Supervised Fine-grained Image Classification via Guassian Mixture Model Oriented Discriminative Learning, CVPR 2020.\n\n[ref2] Weakly Supervised Complementary Parts Models for Fine-Grained Image Classification from the Bottom Up, CVPR 2019.\n\n[ref3] Fine-Grained Visual Classification via Progressive Multi-Granularity Training of Jigsaw Patches, ECCV 2020.\n\n[ref4] Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss, NeurIPS 2019.\n\n[ref5] BBN: Bilateral-Branch Network with Cumulative Learning for Long-Tailed Visual Recognition, CVPR 2020.", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1442/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1442/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Natural World Distribution via Adaptive Confusion Energy Regularization", "authorids": ["~Yen-Chi_Hsu1", "~Cheng-Yao_Hong2", "~Wan-Cyuan_Fan1", "~Ding-Jie_Chen1", "~Ming-Sui_Lee1", "~davi_geiger1", "~Tyng-Luh_Liu1"], "authors": ["Yen-Chi Hsu", "Cheng-Yao Hong", "Wan-Cyuan Fan", "Ding-Jie Chen", "Ming-Sui Lee", "davi geiger", "Tyng-Luh Liu"], "keywords": ["Fine-Grained Visual Classification", "long-tailed distribution", "confusion energy"], "abstract": "We introduce a novel and adaptive batch-wise regularization based on the proposed Batch Confusion Norm (BCN) to flexibly address the natural world distribution which usually involves fine-grained and long-tailed properties at the same time. The Fine-Grained Visual Classification (FGVC) problem is notably characterized by two intriguing properties, significant inter-class similarity and intra-class variations, which cause learning an effective FGVC classifier a challenging task. Existing techniques attempt to capture the discriminative parts by their modified attention mechanism. The long-tailed distribution of visual classification poses a great challenge for handling the class imbalance problem. Most of existing solutions usually focus on the class-balancing strategies, classifier normalization, or alleviating the negative gradient of tailed categories. Depart from the conventional approaches, we propose to tackle both problems simultaneously with the adaptive confusion concept. When inter-class similarity prevails in a batch, the BCN term can alleviate possible overfitting due to exploring image features of fine details. On the other hand, when inter-class similarity is not an issue, the class predictions from different samples would unavoidably yield a substantial BCN loss, and prompt the network learning to further reduce the cross-entropy loss. More importantly, extending the existing confusion energy-based framework to account for long-tailed scenario, BCN can learn to exert proper distribution of confusion strength over tailed and head categories to improve classification performance. While the resulting FGVC model by the BCN technique is effective, the performance can be consistently boosted by incorporating extra attention mechanism. In our experiments, we have obtained state-of-the-art results on several benchmark FGVC datasets, and also demonstrated that our approach is competitive on the popular natural world distribution dataset, iNaturalist2018. ", "one-sentence_summary": "We propose a novel confusion term which can adaptively addresses the fine-grained and long-tailed distribution simultaneously.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hsu|natural_world_distribution_via_adaptive_confusion_energy_regularization", "pdf": "/pdf/584bf002b19b20eb8ae94f1f724a508a910c02d9.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=GEFTdiT8I", "_bibtex": "@misc{\nhsu2021natural,\ntitle={Natural World Distribution via Adaptive Confusion Energy Regularization},\nauthor={Yen-Chi Hsu and Cheng-Yao Hong and Wan-Cyuan Fan and Ding-Jie Chen and Ming-Sui Lee and davi geiger and Tyng-Luh Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=kKwFlM32HV5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "kKwFlM32HV5", "replyto": "kKwFlM32HV5", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1442/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538118514, "tmdate": 1606915807876, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1442/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1442/-/Official_Review"}}}], "count": 10}