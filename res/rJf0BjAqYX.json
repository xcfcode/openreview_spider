{"notes": [{"id": "rJf0BjAqYX", "original": "HylJkiLDYm", "number": 134, "cdate": 1538087750190, "ddate": null, "tcdate": 1538087750190, "tmdate": 1545355402481, "tddate": null, "forum": "rJf0BjAqYX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Like What You Like: Knowledge Distill via Neuron Selectivity Transfer", "abstract": "Despite deep neural networks have demonstrated extraordinary power in various applications, their superior performances are at expense of high storage and computational costs. Consequently, the acceleration and compression of neural networks have attracted much attention recently. Knowledge Transfer (KT), which aims at training a smaller student network by transferring knowledge from a larger teacher model, is one of the popular solutions. In this paper, we propose a novel knowledge transfer method by treating it as a distribution matching problem. Particularly, we match the distributions of neuron selectivity patterns between teacher and student networks. To achieve this goal, we devise a new KT loss function by minimizing the Maximum Mean Discrepancy (MMD) metric between these distributions. Combined with the original loss function, our method can significantly improve the performance of student networks. We validate the effectiveness of our method across several datasets, and further combine it with other KT methods to explore the best possible results. Last but not least, we fine-tune the model to other tasks such as object detection. The results are also encouraging, which confirm the transferability of the learned features.", "keywords": ["Knowledge Distill"], "authorids": ["zehaohuang18@gmail.com", "winsty@gmail.com"], "authors": ["Zehao Huang", "Naiyan Wang"], "TL;DR": "We treat knowledge distill as a distribution matching problem and adopt Maximum Mean Discrepancy to minimize the distances between student features and teacher features.", "pdf": "/pdf/3f4fd0cca1cf1530bd1eb068eddb668ec8c2826e.pdf", "paperhash": "huang|like_what_you_like_knowledge_distill_via_neuron_selectivity_transfer", "_bibtex": "@misc{\nhuang2019like,\ntitle={Like What You Like: Knowledge Distill via Neuron Selectivity Transfer},\nauthor={Zehao Huang and Naiyan Wang},\nyear={2019},\nurl={https://openreview.net/forum?id=rJf0BjAqYX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "BJeIuCzlxE", "original": null, "number": 1, "cdate": 1544724078482, "ddate": null, "tcdate": 1544724078482, "tmdate": 1545354510402, "tddate": null, "forum": "rJf0BjAqYX", "replyto": "rJf0BjAqYX", "invitation": "ICLR.cc/2019/Conference/-/Paper134/Meta_Review", "content": {"metareview": " The paper presents a sensible algorithm for knowledge distillation (KD) from a larger teacher network to a smaller student network by minimizing the Maximum Mean Discrepancy (MMD) between the distributions over students and teachers network activations. As rightly acknowledged by the R3, the benefits of the proposed approach are encouraging in the object detection task, and are less obvious in classification (R1 and R2).\n \nThe reviewers and AC note the following potential weaknesses:\n(1) low technical novelty in light of prior works \u201cDemystifying Neural Style Transfer\u201d by Li et al 2017 and \u201cDeep Transfer Learning with Joint Adaptation Networks\u201d by Long et al 2017 -- See R2\u2019s detailed explanations; (2) lack of empirical evidence that the proposed method is better than the seminal work on KD by Hinton et al, 2014; (3) important practical issues are not justified (e.g. kernel specifications as requested by R3 and R2; accuracy-efficiency trade-off as suggested by R1); (4) presentation clarity.  \nR3 has raised questions regarding deploying the proposed student models on mobile devices without a proper comparison with the MobileNet and ShuffleNet light architectures. This can be seen as a suggestion for future revisions. \n\nThere is reviewer disagreement on this paper and no author rebuttal. The reviewer with a positive view on the manuscript (R3) was reluctant to champion the paper as the authors did not respond to the concerns of the reviewers. \nAC suggests in its current state the manuscript is not ready for a publication. We hope the reviews are useful for improving and revising the paper. ", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Meta-Review"}, "signatures": ["ICLR.cc/2019/Conference/Paper134/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper134/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Like What You Like: Knowledge Distill via Neuron Selectivity Transfer", "abstract": "Despite deep neural networks have demonstrated extraordinary power in various applications, their superior performances are at expense of high storage and computational costs. Consequently, the acceleration and compression of neural networks have attracted much attention recently. Knowledge Transfer (KT), which aims at training a smaller student network by transferring knowledge from a larger teacher model, is one of the popular solutions. In this paper, we propose a novel knowledge transfer method by treating it as a distribution matching problem. Particularly, we match the distributions of neuron selectivity patterns between teacher and student networks. To achieve this goal, we devise a new KT loss function by minimizing the Maximum Mean Discrepancy (MMD) metric between these distributions. Combined with the original loss function, our method can significantly improve the performance of student networks. We validate the effectiveness of our method across several datasets, and further combine it with other KT methods to explore the best possible results. Last but not least, we fine-tune the model to other tasks such as object detection. The results are also encouraging, which confirm the transferability of the learned features.", "keywords": ["Knowledge Distill"], "authorids": ["zehaohuang18@gmail.com", "winsty@gmail.com"], "authors": ["Zehao Huang", "Naiyan Wang"], "TL;DR": "We treat knowledge distill as a distribution matching problem and adopt Maximum Mean Discrepancy to minimize the distances between student features and teacher features.", "pdf": "/pdf/3f4fd0cca1cf1530bd1eb068eddb668ec8c2826e.pdf", "paperhash": "huang|like_what_you_like_knowledge_distill_via_neuron_selectivity_transfer", "_bibtex": "@misc{\nhuang2019like,\ntitle={Like What You Like: Knowledge Distill via Neuron Selectivity Transfer},\nauthor={Zehao Huang and Naiyan Wang},\nyear={2019},\nurl={https://openreview.net/forum?id=rJf0BjAqYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper134/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353324888, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJf0BjAqYX", "replyto": "rJf0BjAqYX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper134/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper134/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper134/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353324888}}}, {"id": "BygynUFT3X", "original": null, "number": 3, "cdate": 1541408422519, "ddate": null, "tcdate": 1541408422519, "tmdate": 1541534252429, "tddate": null, "forum": "rJf0BjAqYX", "replyto": "rJf0BjAqYX", "invitation": "ICLR.cc/2019/Conference/-/Paper134/Official_Review", "content": {"title": "Need to improve clarity and interpretability", "review": "This submission proposes a novel loss function, based on Maximum Mean Discrepancy (MMD), for knowledge transfer (distillation) from a teacher network to a student network, which matches the spatial distribution of neuron activations between the two.\n\nThe proposed approach is interesting but there is significant room for improvement. In particular:\n\n*Clarity*\n\nIt is not clear how the distribution of neuron activation are matched between the teach and student network. The C_T and C_S are not defined specifically enough. Does it include all layers? Or does it only include a specific layer (such as the last convolution layer)?\n\n*Interpretability*\n\nSection 4.1. tries to interpret the approach but it is still not clear why matching distribution is better. The MMD loss proposed could run into problem if the classification task does not involve \"spatial variation\". For example, for a extremely simple task of classifying three classes \"R\", \"G\" and \"B\" where the whole image has the same color of R, G and B respectively, the spatial distribution is uniform and the proposed MMD loss would be 0 even if the student network's channels do not learn discriminative feature maps. Another example is when a layer has H=W=1.\n\n*Significance*\n\nThe experiment shows that polynomial-two kernel gives better result, but Sec. 4.3.2. mentions that it is equivalent to Li et al. (2017b) in this case.\n\n*Practical usefulness not justified*\n\nIn the experimental section, the student network's number of parameters and FLOPS are not detailed, so it is unclear how much efficiency gain the proposed method achieved. Note in practice small networks such as MobileNet and ShuffleNet have achieved significantly better accuracy-efficiency trade-off than the teacher networks considered here (either for CIFAR10 or for ImageNet1k).\n\n*Improvement not significant*\n\nThe results obtained by the proposed approach is not very significant compared to \"KD\" along.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper134/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Like What You Like: Knowledge Distill via Neuron Selectivity Transfer", "abstract": "Despite deep neural networks have demonstrated extraordinary power in various applications, their superior performances are at expense of high storage and computational costs. Consequently, the acceleration and compression of neural networks have attracted much attention recently. Knowledge Transfer (KT), which aims at training a smaller student network by transferring knowledge from a larger teacher model, is one of the popular solutions. In this paper, we propose a novel knowledge transfer method by treating it as a distribution matching problem. Particularly, we match the distributions of neuron selectivity patterns between teacher and student networks. To achieve this goal, we devise a new KT loss function by minimizing the Maximum Mean Discrepancy (MMD) metric between these distributions. Combined with the original loss function, our method can significantly improve the performance of student networks. We validate the effectiveness of our method across several datasets, and further combine it with other KT methods to explore the best possible results. Last but not least, we fine-tune the model to other tasks such as object detection. The results are also encouraging, which confirm the transferability of the learned features.", "keywords": ["Knowledge Distill"], "authorids": ["zehaohuang18@gmail.com", "winsty@gmail.com"], "authors": ["Zehao Huang", "Naiyan Wang"], "TL;DR": "We treat knowledge distill as a distribution matching problem and adopt Maximum Mean Discrepancy to minimize the distances between student features and teacher features.", "pdf": "/pdf/3f4fd0cca1cf1530bd1eb068eddb668ec8c2826e.pdf", "paperhash": "huang|like_what_you_like_knowledge_distill_via_neuron_selectivity_transfer", "_bibtex": "@misc{\nhuang2019like,\ntitle={Like What You Like: Knowledge Distill via Neuron Selectivity Transfer},\nauthor={Zehao Huang and Naiyan Wang},\nyear={2019},\nurl={https://openreview.net/forum?id=rJf0BjAqYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper134/Official_Review", "cdate": 1542234530303, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJf0BjAqYX", "replyto": "rJf0BjAqYX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper134/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335656768, "tmdate": 1552335656768, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper134/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rylOvP0FnX", "original": null, "number": 2, "cdate": 1541166944451, "ddate": null, "tcdate": 1541166944451, "tmdate": 1541534252225, "tddate": null, "forum": "rJf0BjAqYX", "replyto": "rJf0BjAqYX", "invitation": "ICLR.cc/2019/Conference/-/Paper134/Official_Review", "content": {"title": "Unclear presentation, results and novelty", "review": "This paper targets knowledge distillation of a large network to a smaller network. The approach is summarized by equations (3) and (4), which in short proposes that one should use the maximum-mean-discrepancy (MMD) of the network activations as a loss term for distillation.  \n\nWhen considering CIFAR image classification tasks, it is shown that only when using a specific quadratic polynomial kernel (which as described in https://arxiv.org/pdf/1701.01036.pdf is tantamount of applying neural style transfer) the proposed approach is able to match the performance of the seminal paper of Hinton et al.  When embarking to imagenet, the proposed approach is only able to match the performance of standard knowledge distillation by adding the quadratic term (texture in neural style synthesis jargon). This is actually a sensible proposal. Yet, the claims about MMD as a way of explaining neural style transfer has appeared in the paper cited above, which the authors mention.\n\nThe idea of transferring from one domain to another using MMD as a regularizer appeared in https://arxiv.org/pdf/1605.06636.pdf by Long et al --- indeed equation (3) of this paper matches exactly equation (10) of Long et al. Note too that Long et al also discuss what kernels work well and which work poorly due to vanishing gradients and propose parametrised solutions. This is something this paper failed to do.\n\nThe two works cited above make me wonder about the novelty of the current paper.  In fact, this paper ends us being an application of the neural style transfer loss function to network distillation. As such this could be useful, if not already done by someone else previously.\n\nI find that the paper is poorly written, with many typos, and lacks focus on a single concrete story. The CIFAR experiments fail to use KD+NST (ie the thing that works for imagenet - neural style transfer) and section 5.3 appears trivial in light of the cited works. For all these reasons, I am inclined to reject this paper.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper134/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Like What You Like: Knowledge Distill via Neuron Selectivity Transfer", "abstract": "Despite deep neural networks have demonstrated extraordinary power in various applications, their superior performances are at expense of high storage and computational costs. Consequently, the acceleration and compression of neural networks have attracted much attention recently. Knowledge Transfer (KT), which aims at training a smaller student network by transferring knowledge from a larger teacher model, is one of the popular solutions. In this paper, we propose a novel knowledge transfer method by treating it as a distribution matching problem. Particularly, we match the distributions of neuron selectivity patterns between teacher and student networks. To achieve this goal, we devise a new KT loss function by minimizing the Maximum Mean Discrepancy (MMD) metric between these distributions. Combined with the original loss function, our method can significantly improve the performance of student networks. We validate the effectiveness of our method across several datasets, and further combine it with other KT methods to explore the best possible results. Last but not least, we fine-tune the model to other tasks such as object detection. The results are also encouraging, which confirm the transferability of the learned features.", "keywords": ["Knowledge Distill"], "authorids": ["zehaohuang18@gmail.com", "winsty@gmail.com"], "authors": ["Zehao Huang", "Naiyan Wang"], "TL;DR": "We treat knowledge distill as a distribution matching problem and adopt Maximum Mean Discrepancy to minimize the distances between student features and teacher features.", "pdf": "/pdf/3f4fd0cca1cf1530bd1eb068eddb668ec8c2826e.pdf", "paperhash": "huang|like_what_you_like_knowledge_distill_via_neuron_selectivity_transfer", "_bibtex": "@misc{\nhuang2019like,\ntitle={Like What You Like: Knowledge Distill via Neuron Selectivity Transfer},\nauthor={Zehao Huang and Naiyan Wang},\nyear={2019},\nurl={https://openreview.net/forum?id=rJf0BjAqYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper134/Official_Review", "cdate": 1542234530303, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJf0BjAqYX", "replyto": "rJf0BjAqYX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper134/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335656768, "tmdate": 1552335656768, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper134/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BkgLnWiI2X", "original": null, "number": 1, "cdate": 1540956590312, "ddate": null, "tcdate": 1540956590312, "tmdate": 1541534252015, "tddate": null, "forum": "rJf0BjAqYX", "replyto": "rJf0BjAqYX", "invitation": "ICLR.cc/2019/Conference/-/Paper134/Official_Review", "content": {"title": "Simple method with good empirical results and some insightful discussion", "review": "This paper proposes a simple method for knowledge distillation. The teacher and student models are matched using MMD objectives, the author demonstrates different variants of matching kernels specializes to previously proposed variants of knowledge distillation.\n\n- The extensive evaluation suggests that the MMD with polynomial kernel provides better results than the previously proposed method.\n-  It is interesting to see that MMD based transfer has more advantage on the object detection tasks.\n- Can the author provides more insights into the behavior of different kernels, for example visualizing, the gradient map might help us to understand why certain kernel works better than another one?\n- Did you consider translation invariance or other spatial properties when designing your kernels?\n\nIn summary, this is an interesting paper with good empirical results. The technique being used generalization is quite straightforward, but the paper also includes a good amount of discussion on why the proposed approach could be better and I think that really helps the reader.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper134/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Like What You Like: Knowledge Distill via Neuron Selectivity Transfer", "abstract": "Despite deep neural networks have demonstrated extraordinary power in various applications, their superior performances are at expense of high storage and computational costs. Consequently, the acceleration and compression of neural networks have attracted much attention recently. Knowledge Transfer (KT), which aims at training a smaller student network by transferring knowledge from a larger teacher model, is one of the popular solutions. In this paper, we propose a novel knowledge transfer method by treating it as a distribution matching problem. Particularly, we match the distributions of neuron selectivity patterns between teacher and student networks. To achieve this goal, we devise a new KT loss function by minimizing the Maximum Mean Discrepancy (MMD) metric between these distributions. Combined with the original loss function, our method can significantly improve the performance of student networks. We validate the effectiveness of our method across several datasets, and further combine it with other KT methods to explore the best possible results. Last but not least, we fine-tune the model to other tasks such as object detection. The results are also encouraging, which confirm the transferability of the learned features.", "keywords": ["Knowledge Distill"], "authorids": ["zehaohuang18@gmail.com", "winsty@gmail.com"], "authors": ["Zehao Huang", "Naiyan Wang"], "TL;DR": "We treat knowledge distill as a distribution matching problem and adopt Maximum Mean Discrepancy to minimize the distances between student features and teacher features.", "pdf": "/pdf/3f4fd0cca1cf1530bd1eb068eddb668ec8c2826e.pdf", "paperhash": "huang|like_what_you_like_knowledge_distill_via_neuron_selectivity_transfer", "_bibtex": "@misc{\nhuang2019like,\ntitle={Like What You Like: Knowledge Distill via Neuron Selectivity Transfer},\nauthor={Zehao Huang and Naiyan Wang},\nyear={2019},\nurl={https://openreview.net/forum?id=rJf0BjAqYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper134/Official_Review", "cdate": 1542234530303, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJf0BjAqYX", "replyto": "rJf0BjAqYX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper134/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335656768, "tmdate": 1552335656768, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper134/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SJx8fwqrim", "original": null, "number": 1, "cdate": 1539839757521, "ddate": null, "tcdate": 1539839757521, "tmdate": 1539839757521, "tddate": null, "forum": "rJf0BjAqYX", "replyto": "HJg7dddrom", "invitation": "ICLR.cc/2019/Conference/-/Paper134/Official_Comment", "content": {"title": "ResNet-1001", "comment": "Hi, the ResNet-1001 was first proposed by kaiming he in [1].  Please refer [1] for more details. Thanks!\n\n[1] He, Kaiming, et al. \"Identity mappings in deep residual networks.\" European conference on computer vision. Springer, Cham, 2016."}, "signatures": ["ICLR.cc/2019/Conference/Paper134/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper134/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper134/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Like What You Like: Knowledge Distill via Neuron Selectivity Transfer", "abstract": "Despite deep neural networks have demonstrated extraordinary power in various applications, their superior performances are at expense of high storage and computational costs. Consequently, the acceleration and compression of neural networks have attracted much attention recently. Knowledge Transfer (KT), which aims at training a smaller student network by transferring knowledge from a larger teacher model, is one of the popular solutions. In this paper, we propose a novel knowledge transfer method by treating it as a distribution matching problem. Particularly, we match the distributions of neuron selectivity patterns between teacher and student networks. To achieve this goal, we devise a new KT loss function by minimizing the Maximum Mean Discrepancy (MMD) metric between these distributions. Combined with the original loss function, our method can significantly improve the performance of student networks. We validate the effectiveness of our method across several datasets, and further combine it with other KT methods to explore the best possible results. Last but not least, we fine-tune the model to other tasks such as object detection. The results are also encouraging, which confirm the transferability of the learned features.", "keywords": ["Knowledge Distill"], "authorids": ["zehaohuang18@gmail.com", "winsty@gmail.com"], "authors": ["Zehao Huang", "Naiyan Wang"], "TL;DR": "We treat knowledge distill as a distribution matching problem and adopt Maximum Mean Discrepancy to minimize the distances between student features and teacher features.", "pdf": "/pdf/3f4fd0cca1cf1530bd1eb068eddb668ec8c2826e.pdf", "paperhash": "huang|like_what_you_like_knowledge_distill_via_neuron_selectivity_transfer", "_bibtex": "@misc{\nhuang2019like,\ntitle={Like What You Like: Knowledge Distill via Neuron Selectivity Transfer},\nauthor={Zehao Huang and Naiyan Wang},\nyear={2019},\nurl={https://openreview.net/forum?id=rJf0BjAqYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper134/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622214, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJf0BjAqYX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper134/Authors", "ICLR.cc/2019/Conference/Paper134/Reviewers", "ICLR.cc/2019/Conference/Paper134/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper134/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper134/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper134/Authors|ICLR.cc/2019/Conference/Paper134/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper134/Reviewers", "ICLR.cc/2019/Conference/Paper134/Authors", "ICLR.cc/2019/Conference/Paper134/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622214}}}, {"id": "HJg7dddrom", "original": null, "number": 1, "cdate": 1539831915264, "ddate": null, "tcdate": 1539831915264, "tmdate": 1539831915264, "tddate": null, "forum": "rJf0BjAqYX", "replyto": "rJf0BjAqYX", "invitation": "ICLR.cc/2019/Conference/-/Paper134/Public_Comment", "content": {"comment": "Hi, I noticed that you used ResNet-1001 as the teacher model. It's astounding, considering its massive parameters. Could you please disclose more details on how you trained this extremely deep network? ", "title": "Teacher model in the CIFAR experiment"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper134/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Like What You Like: Knowledge Distill via Neuron Selectivity Transfer", "abstract": "Despite deep neural networks have demonstrated extraordinary power in various applications, their superior performances are at expense of high storage and computational costs. Consequently, the acceleration and compression of neural networks have attracted much attention recently. Knowledge Transfer (KT), which aims at training a smaller student network by transferring knowledge from a larger teacher model, is one of the popular solutions. In this paper, we propose a novel knowledge transfer method by treating it as a distribution matching problem. Particularly, we match the distributions of neuron selectivity patterns between teacher and student networks. To achieve this goal, we devise a new KT loss function by minimizing the Maximum Mean Discrepancy (MMD) metric between these distributions. Combined with the original loss function, our method can significantly improve the performance of student networks. We validate the effectiveness of our method across several datasets, and further combine it with other KT methods to explore the best possible results. Last but not least, we fine-tune the model to other tasks such as object detection. The results are also encouraging, which confirm the transferability of the learned features.", "keywords": ["Knowledge Distill"], "authorids": ["zehaohuang18@gmail.com", "winsty@gmail.com"], "authors": ["Zehao Huang", "Naiyan Wang"], "TL;DR": "We treat knowledge distill as a distribution matching problem and adopt Maximum Mean Discrepancy to minimize the distances between student features and teacher features.", "pdf": "/pdf/3f4fd0cca1cf1530bd1eb068eddb668ec8c2826e.pdf", "paperhash": "huang|like_what_you_like_knowledge_distill_via_neuron_selectivity_transfer", "_bibtex": "@misc{\nhuang2019like,\ntitle={Like What You Like: Knowledge Distill via Neuron Selectivity Transfer},\nauthor={Zehao Huang and Naiyan Wang},\nyear={2019},\nurl={https://openreview.net/forum?id=rJf0BjAqYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper134/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311910788, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "rJf0BjAqYX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper134/Authors", "ICLR.cc/2019/Conference/Paper134/Reviewers", "ICLR.cc/2019/Conference/Paper134/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper134/Authors", "ICLR.cc/2019/Conference/Paper134/Reviewers", "ICLR.cc/2019/Conference/Paper134/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311910788}}}], "count": 7}