{"notes": [{"id": "_qJXkf347k", "original": "7LgLMxopN9", "number": 1892, "cdate": 1601308208544, "ddate": null, "tcdate": 1601308208544, "tmdate": 1614985636226, "tddate": null, "forum": "_qJXkf347k", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Reinforcement Learning Based Asymmetrical DNN Modularization for Optimal Loading", "authorids": ["~Brijraj_Singh1", "ya.jain@samsung.com", "~Mayukh_Das1", "praveen.dn@samsung.com"], "authors": ["Brijraj Singh", "Yash Jain", "Mayukh Das", "Praveen Doreswamy Naidu"], "keywords": ["DNN Compression", "Loading time"], "abstract": "Latency of DNN (Deep Neural Network) based prediction is the summation of model loading latency and inference latency. Model loading latency affects the first response from the applications, whereas inference latency affects the subsequent responses. As model loading latency is directly proportional to the model size, this work aims at improving the response time of an intelligent app by reducing the loading latency. The speedup is gained by asymmetrically modularizing the given DNN model among several small child models and loading them in parallel. The decision about number of feasible child models and their corresponding split positions are taken care by reinforcement learning unit (RLU). RLU takes into account the available hardware resources on-device and provides the best splitting index $k$ and their positions  $\\vec{p}$ specific to the DNN model and device, where  $\\vec{p}=(p_1, p_2, ..., p_k)$ and $p_i$  is the end position of $i^{th}$  child: $M_i$. The proposed method has shown significant loading improvement (up to 7X) on popular DNNs, used for camera use-case. The proposed method can be used to speed up the app response. Along with that RLU driven approach facilitates for On-device personalization by separating one module only with trainable layers and loading that particular module while training on-device. ", "one-sentence_summary": "This work proposes an application of reinforcement learning for reducing the DNN loading time.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "singh|reinforcement_learning_based_asymmetrical_dnn_modularization_for_optimal_loading", "pdf": "/pdf/a3d1f74cf7173c1668e3ed0f01271621bdd10bc7.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=x1Funve6K", "_bibtex": "@misc{\nsingh2021reinforcement,\ntitle={Reinforcement Learning Based Asymmetrical {\\{}DNN{\\}} Modularization for Optimal Loading},\nauthor={Brijraj Singh and Yash Jain and Mayukh Das and Praveen Doreswamy Naidu},\nyear={2021},\nurl={https://openreview.net/forum?id=_qJXkf347k}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "QNTtA61hWH", "original": null, "number": 1, "cdate": 1610040525157, "ddate": null, "tcdate": 1610040525157, "tmdate": 1610474134268, "tddate": null, "forum": "_qJXkf347k", "replyto": "_qJXkf347k", "invitation": "ICLR.cc/2021/Conference/Paper1892/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper proposed a novel RL-based solution to the optimal partial of DNNs which is interesting to readers. \nHowever, the paper is not well presented and hard to follow. The lack of comparisons agains existing solutions and inconsistencies in the writing as pointed out by the reviewers largely weakens the submission. \nThere's also no updates to the paper based on reviewers' comments. \n\nThe main reason for the decision is lack of clarity and significance justifications."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reinforcement Learning Based Asymmetrical DNN Modularization for Optimal Loading", "authorids": ["~Brijraj_Singh1", "ya.jain@samsung.com", "~Mayukh_Das1", "praveen.dn@samsung.com"], "authors": ["Brijraj Singh", "Yash Jain", "Mayukh Das", "Praveen Doreswamy Naidu"], "keywords": ["DNN Compression", "Loading time"], "abstract": "Latency of DNN (Deep Neural Network) based prediction is the summation of model loading latency and inference latency. Model loading latency affects the first response from the applications, whereas inference latency affects the subsequent responses. As model loading latency is directly proportional to the model size, this work aims at improving the response time of an intelligent app by reducing the loading latency. The speedup is gained by asymmetrically modularizing the given DNN model among several small child models and loading them in parallel. The decision about number of feasible child models and their corresponding split positions are taken care by reinforcement learning unit (RLU). RLU takes into account the available hardware resources on-device and provides the best splitting index $k$ and their positions  $\\vec{p}$ specific to the DNN model and device, where  $\\vec{p}=(p_1, p_2, ..., p_k)$ and $p_i$  is the end position of $i^{th}$  child: $M_i$. The proposed method has shown significant loading improvement (up to 7X) on popular DNNs, used for camera use-case. The proposed method can be used to speed up the app response. Along with that RLU driven approach facilitates for On-device personalization by separating one module only with trainable layers and loading that particular module while training on-device. ", "one-sentence_summary": "This work proposes an application of reinforcement learning for reducing the DNN loading time.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "singh|reinforcement_learning_based_asymmetrical_dnn_modularization_for_optimal_loading", "pdf": "/pdf/a3d1f74cf7173c1668e3ed0f01271621bdd10bc7.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=x1Funve6K", "_bibtex": "@misc{\nsingh2021reinforcement,\ntitle={Reinforcement Learning Based Asymmetrical {\\{}DNN{\\}} Modularization for Optimal Loading},\nauthor={Brijraj Singh and Yash Jain and Mayukh Das and Praveen Doreswamy Naidu},\nyear={2021},\nurl={https://openreview.net/forum?id=_qJXkf347k}\n}"}, "tags": [], "invitation": {"reply": {"forum": "_qJXkf347k", "replyto": "_qJXkf347k", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040525144, "tmdate": 1610474134251, "id": "ICLR.cc/2021/Conference/Paper1892/-/Decision"}}}, {"id": "WfW_SLujjjF", "original": null, "number": 1, "cdate": 1603853982782, "ddate": null, "tcdate": 1603853982782, "tmdate": 1605024334302, "tddate": null, "forum": "_qJXkf347k", "replyto": "_qJXkf347k", "invitation": "ICLR.cc/2021/Conference/Paper1892/-/Official_Review", "content": {"title": "Hard to follow and agree with", "review": "The authors present a method to optimise the _loading time_ of a deep neural network on e.g. a mobile device. With ever more complicated and bigger networks (notwithstanding approaches to prune networks etc.), loading parameters can cause a noticeable initial delay before the first inference step. The authors propose an RL-based approach to optimise loading parallelism by splitting the model into smaller submodels and show that this speeds up loading time.\n\nI've got several concerns with this submission:\n- The paper was very difficult to read and follow in general, and I'm still not 100% certain my current interpretation is correct. While the language itself could use some polish, the notation is also pretty inconsistent (e.g. $S$ as state space or specific state) and the figures are hard to read. A few terms are not clearly explained: what is a block exactly? A contiguous set of weights in memory? A number of network layers? There's also mention of nodes in Section 4.2.1, are those the same? Furthermore some of the equations also seem to contain mistakes (e.g. $Block_M$ in Eq.2 should probably be $M_k$, final term in Eq. 5 should still be in the brackets).\n- The problem statement does not feel like the right one to me. The problem the authors try to solve is loading delay, however the problem they are actually solving is perfect loading parallelism. Is it not more important to reduce overall delay than to make sure that each submodel can get loaded in exactly the same time? Even if the former would eventually result in the latter (which would be a nice result!), why not optimise total delay directly? Moreover, the authors allow the number of submodels to exceed the number of hardware threads, keeping the rest of them in a load buffer, which can potentially even add more delay compared to pre-specifying the number of submodels to the number of hardware threads. Also more important than the number of hardware threads, I suspect, is the number of simultaneous streams from the secondary storage medium you can effectively use due to I/O contention, which I imagine is usually lower. This would potentially also explain why in Table 1 there's only a 2.5x speed-up, where one would expect 4x with 4 submodels.\n- The authors compare to an existing (\"state-of-the-art\") method but don't explain what this is. How does the proposed method compare to naively loading N chunks of M/N weights, with N being the number of hardware threads (or available streams from the storage medium) and M the total number of weights?\n- The authors propose an RL-based approach (table-based Q-learning) to solve the problem, though provide no real motivation why such a comparatively complicated method is a good idea in this case. While RL is very flexible, it is not always the right solution.\n- Though I'm still not certain, it seems like the authors use a somewhat unusual formalism in between a bandit and a full MDP, where the \"action sequence\" (the number and indices of the splits) is selected at the start of the episode, and then the state incrementally updated. This would explain the reward definition, which would only make sense if the number of splits $C_{split}$ would be fixed at the start of the unroll. as well as the appearance of what I interpret as the state variable $s$ in Algorithm 1. Is this interpretation correct? If so, this choice should definitely be elaborated upon in the text, as it is pretty unusual. I would also think that in this case a pure bandit-style formulation might make more sense.\n- Why the choice for a sparse reward function with an $\\epsilon$-margin instead of a dense reward, which are usually easier to learn?\n- What is meant with \"Vendor's site\" and \"Developer's site\" in Figure 2? Are these separate geographic locations?\n- While the results span different DNN models, it's unclear how the RLU controller was actually trained besides with a general Q-learning approach. How many iterations? Separate Q-tables per DNN model? What hyper parameters? Perhaps more worryingly, Table 3 contains 5 \"observations\". Are these simply 5 different timing measurements of the same model? Does this imply that for Table 1 only a single timing measurement was taken?\n\nOverall I find this submission too hard to follow and agree with to vote for it to be accepted.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1892/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1892/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reinforcement Learning Based Asymmetrical DNN Modularization for Optimal Loading", "authorids": ["~Brijraj_Singh1", "ya.jain@samsung.com", "~Mayukh_Das1", "praveen.dn@samsung.com"], "authors": ["Brijraj Singh", "Yash Jain", "Mayukh Das", "Praveen Doreswamy Naidu"], "keywords": ["DNN Compression", "Loading time"], "abstract": "Latency of DNN (Deep Neural Network) based prediction is the summation of model loading latency and inference latency. Model loading latency affects the first response from the applications, whereas inference latency affects the subsequent responses. As model loading latency is directly proportional to the model size, this work aims at improving the response time of an intelligent app by reducing the loading latency. The speedup is gained by asymmetrically modularizing the given DNN model among several small child models and loading them in parallel. The decision about number of feasible child models and their corresponding split positions are taken care by reinforcement learning unit (RLU). RLU takes into account the available hardware resources on-device and provides the best splitting index $k$ and their positions  $\\vec{p}$ specific to the DNN model and device, where  $\\vec{p}=(p_1, p_2, ..., p_k)$ and $p_i$  is the end position of $i^{th}$  child: $M_i$. The proposed method has shown significant loading improvement (up to 7X) on popular DNNs, used for camera use-case. The proposed method can be used to speed up the app response. Along with that RLU driven approach facilitates for On-device personalization by separating one module only with trainable layers and loading that particular module while training on-device. ", "one-sentence_summary": "This work proposes an application of reinforcement learning for reducing the DNN loading time.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "singh|reinforcement_learning_based_asymmetrical_dnn_modularization_for_optimal_loading", "pdf": "/pdf/a3d1f74cf7173c1668e3ed0f01271621bdd10bc7.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=x1Funve6K", "_bibtex": "@misc{\nsingh2021reinforcement,\ntitle={Reinforcement Learning Based Asymmetrical {\\{}DNN{\\}} Modularization for Optimal Loading},\nauthor={Brijraj Singh and Yash Jain and Mayukh Das and Praveen Doreswamy Naidu},\nyear={2021},\nurl={https://openreview.net/forum?id=_qJXkf347k}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "_qJXkf347k", "replyto": "_qJXkf347k", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1892/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538108457, "tmdate": 1606915807156, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1892/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1892/-/Official_Review"}}}, {"id": "PkUSS0qBop5", "original": null, "number": 3, "cdate": 1603874254545, "ddate": null, "tcdate": 1603874254545, "tmdate": 1605024334240, "tddate": null, "forum": "_qJXkf347k", "replyto": "_qJXkf347k", "invitation": "ICLR.cc/2021/Conference/Paper1892/-/Official_Review", "content": {"title": "Authors propose a reinforcement learning approach for splitting a DNN for faster initialization, but evidence of impact is lacking, and the presentation style should be improved", "review": "The authors propose a reinforcement learning approach for splitting a DNN for faster initialization. More specifically, the inference time can be split to two parts: initializing the prediction model (loading from disk, uploading to memory, initializing for inference) and the actual inference time. This paper focuses on reducing the time of the former operation.\n\nThe strength of the manuscript is that reducing the loading time has not been studied earlier. As the authors claim, most research on computational load of modern machine learning models is focused on reducing the inference time, and the loading time is dismissed.\n\nHowever, the authors do not justify thoroughly why the loading time should be a concern. The startup time of the executable depends on many factors, not only the initialization of the neural network. Moreover, the loading time is a one-time event, so I fail to see the impact of whether the DNN is initialized in 0.74 seconds (proposed) versus 2.18 seconds (conventional). There might be use cases where this difference is indeed significant, but they are not presented in the paper.\n\nThe literary style of the paper is not suitable for a scientific publication. There are many errors in spelling and the clarity of presentation is poor. As a detailed note, there are no paragraph breaks, which makes the reading hard.", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1892/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1892/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reinforcement Learning Based Asymmetrical DNN Modularization for Optimal Loading", "authorids": ["~Brijraj_Singh1", "ya.jain@samsung.com", "~Mayukh_Das1", "praveen.dn@samsung.com"], "authors": ["Brijraj Singh", "Yash Jain", "Mayukh Das", "Praveen Doreswamy Naidu"], "keywords": ["DNN Compression", "Loading time"], "abstract": "Latency of DNN (Deep Neural Network) based prediction is the summation of model loading latency and inference latency. Model loading latency affects the first response from the applications, whereas inference latency affects the subsequent responses. As model loading latency is directly proportional to the model size, this work aims at improving the response time of an intelligent app by reducing the loading latency. The speedup is gained by asymmetrically modularizing the given DNN model among several small child models and loading them in parallel. The decision about number of feasible child models and their corresponding split positions are taken care by reinforcement learning unit (RLU). RLU takes into account the available hardware resources on-device and provides the best splitting index $k$ and their positions  $\\vec{p}$ specific to the DNN model and device, where  $\\vec{p}=(p_1, p_2, ..., p_k)$ and $p_i$  is the end position of $i^{th}$  child: $M_i$. The proposed method has shown significant loading improvement (up to 7X) on popular DNNs, used for camera use-case. The proposed method can be used to speed up the app response. Along with that RLU driven approach facilitates for On-device personalization by separating one module only with trainable layers and loading that particular module while training on-device. ", "one-sentence_summary": "This work proposes an application of reinforcement learning for reducing the DNN loading time.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "singh|reinforcement_learning_based_asymmetrical_dnn_modularization_for_optimal_loading", "pdf": "/pdf/a3d1f74cf7173c1668e3ed0f01271621bdd10bc7.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=x1Funve6K", "_bibtex": "@misc{\nsingh2021reinforcement,\ntitle={Reinforcement Learning Based Asymmetrical {\\{}DNN{\\}} Modularization for Optimal Loading},\nauthor={Brijraj Singh and Yash Jain and Mayukh Das and Praveen Doreswamy Naidu},\nyear={2021},\nurl={https://openreview.net/forum?id=_qJXkf347k}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "_qJXkf347k", "replyto": "_qJXkf347k", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1892/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538108457, "tmdate": 1606915807156, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1892/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1892/-/Official_Review"}}}, {"id": "vAu7n2-MXiN", "original": null, "number": 4, "cdate": 1603909899596, "ddate": null, "tcdate": 1603909899596, "tmdate": 1605024334169, "tddate": null, "forum": "_qJXkf347k", "replyto": "_qJXkf347k", "invitation": "ICLR.cc/2021/Conference/Paper1892/-/Official_Review", "content": {"title": "Review opinion", "review": "This paper aims at optimizing the the loading latency of DNN models by re-partition the model into modules that can be loaded in parallel. The paper adopts reinforcement learning to find the optimal partition. The experiments show that the proposed method achieved significant improvement on several popular models.\n\nStrength of the paper: the problem this paper aims to solve is very meaningful and not addressed by previous work, as far as I know.  \n\nWeaknesses of the paper: \n1/ This paper is not written clearly. For example, in the abstract and the beginning of the intro, the paper aims at optimizing the model loading latency, but in the later part of the intro, it also aims at optimizing on-device training? It is not clear how these two problems are closely related. \n2/ The problem formulation and methods are not expressed in clear and rigorous mathematical terms. For example, what's the meaning of equation(1) and (2)? In equation (3), what is a \"split\"? what is the C_split and I_split? There are verbal explanation to this, but the definition is too vague to understand. The author might want to consider representing a model as a directed-acyclic graphs (DAGs), and use the graph terminologies to describe the problem and methods.\n3/ It is not clear to me what is the motivation of using RL to solve the model partition problem. Since the model is a static graph, the optimization problem can be solved by searching for an optimal partition that minizes the overall latency? The paper decomposes this process into steps, and formulate the search process as a markov decision process and use RL to search for the solution. However, it is not clear if this is necessary, or optimal. \n\nOverall, I think this paper is not well written and should be further polished. ", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1892/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1892/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reinforcement Learning Based Asymmetrical DNN Modularization for Optimal Loading", "authorids": ["~Brijraj_Singh1", "ya.jain@samsung.com", "~Mayukh_Das1", "praveen.dn@samsung.com"], "authors": ["Brijraj Singh", "Yash Jain", "Mayukh Das", "Praveen Doreswamy Naidu"], "keywords": ["DNN Compression", "Loading time"], "abstract": "Latency of DNN (Deep Neural Network) based prediction is the summation of model loading latency and inference latency. Model loading latency affects the first response from the applications, whereas inference latency affects the subsequent responses. As model loading latency is directly proportional to the model size, this work aims at improving the response time of an intelligent app by reducing the loading latency. The speedup is gained by asymmetrically modularizing the given DNN model among several small child models and loading them in parallel. The decision about number of feasible child models and their corresponding split positions are taken care by reinforcement learning unit (RLU). RLU takes into account the available hardware resources on-device and provides the best splitting index $k$ and their positions  $\\vec{p}$ specific to the DNN model and device, where  $\\vec{p}=(p_1, p_2, ..., p_k)$ and $p_i$  is the end position of $i^{th}$  child: $M_i$. The proposed method has shown significant loading improvement (up to 7X) on popular DNNs, used for camera use-case. The proposed method can be used to speed up the app response. Along with that RLU driven approach facilitates for On-device personalization by separating one module only with trainable layers and loading that particular module while training on-device. ", "one-sentence_summary": "This work proposes an application of reinforcement learning for reducing the DNN loading time.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "singh|reinforcement_learning_based_asymmetrical_dnn_modularization_for_optimal_loading", "pdf": "/pdf/a3d1f74cf7173c1668e3ed0f01271621bdd10bc7.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=x1Funve6K", "_bibtex": "@misc{\nsingh2021reinforcement,\ntitle={Reinforcement Learning Based Asymmetrical {\\{}DNN{\\}} Modularization for Optimal Loading},\nauthor={Brijraj Singh and Yash Jain and Mayukh Das and Praveen Doreswamy Naidu},\nyear={2021},\nurl={https://openreview.net/forum?id=_qJXkf347k}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "_qJXkf347k", "replyto": "_qJXkf347k", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1892/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538108457, "tmdate": 1606915807156, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1892/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1892/-/Official_Review"}}}, {"id": "Ek8bo1jrzQG", "original": null, "number": 2, "cdate": 1603859609177, "ddate": null, "tcdate": 1603859609177, "tmdate": 1605024334113, "tddate": null, "forum": "_qJXkf347k", "replyto": "_qJXkf347k", "invitation": "ICLR.cc/2021/Conference/Paper1892/-/Official_Review", "content": {"title": "Reviews", "review": "\nSummary:\n\n  This paper tries to resolve the model loading issue for edge computing, which usually has limited memory such that it needs to flush out the loaded models for other applications.\n\n\nStrong points:\n\n  1. This paper points out another issue when deploying DNN models that usually be ignored. Data loading issue.\n\n  2. This paper also deployed their method on mobile platform.\n\nWeak points:\n  1. The experimental setting is unclear and only few comparisons are available. It is hard to judge the advantage of the proposed algorithm.\n  \n  2. The novelty of the proposed algorithm is limited but it might be effective in practice.\n\n\nRecommendation:\n  Without having clear comparison, I can not evaluate the proposed algorithm.\n\nQuestions:\n\n1. Method:\n\n  1.1 Does inference time matters in the proposed algorithm 1? I do not see how the inference time affects the algorithm.\n\n  1.2 How to train the model with RL specifically? In algorithm 1, does the M are a collection of models or it is the same model? My understanding is the same model and then the author would find the best split method.\n\n  1.3 Why the number of \"available\" threads is not considered as a factor in deciding how to split? In the real applications, the number of \"available\" threads is not fixed, assuming to have access to all threads seems to be too ideal.\n\n2. Experiments:\n\n  2.1 it is very unclear what is the baseline (state of the art authors mentioned) authors compared, e.g. in table 1, what is the \"Original\" means? Does it mean load the whole model in one step? and what is \"Actual\" in Table 3?\n\n  2.2 Why authors do not compare to the heuristic baseline, e.g. split the model based on the number of parameters or number of layers? This should be stronger but straightforward baselines to compare.\n\n3. Others:\n\n  3.1. the notation is unclear, e.g. whar is m in at the line after eq 2?\n\n  3.2. What is s in Algorithm 1? why line 9 is not i == k?\n\n  3.3. What is In in algorithm 2? the index i is abused, line 13 and 15 both use i.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1892/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1892/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reinforcement Learning Based Asymmetrical DNN Modularization for Optimal Loading", "authorids": ["~Brijraj_Singh1", "ya.jain@samsung.com", "~Mayukh_Das1", "praveen.dn@samsung.com"], "authors": ["Brijraj Singh", "Yash Jain", "Mayukh Das", "Praveen Doreswamy Naidu"], "keywords": ["DNN Compression", "Loading time"], "abstract": "Latency of DNN (Deep Neural Network) based prediction is the summation of model loading latency and inference latency. Model loading latency affects the first response from the applications, whereas inference latency affects the subsequent responses. As model loading latency is directly proportional to the model size, this work aims at improving the response time of an intelligent app by reducing the loading latency. The speedup is gained by asymmetrically modularizing the given DNN model among several small child models and loading them in parallel. The decision about number of feasible child models and their corresponding split positions are taken care by reinforcement learning unit (RLU). RLU takes into account the available hardware resources on-device and provides the best splitting index $k$ and their positions  $\\vec{p}$ specific to the DNN model and device, where  $\\vec{p}=(p_1, p_2, ..., p_k)$ and $p_i$  is the end position of $i^{th}$  child: $M_i$. The proposed method has shown significant loading improvement (up to 7X) on popular DNNs, used for camera use-case. The proposed method can be used to speed up the app response. Along with that RLU driven approach facilitates for On-device personalization by separating one module only with trainable layers and loading that particular module while training on-device. ", "one-sentence_summary": "This work proposes an application of reinforcement learning for reducing the DNN loading time.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "singh|reinforcement_learning_based_asymmetrical_dnn_modularization_for_optimal_loading", "pdf": "/pdf/a3d1f74cf7173c1668e3ed0f01271621bdd10bc7.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=x1Funve6K", "_bibtex": "@misc{\nsingh2021reinforcement,\ntitle={Reinforcement Learning Based Asymmetrical {\\{}DNN{\\}} Modularization for Optimal Loading},\nauthor={Brijraj Singh and Yash Jain and Mayukh Das and Praveen Doreswamy Naidu},\nyear={2021},\nurl={https://openreview.net/forum?id=_qJXkf347k}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "_qJXkf347k", "replyto": "_qJXkf347k", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1892/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538108457, "tmdate": 1606915807156, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1892/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1892/-/Official_Review"}}}], "count": 6}