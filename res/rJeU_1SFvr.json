{"notes": [{"id": "gsT7X4hKdV", "original": null, "number": 7, "cdate": 1578656695387, "ddate": null, "tcdate": 1578656695387, "tmdate": 1578656695387, "tddate": null, "forum": "rJeU_1SFvr", "replyto": "2loZWAILWk", "invitation": "ICLR.cc/2020/Conference/Paper1804/-/Official_Comment", "content": {"title": "reply to questions", "comment": "Dear Michal,\n\nThank you for your questions.\n\nActually the data-dependent term has already been dropped from eq. 2. This is explained in the text above eq. 2: \"We focus on the second term in eq. 1 which is at the heart of the min-max game.\" Note that the data-dependent term does not participate in SGA (thus does not affect the following analysis), since its derivative with respect to generator parameters is 0.\n\nSeeing \\Delta z as the third player provides an intuitive interpretation of the algorithm. Although the first and last players share the same loss, they are not identical: the parameters of the third player are *only* \\Delta z. So the derivation following eq.13 simply treats \\Delta z as equal of \\theta_D and \\theta_G."}, "signatures": ["ICLR.cc/2020/Conference/Paper1804/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1804/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yanwu@google.com", "jeffdonahue@google.com", "dbalduzzi@google.com", "simonyan@google.com", "countzero@google.com"], "title": "LOGAN:  Latent Optimisation for Generative Adversarial Networks", "authors": ["Yan Wu", "Jeff Donahue", "David Balduzzi", "Karen Simonyan", "Timothy Lillicrap"], "pdf": "/pdf/067003bb3a14491d779c8e6211d86588718072d7.pdf", "TL;DR": "Latent optimisation improves adversarial training dynamics. We present both theoretical analysis and state-of-the-art image generation with ImageNet 128x128.", "abstract": "Training generative adversarial networks requires balancing of delicate adversarial dynamics. Even with careful tuning, training may diverge or end up in a bad equilibrium with dropped modes. In this work, we introduce a new form of latent optimisation inspired by the CS-GAN and show that it improves adversarial dynamics by enhancing interactions between the discriminator and the generator. We develop supporting theoretical analysis from the perspectives of differentiable games and stochastic approximation. Our experiments demonstrate that latent optimisation can significantly improve GAN training, obtaining state-of-the-art performance for the ImageNet (128 x 128) dataset. Our model achieves an Inception Score (IS) of 148 and an Frechet Inception Distance (FID) of 3.4, an improvement of 17% and 32% in IS and FID respectively, compared with the baseline BigGAN-deep model with the same architecture and number of parameters.", "keywords": ["GAN", "adversarial training", "generative model", "game theory"], "paperhash": "wu|logan_latent_optimisation_for_generative_adversarial_networks", "original_pdf": "/attachment/8dc1e9af07d9412cee04683b297adfc3c0fd78ae.pdf", "_bibtex": "@misc{\nwu2020logan,\ntitle={{\\{}LOGAN{\\}}:  Latent Optimisation for Generative Adversarial Networks},\nauthor={Yan Wu and Jeff Donahue and David Balduzzi and Karen Simonyan and Timothy Lillicrap},\nyear={2020},\nurl={https://openreview.net/forum?id=rJeU_1SFvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJeU_1SFvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1804/Authors", "ICLR.cc/2020/Conference/Paper1804/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1804/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1804/Reviewers", "ICLR.cc/2020/Conference/Paper1804/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1804/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1804/Authors|ICLR.cc/2020/Conference/Paper1804/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504150635, "tmdate": 1576860534672, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1804/Authors", "ICLR.cc/2020/Conference/Paper1804/Reviewers", "ICLR.cc/2020/Conference/Paper1804/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1804/-/Official_Comment"}}}, {"id": "2loZWAILWk", "original": null, "number": 1, "cdate": 1578575249006, "ddate": null, "tcdate": 1578575249006, "tmdate": 1578575249006, "tddate": null, "forum": "rJeU_1SFvr", "replyto": "rJeU_1SFvr", "invitation": "ICLR.cc/2020/Conference/Paper1804/-/Public_Comment", "content": {"title": "Question about derivation of SGA dynamics", "comment": "The dynamics g from eq. (3) seem wrong. The loss of the discriminator also depend on the training data, but this term is dropped without justification. All of the subsequent derivation then cannot be correct.\n\nIn appendix B.1 I don't understand how you can justify the computation of latent step $\\Delta z$ as being the third player. The first and last player in eq. (13) come from the same loss function, so their weights are identical, as in the latent optimization. SGA update will however shift those weights differently, so the weights for the first and third player will not be identical anymore.\n\nThank you for clarification!"}, "signatures": ["~Michal_Sustr1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Michal_Sustr1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yanwu@google.com", "jeffdonahue@google.com", "dbalduzzi@google.com", "simonyan@google.com", "countzero@google.com"], "title": "LOGAN:  Latent Optimisation for Generative Adversarial Networks", "authors": ["Yan Wu", "Jeff Donahue", "David Balduzzi", "Karen Simonyan", "Timothy Lillicrap"], "pdf": "/pdf/067003bb3a14491d779c8e6211d86588718072d7.pdf", "TL;DR": "Latent optimisation improves adversarial training dynamics. We present both theoretical analysis and state-of-the-art image generation with ImageNet 128x128.", "abstract": "Training generative adversarial networks requires balancing of delicate adversarial dynamics. Even with careful tuning, training may diverge or end up in a bad equilibrium with dropped modes. In this work, we introduce a new form of latent optimisation inspired by the CS-GAN and show that it improves adversarial dynamics by enhancing interactions between the discriminator and the generator. We develop supporting theoretical analysis from the perspectives of differentiable games and stochastic approximation. Our experiments demonstrate that latent optimisation can significantly improve GAN training, obtaining state-of-the-art performance for the ImageNet (128 x 128) dataset. Our model achieves an Inception Score (IS) of 148 and an Frechet Inception Distance (FID) of 3.4, an improvement of 17% and 32% in IS and FID respectively, compared with the baseline BigGAN-deep model with the same architecture and number of parameters.", "keywords": ["GAN", "adversarial training", "generative model", "game theory"], "paperhash": "wu|logan_latent_optimisation_for_generative_adversarial_networks", "original_pdf": "/attachment/8dc1e9af07d9412cee04683b297adfc3c0fd78ae.pdf", "_bibtex": "@misc{\nwu2020logan,\ntitle={{\\{}LOGAN{\\}}:  Latent Optimisation for Generative Adversarial Networks},\nauthor={Yan Wu and Jeff Donahue and David Balduzzi and Karen Simonyan and Timothy Lillicrap},\nyear={2020},\nurl={https://openreview.net/forum?id=rJeU_1SFvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJeU_1SFvr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504189557, "tmdate": 1576860568332, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1804/Authors", "ICLR.cc/2020/Conference/Paper1804/Reviewers", "ICLR.cc/2020/Conference/Paper1804/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1804/-/Public_Comment"}}}, {"id": "rJeU_1SFvr", "original": "Hyly9eCuwB", "number": 1804, "cdate": 1569439597612, "ddate": null, "tcdate": 1569439597612, "tmdate": 1577168281980, "tddate": null, "forum": "rJeU_1SFvr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["yanwu@google.com", "jeffdonahue@google.com", "dbalduzzi@google.com", "simonyan@google.com", "countzero@google.com"], "title": "LOGAN:  Latent Optimisation for Generative Adversarial Networks", "authors": ["Yan Wu", "Jeff Donahue", "David Balduzzi", "Karen Simonyan", "Timothy Lillicrap"], "pdf": "/pdf/067003bb3a14491d779c8e6211d86588718072d7.pdf", "TL;DR": "Latent optimisation improves adversarial training dynamics. We present both theoretical analysis and state-of-the-art image generation with ImageNet 128x128.", "abstract": "Training generative adversarial networks requires balancing of delicate adversarial dynamics. Even with careful tuning, training may diverge or end up in a bad equilibrium with dropped modes. In this work, we introduce a new form of latent optimisation inspired by the CS-GAN and show that it improves adversarial dynamics by enhancing interactions between the discriminator and the generator. We develop supporting theoretical analysis from the perspectives of differentiable games and stochastic approximation. Our experiments demonstrate that latent optimisation can significantly improve GAN training, obtaining state-of-the-art performance for the ImageNet (128 x 128) dataset. Our model achieves an Inception Score (IS) of 148 and an Frechet Inception Distance (FID) of 3.4, an improvement of 17% and 32% in IS and FID respectively, compared with the baseline BigGAN-deep model with the same architecture and number of parameters.", "keywords": ["GAN", "adversarial training", "generative model", "game theory"], "paperhash": "wu|logan_latent_optimisation_for_generative_adversarial_networks", "original_pdf": "/attachment/8dc1e9af07d9412cee04683b297adfc3c0fd78ae.pdf", "_bibtex": "@misc{\nwu2020logan,\ntitle={{\\{}LOGAN{\\}}:  Latent Optimisation for Generative Adversarial Networks},\nauthor={Yan Wu and Jeff Donahue and David Balduzzi and Karen Simonyan and Timothy Lillicrap},\nyear={2020},\nurl={https://openreview.net/forum?id=rJeU_1SFvr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "iSaJsBR4fb", "original": null, "number": 6, "cdate": 1577105563378, "ddate": null, "tcdate": 1577105563378, "tmdate": 1577105563378, "tddate": null, "forum": "rJeU_1SFvr", "replyto": "tDgK7z-ny8", "invitation": "ICLR.cc/2020/Conference/Paper1804/-/Official_Comment", "content": {"title": "Response to Meta-review", "comment": "The meta-review ignored the positive (two weak accepts) reviews, while raising new criticism that is tangential to our claims. The AC requires additional experimental comparison to the SGA algorithm.  However, we only use SGA to provide a theoretical insight into our method, and don\u2019t claim that our method is an improvement. Our main contribution is empirical, and consists in improving the state of the art by a large margin (>30%) -- this was recognised by the reviewers, but ignored by the AC. As we point out, and reviewer #1 agrees, our method can be viewed as an approximation of SGA which only requires minimal modification of standard GAN training. We mentioned the less straightforward Hessian-vector implementation suggested by the AC in Appendix B.1, and can discuss this in more detail. Further comparison might be an interesting subject for future work, but it\u2019s not a subject of this paper. This point was not raised by either of the two reviewers.\n\nThe AC also mentioned two papers which we missed: \n\n[1] Sch\u00e4fer & Anandkumar, 2019, Competitive Gradient Descent; \n[2] Sch\u00e4fer, Zheng & Anandkumar, 2019, Implicit competitive regularization in GANs. \n\nThe first was just published at NeurIPS 2019 (uploaded to arXiv in May), while the second is a *concurrent* submission to ICLR 2020, and was uploaded to arXiv in October *after* the ICLR deadline. We are happy to cite these papers, but we were not previously aware of them, and could not possibly be aware of the second one as it was not public at the moment of submission. We also note that neither of the original two reviewers mentioned them in their reviews."}, "signatures": ["ICLR.cc/2020/Conference/Paper1804/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1804/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yanwu@google.com", "jeffdonahue@google.com", "dbalduzzi@google.com", "simonyan@google.com", "countzero@google.com"], "title": "LOGAN:  Latent Optimisation for Generative Adversarial Networks", "authors": ["Yan Wu", "Jeff Donahue", "David Balduzzi", "Karen Simonyan", "Timothy Lillicrap"], "pdf": "/pdf/067003bb3a14491d779c8e6211d86588718072d7.pdf", "TL;DR": "Latent optimisation improves adversarial training dynamics. We present both theoretical analysis and state-of-the-art image generation with ImageNet 128x128.", "abstract": "Training generative adversarial networks requires balancing of delicate adversarial dynamics. Even with careful tuning, training may diverge or end up in a bad equilibrium with dropped modes. In this work, we introduce a new form of latent optimisation inspired by the CS-GAN and show that it improves adversarial dynamics by enhancing interactions between the discriminator and the generator. We develop supporting theoretical analysis from the perspectives of differentiable games and stochastic approximation. Our experiments demonstrate that latent optimisation can significantly improve GAN training, obtaining state-of-the-art performance for the ImageNet (128 x 128) dataset. Our model achieves an Inception Score (IS) of 148 and an Frechet Inception Distance (FID) of 3.4, an improvement of 17% and 32% in IS and FID respectively, compared with the baseline BigGAN-deep model with the same architecture and number of parameters.", "keywords": ["GAN", "adversarial training", "generative model", "game theory"], "paperhash": "wu|logan_latent_optimisation_for_generative_adversarial_networks", "original_pdf": "/attachment/8dc1e9af07d9412cee04683b297adfc3c0fd78ae.pdf", "_bibtex": "@misc{\nwu2020logan,\ntitle={{\\{}LOGAN{\\}}:  Latent Optimisation for Generative Adversarial Networks},\nauthor={Yan Wu and Jeff Donahue and David Balduzzi and Karen Simonyan and Timothy Lillicrap},\nyear={2020},\nurl={https://openreview.net/forum?id=rJeU_1SFvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJeU_1SFvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1804/Authors", "ICLR.cc/2020/Conference/Paper1804/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1804/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1804/Reviewers", "ICLR.cc/2020/Conference/Paper1804/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1804/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1804/Authors|ICLR.cc/2020/Conference/Paper1804/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504150635, "tmdate": 1576860534672, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1804/Authors", "ICLR.cc/2020/Conference/Paper1804/Reviewers", "ICLR.cc/2020/Conference/Paper1804/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1804/-/Official_Comment"}}}, {"id": "tDgK7z-ny8", "original": null, "number": 1, "cdate": 1576798732851, "ddate": null, "tcdate": 1576798732851, "tmdate": 1576800903559, "tddate": null, "forum": "rJeU_1SFvr", "replyto": "rJeU_1SFvr", "invitation": "ICLR.cc/2020/Conference/Paper1804/-/Decision", "content": {"decision": "Reject", "comment": "The authors propose to overcome challenges in GAN training through latent optimization, i.e. updating the latent code, motivated by natural gradients. The authors show improvement over previous methods.  The work is well-motivated, but in my opinion, further experiments and comparisons need to be made before the work can be ready for publication.\n\nThe authors write that \"Unfortunately, SGA is expensive to scale because computing the second-order derivatives with respect to all parameters is expensive\" and further \"Crucially, latent optimization approximates SGA using only second-order derivatives with respect to the latent z and parameters of the discriminator and generator separately. The second-order terms involving parameters of both the discriminator and the generator \u2013 which are extremely expensive to compute \u2013 are not used. For latent z\u2019s with dimensions typically used in GANs (e.g., 128\u2013256, orders of magnitude less than the number of parameters), these can be computed efficiently. In short, latent optimization efficiently couples the gradients of the discriminator and generator, as prescribed by SGA, but using the much lower-dimensional latent source z which makes the adjustment scalable.\"\n\nHowever, this is not true. Computing the Hessian vector product is not that expensive. In fact, it can be computed at a cost comparable to gradient evaluations using automatic differentiation (Pearlmutter (1994)). In frameworks such as PyTorch, this can be done efficiently using double backpropagation, so only twice the cost.  Based on the above, one of the main claims of improvement over existing methods, which is furthermore not investigated experimentally, is false. \n\nIt is unacceptable that the authors do not compare with SGA: both in terms of quality and computational cost since that is the premise of the paper. The authors also miss recent works that successfully ran methods with Hessian-vector products: https://arxiv.org/abs/1905.12103 https://arxiv.org/abs/1910.05852", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yanwu@google.com", "jeffdonahue@google.com", "dbalduzzi@google.com", "simonyan@google.com", "countzero@google.com"], "title": "LOGAN:  Latent Optimisation for Generative Adversarial Networks", "authors": ["Yan Wu", "Jeff Donahue", "David Balduzzi", "Karen Simonyan", "Timothy Lillicrap"], "pdf": "/pdf/067003bb3a14491d779c8e6211d86588718072d7.pdf", "TL;DR": "Latent optimisation improves adversarial training dynamics. We present both theoretical analysis and state-of-the-art image generation with ImageNet 128x128.", "abstract": "Training generative adversarial networks requires balancing of delicate adversarial dynamics. Even with careful tuning, training may diverge or end up in a bad equilibrium with dropped modes. In this work, we introduce a new form of latent optimisation inspired by the CS-GAN and show that it improves adversarial dynamics by enhancing interactions between the discriminator and the generator. We develop supporting theoretical analysis from the perspectives of differentiable games and stochastic approximation. Our experiments demonstrate that latent optimisation can significantly improve GAN training, obtaining state-of-the-art performance for the ImageNet (128 x 128) dataset. Our model achieves an Inception Score (IS) of 148 and an Frechet Inception Distance (FID) of 3.4, an improvement of 17% and 32% in IS and FID respectively, compared with the baseline BigGAN-deep model with the same architecture and number of parameters.", "keywords": ["GAN", "adversarial training", "generative model", "game theory"], "paperhash": "wu|logan_latent_optimisation_for_generative_adversarial_networks", "original_pdf": "/attachment/8dc1e9af07d9412cee04683b297adfc3c0fd78ae.pdf", "_bibtex": "@misc{\nwu2020logan,\ntitle={{\\{}LOGAN{\\}}:  Latent Optimisation for Generative Adversarial Networks},\nauthor={Yan Wu and Jeff Donahue and David Balduzzi and Karen Simonyan and Timothy Lillicrap},\nyear={2020},\nurl={https://openreview.net/forum?id=rJeU_1SFvr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rJeU_1SFvr", "replyto": "rJeU_1SFvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795723798, "tmdate": 1576800275336, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1804/-/Decision"}}}, {"id": "ByembX1QjB", "original": null, "number": 2, "cdate": 1573217018554, "ddate": null, "tcdate": 1573217018554, "tmdate": 1573217173401, "tddate": null, "forum": "rJeU_1SFvr", "replyto": "BJxDhRyCtS", "invitation": "ICLR.cc/2020/Conference/Paper1804/-/Official_Comment", "content": {"title": "Open Source", "comment": "Thank you for your comments. \n\nWe will open source the CIFAR10 model and training code for the camera-ready version. In the meantime, we would be glad to answer your questions about the implementation, as well as to add any modelling details that might be missing. We appreciate your effort in attempting to replicate the results!"}, "signatures": ["ICLR.cc/2020/Conference/Paper1804/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1804/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yanwu@google.com", "jeffdonahue@google.com", "dbalduzzi@google.com", "simonyan@google.com", "countzero@google.com"], "title": "LOGAN:  Latent Optimisation for Generative Adversarial Networks", "authors": ["Yan Wu", "Jeff Donahue", "David Balduzzi", "Karen Simonyan", "Timothy Lillicrap"], "pdf": "/pdf/067003bb3a14491d779c8e6211d86588718072d7.pdf", "TL;DR": "Latent optimisation improves adversarial training dynamics. We present both theoretical analysis and state-of-the-art image generation with ImageNet 128x128.", "abstract": "Training generative adversarial networks requires balancing of delicate adversarial dynamics. Even with careful tuning, training may diverge or end up in a bad equilibrium with dropped modes. In this work, we introduce a new form of latent optimisation inspired by the CS-GAN and show that it improves adversarial dynamics by enhancing interactions between the discriminator and the generator. We develop supporting theoretical analysis from the perspectives of differentiable games and stochastic approximation. Our experiments demonstrate that latent optimisation can significantly improve GAN training, obtaining state-of-the-art performance for the ImageNet (128 x 128) dataset. Our model achieves an Inception Score (IS) of 148 and an Frechet Inception Distance (FID) of 3.4, an improvement of 17% and 32% in IS and FID respectively, compared with the baseline BigGAN-deep model with the same architecture and number of parameters.", "keywords": ["GAN", "adversarial training", "generative model", "game theory"], "paperhash": "wu|logan_latent_optimisation_for_generative_adversarial_networks", "original_pdf": "/attachment/8dc1e9af07d9412cee04683b297adfc3c0fd78ae.pdf", "_bibtex": "@misc{\nwu2020logan,\ntitle={{\\{}LOGAN{\\}}:  Latent Optimisation for Generative Adversarial Networks},\nauthor={Yan Wu and Jeff Donahue and David Balduzzi and Karen Simonyan and Timothy Lillicrap},\nyear={2020},\nurl={https://openreview.net/forum?id=rJeU_1SFvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJeU_1SFvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1804/Authors", "ICLR.cc/2020/Conference/Paper1804/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1804/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1804/Reviewers", "ICLR.cc/2020/Conference/Paper1804/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1804/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1804/Authors|ICLR.cc/2020/Conference/Paper1804/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504150635, "tmdate": 1576860534672, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1804/Authors", "ICLR.cc/2020/Conference/Paper1804/Reviewers", "ICLR.cc/2020/Conference/Paper1804/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1804/-/Official_Comment"}}}, {"id": "rkef_7JQjH", "original": null, "number": 3, "cdate": 1573217129974, "ddate": null, "tcdate": 1573217129974, "tmdate": 1573217129974, "tddate": null, "forum": "rJeU_1SFvr", "replyto": "HJx7zgxTFr", "invitation": "ICLR.cc/2020/Conference/Paper1804/-/Official_Comment", "content": {"title": "Revision and Clarification", "comment": "Thank you for your comments.\n\nWe have included your suggested changes in the current draft. Here we directly answer some main concerns:\n\n1. Symplectic Gradient Adjustment (SGA)\n\nWe have rewritten the section on how LOGAN relates to SGA to make it more clear. The main idea is that the latent step in LOGAN introduces a 3rd player into the game who is on the same side as the generator. \n\nNote that the \u201cHessian\u201d of a game differs significantly from the Hessian of a function: it involves second-order derivatives of multiple functions, corresponding to the losses of the players. The game Hessian thus decomposes into blocks labeled by the \u201ctype\u201d of interaction: G+G, G+D, G+Z, and so on. SGA makes use of the anti-symmetric component A of the game Hessian, where there are three kinds of blocks corresponding to second-order interactions between G+D, G+z and D+z respectively.\n\nSecond-order interactions between G+D are extremely expensive to compute. Thus, LOGAN only adjusts the gradients based on interactions between G+z and D+z. The partial approximation to SGA is well-founded game-theoretically in that it adjusts the dynamics between 2 of 3 possible pairings of the players \u2014 which suffice for a substantial performance gain. \n\nIt is currently unclear how approximations like the Gauss-Newton generalize to game Hessians. \n\n\n2. Natural Gradients (NG) and SGA: \n\nOur main motivation is indeed approximating SGA, and NG is the best way we found to implement the gradient updates in SGA. In other words, SGA does not specify the size of gradient steps, and NG provides an efficient way to adapt the step size under our approximation. We realised that the structure of our submission obscured this storyline, and re-organised it in our current draft.\n\n3. About the likelihood interpretation of the hinge loss (Appendix C): This section is relatively independent from the rest of the paper. NG has long been used as an approximate optimisation method without considering the probabilistic meaning of the loss. Similarly, SGA does not require the loss to be a likelihood function. We nevertheless provided this section for readers interested in the probabilistic view and acknowledged the limit of this interpretation in the text.\n\n4. Your comment about the typo is correct, thanks for pointing it out. It should read: \u201can ideal generator can perfectly fool the discriminator\u201d. This assumption is actually unnecessary, and we removed it. Yes, the likelihood function p(t|z) has the same form as the p in eq. 37. We rewrote this part to make the correspondence more clear.\n\n5. Thank you for pointing out this typo. The clipping should be after updating z. We have not experimented with many other algorithms for optimising z, and would like to investigate other methods including mirror descent in the future. We will also experiment with ELU activation."}, "signatures": ["ICLR.cc/2020/Conference/Paper1804/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1804/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yanwu@google.com", "jeffdonahue@google.com", "dbalduzzi@google.com", "simonyan@google.com", "countzero@google.com"], "title": "LOGAN:  Latent Optimisation for Generative Adversarial Networks", "authors": ["Yan Wu", "Jeff Donahue", "David Balduzzi", "Karen Simonyan", "Timothy Lillicrap"], "pdf": "/pdf/067003bb3a14491d779c8e6211d86588718072d7.pdf", "TL;DR": "Latent optimisation improves adversarial training dynamics. We present both theoretical analysis and state-of-the-art image generation with ImageNet 128x128.", "abstract": "Training generative adversarial networks requires balancing of delicate adversarial dynamics. Even with careful tuning, training may diverge or end up in a bad equilibrium with dropped modes. In this work, we introduce a new form of latent optimisation inspired by the CS-GAN and show that it improves adversarial dynamics by enhancing interactions between the discriminator and the generator. We develop supporting theoretical analysis from the perspectives of differentiable games and stochastic approximation. Our experiments demonstrate that latent optimisation can significantly improve GAN training, obtaining state-of-the-art performance for the ImageNet (128 x 128) dataset. Our model achieves an Inception Score (IS) of 148 and an Frechet Inception Distance (FID) of 3.4, an improvement of 17% and 32% in IS and FID respectively, compared with the baseline BigGAN-deep model with the same architecture and number of parameters.", "keywords": ["GAN", "adversarial training", "generative model", "game theory"], "paperhash": "wu|logan_latent_optimisation_for_generative_adversarial_networks", "original_pdf": "/attachment/8dc1e9af07d9412cee04683b297adfc3c0fd78ae.pdf", "_bibtex": "@misc{\nwu2020logan,\ntitle={{\\{}LOGAN{\\}}:  Latent Optimisation for Generative Adversarial Networks},\nauthor={Yan Wu and Jeff Donahue and David Balduzzi and Karen Simonyan and Timothy Lillicrap},\nyear={2020},\nurl={https://openreview.net/forum?id=rJeU_1SFvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJeU_1SFvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1804/Authors", "ICLR.cc/2020/Conference/Paper1804/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1804/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1804/Reviewers", "ICLR.cc/2020/Conference/Paper1804/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1804/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1804/Authors|ICLR.cc/2020/Conference/Paper1804/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504150635, "tmdate": 1576860534672, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1804/Authors", "ICLR.cc/2020/Conference/Paper1804/Reviewers", "ICLR.cc/2020/Conference/Paper1804/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1804/-/Official_Comment"}}}, {"id": "HJx7zgxTFr", "original": null, "number": 1, "cdate": 1571778570745, "ddate": null, "tcdate": 1571778570745, "tmdate": 1572972421204, "tddate": null, "forum": "rJeU_1SFvr", "replyto": "rJeU_1SFvr", "invitation": "ICLR.cc/2020/Conference/Paper1804/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposes a novel training scheme for GANs, which leads to improved scores w.r.t. state-of-the-art. The idea is to update the sampled latent code in a direction improving the inner maximization in the min-max problem. The work considers both, the gradient direction and a direction motivated by the natural gradient which is shown to yield excellent performance. The overall scheme is motivated as an approximation to the (prohibitively expensive) symplectic gradient adjustment method. \n\nWhile the work contains some typos, it was easy to read and overall very well written. The experimental results are impressive and clearly validate the usefulness of the approach. Therefore, I recommend acceptance of the paper.  \n\nOn the theoretical side, I feel some parts can be improved. I'm willing to further increase my rating if some of the following points are addressed:\n\n1. The connection to SGA is a bit hand-wavy, as terms are dropped or approximated with the only reasoning that they are difficult to compute. In some approximations (e.g. Gauss-Newton approximation of the Hessian) one can argue quite well that certain second-order terms can be dropped under some assumptions (e.g. mild nonlinearity, or vanishing near the optimum).\n\n2. I had some troubles Sec 4 related to the natural gradient.\n(a) What is p(t | z)? Is it the same p as in (37), Appendix C? I find the argument that the hinge loss pushing D(G(z)) < 0 during training hand-wavy, as for natural gradient p(t | z) should always be a valid distribution. There are also some typos which made it hard to follow the arguments there. It probably should read \"an ideal generator can perfectly fool the discriminator\" (not perfectly fool the generator). \n\n(b) Maybe it would be easier to directly argue that one wishes to approximate SGA as well as possible, rather than taking a detour through the natural gradient.\n\n3. Why is the increment \\Delta z clipped in Algorithm 1? Is there a theoretical justification? If the goal of the clipping is to stay inside the support of the uniform distribution, shouldn't it rather be z' = [z + \\Delta z]? A soft clipping (e.g. performing a mirror descent step) might give better gradients.\n\nTypos, minor comments (no influence on my rating):\n- In Eq. (5), it should be \\partial^2 in the last \"Hessian-block\" multiplication.\n- presribed -> prescribed\n- Eqs. (7) and (8) might be easier to parse if one uses a different notation to distinguish between total derivative and partial derivative, i.e., write on the left side df(z') / d\\theta. Also, I think it is clearer to write in the last terms in (7) and (8) \\partial f(z') / \\partial \\delta z instead of \\partial f(z') / \\partial z'.\n- In Appendix B.1, shouldn't it be \\gamma=(1+\\eta)/2 instead of  \\gamma=\\eta (1+\\eta)/2?\n- I've found ELU activations to work well in GAN models which involve the Jacobian w.r.t. z. Maybe it can stabilize things here as well.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1804/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1804/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yanwu@google.com", "jeffdonahue@google.com", "dbalduzzi@google.com", "simonyan@google.com", "countzero@google.com"], "title": "LOGAN:  Latent Optimisation for Generative Adversarial Networks", "authors": ["Yan Wu", "Jeff Donahue", "David Balduzzi", "Karen Simonyan", "Timothy Lillicrap"], "pdf": "/pdf/067003bb3a14491d779c8e6211d86588718072d7.pdf", "TL;DR": "Latent optimisation improves adversarial training dynamics. We present both theoretical analysis and state-of-the-art image generation with ImageNet 128x128.", "abstract": "Training generative adversarial networks requires balancing of delicate adversarial dynamics. Even with careful tuning, training may diverge or end up in a bad equilibrium with dropped modes. In this work, we introduce a new form of latent optimisation inspired by the CS-GAN and show that it improves adversarial dynamics by enhancing interactions between the discriminator and the generator. We develop supporting theoretical analysis from the perspectives of differentiable games and stochastic approximation. Our experiments demonstrate that latent optimisation can significantly improve GAN training, obtaining state-of-the-art performance for the ImageNet (128 x 128) dataset. Our model achieves an Inception Score (IS) of 148 and an Frechet Inception Distance (FID) of 3.4, an improvement of 17% and 32% in IS and FID respectively, compared with the baseline BigGAN-deep model with the same architecture and number of parameters.", "keywords": ["GAN", "adversarial training", "generative model", "game theory"], "paperhash": "wu|logan_latent_optimisation_for_generative_adversarial_networks", "original_pdf": "/attachment/8dc1e9af07d9412cee04683b297adfc3c0fd78ae.pdf", "_bibtex": "@misc{\nwu2020logan,\ntitle={{\\{}LOGAN{\\}}:  Latent Optimisation for Generative Adversarial Networks},\nauthor={Yan Wu and Jeff Donahue and David Balduzzi and Karen Simonyan and Timothy Lillicrap},\nyear={2020},\nurl={https://openreview.net/forum?id=rJeU_1SFvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJeU_1SFvr", "replyto": "rJeU_1SFvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1804/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1804/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575771191890, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1804/Reviewers"], "noninvitees": [], "tcdate": 1570237732050, "tmdate": 1575771191908, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1804/-/Official_Review"}}}, {"id": "BJxDhRyCtS", "original": null, "number": 2, "cdate": 1571843758630, "ddate": null, "tcdate": 1571843758630, "tmdate": 1572972421159, "tddate": null, "forum": "rJeU_1SFvr", "replyto": "rJeU_1SFvr", "invitation": "ICLR.cc/2020/Conference/Paper1804/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\n\nLOGAN optimizes the sampled latent generative vector z in conjunction with the generator and discriminator. By exploiting second order updates, z is optimized to allow for better training and performance of the generator and discriminator.\n\nPros:\n+ A relatively efficient way of exploiting second order dynamics in GAN training via latent space optimization.\n+ A good set of experiments demonstrating the superior performance of the proposed method on both large and small scale models and datasets.\n\nCons:\n- Lack of code\n\nComments:\nAll in all, this appears to be a solid contribution to the GAN literature, addressing some of the limitations of CS-GAN [1]. The lack of open source code accompanying the paper (in this day and age) does it a serious disservice. I have already tried and failed to replicate the cifar10 results. There appears to be some nuance in implementation that would probably clear up if the authors release their code along with the paper.\n\n[1] - http://proceedings.mlr.press/v97/wu19d.html"}, "signatures": ["ICLR.cc/2020/Conference/Paper1804/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1804/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yanwu@google.com", "jeffdonahue@google.com", "dbalduzzi@google.com", "simonyan@google.com", "countzero@google.com"], "title": "LOGAN:  Latent Optimisation for Generative Adversarial Networks", "authors": ["Yan Wu", "Jeff Donahue", "David Balduzzi", "Karen Simonyan", "Timothy Lillicrap"], "pdf": "/pdf/067003bb3a14491d779c8e6211d86588718072d7.pdf", "TL;DR": "Latent optimisation improves adversarial training dynamics. We present both theoretical analysis and state-of-the-art image generation with ImageNet 128x128.", "abstract": "Training generative adversarial networks requires balancing of delicate adversarial dynamics. Even with careful tuning, training may diverge or end up in a bad equilibrium with dropped modes. In this work, we introduce a new form of latent optimisation inspired by the CS-GAN and show that it improves adversarial dynamics by enhancing interactions between the discriminator and the generator. We develop supporting theoretical analysis from the perspectives of differentiable games and stochastic approximation. Our experiments demonstrate that latent optimisation can significantly improve GAN training, obtaining state-of-the-art performance for the ImageNet (128 x 128) dataset. Our model achieves an Inception Score (IS) of 148 and an Frechet Inception Distance (FID) of 3.4, an improvement of 17% and 32% in IS and FID respectively, compared with the baseline BigGAN-deep model with the same architecture and number of parameters.", "keywords": ["GAN", "adversarial training", "generative model", "game theory"], "paperhash": "wu|logan_latent_optimisation_for_generative_adversarial_networks", "original_pdf": "/attachment/8dc1e9af07d9412cee04683b297adfc3c0fd78ae.pdf", "_bibtex": "@misc{\nwu2020logan,\ntitle={{\\{}LOGAN{\\}}:  Latent Optimisation for Generative Adversarial Networks},\nauthor={Yan Wu and Jeff Donahue and David Balduzzi and Karen Simonyan and Timothy Lillicrap},\nyear={2020},\nurl={https://openreview.net/forum?id=rJeU_1SFvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJeU_1SFvr", "replyto": "rJeU_1SFvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1804/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1804/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575771191890, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1804/Reviewers"], "noninvitees": [], "tcdate": 1570237732050, "tmdate": 1575771191908, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1804/-/Official_Review"}}}], "count": 9}