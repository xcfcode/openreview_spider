{"notes": [{"id": "ryeT10VKDH", "original": "HJl-yk7uwr", "number": 910, "cdate": 1569439205347, "ddate": null, "tcdate": 1569439205347, "tmdate": 1577168236300, "tddate": null, "forum": "ryeT10VKDH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["girishj2@illinois.edu", "girishc@illinois.edu"], "title": "Adapt-to-Learn: Policy Transfer in Reinforcement Learning", "authors": ["Girish Joshi", "Girish Chowdhary"], "pdf": "/pdf/de70eac82ba0ad5bf7534eec4d68c82103c86f4e.pdf", "TL;DR": "In this paper, we present an architecture for adapting the policies learned from one RL domain to another.", "abstract": "Efficient and robust policy transfer remains a key challenge in reinforcement learning. Policy transfer through warm initialization, imitation, or interacting over a large set of agents with randomized instances, have been commonly applied to solve a variety of Reinforcement Learning (RL) tasks. However, this is far from how behavior transfer happens in the biological world: Humans and animals are able to quickly adapt the learned behaviors between similar tasks and learn new skills when presented with new situations. Here we seek to answer the question: Will learning to combine adaptation reward with environmental reward lead to a more efficient transfer of policies between domains? We introduce a principled mechanism that can \\textbf{``Adapt-to-Learn\"}, that is adapt the source policy to learn to solve a target task with significant transition differences and uncertainties.  We show through theory and experiments that our method leads to a significantly reduced sample complexity of transferring the policies between the tasks.", "keywords": ["Transfer Learning", "Reinforcement Learning", "Adaptation"], "paperhash": "joshi|adapttolearn_policy_transfer_in_reinforcement_learning", "original_pdf": "/attachment/de70eac82ba0ad5bf7534eec4d68c82103c86f4e.pdf", "_bibtex": "@misc{\njoshi2020adapttolearn,\ntitle={Adapt-to-Learn: Policy Transfer in Reinforcement Learning},\nauthor={Girish Joshi and Girish Chowdhary},\nyear={2020},\nurl={https://openreview.net/forum?id=ryeT10VKDH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "54CM5XuiMv", "original": null, "number": 1, "cdate": 1576798709449, "ddate": null, "tcdate": 1576798709449, "tmdate": 1576800926872, "tddate": null, "forum": "ryeT10VKDH", "replyto": "ryeT10VKDH", "invitation": "ICLR.cc/2020/Conference/Paper910/-/Decision", "content": {"decision": "Reject", "comment": "This paper considers inter-domain policy transfer in reinforcement learning. The proposed approach involves adapting existing policies from a source task to a target task by adding a cost related to the difference between the dynamics and trajectory likelihoods of the two tasks.\n\nThere are three major problems with this paper as it stands, as pointed out by the reviewers. Firstly, the \"KL divergence\" is not a real KL divergence and seems to be only empirically motivated. Then, there are issues with the derivative of the policy gradient. Finally, the theory is not well connected to the proposed algorithm. The rebuttals not only failed to convince the reviewer that raised these issues, but another reviewer lowered their score as a result of these raised points.\n\nThis is a really interesting idea with compelling experiments, but must be rejected at\u00a0this point for the aforementioned reasons.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["girishj2@illinois.edu", "girishc@illinois.edu"], "title": "Adapt-to-Learn: Policy Transfer in Reinforcement Learning", "authors": ["Girish Joshi", "Girish Chowdhary"], "pdf": "/pdf/de70eac82ba0ad5bf7534eec4d68c82103c86f4e.pdf", "TL;DR": "In this paper, we present an architecture for adapting the policies learned from one RL domain to another.", "abstract": "Efficient and robust policy transfer remains a key challenge in reinforcement learning. Policy transfer through warm initialization, imitation, or interacting over a large set of agents with randomized instances, have been commonly applied to solve a variety of Reinforcement Learning (RL) tasks. However, this is far from how behavior transfer happens in the biological world: Humans and animals are able to quickly adapt the learned behaviors between similar tasks and learn new skills when presented with new situations. Here we seek to answer the question: Will learning to combine adaptation reward with environmental reward lead to a more efficient transfer of policies between domains? We introduce a principled mechanism that can \\textbf{``Adapt-to-Learn\"}, that is adapt the source policy to learn to solve a target task with significant transition differences and uncertainties.  We show through theory and experiments that our method leads to a significantly reduced sample complexity of transferring the policies between the tasks.", "keywords": ["Transfer Learning", "Reinforcement Learning", "Adaptation"], "paperhash": "joshi|adapttolearn_policy_transfer_in_reinforcement_learning", "original_pdf": "/attachment/de70eac82ba0ad5bf7534eec4d68c82103c86f4e.pdf", "_bibtex": "@misc{\njoshi2020adapttolearn,\ntitle={Adapt-to-Learn: Policy Transfer in Reinforcement Learning},\nauthor={Girish Joshi and Girish Chowdhary},\nyear={2020},\nurl={https://openreview.net/forum?id=ryeT10VKDH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "ryeT10VKDH", "replyto": "ryeT10VKDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795728139, "tmdate": 1576800280494, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper910/-/Decision"}}}, {"id": "S1x5pV0toH", "original": null, "number": 9, "cdate": 1573672129533, "ddate": null, "tcdate": 1573672129533, "tmdate": 1573672129533, "tddate": null, "forum": "ryeT10VKDH", "replyto": "ByeLhomfsS", "invitation": "ICLR.cc/2020/Conference/Paper910/-/Official_Comment", "content": {"title": "Comments post-rebuttal", "comment": "Dear authors,\n\nThank you for your detailed response and clarification. Here is a follow-up to some of my points.\n\n1. If I understood correctly from the rebuttal, Equation 3 is defining something that is quite different from the KL divergence between source and target trajectory distributions. In particular, p(\\tau) is the actual pdf of \\tau in the target task, but q(\\tau) is the pdf of the sequence of states in \\tau in the source task, assuming the optimal source actions are always taken (i.e., q(\\tau) ignores the actual actions a_t in \\tau and replaces them by the optimal ones according to the source policy a'_t). Is this correct? In such a case, this alternative definition is quite confusing to me and its interpretation is not clear. Could the authors better motivate this choice? Furthermore, all this should be clearly stated in the paper as anyone would expect (3) to be the standard KL divergence.\n\n2. I agree that in standard RL settings the reward is not a function of the policy parameters, but in this case it seems to me that it is. If I look at Equation (8), the \"bonus\" reward term does contain the policy. When plugging the modified reward r' into (10), after decomposing the logarithm into a sum of logs, one of the terms we obtain is -\\beta\\mathbb{E}_{p_\\theta}[\\sum \\log\\pi_\\theta(a_t | s_t)]. This should be proportional to the sum of one-step entropies of the policy and both the distribution p_\\theta over which the expectation is taken and the function inside the expectation depend on \\theta and must be differentiated. However, the derivation of the gradient in the appendix seems to simply neglect this dependency of r' on \\theta. See, e.g., Section 6.2 of [1] where parameter-dependent rewards are considered and the policy-gradient is shown to be, in fact, different.\n\n3. Regarding the sample-complexity analysis, it is still not clear to me what results are proved here. Theorem 4.1 provides a lower-bound on the sample complexity required to obtain an (\\epsilon,\\delta)-PAC algorithm (right?), but where is the upper bound on the sample complexity of the proposed method? Lemma 4.3 bounds the l_\\inf error on the value function of the optimal policy \\pi^* under two different models which are \\epsilon-close. How does that relate to the proposed method? Also, the proof of Theorem 4.1 uses a union bound over all policies, which requires the policy space to be finite. This is a very strong assumption.\n\n4. Related to the previous point, there exist several other transfer algorithms in the literature which provide sample-complexity bounds. For instance, [2,3,4] provide PAC sample-complexity bounds for finite MDPs,  while [5,6,7] provide finite-sample analyses for continuous spaces. It would be good to discuss these works in the paper and compare their results to those obtained here.\n\n[1] Baxter, J., & Bartlett, P. L. (2001). Infinite-horizon policy-gradient estimation. Journal of Artificial Intelligence Research, 15, 319-350.\n[2] Abel, D., Jinnai, Y., Guo, S. Y., Konidaris, G., & Littman, M. (2018, July). Policy and value transfer in lifelong reinforcement learning. In International Conference on Machine Learning (pp. 20-29).\n[3] Brunskill, E., & Li, L. (2013). Sample complexity of multi-task reinforcement learning. arXiv preprint arXiv:1309.6821.\n[4] Mann, T. A., & Choe, Y. (2012, December). Directed Exploration in Reinforcement Learning with Transferred Knowledge. In Ewrl (pp. 59-76).\n[5] Tirinzoni, A., Sessa, A., Pirotta, M., & Restelli, M. (2018, July). Importance Weighted Transfer of Samples in Reinforcement Learning. In International Conference on Machine Learning (pp. 4943-4952).\n[6] Tirinzoni, A., Sanchez, R. R., & Restelli, M. (2018). Transfer of value functions via variational methods. In Advances in Neural Information Processing Systems (pp. 6179-6189).\n[7] Liu, Y., Guo, Z., & Brunskill, E. (2016, May). PAC continuous state online multitask reinforcement learning with identification. In Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems (pp. 438-446). International Foundation for Autonomous Agents and Multiagent Systems.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper910/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper910/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["girishj2@illinois.edu", "girishc@illinois.edu"], "title": "Adapt-to-Learn: Policy Transfer in Reinforcement Learning", "authors": ["Girish Joshi", "Girish Chowdhary"], "pdf": "/pdf/de70eac82ba0ad5bf7534eec4d68c82103c86f4e.pdf", "TL;DR": "In this paper, we present an architecture for adapting the policies learned from one RL domain to another.", "abstract": "Efficient and robust policy transfer remains a key challenge in reinforcement learning. Policy transfer through warm initialization, imitation, or interacting over a large set of agents with randomized instances, have been commonly applied to solve a variety of Reinforcement Learning (RL) tasks. However, this is far from how behavior transfer happens in the biological world: Humans and animals are able to quickly adapt the learned behaviors between similar tasks and learn new skills when presented with new situations. Here we seek to answer the question: Will learning to combine adaptation reward with environmental reward lead to a more efficient transfer of policies between domains? We introduce a principled mechanism that can \\textbf{``Adapt-to-Learn\"}, that is adapt the source policy to learn to solve a target task with significant transition differences and uncertainties.  We show through theory and experiments that our method leads to a significantly reduced sample complexity of transferring the policies between the tasks.", "keywords": ["Transfer Learning", "Reinforcement Learning", "Adaptation"], "paperhash": "joshi|adapttolearn_policy_transfer_in_reinforcement_learning", "original_pdf": "/attachment/de70eac82ba0ad5bf7534eec4d68c82103c86f4e.pdf", "_bibtex": "@misc{\njoshi2020adapttolearn,\ntitle={Adapt-to-Learn: Policy Transfer in Reinforcement Learning},\nauthor={Girish Joshi and Girish Chowdhary},\nyear={2020},\nurl={https://openreview.net/forum?id=ryeT10VKDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryeT10VKDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper910/Authors", "ICLR.cc/2020/Conference/Paper910/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper910/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper910/Reviewers", "ICLR.cc/2020/Conference/Paper910/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper910/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper910/Authors|ICLR.cc/2020/Conference/Paper910/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164312, "tmdate": 1576860552720, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper910/Authors", "ICLR.cc/2020/Conference/Paper910/Reviewers", "ICLR.cc/2020/Conference/Paper910/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper910/-/Official_Comment"}}}, {"id": "HJlnnt22YH", "original": null, "number": 1, "cdate": 1571764659915, "ddate": null, "tcdate": 1571764659915, "tmdate": 1572972536903, "tddate": null, "forum": "ryeT10VKDH", "replyto": "ryeT10VKDH", "invitation": "ICLR.cc/2020/Conference/Paper910/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary\n-------\nThe authors propose an algorithm for transferring policies across domains differing in their transition model. The idea is to \"regularize\" the target policy being learned to generate a trajectory distribution similar to the one induced by the source optimal policy in the respective MDP. The method is proved effective in standard simulated robotic tasks (Mujoco).\n\nMajor comments\n--------------\nThe problem of transfer in RL is fundamental for scaling RL algorithms to real domains. The presented ideas are interesting and could potentially inspire other works in this field. The paper is well-written and easy to read. However, I believe there might be some technical issues with the derivations that overall make me doubt about the significance of this work. My detailed comments follow.\n\n1. Under Assumption 1, the step from (6) to (7) seems flawed. The definition of the KL divergence requires that p is absolutely continuous w.r.t. q. However, under Ass. 1, whenever the trajectory \\tau contains an action that is not taken by \\pi^*, q(\\tau) = 0, which implies that p(\\tau) must be zero as well. The only way to enforce this is to set \\pi_\\theta = \\pi^*, and the solution is trivial. The main issue is that (6) writes the log term of the KL using different action random variables for the numerator and the denominator (a and a'), but these must be the same. Therefore, one cannot simply set to 1 the optimal source actions. Notice also that the action space is defined continuous, hence the value of the pdf in the chosen deterministic action is infinity, not 1.\n\n2. (12) seems to be wrong too. Note that the reward function defined here (r') contains the policy, hence it is a function of \\theta and must be differentiated. So the standard REINFORCE estimator written here does not work theoretically.\n\n3. Assumption 2 basically requires that either we can get new samples from the source domain \"on-demand\", or we have a good simulated model. Both assumptions are often violated in practice. Could the authors elaborate more on why they believe these assumptions to be reasonable?\n\n4. Related to the previous point, assuming the one-step transition models to be Gaussian is convenient, and often reasonable, in practice. But how do we choose the std \\sigma? What if the true models are deterministic?\n\n5. I would disagree with the last sentence of section 3.2.1, which states that minimizing the KL divergence is more sample efficient than standard RL/policy-search methods. One can always define an MDP where the immediate reward is the KL term and that is a standard RL problem. Furthermore, if we compute the expected return when the reward is (8), decomposing the log into the sum of two logs, one of the two expectations we obtain is of the form: E_\\pi[-(s_{t+1} -s'_{t+1})^2], which is a standard \"tracking\" problem.\n\n6. I was quite confused by the theoretical section. The introduction states that the proposed method enjoys a sample complexity of O(nH), where I guess n is the number of state-action pairs. Where does that dependence appear in Section 4? Theorem 4.1 is a standard supervised learning bound. How do we read that in RL? What is C? Is |\\Pi| the cardinality of the set of policies (assumed finite)? Or is it the pseudo-dimension of the policy space? Lemma 4.3 is also a know result in the literature, but what does it tell us about the proposed approach? The idea is that if I have an accurate model \\hat{p} I am also close in value function to the true MDP. But what is \\hat{p} here? The proposed algorithm is not computing any explicit transition model.\n\nMinor comments\n--------------\n- In the last sentence of the introduction, it is not clear what n is in O(nH)\n- In the preliminaries, is the reward R: S x A -> R_+ assumed to be positive or is that a typo?\n- MDPs are defined with a discount factor \\gamma which is however missing in (1) and the next equation (V)\n- In the preliminaries, it is not clear whether the rewards differ or not between domains. It is important to specify that they do not.\n- In (2), T is defined to map R^n -> R^n. What is n? Is it the number of discrete states? Wasn't the state space continuos?\n- I noticed that many sentences in the paper begin without a space after the period of the previous sentence.\n- In section 3, the KL divergence (e.g., eq. 3,7) is written as an expectation that contains the pdf p(\\tau). Either only the log term is written in the expectation, or E is replaced by an integral.\n- In (12), the sum should be up to H and not infinity"}, "signatures": ["ICLR.cc/2020/Conference/Paper910/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper910/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["girishj2@illinois.edu", "girishc@illinois.edu"], "title": "Adapt-to-Learn: Policy Transfer in Reinforcement Learning", "authors": ["Girish Joshi", "Girish Chowdhary"], "pdf": "/pdf/de70eac82ba0ad5bf7534eec4d68c82103c86f4e.pdf", "TL;DR": "In this paper, we present an architecture for adapting the policies learned from one RL domain to another.", "abstract": "Efficient and robust policy transfer remains a key challenge in reinforcement learning. Policy transfer through warm initialization, imitation, or interacting over a large set of agents with randomized instances, have been commonly applied to solve a variety of Reinforcement Learning (RL) tasks. However, this is far from how behavior transfer happens in the biological world: Humans and animals are able to quickly adapt the learned behaviors between similar tasks and learn new skills when presented with new situations. Here we seek to answer the question: Will learning to combine adaptation reward with environmental reward lead to a more efficient transfer of policies between domains? We introduce a principled mechanism that can \\textbf{``Adapt-to-Learn\"}, that is adapt the source policy to learn to solve a target task with significant transition differences and uncertainties.  We show through theory and experiments that our method leads to a significantly reduced sample complexity of transferring the policies between the tasks.", "keywords": ["Transfer Learning", "Reinforcement Learning", "Adaptation"], "paperhash": "joshi|adapttolearn_policy_transfer_in_reinforcement_learning", "original_pdf": "/attachment/de70eac82ba0ad5bf7534eec4d68c82103c86f4e.pdf", "_bibtex": "@misc{\njoshi2020adapttolearn,\ntitle={Adapt-to-Learn: Policy Transfer in Reinforcement Learning},\nauthor={Girish Joshi and Girish Chowdhary},\nyear={2020},\nurl={https://openreview.net/forum?id=ryeT10VKDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ryeT10VKDH", "replyto": "ryeT10VKDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper910/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper910/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575973161170, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper910/Reviewers"], "noninvitees": [], "tcdate": 1570237745200, "tmdate": 1575973161187, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper910/-/Official_Review"}}}, {"id": "rklgQ7uCtH", "original": null, "number": 2, "cdate": 1571877655643, "ddate": null, "tcdate": 1571877655643, "tmdate": 1572972536860, "tddate": null, "forum": "ryeT10VKDH", "replyto": "ryeT10VKDH", "invitation": "ICLR.cc/2020/Conference/Paper910/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "\nThis paper addresses the actively studied problem of efficiently transferring policies across domains in reinforcement learning. Authors propose a framework to transfer policies between tasks in domains with significantly different state transition. The proposed algorithm is based on a policy adaptation mechanism, with the idea that provided that a source optimal policy of a task is available, that policy is adapted to derive the optimal policy of the target task at a low sample complexity. \n\nThe paper is well written and the proposed algorithm is novel and original. The algorithm is very clearly described and theoretical bounds are provided. The performance is evaluated experimentally on a set of tasks, showing promising results.  \n\nCons:\n- Some important previous  works are missing from the related work section. For example, [1, 2] are also concerned with transferring policy across task that share the same domain. \n- The experimental section could benefit from comparing to other transfer RL baselines such as [1].\n- Assumption 2 should be discussed; any suggestion on how to model the KL approximation if the assumption does not hold?\n\nMinor comments for clarity:\n- some acronyms are introduced without ever being spelled out: D-RL, TL, MDP, PAC\n- page 3, equation before (2), is \\gamma = 1?\n- the use of s' for the optimal state, a' for optimal action and then r' for total reward is quite confusing. Specially because s' is in general used to describe a subsequent state in the literature. \n- page 7, first line 'discount' instead of 'discout'\n- font on figures 1 and 2 is very small\n\n1. Barreto et al, Transfer in Deep Reinforcement Learning Using Successor Features and Generalised Policy Improvement, 2018\n2. Ma, Wen, and Bengio Universal Successor Representations for Transfer Reinforcement Learning, 2018"}, "signatures": ["ICLR.cc/2020/Conference/Paper910/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper910/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["girishj2@illinois.edu", "girishc@illinois.edu"], "title": "Adapt-to-Learn: Policy Transfer in Reinforcement Learning", "authors": ["Girish Joshi", "Girish Chowdhary"], "pdf": "/pdf/de70eac82ba0ad5bf7534eec4d68c82103c86f4e.pdf", "TL;DR": "In this paper, we present an architecture for adapting the policies learned from one RL domain to another.", "abstract": "Efficient and robust policy transfer remains a key challenge in reinforcement learning. Policy transfer through warm initialization, imitation, or interacting over a large set of agents with randomized instances, have been commonly applied to solve a variety of Reinforcement Learning (RL) tasks. However, this is far from how behavior transfer happens in the biological world: Humans and animals are able to quickly adapt the learned behaviors between similar tasks and learn new skills when presented with new situations. Here we seek to answer the question: Will learning to combine adaptation reward with environmental reward lead to a more efficient transfer of policies between domains? We introduce a principled mechanism that can \\textbf{``Adapt-to-Learn\"}, that is adapt the source policy to learn to solve a target task with significant transition differences and uncertainties.  We show through theory and experiments that our method leads to a significantly reduced sample complexity of transferring the policies between the tasks.", "keywords": ["Transfer Learning", "Reinforcement Learning", "Adaptation"], "paperhash": "joshi|adapttolearn_policy_transfer_in_reinforcement_learning", "original_pdf": "/attachment/de70eac82ba0ad5bf7534eec4d68c82103c86f4e.pdf", "_bibtex": "@misc{\njoshi2020adapttolearn,\ntitle={Adapt-to-Learn: Policy Transfer in Reinforcement Learning},\nauthor={Girish Joshi and Girish Chowdhary},\nyear={2020},\nurl={https://openreview.net/forum?id=ryeT10VKDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ryeT10VKDH", "replyto": "ryeT10VKDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper910/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper910/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575973161170, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper910/Reviewers"], "noninvitees": [], "tcdate": 1570237745200, "tmdate": 1575973161187, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper910/-/Official_Review"}}}, {"id": "S1lVR3-bqS", "original": null, "number": 3, "cdate": 1572048075873, "ddate": null, "tcdate": 1572048075873, "tmdate": 1572972536816, "tddate": null, "forum": "ryeT10VKDH", "replyto": "ryeT10VKDH", "invitation": "ICLR.cc/2020/Conference/Paper910/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary\nThis paper tackles the problem of transferring a policy from source to target MDP, which differ in the state transition function. The idea is to add an additional cost that is the KL divergence between the trajectory likelihood under target policy (being learned) and target dynamics and the trajectory likelihood under the source policy (assumed optimal and deterministic) and source dynamics. The intuition is that the target policy will learn to match the state distribution of the optimal source policy. Results on MuJoCo locomotion robots with varying physics show that the proposed method performs better on target than warm-started RL or learning from scratch. \n\nI think the problem of transferring knowledge from one task to another in RL is very important for RL to be applicable to more real-world scenarios.\n\nConcerns / Questions\nLine 7 of Alg1 is confusing because it refers to a \u201ctarget task model\u201d, but in Assumption 2, it says only a model of the source transition function is needed. I think it makes sense that only the source transition model is needed because the target next state is given by experience. \nI think the combined assumptions of a) access to expert behavior (same as DAGGER) and b) that the MDPs differ only in dynamics functions and c) access to the source transition model are rather strong. I think (b) is a special case of transfer learning - a lot of transfer learning is concerned with changing reward functions as well, which this method wouldn\u2019t apply to. I think this could be made more clear in the paper. It would be good if all these assumptions were made clear and discussed.\nI think the related work section is missing important areas of research in imitation learning and meta-reinforcement learning. For imitation learning, the approach strikes me as bearing similarity to PRECOG: PREdiction Conditioned On Goals in Visual Multi-Agent Settings (Rhinehart et al.), and the topic of imitation learning should be discussed in general. For meta-RL, mentioning that it shares the same goal of transfer and citing a few main works (e.g. Duan et al. 2016, Wang et al. 2016, Finn et al. 2017 etc) would be good.\n\nWriting Suggestions\nSome terms used throughout the paper are quite unclear (e.g., \u201cunsupervised RL\u201d, \u201cintrinsic adaptation reward\u201d, \u201csupervised reference trajectory tracking\u201d). I suggest standardizing and defining terms early to avoid unnecessary confusion.\nWriting the Bellman operator and the value function equations in Section 2 don\u2019t seem very relevant as they are I think never used again?\nSections 3.1 and 3.2 are quite difficult to understand on first read (e.g., what does \u201cpoint-wise local trajectories\u201d mean?). \nI find Section 3.2.1 a bit misleading, \u201cThe optimization is more akin to supervised learning\u201d - I agree the KL minimization is essentially imitation learning, but you are still doing policy search in addition to it?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper910/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper910/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["girishj2@illinois.edu", "girishc@illinois.edu"], "title": "Adapt-to-Learn: Policy Transfer in Reinforcement Learning", "authors": ["Girish Joshi", "Girish Chowdhary"], "pdf": "/pdf/de70eac82ba0ad5bf7534eec4d68c82103c86f4e.pdf", "TL;DR": "In this paper, we present an architecture for adapting the policies learned from one RL domain to another.", "abstract": "Efficient and robust policy transfer remains a key challenge in reinforcement learning. Policy transfer through warm initialization, imitation, or interacting over a large set of agents with randomized instances, have been commonly applied to solve a variety of Reinforcement Learning (RL) tasks. However, this is far from how behavior transfer happens in the biological world: Humans and animals are able to quickly adapt the learned behaviors between similar tasks and learn new skills when presented with new situations. Here we seek to answer the question: Will learning to combine adaptation reward with environmental reward lead to a more efficient transfer of policies between domains? We introduce a principled mechanism that can \\textbf{``Adapt-to-Learn\"}, that is adapt the source policy to learn to solve a target task with significant transition differences and uncertainties.  We show through theory and experiments that our method leads to a significantly reduced sample complexity of transferring the policies between the tasks.", "keywords": ["Transfer Learning", "Reinforcement Learning", "Adaptation"], "paperhash": "joshi|adapttolearn_policy_transfer_in_reinforcement_learning", "original_pdf": "/attachment/de70eac82ba0ad5bf7534eec4d68c82103c86f4e.pdf", "_bibtex": "@misc{\njoshi2020adapttolearn,\ntitle={Adapt-to-Learn: Policy Transfer in Reinforcement Learning},\nauthor={Girish Joshi and Girish Chowdhary},\nyear={2020},\nurl={https://openreview.net/forum?id=ryeT10VKDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ryeT10VKDH", "replyto": "ryeT10VKDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper910/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper910/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575973161170, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper910/Reviewers"], "noninvitees": [], "tcdate": 1570237745200, "tmdate": 1575973161187, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper910/-/Official_Review"}}}], "count": 6}