{"notes": [{"tddate": null, "tmdate": 1493702853541, "tcdate": 1493702853541, "number": 7, "id": "Skpp_cHkW", "invitation": "ICLR.cc/2017/conference/-/paper597/public/comment", "forum": "BkbY4psgg", "replyto": "BkbY4psgg", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Source code", "comment": "Has the source code for this paper been released by the authors?"}, "nonreaders": ["jonathon@cs.berkeley.edu", "ricshin@cs.berkeley.edu", "dawnsong@cs.berkeley.edu"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Making Neural Programming Architectures Generalize via Recursion", "abstract": "Empirically, neural networks that attempt to learn programs from data have exhibited poor generalizability. Moreover, it has traditionally been difficult to reason about the behavior of these models beyond a certain level of input complexity. In order to address these issues, we propose augmenting neural architectures with a key abstraction: recursion. As an application, we implement recursion in the Neural Programmer-Interpreter framework on four tasks: grade-school addition, bubble sort, topological sort, and quicksort. We demonstrate superior generalizability and interpretability with small amounts of training data. Recursion divides the problem into smaller pieces and drastically reduces the domain of each neural network component, making it tractable to prove guarantees about the overall system\u2019s behavior. Our experience suggests that in order for neural architectures to robustly learn program semantics, it is necessary to incorporate a concept like recursion.", "pdf": "/pdf/342543971002b3e5f08be11d9a6da60b594a6b47.pdf", "paperhash": "cai|making_neural_programming_architectures_generalize_via_recursion", "keywords": ["Deep learning"], "conflicts": ["berkeley.edu"], "authors": ["Jonathon Cai", "Richard Shin", "Dawn Song"], "authorids": ["jonathon@cs.berkeley.edu", "ricshin@cs.berkeley.edu", "dawnsong@cs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287506445, "id": "ICLR.cc/2017/conference/-/paper597/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkbY4psgg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper597/reviewers", "ICLR.cc/2017/conference/paper597/areachairs"], "cdate": 1485287506445}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1489195602756, "tcdate": 1478378616689, "number": 597, "id": "BkbY4psgg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "BkbY4psgg", "signatures": ["~Jonathon_Cai1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Making Neural Programming Architectures Generalize via Recursion", "abstract": "Empirically, neural networks that attempt to learn programs from data have exhibited poor generalizability. Moreover, it has traditionally been difficult to reason about the behavior of these models beyond a certain level of input complexity. In order to address these issues, we propose augmenting neural architectures with a key abstraction: recursion. As an application, we implement recursion in the Neural Programmer-Interpreter framework on four tasks: grade-school addition, bubble sort, topological sort, and quicksort. We demonstrate superior generalizability and interpretability with small amounts of training data. Recursion divides the problem into smaller pieces and drastically reduces the domain of each neural network component, making it tractable to prove guarantees about the overall system\u2019s behavior. Our experience suggests that in order for neural architectures to robustly learn program semantics, it is necessary to incorporate a concept like recursion.", "pdf": "/pdf/342543971002b3e5f08be11d9a6da60b594a6b47.pdf", "paperhash": "cai|making_neural_programming_architectures_generalize_via_recursion", "keywords": ["Deep learning"], "conflicts": ["berkeley.edu"], "authors": ["Jonathon Cai", "Richard Shin", "Dawn Song"], "authorids": ["jonathon@cs.berkeley.edu", "ricshin@cs.berkeley.edu", "dawnsong@cs.berkeley.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396697908, "tcdate": 1486396697908, "number": 1, "id": "BJfQ6GIOe", "invitation": "ICLR.cc/2017/conference/-/paper597/acceptance", "forum": "BkbY4psgg", "replyto": "BkbY4psgg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The reviewers were very favourable, and the paper is on a highly-relevant topic and explores a useful practical trick.", "decision": "Accept (Oral)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Making Neural Programming Architectures Generalize via Recursion", "abstract": "Empirically, neural networks that attempt to learn programs from data have exhibited poor generalizability. Moreover, it has traditionally been difficult to reason about the behavior of these models beyond a certain level of input complexity. In order to address these issues, we propose augmenting neural architectures with a key abstraction: recursion. As an application, we implement recursion in the Neural Programmer-Interpreter framework on four tasks: grade-school addition, bubble sort, topological sort, and quicksort. We demonstrate superior generalizability and interpretability with small amounts of training data. Recursion divides the problem into smaller pieces and drastically reduces the domain of each neural network component, making it tractable to prove guarantees about the overall system\u2019s behavior. Our experience suggests that in order for neural architectures to robustly learn program semantics, it is necessary to incorporate a concept like recursion.", "pdf": "/pdf/342543971002b3e5f08be11d9a6da60b594a6b47.pdf", "paperhash": "cai|making_neural_programming_architectures_generalize_via_recursion", "keywords": ["Deep learning"], "conflicts": ["berkeley.edu"], "authors": ["Jonathon Cai", "Richard Shin", "Dawn Song"], "authorids": ["jonathon@cs.berkeley.edu", "ricshin@cs.berkeley.edu", "dawnsong@cs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396698423, "id": "ICLR.cc/2017/conference/-/paper597/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "BkbY4psgg", "replyto": "BkbY4psgg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396698423}}}, {"tddate": null, "tmdate": 1482435495235, "tcdate": 1482435495235, "number": 6, "id": "HJy3jsKVl", "invitation": "ICLR.cc/2017/conference/-/paper597/public/comment", "forum": "BkbY4psgg", "replyto": "H11ZLC-4l", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "reply to AnonReviewer1", "comment": "Thanks for your comments!\n\nWe do not see any obvious limitations to the proposed model\u2019s ability to infer programs from execution traces, as long as the program semantics is well defined and an adequate training set is used. The model\u2019s ability to learn the program depends heavily on the training set---if the training set is not comprehensive enough, the learned program will not be correct, as the model would lack information about how to handle plausible situations unseen during training. \n\nExecution traces provide very detailed supervision for the model, which makes learning from execution traces much easier than input-output pairs. As we mention in the paper, for future work, an important direction is to reduce the amount of supervision in the training data and to create models that incorporate recursion into the architectures themselves."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Making Neural Programming Architectures Generalize via Recursion", "abstract": "Empirically, neural networks that attempt to learn programs from data have exhibited poor generalizability. Moreover, it has traditionally been difficult to reason about the behavior of these models beyond a certain level of input complexity. In order to address these issues, we propose augmenting neural architectures with a key abstraction: recursion. As an application, we implement recursion in the Neural Programmer-Interpreter framework on four tasks: grade-school addition, bubble sort, topological sort, and quicksort. We demonstrate superior generalizability and interpretability with small amounts of training data. Recursion divides the problem into smaller pieces and drastically reduces the domain of each neural network component, making it tractable to prove guarantees about the overall system\u2019s behavior. Our experience suggests that in order for neural architectures to robustly learn program semantics, it is necessary to incorporate a concept like recursion.", "pdf": "/pdf/342543971002b3e5f08be11d9a6da60b594a6b47.pdf", "paperhash": "cai|making_neural_programming_architectures_generalize_via_recursion", "keywords": ["Deep learning"], "conflicts": ["berkeley.edu"], "authors": ["Jonathon Cai", "Richard Shin", "Dawn Song"], "authorids": ["jonathon@cs.berkeley.edu", "ricshin@cs.berkeley.edu", "dawnsong@cs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287506445, "id": "ICLR.cc/2017/conference/-/paper597/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkbY4psgg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper597/reviewers", "ICLR.cc/2017/conference/paper597/areachairs"], "cdate": 1485287506445}}}, {"tddate": null, "tmdate": 1482435260012, "tcdate": 1482435260012, "number": 5, "id": "BJNTqit4e", "invitation": "ICLR.cc/2017/conference/-/paper597/public/comment", "forum": "BkbY4psgg", "replyto": "ByAJMhb4l", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "reply to AnonReviewer2", "comment": "Thanks for your comments!\n\nYour question about proof over continuous space is similar to one of the questions below, so we refer you to our comment \u201cFeasibility of Verification Procedure\u201d.\n\nWe plan to clean up our source code and release it in the near future."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Making Neural Programming Architectures Generalize via Recursion", "abstract": "Empirically, neural networks that attempt to learn programs from data have exhibited poor generalizability. Moreover, it has traditionally been difficult to reason about the behavior of these models beyond a certain level of input complexity. In order to address these issues, we propose augmenting neural architectures with a key abstraction: recursion. As an application, we implement recursion in the Neural Programmer-Interpreter framework on four tasks: grade-school addition, bubble sort, topological sort, and quicksort. We demonstrate superior generalizability and interpretability with small amounts of training data. Recursion divides the problem into smaller pieces and drastically reduces the domain of each neural network component, making it tractable to prove guarantees about the overall system\u2019s behavior. Our experience suggests that in order for neural architectures to robustly learn program semantics, it is necessary to incorporate a concept like recursion.", "pdf": "/pdf/342543971002b3e5f08be11d9a6da60b594a6b47.pdf", "paperhash": "cai|making_neural_programming_architectures_generalize_via_recursion", "keywords": ["Deep learning"], "conflicts": ["berkeley.edu"], "authors": ["Jonathon Cai", "Richard Shin", "Dawn Song"], "authorids": ["jonathon@cs.berkeley.edu", "ricshin@cs.berkeley.edu", "dawnsong@cs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287506445, "id": "ICLR.cc/2017/conference/-/paper597/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkbY4psgg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper597/reviewers", "ICLR.cc/2017/conference/paper597/areachairs"], "cdate": 1485287506445}}}, {"tddate": null, "tmdate": 1482435033885, "tcdate": 1482434893466, "number": 3, "id": "rJBLFot4g", "invitation": "ICLR.cc/2017/conference/-/paper597/public/comment", "forum": "BkbY4psgg", "replyto": "Bk89RuWEx", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Feasibility of Verification Procedure", "comment": "Thanks for your comment!\n\nWe agree that exhaustively considering all cases in the setting of very large input domains (such as perceptual inputs) is not feasible. In general, it is difficult to reason with 100% certainty about programs involving perceptual inputs. It may be possible, however, to cover many points in the perceptual space that are plausible and to check correctness of the learned program for these inputs. In future work, we would like to consider how to devise a verification procedure more appropriate to problems with very large input domains."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Making Neural Programming Architectures Generalize via Recursion", "abstract": "Empirically, neural networks that attempt to learn programs from data have exhibited poor generalizability. Moreover, it has traditionally been difficult to reason about the behavior of these models beyond a certain level of input complexity. In order to address these issues, we propose augmenting neural architectures with a key abstraction: recursion. As an application, we implement recursion in the Neural Programmer-Interpreter framework on four tasks: grade-school addition, bubble sort, topological sort, and quicksort. We demonstrate superior generalizability and interpretability with small amounts of training data. Recursion divides the problem into smaller pieces and drastically reduces the domain of each neural network component, making it tractable to prove guarantees about the overall system\u2019s behavior. Our experience suggests that in order for neural architectures to robustly learn program semantics, it is necessary to incorporate a concept like recursion.", "pdf": "/pdf/342543971002b3e5f08be11d9a6da60b594a6b47.pdf", "paperhash": "cai|making_neural_programming_architectures_generalize_via_recursion", "keywords": ["Deep learning"], "conflicts": ["berkeley.edu"], "authors": ["Jonathon Cai", "Richard Shin", "Dawn Song"], "authorids": ["jonathon@cs.berkeley.edu", "ricshin@cs.berkeley.edu", "dawnsong@cs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287506445, "id": "ICLR.cc/2017/conference/-/paper597/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkbY4psgg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper597/reviewers", "ICLR.cc/2017/conference/paper597/areachairs"], "cdate": 1485287506445}}}, {"tddate": null, "tmdate": 1482434991137, "tcdate": 1482434991137, "number": 4, "id": "BJwhFjYNx", "invitation": "ICLR.cc/2017/conference/-/paper597/public/comment", "forum": "BkbY4psgg", "replyto": "ryCSIK-Eg", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "added note to Section 3.3", "comment": "We have added a note about this caveat in the paper (end of Section 3.3)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Making Neural Programming Architectures Generalize via Recursion", "abstract": "Empirically, neural networks that attempt to learn programs from data have exhibited poor generalizability. Moreover, it has traditionally been difficult to reason about the behavior of these models beyond a certain level of input complexity. In order to address these issues, we propose augmenting neural architectures with a key abstraction: recursion. As an application, we implement recursion in the Neural Programmer-Interpreter framework on four tasks: grade-school addition, bubble sort, topological sort, and quicksort. We demonstrate superior generalizability and interpretability with small amounts of training data. Recursion divides the problem into smaller pieces and drastically reduces the domain of each neural network component, making it tractable to prove guarantees about the overall system\u2019s behavior. Our experience suggests that in order for neural architectures to robustly learn program semantics, it is necessary to incorporate a concept like recursion.", "pdf": "/pdf/342543971002b3e5f08be11d9a6da60b594a6b47.pdf", "paperhash": "cai|making_neural_programming_architectures_generalize_via_recursion", "keywords": ["Deep learning"], "conflicts": ["berkeley.edu"], "authors": ["Jonathon Cai", "Richard Shin", "Dawn Song"], "authorids": ["jonathon@cs.berkeley.edu", "ricshin@cs.berkeley.edu", "dawnsong@cs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287506445, "id": "ICLR.cc/2017/conference/-/paper597/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkbY4psgg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper597/reviewers", "ICLR.cc/2017/conference/paper597/areachairs"], "cdate": 1485287506445}}}, {"tddate": null, "tmdate": 1481922038571, "tcdate": 1481922038571, "number": 3, "id": "H11ZLC-4l", "invitation": "ICLR.cc/2017/conference/-/paper597/official/review", "forum": "BkbY4psgg", "replyto": "BkbY4psgg", "signatures": ["ICLR.cc/2017/conference/paper597/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper597/AnonReviewer1"], "content": {"title": "A demonstration that NPI can learn to solve Tower of Hanoi!", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper argues that being able to handle recursion is very important for neural programming architectures \u2014 that handling recursion allows for strong generalization to out of domain test cases and learning from smaller amounts of training data.  Most of the paper is a riff on the Reed & de Freitas paper on Neural Programmer Interpreters from ICLR 2016 which learns from program traces \u2014 this paper trains NPI models on traces that have recursive calls.  The authors show how to verify correctness by evaluating the learned program on only a small set of base cases and reduction rules and impressively, show that the NPI architecture is able to perfectly infer Bubblesort and the Tower of Hanoi problems.  \n\nWhat I like is that the idea is super simple and as the authors even mention, the only change is to the execution traces that the training pipeline gets to see.  I\u2019m actually not sure what the right take-away is \u2014 does this mean that we have effectively solved the neural programming problem when the execution traces are available? (and was the problem too easy to begin with?).    For example, a larger input domain (as one of the reviewers also mentions) is MNIST digits and we can imagine a problem where the NPI must infer how to sort MNIST digits from highest to lowest.  In this setting, having execution traces would effectively decouple the problem of recognizing the digits from that of inferring the program logic \u2014 and so the problem would be no harder than learning to recognize MNIST digits and learning to bubble sort from symbols.  What is a problem where we have access to execution traces but cannot infer it using the proposed method?\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Making Neural Programming Architectures Generalize via Recursion", "abstract": "Empirically, neural networks that attempt to learn programs from data have exhibited poor generalizability. Moreover, it has traditionally been difficult to reason about the behavior of these models beyond a certain level of input complexity. In order to address these issues, we propose augmenting neural architectures with a key abstraction: recursion. As an application, we implement recursion in the Neural Programmer-Interpreter framework on four tasks: grade-school addition, bubble sort, topological sort, and quicksort. We demonstrate superior generalizability and interpretability with small amounts of training data. Recursion divides the problem into smaller pieces and drastically reduces the domain of each neural network component, making it tractable to prove guarantees about the overall system\u2019s behavior. Our experience suggests that in order for neural architectures to robustly learn program semantics, it is necessary to incorporate a concept like recursion.", "pdf": "/pdf/342543971002b3e5f08be11d9a6da60b594a6b47.pdf", "paperhash": "cai|making_neural_programming_architectures_generalize_via_recursion", "keywords": ["Deep learning"], "conflicts": ["berkeley.edu"], "authors": ["Jonathon Cai", "Richard Shin", "Dawn Song"], "authorids": ["jonathon@cs.berkeley.edu", "ricshin@cs.berkeley.edu", "dawnsong@cs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512528759, "id": "ICLR.cc/2017/conference/-/paper597/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper597/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper597/AnonReviewer3", "ICLR.cc/2017/conference/paper597/AnonReviewer2", "ICLR.cc/2017/conference/paper597/AnonReviewer1"], "reply": {"forum": "BkbY4psgg", "replyto": "BkbY4psgg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper597/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper597/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512528759}}}, {"tddate": null, "tmdate": 1481912805848, "tcdate": 1481912805848, "number": 2, "id": "ByAJMhb4l", "invitation": "ICLR.cc/2017/conference/-/paper597/official/review", "forum": "BkbY4psgg", "replyto": "BkbY4psgg", "signatures": ["ICLR.cc/2017/conference/paper597/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper597/AnonReviewer2"], "content": {"title": "Nifty extension to make NPI more practical", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This is a very interesting and fairly easy to read paper. \nThe authors present a small, yet nifty approach to make Neural Programming Interpreters significantly more powerful. By allowing recursion, NPI generalizes better from fewer execution traces.\nIt's an interesting example of how a small but non-trivial extension can make a machine learning method significantly more practical.\n\nI also appreciate that the same notation was used in this paper and the original Deepmind paper. As a non-expert on this topic, it was easy to read the original paper in tandem. \n\nMy one point of critique is that the generalization proves are a bit vague. For the numerical examples in the paper, you can iterate over all possible execution paths until the next recursive call. However, how would this approach generalize a continuous input space (e.g. the 3D car example in the original paper). It seems that a prove of generalization will still be intractable in the continuous case? \n\nAre you planning on releasing the source code?", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Making Neural Programming Architectures Generalize via Recursion", "abstract": "Empirically, neural networks that attempt to learn programs from data have exhibited poor generalizability. Moreover, it has traditionally been difficult to reason about the behavior of these models beyond a certain level of input complexity. In order to address these issues, we propose augmenting neural architectures with a key abstraction: recursion. As an application, we implement recursion in the Neural Programmer-Interpreter framework on four tasks: grade-school addition, bubble sort, topological sort, and quicksort. We demonstrate superior generalizability and interpretability with small amounts of training data. Recursion divides the problem into smaller pieces and drastically reduces the domain of each neural network component, making it tractable to prove guarantees about the overall system\u2019s behavior. Our experience suggests that in order for neural architectures to robustly learn program semantics, it is necessary to incorporate a concept like recursion.", "pdf": "/pdf/342543971002b3e5f08be11d9a6da60b594a6b47.pdf", "paperhash": "cai|making_neural_programming_architectures_generalize_via_recursion", "keywords": ["Deep learning"], "conflicts": ["berkeley.edu"], "authors": ["Jonathon Cai", "Richard Shin", "Dawn Song"], "authorids": ["jonathon@cs.berkeley.edu", "ricshin@cs.berkeley.edu", "dawnsong@cs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512528759, "id": "ICLR.cc/2017/conference/-/paper597/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper597/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper597/AnonReviewer3", "ICLR.cc/2017/conference/paper597/AnonReviewer2", "ICLR.cc/2017/conference/paper597/AnonReviewer1"], "reply": {"forum": "BkbY4psgg", "replyto": "BkbY4psgg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper597/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper597/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512528759}}}, {"tddate": null, "tmdate": 1481901637574, "tcdate": 1481901637574, "number": 1, "id": "ryCSIK-Eg", "invitation": "ICLR.cc/2017/conference/-/paper597/official/comment", "forum": "BkbY4psgg", "replyto": "Bk89RuWEx", "signatures": ["ICLR.cc/2017/conference/paper597/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper597/AnonReviewer3"], "content": {"title": "proving vs testing", "comment": "Agree, for perceptual inputs the strategy of exhaustively considering all of the cases will not be feasible, which should be clearly noted as a caveat in the paper if it is not currently. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Making Neural Programming Architectures Generalize via Recursion", "abstract": "Empirically, neural networks that attempt to learn programs from data have exhibited poor generalizability. Moreover, it has traditionally been difficult to reason about the behavior of these models beyond a certain level of input complexity. In order to address these issues, we propose augmenting neural architectures with a key abstraction: recursion. As an application, we implement recursion in the Neural Programmer-Interpreter framework on four tasks: grade-school addition, bubble sort, topological sort, and quicksort. We demonstrate superior generalizability and interpretability with small amounts of training data. Recursion divides the problem into smaller pieces and drastically reduces the domain of each neural network component, making it tractable to prove guarantees about the overall system\u2019s behavior. Our experience suggests that in order for neural architectures to robustly learn program semantics, it is necessary to incorporate a concept like recursion.", "pdf": "/pdf/342543971002b3e5f08be11d9a6da60b594a6b47.pdf", "paperhash": "cai|making_neural_programming_architectures_generalize_via_recursion", "keywords": ["Deep learning"], "conflicts": ["berkeley.edu"], "authors": ["Jonathon Cai", "Richard Shin", "Dawn Song"], "authorids": ["jonathon@cs.berkeley.edu", "ricshin@cs.berkeley.edu", "dawnsong@cs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287506320, "id": "ICLR.cc/2017/conference/-/paper597/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "BkbY4psgg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper597/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper597/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper597/reviewers", "ICLR.cc/2017/conference/paper597/areachairs"], "cdate": 1485287506320}}}, {"tddate": null, "tmdate": 1481899662532, "tcdate": 1481899662532, "number": 2, "id": "Bk89RuWEx", "invitation": "ICLR.cc/2017/conference/-/paper597/public/comment", "forum": "BkbY4psgg", "replyto": "BkrguybEe", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "\"Proving\" vs \"Testing\"", "comment": "I like this paper as well, and think it is a valuable improvement over the NPI baseline.\n\nHowever, even on first reading, I was annoyed by the use of the term \"prove\". The paper proves that under certain circumstances, _testing_ on a finite set of values is sufficient to establish correctness for whole classes of inputs. However, this does not amount to a generally applicable proof strategy. It is only feasible because the input domain is quite simple, and only a handful of cases need to be considered. However, this quickly becomes problematic when the input domain expands, e.g., when one-hot encodings of digits are replaced by MNIST digits. I would expect NPI (and the recursive extension presented here) to have no problem to handle this with an appropriately structured domain-specific encoder, but the \"proof\" strategy described in this paper would not work anymore. Thus, I am not convinced of the practical value of this part of the contribution, as it seems to be unfeasible for everything but toy paper examples.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Making Neural Programming Architectures Generalize via Recursion", "abstract": "Empirically, neural networks that attempt to learn programs from data have exhibited poor generalizability. Moreover, it has traditionally been difficult to reason about the behavior of these models beyond a certain level of input complexity. In order to address these issues, we propose augmenting neural architectures with a key abstraction: recursion. As an application, we implement recursion in the Neural Programmer-Interpreter framework on four tasks: grade-school addition, bubble sort, topological sort, and quicksort. We demonstrate superior generalizability and interpretability with small amounts of training data. Recursion divides the problem into smaller pieces and drastically reduces the domain of each neural network component, making it tractable to prove guarantees about the overall system\u2019s behavior. Our experience suggests that in order for neural architectures to robustly learn program semantics, it is necessary to incorporate a concept like recursion.", "pdf": "/pdf/342543971002b3e5f08be11d9a6da60b594a6b47.pdf", "paperhash": "cai|making_neural_programming_architectures_generalize_via_recursion", "keywords": ["Deep learning"], "conflicts": ["berkeley.edu"], "authors": ["Jonathon Cai", "Richard Shin", "Dawn Song"], "authorids": ["jonathon@cs.berkeley.edu", "ricshin@cs.berkeley.edu", "dawnsong@cs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287506445, "id": "ICLR.cc/2017/conference/-/paper597/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkbY4psgg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper597/reviewers", "ICLR.cc/2017/conference/paper597/areachairs"], "cdate": 1485287506445}}}, {"tddate": null, "tmdate": 1481861100868, "tcdate": 1481861100868, "number": 1, "id": "BkrguybEe", "invitation": "ICLR.cc/2017/conference/-/paper597/official/review", "forum": "BkbY4psgg", "replyto": "BkbY4psgg", "signatures": ["ICLR.cc/2017/conference/paper597/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper597/AnonReviewer3"], "content": {"title": "Greatly improved training and analysis of NPI", "rating": "9: Top 15% of accepted papers, strong accept", "review": "This paper improves significantly upon the original NPI work, showing that the model generalizes far better when trained on traces in recursive form. The authors show better sample complexity and generalization results for addition and bubblesort programs, and add two new and more interesting tasks - topological sort and quicksort (added based on reviewer discussion). Furthermore, they actually *prove* that the algorithms learned by the model generalize perfectly, which to my knowledge is the first time this has been done in neural program induction.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Making Neural Programming Architectures Generalize via Recursion", "abstract": "Empirically, neural networks that attempt to learn programs from data have exhibited poor generalizability. Moreover, it has traditionally been difficult to reason about the behavior of these models beyond a certain level of input complexity. In order to address these issues, we propose augmenting neural architectures with a key abstraction: recursion. As an application, we implement recursion in the Neural Programmer-Interpreter framework on four tasks: grade-school addition, bubble sort, topological sort, and quicksort. We demonstrate superior generalizability and interpretability with small amounts of training data. Recursion divides the problem into smaller pieces and drastically reduces the domain of each neural network component, making it tractable to prove guarantees about the overall system\u2019s behavior. Our experience suggests that in order for neural architectures to robustly learn program semantics, it is necessary to incorporate a concept like recursion.", "pdf": "/pdf/342543971002b3e5f08be11d9a6da60b594a6b47.pdf", "paperhash": "cai|making_neural_programming_architectures_generalize_via_recursion", "keywords": ["Deep learning"], "conflicts": ["berkeley.edu"], "authors": ["Jonathon Cai", "Richard Shin", "Dawn Song"], "authorids": ["jonathon@cs.berkeley.edu", "ricshin@cs.berkeley.edu", "dawnsong@cs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512528759, "id": "ICLR.cc/2017/conference/-/paper597/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper597/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper597/AnonReviewer3", "ICLR.cc/2017/conference/paper597/AnonReviewer2", "ICLR.cc/2017/conference/paper597/AnonReviewer1"], "reply": {"forum": "BkbY4psgg", "replyto": "BkbY4psgg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper597/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper597/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512528759}}}, {"tddate": null, "tmdate": 1481223725806, "tcdate": 1481223709299, "number": 1, "id": "S1BmAQwXe", "invitation": "ICLR.cc/2017/conference/-/paper597/public/comment", "forum": "BkbY4psgg", "replyto": "r1mKPw0Ge", "signatures": ["~Jonathon_Cai1"], "readers": ["everyone"], "writers": ["~Jonathon_Cai1"], "content": {"title": "Updated Paper with Divide and Conquer Task", "comment": "Thank you for your question!\n\nYes, the recursive traces are helpful for learning more sophisticated algorithms. It is natural to implement divide and conquer type algorithms with recursive calls, so we can construct recursive traces and feed those as training data to the model. It is not so hard to implement quick sort in this way -- we have updated our paper with this task. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Making Neural Programming Architectures Generalize via Recursion", "abstract": "Empirically, neural networks that attempt to learn programs from data have exhibited poor generalizability. Moreover, it has traditionally been difficult to reason about the behavior of these models beyond a certain level of input complexity. In order to address these issues, we propose augmenting neural architectures with a key abstraction: recursion. As an application, we implement recursion in the Neural Programmer-Interpreter framework on four tasks: grade-school addition, bubble sort, topological sort, and quicksort. We demonstrate superior generalizability and interpretability with small amounts of training data. Recursion divides the problem into smaller pieces and drastically reduces the domain of each neural network component, making it tractable to prove guarantees about the overall system\u2019s behavior. Our experience suggests that in order for neural architectures to robustly learn program semantics, it is necessary to incorporate a concept like recursion.", "pdf": "/pdf/342543971002b3e5f08be11d9a6da60b594a6b47.pdf", "paperhash": "cai|making_neural_programming_architectures_generalize_via_recursion", "keywords": ["Deep learning"], "conflicts": ["berkeley.edu"], "authors": ["Jonathon Cai", "Richard Shin", "Dawn Song"], "authorids": ["jonathon@cs.berkeley.edu", "ricshin@cs.berkeley.edu", "dawnsong@cs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287506445, "id": "ICLR.cc/2017/conference/-/paper597/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkbY4psgg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper597/reviewers", "ICLR.cc/2017/conference/paper597/areachairs"], "cdate": 1485287506445}}}, {"tddate": null, "tmdate": 1480648571478, "tcdate": 1480648571473, "number": 1, "id": "r1mKPw0Ge", "invitation": "ICLR.cc/2017/conference/-/paper597/pre-review/question", "forum": "BkbY4psgg", "replyto": "BkbY4psgg", "signatures": ["ICLR.cc/2017/conference/paper597/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper597/AnonReviewer3"], "content": {"title": "Application to other tasks", "question": "Nice work! I am glad to see an improved version of NPI. One question - do you think this approach would be helpful in learning divide and conquer type algorithms such as quicksort? The simple problems in the original NPI paper had fairly local structure, e.g. keeping two pointers nearby in an array and swapping their associated values in bubblesort, for instance. I wonder if the recursive form of learning from traces could allow NPI to learn more sophisticated algorithms.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Making Neural Programming Architectures Generalize via Recursion", "abstract": "Empirically, neural networks that attempt to learn programs from data have exhibited poor generalizability. Moreover, it has traditionally been difficult to reason about the behavior of these models beyond a certain level of input complexity. In order to address these issues, we propose augmenting neural architectures with a key abstraction: recursion. As an application, we implement recursion in the Neural Programmer-Interpreter framework on four tasks: grade-school addition, bubble sort, topological sort, and quicksort. We demonstrate superior generalizability and interpretability with small amounts of training data. Recursion divides the problem into smaller pieces and drastically reduces the domain of each neural network component, making it tractable to prove guarantees about the overall system\u2019s behavior. Our experience suggests that in order for neural architectures to robustly learn program semantics, it is necessary to incorporate a concept like recursion.", "pdf": "/pdf/342543971002b3e5f08be11d9a6da60b594a6b47.pdf", "paperhash": "cai|making_neural_programming_architectures_generalize_via_recursion", "keywords": ["Deep learning"], "conflicts": ["berkeley.edu"], "authors": ["Jonathon Cai", "Richard Shin", "Dawn Song"], "authorids": ["jonathon@cs.berkeley.edu", "ricshin@cs.berkeley.edu", "dawnsong@cs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959195236, "id": "ICLR.cc/2017/conference/-/paper597/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper597/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper597/AnonReviewer3"], "reply": {"forum": "BkbY4psgg", "replyto": "BkbY4psgg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper597/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper597/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959195236}}}], "count": 14}