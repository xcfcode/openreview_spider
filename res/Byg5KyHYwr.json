{"notes": [{"id": "Byg5KyHYwr", "original": "HkgFXDC_DS", "number": 1851, "cdate": 1569439618013, "ddate": null, "tcdate": 1569439618013, "tmdate": 1577168284932, "tddate": null, "forum": "Byg5KyHYwr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["guoyijie@umich.edu", "jwook@umich.edu", "moczulski@google.com", "bengio@google.com", "mnorouzi@google.com", "honglak@google.com"], "title": "Self-Imitation Learning via Trajectory-Conditioned Policy for Hard-Exploration Tasks", "authors": ["Yijie Guo", "Jongwook Choi", "Marcin Moczulski", "Samy Bengio", "Mohammad Norouzi", "Honglak Lee"], "pdf": "/pdf/574d0a4dfa347dfe9cefed9f9ce8d3fa23ec61e7.pdf", "TL;DR": "Self-imitation learning of diverse trajectories with trajectory-conditioned policy", "abstract": "Imitation learning from human-expert demonstrations has been shown to be greatly helpful for challenging reinforcement learning problems with sparse environment rewards. However, it is very difficult to achieve similar success without relying on expert demonstrations. Recent works on self-imitation learning showed that imitating the agent's own past good experience could indirectly drive exploration in some environments, but these methods often lead to sub-optimal and myopic behavior. To address this issue, we argue that exploration in diverse directions by imitating diverse trajectories, instead of focusing on limited good trajectories, is more desirable for the hard-exploration tasks. We propose a new method of learning a trajectory-conditioned policy to imitate diverse trajectories from the agent's own past experiences and show that such self-imitation helps avoid myopic behavior and increases the chance of finding a globally optimal solution for hard-exploration tasks, especially when there are misleading rewards. Our method significantly outperforms existing self-imitation learning and count-based exploration methods on various hard-exploration tasks with local optima. In particular, we report a state-of-the-art score of more than 20,000 points on Montezumas Revenge without using expert demonstrations or resetting to arbitrary states.", "keywords": ["imitation learning", "hard-exploration tasks", "exploration and exploitation"], "paperhash": "guo|selfimitation_learning_via_trajectoryconditioned_policy_for_hardexploration_tasks", "original_pdf": "/attachment/e0b09c9e468f04046bdf6d089c77de0e2d7c0a5b.pdf", "_bibtex": "@misc{\nguo2020selfimitation,\ntitle={Self-Imitation Learning via Trajectory-Conditioned Policy for Hard-Exploration Tasks},\nauthor={Yijie Guo and Jongwook Choi and Marcin Moczulski and Samy Bengio and Mohammad Norouzi and Honglak Lee},\nyear={2020},\nurl={https://openreview.net/forum?id=Byg5KyHYwr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "Iur9nqT9Nl", "original": null, "number": 1, "cdate": 1576798734107, "ddate": null, "tcdate": 1576798734107, "tmdate": 1576800902274, "tddate": null, "forum": "Byg5KyHYwr", "replyto": "Byg5KyHYwr", "invitation": "ICLR.cc/2020/Conference/Paper1851/-/Decision", "content": {"decision": "Reject", "comment": "This paper addresses the problem of exploration in challenging RL environments using self-imitation learning. The idea behind the proposed approach is for the agent to imitate a diverse set of its own past trajectories. To achieve this, the authors introduce a policy conditioned on trajectories. The proposed approach is evaluated on various domains including Atari Montezuma's Revenge and MuJoCo.\n\nGiven that the evaluation is purely empirical, the major concern is in the design of experiments. The amount of stochasticity induced by the random initial state alone does not lead to convincing results regarding the performance of the proposed approach compared with baselines (e.g. Go-Explore). With such simple stochasticity, it is not clear why one could not use a model to recover from it and then rely on an existing technique like Go-Explore. Although this paper tackles an important problem (hard-exploration RL tasks), all reviewers agreed that this limitation is crucial and I therefore recommend to reject this paper.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["guoyijie@umich.edu", "jwook@umich.edu", "moczulski@google.com", "bengio@google.com", "mnorouzi@google.com", "honglak@google.com"], "title": "Self-Imitation Learning via Trajectory-Conditioned Policy for Hard-Exploration Tasks", "authors": ["Yijie Guo", "Jongwook Choi", "Marcin Moczulski", "Samy Bengio", "Mohammad Norouzi", "Honglak Lee"], "pdf": "/pdf/574d0a4dfa347dfe9cefed9f9ce8d3fa23ec61e7.pdf", "TL;DR": "Self-imitation learning of diverse trajectories with trajectory-conditioned policy", "abstract": "Imitation learning from human-expert demonstrations has been shown to be greatly helpful for challenging reinforcement learning problems with sparse environment rewards. However, it is very difficult to achieve similar success without relying on expert demonstrations. Recent works on self-imitation learning showed that imitating the agent's own past good experience could indirectly drive exploration in some environments, but these methods often lead to sub-optimal and myopic behavior. To address this issue, we argue that exploration in diverse directions by imitating diverse trajectories, instead of focusing on limited good trajectories, is more desirable for the hard-exploration tasks. We propose a new method of learning a trajectory-conditioned policy to imitate diverse trajectories from the agent's own past experiences and show that such self-imitation helps avoid myopic behavior and increases the chance of finding a globally optimal solution for hard-exploration tasks, especially when there are misleading rewards. Our method significantly outperforms existing self-imitation learning and count-based exploration methods on various hard-exploration tasks with local optima. In particular, we report a state-of-the-art score of more than 20,000 points on Montezumas Revenge without using expert demonstrations or resetting to arbitrary states.", "keywords": ["imitation learning", "hard-exploration tasks", "exploration and exploitation"], "paperhash": "guo|selfimitation_learning_via_trajectoryconditioned_policy_for_hardexploration_tasks", "original_pdf": "/attachment/e0b09c9e468f04046bdf6d089c77de0e2d7c0a5b.pdf", "_bibtex": "@misc{\nguo2020selfimitation,\ntitle={Self-Imitation Learning via Trajectory-Conditioned Policy for Hard-Exploration Tasks},\nauthor={Yijie Guo and Jongwook Choi and Marcin Moczulski and Samy Bengio and Mohammad Norouzi and Honglak Lee},\nyear={2020},\nurl={https://openreview.net/forum?id=Byg5KyHYwr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Byg5KyHYwr", "replyto": "Byg5KyHYwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795708684, "tmdate": 1576800257193, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1851/-/Decision"}}}, {"id": "SyxtVdWg9S", "original": null, "number": 3, "cdate": 1571981361167, "ddate": null, "tcdate": 1571981361167, "tmdate": 1574652787831, "tddate": null, "forum": "Byg5KyHYwr", "replyto": "Byg5KyHYwr", "invitation": "ICLR.cc/2020/Conference/Paper1851/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #1", "review": "Note: the style-formatting of this paper has been heavily tweaked, and so the evaluation should be calibrated for a 9-page paper.\n\nThis paper proposes an approach for diverse self-imitation for hard exploration problems.  The idea is leverage recently proposed self-imitation approaches for learning to imitate good trajectories generated by the policy itself.  By encouraging diversity in the pool of trajectories for self-imitation, the idea is to encourage faster learner -- this basic concept is also used in approaches like prioritized experience replay, albeit at the entire trajectory level rather than individual state/action level.  \n\nThe authors view this approach as a generalization of Go-Explore, since it does not rely on having a reset mechanism.  However, I think this discussion has a lot of subtle nuances pertaining to the stochasticity of the environment (which the authors acknowledge).  For instance, if the environment is deterministic, then why not just do something like Go-Explore, since state-reset is just memorizing a deterministic action sequence? \n\nThe empirical results are very strong, achieving state-of-the-art results for any approach not reliant on a reset mechanism.  All the primary experiments appear to be for deterministic environments.  The results on stochastic environments (in the Appendix) seem pretty weak (but please correct me if I'm mistaken here).  So one major question is whether Go-Explore is a scientifically appropriate benchmark to compare with for this setting.  \n\nIn summary, I'm willing to be convinced that this is an interesting and scientifically novel result.  I have some concerns as expressed above.\n\n\n**** After Author Response ****\nThanks for the response.  I'm willing to raise my score to weak accept.  \n\nI think the authors did a reasonable job addressing my specific questions.  Some further reflection revealed to me that there is a huge opportunity to scientifically investigate how stochasticity impacts the proposed algorithm.  For instance, one could conduct a systematic study (say of the Apple domain) where one varies the degree of stochasticity and measures how the performance the proposed algorithm changes, perhaps relative to Go-Explore on the purely deterministic version of the environment.  It seems a bit of a cop-out to say that Go-Explore is not applicable, and misses out a huge opportunity for real scientific understanding.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1851/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1851/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["guoyijie@umich.edu", "jwook@umich.edu", "moczulski@google.com", "bengio@google.com", "mnorouzi@google.com", "honglak@google.com"], "title": "Self-Imitation Learning via Trajectory-Conditioned Policy for Hard-Exploration Tasks", "authors": ["Yijie Guo", "Jongwook Choi", "Marcin Moczulski", "Samy Bengio", "Mohammad Norouzi", "Honglak Lee"], "pdf": "/pdf/574d0a4dfa347dfe9cefed9f9ce8d3fa23ec61e7.pdf", "TL;DR": "Self-imitation learning of diverse trajectories with trajectory-conditioned policy", "abstract": "Imitation learning from human-expert demonstrations has been shown to be greatly helpful for challenging reinforcement learning problems with sparse environment rewards. However, it is very difficult to achieve similar success without relying on expert demonstrations. Recent works on self-imitation learning showed that imitating the agent's own past good experience could indirectly drive exploration in some environments, but these methods often lead to sub-optimal and myopic behavior. To address this issue, we argue that exploration in diverse directions by imitating diverse trajectories, instead of focusing on limited good trajectories, is more desirable for the hard-exploration tasks. We propose a new method of learning a trajectory-conditioned policy to imitate diverse trajectories from the agent's own past experiences and show that such self-imitation helps avoid myopic behavior and increases the chance of finding a globally optimal solution for hard-exploration tasks, especially when there are misleading rewards. Our method significantly outperforms existing self-imitation learning and count-based exploration methods on various hard-exploration tasks with local optima. In particular, we report a state-of-the-art score of more than 20,000 points on Montezumas Revenge without using expert demonstrations or resetting to arbitrary states.", "keywords": ["imitation learning", "hard-exploration tasks", "exploration and exploitation"], "paperhash": "guo|selfimitation_learning_via_trajectoryconditioned_policy_for_hardexploration_tasks", "original_pdf": "/attachment/e0b09c9e468f04046bdf6d089c77de0e2d7c0a5b.pdf", "_bibtex": "@misc{\nguo2020selfimitation,\ntitle={Self-Imitation Learning via Trajectory-Conditioned Policy for Hard-Exploration Tasks},\nauthor={Yijie Guo and Jongwook Choi and Marcin Moczulski and Samy Bengio and Mohammad Norouzi and Honglak Lee},\nyear={2020},\nurl={https://openreview.net/forum?id=Byg5KyHYwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Byg5KyHYwr", "replyto": "Byg5KyHYwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1851/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1851/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575647158165, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1851/Reviewers"], "noninvitees": [], "tcdate": 1570237731386, "tmdate": 1575647158180, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1851/-/Official_Review"}}}, {"id": "HyeWXOo3iH", "original": null, "number": 13, "cdate": 1573857304920, "ddate": null, "tcdate": 1573857304920, "tmdate": 1573857304920, "tddate": null, "forum": "Byg5KyHYwr", "replyto": "HJxc0JhjsH", "invitation": "ICLR.cc/2020/Conference/Paper1851/-/Official_Comment", "content": {"title": "Significance needs to be demonstrated rather than suggested", "comment": "This reviewer agrees with the authors on the significance of the challenge of hard-exploration problems in stochastic where neither state-reset functionality nor human demonstrations are available. Please do keep working in this area and continue to recruit others to work on this challenge.\n\nWe need this field to move beyond task formulations that admit unsatisfactory-feeling solutions (Go-Explore). They way to get there is not to ignore or avoid using the exploits they used, but to shift our attention to tasks where those exploits no longer work. After Go-Explore, work that attempts to address the exploration challenge needs to somehow get in contact with a challenge that this algorithm can't address. Not all experiments need to be run in the extra-challenging domain, but at least some should to ensure we are addressing the real problem rather than just what remains after the initial exploit pathways have been removed."}, "signatures": ["ICLR.cc/2020/Conference/Paper1851/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1851/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["guoyijie@umich.edu", "jwook@umich.edu", "moczulski@google.com", "bengio@google.com", "mnorouzi@google.com", "honglak@google.com"], "title": "Self-Imitation Learning via Trajectory-Conditioned Policy for Hard-Exploration Tasks", "authors": ["Yijie Guo", "Jongwook Choi", "Marcin Moczulski", "Samy Bengio", "Mohammad Norouzi", "Honglak Lee"], "pdf": "/pdf/574d0a4dfa347dfe9cefed9f9ce8d3fa23ec61e7.pdf", "TL;DR": "Self-imitation learning of diverse trajectories with trajectory-conditioned policy", "abstract": "Imitation learning from human-expert demonstrations has been shown to be greatly helpful for challenging reinforcement learning problems with sparse environment rewards. However, it is very difficult to achieve similar success without relying on expert demonstrations. Recent works on self-imitation learning showed that imitating the agent's own past good experience could indirectly drive exploration in some environments, but these methods often lead to sub-optimal and myopic behavior. To address this issue, we argue that exploration in diverse directions by imitating diverse trajectories, instead of focusing on limited good trajectories, is more desirable for the hard-exploration tasks. We propose a new method of learning a trajectory-conditioned policy to imitate diverse trajectories from the agent's own past experiences and show that such self-imitation helps avoid myopic behavior and increases the chance of finding a globally optimal solution for hard-exploration tasks, especially when there are misleading rewards. Our method significantly outperforms existing self-imitation learning and count-based exploration methods on various hard-exploration tasks with local optima. In particular, we report a state-of-the-art score of more than 20,000 points on Montezumas Revenge without using expert demonstrations or resetting to arbitrary states.", "keywords": ["imitation learning", "hard-exploration tasks", "exploration and exploitation"], "paperhash": "guo|selfimitation_learning_via_trajectoryconditioned_policy_for_hardexploration_tasks", "original_pdf": "/attachment/e0b09c9e468f04046bdf6d089c77de0e2d7c0a5b.pdf", "_bibtex": "@misc{\nguo2020selfimitation,\ntitle={Self-Imitation Learning via Trajectory-Conditioned Policy for Hard-Exploration Tasks},\nauthor={Yijie Guo and Jongwook Choi and Marcin Moczulski and Samy Bengio and Mohammad Norouzi and Honglak Lee},\nyear={2020},\nurl={https://openreview.net/forum?id=Byg5KyHYwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Byg5KyHYwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1851/Authors", "ICLR.cc/2020/Conference/Paper1851/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1851/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1851/Reviewers", "ICLR.cc/2020/Conference/Paper1851/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1851/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1851/Authors|ICLR.cc/2020/Conference/Paper1851/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149978, "tmdate": 1576860533482, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1851/Authors", "ICLR.cc/2020/Conference/Paper1851/Reviewers", "ICLR.cc/2020/Conference/Paper1851/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1851/-/Official_Comment"}}}, {"id": "HJxc0JhjsH", "original": null, "number": 8, "cdate": 1573793746342, "ddate": null, "tcdate": 1573793746342, "tmdate": 1573793911475, "tddate": null, "forum": "Byg5KyHYwr", "replyto": "ryexs85aYS", "invitation": "ICLR.cc/2020/Conference/Paper1851/-/Official_Comment", "content": {"title": "Response to Review #2 (Part 2/2)", "comment": "Second, it is true that \u201cPure-exploration algorithms (Go-Explore), not burdened by interleaving policy learning, achieve far superior scores\u201d on Montezuma\u2019s Revenge with the direct state-reset function and deterministic environments. However, on Montezuma\u2019s Revenge without such strong assumptions, which has been well-known as a hard-exploration game for a long time and has been studied by many previous works (Ostrovski et al., 2017; Tang et al., 2017; Burda et al., 2018; Pohlen et al., 2018), to our best knowledge, our work is the first to successfully train the agent to consistently proceed to the second floor and achieve a score over 20,000 without the help of human expert demonstrations. Perhaps our method is not quite simple, but there is no simpler method yet for Montezuma\u2019s Revenge (a notorious sparse-reward problem with moderate stochasticity) without relying on any of state-reset function or human expert demonstrations that can achieve a competitive score to ours. \n\nIn conclusion, we are studying a more difficult problem than Go-Explore. For this problem, no previous work (including Go-Explore, as discussed in Appendix H) could perform better than our method. Therefore, we believe the existence of Go-Explore should not be the ground for a rejection or undervaluing our work. \n\nPlease note that our paper does not aim solely at solving Montezuma\u2019s Revenge. We would like to study hard-exploration tasks with sparse and misleading rewards. We selected Montezuma\u2019s Revenge, which is a notoriously difficult hard-exploration environment in the literature, as one instance. To fully support our conclusion, we also conducted environments on other interesting domains, including Apple-Gold, Deep Sea and MuJoco maze. We expect that our method could work for real-world tasks such as robotic manipulation, but we leave as future work.\n\nWe would like to ask the reviewer to reconsider the significance and difficulty of the problems we are studying. We strongly believe that the hard-exploration problem (such as Montezuma\u2019s Revenge) without a state-reset function, without human expert demonstration and with stochasticity (both in terms of initial state and consequences of taken actions) is a very difficult problem. We are not aware of any publications approaching it with comparable success. We appreciate your time and reconsideration.\n\nReferences:\nKulkarni, T. D., Narasimhan, K., Saeedi, A., & Tenenbaum, J. (2016). Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. In Advances in neural information processing systems (pp. 3675-3683).\nLiu, E. Z., Keramati, R., Seshadri, S., Guu, K., Pasupat, P., Brunskill, E., & Liang, P. (2018). Learning Abstract Models for Long-Horizon Exploration.\nOstrovski, G., Bellemare, M. G., van den Oord, A., & Munos, R. (2017, August). Count-based exploration with neural density models. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 (pp. 2721-2730). JMLR. org.\nTang, H., Houthooft, R., Foote, D., Stooke, A., Chen, O. X., Duan, Y., ... & Abbeel, P. (2017). # Exploration: A study of count-based exploration for deep reinforcement learning. In Advances in neural information processing systems (pp. 2753-2762).\nBurda, Y., Edwards, H., Storkey, A., & Klimov, O. (2018). Exploration by random network distillation. arXiv preprint arXiv:1810.12894.\nPohlen, T., Piot, B., Hester, T., Azar, M. G., Horgan, D., Budden, D., ... & Hessel, M. (2018). Observe and look further: Achieving consistent performance on atari. arXiv preprint arXiv:1805.11593.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1851/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1851/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["guoyijie@umich.edu", "jwook@umich.edu", "moczulski@google.com", "bengio@google.com", "mnorouzi@google.com", "honglak@google.com"], "title": "Self-Imitation Learning via Trajectory-Conditioned Policy for Hard-Exploration Tasks", "authors": ["Yijie Guo", "Jongwook Choi", "Marcin Moczulski", "Samy Bengio", "Mohammad Norouzi", "Honglak Lee"], "pdf": "/pdf/574d0a4dfa347dfe9cefed9f9ce8d3fa23ec61e7.pdf", "TL;DR": "Self-imitation learning of diverse trajectories with trajectory-conditioned policy", "abstract": "Imitation learning from human-expert demonstrations has been shown to be greatly helpful for challenging reinforcement learning problems with sparse environment rewards. However, it is very difficult to achieve similar success without relying on expert demonstrations. Recent works on self-imitation learning showed that imitating the agent's own past good experience could indirectly drive exploration in some environments, but these methods often lead to sub-optimal and myopic behavior. To address this issue, we argue that exploration in diverse directions by imitating diverse trajectories, instead of focusing on limited good trajectories, is more desirable for the hard-exploration tasks. We propose a new method of learning a trajectory-conditioned policy to imitate diverse trajectories from the agent's own past experiences and show that such self-imitation helps avoid myopic behavior and increases the chance of finding a globally optimal solution for hard-exploration tasks, especially when there are misleading rewards. Our method significantly outperforms existing self-imitation learning and count-based exploration methods on various hard-exploration tasks with local optima. In particular, we report a state-of-the-art score of more than 20,000 points on Montezumas Revenge without using expert demonstrations or resetting to arbitrary states.", "keywords": ["imitation learning", "hard-exploration tasks", "exploration and exploitation"], "paperhash": "guo|selfimitation_learning_via_trajectoryconditioned_policy_for_hardexploration_tasks", "original_pdf": "/attachment/e0b09c9e468f04046bdf6d089c77de0e2d7c0a5b.pdf", "_bibtex": "@misc{\nguo2020selfimitation,\ntitle={Self-Imitation Learning via Trajectory-Conditioned Policy for Hard-Exploration Tasks},\nauthor={Yijie Guo and Jongwook Choi and Marcin Moczulski and Samy Bengio and Mohammad Norouzi and Honglak Lee},\nyear={2020},\nurl={https://openreview.net/forum?id=Byg5KyHYwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Byg5KyHYwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1851/Authors", "ICLR.cc/2020/Conference/Paper1851/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1851/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1851/Reviewers", "ICLR.cc/2020/Conference/Paper1851/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1851/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1851/Authors|ICLR.cc/2020/Conference/Paper1851/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149978, "tmdate": 1576860533482, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1851/Authors", "ICLR.cc/2020/Conference/Paper1851/Reviewers", "ICLR.cc/2020/Conference/Paper1851/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1851/-/Official_Comment"}}}, {"id": "HkxBme2ijr", "original": null, "number": 9, "cdate": 1573793820981, "ddate": null, "tcdate": 1573793820981, "tmdate": 1573793882069, "tddate": null, "forum": "Byg5KyHYwr", "replyto": "ryexs85aYS", "invitation": "ICLR.cc/2020/Conference/Paper1851/-/Official_Comment", "content": {"title": "Response to Review #2 (Part 1/2) ", "comment": "Dear Reviewer #2:\n\nThank you for the comments. \n\n>>> Why trajectory-conditioned policy over just goal-conditioned policy? The note in the related work section doesn't paint a clear enough picture.\n\nThe trajectory-conditioned policy can be thought of as an instance of the goal-conditioned policy, though our \u201cgoal\u201d is the trajectory with rich intermediate information about how to achieve the final goal state, thus making it easier for the agent to reach the goal. \n\nLet\u2019s consider other formulations of goal-conditioned policy. If the goal is only the single final state (Kulkarni et al., 2016), it may be difficult to visit the goal state far away from the initial state, especially when the goal is only visited by the agent for just a few times. A good example is a game like Montezuma\u2019s Revenge or Pitfall where there could be many dangers and obstructions along the way to the goal state (e.g., thousands of steps away from the initial state). If the goal includes intermediate information (e.g., a sequence of a small number of sub-goals) aiding the agent towards the final goal state (Liu et al., 2018), the problem becomes easier but it may be still nontrivial to reach the individual sub-goals for long-horizon problems. In addition, learning such a goal-conditioned policy may still require substantial amounts of samples. Our trajectory-conditioned policy is one instance of including the intermediate but more dense information in the goal, making it easier to imitate the previous trajectory with dense imitation reward, and we empirically show that it works well on various domains. We agree there could be alternative (potentially simpler) design choices for our trajectory-conditioned policy, which we will explore in future work.\n\n>>> This reviewer moves to reject the paper primarily for not balancing the high complexity of the solution to the lower difficulty of the problem. Pure-exploration algorithms (Go-Explore), not burdened by interleaving policy learning, achieve far superior scores.\n\nWe respectfully disagree that the problems we investigated in this paper, especially Montezuma\u2019s Revenge and Pitfall, are of \u201clower difficulty\u201d due to the existence of Go-Explore.\n\nFirst, as we mentioned in Related Work, it is worth noting that the success of Go-Explore heavily relies on the assumption that the environment can be reset to an arbitrary state and the environment is completely deterministic in the exploration phase. We argue that this assumption is infeasible in real-life environments where a high-fidelity simulator may not be available (such as complex robotic tasks) and takes an unfair advantage over the \u201creset-free\u201d methods. When there is no direct state-reset function and there is stochasticity in the environment, memorizing the past action sequence will not lead the agent to the state of interest (we added these experiments in Appendix K). Therefore in this setting, a more sophisticated \u201cpolicy learning\u201d is necessary to enable revisiting states of interest. One contribution of our paper is to remove the reliance on the assumption by learning a trajectory-conditioned policy for visiting diverse regions. Therefore, our method could work well in environments without a simulator.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1851/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1851/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["guoyijie@umich.edu", "jwook@umich.edu", "moczulski@google.com", "bengio@google.com", "mnorouzi@google.com", "honglak@google.com"], "title": "Self-Imitation Learning via Trajectory-Conditioned Policy for Hard-Exploration Tasks", "authors": ["Yijie Guo", "Jongwook Choi", "Marcin Moczulski", "Samy Bengio", "Mohammad Norouzi", "Honglak Lee"], "pdf": "/pdf/574d0a4dfa347dfe9cefed9f9ce8d3fa23ec61e7.pdf", "TL;DR": "Self-imitation learning of diverse trajectories with trajectory-conditioned policy", "abstract": "Imitation learning from human-expert demonstrations has been shown to be greatly helpful for challenging reinforcement learning problems with sparse environment rewards. However, it is very difficult to achieve similar success without relying on expert demonstrations. Recent works on self-imitation learning showed that imitating the agent's own past good experience could indirectly drive exploration in some environments, but these methods often lead to sub-optimal and myopic behavior. To address this issue, we argue that exploration in diverse directions by imitating diverse trajectories, instead of focusing on limited good trajectories, is more desirable for the hard-exploration tasks. We propose a new method of learning a trajectory-conditioned policy to imitate diverse trajectories from the agent's own past experiences and show that such self-imitation helps avoid myopic behavior and increases the chance of finding a globally optimal solution for hard-exploration tasks, especially when there are misleading rewards. Our method significantly outperforms existing self-imitation learning and count-based exploration methods on various hard-exploration tasks with local optima. In particular, we report a state-of-the-art score of more than 20,000 points on Montezumas Revenge without using expert demonstrations or resetting to arbitrary states.", "keywords": ["imitation learning", "hard-exploration tasks", "exploration and exploitation"], "paperhash": "guo|selfimitation_learning_via_trajectoryconditioned_policy_for_hardexploration_tasks", "original_pdf": "/attachment/e0b09c9e468f04046bdf6d089c77de0e2d7c0a5b.pdf", "_bibtex": "@misc{\nguo2020selfimitation,\ntitle={Self-Imitation Learning via Trajectory-Conditioned Policy for Hard-Exploration Tasks},\nauthor={Yijie Guo and Jongwook Choi and Marcin Moczulski and Samy Bengio and Mohammad Norouzi and Honglak Lee},\nyear={2020},\nurl={https://openreview.net/forum?id=Byg5KyHYwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Byg5KyHYwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1851/Authors", "ICLR.cc/2020/Conference/Paper1851/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1851/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1851/Reviewers", "ICLR.cc/2020/Conference/Paper1851/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1851/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1851/Authors|ICLR.cc/2020/Conference/Paper1851/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149978, "tmdate": 1576860533482, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1851/Authors", "ICLR.cc/2020/Conference/Paper1851/Reviewers", "ICLR.cc/2020/Conference/Paper1851/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1851/-/Official_Comment"}}}, {"id": "rJxv9CsijH", "original": null, "number": 7, "cdate": 1573793423013, "ddate": null, "tcdate": 1573793423013, "tmdate": 1573793423013, "tddate": null, "forum": "Byg5KyHYwr", "replyto": "rJgVvrnCKS", "invitation": "ICLR.cc/2020/Conference/Paper1851/-/Official_Comment", "content": {"title": "Response to Review #3", "comment": "Dear Reviewer #3:\n\nThank you for the clear and constructive feedback.\n\nFor question 1, as shown in Appendix L, we conducted the experiments with the traditional exploration mechanism of the epsilon-greedy strategy. We combined epsilon-greedy policy with PPO (Schulman et al. 2017) framework and DQN (Mnih et al. 2015) framework and searched many values of the hyper-parameters for the epsilon scheduling. Even though we have put much effort to push the experiments with random exploration, the performance is much worse than DTSIL on these hard-exploration tasks. Especially for Montezuma\u2019s Revenge, the epsilon-greedy policy with random exploration achieves a score less than 100, which is consistent with the experimental results from previous works (Mnih et al. 2015, Schulman et al. 2017).\n\nAlso, we would like to emphasize that our main baseline method, i.e. count-based exploration, is also one of the classic, well-performing exploration mechanisms (Strehl & Littman 2005, Kolter & Ng 2009). However, we found that a simple combination of count-based exploration with standard RL method (e.g., PPO) still requires encountering many similar high-reward episodes to learn good behavior and empirically performs worse than our proposed method. We believe that it is because our method can better leverage a few samples of high-reward trajectories by learning to explore the variants of those past trajectories.\n\nFor question 2, we added Appendix J for the ablation study of the hyper-parameter $\\Delta t$. The only constraint is that $\\Delta t$ should be less than m (length of demonstration segment to imitate as input into the policy; we set m=10 for all our experiments by considering computational constraints). In general, we found that allowing the agent some flexibility of imitation by setting $\\Delta t$ close to m works well. For easy domains, $\\Delta t=2,4,8$ does not show much difference in policy performance. For more difficult domains, $\\Delta t=8$ works better because we provide imitation rewards more leniently to the agent to encourage imitation of the demonstration. In summary, $\\Delta t=8$ performs well for all of our primary experiment environments.\n\nWe hope that our response above will address your concerns and thank you again for the suggestions.\n\nReferences:\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., ... & Petersen, S. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), 529.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.\nStrehl, A. L., & Littman, M. L. (2005, August). A theoretical analysis of model-based interval estimation. In Proceedings of the 22nd international conference on Machine learning (pp. 856-863). ACM.\nKolter, J. Z., & Ng, A. Y. (2009, June). Near-Bayesian exploration in polynomial time. In Proceedings of the 26th Annual International Conference on Machine Learning (pp. 513-520). ACM.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1851/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1851/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["guoyijie@umich.edu", "jwook@umich.edu", "moczulski@google.com", "bengio@google.com", "mnorouzi@google.com", "honglak@google.com"], "title": "Self-Imitation Learning via Trajectory-Conditioned Policy for Hard-Exploration Tasks", "authors": ["Yijie Guo", "Jongwook Choi", "Marcin Moczulski", "Samy Bengio", "Mohammad Norouzi", "Honglak Lee"], "pdf": "/pdf/574d0a4dfa347dfe9cefed9f9ce8d3fa23ec61e7.pdf", "TL;DR": "Self-imitation learning of diverse trajectories with trajectory-conditioned policy", "abstract": "Imitation learning from human-expert demonstrations has been shown to be greatly helpful for challenging reinforcement learning problems with sparse environment rewards. However, it is very difficult to achieve similar success without relying on expert demonstrations. Recent works on self-imitation learning showed that imitating the agent's own past good experience could indirectly drive exploration in some environments, but these methods often lead to sub-optimal and myopic behavior. To address this issue, we argue that exploration in diverse directions by imitating diverse trajectories, instead of focusing on limited good trajectories, is more desirable for the hard-exploration tasks. We propose a new method of learning a trajectory-conditioned policy to imitate diverse trajectories from the agent's own past experiences and show that such self-imitation helps avoid myopic behavior and increases the chance of finding a globally optimal solution for hard-exploration tasks, especially when there are misleading rewards. Our method significantly outperforms existing self-imitation learning and count-based exploration methods on various hard-exploration tasks with local optima. In particular, we report a state-of-the-art score of more than 20,000 points on Montezumas Revenge without using expert demonstrations or resetting to arbitrary states.", "keywords": ["imitation learning", "hard-exploration tasks", "exploration and exploitation"], "paperhash": "guo|selfimitation_learning_via_trajectoryconditioned_policy_for_hardexploration_tasks", "original_pdf": "/attachment/e0b09c9e468f04046bdf6d089c77de0e2d7c0a5b.pdf", "_bibtex": "@misc{\nguo2020selfimitation,\ntitle={Self-Imitation Learning via Trajectory-Conditioned Policy for Hard-Exploration Tasks},\nauthor={Yijie Guo and Jongwook Choi and Marcin Moczulski and Samy Bengio and Mohammad Norouzi and Honglak Lee},\nyear={2020},\nurl={https://openreview.net/forum?id=Byg5KyHYwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Byg5KyHYwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1851/Authors", "ICLR.cc/2020/Conference/Paper1851/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1851/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1851/Reviewers", "ICLR.cc/2020/Conference/Paper1851/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1851/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1851/Authors|ICLR.cc/2020/Conference/Paper1851/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149978, "tmdate": 1576860533482, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1851/Authors", "ICLR.cc/2020/Conference/Paper1851/Reviewers", "ICLR.cc/2020/Conference/Paper1851/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1851/-/Official_Comment"}}}, {"id": "rylzEpoojB", "original": null, "number": 6, "cdate": 1573793066393, "ddate": null, "tcdate": 1573793066393, "tmdate": 1573793066393, "tddate": null, "forum": "Byg5KyHYwr", "replyto": "SyxtVdWg9S", "invitation": "ICLR.cc/2020/Conference/Paper1851/-/Official_Comment", "content": {"title": "Response to Review #1", "comment": "Dear Reviewer #1:\n\nThanks for your detailed and helpful feedback. We have simply moved the implementation details in Section 4 to the Appendix so that the entire paper can fit into 8 pages without much tweaking of style-format.\n\nAbout the environment setting, as explained at the start of section 4.2, 4.3 and listed in Table 3, Appendix E, we considered the stochastic environments for all the primary experiments, including Apple-Gold, Montezuma\u2019s Revenge, Pitfall, and MuJoco. The initial state of the agent in the experiment is randomized. The mechanism of initial random no-ops is one of the standard ways to introduce stochasticity in the Atari environment (Machado et al., 2018). The mechanism of random initial location from a Gaussian distribution in MuJoco maze is the same as in standard MuJuco tasks (Brockman et al.,2016).\n\nIn Appendix C.1, we showed DTSIL outperforms the baselines and achieves near-optimal episode reward with different forms of stochasticity in the Apple-Gold domain (i.e. random initial location of the agent, sticky action, and random initial location of the treasure). In the Apple-Gold domain, as we introduced in Figure 1, the agent achieves reward +1 when collecting an apple, gets reward +10 when collecting the treasure, but gets reward -0.05 when taking a step in the rocky region. Therefore, with the time limit of 45 steps, the optimal trajectory is to go through the rocky region for 30 steps and reach the treasure to get the total episode reward of 8.5. Different from the baselines, DTSIL successfully finds the optimal path and converges to good behavior.\n\nIn order to show the difficulty in policy learning in these stochastic environments, we added an additional baseline for DTSIL in Appendix K. Specifically, we stored and repeated the action sequence from the demonstration trajectory. When the environment is deterministic, repeating the action sequence should perfectly lead the agent to imitate the demonstration and reach the final state of interest. However, as shown in Figure 21 and 22 in Appendix K, on environments with random initial states (which is a standard type of moderate-degree stochasticity in the literature), memorizing the action sequence is not sufficient. The success ratio in imitation is much lower than DTSIL. Thus the agent could not revisit the novel regions as efficiently as DTSIL to discover better trajectories and converge to a better total episode reward.\n\nWe hope this information will address your concerns about the deterministic environments and thank you again for your comments. \n\nReferences:\nMachado, M. C., Bellemare, M. G., Talvitie, E., Veness, J., Hausknecht, M., & Bowling, M. (2018). Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents. Journal of Artificial Intelligence Research, 61, 523-562.\nBrockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., & Zaremba, W. (2016). Openai gym. arXiv preprint arXiv:1606.01540.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1851/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1851/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["guoyijie@umich.edu", "jwook@umich.edu", "moczulski@google.com", "bengio@google.com", "mnorouzi@google.com", "honglak@google.com"], "title": "Self-Imitation Learning via Trajectory-Conditioned Policy for Hard-Exploration Tasks", "authors": ["Yijie Guo", "Jongwook Choi", "Marcin Moczulski", "Samy Bengio", "Mohammad Norouzi", "Honglak Lee"], "pdf": "/pdf/574d0a4dfa347dfe9cefed9f9ce8d3fa23ec61e7.pdf", "TL;DR": "Self-imitation learning of diverse trajectories with trajectory-conditioned policy", "abstract": "Imitation learning from human-expert demonstrations has been shown to be greatly helpful for challenging reinforcement learning problems with sparse environment rewards. However, it is very difficult to achieve similar success without relying on expert demonstrations. Recent works on self-imitation learning showed that imitating the agent's own past good experience could indirectly drive exploration in some environments, but these methods often lead to sub-optimal and myopic behavior. To address this issue, we argue that exploration in diverse directions by imitating diverse trajectories, instead of focusing on limited good trajectories, is more desirable for the hard-exploration tasks. We propose a new method of learning a trajectory-conditioned policy to imitate diverse trajectories from the agent's own past experiences and show that such self-imitation helps avoid myopic behavior and increases the chance of finding a globally optimal solution for hard-exploration tasks, especially when there are misleading rewards. Our method significantly outperforms existing self-imitation learning and count-based exploration methods on various hard-exploration tasks with local optima. In particular, we report a state-of-the-art score of more than 20,000 points on Montezumas Revenge without using expert demonstrations or resetting to arbitrary states.", "keywords": ["imitation learning", "hard-exploration tasks", "exploration and exploitation"], "paperhash": "guo|selfimitation_learning_via_trajectoryconditioned_policy_for_hardexploration_tasks", "original_pdf": "/attachment/e0b09c9e468f04046bdf6d089c77de0e2d7c0a5b.pdf", "_bibtex": "@misc{\nguo2020selfimitation,\ntitle={Self-Imitation Learning via Trajectory-Conditioned Policy for Hard-Exploration Tasks},\nauthor={Yijie Guo and Jongwook Choi and Marcin Moczulski and Samy Bengio and Mohammad Norouzi and Honglak Lee},\nyear={2020},\nurl={https://openreview.net/forum?id=Byg5KyHYwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Byg5KyHYwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1851/Authors", "ICLR.cc/2020/Conference/Paper1851/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1851/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1851/Reviewers", "ICLR.cc/2020/Conference/Paper1851/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1851/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1851/Authors|ICLR.cc/2020/Conference/Paper1851/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149978, "tmdate": 1576860533482, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1851/Authors", "ICLR.cc/2020/Conference/Paper1851/Reviewers", "ICLR.cc/2020/Conference/Paper1851/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1851/-/Official_Comment"}}}, {"id": "ryexs85aYS", "original": null, "number": 1, "cdate": 1571821207865, "ddate": null, "tcdate": 1571821207865, "tmdate": 1572972415483, "tddate": null, "forum": "Byg5KyHYwr", "replyto": "Byg5KyHYwr", "invitation": "ICLR.cc/2020/Conference/Paper1851/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper addresses the challenge of hard exploration tasks. The approach taken is to apply self-imitation to a diverse selection of trajectories from past experience -- practice re-doing the strangest things you've ever done. This is claimed to drive more efficient exploration in sparse-reward problems, leading to SOTA results for Montezuma's Revenge without certain common aides.\n\nThe approach is incompletely motivated. Why trajectory-conditioned policy over just goal-conditioned policy? The note in the related work section doesn't paint a clear enough picture. The trajectory buffer management strategy feels complex. Why this use strategy specifically? Could a simpler design be ruled out? In 2019 (post Go-Explore), it's not clear Montezuma's revenge poses a significant exploration challenge -- exploration doesn't even need to be interleaved with learning. Why are these three the right domains to show off these techniques?\n\nThis reviewer moves to reject the paper primarily for not balancing the high complexity of the solution to the lower difficulty of the problem. Pure-exploration algorithms (Go-Explore), not burdened by interleaving policy learning, achieve far superior scores. If the authors want to escape the shadow of this kind of technique which cheats by some framings of RL, more appropriate demonstration environments must be selected."}, "signatures": ["ICLR.cc/2020/Conference/Paper1851/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1851/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["guoyijie@umich.edu", "jwook@umich.edu", "moczulski@google.com", "bengio@google.com", "mnorouzi@google.com", "honglak@google.com"], "title": "Self-Imitation Learning via Trajectory-Conditioned Policy for Hard-Exploration Tasks", "authors": ["Yijie Guo", "Jongwook Choi", "Marcin Moczulski", "Samy Bengio", "Mohammad Norouzi", "Honglak Lee"], "pdf": "/pdf/574d0a4dfa347dfe9cefed9f9ce8d3fa23ec61e7.pdf", "TL;DR": "Self-imitation learning of diverse trajectories with trajectory-conditioned policy", "abstract": "Imitation learning from human-expert demonstrations has been shown to be greatly helpful for challenging reinforcement learning problems with sparse environment rewards. However, it is very difficult to achieve similar success without relying on expert demonstrations. Recent works on self-imitation learning showed that imitating the agent's own past good experience could indirectly drive exploration in some environments, but these methods often lead to sub-optimal and myopic behavior. To address this issue, we argue that exploration in diverse directions by imitating diverse trajectories, instead of focusing on limited good trajectories, is more desirable for the hard-exploration tasks. We propose a new method of learning a trajectory-conditioned policy to imitate diverse trajectories from the agent's own past experiences and show that such self-imitation helps avoid myopic behavior and increases the chance of finding a globally optimal solution for hard-exploration tasks, especially when there are misleading rewards. Our method significantly outperforms existing self-imitation learning and count-based exploration methods on various hard-exploration tasks with local optima. In particular, we report a state-of-the-art score of more than 20,000 points on Montezumas Revenge without using expert demonstrations or resetting to arbitrary states.", "keywords": ["imitation learning", "hard-exploration tasks", "exploration and exploitation"], "paperhash": "guo|selfimitation_learning_via_trajectoryconditioned_policy_for_hardexploration_tasks", "original_pdf": "/attachment/e0b09c9e468f04046bdf6d089c77de0e2d7c0a5b.pdf", "_bibtex": "@misc{\nguo2020selfimitation,\ntitle={Self-Imitation Learning via Trajectory-Conditioned Policy for Hard-Exploration Tasks},\nauthor={Yijie Guo and Jongwook Choi and Marcin Moczulski and Samy Bengio and Mohammad Norouzi and Honglak Lee},\nyear={2020},\nurl={https://openreview.net/forum?id=Byg5KyHYwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Byg5KyHYwr", "replyto": "Byg5KyHYwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1851/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1851/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575647158165, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1851/Reviewers"], "noninvitees": [], "tcdate": 1570237731386, "tmdate": 1575647158180, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1851/-/Official_Review"}}}, {"id": "rJgVvrnCKS", "original": null, "number": 2, "cdate": 1571894619745, "ddate": null, "tcdate": 1571894619745, "tmdate": 1572972415433, "tddate": null, "forum": "Byg5KyHYwr", "replyto": "Byg5KyHYwr", "invitation": "ICLR.cc/2020/Conference/Paper1851/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The authors identify and address the problem of sub-optimal and myopic behaviors of self-imitation learning in environments with sparse rewards. The authors propose DTSIL to learn a trajectory-conditioned policy to imitate diverse trajectories from the agent\u2019s own past experience. Unlike other self-imitation learning methods, the proposed method not only leverages sub-trajectories with high rewards, but lower-reward trajectories to encourage agent exploration diversity. The authors claim the proposed method to be more likely to find a global optimal solution. \n\nOverall, this paper is well-written with comprehensive experimental results. The proposed trajectory-conditioned policy sounds, since rewarded trajectory carries significant information of the goal in the exploration problem. Extensive experimental results demonstrated the effectiveness of the proposed DTSIL. However, I have a few concerns below, that prevent me from giving a direct acceptance. \n\n1. The proposed DTSIL changes the original MDP with sparse reward to an MDP with denser reward, which allows the training process to explore more in the \u201cspace\u201d closer to the collected high reward trajectories. Such \u201cexploration\u201d sounds promising. However, it would be nice to compare it with traditional reinforcement learning (e.g., with \\epsilon-greedy policy for random exploration)?\n\n2. In appendix D, the authors discussed what the parameter \\delta_t controls, however, it is unclear how \\delta_t should be chosen in implementation. The authors did not explain how \\delta_t was selected in their experiments. Choosing the right \\delta_t may be hard, but it would be nice to introduce what \u201cheuristics\u201d the authors used and suggest to readers. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1851/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1851/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["guoyijie@umich.edu", "jwook@umich.edu", "moczulski@google.com", "bengio@google.com", "mnorouzi@google.com", "honglak@google.com"], "title": "Self-Imitation Learning via Trajectory-Conditioned Policy for Hard-Exploration Tasks", "authors": ["Yijie Guo", "Jongwook Choi", "Marcin Moczulski", "Samy Bengio", "Mohammad Norouzi", "Honglak Lee"], "pdf": "/pdf/574d0a4dfa347dfe9cefed9f9ce8d3fa23ec61e7.pdf", "TL;DR": "Self-imitation learning of diverse trajectories with trajectory-conditioned policy", "abstract": "Imitation learning from human-expert demonstrations has been shown to be greatly helpful for challenging reinforcement learning problems with sparse environment rewards. However, it is very difficult to achieve similar success without relying on expert demonstrations. Recent works on self-imitation learning showed that imitating the agent's own past good experience could indirectly drive exploration in some environments, but these methods often lead to sub-optimal and myopic behavior. To address this issue, we argue that exploration in diverse directions by imitating diverse trajectories, instead of focusing on limited good trajectories, is more desirable for the hard-exploration tasks. We propose a new method of learning a trajectory-conditioned policy to imitate diverse trajectories from the agent's own past experiences and show that such self-imitation helps avoid myopic behavior and increases the chance of finding a globally optimal solution for hard-exploration tasks, especially when there are misleading rewards. Our method significantly outperforms existing self-imitation learning and count-based exploration methods on various hard-exploration tasks with local optima. In particular, we report a state-of-the-art score of more than 20,000 points on Montezumas Revenge without using expert demonstrations or resetting to arbitrary states.", "keywords": ["imitation learning", "hard-exploration tasks", "exploration and exploitation"], "paperhash": "guo|selfimitation_learning_via_trajectoryconditioned_policy_for_hardexploration_tasks", "original_pdf": "/attachment/e0b09c9e468f04046bdf6d089c77de0e2d7c0a5b.pdf", "_bibtex": "@misc{\nguo2020selfimitation,\ntitle={Self-Imitation Learning via Trajectory-Conditioned Policy for Hard-Exploration Tasks},\nauthor={Yijie Guo and Jongwook Choi and Marcin Moczulski and Samy Bengio and Mohammad Norouzi and Honglak Lee},\nyear={2020},\nurl={https://openreview.net/forum?id=Byg5KyHYwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Byg5KyHYwr", "replyto": "Byg5KyHYwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1851/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1851/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575647158165, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1851/Reviewers"], "noninvitees": [], "tcdate": 1570237731386, "tmdate": 1575647158180, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1851/-/Official_Review"}}}], "count": 10}