{"notes": [{"id": "SygkSkSFDB", "original": "ryevMxTOvr", "number": 1677, "cdate": 1569439543439, "ddate": null, "tcdate": 1569439543439, "tmdate": 1577168230758, "tddate": null, "forum": "SygkSkSFDB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "On the expected running time of nonconvex optimization with early stopping", "authors": ["Thomas Flynn", "Kwang Min Yu", "Abid Malik", "Shinjae Yoo", "Nicholas D'Imperio"], "authorids": ["thomasflynn918@gmail.com", "kyu@bnl.gov", "amalik@bnl.gov", "sjyoo@bnl.gov", "dimperio@bnl.gov"], "keywords": ["non-convex", "stopping times", "statistics", "gradient descent", "early stopping"], "TL;DR": "How to bound the expected number of iterations before gradient descent finds a stationary point", "abstract": "This work examines the convergence of stochastic gradient algorithms that use early stopping based on a validation function, wherein optimization ends when the magnitude of a validation function gradient drops below a threshold. We derive conditions that guarantee this stopping rule is well-defined and analyze the expected number of iterations and gradient evaluations needed to meet this criteria. The guarantee accounts for the distance between the training and validation sets, measured with the Wasserstein distance. We develop the approach for stochastic gradient descent (SGD), allowing for biased update directions subject to a Lyapunov condition. We apply the approach to obtain new bounds on the expected running time of several algorithms, including Decentralized SGD (DSGD), a variant of decentralized SGD, known as \\textit{Stacked SGD}, and the stochastic variance reduced gradient (SVRG) algorithm. Finally, we consider the generalization properties of the iterate returned by early stopping.", "pdf": "/pdf/2b0abd7b1c162b99f9e3cd41e53759885ba710d4.pdf", "paperhash": "flynn|on_the_expected_running_time_of_nonconvex_optimization_with_early_stopping", "original_pdf": "/attachment/2b0abd7b1c162b99f9e3cd41e53759885ba710d4.pdf", "_bibtex": "@misc{\nflynn2020on,\ntitle={On the expected running time of nonconvex optimization with early stopping},\nauthor={Thomas Flynn and Kwang Min Yu and Abid Malik and Shinjae Yoo and Nicholas D'Imperio},\nyear={2020},\nurl={https://openreview.net/forum?id=SygkSkSFDB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "ExJmOU7UHz", "original": null, "number": 1, "cdate": 1576798729583, "ddate": null, "tcdate": 1576798729583, "tmdate": 1576800906923, "tddate": null, "forum": "SygkSkSFDB", "replyto": "SygkSkSFDB", "invitation": "ICLR.cc/2020/Conference/Paper1677/-/Decision", "content": {"decision": "Reject", "comment": "The authors made no response to reviewers. Based on current reviews, the paper is suggested a rejection as majority.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the expected running time of nonconvex optimization with early stopping", "authors": ["Thomas Flynn", "Kwang Min Yu", "Abid Malik", "Shinjae Yoo", "Nicholas D'Imperio"], "authorids": ["thomasflynn918@gmail.com", "kyu@bnl.gov", "amalik@bnl.gov", "sjyoo@bnl.gov", "dimperio@bnl.gov"], "keywords": ["non-convex", "stopping times", "statistics", "gradient descent", "early stopping"], "TL;DR": "How to bound the expected number of iterations before gradient descent finds a stationary point", "abstract": "This work examines the convergence of stochastic gradient algorithms that use early stopping based on a validation function, wherein optimization ends when the magnitude of a validation function gradient drops below a threshold. We derive conditions that guarantee this stopping rule is well-defined and analyze the expected number of iterations and gradient evaluations needed to meet this criteria. The guarantee accounts for the distance between the training and validation sets, measured with the Wasserstein distance. We develop the approach for stochastic gradient descent (SGD), allowing for biased update directions subject to a Lyapunov condition. We apply the approach to obtain new bounds on the expected running time of several algorithms, including Decentralized SGD (DSGD), a variant of decentralized SGD, known as \\textit{Stacked SGD}, and the stochastic variance reduced gradient (SVRG) algorithm. Finally, we consider the generalization properties of the iterate returned by early stopping.", "pdf": "/pdf/2b0abd7b1c162b99f9e3cd41e53759885ba710d4.pdf", "paperhash": "flynn|on_the_expected_running_time_of_nonconvex_optimization_with_early_stopping", "original_pdf": "/attachment/2b0abd7b1c162b99f9e3cd41e53759885ba710d4.pdf", "_bibtex": "@misc{\nflynn2020on,\ntitle={On the expected running time of nonconvex optimization with early stopping},\nauthor={Thomas Flynn and Kwang Min Yu and Abid Malik and Shinjae Yoo and Nicholas D'Imperio},\nyear={2020},\nurl={https://openreview.net/forum?id=SygkSkSFDB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SygkSkSFDB", "replyto": "SygkSkSFDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795726450, "tmdate": 1576800278586, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1677/-/Decision"}}}, {"id": "rJgTmIO6Kr", "original": null, "number": 2, "cdate": 1571812901102, "ddate": null, "tcdate": 1571812901102, "tmdate": 1574260979637, "tddate": null, "forum": "SygkSkSFDB", "replyto": "SygkSkSFDB", "invitation": "ICLR.cc/2020/Conference/Paper1677/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "In this paper, the authors consider stochastic optimization in the setting where a validation function is used to guide the termination of the algorithm. In more details, the algorithm terminates if the gradient of the validation function at an iterate is smaller than a threshold. In this framework, the authors consider several variants of SGD, including distributed variant and SVRG, for each of which the authors study the expected number of iterations for a prescribed accuracy under an assumption between the training and validation set.\n\nWhile the use of a validation function is useful for early stopping, it introduces additional cost.\n\nWhile bounds on the expected number of iterations are derived for several variants of SGD, it seems that most arguments are adapted from the existing analysis to take into account the validation function.\n\nIn Corollary 3.4 and Corollary 3.5, the bound is an increasing function of m. This suggests that the best choice would be m=1. However, in this case, one needs to calculate the gradient of the validation function at each iteration, which may wastes a lot of computation.\n\nThe authors consider constant step sizes. In practice, step sizes are often decreasing along the optimization. Can the analysis be extended to cover the case with decreasing step sizes?\n\nIn eq (30), there is a missing factor of 2.\n\nThere is a required $\\epsilon>G62d_1(\\mu_V,\\mu_T)^2$ in the results. Therefore, to achieve a high accuracy we need $d_1(\\mu_T,\\mu_T)$ to be small. How many numbers of sample size to make $d_1(\\mu_V,\\mu_T)$ small? This has an influence on the computational cost.\n\n\n----------------------\nAfter rebuttal:\n\nThe authors do not respond. I would like to keep my original score.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1677/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1677/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the expected running time of nonconvex optimization with early stopping", "authors": ["Thomas Flynn", "Kwang Min Yu", "Abid Malik", "Shinjae Yoo", "Nicholas D'Imperio"], "authorids": ["thomasflynn918@gmail.com", "kyu@bnl.gov", "amalik@bnl.gov", "sjyoo@bnl.gov", "dimperio@bnl.gov"], "keywords": ["non-convex", "stopping times", "statistics", "gradient descent", "early stopping"], "TL;DR": "How to bound the expected number of iterations before gradient descent finds a stationary point", "abstract": "This work examines the convergence of stochastic gradient algorithms that use early stopping based on a validation function, wherein optimization ends when the magnitude of a validation function gradient drops below a threshold. We derive conditions that guarantee this stopping rule is well-defined and analyze the expected number of iterations and gradient evaluations needed to meet this criteria. The guarantee accounts for the distance between the training and validation sets, measured with the Wasserstein distance. We develop the approach for stochastic gradient descent (SGD), allowing for biased update directions subject to a Lyapunov condition. We apply the approach to obtain new bounds on the expected running time of several algorithms, including Decentralized SGD (DSGD), a variant of decentralized SGD, known as \\textit{Stacked SGD}, and the stochastic variance reduced gradient (SVRG) algorithm. Finally, we consider the generalization properties of the iterate returned by early stopping.", "pdf": "/pdf/2b0abd7b1c162b99f9e3cd41e53759885ba710d4.pdf", "paperhash": "flynn|on_the_expected_running_time_of_nonconvex_optimization_with_early_stopping", "original_pdf": "/attachment/2b0abd7b1c162b99f9e3cd41e53759885ba710d4.pdf", "_bibtex": "@misc{\nflynn2020on,\ntitle={On the expected running time of nonconvex optimization with early stopping},\nauthor={Thomas Flynn and Kwang Min Yu and Abid Malik and Shinjae Yoo and Nicholas D'Imperio},\nyear={2020},\nurl={https://openreview.net/forum?id=SygkSkSFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SygkSkSFDB", "replyto": "SygkSkSFDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1677/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1677/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575634717975, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1677/Reviewers"], "noninvitees": [], "tcdate": 1570237733902, "tmdate": 1575634717989, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1677/-/Official_Review"}}}, {"id": "r1gFw9KcFH", "original": null, "number": 1, "cdate": 1571621472876, "ddate": null, "tcdate": 1571621472876, "tmdate": 1574038509124, "tddate": null, "forum": "SygkSkSFDB", "replyto": "SygkSkSFDB", "invitation": "ICLR.cc/2020/Conference/Paper1677/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "The paper studies the problem of the number of first-order-oracle calls for the SGD type of algorithms to find a stationary point of the objective function. The main results in the paper are built upon a new, general framework to analyze the SGD type of algorithms.\n\n\nThe main framework can be summarized as follows: At each iteration, the algorithm receives h_t, a (potentially biased) estimator of the gradient at a given point x_t, and performs a simple update x_{t + 1} = x_t - \\eta * h_t. The framework says that as long as the norm (V_t) of \\Delta_t = h_t - v_t (where v_t is an unbiased estimator of the true gradient with bounded variance) satisfies a particular Lyapunov-type inequality, then the algorithm can find an epsilon-stationary point as long as epsilon is not too small. \n\n\nThe analysis of the framework is quite standard, one only needs to write the decrement in function value at each iteration into the following three terms: the norm of the true gradient of the function, \\delta_t: the difference between v_t and the true gradient (so E[\\delta_t] = 0) and \\Delta_t: the difference between the received gradient h_t and v_t.\n\n\nThe authors showed some application of this framework in Stacked SGD and decentralized SGD. The main intuitions of these applications are (1). \\Delta_t comes from the synchronization difference of the nodes when computing the gradient. (2). The shrinking of V_t is due to the  (better) synchronization at each iteration. (3). The increment of V_t is due to the gradient update. \n\nOverall, I find the general framework quite interesting and potentially useful for future research and could be used as a guide for choosing the proper algorithm in distributed computation.  The bounds in this paper are also in principle tight. The only question I have about this result is the dependency of m (the number of iterations between each evaluation of the gradient norm of the underlying function). (1). How can this (the evaluation of the gradient norm of the underlying function)) be done in a decentralized environment? What is the computation overhead?  (For example in DSGD, how can we compute \\bar{x}_t?) (2). It seems that the computation cost (number of IFO) scales quadratically with respect to m. What is the intuition for this scaling? It appears to me that the scaling should be linear or better (the worst case is that within the \"m\" iterations, only one iteration has gradient >= epsilon). The authors should elaborate more on this point.\n\n\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1677/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1677/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the expected running time of nonconvex optimization with early stopping", "authors": ["Thomas Flynn", "Kwang Min Yu", "Abid Malik", "Shinjae Yoo", "Nicholas D'Imperio"], "authorids": ["thomasflynn918@gmail.com", "kyu@bnl.gov", "amalik@bnl.gov", "sjyoo@bnl.gov", "dimperio@bnl.gov"], "keywords": ["non-convex", "stopping times", "statistics", "gradient descent", "early stopping"], "TL;DR": "How to bound the expected number of iterations before gradient descent finds a stationary point", "abstract": "This work examines the convergence of stochastic gradient algorithms that use early stopping based on a validation function, wherein optimization ends when the magnitude of a validation function gradient drops below a threshold. We derive conditions that guarantee this stopping rule is well-defined and analyze the expected number of iterations and gradient evaluations needed to meet this criteria. The guarantee accounts for the distance between the training and validation sets, measured with the Wasserstein distance. We develop the approach for stochastic gradient descent (SGD), allowing for biased update directions subject to a Lyapunov condition. We apply the approach to obtain new bounds on the expected running time of several algorithms, including Decentralized SGD (DSGD), a variant of decentralized SGD, known as \\textit{Stacked SGD}, and the stochastic variance reduced gradient (SVRG) algorithm. Finally, we consider the generalization properties of the iterate returned by early stopping.", "pdf": "/pdf/2b0abd7b1c162b99f9e3cd41e53759885ba710d4.pdf", "paperhash": "flynn|on_the_expected_running_time_of_nonconvex_optimization_with_early_stopping", "original_pdf": "/attachment/2b0abd7b1c162b99f9e3cd41e53759885ba710d4.pdf", "_bibtex": "@misc{\nflynn2020on,\ntitle={On the expected running time of nonconvex optimization with early stopping},\nauthor={Thomas Flynn and Kwang Min Yu and Abid Malik and Shinjae Yoo and Nicholas D'Imperio},\nyear={2020},\nurl={https://openreview.net/forum?id=SygkSkSFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SygkSkSFDB", "replyto": "SygkSkSFDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1677/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1677/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575634717975, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1677/Reviewers"], "noninvitees": [], "tcdate": 1570237733902, "tmdate": 1575634717989, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1677/-/Official_Review"}}}, {"id": "B1ghnfO49B", "original": null, "number": 3, "cdate": 1572270771888, "ddate": null, "tcdate": 1572270771888, "tmdate": 1572972437482, "tddate": null, "forum": "SygkSkSFDB", "replyto": "SygkSkSFDB", "invitation": "ICLR.cc/2020/Conference/Paper1677/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes an optimization approach in which the optimizer computes the gradient on a given function yet uses another to decide a stopping time. Conceptually those functions are empirical errors on train and validation folds in the most common setting, although the authors seem to use other settings later in the paper to consider decentralized optimization schemes. The authors introduce a bound on the Wasserstein distance between the train and validation distributions in their analysis which plays a crucial role in their results. The authors use these results to motivate variants of existing optimization algorithms. \n\nThe paper is interesting but its message is a bit blurred to me. I had trouble pinpointing one main contribution, since the paper is split as theory (with some results) and a collection of slightly modified SGD type algorithms that are now impacted by this \"gradient somewhere / monitor progress elsewhere\". The theoretical results are worth reading and the idea appealing. \n\nThe paper also requires a *lot* of polishing. It has been sloppily written. For these reasons I am inclined to reject the paper and encourage the authors to improve their draft with a better formulation.\n\n\nMinor comments:\n- I have found the \"main contributions\" paragraph to be poorly phrased. Since the authors only monitor the validation loss and not the training loss, I do not think this falls into the \"standard\" definition of early stopping. \n- please use citet and citep consistently. \n- please add labels to figures and format them properly (e.g. SSGD on p.6) \n- unsure about the format used to display f_T(x_t) in p.6\n- Villani 2008 has over 900 pages. any page in particular?\n- Assumption 2.3 requires significantly more work... All bounds scale as G^2 (e.g. eq.9, 10,11), therefore an idea of what G's impact on the analysis sounds crucial. In Example 2.4 the authors start working out an example, but wouldn't it be more interesting to carry that out completely, e.g. for the KL? What kind of bound would that result in?\n- I find it disturbing that important comments on some of the crucial quantities (such as descent direction Eq.3) are left out of the algorithmic box... This defeats the purpose of having an algorithmic box.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1677/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1677/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the expected running time of nonconvex optimization with early stopping", "authors": ["Thomas Flynn", "Kwang Min Yu", "Abid Malik", "Shinjae Yoo", "Nicholas D'Imperio"], "authorids": ["thomasflynn918@gmail.com", "kyu@bnl.gov", "amalik@bnl.gov", "sjyoo@bnl.gov", "dimperio@bnl.gov"], "keywords": ["non-convex", "stopping times", "statistics", "gradient descent", "early stopping"], "TL;DR": "How to bound the expected number of iterations before gradient descent finds a stationary point", "abstract": "This work examines the convergence of stochastic gradient algorithms that use early stopping based on a validation function, wherein optimization ends when the magnitude of a validation function gradient drops below a threshold. We derive conditions that guarantee this stopping rule is well-defined and analyze the expected number of iterations and gradient evaluations needed to meet this criteria. The guarantee accounts for the distance between the training and validation sets, measured with the Wasserstein distance. We develop the approach for stochastic gradient descent (SGD), allowing for biased update directions subject to a Lyapunov condition. We apply the approach to obtain new bounds on the expected running time of several algorithms, including Decentralized SGD (DSGD), a variant of decentralized SGD, known as \\textit{Stacked SGD}, and the stochastic variance reduced gradient (SVRG) algorithm. Finally, we consider the generalization properties of the iterate returned by early stopping.", "pdf": "/pdf/2b0abd7b1c162b99f9e3cd41e53759885ba710d4.pdf", "paperhash": "flynn|on_the_expected_running_time_of_nonconvex_optimization_with_early_stopping", "original_pdf": "/attachment/2b0abd7b1c162b99f9e3cd41e53759885ba710d4.pdf", "_bibtex": "@misc{\nflynn2020on,\ntitle={On the expected running time of nonconvex optimization with early stopping},\nauthor={Thomas Flynn and Kwang Min Yu and Abid Malik and Shinjae Yoo and Nicholas D'Imperio},\nyear={2020},\nurl={https://openreview.net/forum?id=SygkSkSFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SygkSkSFDB", "replyto": "SygkSkSFDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1677/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1677/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575634717975, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1677/Reviewers"], "noninvitees": [], "tcdate": 1570237733902, "tmdate": 1575634717989, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1677/-/Official_Review"}}}], "count": 5}