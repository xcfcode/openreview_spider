{"notes": [{"id": "NTP9OdaT6nm", "original": "m2iybEfUkbg", "number": 2291, "cdate": 1601308252518, "ddate": null, "tcdate": 1601308252518, "tmdate": 1614985722603, "tddate": null, "forum": "NTP9OdaT6nm", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Formal Language Constrained Markov Decision Processes", "authorids": ["~Eleanor_Quint1", "~Dong_Xu1", "~Samuel_W_Flint1", "~Stephen_D_Scott1", "~Matthew_Dwyer1"], "authors": ["Eleanor Quint", "Dong Xu", "Samuel W Flint", "Stephen D Scott", "Matthew Dwyer"], "keywords": ["safe reinforcement learning", "formal languages", "constrained Markov decision process", "safety gym", "safety"], "abstract": "In order to satisfy safety conditions, an agent may be constrained from acting freely. A safe controller can be designed a priori if an environment is well understood, but not when learning is employed. In particular, reinforcement learned (RL) controllers require exploration, which can be hazardous in safety critical situations. We study the benefits of giving structure to the constraints of a constrained Markov decision process by specifying them in formal languages as a step towards using safety methods from software engineering and controller synthesis. We instantiate these constraints as finite automata to efficiently recognise constraint violations. Constraint states are then used to augment the underlying MDP state and to learn a dense cost function, easing the problem of quickly learning joint MDP/constraint dynamics. We empirically evaluate the effect of these methods on training a variety of RL algorithms over several constraints specified in Safety Gym, MuJoCo, and Atari environments.", "one-sentence_summary": "Specify safety constraints with formal languages to learn constraint structure representation and densely shape the CMDP cost function", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "quint|formal_language_constrained_markov_decision_processes", "supplementary_material": "/attachment/eafc6a33d141c356320b069de7bef68e1b0482e6.zip", "pdf": "/pdf/b7207b0628a0fcd319c603230ccf9af4e1863532.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zDlqvFg86y", "_bibtex": "@misc{\nquint2021formal,\ntitle={Formal Language Constrained Markov Decision Processes},\nauthor={Eleanor Quint and Dong Xu and Samuel W Flint and Stephen D Scott and Matthew Dwyer},\nyear={2021},\nurl={https://openreview.net/forum?id=NTP9OdaT6nm}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "05eoG3uM9ST", "original": null, "number": 1, "cdate": 1610040418073, "ddate": null, "tcdate": 1610040418073, "tmdate": 1610474016459, "tddate": null, "forum": "NTP9OdaT6nm", "replyto": "NTP9OdaT6nm", "invitation": "ICLR.cc/2021/Conference/Paper2291/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper proposes formulating safety constraints as formal language constrains, as a step toward bridging the gap between ML and software engineering, and enabling safe exploration in RL.  The authors responded and improved the paper significantly during the rebuttal period. Despite that, the reviewers raise the question, and I agree, that the significance of the paper, especially the novelty of the method, do not meet ICLR standard. The future version of the paper should be developed more in terms of the novelty, evaluations, and related works. \n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Formal Language Constrained Markov Decision Processes", "authorids": ["~Eleanor_Quint1", "~Dong_Xu1", "~Samuel_W_Flint1", "~Stephen_D_Scott1", "~Matthew_Dwyer1"], "authors": ["Eleanor Quint", "Dong Xu", "Samuel W Flint", "Stephen D Scott", "Matthew Dwyer"], "keywords": ["safe reinforcement learning", "formal languages", "constrained Markov decision process", "safety gym", "safety"], "abstract": "In order to satisfy safety conditions, an agent may be constrained from acting freely. A safe controller can be designed a priori if an environment is well understood, but not when learning is employed. In particular, reinforcement learned (RL) controllers require exploration, which can be hazardous in safety critical situations. We study the benefits of giving structure to the constraints of a constrained Markov decision process by specifying them in formal languages as a step towards using safety methods from software engineering and controller synthesis. We instantiate these constraints as finite automata to efficiently recognise constraint violations. Constraint states are then used to augment the underlying MDP state and to learn a dense cost function, easing the problem of quickly learning joint MDP/constraint dynamics. We empirically evaluate the effect of these methods on training a variety of RL algorithms over several constraints specified in Safety Gym, MuJoCo, and Atari environments.", "one-sentence_summary": "Specify safety constraints with formal languages to learn constraint structure representation and densely shape the CMDP cost function", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "quint|formal_language_constrained_markov_decision_processes", "supplementary_material": "/attachment/eafc6a33d141c356320b069de7bef68e1b0482e6.zip", "pdf": "/pdf/b7207b0628a0fcd319c603230ccf9af4e1863532.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zDlqvFg86y", "_bibtex": "@misc{\nquint2021formal,\ntitle={Formal Language Constrained Markov Decision Processes},\nauthor={Eleanor Quint and Dong Xu and Samuel W Flint and Stephen D Scott and Matthew Dwyer},\nyear={2021},\nurl={https://openreview.net/forum?id=NTP9OdaT6nm}\n}"}, "tags": [], "invitation": {"reply": {"forum": "NTP9OdaT6nm", "replyto": "NTP9OdaT6nm", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040418059, "tmdate": 1610474016443, "id": "ICLR.cc/2021/Conference/Paper2291/-/Decision"}}}, {"id": "LacrlxL88Hi", "original": null, "number": 3, "cdate": 1603934416236, "ddate": null, "tcdate": 1603934416236, "tmdate": 1606817499621, "tddate": null, "forum": "NTP9OdaT6nm", "replyto": "NTP9OdaT6nm", "invitation": "ICLR.cc/2021/Conference/Paper2291/-/Official_Review", "content": {"title": "Unclear contribution and lack of contrast and comparison with relevant literature", "review": "#### Summary\nThe paper proposes a constrained reinforcement learning (RL) formulation relying on constraints written in a formal language. The proposed formulation is based on constrained Markov decision processes where the constraint is represented as a deterministic finite automaton that rejects any trajectory violating the constraint. The proposed solution relies on transforming the automaton's sparse binary cost into an approximate dense cost and augmenting that with the reward objective. The paper presents a series of results from simulations in Safety Gym, MuJoCo, and Atari environments.\n\n#### Strength\n1. Constrained RL is certainly an important research area, having a variety of applications such as safety-critical problems.\n2. The constraints written in a formal language can represent structured properties, including non-Markovian ones.\n3. While a constraint in a formal language requires some domain knowledge, if it is correct, it can accelerate the training phase.\n\n#### Weakness\n1. The paper cites some of the related works; however, it is unable to distinguish its contributions compared to the existing methods. In particular, some of the most relevant approaches are using linear temporal logic (LTL) formulas for reward shaping (Camacho et al., 2017a;b), using LTL constraints in RL (Hasanbeig et al., 2018), and shielding mechanisms (Jansen et al., 2018; Alshiekh et al., 2018). The paper cites these references but does not differentiate itself from them.\n2. The experimental results of the paper are limited in terms of comparison with the existing methods. In particular, the only comparison is with baselines that do not use this automaton-based side information. However, since there exist many works with the capability of using such side information (some are mentioned in the previous point), these comparisons are essential for correctly evaluating the paper\u2019s contribution and significance.\n\n#### Recommended Decision\nGiven the paper's unclear contribution and lack of necessary comparison with the existing literature, I recommend rejecting the paper in its current form.\n\n#### Supporting Arguments\n1. The paper motivates the automaton-based constraints mostly for safety-critical applications. Nonetheless, the soft version of the constraints still lead to constraint violations and provide no safety guarantees.\n2. In addition to citing the related work, the paper needs to clearly state the differences with the existing methods.\n3. The writing requires somewhat considerable polishing. There are many typos, grammatical errors, and awkward phrases. Some notations are undefined, e.g., state space $S$ and action space $A$, $\\tau$, operators in regular expressions, alphabets $n$ and $f$ in Figure 1(b), and the evaluation metric called accumulated cost regret. While the reader can probably infer some of them, the paper should be self-contained in this regard.\n4. Figure 2 is not mentioned nor discussed in the paper. Some figures and tables are inserted far from where they are initially mentioned.\n5. To properly answer the second question posed in Section 4.3 regarding the choice of hyperparameters, a criterion, strategy, or heuristic is required. Reporting the better hyperparameters for some instances, on its own, is not sufficient as an answer.\n\n#### Questions\n1. How is $t_v(q_t)$ estimated using rollouts, given that the framework and the simulations seem to be for the model-free setting?\n2. Could you please explain the reasoning for the choice of $t_v^{baseline}$?\n\n#### Additional Feedback for Improving the Paper\n1. The following references are also highly related to this paper:\n    - Zhu, He, et al. \"An inductive synthesis framework for verifiable reinforcement learning.\" Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation. 2019.\n    - Fulton, Nathan, and Andr\u00e9 Platzer. \"Verifiably safe off-model reinforcement learning.\" International Conference on Tools and Algorithms for the Construction and Analysis of Systems. Springer, Cham, 2019.\n2. Compound adjectives require hyphens, e.g., \u201cformal language constrained MDP\u201d $\\to$ \u201cformal-language-constrained MDP\u201d.\n3. The clarity of the paragraphs on \u201cHard Constraints and Action Shaping\u201d can be improved.\n4. Please further motivate and explain why the constraints introduced in the simulation setting (except \u201cproximity\u201d) are required.\n5. Please make the bibliography consistent in terms of details provided for each reference and the formatting. Also, the year of some references is repeated.\n\n---\n#### UPDATE\nI thank the authors for their response. I am still concerned about the paper's novelty and contribution compared to the existing work as the differences seem minor. Also, the comparison with the approach by Camacho et al. (2017a;b) is not sufficient since different values of hyperparameter $\\lambda$ may perform better. Given the authors' revisions and the added results, I have increased my score. However, I am still inclined toward rejecting the paper.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2291/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2291/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Formal Language Constrained Markov Decision Processes", "authorids": ["~Eleanor_Quint1", "~Dong_Xu1", "~Samuel_W_Flint1", "~Stephen_D_Scott1", "~Matthew_Dwyer1"], "authors": ["Eleanor Quint", "Dong Xu", "Samuel W Flint", "Stephen D Scott", "Matthew Dwyer"], "keywords": ["safe reinforcement learning", "formal languages", "constrained Markov decision process", "safety gym", "safety"], "abstract": "In order to satisfy safety conditions, an agent may be constrained from acting freely. A safe controller can be designed a priori if an environment is well understood, but not when learning is employed. In particular, reinforcement learned (RL) controllers require exploration, which can be hazardous in safety critical situations. We study the benefits of giving structure to the constraints of a constrained Markov decision process by specifying them in formal languages as a step towards using safety methods from software engineering and controller synthesis. We instantiate these constraints as finite automata to efficiently recognise constraint violations. Constraint states are then used to augment the underlying MDP state and to learn a dense cost function, easing the problem of quickly learning joint MDP/constraint dynamics. We empirically evaluate the effect of these methods on training a variety of RL algorithms over several constraints specified in Safety Gym, MuJoCo, and Atari environments.", "one-sentence_summary": "Specify safety constraints with formal languages to learn constraint structure representation and densely shape the CMDP cost function", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "quint|formal_language_constrained_markov_decision_processes", "supplementary_material": "/attachment/eafc6a33d141c356320b069de7bef68e1b0482e6.zip", "pdf": "/pdf/b7207b0628a0fcd319c603230ccf9af4e1863532.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zDlqvFg86y", "_bibtex": "@misc{\nquint2021formal,\ntitle={Formal Language Constrained Markov Decision Processes},\nauthor={Eleanor Quint and Dong Xu and Samuel W Flint and Stephen D Scott and Matthew Dwyer},\nyear={2021},\nurl={https://openreview.net/forum?id=NTP9OdaT6nm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "NTP9OdaT6nm", "replyto": "NTP9OdaT6nm", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2291/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538099696, "tmdate": 1606915777656, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2291/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2291/-/Official_Review"}}}, {"id": "DUXFRioCrG4", "original": null, "number": 4, "cdate": 1604007549356, "ddate": null, "tcdate": 1604007549356, "tmdate": 1606813420998, "tddate": null, "forum": "NTP9OdaT6nm", "replyto": "NTP9OdaT6nm", "invitation": "ICLR.cc/2021/Conference/Paper2291/-/Official_Review", "content": {"title": "Novelty and technical contribution of the paper is unclear.", "review": "update after rebuttal: I think the paper, with the added discussion, improved. \n\nSummary and Contribution\n\nThis paper concerns safe reinforcement learning. In particular, it takes the perspective of constraining system behavior using formal languages instead of the usual constraint MDP framework where an additional (simple) cost function is used. The constraints are given as finite automata which are basically used as a product with the original MDP state space. The paper provides an empirical evaluation using several state of the art benchmarks.\n\n\n\nReasons for Score\n\nWith more details on the correctness, better literature study, and more experiments, the paper would be a clear accept. As of now, I see it as marginally above the acceptance threshold.\n\nStrengths\n\n- A relevant problem is tackled.\n- The usage of formal languages is relevant and has been demonstrated to be helpful in a number of results\n\nWeaknesses\n\n- The contribution of the paper is not clear. Several other works have done very similar approaches, and in much more depth.\n- The assumptions are not clear.\n\nQuestions for Authors\n\n- Please compare your work to the literature listed below. What is novel, how does it relate?\n- What is the exact assumption on prior knowledge? Does the MDP need to be known beforehand, or is the exploration of an RL algorithm basically guided by the finite automaton? \n- In the construction of the formal setting, what is different to the standard product construction of MDPs and finite automata? \n\n\nDetailed Comments\n\n- related work\n\nAt least three papers (one of them is cited) constrain MDP exploration using regular languages (in fact, omega-regular languages). I don't see the novelty, but I'm happy to be convinced otherwise. See:\n\nMohammadhosein Hasanbeig, Alessandro Abate, Daniel Kroening:\nLogically-Correct Reinforcement Learning. CoRR abs/1801.08099 (2018)\n\nMohammadhosein Hasanbeig, Alessandro Abate, Daniel Kroening:\nCautious Reinforcement Learning with Logical Constraints. AAMAS 2020: 483-491\n\nErnst Moritz Hahn, Mateo Perez, Sven Schewe, Fabio Somenzi, Ashutosh Trivedi, Dominik Wojtczak:\nOmega-Regular Objectives in Model-Free Reinforcement Learning. TACAS (1) 2019: 395-412\n\n- formal construction\nIn the very short theory part of the paper, the construction seems to me just as the standard construction to a product of an MDP and a finite (omega-regular) automaton. In a nutshell, one has a labelled MDP where the labels correspond to the alphabet of an automaton. Then, a run of the MDP produces a sequence of labels (a trace), which can be checked by the automaton. Now, a product construction of both yields an MDP where a reachability computation for so-called end components gives the probability to satisfy the formal language constraint. See for instance text books such as Baier and Katoen, Principles of Model Checking. As this part is the main body of the technical body of the paper, I would really like to understand, what is the difference. \n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2291/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2291/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Formal Language Constrained Markov Decision Processes", "authorids": ["~Eleanor_Quint1", "~Dong_Xu1", "~Samuel_W_Flint1", "~Stephen_D_Scott1", "~Matthew_Dwyer1"], "authors": ["Eleanor Quint", "Dong Xu", "Samuel W Flint", "Stephen D Scott", "Matthew Dwyer"], "keywords": ["safe reinforcement learning", "formal languages", "constrained Markov decision process", "safety gym", "safety"], "abstract": "In order to satisfy safety conditions, an agent may be constrained from acting freely. A safe controller can be designed a priori if an environment is well understood, but not when learning is employed. In particular, reinforcement learned (RL) controllers require exploration, which can be hazardous in safety critical situations. We study the benefits of giving structure to the constraints of a constrained Markov decision process by specifying them in formal languages as a step towards using safety methods from software engineering and controller synthesis. We instantiate these constraints as finite automata to efficiently recognise constraint violations. Constraint states are then used to augment the underlying MDP state and to learn a dense cost function, easing the problem of quickly learning joint MDP/constraint dynamics. We empirically evaluate the effect of these methods on training a variety of RL algorithms over several constraints specified in Safety Gym, MuJoCo, and Atari environments.", "one-sentence_summary": "Specify safety constraints with formal languages to learn constraint structure representation and densely shape the CMDP cost function", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "quint|formal_language_constrained_markov_decision_processes", "supplementary_material": "/attachment/eafc6a33d141c356320b069de7bef68e1b0482e6.zip", "pdf": "/pdf/b7207b0628a0fcd319c603230ccf9af4e1863532.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zDlqvFg86y", "_bibtex": "@misc{\nquint2021formal,\ntitle={Formal Language Constrained Markov Decision Processes},\nauthor={Eleanor Quint and Dong Xu and Samuel W Flint and Stephen D Scott and Matthew Dwyer},\nyear={2021},\nurl={https://openreview.net/forum?id=NTP9OdaT6nm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "NTP9OdaT6nm", "replyto": "NTP9OdaT6nm", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2291/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538099696, "tmdate": 1606915777656, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2291/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2291/-/Official_Review"}}}, {"id": "8omNL6e1Jhf", "original": null, "number": 1, "cdate": 1603657462434, "ddate": null, "tcdate": 1603657462434, "tmdate": 1606304815886, "tddate": null, "forum": "NTP9OdaT6nm", "replyto": "NTP9OdaT6nm", "invitation": "ICLR.cc/2021/Conference/Paper2291/-/Official_Review", "content": {"title": "Novel idea with encouraging results, concerns re: scalability and some clarity issues", "review": "The paper builds on the constrained MDP framework (Altman, 1999), by considering the special-case where the cost functions are defined in terms of states from a parser of a formal language. In the experiments the work uses deterministic finite automata (DFA) but in principle other more expressive classes could be used. Using a formal language to specify constraints may simplify model checking (although this is left to future work). \n\nPros:\n  - Novel idea: not aware of any other work using formal languages for MDP constrained. Although as the paper notes, formal languages (especially those derived from LTL formulae) have been used elsewhere in RL.\n  - Some encouraging empirical results: outperforms baseline in safety Gym, and is able to get higher episode return (!) as well as lower violations in some MuJoCo and Atari environments. (The effect of higher returns likely disappears with longer training or more powerful RL algorithms -- but the constraints being a useful inductive bias is nice.)\n \nCons:\n  - While the paper shows that useful constraints *can* be expressed in formal languages, it does not demonstrate that formal languages are a useful way of expressing the constraints. All the constraints in section 4.1 could have been specified quite easily by other means -- e.g. directly writing a cost function in Python. There is vague discussion in section 1 & 5 about how formal languages could enable model checking or connect more generally with safety engineering, but I'm unconvinced.\n\n   In particular, while formal languages might make for a specification that is amenable to verification, the neural network policy that is learned will still be difficult to verify -- especially when the properties depend on the (unknown) transition dynamics as they often do. Moreover, there is no reason why the constraints used in training need to be the same as those used for checking. Why not use a more expressive set of constraints in training, and then check the subset we can easily specify in a formal language?\n  - The method requires hand-designing appropriate constraints. In the experiments these were different for each class of environments (safety Gym, MuJoCo, Atari) and in some cases were specific to the environment (e.g. \"paddle-ball\" and \"danger-zone\"). I'm sympathetic to this -- safe exploration must need some extra source of information besides just task reward -- but given this there needs to be some discussion or ideally evaluation of the usability of this procedure. It certainly seems intractable for a normal end-user (unlike methods based on e.g. learning from demonstrations or preference comparisons).\n\n  Moreover, it seems quite challenging even for experienced RL practitioners to scale into complex environments. For example, the Atari constraints require having access to some high-level representation of the environment (paddle position, ball position). In real-world robotics, an analogue of this would require training a real-time object localization system.\n  - The clarity of the submission could be improved in places. In particular, a clear up-front definition of the problem setting would be useful. Are the transition dynamics known? Unknown? Not known analytically but with oracle query access? Are the cost functions Markovian or not?\n  - Hard action space seems to require a resettable environment or known transition dynamics, which weakens the results. RL usually assumes unknown transition dynamics: you should compare to baselines that also make use of this information for fairness, or at least make explicit this difference.\n\nI find the paper borderline -- the idea is interesting and the results encouraging -- but the serious open questions re: whether formal languages are useful and, if they are, whether the approach is scalable make me vote against acceptance.\n\nSome particular things I would suggest addressing in revisions:\n  1. Greater justification of why formal languages. This is the most important point here and I'd consider increasing my vote if the usefulness of this approach was clarified. For example, can you cite any papers that use formal languages as specification to validate black-box systems (like RL policy + unknown MDP)?\n  2. Discussion of how this method can scale, ideally with empirical validation. For example, could you actually learn a formal language specification (probably hard but impressive), or at least have a case-study where you apply this approach to something that doesn't have a nice low-dimensional state representation? Alternatively, if this method is limited to low-dimensional environments -- why use your method here and not a more classical control method?\n\nSome clarifying questions I'd also appreciate if the authors could address during the discussion:\n  1. How is $t_v$ computed? It seems underdefined -- it is a function of $q_t$ but the natural language definition given seems like it is in terms of constraint violations from rollouts of the current MDP state. As I understand it, many different states $s_t$ could lead to the same $q_t$ -- nor is making it a function of $s_t$ sufficient since the same $s_t$ could lead to different $q_t$ depending on previous states!\n\n    Even if this issue is resolved (e.g. making it a function of both $s_t$ and $q_t$), computing it seems like it would require rollouts from $s_t$. This seems very expensive -- is my understanding correct? If so, could you report wall-clock times, or possibly train different algorithms for a fixed wall-clock time? Otherwise comparing algorithms for the same number of training timesteps seems unfair if the shaping effectively involves extra hidden environment interactions.\n  2. Perhaps related to this confusion, you state in the hard action shaping section, that \"before finalizing that choice, simulates stepping the DFA with the resulting token from the translation function\". I am confused how one does this -- isn't the point we don't know the transition dynamics, so do not know the next MDP state we will end up in? If we do have a transition model, why don't we do planning instead to never take a sequence of actions that violate constraints?\n  3. Any ideas why hard action shaping in both Training and Evaluation sometimes does worse than just during Training or just during Evaluation?\n\nSome more detailed suggestions for improvements (no need to respond to these, they did not affect my decision in any significant way):\n  - Page 2, \u201cteacher advice\u201d. \u201cRL safety\u201d is not a synonym for \u201csafe exploration\u201d, since there are other forms of safety related to RL (e.g. satisfying constraints in the final policy; optimizing a desirable objective, like user happiness, rather than a misaligned one, like short-term user engagement). \n  - Page 3, \\delta: what is $A$? It is not defined anywhere that I can see \u2013 should this be $\\Sigma$?\n  - Page 3, $D_C$ is not a function since it takes only a single token but outputs the next state, which is dependent on the current state (not supplied). This also makes the constraints non-Markovian which seems like it should be emphasized more (although this is somewhat implicit in the \u201cConstraint State Augmentation\u201d section).\n  - Page 3, Constraint State Augmentation: I think the dimensions should be integer ceiling not integer floor.\n  - Page 3, Learned Dense Cost: perhaps make $t_{v}$ upper case since it's a random variable. This method also seems a bit ad-hoc \u2013 is there a theoretical justification for it? If not, how do other choices (e.g. a different exponent base to 1/2) change things?\n  - Page 7-8: tables of numbers were a bit hard to digest. Consider turning some of them into figures, e.g. bar charts?\n  - Table 5: a bold entry for \u201cTraining Only\u201d in Breakout seems incorrect (for dithering, \u201cTraining and Evaluation\u201d does better).\n  - Discussion: \u201cwas never be empty\u201d->\u201dwas never empty\u201d. (\u201cBest effort\u201d also seems an odd phrase for this property.)\n\n### Update: the author's revision have clarified many of the points of confusion above, and have largely addressed my concern re: the value of formal languages in this setting. I continue to be concerned about the scalability of this class of methods, but since it is a problem common to other work in this field, I do not want to hold this too much against the current submission. Given this I have increased my score to a 6. ###", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2291/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2291/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Formal Language Constrained Markov Decision Processes", "authorids": ["~Eleanor_Quint1", "~Dong_Xu1", "~Samuel_W_Flint1", "~Stephen_D_Scott1", "~Matthew_Dwyer1"], "authors": ["Eleanor Quint", "Dong Xu", "Samuel W Flint", "Stephen D Scott", "Matthew Dwyer"], "keywords": ["safe reinforcement learning", "formal languages", "constrained Markov decision process", "safety gym", "safety"], "abstract": "In order to satisfy safety conditions, an agent may be constrained from acting freely. A safe controller can be designed a priori if an environment is well understood, but not when learning is employed. In particular, reinforcement learned (RL) controllers require exploration, which can be hazardous in safety critical situations. We study the benefits of giving structure to the constraints of a constrained Markov decision process by specifying them in formal languages as a step towards using safety methods from software engineering and controller synthesis. We instantiate these constraints as finite automata to efficiently recognise constraint violations. Constraint states are then used to augment the underlying MDP state and to learn a dense cost function, easing the problem of quickly learning joint MDP/constraint dynamics. We empirically evaluate the effect of these methods on training a variety of RL algorithms over several constraints specified in Safety Gym, MuJoCo, and Atari environments.", "one-sentence_summary": "Specify safety constraints with formal languages to learn constraint structure representation and densely shape the CMDP cost function", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "quint|formal_language_constrained_markov_decision_processes", "supplementary_material": "/attachment/eafc6a33d141c356320b069de7bef68e1b0482e6.zip", "pdf": "/pdf/b7207b0628a0fcd319c603230ccf9af4e1863532.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zDlqvFg86y", "_bibtex": "@misc{\nquint2021formal,\ntitle={Formal Language Constrained Markov Decision Processes},\nauthor={Eleanor Quint and Dong Xu and Samuel W Flint and Stephen D Scott and Matthew Dwyer},\nyear={2021},\nurl={https://openreview.net/forum?id=NTP9OdaT6nm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "NTP9OdaT6nm", "replyto": "NTP9OdaT6nm", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2291/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538099696, "tmdate": 1606915777656, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2291/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2291/-/Official_Review"}}}, {"id": "G7IP8Caz1VU", "original": null, "number": 10, "cdate": 1606304725687, "ddate": null, "tcdate": 1606304725687, "tmdate": 1606304725687, "tddate": null, "forum": "NTP9OdaT6nm", "replyto": "NbEcKkiMBYF", "invitation": "ICLR.cc/2021/Conference/Paper2291/-/Official_Comment", "content": {"title": "Thanks for the clarification", "comment": "Thank you for the detailed response. This has addressed some of my concerns. In particular, I better understand the value of using formal languages in this setting.  I'm still not convinced about the method's scalability given the requirement for hand-designed constraints but, as you point out, this is a more general problem in this domain. Given this, I'll increase my score."}, "signatures": ["ICLR.cc/2021/Conference/Paper2291/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2291/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Formal Language Constrained Markov Decision Processes", "authorids": ["~Eleanor_Quint1", "~Dong_Xu1", "~Samuel_W_Flint1", "~Stephen_D_Scott1", "~Matthew_Dwyer1"], "authors": ["Eleanor Quint", "Dong Xu", "Samuel W Flint", "Stephen D Scott", "Matthew Dwyer"], "keywords": ["safe reinforcement learning", "formal languages", "constrained Markov decision process", "safety gym", "safety"], "abstract": "In order to satisfy safety conditions, an agent may be constrained from acting freely. A safe controller can be designed a priori if an environment is well understood, but not when learning is employed. In particular, reinforcement learned (RL) controllers require exploration, which can be hazardous in safety critical situations. We study the benefits of giving structure to the constraints of a constrained Markov decision process by specifying them in formal languages as a step towards using safety methods from software engineering and controller synthesis. We instantiate these constraints as finite automata to efficiently recognise constraint violations. Constraint states are then used to augment the underlying MDP state and to learn a dense cost function, easing the problem of quickly learning joint MDP/constraint dynamics. We empirically evaluate the effect of these methods on training a variety of RL algorithms over several constraints specified in Safety Gym, MuJoCo, and Atari environments.", "one-sentence_summary": "Specify safety constraints with formal languages to learn constraint structure representation and densely shape the CMDP cost function", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "quint|formal_language_constrained_markov_decision_processes", "supplementary_material": "/attachment/eafc6a33d141c356320b069de7bef68e1b0482e6.zip", "pdf": "/pdf/b7207b0628a0fcd319c603230ccf9af4e1863532.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zDlqvFg86y", "_bibtex": "@misc{\nquint2021formal,\ntitle={Formal Language Constrained Markov Decision Processes},\nauthor={Eleanor Quint and Dong Xu and Samuel W Flint and Stephen D Scott and Matthew Dwyer},\nyear={2021},\nurl={https://openreview.net/forum?id=NTP9OdaT6nm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NTP9OdaT6nm", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2291/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2291/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2291/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2291/Authors|ICLR.cc/2021/Conference/Paper2291/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2291/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850104, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2291/-/Official_Comment"}}}, {"id": "vWGvXmhOsg2", "original": null, "number": 9, "cdate": 1606196609663, "ddate": null, "tcdate": 1606196609663, "tmdate": 1606246543857, "tddate": null, "forum": "NTP9OdaT6nm", "replyto": "Oxw_UHm5sC0", "invitation": "ICLR.cc/2021/Conference/Paper2291/-/Official_Comment", "content": {"title": "Author Response (Camacho et al. experiment results)", "comment": "The method of Camacho et al. (2017b), uses reward shaping with a potential function such that the potential of each automaton state grows linearly as the distance to an accepting state decreases. Originally, they use it for specifying objectives, but there is nothing essential about the method that keeps it from being used to specify constraints instead. Our analysis is that, aside from in the Point Goal1 environment, the lack of flexibility in the shaping leads to policies which are extremely conservative. Qualitatively, the learned policy doesn't move hardly at all, seemingly in order to avoid potentially violating the constraint. This problem can be ameliorated either by not using shaping and only penalising when an accepting state is reached (allowing for more flexibility in the policy when not in close proximity to violating states), or learning the shaping function as we propose, in order to correctly assign very low cost to moving into automaton states which are found to be safe in training.\n\nThese are the results of running Camacho et al. (2017b) in Safety Gym environments with reward shaping coefficient -0.001 (equal to the lowest value we employ in our methods). The values have been normalised with the same baselines as the results of Table 1 in the submission. More results will be posted as they finish, as a last few experiments are still running.\n\nPoint Goal 1\nReturn:\t\t 0.3141\nViolation:\t 0.0928\nCost Rate:\t 0.1037\n\nPoint Goal 2\nReturn:\t\t-0.0193\nViolation:\t 0.1799\nCost Rate:\t 0.2404\n\nPoint Button 1\nReturn:\t\t-0.0086\nViolation:\t 0.0463\nCost Rate:\t 0.102\n\nPoint Button 2\nReturn:\t\t-0.0578\nViolation:\t 0.0547\nCost Rate:\t 0.016\n\nPoint Push 1\nReturn:\t\t-0.0062\nViolation:\t 0.0779\nCost Rate:\t 0.0690\n\nPoint Push 2\nReturn:\t\t-0.9914\nViolation:\t 0.0963\nCost Rate:\t 0.079\t\n\nCar Goal 1\nReturn:\t\t-0.0031\nViolation:\t 0.154\nCost Rate:\t 0.1919\n\nCar Goal 2 Return: 0.0021 Violation: 0.0715\tCost Rate: 0.0872\n\nCar Button 1 Return: 0.0252 Violation: 0.1418 Cost Rate: 0.0494\n\nCar Button 2 Return: -0.0083 Violation: 0.0757 Cost Rate: 0.087\n\nCar Push 1 Return: 0.0085 Violation: 0.0581 Cost Rate: 0.0722\n\nCar Push 2  Return: 0.096 Violation: 0.0493 Cost Rate: 0.0231\n\nedit: Added the rest of the experiments in Car Goal2, Car Button1,2, Car Push1,2"}, "signatures": ["ICLR.cc/2021/Conference/Paper2291/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2291/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Formal Language Constrained Markov Decision Processes", "authorids": ["~Eleanor_Quint1", "~Dong_Xu1", "~Samuel_W_Flint1", "~Stephen_D_Scott1", "~Matthew_Dwyer1"], "authors": ["Eleanor Quint", "Dong Xu", "Samuel W Flint", "Stephen D Scott", "Matthew Dwyer"], "keywords": ["safe reinforcement learning", "formal languages", "constrained Markov decision process", "safety gym", "safety"], "abstract": "In order to satisfy safety conditions, an agent may be constrained from acting freely. A safe controller can be designed a priori if an environment is well understood, but not when learning is employed. In particular, reinforcement learned (RL) controllers require exploration, which can be hazardous in safety critical situations. We study the benefits of giving structure to the constraints of a constrained Markov decision process by specifying them in formal languages as a step towards using safety methods from software engineering and controller synthesis. We instantiate these constraints as finite automata to efficiently recognise constraint violations. Constraint states are then used to augment the underlying MDP state and to learn a dense cost function, easing the problem of quickly learning joint MDP/constraint dynamics. We empirically evaluate the effect of these methods on training a variety of RL algorithms over several constraints specified in Safety Gym, MuJoCo, and Atari environments.", "one-sentence_summary": "Specify safety constraints with formal languages to learn constraint structure representation and densely shape the CMDP cost function", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "quint|formal_language_constrained_markov_decision_processes", "supplementary_material": "/attachment/eafc6a33d141c356320b069de7bef68e1b0482e6.zip", "pdf": "/pdf/b7207b0628a0fcd319c603230ccf9af4e1863532.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zDlqvFg86y", "_bibtex": "@misc{\nquint2021formal,\ntitle={Formal Language Constrained Markov Decision Processes},\nauthor={Eleanor Quint and Dong Xu and Samuel W Flint and Stephen D Scott and Matthew Dwyer},\nyear={2021},\nurl={https://openreview.net/forum?id=NTP9OdaT6nm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NTP9OdaT6nm", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2291/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2291/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2291/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2291/Authors|ICLR.cc/2021/Conference/Paper2291/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2291/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850104, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2291/-/Official_Comment"}}}, {"id": "NVgWm9tEsT4", "original": null, "number": 8, "cdate": 1606194408696, "ddate": null, "tcdate": 1606194408696, "tmdate": 1606194408696, "tddate": null, "forum": "NTP9OdaT6nm", "replyto": "NTP9OdaT6nm", "invitation": "ICLR.cc/2021/Conference/Paper2291/-/Official_Comment", "content": {"title": "Rebuttal Revision Change Notes", "comment": "Thank you for all of the very helpful feedback! To summarise our changes to the submission:\n\nSec. 2, par. Formal Languages\n- Added citations:\n\t- Zhu, He, et al. \"An inductive synthesis framework for verifiable reinforcement learning.\" Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation. 2019.\n\t- Fulton, Nathan, and Andr\u00e9 Platzer. \"Verifiably safe off-model reinforcement learning.\" International Conference on Tools and Algorithms for the Construction and Analysis of Systems. Springer, Cham, 2019.\n\t- Mohammadhosein Hasanbeig, Alessandro Abate, Daniel Kroening: Cautious Reinforcement Learning with Logical Constraints. AAMAS 2020: 483-491\n\t- Ernst Moritz Hahn, Mateo Perez, Sven Schewe, Fabio Somenzi, Ashutosh Trivedi, Dominik Wojtczak: Omega-Regular Objectives in Model-Free Reinforcement Learning. TACAS (1) 2019: 395-412\n\nSec. 2, par. Teacher Advice\n- \"safe RL\" -> \"safe exploration\"\n- Added text differentiating our work from the cited shielding papers, (Jansen et al. 2018) and (Alshiekh et al., 2018), and the papers on solving non-Markovian reward decision processes (Camacho et al., 2017a;b)\n\nSec. 3:\n- Updated the definition of $D_c$ and the CMDP const function $c$ to account for the tracking of the constraint state $Q_c$\n- Added a note specifying that no knowledge of the MDP transition function is necessary nor does \n\nSec. 3, par. Constraint State Augmentation:\n- Changed incorrect floor to ceiling\n\nSec. 3, par. Recognizer Function\n- Corrected part of the definition of $\\delta$ from \"A\" -> $\\Sigma$\n\nSec. 3, par. Learned Dense Cost\n- $t_v$ -> $T_v$ to properly reflect that it's a random variable\n- Added that $T_v$ is calculated as a Monte Carlo estimate of the expected value and is tracked by an exponential moving average.\n\nSec. 4.1:\n- Added a note that the cost functions are binary unless we're performing dense cost shaping.\n\nSec. 4.3.2:\n- Added very brief discussion of Fig. 2 to the text.\n\nSec. 5:\n- \u201cwas never be empty\u201d -> \u201dwas never empty\u201d\n\nTables:\n- Fixed bolding"}, "signatures": ["ICLR.cc/2021/Conference/Paper2291/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2291/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Formal Language Constrained Markov Decision Processes", "authorids": ["~Eleanor_Quint1", "~Dong_Xu1", "~Samuel_W_Flint1", "~Stephen_D_Scott1", "~Matthew_Dwyer1"], "authors": ["Eleanor Quint", "Dong Xu", "Samuel W Flint", "Stephen D Scott", "Matthew Dwyer"], "keywords": ["safe reinforcement learning", "formal languages", "constrained Markov decision process", "safety gym", "safety"], "abstract": "In order to satisfy safety conditions, an agent may be constrained from acting freely. A safe controller can be designed a priori if an environment is well understood, but not when learning is employed. In particular, reinforcement learned (RL) controllers require exploration, which can be hazardous in safety critical situations. We study the benefits of giving structure to the constraints of a constrained Markov decision process by specifying them in formal languages as a step towards using safety methods from software engineering and controller synthesis. We instantiate these constraints as finite automata to efficiently recognise constraint violations. Constraint states are then used to augment the underlying MDP state and to learn a dense cost function, easing the problem of quickly learning joint MDP/constraint dynamics. We empirically evaluate the effect of these methods on training a variety of RL algorithms over several constraints specified in Safety Gym, MuJoCo, and Atari environments.", "one-sentence_summary": "Specify safety constraints with formal languages to learn constraint structure representation and densely shape the CMDP cost function", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "quint|formal_language_constrained_markov_decision_processes", "supplementary_material": "/attachment/eafc6a33d141c356320b069de7bef68e1b0482e6.zip", "pdf": "/pdf/b7207b0628a0fcd319c603230ccf9af4e1863532.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zDlqvFg86y", "_bibtex": "@misc{\nquint2021formal,\ntitle={Formal Language Constrained Markov Decision Processes},\nauthor={Eleanor Quint and Dong Xu and Samuel W Flint and Stephen D Scott and Matthew Dwyer},\nyear={2021},\nurl={https://openreview.net/forum?id=NTP9OdaT6nm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NTP9OdaT6nm", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2291/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2291/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2291/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2291/Authors|ICLR.cc/2021/Conference/Paper2291/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2291/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850104, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2291/-/Official_Comment"}}}, {"id": "vkCR2s1-aF-", "original": null, "number": 7, "cdate": 1606193904998, "ddate": null, "tcdate": 1606193904998, "tmdate": 1606193949614, "tddate": null, "forum": "NTP9OdaT6nm", "replyto": "DUXFRioCrG4", "invitation": "ICLR.cc/2021/Conference/Paper2291/-/Official_Comment", "content": {"title": "Author Response", "comment": "Q. Please compare your work to the literature listed below. What is novel, and how does it relate?\n\nA. The main novelty of our work is the use of the CMDP framework with constraint automata and methods which make the use of the inductive bias of the automaton state with RL policies represented by neural networks (though there is nothing essential about neural networks), as well as experiments which quantify feasibility and utility. The suggested papers are similar to ours in their use of automata encoding constraints and the use of a mechanism to encourage an RL policy to satisfy those constraints, but the mechanisms employed by those papers seem relatively bespoke and limit the previous work to relatively small MDPs. The suggested papers' use of formal languages in RL safety further motivates our work in combining automata and deep learning in a more general way.\n\nThe most significant differences between our work and Hasanbeig et al. (2020) are 1) their use of the observation area which requires knowledge of the neighbors in the MDP graph, which is unsuitable for use in large, highly connected, or unknown MDPs (vs. no such requirement) and, 2) their mechanism used to enforce safety constraints (\"safe padding\" and double learners for them vs. CMDP formalism and cost/reward shaping or Lagrangian method for ours). Hasanbeig et al. (2018) is similar.\n\nHahn et al.'s goal is to reach an objective almost surely, and uses reward shaping with very large values compared to the underlying reward function to ensure that off-the-shelf model-free RL will satisfy the objective. This strategy does not work in the case of a constraint rather than an objective because it will lead to an overly conservative policy, similar to the case in our experiments where a large reward shaping value leads to both very low constraint violations and return.\n\nOverall, the core ideas are similar, but there are significant problems with efficiency when Hasabeig et al.~(2018,2020) is applied to larger, unknown MDPs, or with applying the objective-specification method of Hahn et al. to specifying a constraint instead. \n\nQ. What is the exact assumption on prior knowledge? Does the MDP need to be known beforehand, or is the exploration of an RL algorithm basically guided by the finite automaton?\n\nA. The proposed setting does not require knowledge of MDP dynamics, and the RL agent must use exploration to learn a policy; epsilon-greedy exploration was employed in our experiments. The finite automaton can, through the cost or reward signals, provide a weak incentive or disincentive to visit certain MDP states, which allows a human with any knowledge of MDP dynamics to provide a weak inductive bias on the exploration (e.g., what action sequences are known to perform poorly a priori and thus should not be used repeatedly in exploration, like violently overactuating a robot limb or dithering by moving back and forth repeatedly).\n\nQ. How is this different from the standard approach described in e.g., the textbook ``Principles of Model Checking''?\n\nA. The core approach of using a product MDP is identical; however, calculating the probability of satisfying the formal language constraint in an MDP as large as we're working in is intractable, so we use optimisation instead to try to find a policy which best satisfies the dual objectives of constraint satisfaction and accumulated reward over a rollout. The novelty in this work isn't in the idea of the product MDP, but rather the use of the CMDP framework with the product MDP in order to bring together well known CMDP mechanisms with formal language constraints, as well as the methods of augmenting the RL policy input with the constraint automaton state and dense cost shaping. I.e., we study how to get the product MDP construction to work well with large MDPs and modern RL algorithms which train policies represented by neural networks. This is demonstrated by the choice of environments in our experiments."}, "signatures": ["ICLR.cc/2021/Conference/Paper2291/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2291/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Formal Language Constrained Markov Decision Processes", "authorids": ["~Eleanor_Quint1", "~Dong_Xu1", "~Samuel_W_Flint1", "~Stephen_D_Scott1", "~Matthew_Dwyer1"], "authors": ["Eleanor Quint", "Dong Xu", "Samuel W Flint", "Stephen D Scott", "Matthew Dwyer"], "keywords": ["safe reinforcement learning", "formal languages", "constrained Markov decision process", "safety gym", "safety"], "abstract": "In order to satisfy safety conditions, an agent may be constrained from acting freely. A safe controller can be designed a priori if an environment is well understood, but not when learning is employed. In particular, reinforcement learned (RL) controllers require exploration, which can be hazardous in safety critical situations. We study the benefits of giving structure to the constraints of a constrained Markov decision process by specifying them in formal languages as a step towards using safety methods from software engineering and controller synthesis. We instantiate these constraints as finite automata to efficiently recognise constraint violations. Constraint states are then used to augment the underlying MDP state and to learn a dense cost function, easing the problem of quickly learning joint MDP/constraint dynamics. We empirically evaluate the effect of these methods on training a variety of RL algorithms over several constraints specified in Safety Gym, MuJoCo, and Atari environments.", "one-sentence_summary": "Specify safety constraints with formal languages to learn constraint structure representation and densely shape the CMDP cost function", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "quint|formal_language_constrained_markov_decision_processes", "supplementary_material": "/attachment/eafc6a33d141c356320b069de7bef68e1b0482e6.zip", "pdf": "/pdf/b7207b0628a0fcd319c603230ccf9af4e1863532.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zDlqvFg86y", "_bibtex": "@misc{\nquint2021formal,\ntitle={Formal Language Constrained Markov Decision Processes},\nauthor={Eleanor Quint and Dong Xu and Samuel W Flint and Stephen D Scott and Matthew Dwyer},\nyear={2021},\nurl={https://openreview.net/forum?id=NTP9OdaT6nm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NTP9OdaT6nm", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2291/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2291/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2291/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2291/Authors|ICLR.cc/2021/Conference/Paper2291/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2291/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850104, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2291/-/Official_Comment"}}}, {"id": "Oxw_UHm5sC0", "original": null, "number": 6, "cdate": 1606193689848, "ddate": null, "tcdate": 1606193689848, "tmdate": 1606193857616, "tddate": null, "forum": "NTP9OdaT6nm", "replyto": "LacrlxL88Hi", "invitation": "ICLR.cc/2021/Conference/Paper2291/-/Official_Comment", "content": {"title": "Author Response", "comment": "We'll add the differences between our work and the works listed to the related work section of the paper, but we'll list them here as well:\n\n- Camacho et al. (2017a;b) both focus on the problem of using automata to represent non-Markovian reward functions and reward shaping to train the policy. Our work uses the CMDP formalism and cost function, and shapes the cost function with a learned rather than static potential function on the automata states. Further, our work augments the MDP observation with the automata state and Camacho et al. (2017a;b) do not. Further, our framework strictly generalises the methods in these papers: We can recreate their methods by using reward shaping as the CMDP mechanism and a particular choice of dense cost shaping which is not learned, as well as not using state augmentation in policy learning. We'll post results of our experiments with the method described in Camacho et al. 2017b on Safety Gym environments below.\n\n- Hasanbeig et al.  (2018) forms a product MDP to avoid constraint violations defined in LTL, but approximates the Q-function with one network per automaton state and defines a reward function which only returns a positive value when the automaton transitions to an accepting state. Our work doesn't prescribe anything about the form of the learner other than having an input for the automaton state, or the MDP reward function (e.g., when we use a Lagrangian method rather than reward shaping). We do not compare their methods to ours experimentally because they study behavioural constraints which must be satisfied, rather than avoided, and define the reward function accordingly. It is unclear how the reward function they define should be adapted to the case of avoiding constraint violations.\n\n- Jansen et al. (2018) and Alshiekh et al. (2018) each construct a probabilistic shield to avoid unsafe behavior similar to our hard constraints and action shaping. However, each requires the underlying MDP states to be enumerable and few enough so that a shield can be constructed efficiently, which our hard shaping does not need. Further, neither explore the addition of constraint state to the MDP state for learning. We do not compare their methods to ours experimentally because they are intractable in the environments used in our experiments.\n\nQ. How is $t_v$ estimated using rollouts, given that the framework and the simulations seem to be for the model-free setting?\n\nA. Similar to how the Lagrangian penalty parameter is calculated, we calculate an empirical rolling average (essentially a Monte Carlo estimate over trajectories) for each constraint state.\n\nQ. Reasoning for choice of $t_v^{baseline}$\n\nA. Similar to Ray et al. (2019) [1], we set a target for how many violations per 1000 steps are allowable. This target is used both in the calculation of the Lagrangian penalty parameter and the cost shaping function we propose. Its effect in the equation is to make the exponent close to 1 when the empirical $t_v$ is close to the target and make the exponent greater than or less than 1 exactly when $t_v$ is greater than or less than the target.\n\n[1] Alex Ray, Joshua Achiam, and Dario Amodei. Benchmarking safe exploration in deep reinforcement\nlearning. arXiv preprint arXiv:1910.01708, 2019.\n\nQ. Please further motivate and explain why the constraints introduced in the simulation setting (except \u201cproximity\u201d) are required.\n\nA. Constraints which model undesirable, but not catastrophic,  properties of a robotic policy, e.g., overactuation of a motor with \"actuation\" and high frequency jitter with \"dithering\", were introduced in Mujoco as a model robotics environment. These same constraints are applied to Atari to see if simple, action-based constraints could help guide policy learning to higher reward outcomes as a form of teacher advice. The \"paddle ball\" and \"dangerzone\" constraints were designed for their Atari environment specifically to see if the same effect could be achieved with a weak notion of safety (i.e., avoiding states which end the episode).\n\nThank you for bringing the papers you listed to our attention. We'll add them to the related work."}, "signatures": ["ICLR.cc/2021/Conference/Paper2291/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2291/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Formal Language Constrained Markov Decision Processes", "authorids": ["~Eleanor_Quint1", "~Dong_Xu1", "~Samuel_W_Flint1", "~Stephen_D_Scott1", "~Matthew_Dwyer1"], "authors": ["Eleanor Quint", "Dong Xu", "Samuel W Flint", "Stephen D Scott", "Matthew Dwyer"], "keywords": ["safe reinforcement learning", "formal languages", "constrained Markov decision process", "safety gym", "safety"], "abstract": "In order to satisfy safety conditions, an agent may be constrained from acting freely. A safe controller can be designed a priori if an environment is well understood, but not when learning is employed. In particular, reinforcement learned (RL) controllers require exploration, which can be hazardous in safety critical situations. We study the benefits of giving structure to the constraints of a constrained Markov decision process by specifying them in formal languages as a step towards using safety methods from software engineering and controller synthesis. We instantiate these constraints as finite automata to efficiently recognise constraint violations. Constraint states are then used to augment the underlying MDP state and to learn a dense cost function, easing the problem of quickly learning joint MDP/constraint dynamics. We empirically evaluate the effect of these methods on training a variety of RL algorithms over several constraints specified in Safety Gym, MuJoCo, and Atari environments.", "one-sentence_summary": "Specify safety constraints with formal languages to learn constraint structure representation and densely shape the CMDP cost function", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "quint|formal_language_constrained_markov_decision_processes", "supplementary_material": "/attachment/eafc6a33d141c356320b069de7bef68e1b0482e6.zip", "pdf": "/pdf/b7207b0628a0fcd319c603230ccf9af4e1863532.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zDlqvFg86y", "_bibtex": "@misc{\nquint2021formal,\ntitle={Formal Language Constrained Markov Decision Processes},\nauthor={Eleanor Quint and Dong Xu and Samuel W Flint and Stephen D Scott and Matthew Dwyer},\nyear={2021},\nurl={https://openreview.net/forum?id=NTP9OdaT6nm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NTP9OdaT6nm", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2291/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2291/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2291/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2291/Authors|ICLR.cc/2021/Conference/Paper2291/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2291/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850104, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2291/-/Official_Comment"}}}, {"id": "bfd9djH3Q9j", "original": null, "number": 5, "cdate": 1606193514398, "ddate": null, "tcdate": 1606193514398, "tmdate": 1606193514398, "tddate": null, "forum": "NTP9OdaT6nm", "replyto": "oSiG5pIO594", "invitation": "ICLR.cc/2021/Conference/Paper2291/-/Official_Comment", "content": {"title": "Author Response", "comment": "Q. I couldn't find the definition of the cost assignment functions for the environments. Are they just binary?\n\nA. Yes, in all environments the cost function was binary. Binary cost is built into the Safety Gym domain and we used binary cost elsewhere for consistency. We'll note this in the paper.\n\nQ. When a constraint gets violated, the DFA gets into an accepting state but it seems the episode does not get terminated. Does the DFA get reset in the next step?\n\nA. Yes it does. Note that in state-based constraints, like Safety Gym, the constraint can be violated in consecutive timesteps if no action has been taken to move out of the forbidden state (i.e., the DFA is reset but in the next step transitions into a violating state again).\n\nQ. Figure 2 is not discussed in the text at all. Do the data points represent different values for $\\lambda$?\n\nA. Yes, the data points in Fig. 2 represent different values for $\\lambda$, the reward shaping coefficient. The text discussing it which was accidentally omitted from Sec. 4.3 will be posted in a separate thread. Apologies.\n\nTo address the first con listed, the significance of this work is partly in studying how to bring methods from cyberphysical systems and software engineering (i.e., formal language constraints) together with RL methods (i.e., the CMDP formalism) in a way that works well with modern RL algorithms and that solves challenging environments proposed by Ray et al. [1]. Though the added benefits of formal languages like verification are not studied in this paper, they are an important part of the motivation to study the use of formal language constraints in RL. Future work aside, we believe the clear improvements in empirical performance in Safety Gym environments proposed by Ray et al. clearly demonstrates the benefits of using the specific formal language CMDP construction we proposed along with the state augmentation it enables (along with the benefits in Mujoco and Atari environments).\n\n[1] Alex Ray, Joshua Achiam, and Dario Amodei. Benchmarking safe exploration in deep reinforcement learning. arXiv preprint arXiv:1910.01708, 2019"}, "signatures": ["ICLR.cc/2021/Conference/Paper2291/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2291/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Formal Language Constrained Markov Decision Processes", "authorids": ["~Eleanor_Quint1", "~Dong_Xu1", "~Samuel_W_Flint1", "~Stephen_D_Scott1", "~Matthew_Dwyer1"], "authors": ["Eleanor Quint", "Dong Xu", "Samuel W Flint", "Stephen D Scott", "Matthew Dwyer"], "keywords": ["safe reinforcement learning", "formal languages", "constrained Markov decision process", "safety gym", "safety"], "abstract": "In order to satisfy safety conditions, an agent may be constrained from acting freely. A safe controller can be designed a priori if an environment is well understood, but not when learning is employed. In particular, reinforcement learned (RL) controllers require exploration, which can be hazardous in safety critical situations. We study the benefits of giving structure to the constraints of a constrained Markov decision process by specifying them in formal languages as a step towards using safety methods from software engineering and controller synthesis. We instantiate these constraints as finite automata to efficiently recognise constraint violations. Constraint states are then used to augment the underlying MDP state and to learn a dense cost function, easing the problem of quickly learning joint MDP/constraint dynamics. We empirically evaluate the effect of these methods on training a variety of RL algorithms over several constraints specified in Safety Gym, MuJoCo, and Atari environments.", "one-sentence_summary": "Specify safety constraints with formal languages to learn constraint structure representation and densely shape the CMDP cost function", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "quint|formal_language_constrained_markov_decision_processes", "supplementary_material": "/attachment/eafc6a33d141c356320b069de7bef68e1b0482e6.zip", "pdf": "/pdf/b7207b0628a0fcd319c603230ccf9af4e1863532.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zDlqvFg86y", "_bibtex": "@misc{\nquint2021formal,\ntitle={Formal Language Constrained Markov Decision Processes},\nauthor={Eleanor Quint and Dong Xu and Samuel W Flint and Stephen D Scott and Matthew Dwyer},\nyear={2021},\nurl={https://openreview.net/forum?id=NTP9OdaT6nm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NTP9OdaT6nm", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2291/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2291/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2291/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2291/Authors|ICLR.cc/2021/Conference/Paper2291/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2291/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850104, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2291/-/Official_Comment"}}}, {"id": "NbEcKkiMBYF", "original": null, "number": 4, "cdate": 1606193446087, "ddate": null, "tcdate": 1606193446087, "tmdate": 1606193446087, "tddate": null, "forum": "NTP9OdaT6nm", "replyto": "uyY0giE9VDT", "invitation": "ICLR.cc/2021/Conference/Paper2291/-/Official_Comment", "content": {"title": "Author Response Pt. 3", "comment": "Q. How is $t_v$ computed?\n\nA. Similar to how the Lagrangian penalty parameter is calculated, we calculate  an  empirical  rolling  average  (essentially a Monte  Carlo estimate  over trajectories) for each constraint state. It is true that many different trajectores can lead to the same $q_t$, but the estimation can be agnostic to the particular MDP state and trajectory because the constraint state is Markovian and is calculating an expected value over trajectories. It's assumed that the constraint state alone is sufficient to estimate a reasonable average over trajectories, especially for a fixed or slowly changing policy. Our experiments do not use any more rollouts than the typical RL training loop or the methods we compare against.\n\nQ. How do we simulate a DFA step?\n\nA. We only perform this lookahead in state-independent constraints in our experiments. This is a weakness of our method that we propose the use of the methods of Dalal et al. [1] to remedy, which transforms state-based constraints into action-based constraints by learning a linear approximation of the state and action-conditional cost function. We will be implementing exactly this method in future work.\n\n[1] G. Dalal, K. Dvijotham, M. Vecerik, T. Hester, C. Paduraru, and Y. Tassa. Safe Exploration in Continuous Action Spaces. 2018.\n\nQ. Any ideas why hard action shaping in both Training and Evaluation sometimes does worse than just during Training or just during Evaluation?\n\nA. Our hypothesis is that when a constraint is too restrictive for an environment, it will perform better when not employed at test time, and will perform better when not employed at train time when useful intermediate policies (i.e., those that are employed in training but are different from the policy at the end of training) are forbidden by the constraint. A potential direction of future work might be to study when hard, action-shaping constraints would be expected to help or hurt performance by their effect on exploration and the final learned policy."}, "signatures": ["ICLR.cc/2021/Conference/Paper2291/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2291/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Formal Language Constrained Markov Decision Processes", "authorids": ["~Eleanor_Quint1", "~Dong_Xu1", "~Samuel_W_Flint1", "~Stephen_D_Scott1", "~Matthew_Dwyer1"], "authors": ["Eleanor Quint", "Dong Xu", "Samuel W Flint", "Stephen D Scott", "Matthew Dwyer"], "keywords": ["safe reinforcement learning", "formal languages", "constrained Markov decision process", "safety gym", "safety"], "abstract": "In order to satisfy safety conditions, an agent may be constrained from acting freely. A safe controller can be designed a priori if an environment is well understood, but not when learning is employed. In particular, reinforcement learned (RL) controllers require exploration, which can be hazardous in safety critical situations. We study the benefits of giving structure to the constraints of a constrained Markov decision process by specifying them in formal languages as a step towards using safety methods from software engineering and controller synthesis. We instantiate these constraints as finite automata to efficiently recognise constraint violations. Constraint states are then used to augment the underlying MDP state and to learn a dense cost function, easing the problem of quickly learning joint MDP/constraint dynamics. We empirically evaluate the effect of these methods on training a variety of RL algorithms over several constraints specified in Safety Gym, MuJoCo, and Atari environments.", "one-sentence_summary": "Specify safety constraints with formal languages to learn constraint structure representation and densely shape the CMDP cost function", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "quint|formal_language_constrained_markov_decision_processes", "supplementary_material": "/attachment/eafc6a33d141c356320b069de7bef68e1b0482e6.zip", "pdf": "/pdf/b7207b0628a0fcd319c603230ccf9af4e1863532.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zDlqvFg86y", "_bibtex": "@misc{\nquint2021formal,\ntitle={Formal Language Constrained Markov Decision Processes},\nauthor={Eleanor Quint and Dong Xu and Samuel W Flint and Stephen D Scott and Matthew Dwyer},\nyear={2021},\nurl={https://openreview.net/forum?id=NTP9OdaT6nm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NTP9OdaT6nm", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2291/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2291/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2291/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2291/Authors|ICLR.cc/2021/Conference/Paper2291/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2291/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850104, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2291/-/Official_Comment"}}}, {"id": "uyY0giE9VDT", "original": null, "number": 3, "cdate": 1606193396612, "ddate": null, "tcdate": 1606193396612, "tmdate": 1606193396612, "tddate": null, "forum": "NTP9OdaT6nm", "replyto": "avSku0KpAM", "invitation": "ICLR.cc/2021/Conference/Paper2291/-/Official_Comment", "content": {"title": "Author Response Pt. 2", "comment": "Q. The clarity of the submission could be improved in places. In particular, a clear up-front definition of the problem setting would be useful. Are the transition dynamics known? Unknown? Not known analytically but with oracle query access? Are the cost functions Markovian or not?\n\nA. We assume that transition dynamics are unknown and that exploration (we use epsilon greedy in our experiments) must be used to learn a policy. The cost functions may be non-Markovian in their specification, but the inclusion of the automata state in the MDP state makes the representation of any constraint Markovian, simliar to what is explored in Camacho et al. [1], which studies the solution of non-Markovian Reward Decision Processes with MDP solvers by a transformation using automata. We'll clarify this in the paper.\n\n[1] Alberto  Camacho,  Oscar  Chen,  Scott  Sanner,  and  Sheila  A  McIlraith. Decision-making  with non-Markovian rewards:  From LTL to automata-based reward shaping.  In Proceedings of the Multi-disciplinary Conference on Reinforcement Learning and Decision Making (RLDM), pp.279\u2013283, 2017b.\n\nQ. Hard action space seems to require a resettable environment or known transition dynamics, which weakens the results. RL usually assumes unknown transition dynamics: you should compare to baselines that also make use of this information for fairness, or at least make explicit this difference.\n\nA. We do not use known transition dynamics in our experiments, though we do assume that an environment is either resettable or has access to a fall-back known-safe policy that can take over in the event that the main policy is unsafe. The latter assumption is not an uncommon one in shielding type methods (e.g., Mao et al. [1] uses such an assumption)\n\n[1] Hongzi Mao,  Malte Schwarzkopf,  Hao He,  and Mohammad Alizadeh.  Towards safe online reinforcement learning in computer systems. In 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), 2019.\n\nQ. Greater justification of why formal languages. This is the most important point here and I'd consider increasing my vote if the usefulness of this approach was clarified. For example, can you cite any papers that use formal languages as specification to validate black-box systems (like RL policy + unknown MDP)?\n\nA. In addition to the papers cited earlier in this response, there are many other works which use formal languages as a safety specification on Markov decision processes. Baier et al. [1] introduces an LTL-based formal language called ProbMela and does probabilistic model checking on MDPs with known dynamics. Zhang et al. [2] uses an LTL-based formal language to specify constraints on cyberphysical systems and a method of checking those constraints in adaptive systems. Brazdil et al. [3] discusses verification of MDPs with qualitative PECTL* objectives, another type of formal language. Bouchekir and Boukala [4] propose a probabilistic symbolic compositional verification approach to verify probabilistic systems where each component is a Markov decision process. Tran et al. [5] verify RL policies represented with neural networks. \n\nAll of these listed papers assume known MDP dynamics. However, as discussed above, verification of MDPs with unknown dynamics is an open problem. Even without full verification, however, we believe that the use of verification methods in reducing constraint violations is a significant line of work, as discussed in Ray et al. [6].\n\n[1]  Christel  Baier,  Frank  Ciesinski,  and  Marcus  Gro\u00dfer.   Probmela  and  verification  of  markov  decision  processes. ACM SIGMETRICS Performance Evaluation Review, 32(4):22\u201327, 2005.\n\n[2]  Ji Zhang, Heather J Goldsby, and Betty HC Cheng. Modular verification of dynamically adaptive systems. In Proceedings of the 8th ACM international conference on Aspect-oriented software development, pages 161\u2013172, 2009.\n\n[3]  Tomas Brazdil,  Vojtech Forejt,  and Antonin Kucera.  Controller synthesis and  verification  for  markov  decision  processes  with  qualitative  branchingtime objectives.  In International Colloquium on Automata, Languages, and Programming, pages 148\u2013159. Springer, 2008.\n\n[4]  Redouane  Bouchekir  and  Mohand  Cherif  Boukala.   Learning-based  symbolic assume-guarantee reasoning for markov decision process by using interval markov process. Innovations in Systems and Software Engineering, 14(3):229\u2013244, 2018.\n\n[5]  Hoang-Dung  Tran,  Feiyang  Cai,  Manzanas  Lopez  Diego,  Patrick  Musau, Taylor T Johnson,  and Xenofon Koutsoukos.  Safety verification of cyber-physical systems with reinforcement learning control. ACM Transactions on Embedded Computing Systems (TECS), 18(5s):1\u201322, 2019.\n\n[6]  Alex  Ray,  Joshua  Achiam,  and  Dario  Amodei. Benchmarking  safe  exploration  in  deep  reinforcement  learning. arXiv preprint arXiv:1910.01708,2019."}, "signatures": ["ICLR.cc/2021/Conference/Paper2291/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2291/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Formal Language Constrained Markov Decision Processes", "authorids": ["~Eleanor_Quint1", "~Dong_Xu1", "~Samuel_W_Flint1", "~Stephen_D_Scott1", "~Matthew_Dwyer1"], "authors": ["Eleanor Quint", "Dong Xu", "Samuel W Flint", "Stephen D Scott", "Matthew Dwyer"], "keywords": ["safe reinforcement learning", "formal languages", "constrained Markov decision process", "safety gym", "safety"], "abstract": "In order to satisfy safety conditions, an agent may be constrained from acting freely. A safe controller can be designed a priori if an environment is well understood, but not when learning is employed. In particular, reinforcement learned (RL) controllers require exploration, which can be hazardous in safety critical situations. We study the benefits of giving structure to the constraints of a constrained Markov decision process by specifying them in formal languages as a step towards using safety methods from software engineering and controller synthesis. We instantiate these constraints as finite automata to efficiently recognise constraint violations. Constraint states are then used to augment the underlying MDP state and to learn a dense cost function, easing the problem of quickly learning joint MDP/constraint dynamics. We empirically evaluate the effect of these methods on training a variety of RL algorithms over several constraints specified in Safety Gym, MuJoCo, and Atari environments.", "one-sentence_summary": "Specify safety constraints with formal languages to learn constraint structure representation and densely shape the CMDP cost function", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "quint|formal_language_constrained_markov_decision_processes", "supplementary_material": "/attachment/eafc6a33d141c356320b069de7bef68e1b0482e6.zip", "pdf": "/pdf/b7207b0628a0fcd319c603230ccf9af4e1863532.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zDlqvFg86y", "_bibtex": "@misc{\nquint2021formal,\ntitle={Formal Language Constrained Markov Decision Processes},\nauthor={Eleanor Quint and Dong Xu and Samuel W Flint and Stephen D Scott and Matthew Dwyer},\nyear={2021},\nurl={https://openreview.net/forum?id=NTP9OdaT6nm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NTP9OdaT6nm", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2291/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2291/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2291/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2291/Authors|ICLR.cc/2021/Conference/Paper2291/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2291/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850104, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2291/-/Official_Comment"}}}, {"id": "avSku0KpAM", "original": null, "number": 2, "cdate": 1606193307502, "ddate": null, "tcdate": 1606193307502, "tmdate": 1606193307502, "tddate": null, "forum": "NTP9OdaT6nm", "replyto": "8omNL6e1Jhf", "invitation": "ICLR.cc/2021/Conference/Paper2291/-/Official_Comment", "content": {"title": "Author Response", "comment": "Q. Why not use a more expressive set of constraints in training, and then check the subset we can easily specify in a formal language?\n\nA. In principle, there is no reason why someone using methods similar to ours couldn't do exactly that. We had two main reasons for using formal language constraints. First, we are interested in being able to provide probabilistic guarantees on learned policies, with methods which are well known in software engineering and cyberphysical systems, which are usable downstream in larger, multi-component software systems and compatible with well known methods in contract-based formal verification. E.g., De Miguel et al. [0], which discusses the use of formal safety languages in integrating software engineering and safety engineering, Hatcliff et al. [1] and Armstrong et al. [2], which survey behavioural interface specification languages with a focus toward automatic program verification; Moy and Marche [3], which automatically infers and composes safety contracts from formal specifications of safety in software components; and Aziz et al.~[4] and Wasilewski et al. [5], which discuss the uses and the benefits of a family of formal languages called Domain Specific Modeling Languages, used in modelling cyberphysical systems. The second reason is that the automata which encode formal language constraints have a useful state and we found satisfactory empirical performance in our experiments using this state in augmenting MDP state for representation learning in the policy as well as reward/cost shaping.\n\nWith respect to the problem of verifying RL policies, progress has already been made in verifying neural network-represented policies in cyberphysical systems (e.g., Tran et al. [6]), and formal languages are one way to specify checkable constraints with existing neural network verification methods. The problem of handling unknown MDP dynamics is a significant one in the RL verification problem, but more work is required in the field to tackle this much larger problem.\n\n[0] Miguel A De Miguel, Javier Fern\u0301andez Briones, Juan Pedro Silva, and Alejandro Alonso.  Integration of safety analysis in model-driven software development. IET software, 2(3):260\u2013280, 2008.\n\n[1]  John  Hatcliff,  Gary  T  Leavens,  K  Rustan  M  Leino,  Peter  Muller,  and Matthew Parkinson.   Behavioral  interface  specification  languages. ACM Computing Surveys (CSUR), 44(3):1\u201358, 2012.\n\n[2]  Robert C Armstrong, Ratish J Punnoose, Matthew H Wong, and Jackson R Mayo.  Survey of existing tools for formal verification. SANDIA REPORTS 2014-20533, 2014.\n\n[3]  Yannick Moy and Claude Marche.  Modular inference of subprogram contracts for safety checking. Journal of Symbolic Computation, 45(11):1184\u20131211, 2010.\n\n[4]  Muhammad Waqar Aziz and Muhammad Rashid. Domain specific modeling language for cyber physical systems.  In 2016 International Conference onInformation Systems Engineering (ICISE), pages 29\u201333. IEEE, 2016.\n\n[5]  Michael Wasilewski, Wilhelm Hasselbring, and Dirk Nowotka.  Defining requirements on domain-specific languages in model-driven software engineering  of  safety-critical  systems. Software Engineering 2013 Workshop,2013.\n\n[6]  Hoang-Dung  Tran,  Feiyang  Cai,  Manzanas  Lopez  Diego,  Patrick  Musau, Taylor T Johnson,  and Xenofon Koutsoukos.  Safety verification of cyber-physical systems with reinforcement learning control. ACM Transactions on Embedded Computing Systems (TECS), 18(5s):1\u201322, 2019.\n\nQ. Discussion of how this method can scale\n\nA. We expect that the proposed methods can scale to very challenging environments and complex constraints as it makes no assumptions about the size/complexity of the underlying MDP or the size/complexity of the constraint. The use of relatively high-level features as input to the constraint automata is a common feature of many experiments found in work on constraints, so the challenge of scaling constraints is one that is not unique to our work. We aren't familiar with any existing work which learns a constraint from low-level features.\n\nWe are looking forward to future work in learning a translation function to apply constraints to environments in which high-level features are more difficult to capture. Because this is a significant challenge in its own respect, we believe this direction is beyond the scope of the present work and would be more appropriately studied in its own paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper2291/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2291/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Formal Language Constrained Markov Decision Processes", "authorids": ["~Eleanor_Quint1", "~Dong_Xu1", "~Samuel_W_Flint1", "~Stephen_D_Scott1", "~Matthew_Dwyer1"], "authors": ["Eleanor Quint", "Dong Xu", "Samuel W Flint", "Stephen D Scott", "Matthew Dwyer"], "keywords": ["safe reinforcement learning", "formal languages", "constrained Markov decision process", "safety gym", "safety"], "abstract": "In order to satisfy safety conditions, an agent may be constrained from acting freely. A safe controller can be designed a priori if an environment is well understood, but not when learning is employed. In particular, reinforcement learned (RL) controllers require exploration, which can be hazardous in safety critical situations. We study the benefits of giving structure to the constraints of a constrained Markov decision process by specifying them in formal languages as a step towards using safety methods from software engineering and controller synthesis. We instantiate these constraints as finite automata to efficiently recognise constraint violations. Constraint states are then used to augment the underlying MDP state and to learn a dense cost function, easing the problem of quickly learning joint MDP/constraint dynamics. We empirically evaluate the effect of these methods on training a variety of RL algorithms over several constraints specified in Safety Gym, MuJoCo, and Atari environments.", "one-sentence_summary": "Specify safety constraints with formal languages to learn constraint structure representation and densely shape the CMDP cost function", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "quint|formal_language_constrained_markov_decision_processes", "supplementary_material": "/attachment/eafc6a33d141c356320b069de7bef68e1b0482e6.zip", "pdf": "/pdf/b7207b0628a0fcd319c603230ccf9af4e1863532.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zDlqvFg86y", "_bibtex": "@misc{\nquint2021formal,\ntitle={Formal Language Constrained Markov Decision Processes},\nauthor={Eleanor Quint and Dong Xu and Samuel W Flint and Stephen D Scott and Matthew Dwyer},\nyear={2021},\nurl={https://openreview.net/forum?id=NTP9OdaT6nm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NTP9OdaT6nm", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2291/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2291/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2291/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2291/Authors|ICLR.cc/2021/Conference/Paper2291/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2291/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850104, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2291/-/Official_Comment"}}}, {"id": "oSiG5pIO594", "original": null, "number": 2, "cdate": 1603839223791, "ddate": null, "tcdate": 1603839223791, "tmdate": 1605024245492, "tddate": null, "forum": "NTP9OdaT6nm", "replyto": "NTP9OdaT6nm", "invitation": "ICLR.cc/2021/Conference/Paper2291/-/Official_Review", "content": {"title": "Novel design strategy for constrained RL", "review": "#### Summary\nThe authors propose to use formal languages, specifically DFAs, as a mechanism to specify constraints in a constrained MDP setting. This has the benefit of being able to rely on a large body of existing work on identification, safety verification, etc. The strategy relies on decomposing the constraint into a translation, recogniser & cost assignment function that connect the MDP to the DFA. The mentioned cost can then be combined with existing solution for solving cMDPs, such as reward shaping and Lagrangian methods. The key observation is that adding the recogniser state to the observations of the policy can result in significant gains in both performance and constraint satisfaction. A range of results are presented across different environment suites and hyper parameters.\n\n#### Pros\n- A novel design strategy for specifying constraints in constrained MDPs, which have become very popular again as means of learning safe controllers in e.g. robotics.\n- Extensive evaluation on both discrete and continuous domains with various constraints, optimised hypers and multiple seeds.\n- Results are largely in favour of the proposed method, esp. the benefit of adding the constraint state as an observation.\n- Very clear and concise presentation (except minor comments, see below).\n\n#### Cons\n- The significance of this work is perhaps lower. While using the framework of formal languages to define constraints is a novel design strategy, the methods employed in this paper to resolve the resulting cMDP are not novel beyond adding the constraint state to the policy's observations. Any of the mentioned benefits of using formal languages, such as verification, are not actually investigated. I also would have liked to see more exploration of the benefit / shape of the dense cost function, as this now gets lost as a binary hyper parameter in the tables.\n- The chosen constraints are not very interesting as they pertain largely to sequences of actions only (except for Safety Gym). Even for what would intuitively be state-dependent constraints such as paddle ball distance, the authors specify the translation function in such a way that it becomes an action-only constraint. This makes action shaping significantly easier.\n\n#### Comments\n- I couldn't find the definition of the cost assignment functions for the environments. Are they just binary?\n- When a constraint gets violated, the DFA gets into an accepting state but it seems the episode does not get terminated. Does the DFA get reset in the next step?\n- Using \"reward shaping\", \"cost shaping\" and \"dense reward\" can be confusing as they're often used interchangeably in other works.\n- Figure 2 is not discussed in the text at all. Do the data points represent different values for $\\lambda$?\n- In Table 3 seaquest actuation and Table breakout dithering the wrong result is marked in bold (indicating best performance).\n\n#### Conclusion\nOverall, even though the significance is perhaps limited, I vote to borderline accept this paper due to the clarity and thorough evaluation.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2291/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2291/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Formal Language Constrained Markov Decision Processes", "authorids": ["~Eleanor_Quint1", "~Dong_Xu1", "~Samuel_W_Flint1", "~Stephen_D_Scott1", "~Matthew_Dwyer1"], "authors": ["Eleanor Quint", "Dong Xu", "Samuel W Flint", "Stephen D Scott", "Matthew Dwyer"], "keywords": ["safe reinforcement learning", "formal languages", "constrained Markov decision process", "safety gym", "safety"], "abstract": "In order to satisfy safety conditions, an agent may be constrained from acting freely. A safe controller can be designed a priori if an environment is well understood, but not when learning is employed. In particular, reinforcement learned (RL) controllers require exploration, which can be hazardous in safety critical situations. We study the benefits of giving structure to the constraints of a constrained Markov decision process by specifying them in formal languages as a step towards using safety methods from software engineering and controller synthesis. We instantiate these constraints as finite automata to efficiently recognise constraint violations. Constraint states are then used to augment the underlying MDP state and to learn a dense cost function, easing the problem of quickly learning joint MDP/constraint dynamics. We empirically evaluate the effect of these methods on training a variety of RL algorithms over several constraints specified in Safety Gym, MuJoCo, and Atari environments.", "one-sentence_summary": "Specify safety constraints with formal languages to learn constraint structure representation and densely shape the CMDP cost function", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "quint|formal_language_constrained_markov_decision_processes", "supplementary_material": "/attachment/eafc6a33d141c356320b069de7bef68e1b0482e6.zip", "pdf": "/pdf/b7207b0628a0fcd319c603230ccf9af4e1863532.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zDlqvFg86y", "_bibtex": "@misc{\nquint2021formal,\ntitle={Formal Language Constrained Markov Decision Processes},\nauthor={Eleanor Quint and Dong Xu and Samuel W Flint and Stephen D Scott and Matthew Dwyer},\nyear={2021},\nurl={https://openreview.net/forum?id=NTP9OdaT6nm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "NTP9OdaT6nm", "replyto": "NTP9OdaT6nm", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2291/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538099696, "tmdate": 1606915777656, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2291/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2291/-/Official_Review"}}}], "count": 15}