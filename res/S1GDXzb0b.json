{"notes": [{"tddate": null, "ddate": null, "tmdate": 1557475086379, "tcdate": 1513946023384, "number": 2, "cdate": 1513946023384, "id": "rJynoO9zM", "invitation": "ICLR.cc/2018/Conference/-/Paper805/Official_Comment", "forum": "S1GDXzb0b", "replyto": "rJfWQ9OeG", "signatures": ["ICLR.cc/2018/Conference/Paper805/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper805/Authors"], "content": {"title": "Rebuttal", "comment": "Thank you for the overall encouraging review. We address some of the concerns in the following,\n\nQ : Not clear that method converges on all problems. \nA: Yes it does not converge on all dynamics models. Currently, the main drawback of the method is that it cannot model complex dynamics models like raw video transitions as mentioned in the anonymous comment also.\n\nQ : Not clear that the method is able to extract the state from video \u2014 authors had to extract position manually\nA: Learning the useful state representations from raw video is a challenging problem. In literature, Pathak. et al. ICML 2017 proposes to use a feature extractor \\phi, which learns to predict the action given the current and next state. However, in our case, we simplify the assumption by manually specifying parts of the state that depends on the actions. This is a limitation of the proposed method but we hope to address this issue in the future versions using methods in literature, such as Pathak. et al. ICML 2017.\n\nThe overall approach and algorithms are described fairly clearly. Some minor typos here and there.\nA : We changed the typos and reordered the figures.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Model-based imitation learning from state trajectories", "abstract": "Imitation learning from demonstrations usually relies on learning a policy from trajectories of optimal states and actions. However, in real life expert demonstrations, often the action information is missing and only state trajectories are available. We present a model-based imitation learning method that can learn environment-specific optimal actions only from expert state trajectories. Our proposed method starts with a model-free reinforcement learning algorithm with a heuristic reward signal to sample environment dynamics, which is then used to train the state-transition probability. Subsequently, we learn the optimal actions from expert state trajectories by supervised learning, while back-propagating the error gradients through the modeled environment dynamics. Experimental evaluations show that our proposed method successfully achieves performance similar to (state, action) trajectory-based traditional imitation learning methods even in the absence of action information, with much fewer iterations compared to conventional model-free reinforcement learning methods. We also demonstrate that our method can learn to act from only video demonstrations of expert agent for simple games and can learn to achieve desired performance in less number of iterations.", "pdf": "/pdf/79200e781e071a6601153638fb62456de8394b15.pdf", "TL;DR": "Learning to imitate an expert in the absence of optimal actions learning a dynamics model while exploring the environment.", "paperhash": "chaudhury|modelbased_imitation_learning_from_state_trajectories", "_bibtex": "@misc{\nchaudhury2018modelbased,\ntitle={Model-based imitation learning from state trajectories},\nauthor={Subhajit Chaudhury and Daiki Kimura and Tadanobu Inoue and Ryuki Tachibana},\nyear={2018},\nurl={https://openreview.net/forum?id=S1GDXzb0b},\n}", "keywords": ["Model based reinforcement learning", "Imitation learning", "dynamics model"], "authors": ["Subhajit Chaudhury", "Daiki Kimura", "Tadanobu Inoue", "Ryuki Tachibana"], "authorids": ["subhajit@jp.ibm.com", "daiki@jp.ibm.com", "inouet@jp.ibm.com", "ryuki@jp.ibm.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825727442, "id": "ICLR.cc/2018/Conference/-/Paper805/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "S1GDXzb0b", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper805/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper805/Authors|ICLR.cc/2018/Conference/Paper805/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper805/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper805/Authors|ICLR.cc/2018/Conference/Paper805/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper805/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper805/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper805/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper805/Reviewers", "ICLR.cc/2018/Conference/Paper805/Authors", "ICLR.cc/2018/Conference/Paper805/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825727442}}}, {"tddate": null, "ddate": null, "tmdate": 1557475081768, "tcdate": 1513946408003, "number": 4, "cdate": 1513946408003, "id": "SJlNadqfM", "invitation": "ICLR.cc/2018/Conference/-/Paper805/Official_Comment", "forum": "S1GDXzb0b", "replyto": "B1V5vHb1M", "signatures": ["ICLR.cc/2018/Conference/Paper805/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper805/Authors"], "content": {"title": "Alternative simple baseline ", "comment": "Thank you for illustrating an alternative model-based method. We believe this is a useful baseline method that we should compare our method against. We agree that since our experiments are simple in nature, this proposed alternative method might perform equally well compared to the proposed method. \n\nHowever, the main difference between the suggested method and the proposed method, is that, the suggested method attempts to learn the inverse dynamics of the system (eg. given two locations (a,b) in space of the end-effector, find the torque value for moving a robotic arm from a to b) which might be difficult to learn in a general setting. Our proposed method learns the forward dynamics (eg. given current locations 'a' of the end-effector and torque values, find the next location), which might have a well-defined equation in mechanics in most general cases. However, we do agree that for the simple experimental evaluations that we have performed both, inverse and forward dynamics might be of equal difficulty.\n\nNo, in our case we manually specify the location of the flappy-bird as \\phi(s_t). It is challenging to directly learn dynamics model between raw video streams as has been already pointed out in the comment, and it is a limitation of the current proposed method. One method can be to automatically learn \\phi(s_t) in case of high dimensional inputs, using action prediction from consecutive states, using ideas in prior methods, like Pathak. et al. ICML 2017.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Model-based imitation learning from state trajectories", "abstract": "Imitation learning from demonstrations usually relies on learning a policy from trajectories of optimal states and actions. However, in real life expert demonstrations, often the action information is missing and only state trajectories are available. We present a model-based imitation learning method that can learn environment-specific optimal actions only from expert state trajectories. Our proposed method starts with a model-free reinforcement learning algorithm with a heuristic reward signal to sample environment dynamics, which is then used to train the state-transition probability. Subsequently, we learn the optimal actions from expert state trajectories by supervised learning, while back-propagating the error gradients through the modeled environment dynamics. Experimental evaluations show that our proposed method successfully achieves performance similar to (state, action) trajectory-based traditional imitation learning methods even in the absence of action information, with much fewer iterations compared to conventional model-free reinforcement learning methods. We also demonstrate that our method can learn to act from only video demonstrations of expert agent for simple games and can learn to achieve desired performance in less number of iterations.", "pdf": "/pdf/79200e781e071a6601153638fb62456de8394b15.pdf", "TL;DR": "Learning to imitate an expert in the absence of optimal actions learning a dynamics model while exploring the environment.", "paperhash": "chaudhury|modelbased_imitation_learning_from_state_trajectories", "_bibtex": "@misc{\nchaudhury2018modelbased,\ntitle={Model-based imitation learning from state trajectories},\nauthor={Subhajit Chaudhury and Daiki Kimura and Tadanobu Inoue and Ryuki Tachibana},\nyear={2018},\nurl={https://openreview.net/forum?id=S1GDXzb0b},\n}", "keywords": ["Model based reinforcement learning", "Imitation learning", "dynamics model"], "authors": ["Subhajit Chaudhury", "Daiki Kimura", "Tadanobu Inoue", "Ryuki Tachibana"], "authorids": ["subhajit@jp.ibm.com", "daiki@jp.ibm.com", "inouet@jp.ibm.com", "ryuki@jp.ibm.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825727442, "id": "ICLR.cc/2018/Conference/-/Paper805/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "S1GDXzb0b", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper805/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper805/Authors|ICLR.cc/2018/Conference/Paper805/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper805/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper805/Authors|ICLR.cc/2018/Conference/Paper805/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper805/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper805/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper805/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper805/Reviewers", "ICLR.cc/2018/Conference/Paper805/Authors", "ICLR.cc/2018/Conference/Paper805/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825727442}}}, {"tddate": null, "ddate": null, "tmdate": 1557475078803, "tcdate": 1513946174007, "number": 3, "cdate": 1513946174007, "id": "ry8Bn_5Mz", "invitation": "ICLR.cc/2018/Conference/-/Paper805/Official_Comment", "forum": "S1GDXzb0b", "replyto": "SymLN__gM", "signatures": ["ICLR.cc/2018/Conference/Paper805/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper805/Authors"], "content": {"title": "Rebuttal", "comment": "We thank the reviewer for the overall constructive comments.\n\nQ : It does not cite or discuss a very important piece of related work: \"Imitation from Observation: Learning to Imitate Behaviors from Raw Video via Context Translation\" (Liu et al., 2017)\nA: Thank you for pointing out the relevant prior work using observations only. We added citation to this work in the introduction section of the paper. Our main contribution in this work is to show that the proposed method uses a combination of model-based and model-free methods for acceleration in imitation learning from observations alone. Although the mentioned prior work is similar to the proposed method, it's main focus is on transferring learned tasks on expert observations in a source domain to a novel target domain.\n\nQ: The empirical results are unconvincing - it seems like in all problems they use there is a straightforward mapping from state feature differences to actions, as pointed out in an anonymous comment.\nA : We agree that our experiments are simple in nature, with easy to learn dynamics model, which is a drawback of the current evaluation scheme. However, the main contribution of this paper is to present the novel idea that combination of proposed model-based and model-free training has the advantage of accelerated training for imitation learning from observation alone, which can be illustrated by these simple setups. In the future, we plan to build upon the current idea on complex dynamics model setup as well for future work."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Model-based imitation learning from state trajectories", "abstract": "Imitation learning from demonstrations usually relies on learning a policy from trajectories of optimal states and actions. However, in real life expert demonstrations, often the action information is missing and only state trajectories are available. We present a model-based imitation learning method that can learn environment-specific optimal actions only from expert state trajectories. Our proposed method starts with a model-free reinforcement learning algorithm with a heuristic reward signal to sample environment dynamics, which is then used to train the state-transition probability. Subsequently, we learn the optimal actions from expert state trajectories by supervised learning, while back-propagating the error gradients through the modeled environment dynamics. Experimental evaluations show that our proposed method successfully achieves performance similar to (state, action) trajectory-based traditional imitation learning methods even in the absence of action information, with much fewer iterations compared to conventional model-free reinforcement learning methods. We also demonstrate that our method can learn to act from only video demonstrations of expert agent for simple games and can learn to achieve desired performance in less number of iterations.", "pdf": "/pdf/79200e781e071a6601153638fb62456de8394b15.pdf", "TL;DR": "Learning to imitate an expert in the absence of optimal actions learning a dynamics model while exploring the environment.", "paperhash": "chaudhury|modelbased_imitation_learning_from_state_trajectories", "_bibtex": "@misc{\nchaudhury2018modelbased,\ntitle={Model-based imitation learning from state trajectories},\nauthor={Subhajit Chaudhury and Daiki Kimura and Tadanobu Inoue and Ryuki Tachibana},\nyear={2018},\nurl={https://openreview.net/forum?id=S1GDXzb0b},\n}", "keywords": ["Model based reinforcement learning", "Imitation learning", "dynamics model"], "authors": ["Subhajit Chaudhury", "Daiki Kimura", "Tadanobu Inoue", "Ryuki Tachibana"], "authorids": ["subhajit@jp.ibm.com", "daiki@jp.ibm.com", "inouet@jp.ibm.com", "ryuki@jp.ibm.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825727442, "id": "ICLR.cc/2018/Conference/-/Paper805/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "S1GDXzb0b", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper805/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper805/Authors|ICLR.cc/2018/Conference/Paper805/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper805/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper805/Authors|ICLR.cc/2018/Conference/Paper805/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper805/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper805/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper805/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper805/Reviewers", "ICLR.cc/2018/Conference/Paper805/Authors", "ICLR.cc/2018/Conference/Paper805/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825727442}}}, {"tddate": null, "ddate": null, "tmdate": 1518730164531, "tcdate": 1509135193966, "number": 805, "cdate": 1518730164522, "id": "S1GDXzb0b", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "S1GDXzb0b", "original": "Sk-PmG-A-", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Model-based imitation learning from state trajectories", "abstract": "Imitation learning from demonstrations usually relies on learning a policy from trajectories of optimal states and actions. However, in real life expert demonstrations, often the action information is missing and only state trajectories are available. We present a model-based imitation learning method that can learn environment-specific optimal actions only from expert state trajectories. Our proposed method starts with a model-free reinforcement learning algorithm with a heuristic reward signal to sample environment dynamics, which is then used to train the state-transition probability. Subsequently, we learn the optimal actions from expert state trajectories by supervised learning, while back-propagating the error gradients through the modeled environment dynamics. Experimental evaluations show that our proposed method successfully achieves performance similar to (state, action) trajectory-based traditional imitation learning methods even in the absence of action information, with much fewer iterations compared to conventional model-free reinforcement learning methods. We also demonstrate that our method can learn to act from only video demonstrations of expert agent for simple games and can learn to achieve desired performance in less number of iterations.", "pdf": "/pdf/79200e781e071a6601153638fb62456de8394b15.pdf", "TL;DR": "Learning to imitate an expert in the absence of optimal actions learning a dynamics model while exploring the environment.", "paperhash": "chaudhury|modelbased_imitation_learning_from_state_trajectories", "_bibtex": "@misc{\nchaudhury2018modelbased,\ntitle={Model-based imitation learning from state trajectories},\nauthor={Subhajit Chaudhury and Daiki Kimura and Tadanobu Inoue and Ryuki Tachibana},\nyear={2018},\nurl={https://openreview.net/forum?id=S1GDXzb0b},\n}", "keywords": ["Model based reinforcement learning", "Imitation learning", "dynamics model"], "authors": ["Subhajit Chaudhury", "Daiki Kimura", "Tadanobu Inoue", "Ryuki Tachibana"], "authorids": ["subhajit@jp.ibm.com", "daiki@jp.ibm.com", "inouet@jp.ibm.com", "ryuki@jp.ibm.com"]}, "nonreaders": [], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260075708, "tcdate": 1517250236502, "number": 882, "cdate": 1517250236487, "id": "B1NTI1TSG", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "S1GDXzb0b", "replyto": "S1GDXzb0b", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Conference Acceptance Decision", "comment": "The paper is hard to follow at times. The heuristic reward has little justification -- not clear how\nthis would extend to other domains. Lack of empirical comparisons (see e.g. Hester et al., Deep Q-Learning from Demonstrations, 2017). "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Model-based imitation learning from state trajectories", "abstract": "Imitation learning from demonstrations usually relies on learning a policy from trajectories of optimal states and actions. However, in real life expert demonstrations, often the action information is missing and only state trajectories are available. We present a model-based imitation learning method that can learn environment-specific optimal actions only from expert state trajectories. Our proposed method starts with a model-free reinforcement learning algorithm with a heuristic reward signal to sample environment dynamics, which is then used to train the state-transition probability. Subsequently, we learn the optimal actions from expert state trajectories by supervised learning, while back-propagating the error gradients through the modeled environment dynamics. Experimental evaluations show that our proposed method successfully achieves performance similar to (state, action) trajectory-based traditional imitation learning methods even in the absence of action information, with much fewer iterations compared to conventional model-free reinforcement learning methods. We also demonstrate that our method can learn to act from only video demonstrations of expert agent for simple games and can learn to achieve desired performance in less number of iterations.", "pdf": "/pdf/79200e781e071a6601153638fb62456de8394b15.pdf", "TL;DR": "Learning to imitate an expert in the absence of optimal actions learning a dynamics model while exploring the environment.", "paperhash": "chaudhury|modelbased_imitation_learning_from_state_trajectories", "_bibtex": "@misc{\nchaudhury2018modelbased,\ntitle={Model-based imitation learning from state trajectories},\nauthor={Subhajit Chaudhury and Daiki Kimura and Tadanobu Inoue and Ryuki Tachibana},\nyear={2018},\nurl={https://openreview.net/forum?id=S1GDXzb0b},\n}", "keywords": ["Model based reinforcement learning", "Imitation learning", "dynamics model"], "authors": ["Subhajit Chaudhury", "Daiki Kimura", "Tadanobu Inoue", "Ryuki Tachibana"], "authorids": ["subhajit@jp.ibm.com", "daiki@jp.ibm.com", "inouet@jp.ibm.com", "ryuki@jp.ibm.com"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1516043290317, "tcdate": 1511723769634, "number": 2, "cdate": 1511723769634, "id": "rJfWQ9OeG", "invitation": "ICLR.cc/2018/Conference/-/Paper805/Official_Review", "forum": "S1GDXzb0b", "replyto": "S1GDXzb0b", "signatures": ["ICLR.cc/2018/Conference/Paper805/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Interesting argument for model-based imitation learning", "rating": "7: Good paper, accept", "review": "Model-Based Imitation Learning from State Trajectories\n\nSIGNIFICANCE AND ORIGINALITY:\n\nThe authors propose a model-based method for accelerating the learning of a policy\nby observing only the state transitions of an expert trace.\nThis is an important problem in many fields such as robotics where\nfinding a feasible policy is hard using pure RL methods.\n\nThe authors propose a unique two step method to find a high-quality model-based policy.\n\nFirst: To create the environment model for the model-based learner, \n they need a source of state transitions with actions ( St, At,xa St+1 ).\nTo generate these samples, they first employ a model-free algorithm.\nThe model-free algorithm is trained to try to duplicate the expert state at each trajectory.\nIn continuous domains, the state is not unique \u2026 so they build a soft next state predictor\nthat gives a probability over next states favoring those demonstrated by the expert.\nSince the transitions were generated by the agent acting in the environment,\nthese transitions have both states and actions ( St, At, St+1 ).\nThese are added to a pool.\n\nThe authors argue that the policy found by this model-free learner is\nnot highly accurate or guaranteed to converge, but presumably is good at\ngenerating transitions relevant to the expert\u2019s policy.\n(Perhaps slowly reducing the \\sigma in the reward would improve accuracy?)\nI guess if expert trace data is sparse, the model-free learner can generate a lot \nof transitions which enable it to create accurate dynamics models which in turn\nallow it to extract more information out of sparse expert traces?\n\nSecond: They then train a model based agent using the collected transitions ( St, At, St+1 ).\nThey formulate the problem as a maximum likelihood problem with two terms: \nan action dynamics model which is learned from local exploration using the learner\u2019s own actions and outcomes\nand expert policy model in terms of the actions learned above \nthat maximizes the probability of the observed expert\u2019s trajectory.\nThis is a nice clean formulation that integrates the two processes.\nI thought the comparison to an encoder - decoder network was interesting.\n\nThe authors do a good job of positioning the work in the context of recent work in IML.\n\nIt looks like the authors extract position information from flappy bird frames, \nso the algorithm is only using images for obstacle reasoning?\n\n\nQUALITY\n\nThe propose model is described fairly completely and evaluated on \na \u201creaching\" problem and the \"flappy bird\u201d game domain.\nThe evaluation framework is described in enough detail to replicate the results.\n\nInterestingly, the assisted method starts off much higher in the \u201creacher\u201d task.\nPresumably this task is easy to observe the correct actions.\n\nThe flappy bird test shows off the difference between unassisted learning (DQN),\nmodel free learning with the heuristic reward (DQN+reward prediction) \nand model based learning. \n\nInterestingly, DQN + heuristic reward approaches expert performance\nwhile behavioral cloning never achieves expert performance level even though it has actions.\n\nWhy does the model-based method only run to 600 steps and stopped before convergence??\nDoes it not converge to expert level?? If so, this would be useful to know.\n\nThere are minor grammatical mistakes that can be corrected.\n\nAfter equation 5, the authors suggest categorical loss for discrete problems, \nbut cross-entropy loss might work better. Maybe this is what they meant.\n\n\nCLARITY\n\nThe overall approach and algorithms are described fairly clearly. Some minor typos here and there.\n\nAlgorithm 1 does not make clear the relationship between the model learned in step 2 and the algorithms in steps 4 to 6.\n\nI would reverse the order of a few things to align with a right to left ordering principle. \nIn Figure 1, put the model free transition generator on the left and the model-based sample consumer on the right.\nIn Figure 3, put the \u201creacher\u201d test on the left and the \u201cflappy bird\u201d on the right.\n\n\nPROS AND CONS\n\nInteresting idea for learning quickly from small numbers of samples of expert state trajectories. \n\nNot clear that method converges on all problems. \n\nNot clear that the method is able to extract the state from video \u2014 authors had to extract position manually\n(this point is more about their deep architecture than the imitation framework they describe -\nthough perhaps a key argument for the authors is the ability to work with small numbers of \nexpert samples and still be able to train deep methods ) ??\n\n\nPOST REVIEW SUBMISSION:\n\nThe authors make a number of clarifying comments to improve the text and add the reference suggested by another reviewer. ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Model-based imitation learning from state trajectories", "abstract": "Imitation learning from demonstrations usually relies on learning a policy from trajectories of optimal states and actions. However, in real life expert demonstrations, often the action information is missing and only state trajectories are available. We present a model-based imitation learning method that can learn environment-specific optimal actions only from expert state trajectories. Our proposed method starts with a model-free reinforcement learning algorithm with a heuristic reward signal to sample environment dynamics, which is then used to train the state-transition probability. Subsequently, we learn the optimal actions from expert state trajectories by supervised learning, while back-propagating the error gradients through the modeled environment dynamics. Experimental evaluations show that our proposed method successfully achieves performance similar to (state, action) trajectory-based traditional imitation learning methods even in the absence of action information, with much fewer iterations compared to conventional model-free reinforcement learning methods. We also demonstrate that our method can learn to act from only video demonstrations of expert agent for simple games and can learn to achieve desired performance in less number of iterations.", "pdf": "/pdf/79200e781e071a6601153638fb62456de8394b15.pdf", "TL;DR": "Learning to imitate an expert in the absence of optimal actions learning a dynamics model while exploring the environment.", "paperhash": "chaudhury|modelbased_imitation_learning_from_state_trajectories", "_bibtex": "@misc{\nchaudhury2018modelbased,\ntitle={Model-based imitation learning from state trajectories},\nauthor={Subhajit Chaudhury and Daiki Kimura and Tadanobu Inoue and Ryuki Tachibana},\nyear={2018},\nurl={https://openreview.net/forum?id=S1GDXzb0b},\n}", "keywords": ["Model based reinforcement learning", "Imitation learning", "dynamics model"], "authors": ["Subhajit Chaudhury", "Daiki Kimura", "Tadanobu Inoue", "Ryuki Tachibana"], "authorids": ["subhajit@jp.ibm.com", "daiki@jp.ibm.com", "inouet@jp.ibm.com", "ryuki@jp.ibm.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642513748, "id": "ICLR.cc/2018/Conference/-/Paper805/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper805/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper805/AnonReviewer2", "ICLR.cc/2018/Conference/Paper805/AnonReviewer1", "ICLR.cc/2018/Conference/Paper805/AnonReviewer3"], "reply": {"forum": "S1GDXzb0b", "replyto": "S1GDXzb0b", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper805/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642513748}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642513847, "tcdate": 1511715915298, "number": 1, "cdate": 1511715915298, "id": "SymLN__gM", "invitation": "ICLR.cc/2018/Conference/-/Paper805/Official_Review", "forum": "S1GDXzb0b", "replyto": "S1GDXzb0b", "signatures": ["ICLR.cc/2018/Conference/Paper805/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "Addresses an important problem, but misses existing research and results are unconvincing", "rating": "4: Ok but not good enough - rejection", "review": "The problem addressed here is imitation learning when no action information is available, which is an important problem in robotics for instance. The main idea of the proposed method is to produce a policy that matches the states observed in the expert trajectories, and this is achieved via a somewhat complex mix of model-free and model-based learning.\n\nMy main issues with the paper are:\n- It does not cite or discuss a very important piece of related work: \"Imitation from Observation: Learning to Imitate Behaviors from Raw Video via Context Translation\" (Liu et al., 2017)\n- The empirical results are unconvincing - it seems like in all problems they use there is a straightforward mapping from state feature differences to actions, as pointed out in an anonymous comment.\n\nAdditionally, it would have been nice to show empirically how helpful the model-based component of their approach is.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Model-based imitation learning from state trajectories", "abstract": "Imitation learning from demonstrations usually relies on learning a policy from trajectories of optimal states and actions. However, in real life expert demonstrations, often the action information is missing and only state trajectories are available. We present a model-based imitation learning method that can learn environment-specific optimal actions only from expert state trajectories. Our proposed method starts with a model-free reinforcement learning algorithm with a heuristic reward signal to sample environment dynamics, which is then used to train the state-transition probability. Subsequently, we learn the optimal actions from expert state trajectories by supervised learning, while back-propagating the error gradients through the modeled environment dynamics. Experimental evaluations show that our proposed method successfully achieves performance similar to (state, action) trajectory-based traditional imitation learning methods even in the absence of action information, with much fewer iterations compared to conventional model-free reinforcement learning methods. We also demonstrate that our method can learn to act from only video demonstrations of expert agent for simple games and can learn to achieve desired performance in less number of iterations.", "pdf": "/pdf/79200e781e071a6601153638fb62456de8394b15.pdf", "TL;DR": "Learning to imitate an expert in the absence of optimal actions learning a dynamics model while exploring the environment.", "paperhash": "chaudhury|modelbased_imitation_learning_from_state_trajectories", "_bibtex": "@misc{\nchaudhury2018modelbased,\ntitle={Model-based imitation learning from state trajectories},\nauthor={Subhajit Chaudhury and Daiki Kimura and Tadanobu Inoue and Ryuki Tachibana},\nyear={2018},\nurl={https://openreview.net/forum?id=S1GDXzb0b},\n}", "keywords": ["Model based reinforcement learning", "Imitation learning", "dynamics model"], "authors": ["Subhajit Chaudhury", "Daiki Kimura", "Tadanobu Inoue", "Ryuki Tachibana"], "authorids": ["subhajit@jp.ibm.com", "daiki@jp.ibm.com", "inouet@jp.ibm.com", "ryuki@jp.ibm.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642513748, "id": "ICLR.cc/2018/Conference/-/Paper805/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper805/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper805/AnonReviewer2", "ICLR.cc/2018/Conference/Paper805/AnonReviewer1", "ICLR.cc/2018/Conference/Paper805/AnonReviewer3"], "reply": {"forum": "S1GDXzb0b", "replyto": "S1GDXzb0b", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper805/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642513748}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642513763, "tcdate": 1512077743690, "number": 3, "cdate": 1512077743690, "id": "HJO3Kl0ef", "invitation": "ICLR.cc/2018/Conference/-/Paper805/Official_Review", "forum": "S1GDXzb0b", "replyto": "S1GDXzb0b", "signatures": ["ICLR.cc/2018/Conference/Paper805/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "In summary, this is a poorly written paper that seems to rely on a lot of heuristics that are not well motivated. Also the results are not convincing. Clear reject.", "rating": "3: Clear rejection", "review": "The paper presents a model-based imitation learning framework which learns the state transition distribution of the expert. A model-based policy is learned that should matches the expert transition dynamics. The approach can be used for imitation learning when the actions of the expert are not observed, but only the state transitions (which is an important special case).  \n\nPros:\n- The paper concentrates on an interesting special case of imitation learning\n\nCons:\n- The paper is written very confusingly and hard to understand. The algorithm needs to be better motivated and explained and the paper needs proof reading.\n- The algorithm is based on many heuristics that are not well motivated. \n- The algorithm is only optimizing the one step error function for imitation learning but not the long term behavior. It heavily relies on the learned transition dynamics of the expert p(s_t+1|s_t). This transition model will be wrong if we go away from the expert's trajectories. Hence, I do not see why we should use p(s_t+1|s_t) to define the reward function. It does not prevent the single step \nerrors of the policy to accumulate (which is the main goal of inverse reinforcement learning)\n- The results are not convincing\n- Other algorithms (such as GAIL) could be used in the same setup (no action observations). Comparisons to other imitation learning approaches are needed.\n\nIn summary, this is a poorly written paper that seems to rely on a lot of heuristics that are not well motivated. Also the results are not convincing. Clear reject.\n\n\nMore detailed comments\n- It is unclear why a model-based and model-free policy need to be used. Is the model-based policy used at any time in the algorithm? If it is just used as final result, why train it iteratively? Why can we not just also use the model-based policy for data collection?\n- It is unclear why the heuristic reward function makes sense. First of all, the defined reward is stochastic as \\hat{s}_t+1 is a sample from the next state from the expert's transition model. Why do not we use the mean of the transition model here, then it would not be stochastic any more. Second, a much simpler reward could be used that essentially does the same thing. Instead of requiring a learned dynamics model f_E for predicting the next state, we can just use the experienced next state s_t+1. Note that the reward function for time step t can depend on s_t+1 in an MDP.  \n- The objective that is optimized (Eq. 4) is not well defined. A function is not an objective function if we can only optimize part of it for theta while keeping theta fixed for the other part. It is unclear which objective the real algorithm optimizes\n- There are quite a few confusions in terms of notation. Sometimes, a stochastic transition model p(s_t+1|s_t, a_t) is used and sometimes a deterministic model f_E(s,a). It is unclear how they relate. \n- Many other imitation learning techniques could be used in this setup including max-entropy inverse RL [1], IRL by distribution matching [2] and the approach given in [3] and GAIL. A comparison to at least a subset of these methods is needed\n\n[1] B. Ziebart et al, Maximum Entropy Inverse Reinforcement Learning, AAAI 2008\n[2] Arenz, O.; Abdulsamad, H.; Neumann, G. (2016). Optimal Control and Inverse Optimal Control by Distribution Matching, Proceedings of the International Conference on Intelligent Robots and Systems (IROS)\n[3] P Englert, A Paraschos, J Peters, MP Deisenroth, Model-based Imitation Learning by Probabilistic Trajectory Matching, IEEE International Conference on Robotics and Automation", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Model-based imitation learning from state trajectories", "abstract": "Imitation learning from demonstrations usually relies on learning a policy from trajectories of optimal states and actions. However, in real life expert demonstrations, often the action information is missing and only state trajectories are available. We present a model-based imitation learning method that can learn environment-specific optimal actions only from expert state trajectories. Our proposed method starts with a model-free reinforcement learning algorithm with a heuristic reward signal to sample environment dynamics, which is then used to train the state-transition probability. Subsequently, we learn the optimal actions from expert state trajectories by supervised learning, while back-propagating the error gradients through the modeled environment dynamics. Experimental evaluations show that our proposed method successfully achieves performance similar to (state, action) trajectory-based traditional imitation learning methods even in the absence of action information, with much fewer iterations compared to conventional model-free reinforcement learning methods. We also demonstrate that our method can learn to act from only video demonstrations of expert agent for simple games and can learn to achieve desired performance in less number of iterations.", "pdf": "/pdf/79200e781e071a6601153638fb62456de8394b15.pdf", "TL;DR": "Learning to imitate an expert in the absence of optimal actions learning a dynamics model while exploring the environment.", "paperhash": "chaudhury|modelbased_imitation_learning_from_state_trajectories", "_bibtex": "@misc{\nchaudhury2018modelbased,\ntitle={Model-based imitation learning from state trajectories},\nauthor={Subhajit Chaudhury and Daiki Kimura and Tadanobu Inoue and Ryuki Tachibana},\nyear={2018},\nurl={https://openreview.net/forum?id=S1GDXzb0b},\n}", "keywords": ["Model based reinforcement learning", "Imitation learning", "dynamics model"], "authors": ["Subhajit Chaudhury", "Daiki Kimura", "Tadanobu Inoue", "Ryuki Tachibana"], "authorids": ["subhajit@jp.ibm.com", "daiki@jp.ibm.com", "inouet@jp.ibm.com", "ryuki@jp.ibm.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642513748, "id": "ICLR.cc/2018/Conference/-/Paper805/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper805/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper805/AnonReviewer2", "ICLR.cc/2018/Conference/Paper805/AnonReviewer1", "ICLR.cc/2018/Conference/Paper805/AnonReviewer3"], "reply": {"forum": "S1GDXzb0b", "replyto": "S1GDXzb0b", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper805/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642513748}}}, {"tddate": null, "ddate": null, "tmdate": 1510197132281, "tcdate": 1510197132281, "number": 1, "cdate": 1510197132281, "id": "B1V5vHb1M", "invitation": "ICLR.cc/2018/Conference/-/Paper805/Public_Comment", "forum": "S1GDXzb0b", "replyto": "S1GDXzb0b", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Simple baseline", "comment": "I think there is an important baseline missing, namely:\n- Train a model to predict the action a_t using (phi(s_t), phi(s_{t+1})) as input, using samples obtained from the model-free policy. \n- Use this model to predict actions performed in the expert trajectories. \n- Train a standard imitation learner using the expert state trajectories together with the predicted actions as targets.\n\nThis should perform similarly to behavior cloning using true actions if the learned action predictor is accurate, which I think should be the case due to the way states are represented. For example, if I understand correctly in the 2D Obstacle Avoider task the action is simply a_t = phi(s_{t+1}) - phi(s_t). In Flappy Bird, the action is 1 if phi(s_{t+1})-phi(s_t) > 0 and -1 otherwise. It should be possible to learn both of these action predictors using very few samples. \n\nThis baseline would have more trouble if the inputs were videos and the hardcoded phi(s_t) was not provided, because the classifier would receive a high-dimensional input and would need more samples with known actions to fit its parameters. Did you try your method on Flappy Bird using only video, without providing the phi(s_t) as input?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Model-based imitation learning from state trajectories", "abstract": "Imitation learning from demonstrations usually relies on learning a policy from trajectories of optimal states and actions. However, in real life expert demonstrations, often the action information is missing and only state trajectories are available. We present a model-based imitation learning method that can learn environment-specific optimal actions only from expert state trajectories. Our proposed method starts with a model-free reinforcement learning algorithm with a heuristic reward signal to sample environment dynamics, which is then used to train the state-transition probability. Subsequently, we learn the optimal actions from expert state trajectories by supervised learning, while back-propagating the error gradients through the modeled environment dynamics. Experimental evaluations show that our proposed method successfully achieves performance similar to (state, action) trajectory-based traditional imitation learning methods even in the absence of action information, with much fewer iterations compared to conventional model-free reinforcement learning methods. We also demonstrate that our method can learn to act from only video demonstrations of expert agent for simple games and can learn to achieve desired performance in less number of iterations.", "pdf": "/pdf/79200e781e071a6601153638fb62456de8394b15.pdf", "TL;DR": "Learning to imitate an expert in the absence of optimal actions learning a dynamics model while exploring the environment.", "paperhash": "chaudhury|modelbased_imitation_learning_from_state_trajectories", "_bibtex": "@misc{\nchaudhury2018modelbased,\ntitle={Model-based imitation learning from state trajectories},\nauthor={Subhajit Chaudhury and Daiki Kimura and Tadanobu Inoue and Ryuki Tachibana},\nyear={2018},\nurl={https://openreview.net/forum?id=S1GDXzb0b},\n}", "keywords": ["Model based reinforcement learning", "Imitation learning", "dynamics model"], "authors": ["Subhajit Chaudhury", "Daiki Kimura", "Tadanobu Inoue", "Ryuki Tachibana"], "authorids": ["subhajit@jp.ibm.com", "daiki@jp.ibm.com", "inouet@jp.ibm.com", "ryuki@jp.ibm.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791678150, "id": "ICLR.cc/2018/Conference/-/Paper805/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "S1GDXzb0b", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper805/Authors", "ICLR.cc/2018/Conference/Paper805/Reviewers", "ICLR.cc/2018/Conference/Paper805/Area_Chair"], "cdate": 1512791678150}}}], "count": 9}