{"notes": [{"id": "BJxPk2A9Km", "original": "BkePw0hqYX", "number": 992, "cdate": 1538087902706, "ddate": null, "tcdate": 1538087902706, "tmdate": 1545355420194, "tddate": null, "forum": "BJxPk2A9Km", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Learning What to Remember: Long-term Episodic Memory Networks for Learning from Streaming Data", "abstract": "Current generation of memory-augmented neural networks has limited scalability as they cannot efficiently process data that are too large to fit in the external memory storage. One example of this is lifelong learning scenario where the model receives unlimited length of data stream as an input which contains vast majority of uninformative entries. We tackle this problem by proposing a memory network fit for long-term lifelong learning scenario, which we refer to as Long-term Episodic Memory Networks (LEMN), that features a RNN-based retention agent that learns to replace less important memory entries based on the retention probability generated on each entry that is learned to identify data instances of generic importance relative to other memory entries, as well as its historical importance. Such learning of retention agent allows our long-term episodic memory network to retain memory entries of generic importance for a given task. We validate our model on a path-finding task as well as synthetic and real question answering tasks, on which our model achieves significant improvements over the memory augmented networks with rule-based memory scheduling as well as an RL-based baseline that does not consider relative or historical importance of the memory.", "keywords": ["Memory Network", "Lifelong Learning"], "authorids": ["hyunwooj@kaist.ac.kr", "mshan92@kaist.ac.kr", "zzxc1133@kaist.ac.kr", "sjhwang82@kaist.ac.kr"], "authors": ["Hyunwoo Jung", "Moonsu Han", "Minki Kang", "Sungju Hwang"], "pdf": "/pdf/a89d3595d2450b94432bfcdd2f8a8faa1b677edf.pdf", "paperhash": "jung|learning_what_to_remember_longterm_episodic_memory_networks_for_learning_from_streaming_data", "_bibtex": "@misc{\njung2019learning,\ntitle={Learning What to Remember: Long-term Episodic Memory Networks for Learning from Streaming Data},\nauthor={Hyunwoo Jung and Moonsu Han and Minki Kang and Sungju Hwang},\nyear={2019},\nurl={https://openreview.net/forum?id=BJxPk2A9Km},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HklkLqX0h7", "original": null, "number": 1, "cdate": 1541450310794, "ddate": null, "tcdate": 1541450310794, "tmdate": 1545354494976, "tddate": null, "forum": "BJxPk2A9Km", "replyto": "BJxPk2A9Km", "invitation": "ICLR.cc/2019/Conference/-/Paper992/Meta_Review", "content": {"metareview": "\nPros:\n- This is an interesting and relevant topic\n- It is well motivated and mostly clear\n\nCons:\n- The motivation, large amounts of data such as occur in lifelong learning, is not well examined in the evaluation which focuses on quite small problems.  For an example of work which addresses the lifelong memory management issue (though does not learn a memory management policy) see [1].\n- In general the evaluation is not adequate to the claims.\n- Reviewer 2 is concerned with the use of a bi-directional RNN for the comparison of memory entries since it may overfit to order.\n- Reviewer 1 is somewhat concerned with novelty over other memory management schemes.\n\n[1] Scalable Recollections for Continual Lifelong Learning. https://arxiv.org/pdf/1711.06761.pdf", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Important problem but the evalution is not good enough"}, "signatures": ["ICLR.cc/2019/Conference/Paper992/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper992/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning What to Remember: Long-term Episodic Memory Networks for Learning from Streaming Data", "abstract": "Current generation of memory-augmented neural networks has limited scalability as they cannot efficiently process data that are too large to fit in the external memory storage. One example of this is lifelong learning scenario where the model receives unlimited length of data stream as an input which contains vast majority of uninformative entries. We tackle this problem by proposing a memory network fit for long-term lifelong learning scenario, which we refer to as Long-term Episodic Memory Networks (LEMN), that features a RNN-based retention agent that learns to replace less important memory entries based on the retention probability generated on each entry that is learned to identify data instances of generic importance relative to other memory entries, as well as its historical importance. Such learning of retention agent allows our long-term episodic memory network to retain memory entries of generic importance for a given task. We validate our model on a path-finding task as well as synthetic and real question answering tasks, on which our model achieves significant improvements over the memory augmented networks with rule-based memory scheduling as well as an RL-based baseline that does not consider relative or historical importance of the memory.", "keywords": ["Memory Network", "Lifelong Learning"], "authorids": ["hyunwooj@kaist.ac.kr", "mshan92@kaist.ac.kr", "zzxc1133@kaist.ac.kr", "sjhwang82@kaist.ac.kr"], "authors": ["Hyunwoo Jung", "Moonsu Han", "Minki Kang", "Sungju Hwang"], "pdf": "/pdf/a89d3595d2450b94432bfcdd2f8a8faa1b677edf.pdf", "paperhash": "jung|learning_what_to_remember_longterm_episodic_memory_networks_for_learning_from_streaming_data", "_bibtex": "@misc{\njung2019learning,\ntitle={Learning What to Remember: Long-term Episodic Memory Networks for Learning from Streaming Data},\nauthor={Hyunwoo Jung and Moonsu Han and Minki Kang and Sungju Hwang},\nyear={2019},\nurl={https://openreview.net/forum?id=BJxPk2A9Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper992/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353008929, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJxPk2A9Km", "replyto": "BJxPk2A9Km", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper992/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper992/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper992/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353008929}}}, {"id": "SylVaKxFC7", "original": null, "number": 3, "cdate": 1543207355977, "ddate": null, "tcdate": 1543207355977, "tmdate": 1543237561606, "tddate": null, "forum": "BJxPk2A9Km", "replyto": "BkeQQ3N7nQ", "invitation": "ICLR.cc/2019/Conference/-/Paper992/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "Thank you for your thoughtful comments regarding our paper.\n\n\u201cHowever, as we know RNN models including GRU are suitable for those data that have sequence order. More specifically, bidirectional RNN models are used when we want to obtain not only the impact from beginning to end but also the impact from the end to the beginning. In addition, by using bidirectional RNN, we cannot obtain the relationship between each memory entry. If the authors want to realize that, it is necessary to disrupt the order of the memory entries and input the disordered entries into RNN models for n! times where n is the number of the memory entries and this will cost many computations.\u201d\n\nWe thank the reviewer for kindly describing the way of computing the relation without the sequential order and its cost. However, incorporating the sequential order is our intention because the sequential order matters when estimating the importance of episodic memory cells. For example, in bAbI task 2, there are two sentences, \u201cA went to X\u201d and \u201cA went to Y\u201d in memory. Then, the importance of the latter should be greater than the former because the place where A was in is no longer useful to answer the future questions. Thus, we have modeled our writing mechanisms using a bidirectional RNN among the memory cells so that it can consider the spatial information of the memory. We added the experiment for the effect of disrupting the order of the memory in the revision in a similar way to [Santoro et al. 16] and observed that it results in the performance degeneration. For example, the error rate of ST-LEMN with memory size 5 on Noisy task increases by 28.9% point (see Table 4 in Appendix A) with occasional shuffling of memory entries.\n\n[Santoro et al. 16] Meta-Learning with Memory-Augmented Neural Networks, ICML 2016\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper992/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper992/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper992/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning What to Remember: Long-term Episodic Memory Networks for Learning from Streaming Data", "abstract": "Current generation of memory-augmented neural networks has limited scalability as they cannot efficiently process data that are too large to fit in the external memory storage. One example of this is lifelong learning scenario where the model receives unlimited length of data stream as an input which contains vast majority of uninformative entries. We tackle this problem by proposing a memory network fit for long-term lifelong learning scenario, which we refer to as Long-term Episodic Memory Networks (LEMN), that features a RNN-based retention agent that learns to replace less important memory entries based on the retention probability generated on each entry that is learned to identify data instances of generic importance relative to other memory entries, as well as its historical importance. Such learning of retention agent allows our long-term episodic memory network to retain memory entries of generic importance for a given task. We validate our model on a path-finding task as well as synthetic and real question answering tasks, on which our model achieves significant improvements over the memory augmented networks with rule-based memory scheduling as well as an RL-based baseline that does not consider relative or historical importance of the memory.", "keywords": ["Memory Network", "Lifelong Learning"], "authorids": ["hyunwooj@kaist.ac.kr", "mshan92@kaist.ac.kr", "zzxc1133@kaist.ac.kr", "sjhwang82@kaist.ac.kr"], "authors": ["Hyunwoo Jung", "Moonsu Han", "Minki Kang", "Sungju Hwang"], "pdf": "/pdf/a89d3595d2450b94432bfcdd2f8a8faa1b677edf.pdf", "paperhash": "jung|learning_what_to_remember_longterm_episodic_memory_networks_for_learning_from_streaming_data", "_bibtex": "@misc{\njung2019learning,\ntitle={Learning What to Remember: Long-term Episodic Memory Networks for Learning from Streaming Data},\nauthor={Hyunwoo Jung and Moonsu Han and Minki Kang and Sungju Hwang},\nyear={2019},\nurl={https://openreview.net/forum?id=BJxPk2A9Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper992/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611145, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJxPk2A9Km", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper992/Authors", "ICLR.cc/2019/Conference/Paper992/Reviewers", "ICLR.cc/2019/Conference/Paper992/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper992/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper992/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper992/Authors|ICLR.cc/2019/Conference/Paper992/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper992/Reviewers", "ICLR.cc/2019/Conference/Paper992/Authors", "ICLR.cc/2019/Conference/Paper992/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611145}}}, {"id": "BkgKLulYAX", "original": null, "number": 1, "cdate": 1543206992611, "ddate": null, "tcdate": 1543206992611, "tmdate": 1543237359549, "tddate": null, "forum": "BJxPk2A9Km", "replyto": "Bye_Z58cnQ", "invitation": "ICLR.cc/2019/Conference/-/Paper992/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "Thank you for your thoughtful comments regarding our paper.\n\n\u201cFor example, the paper lacks comparison with differentiable neural computer (DNC) [1], the well-known memory-augmented neural networks.\u201d\n\nWe agree that the comparison with DNC is of great interest. However, we did not compare DNC with our model because it takes as an input a sequence of words instead of sentences, which makes the direct comparison difficult in terms of the memory size and its efficiency. Actually, this is the reason why we adopted the writing mechanism of DNTM, which is a counterpart of DNC, as our baseline.\n\n\u201cAbbreviations should be made clear. E.g., MQN should be written in the full form before using it. The MQN should be cited with Oh et al (2016).\u201d\n\nThanks for your suggestion. In the revision, we clarified the full name of MQN and cited it in Section 2.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper992/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper992/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper992/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning What to Remember: Long-term Episodic Memory Networks for Learning from Streaming Data", "abstract": "Current generation of memory-augmented neural networks has limited scalability as they cannot efficiently process data that are too large to fit in the external memory storage. One example of this is lifelong learning scenario where the model receives unlimited length of data stream as an input which contains vast majority of uninformative entries. We tackle this problem by proposing a memory network fit for long-term lifelong learning scenario, which we refer to as Long-term Episodic Memory Networks (LEMN), that features a RNN-based retention agent that learns to replace less important memory entries based on the retention probability generated on each entry that is learned to identify data instances of generic importance relative to other memory entries, as well as its historical importance. Such learning of retention agent allows our long-term episodic memory network to retain memory entries of generic importance for a given task. We validate our model on a path-finding task as well as synthetic and real question answering tasks, on which our model achieves significant improvements over the memory augmented networks with rule-based memory scheduling as well as an RL-based baseline that does not consider relative or historical importance of the memory.", "keywords": ["Memory Network", "Lifelong Learning"], "authorids": ["hyunwooj@kaist.ac.kr", "mshan92@kaist.ac.kr", "zzxc1133@kaist.ac.kr", "sjhwang82@kaist.ac.kr"], "authors": ["Hyunwoo Jung", "Moonsu Han", "Minki Kang", "Sungju Hwang"], "pdf": "/pdf/a89d3595d2450b94432bfcdd2f8a8faa1b677edf.pdf", "paperhash": "jung|learning_what_to_remember_longterm_episodic_memory_networks_for_learning_from_streaming_data", "_bibtex": "@misc{\njung2019learning,\ntitle={Learning What to Remember: Long-term Episodic Memory Networks for Learning from Streaming Data},\nauthor={Hyunwoo Jung and Moonsu Han and Minki Kang and Sungju Hwang},\nyear={2019},\nurl={https://openreview.net/forum?id=BJxPk2A9Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper992/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611145, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJxPk2A9Km", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper992/Authors", "ICLR.cc/2019/Conference/Paper992/Reviewers", "ICLR.cc/2019/Conference/Paper992/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper992/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper992/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper992/Authors|ICLR.cc/2019/Conference/Paper992/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper992/Reviewers", "ICLR.cc/2019/Conference/Paper992/Authors", "ICLR.cc/2019/Conference/Paper992/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611145}}}, {"id": "H1eTt_gY0X", "original": null, "number": 2, "cdate": 1543207044857, "ddate": null, "tcdate": 1543207044857, "tmdate": 1543207044857, "tddate": null, "forum": "BJxPk2A9Km", "replyto": "SkgHfWgLoQ", "invitation": "ICLR.cc/2019/Conference/-/Paper992/Official_Comment", "content": {"title": "Response to AnonReviewer1", "comment": "Thank you for your thoughtful comments regarding our paper.\n\n\u201cRegarding the work\u2019s novelty, there is a precedent for using RL-based write schemes (DNTM from Gulcehre et al, 2016), which the authors point out. I am not entirely convinced that the proposed writing scheme is a substantial addition over this past work, but I am not overly concerned about this since proper due credit is assigned in the paper.\u201d\n\nAs described in Section 3, IM-LEMN, a counterpart of DNTM, takes into account only matching scores between the current input and each memory cell to estimate the importance. Our addition to the model is two folds: spatial and temporal relation between memory cells. In Section 4.1 and 4.3, we have shown that our addition results in substantial performance improvements. In addition, to the best of our knowledge, not only does our work\u2019s novelty involve proposing new writing scheme but it also involves posing the memory efficiency problem in lifelong learning and investigating how our models and the conventional models perform in the setting.\n\n\u201cInstead, the authors chose to artificially cripple the size of their memory (using, for example, just a handful of memory \u201cslots\u201d) and demonstrate its performance on tasks that are otherwise completely within the realm of being solved by conventional memory models.\u201d\n\nAs described in Section 4.3, TriviaQA is a huge dataset so that conventional models such as BiDAF are not able to encode the entire context into memory due to the limitation of computation and memory cost of hardware. So, it is not that we cripple the size of our memory but we set the feasible size. In Section 4.1, I-Maze experiment, we show that conventional models (MQN and FRMQN) with our retention agent can find the appropriate goal regardless of the size of the maze. In contrast to our models, the conventional memory models require the memory as large as the maze.\n\n\u201c(1) The use of a single bAbI task is questionable. Why not run the model on the full suite? (2) How do conventional memory models perform on the tasks?\u201d\n\nWe agree with that running the model on the full suite of bAbI tasks is of great interest. However, we focused more on running the TriviaQA experiments because bAbI tasks are synthetic but TriviaQA is a real question answering dataset.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper992/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper992/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper992/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning What to Remember: Long-term Episodic Memory Networks for Learning from Streaming Data", "abstract": "Current generation of memory-augmented neural networks has limited scalability as they cannot efficiently process data that are too large to fit in the external memory storage. One example of this is lifelong learning scenario where the model receives unlimited length of data stream as an input which contains vast majority of uninformative entries. We tackle this problem by proposing a memory network fit for long-term lifelong learning scenario, which we refer to as Long-term Episodic Memory Networks (LEMN), that features a RNN-based retention agent that learns to replace less important memory entries based on the retention probability generated on each entry that is learned to identify data instances of generic importance relative to other memory entries, as well as its historical importance. Such learning of retention agent allows our long-term episodic memory network to retain memory entries of generic importance for a given task. We validate our model on a path-finding task as well as synthetic and real question answering tasks, on which our model achieves significant improvements over the memory augmented networks with rule-based memory scheduling as well as an RL-based baseline that does not consider relative or historical importance of the memory.", "keywords": ["Memory Network", "Lifelong Learning"], "authorids": ["hyunwooj@kaist.ac.kr", "mshan92@kaist.ac.kr", "zzxc1133@kaist.ac.kr", "sjhwang82@kaist.ac.kr"], "authors": ["Hyunwoo Jung", "Moonsu Han", "Minki Kang", "Sungju Hwang"], "pdf": "/pdf/a89d3595d2450b94432bfcdd2f8a8faa1b677edf.pdf", "paperhash": "jung|learning_what_to_remember_longterm_episodic_memory_networks_for_learning_from_streaming_data", "_bibtex": "@misc{\njung2019learning,\ntitle={Learning What to Remember: Long-term Episodic Memory Networks for Learning from Streaming Data},\nauthor={Hyunwoo Jung and Moonsu Han and Minki Kang and Sungju Hwang},\nyear={2019},\nurl={https://openreview.net/forum?id=BJxPk2A9Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper992/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611145, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJxPk2A9Km", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper992/Authors", "ICLR.cc/2019/Conference/Paper992/Reviewers", "ICLR.cc/2019/Conference/Paper992/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper992/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper992/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper992/Authors|ICLR.cc/2019/Conference/Paper992/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper992/Reviewers", "ICLR.cc/2019/Conference/Paper992/Authors", "ICLR.cc/2019/Conference/Paper992/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611145}}}, {"id": "Bye_Z58cnQ", "original": null, "number": 3, "cdate": 1541200384358, "ddate": null, "tcdate": 1541200384358, "tmdate": 1541533515196, "tddate": null, "forum": "BJxPk2A9Km", "replyto": "BJxPk2A9Km", "invitation": "ICLR.cc/2019/Conference/-/Paper992/Official_Review", "content": {"title": "Important problem, interesting solutions but less convincing evaluation", "review": "Summary\n========\nThe paper focuses on memory management problem of memory-augmented neural networks when the length of the streaming data is much larger than the number of memory entries. The paper proposes Long-term Episodic Memory Networks (LEMN) which learn a RNN-based agent to erase less important memory entries for storing incoming data by computing a retention score for each memory entry based on:\n* The importance relative to other memory entries: a RNN through all memory entries. \n* An entry\u2019s historical importance: a RNN on an entry\u2019s hidden values over time. \n\nComment\n========\nThe target problem of memory management in MANN is of importance, and the solutions are interesting, especially the Spatio-Temporal LEMN, where both spatial dependencies between memory slots and temporal evolution of each slot itself are modeled.\n\nHowever, the experiments give only proof of concepts without comparison against state-of-the-art for each task. For example, the paper lacks comparison with differentiable neural computer (DNC) [1], the well-known memory-augmented neural networks. Since the DNC also has the ability to keep track on the usage information of memory entries and decide whether to free them or not, there should be a comparison between the proposed LEMN and the DNC. \n\nThe model can be considered as an extension of the DNTM [2], referred to as IM-LEMN in the paper, with the introduction of recurrent connection over space and time. Although comparisons between the LEMN and IM-LEMN are available in section 4.2 and 4.3, there should be a similar comparison in section 4.1 to see whether the addition of recurrent connections brings benefits or not. \n\nAbbreviations should be made clear. E.g., MQN should be written in the full form before using it. The MQN should be cited with Oh et al (2016). \n\nReferences:\n \n[1] Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka GrabskaBarwinska, Sergio Gomez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, Adria Puigdomenech Badia, Karl Moritz Hermann, Yori Zwols, Georg Ostrovski, Adam Cain, Helen King, Christopher Summerfield, Phil Blunsom, Koray Kavukcuoglu, and Demis Hassabis. Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626): 471\u2013476, 2016. doi: 10.1038/nature20101. \n\n[2] Caglar Gulc\u00b8ehre, Sarath Chandar, Kyunghyun Cho, and Yoshua Bengio. Dynamic neural Turing machine with soft and hard addressing schemes. CoRR, abs/1607.00036, 2016. ", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper992/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning What to Remember: Long-term Episodic Memory Networks for Learning from Streaming Data", "abstract": "Current generation of memory-augmented neural networks has limited scalability as they cannot efficiently process data that are too large to fit in the external memory storage. One example of this is lifelong learning scenario where the model receives unlimited length of data stream as an input which contains vast majority of uninformative entries. We tackle this problem by proposing a memory network fit for long-term lifelong learning scenario, which we refer to as Long-term Episodic Memory Networks (LEMN), that features a RNN-based retention agent that learns to replace less important memory entries based on the retention probability generated on each entry that is learned to identify data instances of generic importance relative to other memory entries, as well as its historical importance. Such learning of retention agent allows our long-term episodic memory network to retain memory entries of generic importance for a given task. We validate our model on a path-finding task as well as synthetic and real question answering tasks, on which our model achieves significant improvements over the memory augmented networks with rule-based memory scheduling as well as an RL-based baseline that does not consider relative or historical importance of the memory.", "keywords": ["Memory Network", "Lifelong Learning"], "authorids": ["hyunwooj@kaist.ac.kr", "mshan92@kaist.ac.kr", "zzxc1133@kaist.ac.kr", "sjhwang82@kaist.ac.kr"], "authors": ["Hyunwoo Jung", "Moonsu Han", "Minki Kang", "Sungju Hwang"], "pdf": "/pdf/a89d3595d2450b94432bfcdd2f8a8faa1b677edf.pdf", "paperhash": "jung|learning_what_to_remember_longterm_episodic_memory_networks_for_learning_from_streaming_data", "_bibtex": "@misc{\njung2019learning,\ntitle={Learning What to Remember: Long-term Episodic Memory Networks for Learning from Streaming Data},\nauthor={Hyunwoo Jung and Moonsu Han and Minki Kang and Sungju Hwang},\nyear={2019},\nurl={https://openreview.net/forum?id=BJxPk2A9Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper992/Official_Review", "cdate": 1542234330878, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJxPk2A9Km", "replyto": "BJxPk2A9Km", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper992/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335848780, "tmdate": 1552335848780, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper992/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BkeQQ3N7nQ", "original": null, "number": 2, "cdate": 1540733978690, "ddate": null, "tcdate": 1540733978690, "tmdate": 1541533514944, "tddate": null, "forum": "BJxPk2A9Km", "replyto": "BJxPk2A9Km", "invitation": "ICLR.cc/2019/Conference/-/Paper992/Official_Review", "content": {"title": "An interesting topic but need to think of strategy that is more reasonable to compute the similarity between each memory entry", "review": "This paper attempts to study memory-augmented neural networks when the size of the data is too large. The solution is to maintain a fix-sized episodic memory to remember the important data instances and at the same time erase the unimportant instances. To do so, the authors improve the method called DNTM (Gulcehre et al., 2016) by incorporating the similarity between each memory entry besides the similarity between the current data the each memory entry. Experiments show the effectiveness of the proposed method.\n\nHere are my detailed comments:\nThis is an interesting topic where augmented memory is used to improve the performance of neural networks. It is important to put the most important information in the limited external memory and discard the less important contents. In the work DNTM, the similarity of the current data instance and each memory entry is introduced to determine which memory entry should be rewritten. The authors think that this measurement is not enough and consider the relationship between each memory entry. In my opinion, this is a reasonable extra measurement since the information is also important if it has strong connection with other stored information.\n\nHowever, a deficiency of this work is that the relationship between each memory entry is not calculated in a reasonable way because the authors only use the bidirectional GRU to do this. From the motivation, we know that the authors want to obtain the relationship between every memory entry. However, as we know RNN models including GRU are suitable for those data that have sequence order. More specifically, bidirectional RNN models are used when we want to obtain not only the impact from beginning to end but also the impact from the end to the beginning. In addition, by using bidirectional RNN, we cannot obtain the relationship between each memory entry. If the authors want to realize that, it is necessary to disrupt the order of the memory entries and input the disordered entries into RNN models for n! times where n is the number of the memory entries and this will cost many computations. Although in experiments the proposed method shows its effectiveness and outperforms the baseline methods, the baseline methods are not enough to convince me that the proposed method is effective. I strongly suggest that the authors could incorporate more works that is state-of-the-art as baseline methods and consider strategies that are more reasonable to compute the relationship between each memory entry.\n\nBesides, there are some grammar mistakes and typos, especially about the usage of article and correctness on singular and plural. The paper needs more careful proofreading.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper992/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning What to Remember: Long-term Episodic Memory Networks for Learning from Streaming Data", "abstract": "Current generation of memory-augmented neural networks has limited scalability as they cannot efficiently process data that are too large to fit in the external memory storage. One example of this is lifelong learning scenario where the model receives unlimited length of data stream as an input which contains vast majority of uninformative entries. We tackle this problem by proposing a memory network fit for long-term lifelong learning scenario, which we refer to as Long-term Episodic Memory Networks (LEMN), that features a RNN-based retention agent that learns to replace less important memory entries based on the retention probability generated on each entry that is learned to identify data instances of generic importance relative to other memory entries, as well as its historical importance. Such learning of retention agent allows our long-term episodic memory network to retain memory entries of generic importance for a given task. We validate our model on a path-finding task as well as synthetic and real question answering tasks, on which our model achieves significant improvements over the memory augmented networks with rule-based memory scheduling as well as an RL-based baseline that does not consider relative or historical importance of the memory.", "keywords": ["Memory Network", "Lifelong Learning"], "authorids": ["hyunwooj@kaist.ac.kr", "mshan92@kaist.ac.kr", "zzxc1133@kaist.ac.kr", "sjhwang82@kaist.ac.kr"], "authors": ["Hyunwoo Jung", "Moonsu Han", "Minki Kang", "Sungju Hwang"], "pdf": "/pdf/a89d3595d2450b94432bfcdd2f8a8faa1b677edf.pdf", "paperhash": "jung|learning_what_to_remember_longterm_episodic_memory_networks_for_learning_from_streaming_data", "_bibtex": "@misc{\njung2019learning,\ntitle={Learning What to Remember: Long-term Episodic Memory Networks for Learning from Streaming Data},\nauthor={Hyunwoo Jung and Moonsu Han and Minki Kang and Sungju Hwang},\nyear={2019},\nurl={https://openreview.net/forum?id=BJxPk2A9Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper992/Official_Review", "cdate": 1542234330878, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJxPk2A9Km", "replyto": "BJxPk2A9Km", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper992/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335848780, "tmdate": 1552335848780, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper992/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SkgHfWgLoQ", "original": null, "number": 1, "cdate": 1539862797498, "ddate": null, "tcdate": 1539862797498, "tmdate": 1541533514734, "tddate": null, "forum": "BJxPk2A9Km", "replyto": "BJxPk2A9Km", "invitation": "ICLR.cc/2019/Conference/-/Paper992/Official_Review", "content": {"title": "Interesting idea and important problem area, but needs to be stress-tested", "review": "This work tackles the problems encountered by bounded memory storage mechanisms when faced with abundant data, of which much may be irrelevant or redundant. Such a problem is faced in lifelong learning settings, where a limitless data stream must somehow be encoded and stored so as to be useful at later points in time. \n\nThe researchers propose a solution based on \u201clearning what to remember\u201d. That is, rather than encode every observation (which can quickly become problematic), the model learns to replace less important memories. The importance of a memory is determined by its correlation with future reward; a \u201cmemory retention policy\u201d is learned via reinforcement learning, wherein the model learns to retain or discard memories based on these actions\u2019 (i.e., retentions) impact on future reward. Experiments to show the effectiveness of this mechanism include gridworld IMaze and Random Mazes, bAbI question answering (task 2), and Trivia QA. \n\nAltogether the work does well to clearly describe an interesting approach to an important problem. The model is motivated and explained well, and there were no issues with understanding its inner workings. \n\nRegarding the work\u2019s novelty, there is a precedent for using RL-based write schemes (DNTM from Gulcehre et al, 2016), which the authors point out. I am not entirely convinced that the proposed writing scheme is a substantial addition over this past work, but I am not overly concerned about this since proper due credit is assigned in the paper. Perhaps a bit more discussion about the advantages of the proposed writing scheme could go a long way, since as it stands now, the paper simply claims that this past work \u201conly considers the pairwise relationships between the current data instance and each individual memory\u201d, and I\u2019m not sure how much substance actually underlies this difference.\n\nUnfortunately I think there is a fundamental problem with the work. The model is a proposed solution for problems with vast amounts of streaming data; problems that, presumably, current memory models would struggle with. However, the tasks in the paper do not fall in this domain. Instead, the authors chose to artificially cripple the size of their memory (using, for example, just a handful of memory \u201cslots\u201d) and demonstrate its performance on tasks that are otherwise completely within the realm of being solved by conventional memory models. This is fine as a jumping off point for the research, but for the model to be taken seriously as a valid solution to problems involving such a scale of data that current models cannot even cope, then it needs to show its worth on problems involving such a scale of data that current models cannot cope. \n\nDemonstrating success here is important for a few reasons. First, such high-data scenarios may involve situations where many, many memories need to be encoded and considered for the future, since they are all useful or necessary for future performance. The experiments do not show whether the model can scale to, say, 100 or 1000 memories, which is within the realm of being \u201creasonable\u201d for current memory architectures. Second, high-data scenarios may involve an abundant amount of distracting, irrelevant data. This places particularly tough demands on the RL-based writing mechanism, which will undoubtedly face problems with temporal credit assignment if: (a) the time between encoding and retrieval is long, and (b) there is high reward noise in the intermediate time. Thus, the authors should stress-test the components of their model, since these stresses will undoubtedly exist in the problems that the model is proposed to solve.\n\nSome other minor considerations include the following. (1) The use of a single bAbI task is questionable. Why not run the model on the full suite? (2) How do conventional memory models perform on the tasks? Why are the baselines only variants of the proposed model? \n\nTo conclude and summarize, as a proposed solution to scenarios with streams of abundant data -- which the authors claim is a domain that current memory models may struggle -- the proposed model should tackle problems that: 1) have characteristics more reminiscent of these scenarios, and 2) are problems on which current memory models struggle, for the reasons claimed in the paper. In particular, it would be valuable to see model performance on tasks wherein very long stretches of time need to be considered. This is important because it can address questions with memory scaling (how does the model cope with more than a handful of memories?), and issues that would crop up in a reinforcement learning-based approach to memory retention over long time intervals (namely, long-term temporal credit assignment). \n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper992/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning What to Remember: Long-term Episodic Memory Networks for Learning from Streaming Data", "abstract": "Current generation of memory-augmented neural networks has limited scalability as they cannot efficiently process data that are too large to fit in the external memory storage. One example of this is lifelong learning scenario where the model receives unlimited length of data stream as an input which contains vast majority of uninformative entries. We tackle this problem by proposing a memory network fit for long-term lifelong learning scenario, which we refer to as Long-term Episodic Memory Networks (LEMN), that features a RNN-based retention agent that learns to replace less important memory entries based on the retention probability generated on each entry that is learned to identify data instances of generic importance relative to other memory entries, as well as its historical importance. Such learning of retention agent allows our long-term episodic memory network to retain memory entries of generic importance for a given task. We validate our model on a path-finding task as well as synthetic and real question answering tasks, on which our model achieves significant improvements over the memory augmented networks with rule-based memory scheduling as well as an RL-based baseline that does not consider relative or historical importance of the memory.", "keywords": ["Memory Network", "Lifelong Learning"], "authorids": ["hyunwooj@kaist.ac.kr", "mshan92@kaist.ac.kr", "zzxc1133@kaist.ac.kr", "sjhwang82@kaist.ac.kr"], "authors": ["Hyunwoo Jung", "Moonsu Han", "Minki Kang", "Sungju Hwang"], "pdf": "/pdf/a89d3595d2450b94432bfcdd2f8a8faa1b677edf.pdf", "paperhash": "jung|learning_what_to_remember_longterm_episodic_memory_networks_for_learning_from_streaming_data", "_bibtex": "@misc{\njung2019learning,\ntitle={Learning What to Remember: Long-term Episodic Memory Networks for Learning from Streaming Data},\nauthor={Hyunwoo Jung and Moonsu Han and Minki Kang and Sungju Hwang},\nyear={2019},\nurl={https://openreview.net/forum?id=BJxPk2A9Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper992/Official_Review", "cdate": 1542234330878, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJxPk2A9Km", "replyto": "BJxPk2A9Km", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper992/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335848780, "tmdate": 1552335848780, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper992/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 8}