{"notes": [{"id": "H1gupiC5KQ", "original": "SklanFpcFX", "number": 815, "cdate": 1538087871734, "ddate": null, "tcdate": 1538087871734, "tmdate": 1545355391059, "tddate": null, "forum": "H1gupiC5KQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "The wisdom of the crowd: reliable deep reinforcement learning through ensembles of Q-functions", "abstract": "Reinforcement learning agents learn by exploring the environment and then exploiting what they have learned.\nThis frees the human trainers from having to know the preferred action or intrinsic value of each encountered state.\nThe cost of this freedom is reinforcement learning is slower and more unstable than supervised learning.\nWe explore the possibility that ensemble methods can remedy these shortcomings and do so by investigating a novel technique which harnesses the wisdom of the crowds by bagging Q-function approximator estimates.\n\nOur results show that this proposed approach improves all three tasks and reinforcement learning approaches attempted.\nWe are able to demonstrate that this is a direct result of the increased stability of the action portion of the state-action-value function used by Q-learning to select actions and by policy gradient methods to train the policy.\n", "keywords": ["reinforcement learning", "ensembles", "deep learning", "neural network"], "authorids": ["daniel.elliott18@alumni.colostate.edu", "chuck.anderson@colostate.edu"], "authors": ["Daniel Elliott", "Charles Anderson"], "TL;DR": "Examined how a simple ensemble approach can tackle the biggest challenges in Q-learning.", "pdf": "/pdf/c699a7a6b6f899b5f49955993f10032247bd556a.pdf", "paperhash": "elliott|the_wisdom_of_the_crowd_reliable_deep_reinforcement_learning_through_ensembles_of_qfunctions", "_bibtex": "@misc{\nelliott2019the,\ntitle={The wisdom of the crowd: reliable deep reinforcement learning through ensembles of Q-functions},\nauthor={Daniel Elliott and Charles Anderson},\nyear={2019},\nurl={https://openreview.net/forum?id=H1gupiC5KQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HkgEj32geV", "original": null, "number": 1, "cdate": 1544764571706, "ddate": null, "tcdate": 1544764571706, "tmdate": 1545354519898, "tddate": null, "forum": "H1gupiC5KQ", "replyto": "H1gupiC5KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper815/Meta_Review", "content": {"metareview": "The paper suggests using an ensemble of Q functions for Q-learning. This idea is related to bootstrapped DQN and more recent work on distributional RL and quantile regression in RL. Given the similarity, a comparison against these approaches (or a subset of those) is necessary. The experiments are limited to very simple environment (e.g. swing-up and cart-pole). The paper in its current form does not pass the bar for acceptance at ICLR.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "The paper can be improved"}, "signatures": ["ICLR.cc/2019/Conference/Paper815/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper815/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The wisdom of the crowd: reliable deep reinforcement learning through ensembles of Q-functions", "abstract": "Reinforcement learning agents learn by exploring the environment and then exploiting what they have learned.\nThis frees the human trainers from having to know the preferred action or intrinsic value of each encountered state.\nThe cost of this freedom is reinforcement learning is slower and more unstable than supervised learning.\nWe explore the possibility that ensemble methods can remedy these shortcomings and do so by investigating a novel technique which harnesses the wisdom of the crowds by bagging Q-function approximator estimates.\n\nOur results show that this proposed approach improves all three tasks and reinforcement learning approaches attempted.\nWe are able to demonstrate that this is a direct result of the increased stability of the action portion of the state-action-value function used by Q-learning to select actions and by policy gradient methods to train the policy.\n", "keywords": ["reinforcement learning", "ensembles", "deep learning", "neural network"], "authorids": ["daniel.elliott18@alumni.colostate.edu", "chuck.anderson@colostate.edu"], "authors": ["Daniel Elliott", "Charles Anderson"], "TL;DR": "Examined how a simple ensemble approach can tackle the biggest challenges in Q-learning.", "pdf": "/pdf/c699a7a6b6f899b5f49955993f10032247bd556a.pdf", "paperhash": "elliott|the_wisdom_of_the_crowd_reliable_deep_reinforcement_learning_through_ensembles_of_qfunctions", "_bibtex": "@misc{\nelliott2019the,\ntitle={The wisdom of the crowd: reliable deep reinforcement learning through ensembles of Q-functions},\nauthor={Daniel Elliott and Charles Anderson},\nyear={2019},\nurl={https://openreview.net/forum?id=H1gupiC5KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper815/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353076321, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1gupiC5KQ", "replyto": "H1gupiC5KQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper815/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper815/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper815/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353076321}}}, {"id": "rylFGfkfTX", "original": null, "number": 3, "cdate": 1541693968866, "ddate": null, "tcdate": 1541693968866, "tmdate": 1541693968866, "tddate": null, "forum": "H1gupiC5KQ", "replyto": "H1gupiC5KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper815/Official_Review", "content": {"title": "Not enough novelty", "review": "This paper proposes a cute idea as suggesting ensembles of Q-function approximations rather than a singular DQN. \n\nHowever, at the core of it, this boils down to previously studied methods in the literature, one of which also is not cited here: \n\n@inproceedings{osband2016deep,\n  title={Deep exploration via bootstrapped DQN},\n  author={Osband, Ian and Blundell, Charles and Pritzel, Alexander and Van Roy, Benjamin},\n  booktitle={Advances in neural information processing systems},\n  pages={4026--4034},\n  year={2016}\n}\n\nExperiments provided in this paper compares with only the weak baseline of single DQN, however, it fails to compare other similar ideas in the literature such as the above paper. Hence, this paper lacks enough novelty for publication, and it is not clear from the experiments that the specific method proposed in this paper is better than others in the SOTA. ", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper815/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The wisdom of the crowd: reliable deep reinforcement learning through ensembles of Q-functions", "abstract": "Reinforcement learning agents learn by exploring the environment and then exploiting what they have learned.\nThis frees the human trainers from having to know the preferred action or intrinsic value of each encountered state.\nThe cost of this freedom is reinforcement learning is slower and more unstable than supervised learning.\nWe explore the possibility that ensemble methods can remedy these shortcomings and do so by investigating a novel technique which harnesses the wisdom of the crowds by bagging Q-function approximator estimates.\n\nOur results show that this proposed approach improves all three tasks and reinforcement learning approaches attempted.\nWe are able to demonstrate that this is a direct result of the increased stability of the action portion of the state-action-value function used by Q-learning to select actions and by policy gradient methods to train the policy.\n", "keywords": ["reinforcement learning", "ensembles", "deep learning", "neural network"], "authorids": ["daniel.elliott18@alumni.colostate.edu", "chuck.anderson@colostate.edu"], "authors": ["Daniel Elliott", "Charles Anderson"], "TL;DR": "Examined how a simple ensemble approach can tackle the biggest challenges in Q-learning.", "pdf": "/pdf/c699a7a6b6f899b5f49955993f10032247bd556a.pdf", "paperhash": "elliott|the_wisdom_of_the_crowd_reliable_deep_reinforcement_learning_through_ensembles_of_qfunctions", "_bibtex": "@misc{\nelliott2019the,\ntitle={The wisdom of the crowd: reliable deep reinforcement learning through ensembles of Q-functions},\nauthor={Daniel Elliott and Charles Anderson},\nyear={2019},\nurl={https://openreview.net/forum?id=H1gupiC5KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper815/Official_Review", "cdate": 1542234370707, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1gupiC5KQ", "replyto": "H1gupiC5KQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper815/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335808698, "tmdate": 1552335808698, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper815/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rygPQvCc37", "original": null, "number": 2, "cdate": 1541232414864, "ddate": null, "tcdate": 1541232414864, "tmdate": 1541533667852, "tddate": null, "forum": "H1gupiC5KQ", "replyto": "H1gupiC5KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper815/Official_Review", "content": {"title": "Interesting idea while the experiments are not enough.", "review": "This paper proposes the deep reinforcement learning with ensembles of Q-functions. Its main idea is updating multiple Q-functions, instead of one, with independently sampled experience replay memory, then take the action selected by the ensemble. Experimental results demonstrate that the proposed method can achieve better performance than non-ensemble one under the same training steps, and the decision space can also be stabilized.\n\nThis paper is well-written. The main ideas and claims are clearly expressed. Using ensembles of Q-function can naturally reduce the variance of decisions, so it can speed up the training procedure for certain tasks. This idea is simple and works well. The main contribution is it provides a way to reduce the number of interactions with the environment. My main concern about the paper is the time cost. Since the method requires updating multiple Q-functions, it may cost much more time for each RL time step, so I\u2019m not sure whether the ensemble method can outperform the non-ensemble one within the same time period. This problem is important for practical usage. However, the authors didn\u2019t show these results in the paper.\n\nMinor things:\n+The main idea is described too sketchily. I think more examples, such as in section 8.1, should be put in the main text.\n+Page6 Line2, duplicated \u2018the\u2019.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper815/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The wisdom of the crowd: reliable deep reinforcement learning through ensembles of Q-functions", "abstract": "Reinforcement learning agents learn by exploring the environment and then exploiting what they have learned.\nThis frees the human trainers from having to know the preferred action or intrinsic value of each encountered state.\nThe cost of this freedom is reinforcement learning is slower and more unstable than supervised learning.\nWe explore the possibility that ensemble methods can remedy these shortcomings and do so by investigating a novel technique which harnesses the wisdom of the crowds by bagging Q-function approximator estimates.\n\nOur results show that this proposed approach improves all three tasks and reinforcement learning approaches attempted.\nWe are able to demonstrate that this is a direct result of the increased stability of the action portion of the state-action-value function used by Q-learning to select actions and by policy gradient methods to train the policy.\n", "keywords": ["reinforcement learning", "ensembles", "deep learning", "neural network"], "authorids": ["daniel.elliott18@alumni.colostate.edu", "chuck.anderson@colostate.edu"], "authors": ["Daniel Elliott", "Charles Anderson"], "TL;DR": "Examined how a simple ensemble approach can tackle the biggest challenges in Q-learning.", "pdf": "/pdf/c699a7a6b6f899b5f49955993f10032247bd556a.pdf", "paperhash": "elliott|the_wisdom_of_the_crowd_reliable_deep_reinforcement_learning_through_ensembles_of_qfunctions", "_bibtex": "@misc{\nelliott2019the,\ntitle={The wisdom of the crowd: reliable deep reinforcement learning through ensembles of Q-functions},\nauthor={Daniel Elliott and Charles Anderson},\nyear={2019},\nurl={https://openreview.net/forum?id=H1gupiC5KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper815/Official_Review", "cdate": 1542234370707, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1gupiC5KQ", "replyto": "H1gupiC5KQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper815/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335808698, "tmdate": 1552335808698, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper815/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJxyZpEH37", "original": null, "number": 1, "cdate": 1540865270882, "ddate": null, "tcdate": 1540865270882, "tmdate": 1541533667643, "tddate": null, "forum": "H1gupiC5KQ", "replyto": "H1gupiC5KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper815/Official_Review", "content": {"title": "review report", "review": "This paper introduces an ensemble version of Deep RL by bagging Q-function approximation estimates. In the experiments, the performance of the proposed work is compared to the baseline, single DQN. In spite of the contribution, this paper has a critical issue. \n\nIt has been extensively studied in the literature that ensemble DQN could lead to better performance than a single DQN. See the seminal work by Osband et al. (2016). The authors did not cite this paper, not to say a long list of recent works who have cited this seminal work. This indicates that the authors fail to conduct a serious literature review. In addition, more comprehensive experiments are required to compare the proposed work with the state-of-the-art ensemble DQN methods.\n\nOsband et al. (2016), Deep Exploration via Bootstrapped DQN. NIPS.\n\n\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper815/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The wisdom of the crowd: reliable deep reinforcement learning through ensembles of Q-functions", "abstract": "Reinforcement learning agents learn by exploring the environment and then exploiting what they have learned.\nThis frees the human trainers from having to know the preferred action or intrinsic value of each encountered state.\nThe cost of this freedom is reinforcement learning is slower and more unstable than supervised learning.\nWe explore the possibility that ensemble methods can remedy these shortcomings and do so by investigating a novel technique which harnesses the wisdom of the crowds by bagging Q-function approximator estimates.\n\nOur results show that this proposed approach improves all three tasks and reinforcement learning approaches attempted.\nWe are able to demonstrate that this is a direct result of the increased stability of the action portion of the state-action-value function used by Q-learning to select actions and by policy gradient methods to train the policy.\n", "keywords": ["reinforcement learning", "ensembles", "deep learning", "neural network"], "authorids": ["daniel.elliott18@alumni.colostate.edu", "chuck.anderson@colostate.edu"], "authors": ["Daniel Elliott", "Charles Anderson"], "TL;DR": "Examined how a simple ensemble approach can tackle the biggest challenges in Q-learning.", "pdf": "/pdf/c699a7a6b6f899b5f49955993f10032247bd556a.pdf", "paperhash": "elliott|the_wisdom_of_the_crowd_reliable_deep_reinforcement_learning_through_ensembles_of_qfunctions", "_bibtex": "@misc{\nelliott2019the,\ntitle={The wisdom of the crowd: reliable deep reinforcement learning through ensembles of Q-functions},\nauthor={Daniel Elliott and Charles Anderson},\nyear={2019},\nurl={https://openreview.net/forum?id=H1gupiC5KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper815/Official_Review", "cdate": 1542234370707, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1gupiC5KQ", "replyto": "H1gupiC5KQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper815/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335808698, "tmdate": 1552335808698, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper815/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}