{"notes": [{"id": "5NA1PinlGFu", "original": "F9XokM8Wvc5", "number": 3388, "cdate": 1601308375967, "ddate": null, "tcdate": 1601308375967, "tmdate": 1615106399725, "tddate": null, "forum": "5NA1PinlGFu", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Colorization Transformer", "authorids": ["~Manoj_Kumar1", "~Dirk_Weissenborn1", "~Nal_Kalchbrenner1"], "authors": ["Manoj Kumar", "Dirk Weissenborn", "Nal Kalchbrenner"], "keywords": [], "abstract": "We present the Colorization Transformer, a novel approach for diverse high fidelity image colorization based on self-attention. Given a grayscale image, the colorization proceeds in three steps. We first use a conditional autoregressive transformer to produce a low resolution coarse coloring of the grayscale image. Our architecture adopts conditional transformer layers to effectively condition grayscale input. Two subsequent fully parallel networks upsample the coarse colored low resolution image into a finely colored high resolution image. Sampling from the Colorization Transformer produces diverse colorings whose fidelity outperforms the previous state-of-the-art on colorising ImageNet based on FID results and based on a human evaluation in a Mechanical Turk test. Remarkably, in  more than 60\\% of cases human evaluators prefer the highest rated among three generated colorings over the ground truth. The code and pre-trained checkpoints for Colorization Transformer are publicly available  at https://github.com/google-research/google-research/tree/master/coltran", "one-sentence_summary": "Self-attention for colorization", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kumar|colorization_transformer", "pdf": "/pdf/f2f5d9057587995de8d113d1ba35dd7d8b98f48e.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkumar2021colorization,\ntitle={Colorization Transformer},\nauthor={Manoj Kumar and Dirk Weissenborn and Nal Kalchbrenner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5NA1PinlGFu}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "Z1D2IIuo5ab", "original": null, "number": 1, "cdate": 1610040463435, "ddate": null, "tcdate": 1610040463435, "tmdate": 1610474066507, "tddate": null, "forum": "5NA1PinlGFu", "replyto": "5NA1PinlGFu", "invitation": "ICLR.cc/2021/Conference/Paper3388/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "The paper initially received a mixed rating, with two reviewers rate the paper below the bar and two above the bar. The raised concerns include the need for an autoregressive model for upsampling and the effect of batch sizes. These concerns were well-addressed in the rebuttal. Both of the reviewers that originally rated the paper below the bar raise the scores. After consulting the paper, the reviews, and the rebuttal, the AC agrees that the paper has its merits and is happy to accept the paper.\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Colorization Transformer", "authorids": ["~Manoj_Kumar1", "~Dirk_Weissenborn1", "~Nal_Kalchbrenner1"], "authors": ["Manoj Kumar", "Dirk Weissenborn", "Nal Kalchbrenner"], "keywords": [], "abstract": "We present the Colorization Transformer, a novel approach for diverse high fidelity image colorization based on self-attention. Given a grayscale image, the colorization proceeds in three steps. We first use a conditional autoregressive transformer to produce a low resolution coarse coloring of the grayscale image. Our architecture adopts conditional transformer layers to effectively condition grayscale input. Two subsequent fully parallel networks upsample the coarse colored low resolution image into a finely colored high resolution image. Sampling from the Colorization Transformer produces diverse colorings whose fidelity outperforms the previous state-of-the-art on colorising ImageNet based on FID results and based on a human evaluation in a Mechanical Turk test. Remarkably, in  more than 60\\% of cases human evaluators prefer the highest rated among three generated colorings over the ground truth. The code and pre-trained checkpoints for Colorization Transformer are publicly available  at https://github.com/google-research/google-research/tree/master/coltran", "one-sentence_summary": "Self-attention for colorization", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kumar|colorization_transformer", "pdf": "/pdf/f2f5d9057587995de8d113d1ba35dd7d8b98f48e.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkumar2021colorization,\ntitle={Colorization Transformer},\nauthor={Manoj Kumar and Dirk Weissenborn and Nal Kalchbrenner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5NA1PinlGFu}\n}"}, "tags": [], "invitation": {"reply": {"forum": "5NA1PinlGFu", "replyto": "5NA1PinlGFu", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040463422, "tmdate": 1610474066491, "id": "ICLR.cc/2021/Conference/Paper3388/-/Decision"}}}, {"id": "IUQ6RLQFgRR", "original": null, "number": 3, "cdate": 1603943667668, "ddate": null, "tcdate": 1603943667668, "tmdate": 1607138401641, "tddate": null, "forum": "5NA1PinlGFu", "replyto": "5NA1PinlGFu", "invitation": "ICLR.cc/2021/Conference/Paper3388/-/Official_Review", "content": {"title": "Reasonable approach / Well-written / Better than baselines", "review": "Update: I really appreciate the authors' efforts to address my original concerns. I believe that this work is a nice application of transformers to image colorization. The paper is well-written and the performance of proposed transformer architecture is strong. I think that this work is above the threshold of acceptance.\n\n**Strengths**\nThe motivation of the proposed architecture is reasonable. The paper is generally well-written. \n\n**Major comments**\nIt\u2019s better to include some discussion on regularization effects from Eq. (4). Eq. (4) seems to be helpful to capture the overall structure in an image, rather than capturing only local correlation from autoregressive formulation.\n\nFor upsampling, do we really need to make use of an autoregressive model? A stack of transposed convolutions might be working well, because we only need to upsample input/color resolutions. I totally agree that autoregressive formulation does help achieve better results, but it may be possible to achieve similar performance by just using transposed convolutions. \n\nIt\u2019s better to include more details for reproducing results. \n(1) Even if different batch sizes used (224, 768 and 32), learning rates for all experiments are fixed 3e-4?\n(2) How many epochs or steps are required for convergence?\n(3) Figure 6 shows that EMA is extremely important. How about using cosine annealing for a learning rate scheduler? It may help achieve more robust FID scores without EMA.\n(4) Compared to baselines, this approach is extremely slow due to the autoregressive sampling. It\u2019s better to report inference time.\n\nI'm not sure that conditional layer normalization is indeed helpful. \n\n**Minor comments**\nThe x-axis title of figure 4c (\u201ctraining steps\u201d) seems to be wrong.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3388/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3388/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Colorization Transformer", "authorids": ["~Manoj_Kumar1", "~Dirk_Weissenborn1", "~Nal_Kalchbrenner1"], "authors": ["Manoj Kumar", "Dirk Weissenborn", "Nal Kalchbrenner"], "keywords": [], "abstract": "We present the Colorization Transformer, a novel approach for diverse high fidelity image colorization based on self-attention. Given a grayscale image, the colorization proceeds in three steps. We first use a conditional autoregressive transformer to produce a low resolution coarse coloring of the grayscale image. Our architecture adopts conditional transformer layers to effectively condition grayscale input. Two subsequent fully parallel networks upsample the coarse colored low resolution image into a finely colored high resolution image. Sampling from the Colorization Transformer produces diverse colorings whose fidelity outperforms the previous state-of-the-art on colorising ImageNet based on FID results and based on a human evaluation in a Mechanical Turk test. Remarkably, in  more than 60\\% of cases human evaluators prefer the highest rated among three generated colorings over the ground truth. The code and pre-trained checkpoints for Colorization Transformer are publicly available  at https://github.com/google-research/google-research/tree/master/coltran", "one-sentence_summary": "Self-attention for colorization", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kumar|colorization_transformer", "pdf": "/pdf/f2f5d9057587995de8d113d1ba35dd7d8b98f48e.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkumar2021colorization,\ntitle={Colorization Transformer},\nauthor={Manoj Kumar and Dirk Weissenborn and Nal Kalchbrenner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5NA1PinlGFu}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "5NA1PinlGFu", "replyto": "5NA1PinlGFu", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3388/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538076779, "tmdate": 1606915790302, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3388/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3388/-/Official_Review"}}}, {"id": "Ch646_YhDE1", "original": null, "number": 1, "cdate": 1603340156951, "ddate": null, "tcdate": 1603340156951, "tmdate": 1606958586671, "tddate": null, "forum": "5NA1PinlGFu", "replyto": "5NA1PinlGFu", "invitation": "ICLR.cc/2021/Conference/Paper3388/-/Official_Review", "content": {"title": "Extensive experiments and strong performance, novelty is a bit incremental.", "review": "Update: Thanks for the additional ablation studies. I would like to keep my original evaluation which is acceptance.\n\n--------------------\n\nThis paper proposes a transformer architecture for image colorization. It uses an axial transformer to process the low-resolution grayscale image, and uses a conditional version of the axial transformer to predict a low-resolution color image autoregressively conditioned on the gray image. It then uses an axial transformer to predict the final high-resolution output pixels.\n\nPros:\n+ The paper is well-written and easy to read. The literature review is comprehensive.\n+ Image colorization is an important problem in computer vision. To my knowledge, this is the first paper that applies Transformer to colorization. It could potentially be very impacted and inspire future work.\n+ Both automatic metric (FID) and human evaluation are used to compare the method with existing approaches. The performance of the proposed method significantly outperforms the previous state of the art. The qualitative examples are very impressive as well.\n+ The paper performs extensive ablation studies (Figure 3) to verify the contribution of different components. \n\nCons:\n- The technical novelty of this paper is a bit limited. It basically applies existing conditioning techniques to the axial transformer and uses it for image colorization.\n- It seems that no cLN (Fig. 3 mid) is better than cLN with mean-pool only (Fig. 3 right), which is a bit counterintuitive. Any possible explanation? Also, is there a reason to use the globally aggregated context for cLN but not for cMLP/cAtt? An ablation study on that would be helpful. Besides, there is an ablation study on shift-only modulation but I am curious about how scale-only modulation performs.\n- It would be nice to show the number of parameters, training/inference speed of the proposed approach, and compare them to the baselines.\n- Please add references to all baseline methods compared in Table 2. I'm able to find the citation of PixColor in other parts of the paper, but cannot find most of the others'.\n\nMinor problems that do not affect my score:\n- P1: determinisitic -> deterministic\n- The aggregated context is denoted as \\hat{c} in Table 1 but as \\bar{c} in section 4.3.\n- It would be better to use the vector format for Figure 3/4, and enlarge Figure 5 a bit.\n\nOverall, I vote for acceptance. The novelty is not huge but I still think it would be a nice paper for ICLR and have impacts on the field given its strong empirical performance.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3388/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3388/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Colorization Transformer", "authorids": ["~Manoj_Kumar1", "~Dirk_Weissenborn1", "~Nal_Kalchbrenner1"], "authors": ["Manoj Kumar", "Dirk Weissenborn", "Nal Kalchbrenner"], "keywords": [], "abstract": "We present the Colorization Transformer, a novel approach for diverse high fidelity image colorization based on self-attention. Given a grayscale image, the colorization proceeds in three steps. We first use a conditional autoregressive transformer to produce a low resolution coarse coloring of the grayscale image. Our architecture adopts conditional transformer layers to effectively condition grayscale input. Two subsequent fully parallel networks upsample the coarse colored low resolution image into a finely colored high resolution image. Sampling from the Colorization Transformer produces diverse colorings whose fidelity outperforms the previous state-of-the-art on colorising ImageNet based on FID results and based on a human evaluation in a Mechanical Turk test. Remarkably, in  more than 60\\% of cases human evaluators prefer the highest rated among three generated colorings over the ground truth. The code and pre-trained checkpoints for Colorization Transformer are publicly available  at https://github.com/google-research/google-research/tree/master/coltran", "one-sentence_summary": "Self-attention for colorization", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kumar|colorization_transformer", "pdf": "/pdf/f2f5d9057587995de8d113d1ba35dd7d8b98f48e.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkumar2021colorization,\ntitle={Colorization Transformer},\nauthor={Manoj Kumar and Dirk Weissenborn and Nal Kalchbrenner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5NA1PinlGFu}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "5NA1PinlGFu", "replyto": "5NA1PinlGFu", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3388/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538076779, "tmdate": 1606915790302, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3388/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3388/-/Official_Review"}}}, {"id": "lSNRY4-0hgd", "original": null, "number": 4, "cdate": 1603943684088, "ddate": null, "tcdate": 1603943684088, "tmdate": 1606821371784, "tddate": null, "forum": "5NA1PinlGFu", "replyto": "5NA1PinlGFu", "invitation": "ICLR.cc/2021/Conference/Paper3388/-/Official_Review", "content": {"title": "Lack of clarity and novelty, weak evaluation", "review": "Thank the authors for addressing reviewers' comments extensively. After rebuttal, I agree with the significance of the proposed method in terms of performance improvement in this particular task. However, the technical novelty is still limited. Thus, I increased my rating to 5.\n\nIn this paper, the authors propose an autoregressive image colorization method based on self-attention. The proposed method first infers an initial low-resolution colorization in an autoregressive manner, then upsamples both spatial resolution and color depth. The authors adopt self-attention to encode contextual information of the scene. Experimental results show that each component of the proposed method is effective and the proposed method outperforms an existing autoregressive method.\n\nOverall, it is difficult to understand the contribution of this paper. I think it is because the writing in Sec. 1 and 2 is unclear. Particularly, the writing of introduction needs a significant improvement as the authors reveal too much details of this paper instead of describing the high-level motivation of the proposed method and the technical contribution. The clarity also needs to be improved in the method and experiment sections. (e.g. ColTran Core in Fig. 2 is confusing. It looks complicated, but the writing is too short. what is the ground-truth in the objective? what about Table 2? each baseline is not explained and cited.)\n\nTechnical novelty is incremental. I could not understand the motivation of the proposed network due to the clarity issue, but this paper generally adopts existing methods such as an autoregressive model and self-attention blocks to apply them to an image colorization problem, which limits the novelty of this paper.\n\nEvaluation is weak. PixColor is an old model (in 2017), so recent methods and state-of-the-art methods should be compared. I could not find out what the baseline methods in Table 2 are, but they do not look like state-of-the-art models. Performance gain over the previous autoregressive model using widely-used self-attention blocks is not enough for accepting this paper.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3388/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3388/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Colorization Transformer", "authorids": ["~Manoj_Kumar1", "~Dirk_Weissenborn1", "~Nal_Kalchbrenner1"], "authors": ["Manoj Kumar", "Dirk Weissenborn", "Nal Kalchbrenner"], "keywords": [], "abstract": "We present the Colorization Transformer, a novel approach for diverse high fidelity image colorization based on self-attention. Given a grayscale image, the colorization proceeds in three steps. We first use a conditional autoregressive transformer to produce a low resolution coarse coloring of the grayscale image. Our architecture adopts conditional transformer layers to effectively condition grayscale input. Two subsequent fully parallel networks upsample the coarse colored low resolution image into a finely colored high resolution image. Sampling from the Colorization Transformer produces diverse colorings whose fidelity outperforms the previous state-of-the-art on colorising ImageNet based on FID results and based on a human evaluation in a Mechanical Turk test. Remarkably, in  more than 60\\% of cases human evaluators prefer the highest rated among three generated colorings over the ground truth. The code and pre-trained checkpoints for Colorization Transformer are publicly available  at https://github.com/google-research/google-research/tree/master/coltran", "one-sentence_summary": "Self-attention for colorization", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kumar|colorization_transformer", "pdf": "/pdf/f2f5d9057587995de8d113d1ba35dd7d8b98f48e.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkumar2021colorization,\ntitle={Colorization Transformer},\nauthor={Manoj Kumar and Dirk Weissenborn and Nal Kalchbrenner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5NA1PinlGFu}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "5NA1PinlGFu", "replyto": "5NA1PinlGFu", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3388/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538076779, "tmdate": 1606915790302, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3388/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3388/-/Official_Review"}}}, {"id": "v6Tpi142mBb", "original": null, "number": 2, "cdate": 1603915973747, "ddate": null, "tcdate": 1603915973747, "tmdate": 1606774282991, "tddate": null, "forum": "5NA1PinlGFu", "replyto": "5NA1PinlGFu", "invitation": "ICLR.cc/2021/Conference/Paper3388/-/Official_Review", "content": {"title": "Image colorization based on self-attention", "review": "--- Update ---\nThe authors have addressed several concerns that I had regarding the work.  While this is largely an application of a previous method, they have made some application-specific decisions in order to achieve the significant boost in performance on colorization that they saw.  While the metrics for this task are much improved, there are still some things to be desired on the qualitative results (i.e. the diversity of results tends to be in blocks, as opposed to high within-image color variability).  Nevertheless, I think the improvement from this approach my guide future work in this area.  Given the author's responses and changes made, I have amended my recommendation accordingly. \n\n__1.  Summary__\nThe authors propose a method for image colorization based on self-attention largely following the architecture of the Axial Transformer (Ho et al., 2019b).  This approach outperforms several SOTA colorization models on FID and human evaluation.  \n\n\n__2a. Strong Points__\nThe motivation for this work is clear.  Image colorization has many applications and while past approaches have significantly advanced in the past few years, there is certainly much left to be explored in this space.\n\nThe recap/explanation of the Axial Transformer is clear and concise.  My concern (see below) is not with the articulation of this section, but more on the reliance of an approach that hasn\u2019t been accepted via peer review.  \n\nThe performance of this method using both FID as well as using human evaluators is compelling.  \n\nBreaking the problem of colorization into two intermediate low resolution images is a nice approach for enabling larger models.  One question would be how well a single model would perform if smaller images were all that was required.  \n\nThe ablation studies show how different components impact the performance.  \n\n\n__2b. Weak Points__\nAll three modules of this approach are based on method of (Ho et al., 2019b), which is available on arxiv, but was rejected from ICLR 2020.  The current work is focused on the application of that method. This makes for a bit of a tricky situation.  The description of the Axial Transformer is given in section 4, but it is only textual and refers the readers back to the pre-print for more detail.  Since this is the central method of the current work, at a minimum I think it requires more explanation/justification as opposed to pointing to a work that has not been accepted via peer review.  \n\nWhile the language of the paper is fine, the overall flow of the paper is lacking a bit of narrative.  Overall I found myself having to jump around to find the definition and explanation of important things.  Particularly within the description of the model, it would be good to add some language to help the sections flow- currently they feel very independent.  Alternately, if maybe help if in the beginning part of the model, the different model components (fc, fs, etc.) are named there.  Related, the Architecture Section feels out of place after the Model description.  There are references to the attention layers in the model description which are not explored until the Architecture section.  Perhaps it makes sense to put the Architecture section first because it\u2019s addressing layers/mechanisms that span all aspects of the model.  Or perhaps combining the two sections?  Right now it feels like there are two methods sections.\n\nSome of the text around Eqn(7) seems to be missing because the sentence structure doesn't make sense.\n\nIt\u2019s not clear what some of the labels in Figure 3 mean.  You have to go into the text to find out what MLP 4x means, for example, and then when you find it in section 5.2, you have to go back to section 4.3 to actually understand what it means.  \n\nThe ablation studies feel like they\u2019re done in relative isolation.  It would be useful to know, for example, how the lower performance of using the standard Axial Transformer vs. the conditional Axial transformer impacts the final results, not just that portion.  The section \u201cConditioning Details\u201d in 5.2 just feels like a results dump.  It\u2019s unclear what motivates those particular ablation choices and what those results tell the reader more generally about this approach.  Some kind of context or discussion would be useful.  In general, this section feels like it\u2019s being included just to show that ablation studies were performed without providing any greater understanding as to the approach (to potentially motivate future work or other examples, for instance).  The descriptions are also very terse.  If these experiments add meaningful insight to this approach, then they belong in the main text with additional explanation and discussion.  If they are merely a justification that this approach works, then I would suggest moving most of this section to the appendix and using the space to give better explanation of the methods and results which are central to the application.  \n\nSome of the models which the current method is compared to (Table 2) are not referenced to the best of my knowledge.  What does \"CNN\" mean in this case?  Do all of these methods use a combined spatial and color upsampling method?  If not, how were they implemented?  This is actually a pretty significant issue as it limits the reproducibility of the comparative experiments.  \n\n\n__3. Recommendation__\nReject.  While the results are compelling, the work largely relies on a method which has not been accepted via peer review.  That in and of itself does not warrant rejection, but I believe it contributed to some of the difficulties in explaining the approach, the motivation behind the approach, the results of the ablation studies, etc., which make the paper extremely difficult to follow, likely difficult to build upon, and potentially difficult to reproduce.\n\n\n__4. Recommendation Explanation__\nI would argue that the main goal of this paper is to show a novel application of the Axial Transformer approach of Ho et al 2019b and this is done by adapting that method to the task of Image Colorization.  I would argue the focus is around applying that method, not exclusively doing better Image Colorization, because there is no discussion around how this advances our understanding of image colorization broadly.  Nevertheless, that (showing the usefulness of an approach to a new task) is a valid objective, but because Ho et al 2019b has not been formally accepted, it also somewhat then requires this work to explain and justify approaches of that work.  I believe that challenge has a lot to do with some of the difficulties in the paper around the methods and experiment explanation.  \n\nWhile the (within sentence) language is clear, the overall flow of the paper is  difficult to follow.  It feels like the authors were strongly up-against the page limit, so important explanation and discussion was omitted or made very terse.  For example, the ablation studies, while thorough, sort of feel dumped there.  There's no discussion as to why those and not other experiments were run and what the results of those experiments tell us more broadly.  Similarly, the model and architecture section seem like they should be more intertwined.  As another example, some of the methods in Table 2 are not referenced anywhere and it's not clear how they were used in this context (did they start with a low res image, or high-res image).  That calls the reproducibility of the comparison studies into question.\n\n\n__5.  Questions__\nOverall it seems like every generated image has a red, green, and blue variant.  Were they sampled in a particular manner to guarantee this?  Obviously it is possible to draw other samples, but do they all largely fall into one of these three coarse categories?  When the performance is poor for a given sample, it usually because entire swaths of the image are being painted in with a very non-natural color (like someone\u2019s face being green, or the entire picture having a blue-ish exposure).  Can you speak to this and other common \u201cmistakes\u201d that are observed?  How do these compare with some of the other methods you compared yours against?  Are there simply fewer \u201cmistakes\u201d (i.e. non-natural images), or are the types of imperfections created by this approach different that would warrant different use-cases?\n\nIt seems like a lot of compute (16 TPUv2) was used and the batch size was relatively large.  Is the large batch size necessary for obtaining these results, or could a smaller amount of compute and smaller batch size be used?\n\nWhy does training baselines with 2x and 4x wider MLP dimensions make \u201ca fair comparison\u201d?  Is \u201cBaseline\u201d in Figure 3, x1 (standard) MLP but no conditioning?  Why would x1 be better than x4, but worse than x2?\n\nThe caption of Figure 2 feels a bit imbalanced.  ColTran core is called out specifically, but then the ColTran Upsamplers are not referenced.  Is the \u201cAxial Transformer\u201d just the right branch of the ColTran Core (which the figure seems to suggest) or the entire ColTran core, as the caption seems to suggest.\n\nOn pg. 3 \u201cColTran Core\u201d it is stated that \u201cwe also train a parallel prediction head which we found beneficial for regularization\u201d.  I think it would be useful to given additional explanation here as it\u2019s a fairly significant architectural choice.  If results of not including this head exist, perhaps it would be useful to show this in the appendix.  Otherwise a brief explanation as to why this additional head aids the regularization would be useful.  Since this is an instantiation of the Axial Transformer, is this prediction head added to that approach for this particular task, or is this already a part of the standard Axial Transformer (and therefore maintained here for consistency)?  Ah, this is explored further in section 5.3\u2026. It would be helpful to the reader to reference this section when you introduce the prediction head (i.e. that the impact will be explored in section 5.3).\n\nIn 4.2 it says they \u201cadapt the Axial Transformer model for colorization\u201d.  Can you elaborate on the adaptation?  It\u2019s not clear (without looking up that reference), what belongs to the original approach vs. what was added/changed here for this specific task.\n\nIt feels odd to mention the number of axial attention blocks in the training section as opposed to the model or architecture.  This is a fundamental architectural choice, is it not?  \n\nWhy are the set of models compared via FID and Human Evaluation different?\n\n\n__6.  Feedback__\nThe demonstrated colorization scores and output are compelling, however, I believe the structure of text is very detrimental.  I think it would potentially be feasible to fully rework the text to make it more readable and reproducible and therefore a solid publication because the result is compelling, but as it stands, there is substantial rewritting which would need to be done in my opinion.\n\nIn \u201cModel: ColTran Core\u201d fc is described as a conditional, auto-regressive axial transformer.  While the definition of pc and pc~ are stated thereafter, there is not any further description as to what this means and/or a citation.  The Ho et al. citation is provided in the Figure 2 caption.  At a minimum that citation should be given here as well, but it would be good to give a textual description as to what an \u201ca conditional auto-regressive axial transformer\u201d is since it is not a commonly used architecture.  \n\nThe second paragraph under ColTran Upsamplers (In our experiments\u2026) is slightly confusing.  It seems to suggest that parallel upsampling is sufficient and advantageous for a number of reasons, but that prediction is chosen to reduce color inconsistencies.  Then it seems to go back to again say that Parallel upsampling has a huge advantage of being fast.  This is perhaps also confusing because there is a \u201cSample\u201d label in Figure 2.  The confusion is less about the validity of the approach and more that the language (in conjunction with the figure) is difficult to follow for someone not already familiar with Guadarrama 2017.\n\nWhile not necessary, it would be interesting to see how this approach performs on out of domain images (i.e. not from ImageNet).  \n\nIn 5.5, it\u2019s stayed you follow the protocol used in PixColor.  It would probably be best to additionally include the citation here, or the citation in place of \u201cPixColor\u201d even though that work is cited near the beginning of the paper (when the reader comes to this section, they may be unfamiliar with this approach and would like to go directly to that reference as opposed to having the search for \u201cPixColor\u201d and then go find the reference).  ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3388/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3388/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Colorization Transformer", "authorids": ["~Manoj_Kumar1", "~Dirk_Weissenborn1", "~Nal_Kalchbrenner1"], "authors": ["Manoj Kumar", "Dirk Weissenborn", "Nal Kalchbrenner"], "keywords": [], "abstract": "We present the Colorization Transformer, a novel approach for diverse high fidelity image colorization based on self-attention. Given a grayscale image, the colorization proceeds in three steps. We first use a conditional autoregressive transformer to produce a low resolution coarse coloring of the grayscale image. Our architecture adopts conditional transformer layers to effectively condition grayscale input. Two subsequent fully parallel networks upsample the coarse colored low resolution image into a finely colored high resolution image. Sampling from the Colorization Transformer produces diverse colorings whose fidelity outperforms the previous state-of-the-art on colorising ImageNet based on FID results and based on a human evaluation in a Mechanical Turk test. Remarkably, in  more than 60\\% of cases human evaluators prefer the highest rated among three generated colorings over the ground truth. The code and pre-trained checkpoints for Colorization Transformer are publicly available  at https://github.com/google-research/google-research/tree/master/coltran", "one-sentence_summary": "Self-attention for colorization", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kumar|colorization_transformer", "pdf": "/pdf/f2f5d9057587995de8d113d1ba35dd7d8b98f48e.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkumar2021colorization,\ntitle={Colorization Transformer},\nauthor={Manoj Kumar and Dirk Weissenborn and Nal Kalchbrenner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5NA1PinlGFu}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "5NA1PinlGFu", "replyto": "5NA1PinlGFu", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3388/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538076779, "tmdate": 1606915790302, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3388/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3388/-/Official_Review"}}}, {"id": "CXBmHZzufTC", "original": null, "number": 13, "cdate": 1606197977816, "ddate": null, "tcdate": 1606197977816, "tmdate": 1606205848492, "tddate": null, "forum": "5NA1PinlGFu", "replyto": "5NA1PinlGFu", "invitation": "ICLR.cc/2021/Conference/Paper3388/-/Official_Comment", "content": {"title": "End of discussion phase", "comment": "Hi all,\nSince the end of the discussion phase is fast approaching, we would like to know if we our rebuttal helped to clarify some concerns. If there were other concerns that we could help to clarify, please do let us know.\nThanks."}, "signatures": ["ICLR.cc/2021/Conference/Paper3388/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3388/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Colorization Transformer", "authorids": ["~Manoj_Kumar1", "~Dirk_Weissenborn1", "~Nal_Kalchbrenner1"], "authors": ["Manoj Kumar", "Dirk Weissenborn", "Nal Kalchbrenner"], "keywords": [], "abstract": "We present the Colorization Transformer, a novel approach for diverse high fidelity image colorization based on self-attention. Given a grayscale image, the colorization proceeds in three steps. We first use a conditional autoregressive transformer to produce a low resolution coarse coloring of the grayscale image. Our architecture adopts conditional transformer layers to effectively condition grayscale input. Two subsequent fully parallel networks upsample the coarse colored low resolution image into a finely colored high resolution image. Sampling from the Colorization Transformer produces diverse colorings whose fidelity outperforms the previous state-of-the-art on colorising ImageNet based on FID results and based on a human evaluation in a Mechanical Turk test. Remarkably, in  more than 60\\% of cases human evaluators prefer the highest rated among three generated colorings over the ground truth. The code and pre-trained checkpoints for Colorization Transformer are publicly available  at https://github.com/google-research/google-research/tree/master/coltran", "one-sentence_summary": "Self-attention for colorization", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kumar|colorization_transformer", "pdf": "/pdf/f2f5d9057587995de8d113d1ba35dd7d8b98f48e.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkumar2021colorization,\ntitle={Colorization Transformer},\nauthor={Manoj Kumar and Dirk Weissenborn and Nal Kalchbrenner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5NA1PinlGFu}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "5NA1PinlGFu", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3388/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3388/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3388/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3388/Authors|ICLR.cc/2021/Conference/Paper3388/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3388/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923838088, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3388/-/Official_Comment"}}}, {"id": "o3RTc03AFF_", "original": null, "number": 12, "cdate": 1605717431535, "ddate": null, "tcdate": 1605717431535, "tmdate": 1605869957535, "tddate": null, "forum": "5NA1PinlGFu", "replyto": "5NA1PinlGFu", "invitation": "ICLR.cc/2021/Conference/Paper3388/-/Official_Comment", "content": {"title": "Rebuttal", "comment": "Thanks everyone for your time and reviews. Here are a summary of the changes\n\n**Writing**\n* Restructured the end of the introduction. It now highlights the motivation of the network and contributions of the paper.\n* Moved **Row and Column Self Attention** and **Axial Transformer** to **Section 3 Background: Axial Transformer**\n* Expanded *subsection \u201cAxial Transformer\u201d with paragraphs **Outer Decoder, Inner Decoder and Encoder** and equations.\n* Merged the remainder of the \u201cModel\u201d section and \u201cArchitecture\u201d into a **Section 4 Proposed Architecture**\n* Expanded the **Ablation Studies (Section 5.2)** to add some insights from each experiment\n* Added citations to all models in **Table 2**\n* Added a small background on autoregressive models to **Appendix A**\n* Added #parameters and inference speed comparisons to the **Appendix H**\n * Edit: Nov 20, provided a bit more detail on the semi-parallel sampling mechanism. \n\n**Experiments**\n* Added out-of-domain colorizations on Celeb-A and LSUN **Appendix D**\n* Added experiments on the low-batch size regime **Appendix E**\n* Added final FID results on the baseline Axial Transformer to **Table 2 (ColTran-B)**\n* Added shift modulation experiments to the graph in **Figure 3**.\n* Added global conditioning on cAtt and cMLP experiment **Appendix G**\n\nGiven the strong empirical performance and experimentation (mentioned by multiple reviewers) and our now improved writing,\nWe hope that all our contributions as a whole would be of interest to the ICLR audience.\n\nPlease reconsider your scores after the rebuttal revision."}, "signatures": ["ICLR.cc/2021/Conference/Paper3388/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3388/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Colorization Transformer", "authorids": ["~Manoj_Kumar1", "~Dirk_Weissenborn1", "~Nal_Kalchbrenner1"], "authors": ["Manoj Kumar", "Dirk Weissenborn", "Nal Kalchbrenner"], "keywords": [], "abstract": "We present the Colorization Transformer, a novel approach for diverse high fidelity image colorization based on self-attention. Given a grayscale image, the colorization proceeds in three steps. We first use a conditional autoregressive transformer to produce a low resolution coarse coloring of the grayscale image. Our architecture adopts conditional transformer layers to effectively condition grayscale input. Two subsequent fully parallel networks upsample the coarse colored low resolution image into a finely colored high resolution image. Sampling from the Colorization Transformer produces diverse colorings whose fidelity outperforms the previous state-of-the-art on colorising ImageNet based on FID results and based on a human evaluation in a Mechanical Turk test. Remarkably, in  more than 60\\% of cases human evaluators prefer the highest rated among three generated colorings over the ground truth. The code and pre-trained checkpoints for Colorization Transformer are publicly available  at https://github.com/google-research/google-research/tree/master/coltran", "one-sentence_summary": "Self-attention for colorization", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kumar|colorization_transformer", "pdf": "/pdf/f2f5d9057587995de8d113d1ba35dd7d8b98f48e.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkumar2021colorization,\ntitle={Colorization Transformer},\nauthor={Manoj Kumar and Dirk Weissenborn and Nal Kalchbrenner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5NA1PinlGFu}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "5NA1PinlGFu", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3388/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3388/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3388/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3388/Authors|ICLR.cc/2021/Conference/Paper3388/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3388/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923838088, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3388/-/Official_Comment"}}}, {"id": "TPTSk_hj0NC", "original": null, "number": 9, "cdate": 1605712229267, "ddate": null, "tcdate": 1605712229267, "tmdate": 1605801787019, "tddate": null, "forum": "5NA1PinlGFu", "replyto": "v6Tpi142mBb", "invitation": "ICLR.cc/2021/Conference/Paper3388/-/Official_Comment", "content": {"title": "Review Response: AnonReviewer 4 (Part 2) ", "comment": "**Ablation studies**\n\nWe rewrote the Ablation Studies subsection of the paper. The \u201cconditioning details\u201d section is expanded with bullet points providing a high-level motivation for each experiment. We add it here for convenience.\n\n* We added final FID numbers for the baseline Axial Transformer that conditions only via skip-connections without conditioning layers (**ColTran - B**) in Table 2. The baseline achieves a FID Score of 21.6 (significantly better than the baselines but much worse than ours)\n* **Importance of each conditional component:** We perform a leave-one-out study to determine the importance of each conditional component. We remove each conditional component one at a time and retrain the new ablated model. The curves *no cLN*, *no cMLP* and *no cAtt* in the middle of Figure 3 quantifies our results. While each conditional component improves final performance, cAtt plays the most important role.\n*  **Multiplicative vs Additive Interactions:** Conditional transformer layers employ both conditional shifts and scales consisting of additive and multiplicative interactions, respectively. The curves *Scale* and *Shift* on the right hand side of Figure 3 demonstrate the impact of these interactions via ablated architectures that use conditional shifts and conditional scales only. While both types of interactions are important, multiplicative interactions have a much stronger impact.\n* **Context-aware dot product attention:** Self-attention computes similarity between pixel representations using a dot product between $k$ and $q$, cAtt applies conditional shifts and scales on $q$, $k$ and allow modifying this similarity based on contextual information. The curve *cAtt, only v* depicts that removing this property, by conditioning only on $v$ leads to worse results.\n* **Fixed vs adaptive global representation** cLN aggregates global information with a flexible learnable spatial pooling layer. We experimented with a fixed mean pooling layer forcing all the cLN layers to use the same global representation with the same per-pixel weight. The curve *cLN, mean pool* on the right of Figure 3 shows that enforcing this constraint causes inferior performance as compared to even having no cLN. This indicates that different aggregations of global representations are important for different cLN layers.\n* We expanded the caption in Figure 3 and added a short description of what each label means . This is meant to be a visual aid to the more detailed explanations in Section 5.2\n* We moved the curve computing Gated operations for conditioning and the additional ablation suggested by AnonRev1 to the **appendix G**\n\n**Other questions**\n\n**Q: Overall it seems like every generated image has a red, green, and blue variant. Were they sampled in a particular manner to guarantee this? Obviously it is possible to draw other samples, but do they all largely fall into one of these three coarse categories?**\n\nAll our generated images are displayed with pixel-by-pixel sampling. They were not sampled in any other manner. We analyzed what the most dominant coarse color is per-image across 5000 images. The dominant hues ordered by counts are black, white, brown, blue and green. Here is the color band of the top 50 colors (https://ibb.co/Jx9htXq)\n\n**Q: When the performance is poor for a given sample, it usually because entire swaths of the image are being painted in with a very non-natural color (like someone\u2019s face being green, or the entire picture having a blue-ish exposure). Can you speak to this and other common \u201cmistakes\u201d that are observed?**\n\nThis is true. Every now and then, we can sample a coarse color for a pixel that has low probability and which the model has not seen before. This can then have a cascading effect leading to such mistakes. Some other mistakes which achieved a 0% fool rate are in Appendix I:\n* Color bleeding when edges are not detected correctly.\n* Inability to color highly complex scenes, such as large no of small objects and complex textures, e.g the dress of a soldier.\n* Once in a while, also we observe that the model returns the grayscale image as a sample. But this is pretty rare.\n\n**Q: How do these compare with some of the other methods you compared yours against? Are there simply fewer \u201cmistakes\u201d (i.e. non-natural images), or are the types of imperfections created by this approach different that would warrant different use-cases?**\nArtifacts such as color-bleeding and unnatural colors are common among the probabilistic colorization models that we compare against on inspection of the samples. You are right that on an average our model generates more natural colorizations avoiding such artifacts given the human evaluation results.\n\nAutoregressive colorization also has a human-in-the-loop use case. For every-pixel, the model can display x most probable colors that the user can choose from and the colorization can be guided by the user.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3388/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3388/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Colorization Transformer", "authorids": ["~Manoj_Kumar1", "~Dirk_Weissenborn1", "~Nal_Kalchbrenner1"], "authors": ["Manoj Kumar", "Dirk Weissenborn", "Nal Kalchbrenner"], "keywords": [], "abstract": "We present the Colorization Transformer, a novel approach for diverse high fidelity image colorization based on self-attention. Given a grayscale image, the colorization proceeds in three steps. We first use a conditional autoregressive transformer to produce a low resolution coarse coloring of the grayscale image. Our architecture adopts conditional transformer layers to effectively condition grayscale input. Two subsequent fully parallel networks upsample the coarse colored low resolution image into a finely colored high resolution image. Sampling from the Colorization Transformer produces diverse colorings whose fidelity outperforms the previous state-of-the-art on colorising ImageNet based on FID results and based on a human evaluation in a Mechanical Turk test. Remarkably, in  more than 60\\% of cases human evaluators prefer the highest rated among three generated colorings over the ground truth. The code and pre-trained checkpoints for Colorization Transformer are publicly available  at https://github.com/google-research/google-research/tree/master/coltran", "one-sentence_summary": "Self-attention for colorization", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kumar|colorization_transformer", "pdf": "/pdf/f2f5d9057587995de8d113d1ba35dd7d8b98f48e.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkumar2021colorization,\ntitle={Colorization Transformer},\nauthor={Manoj Kumar and Dirk Weissenborn and Nal Kalchbrenner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5NA1PinlGFu}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "5NA1PinlGFu", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3388/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3388/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3388/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3388/Authors|ICLR.cc/2021/Conference/Paper3388/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3388/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923838088, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3388/-/Official_Comment"}}}, {"id": "pa0_QH7KcHr", "original": null, "number": 11, "cdate": 1605717019686, "ddate": null, "tcdate": 1605717019686, "tmdate": 1605717019686, "tddate": null, "forum": "5NA1PinlGFu", "replyto": "Ch646_YhDE1", "invitation": "ICLR.cc/2021/Conference/Paper3388/-/Official_Comment", "content": {"title": "Review Response: AnonReviewer 1", "comment": "Thanks for your reviews. Please find attached our response.\n\n**Additional ablations**\n* **Scale-only modulation** - We added this curve to **Figure 3**. Scale only modulations perform much better than shift only modulations. Our intuition is that scaling allows to increase or decrease per-pixel activations more easily as compared to biasing. We speculate that this could be useful (for eg to turn off the contributions of individual pixels based on the context when we compute dot products for self-attention)\n* **Global context for cMLP / cATT** - We added this curve to **Appendix G**. Our model performs much worse.\nOur intuition follows from 1). A global context for cAtt means that all key, query and value pairs are scaled/biased by constant values $c_k, c_q$ and $c_v$ The dot-product between k and q would be either $c_k * c_q * k \\cdot q$ (for scaling) and $k \\cdot q + c_k \\sum{q} + c_q \\sum{k} + c_k * c_q$. We speculate that this can have a net-effect in increasing the magnitude of dot-product attention and may lead to difficulties in optimization. Elementwise operations (as done in cAtt) are more flexible. For cMLP this effect is purely empirical.\n* **cLN with mean pooling performing worse than no cLN** We expanded a bit on this **Section 5.1**\nA fixed mean pooling layer forces all the cLN layers to use the same global representation with the same weightage per-pixel. The ablation indicates, it is likely that different global representations are meaningful for different cLN layers. Allowing the per-pixel weights to be learnable offers some degrees of freedom and hence different LayerNorm layers can make use of different aggregated global representations.\n\n**Number of parameters / training / inference speed**\nWe added a small section in **Appendix H** comparing PixColor to ColTran\n* **Training parameters**:  ColTran has a total of ColTran core (46M) + Color Upsampler (14M) + Spatial Upsampler (14M) = 74M parameters. This is lesser than PixColor that has Conditioning network (44M) + Colorizer network (11M) + Refinement Network (28M) = 83M parameters.\n* **Inference speed** ColTran core can sample 64x64 grayscale images in 4-5 minutes P100 GPU vs PixColor that takes ~10 minutes to colorize 28x28 grayscale images on a K40 GPU. Sampling 28x28 colorizations takes just around 30 seconds. The upsampler networks take in the order of milliseconds.\n* **Training time** Our training time is comparable to PixColor (~3 days). However, we are able to reach FID scores of 20.38 within a single day as compared to PixColor\u2019s final FID 24.32\n\n**References**\nWe added references to all baselines in Table 2. They are described in the related work section.\n\n**Technical novelty**\nWe added a section to the introduction better highlighting the contributions of this paper.\n* First application of transformers for high-resolution ($256 \\times 256$) image colorization.\n* We introduce conditional transformer layers for low-resolution coarse colorization in Section 4.1. The conditional layers incorporate conditioning information via multiple learnable components that are applied per-pixel and per-channel. We validate the contribution of each component with extensive experimentation and ablation studies.\n* We propose training an auxiliary parallel prediction model jointly with the low resolution coarse colorization model in Section 4.2. Improved FID scores demonstrate the usefulness of this auxiliary model."}, "signatures": ["ICLR.cc/2021/Conference/Paper3388/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3388/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Colorization Transformer", "authorids": ["~Manoj_Kumar1", "~Dirk_Weissenborn1", "~Nal_Kalchbrenner1"], "authors": ["Manoj Kumar", "Dirk Weissenborn", "Nal Kalchbrenner"], "keywords": [], "abstract": "We present the Colorization Transformer, a novel approach for diverse high fidelity image colorization based on self-attention. Given a grayscale image, the colorization proceeds in three steps. We first use a conditional autoregressive transformer to produce a low resolution coarse coloring of the grayscale image. Our architecture adopts conditional transformer layers to effectively condition grayscale input. Two subsequent fully parallel networks upsample the coarse colored low resolution image into a finely colored high resolution image. Sampling from the Colorization Transformer produces diverse colorings whose fidelity outperforms the previous state-of-the-art on colorising ImageNet based on FID results and based on a human evaluation in a Mechanical Turk test. Remarkably, in  more than 60\\% of cases human evaluators prefer the highest rated among three generated colorings over the ground truth. The code and pre-trained checkpoints for Colorization Transformer are publicly available  at https://github.com/google-research/google-research/tree/master/coltran", "one-sentence_summary": "Self-attention for colorization", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kumar|colorization_transformer", "pdf": "/pdf/f2f5d9057587995de8d113d1ba35dd7d8b98f48e.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkumar2021colorization,\ntitle={Colorization Transformer},\nauthor={Manoj Kumar and Dirk Weissenborn and Nal Kalchbrenner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5NA1PinlGFu}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "5NA1PinlGFu", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3388/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3388/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3388/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3388/Authors|ICLR.cc/2021/Conference/Paper3388/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3388/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923838088, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3388/-/Official_Comment"}}}, {"id": "5MoaqioViEY", "original": null, "number": 10, "cdate": 1605714846514, "ddate": null, "tcdate": 1605714846514, "tmdate": 1605714978553, "tddate": null, "forum": "5NA1PinlGFu", "replyto": "v6Tpi142mBb", "invitation": "ICLR.cc/2021/Conference/Paper3388/-/Official_Comment", "content": {"title": "Review Response: AnonReviewer 4 (Part 3)", "comment": "**Q: It seems like a lot of compute (16 TPUv2) was used and the batch size was relatively large. Is the large batch size necessary for obtaining these results, or could a smaller amount of compute and smaller batch size be used?**\n\n16 TPUv2 chips are the second lowest configuration available to us. As requested, we additionally trained ColTran core and the upsamplers on 4 TPUv2 chips (the lowest configuration) with a reduced-batch size of 56 and 192 each. For the spatial upsampler, we found that a batch-size of 8 was sub-optimal and led to a large deterioration in loss. We thus used a smaller spatial upsampler with 2 axial attention blocks with a batch-size of 16 and trained it also on 4 TPUv2 chips. Our FID drops from 19.71 to 20.9 which is still significantly better than the other models.\nWe note that in this experiment, we use just 12 TPUv2 chips in total while PixColor uses a total of 16 GPUs.\nWe added the above analysis to **Appendix E**\n\n**Q: Why does training baselines with 2x and 4x wider MLP dimensions make \u201ca fair comparison\u201d? Is \u201cBaseline\u201d in Figure 3, x1 (standard) MLP but no conditioning? Why would x1 be better than x4, but worse than x2?**\n\nWe edited the corresponding subsection in **Section 5.2**. Our baselines MLP2x and MLP 4x (now renamed to ColTran-B 2x and ColTran-B 4x) are original Axial Transformer networks that condition via just skip-connections.  Both *ColTran-B 2x* and *ColTran-B 4x* have an increased parameter count via $1 \\times 1$ dense layers which are the same operations due to which ColTran has an increased parameter count. So it makes for a fair comparison. Our results show that the increased performance cannot be explained solely by the fact that our model has more parameters.\n\nRe: why x1 performs better than x4, this is purely empirical. Our intuition is that sometimes wider networks can lead to worse performance due to the difficulty in optimization. We ran a small hyperparameter sweep over the learning rates for x1, x2 and x4 and report the best performance.\n\n**Q: The caption of Figure 2 feels a bit imbalanced.**\nWe expanded the caption of Figure 2 to give a short description about both ColTran core and the upsamplers. The figure now depicts the \"outer decoder\", \"inner decoder\" and \"encoder\" which were contributions of [Ho et.al 2019]. We now clarify our contributions in the caption itself.\n\n**Q: Auxiliary parallel head**\nThis is a contribution of our paper. As noted. we investigate the impact of this auxiliary parallel head in Section 5.3. We added a few words in Section 4.2 that we will study the effect of this later in Section 5.3.\n\n**Q: Adapt the Axial Transformer model for colorization.**\nWe removed this. All our architectural modifications or adaptations (i.e the conditional transformer layers and auxiliary parallel head) are now described in Section 4.3 and background information is described in Section 3.\n\n**Q: Number of axial attention blocks**\n\nWe believe that this is a hyperparameter of the network similar to the learning rate, optimizer choice and hidden size. We did a very small sweep using the baseline axial transformer (no conditional layers) with the following configurations to come with this number.\n* hidden size = 512, number of blocks = 4\n* hidden size = 1024, number of blocks = 2\n*  hidden size = 512, number of blocks = 2\n\nOnce we found the optimal configuration, we fixed this for all future architecture design.\nWe added the above to **Appendix F**\n\n**Q: In model, ColTran core\u2026.**\n\nIn our latest version of the paper, the \u201cColTran core\u201d paragraph and its corresponding equations have been merged in **Section 4.1 ColTran core**.  We introduce the terminology axial transformer and axial attention in **Section 3** before describing these components.\n\n\n**Q: The second paragraph under ColTran Upsamplers (In our experiments\u2026) is slightly confusing.**\n\nWe have moved this explanation to the end of the current section 4.3.\nWe upsample all pixels in parallel (parallel upsampling) to predict a distribution over each-pixel in the high resolution image (Eq 6). Instead of sampling from this predicted distribution, we instead use the argmax. There might be a slight confusion between \"upsampling\" and \"sampling\" which we clarify.\n\n**Q: Out of domain images might be interesting**\n\nColorizations of out-of-domain datasets **LSUN-bedrooms** and **Celeb-A** have been added to **Appendix D**. We neither cherry picked these images nor finetune/retrain our model on these datasets. Barring a couple of outliers, our colorizations are realistic. We will colorize \u201cin-the-wild\u201d grayscale images for the final version.\n\n**Q: Probably be best to additionally include the citation here**\nWe added the citation of PixColor to this section.\n\nWe believe we have clarified all your comments and improved the writing. We are looking forward to reading your updated impression of the paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper3388/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3388/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Colorization Transformer", "authorids": ["~Manoj_Kumar1", "~Dirk_Weissenborn1", "~Nal_Kalchbrenner1"], "authors": ["Manoj Kumar", "Dirk Weissenborn", "Nal Kalchbrenner"], "keywords": [], "abstract": "We present the Colorization Transformer, a novel approach for diverse high fidelity image colorization based on self-attention. Given a grayscale image, the colorization proceeds in three steps. We first use a conditional autoregressive transformer to produce a low resolution coarse coloring of the grayscale image. Our architecture adopts conditional transformer layers to effectively condition grayscale input. Two subsequent fully parallel networks upsample the coarse colored low resolution image into a finely colored high resolution image. Sampling from the Colorization Transformer produces diverse colorings whose fidelity outperforms the previous state-of-the-art on colorising ImageNet based on FID results and based on a human evaluation in a Mechanical Turk test. Remarkably, in  more than 60\\% of cases human evaluators prefer the highest rated among three generated colorings over the ground truth. The code and pre-trained checkpoints for Colorization Transformer are publicly available  at https://github.com/google-research/google-research/tree/master/coltran", "one-sentence_summary": "Self-attention for colorization", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kumar|colorization_transformer", "pdf": "/pdf/f2f5d9057587995de8d113d1ba35dd7d8b98f48e.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkumar2021colorization,\ntitle={Colorization Transformer},\nauthor={Manoj Kumar and Dirk Weissenborn and Nal Kalchbrenner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5NA1PinlGFu}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "5NA1PinlGFu", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3388/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3388/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3388/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3388/Authors|ICLR.cc/2021/Conference/Paper3388/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3388/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923838088, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3388/-/Official_Comment"}}}, {"id": "eb8g8LJaz96", "original": null, "number": 8, "cdate": 1605710653815, "ddate": null, "tcdate": 1605710653815, "tmdate": 1605712318572, "tddate": null, "forum": "5NA1PinlGFu", "replyto": "v6Tpi142mBb", "invitation": "ICLR.cc/2021/Conference/Paper3388/-/Official_Comment", "content": {"title": "Review Response: AnonReviewer 4 (Part 1)", "comment": "We thank you for investing a significant amount of time in providing detailed reviews and to help us improve the quality of the paper. We have significantly restructured the writing to incorporate your suggestions. We first address the most pressing concerns followed by the minor comments. All our responses are reflected in the latest version of the draft.\n\n**Motivation for using axial transformer**\n\nA summarized version of the points below is added to the end of **Introduction**\n\n* Axial Transformer achieves state-of-the-art on unconditional image generation (at the time of submission) measured using bits-per-pixel on ImageNet32 and Imagenet64 without the usage of custom kernels. It is thus very appealing to use as an autoregressive backbone for colorization.\n* ColTran shares the highly useful advantages of Axial Transformer which is the ability to capture a global receptive field with two layers and efficient implementation using matrix multiplication on modern accelerators such as TPUs.\n* The semi-parallel sampling in Axial Transformer enables us to sample colorizations much faster than prior autoregressive colorization models. As a result, ColTran core can sample 64x64 grayscale images in around 5 minutes P100 GPU vs PixColor that takes ~10 minutes to colorize 28x28 grayscale images on a K40 GPU. Sampling 28x28 colorizations takes just around 30 seconds. \n* AxialDeepLab, a model that applies axial self-attention to semantic segmentation (which at core is the same technique as Axial Transformer modulo masking operations) was recently accepted to ECCV 2020 (https://arxiv.org/abs/2003.07853). We added a citation to this paper in the introduction. \n* The openreview of the Axial Transformer ICLR indicates the method was rejected primarily due to lack of clarity on the contribution of the paper. Two reviewers point out that the claim of the paper was a general purpose technique to improve self-attention in multidimensional transformers while the scope of the paper is indeed limited to autoregressive image modelling.\n* The code for the Axial Transformer is fully open sourced, which can help in removing any ambiguity about the implementation details. We will open source our code as well.\n\n**More explanation on Axial Transformer**\n\n* We expanded the subsection that explains Axial Transformer in section 3.2.\n* The section now contains 3 paragraphs describing the outer decoder, inner decoder and encoder with their corresponding equations.\n* We added a couple of sentences describing the semi-parallel sampling scheme of the Axial Transformer.\n\n**Improving the flow of the paper**\nAs requested, we restructured the methods / architecture section in the latest version of the draft and made the following changes. We hope the new narrative is clearer.\n\n* We introduced Section 3 \u201cBackground: Axial Transformer\u201d and moved the subsections \u201cRow and Column Self-Attention\u201d and \u201cAxial Transformer\u201d into this. This section is meant for the paper to be self-contained. All terminology revolving the Axial Transformer and axial self-attention is introduced in this section. \n* We combined Section 3 \u201cModel\u201d and Section 4 \u201cArchitecture\u201d, to form a \u201cProposed Architecture\u201d section to make it more intertwined. There are the changes.\n    * **Introduction of Section 4**: Conditional distributions modeled by the three networks.\n    * **Section 4.1**: ColTran core, the equation and the architectural modifications.\n    * **Section 4.2**: Auxiliary parallel head and their equation.\n    * **Section 4.3**: The color and spatial upsamplers and their equations.\n* In short, Section 3 now contains the background material and Section 4 contains the modifications for high-resolution colorization which are contributions of this work.\n\n**Results in Table 2**\n* We added citations to all the baseline models in Table 2 and what underlying generative model they rely on in the Related Work section,\n* We obtained results on FID from cINN [Ardizzone et.al, 2019] and the results on human evaluation results from PixColor. To compute the FID results of PixColor, we used 5000 samples which were provided by the original authors.\n* All of these techniques perform high resolution colorization from a grayscale image, so the numbers are directly comparable. CNN is a deterministic baseline used in [Ardizzone et al 2019]. We removed it as CIC, LRAC and LTBC are also based on deterministic convolutional neural networks."}, "signatures": ["ICLR.cc/2021/Conference/Paper3388/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3388/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Colorization Transformer", "authorids": ["~Manoj_Kumar1", "~Dirk_Weissenborn1", "~Nal_Kalchbrenner1"], "authors": ["Manoj Kumar", "Dirk Weissenborn", "Nal Kalchbrenner"], "keywords": [], "abstract": "We present the Colorization Transformer, a novel approach for diverse high fidelity image colorization based on self-attention. Given a grayscale image, the colorization proceeds in three steps. We first use a conditional autoregressive transformer to produce a low resolution coarse coloring of the grayscale image. Our architecture adopts conditional transformer layers to effectively condition grayscale input. Two subsequent fully parallel networks upsample the coarse colored low resolution image into a finely colored high resolution image. Sampling from the Colorization Transformer produces diverse colorings whose fidelity outperforms the previous state-of-the-art on colorising ImageNet based on FID results and based on a human evaluation in a Mechanical Turk test. Remarkably, in  more than 60\\% of cases human evaluators prefer the highest rated among three generated colorings over the ground truth. The code and pre-trained checkpoints for Colorization Transformer are publicly available  at https://github.com/google-research/google-research/tree/master/coltran", "one-sentence_summary": "Self-attention for colorization", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kumar|colorization_transformer", "pdf": "/pdf/f2f5d9057587995de8d113d1ba35dd7d8b98f48e.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkumar2021colorization,\ntitle={Colorization Transformer},\nauthor={Manoj Kumar and Dirk Weissenborn and Nal Kalchbrenner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5NA1PinlGFu}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "5NA1PinlGFu", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3388/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3388/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3388/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3388/Authors|ICLR.cc/2021/Conference/Paper3388/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3388/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923838088, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3388/-/Official_Comment"}}}, {"id": "82NU_eMgcsV", "original": null, "number": 6, "cdate": 1605707216942, "ddate": null, "tcdate": 1605707216942, "tmdate": 1605710851262, "tddate": null, "forum": "5NA1PinlGFu", "replyto": "lSNRY4-0hgd", "invitation": "ICLR.cc/2021/Conference/Paper3388/-/Official_Comment", "content": {"title": "Review Response: AnonReviewer 3 [2 / 2]", "comment": "**Evaluation**\n\n* We added the most recent baseline in the colorization literature cINN, [Ardizzone et al, 2019], that uses a combination of a VGG network + Glow [Kingma et al, 2018] in Table 2. The model performs slightly worse than PixColor. \n* We extend the original axial transformer [Ho 2019 et al] for image colorization which is a contribution of our work. This is a strong baseline in itself. (ColTran - B) in Table 2 and Figure 3.\n* We improve upon this baseline significantly by our architectural modifications. Comparisons to ColTran-B are provided in Table 2 and Figure 3.\n* We did an extensive literature survey on existing generative colorization techniques in Section 2 as noted by.Reviewer 1. PixColor, despite coming out in 2017, is still a state-of-the-art colorization model.\n* Image colorization is a underexplored yet important research area (noted by Reviewer 1 and 4). Hence it is not entirely surprising that there is a lot of scope to improve existing state-of-the-art colorization techniques as compared to unconditional image generation.\n* The quality of our colorizations are almost imperceptible from the ground-truth barring a few outliers as reflected in mechanical turk results.\n* All-in-all,  we believe advancing the state-of-the-art significantly (~20% relative improvement) over prior techniques which all have FID scores between 24 and 26 and setting a strong baseline for future research in colorization should be considered a positive of the paper and not a negative.\n\nWe believe that our latest version of the draft plus our response clarifies some of the concerns raised."}, "signatures": ["ICLR.cc/2021/Conference/Paper3388/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3388/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Colorization Transformer", "authorids": ["~Manoj_Kumar1", "~Dirk_Weissenborn1", "~Nal_Kalchbrenner1"], "authors": ["Manoj Kumar", "Dirk Weissenborn", "Nal Kalchbrenner"], "keywords": [], "abstract": "We present the Colorization Transformer, a novel approach for diverse high fidelity image colorization based on self-attention. Given a grayscale image, the colorization proceeds in three steps. We first use a conditional autoregressive transformer to produce a low resolution coarse coloring of the grayscale image. Our architecture adopts conditional transformer layers to effectively condition grayscale input. Two subsequent fully parallel networks upsample the coarse colored low resolution image into a finely colored high resolution image. Sampling from the Colorization Transformer produces diverse colorings whose fidelity outperforms the previous state-of-the-art on colorising ImageNet based on FID results and based on a human evaluation in a Mechanical Turk test. Remarkably, in  more than 60\\% of cases human evaluators prefer the highest rated among three generated colorings over the ground truth. The code and pre-trained checkpoints for Colorization Transformer are publicly available  at https://github.com/google-research/google-research/tree/master/coltran", "one-sentence_summary": "Self-attention for colorization", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kumar|colorization_transformer", "pdf": "/pdf/f2f5d9057587995de8d113d1ba35dd7d8b98f48e.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkumar2021colorization,\ntitle={Colorization Transformer},\nauthor={Manoj Kumar and Dirk Weissenborn and Nal Kalchbrenner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5NA1PinlGFu}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "5NA1PinlGFu", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3388/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3388/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3388/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3388/Authors|ICLR.cc/2021/Conference/Paper3388/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3388/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923838088, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3388/-/Official_Comment"}}}, {"id": "WDPGI8V0y5", "original": null, "number": 7, "cdate": 1605707756523, "ddate": null, "tcdate": 1605707756523, "tmdate": 1605707756523, "tddate": null, "forum": "5NA1PinlGFu", "replyto": "IUQ6RLQFgRR", "invitation": "ICLR.cc/2021/Conference/Paper3388/-/Official_Comment", "content": {"title": "Review Response: AnonReviewer 2 ", "comment": "We thank you for your reviews. Please find attached our response.\n\n**Effect of the auxiliary parallel model**\nWe added a bit that this helps to capture global structure in Section 4.2. We perform a detailed empirical analysis on the effect of this model in **Section 5.3**\n\n**Upsamplers**\nWe would like to clarify that our upsampling is done by parallel self-attention based models and not autoregressive. However, it is true that upsampling / refinement can be potentially done by convolutional architectures. However:\n* The spatial refinement network in PixColor uses 28M parameter whereas our spatial upsampler uses just 13M parameters. We require deeper convolutional networks to perform upsampling.\n* From a practical perspective, it makes our architecture a bit more complicated. Currently our architecture is conceptually simple and employs only axial attention blocks with optional / conditioning + masking. In future, we could explore combining convolutions + attention in different parts of the network to improve colorization performance.\n\n**Other**\n* Yes, that is true, we used the same learning rate for all models. The spatial upsampler receives gradients (albeit correlated) from 256x256 pixels, as compared to the colorizer and the color upsampler, which might explain why a smaller batch-size for the spatial upsampler is sufficient. We did not tune hyperparameters extensively. Once we found an architecture that colorizes low resolution images coarsely, we used the same training and architecture setup for the color and spatial upsampler. It may be possible to improve our results with an extensive hyperparameter sweep. \n* We train our colorizer for 450K steps, the color upsampler for 300K steps and spatial upsampler for 150K steps. In general, we found longer training to improve performance for the colorizer and not so much for the upsamplers. We added this to the training subsection in **Section 5.1**\n* Applying a cosine learning rate schedule requires 2 hyperparameters to tune whereas we apply polyak averaging with a value of 0.999, In future, we can experiment with different learning rate schedules.\n* ColTran core can sample 64x64 grayscale images in 4-5 minutes P100 GPU vs PixColor that takes ~10 minutes to colorize 28x28 grayscale images on a K40 GPU. Sampling 28x28 colorizations takes just around 30 seconds. The upsampler networks take in the order of milliseconds. We added this analysis to **Appendix H**.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3388/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3388/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Colorization Transformer", "authorids": ["~Manoj_Kumar1", "~Dirk_Weissenborn1", "~Nal_Kalchbrenner1"], "authors": ["Manoj Kumar", "Dirk Weissenborn", "Nal Kalchbrenner"], "keywords": [], "abstract": "We present the Colorization Transformer, a novel approach for diverse high fidelity image colorization based on self-attention. Given a grayscale image, the colorization proceeds in three steps. We first use a conditional autoregressive transformer to produce a low resolution coarse coloring of the grayscale image. Our architecture adopts conditional transformer layers to effectively condition grayscale input. Two subsequent fully parallel networks upsample the coarse colored low resolution image into a finely colored high resolution image. Sampling from the Colorization Transformer produces diverse colorings whose fidelity outperforms the previous state-of-the-art on colorising ImageNet based on FID results and based on a human evaluation in a Mechanical Turk test. Remarkably, in  more than 60\\% of cases human evaluators prefer the highest rated among three generated colorings over the ground truth. The code and pre-trained checkpoints for Colorization Transformer are publicly available  at https://github.com/google-research/google-research/tree/master/coltran", "one-sentence_summary": "Self-attention for colorization", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kumar|colorization_transformer", "pdf": "/pdf/f2f5d9057587995de8d113d1ba35dd7d8b98f48e.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkumar2021colorization,\ntitle={Colorization Transformer},\nauthor={Manoj Kumar and Dirk Weissenborn and Nal Kalchbrenner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5NA1PinlGFu}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "5NA1PinlGFu", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3388/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3388/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3388/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3388/Authors|ICLR.cc/2021/Conference/Paper3388/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3388/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923838088, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3388/-/Official_Comment"}}}, {"id": "ZCar2M-smP", "original": null, "number": 5, "cdate": 1605706708622, "ddate": null, "tcdate": 1605706708622, "tmdate": 1605706708622, "tddate": null, "forum": "5NA1PinlGFu", "replyto": "lSNRY4-0hgd", "invitation": "ICLR.cc/2021/Conference/Paper3388/-/Official_Comment", "content": {"title": "Review Response: AnonReviewer 3 [1/2]", "comment": "We thank you for your reviews. Please find attached our response. We believe the latest version of the draft improves clarity, highlights the contributions of our work and motivates the architecture better.\n\n**Technical contributions**\nWe have added a summarized version of the following points to the end of the **Introduction (Section 1)** highlighting the technical contributions of our paper.\n* First application of transformers for high-resolution ($256 \\times 256$) image colorization. Axial transformers which we base our technique upon were initially applied to model only low resolution $64 \\times 64$ images. Other related techniques that rely exclusively on self-attention to model images such as the Sparse Transformer [Child et. al 2019] and Image Transformer [Parmar et. al 2019] limit to resolutions of 64x64 and below. Scaling transformers to the task of coloring $256 \\times 256$ grayscale images or equivalently modeling $\\sim$ 200K symbols is a challenging task and our paper accomplishes this task quite successfully.\n* While Axial Transformers support conditioning by biasing the input, we find that **directly conditioning the transformer layers** can improve results significantly. We introduce conditional transformer layers for low-resolution coarse colorization in **Section 4.1**. The conditional layers incorporate conditioning information via multiple learnable components that are applied per-pixel and per-channel. We validate the contribution of each component with extensive experimentation and ablation studies that were appreciated by multiple reviewers. Our experiments can provide insight on how to effectively condition spatial information in a transformer for related tasks such as image editing and restoration.\n* We propose training an **auxiliary parallel prediction model** jointly with the low resolution coarse colorization model in **Section 4.2**. Improved FID scores demonstrate the usefulness of this auxiliary model.\n\n**Motivation**\nWe agree that the motivation was not clearly described in the first draft of our paper. Here we describe the motivation behind different components of our architecture. We added a summarized version discussing this towards the end of the **third paragraph in Section 1**\n*  **Motivation for axial self-attention blocks**  -  The main advantages of axial self-attention blocks are the ability to capture a global receptive field with only two layers and $\\mathcal{O}(D \\sqrt{D})$ instead of $\\mathcal{O}(D^2)$ complexity. They can be implemented efficiently using matrix-multiplications on modern accelerators such as TPUs. \n* **Motivation for using three sub-networks**: Generating high-resolution images using only self-attention is computationally challenging and hence prior work on unconditional image generation using self-attention limits to generating small images $64 \\times 64$. To alleviate the inherent complexity in colorizing high-resolution grayscale images, we decompose the task into three simpler sequential subtasks: coarse low resolution colorization, color super-resolution and spatial super-resolution and use a separate network for each. This enables us to train larger models for colorization. \n* **Motivation for the choice of Axial Transformer** - Axial Transformer is state-of-the-art in unconditional image generation benchmarks (ImageNet 32, ImageNet 64) at the time of submission without the usage of custom GPU kernels. The Axial Transformer has a semi-parallel sampling mechanism which enables us to colorize 64x64 grayscale images in 4-5 minutes P100 GPU vs PixColor that takes ~10 minutes to colorize 28x28 grayscale images on a K40 GPU. Sampling 28x28 colorizations takes just around 30 seconds.\n* **Motivation for conditional transformer layers** - Conditioning every layer via multiple components allows stronger gradient signals through the encoder and as an effect the encoder can learn better contextual representations. This improves our results compared to a baseline Axial Tranformer both in Table 2 and Figure 3.\n\n**Clarity**\nIn the latest version of the draft, we have restructured the writing to improve readability.\n* We added citations to Table 2. We provide a short description of what generative model each baseline is based upon in the Related Work section.\n* We expanded the caption in Figure 2. We expanded the \"Axial Transformer\" subsection and now explain every component of the figure (Outer Decoder, Inner Decoder, Encoder) in detail with equations. Our modifications made to the architecture are now described in Section 4. \n* The ground-truth coarse low resolution image is both the input to the decoder and the target during training. Masked layers ensure that the conditional distribution over each pixel depends solely on information from previous ground-truth pixels. \n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3388/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3388/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Colorization Transformer", "authorids": ["~Manoj_Kumar1", "~Dirk_Weissenborn1", "~Nal_Kalchbrenner1"], "authors": ["Manoj Kumar", "Dirk Weissenborn", "Nal Kalchbrenner"], "keywords": [], "abstract": "We present the Colorization Transformer, a novel approach for diverse high fidelity image colorization based on self-attention. Given a grayscale image, the colorization proceeds in three steps. We first use a conditional autoregressive transformer to produce a low resolution coarse coloring of the grayscale image. Our architecture adopts conditional transformer layers to effectively condition grayscale input. Two subsequent fully parallel networks upsample the coarse colored low resolution image into a finely colored high resolution image. Sampling from the Colorization Transformer produces diverse colorings whose fidelity outperforms the previous state-of-the-art on colorising ImageNet based on FID results and based on a human evaluation in a Mechanical Turk test. Remarkably, in  more than 60\\% of cases human evaluators prefer the highest rated among three generated colorings over the ground truth. The code and pre-trained checkpoints for Colorization Transformer are publicly available  at https://github.com/google-research/google-research/tree/master/coltran", "one-sentence_summary": "Self-attention for colorization", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kumar|colorization_transformer", "pdf": "/pdf/f2f5d9057587995de8d113d1ba35dd7d8b98f48e.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkumar2021colorization,\ntitle={Colorization Transformer},\nauthor={Manoj Kumar and Dirk Weissenborn and Nal Kalchbrenner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5NA1PinlGFu}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "5NA1PinlGFu", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3388/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3388/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3388/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3388/Authors|ICLR.cc/2021/Conference/Paper3388/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3388/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923838088, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3388/-/Official_Comment"}}}], "count": 15}