{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488570166242, "tcdate": 1478287448570, "number": 352, "id": "SyWvgP5el", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "SyWvgP5el", "signatures": ["~Aravind_Rajeswaran1"], "readers": ["everyone"], "content": {"title": "EPOpt: Learning Robust Neural Network Policies Using Model Ensembles", "abstract": "Sample complexity and safety are major challenges when learning policies with reinforcement learning for real-world tasks, especially when the policies are represented using rich function approximators like deep neural networks. Model-based methods where the real-world target domain is approximated using a simulated source domain provide an avenue to tackle the above challenges by augmenting real data with simulated data. However, discrepancies between the simulated source domain and the target domain pose a challenge for simulated training. We introduce the EPOpt algorithm, which uses an ensemble of simulated source domains and a form of adversarial training to learn policies that are robust and generalize to a broad range of possible target domains, including to unmodeled effects. Further, the probability distribution over source domains in the ensemble can be adapted using data from the target domain and approximate Bayesian methods, to progressively make it a better approximation. Thus, learning on a model ensemble, along with source domain adaptation, provides the benefit of both robustness and learning.", "pdf": "/pdf/96d9f09439812742c506b34c1a236e31029ed816.pdf", "TL;DR": "An ensemble optimization approach to help transfer neural network policies from simulated domains to real-world target domains.", "paperhash": "rajeswaran|epopt_learning_robust_neural_network_policies_using_model_ensembles", "keywords": ["Reinforcement Learning", "Applications"], "conflicts": ["washington.edu", "berkeley.edu", "iitm.ac.in"], "authors": ["Aravind Rajeswaran", "Sarvjeet Ghotra", "Balaraman Ravindran", "Sergey Levine"], "authorids": ["aravraj@cs.washington.edu", "sarvjeet.13it236@nitk.edu.in", "ravi@cse.iitm.ac.in", "svlevine@eecs.berkeley.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396533188, "tcdate": 1486396533188, "number": 1, "id": "r1T_2G8ue", "invitation": "ICLR.cc/2017/conference/-/paper352/acceptance", "forum": "SyWvgP5el", "replyto": "SyWvgP5el", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The approach here looks at learning policies that are robust over a parameterized class of MDPs (in the sense the probability that the policy doesn't perform well is small over this class. The idea is fairly straightforward, but the algorithm seems novel and the results show that the approach does seem to provide substantial benefit. The reviewers were all in agreement that the paper is worth accepting.\n \n Pros:\n + Nice application of robust (really stochastic, since these are chance constraints) optimization to policy search\n + Compelling demonstration of the improved range of good performance over methods like vanilla TRPO\n \n Cons:\n - The question of how to parameterize a class of MDPs for real-world scenarios is still somewhat unclear\n - The description of the method as optimizing CVaR seems incorrect, since they appear to be using an actual chance constraint, whereas CVaR is essentially a convex relaxation ... this may be related to the work in (Tamar, 2015), but needs to be better explained if so.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EPOpt: Learning Robust Neural Network Policies Using Model Ensembles", "abstract": "Sample complexity and safety are major challenges when learning policies with reinforcement learning for real-world tasks, especially when the policies are represented using rich function approximators like deep neural networks. Model-based methods where the real-world target domain is approximated using a simulated source domain provide an avenue to tackle the above challenges by augmenting real data with simulated data. However, discrepancies between the simulated source domain and the target domain pose a challenge for simulated training. We introduce the EPOpt algorithm, which uses an ensemble of simulated source domains and a form of adversarial training to learn policies that are robust and generalize to a broad range of possible target domains, including to unmodeled effects. Further, the probability distribution over source domains in the ensemble can be adapted using data from the target domain and approximate Bayesian methods, to progressively make it a better approximation. Thus, learning on a model ensemble, along with source domain adaptation, provides the benefit of both robustness and learning.", "pdf": "/pdf/96d9f09439812742c506b34c1a236e31029ed816.pdf", "TL;DR": "An ensemble optimization approach to help transfer neural network policies from simulated domains to real-world target domains.", "paperhash": "rajeswaran|epopt_learning_robust_neural_network_policies_using_model_ensembles", "keywords": ["Reinforcement Learning", "Applications"], "conflicts": ["washington.edu", "berkeley.edu", "iitm.ac.in"], "authors": ["Aravind Rajeswaran", "Sarvjeet Ghotra", "Balaraman Ravindran", "Sergey Levine"], "authorids": ["aravraj@cs.washington.edu", "sarvjeet.13it236@nitk.edu.in", "ravi@cse.iitm.ac.in", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396533730, "id": "ICLR.cc/2017/conference/-/paper352/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "SyWvgP5el", "replyto": "SyWvgP5el", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396533730}}}, {"tddate": null, "tmdate": 1482415930572, "tcdate": 1482415877676, "number": 3, "id": "r1RW1DKEe", "invitation": "ICLR.cc/2017/conference/-/paper352/public/comment", "forum": "SyWvgP5el", "replyto": "r1tEPyWEg", "signatures": ["~Aravind_Rajeswaran1"], "readers": ["everyone"], "writers": ["~Aravind_Rajeswaran1"], "content": {"title": "incorporated your suggestions", "comment": "Thank you for reviewing our paper and for the valuable feedback.\n\nFollowing your suggestions, we have added details about the need for a baseline (appendix A.6) and results when using the REINFORCE algorithm for BatchPolOpt step (appendix A.7). Our experiments suggest that a baseline is important for policy optimization to work in practice, especially for the CVaR case."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EPOpt: Learning Robust Neural Network Policies Using Model Ensembles", "abstract": "Sample complexity and safety are major challenges when learning policies with reinforcement learning for real-world tasks, especially when the policies are represented using rich function approximators like deep neural networks. Model-based methods where the real-world target domain is approximated using a simulated source domain provide an avenue to tackle the above challenges by augmenting real data with simulated data. However, discrepancies between the simulated source domain and the target domain pose a challenge for simulated training. We introduce the EPOpt algorithm, which uses an ensemble of simulated source domains and a form of adversarial training to learn policies that are robust and generalize to a broad range of possible target domains, including to unmodeled effects. Further, the probability distribution over source domains in the ensemble can be adapted using data from the target domain and approximate Bayesian methods, to progressively make it a better approximation. Thus, learning on a model ensemble, along with source domain adaptation, provides the benefit of both robustness and learning.", "pdf": "/pdf/96d9f09439812742c506b34c1a236e31029ed816.pdf", "TL;DR": "An ensemble optimization approach to help transfer neural network policies from simulated domains to real-world target domains.", "paperhash": "rajeswaran|epopt_learning_robust_neural_network_policies_using_model_ensembles", "keywords": ["Reinforcement Learning", "Applications"], "conflicts": ["washington.edu", "berkeley.edu", "iitm.ac.in"], "authors": ["Aravind Rajeswaran", "Sarvjeet Ghotra", "Balaraman Ravindran", "Sergey Levine"], "authorids": ["aravraj@cs.washington.edu", "sarvjeet.13it236@nitk.edu.in", "ravi@cse.iitm.ac.in", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287612488, "id": "ICLR.cc/2017/conference/-/paper352/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SyWvgP5el", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper352/reviewers", "ICLR.cc/2017/conference/paper352/areachairs"], "cdate": 1485287612488}}}, {"tddate": null, "tmdate": 1482252704254, "tcdate": 1482171454648, "number": 3, "id": "BkwBVorNl", "invitation": "ICLR.cc/2017/conference/-/paper352/official/review", "forum": "SyWvgP5el", "replyto": "SyWvgP5el", "signatures": ["ICLR.cc/2017/conference/paper352/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper352/AnonReviewer3"], "content": {"title": "Ensemble training and transfer, a good submission", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper explores ensemble optimisation in the context of policy-gradient training. Ensemble training has been a low-hanging fruit for many years in the this space and this paper finally touches on this interesting subject. The paper is well written and accessible. In particular the questions posed in section 4 are well posed and interesting.\n\nThat said the paper does have some very weak points, most obviously that all of its results are for a very particular choice of domain+parameters. I eagerly look forward to the journal version where these experiments are repeated for all sorts of source domain/target domain/parameter combinations.\n\n<rant \nFinally a stylistic comment that the authors can feel free to ignore. I don't like the trend of every paper coming up with a new acronymy wEiRDLY cAsEd name. Especially here when the idea is so simple. Why not use words? English words from the dictionary. Instead of \"EPOpt\" and \"EPOpt-e\", you can write \"ensemble training\" and \"robust ensemble training\". Is that not clearer?\n/>", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EPOpt: Learning Robust Neural Network Policies Using Model Ensembles", "abstract": "Sample complexity and safety are major challenges when learning policies with reinforcement learning for real-world tasks, especially when the policies are represented using rich function approximators like deep neural networks. Model-based methods where the real-world target domain is approximated using a simulated source domain provide an avenue to tackle the above challenges by augmenting real data with simulated data. However, discrepancies between the simulated source domain and the target domain pose a challenge for simulated training. We introduce the EPOpt algorithm, which uses an ensemble of simulated source domains and a form of adversarial training to learn policies that are robust and generalize to a broad range of possible target domains, including to unmodeled effects. Further, the probability distribution over source domains in the ensemble can be adapted using data from the target domain and approximate Bayesian methods, to progressively make it a better approximation. Thus, learning on a model ensemble, along with source domain adaptation, provides the benefit of both robustness and learning.", "pdf": "/pdf/96d9f09439812742c506b34c1a236e31029ed816.pdf", "TL;DR": "An ensemble optimization approach to help transfer neural network policies from simulated domains to real-world target domains.", "paperhash": "rajeswaran|epopt_learning_robust_neural_network_policies_using_model_ensembles", "keywords": ["Reinforcement Learning", "Applications"], "conflicts": ["washington.edu", "berkeley.edu", "iitm.ac.in"], "authors": ["Aravind Rajeswaran", "Sarvjeet Ghotra", "Balaraman Ravindran", "Sergey Levine"], "authorids": ["aravraj@cs.washington.edu", "sarvjeet.13it236@nitk.edu.in", "ravi@cse.iitm.ac.in", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512615535, "id": "ICLR.cc/2017/conference/-/paper352/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper352/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper352/AnonReviewer1", "ICLR.cc/2017/conference/paper352/AnonReviewer2", "ICLR.cc/2017/conference/paper352/AnonReviewer3"], "reply": {"forum": "SyWvgP5el", "replyto": "SyWvgP5el", "writers": {"values-regex": "ICLR.cc/2017/conference/paper352/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper352/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512615535}}}, {"tddate": null, "tmdate": 1481921182620, "tcdate": 1481921182620, "number": 2, "id": "BJwiMAWVe", "invitation": "ICLR.cc/2017/conference/-/paper352/official/review", "forum": "SyWvgP5el", "replyto": "SyWvgP5el", "signatures": ["ICLR.cc/2017/conference/paper352/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper352/AnonReviewer2"], "content": {"title": "ICLR 2017 conference review", "rating": "7: Good paper, accept", "review": "Paper addresses systematic discrepancies between simulated and real-world policy control domains. Proposed method contains two ideas: 1) training on an ensemble of models in an adversarial fashion to learn policies that are robust to errors and 2) adaptation of the source domain ensemble using data from a (real-world) target domain. \n\n> Significance\n\nPaper addresses and important and significant problem. The approach taken in addressing it is also interesting \n\n> Clarity\n\nPaper is well written, but does require domain knowledge to understand. \n\nMy main concerns were well addressed by the rebuttal and corresponding revisions to the paper. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EPOpt: Learning Robust Neural Network Policies Using Model Ensembles", "abstract": "Sample complexity and safety are major challenges when learning policies with reinforcement learning for real-world tasks, especially when the policies are represented using rich function approximators like deep neural networks. Model-based methods where the real-world target domain is approximated using a simulated source domain provide an avenue to tackle the above challenges by augmenting real data with simulated data. However, discrepancies between the simulated source domain and the target domain pose a challenge for simulated training. We introduce the EPOpt algorithm, which uses an ensemble of simulated source domains and a form of adversarial training to learn policies that are robust and generalize to a broad range of possible target domains, including to unmodeled effects. Further, the probability distribution over source domains in the ensemble can be adapted using data from the target domain and approximate Bayesian methods, to progressively make it a better approximation. Thus, learning on a model ensemble, along with source domain adaptation, provides the benefit of both robustness and learning.", "pdf": "/pdf/96d9f09439812742c506b34c1a236e31029ed816.pdf", "TL;DR": "An ensemble optimization approach to help transfer neural network policies from simulated domains to real-world target domains.", "paperhash": "rajeswaran|epopt_learning_robust_neural_network_policies_using_model_ensembles", "keywords": ["Reinforcement Learning", "Applications"], "conflicts": ["washington.edu", "berkeley.edu", "iitm.ac.in"], "authors": ["Aravind Rajeswaran", "Sarvjeet Ghotra", "Balaraman Ravindran", "Sergey Levine"], "authorids": ["aravraj@cs.washington.edu", "sarvjeet.13it236@nitk.edu.in", "ravi@cse.iitm.ac.in", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512615535, "id": "ICLR.cc/2017/conference/-/paper352/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper352/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper352/AnonReviewer1", "ICLR.cc/2017/conference/paper352/AnonReviewer2", "ICLR.cc/2017/conference/paper352/AnonReviewer3"], "reply": {"forum": "SyWvgP5el", "replyto": "SyWvgP5el", "writers": {"values-regex": "ICLR.cc/2017/conference/paper352/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper352/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512615535}}}, {"tddate": null, "tmdate": 1481860913410, "tcdate": 1481860913410, "number": 1, "id": "r1tEPyWEg", "invitation": "ICLR.cc/2017/conference/-/paper352/official/review", "forum": "SyWvgP5el", "replyto": "SyWvgP5el", "signatures": ["ICLR.cc/2017/conference/paper352/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper352/AnonReviewer1"], "content": {"title": "Review", "rating": "7: Good paper, accept", "review": "The paper looks at the problem of transferring a policy learned in a simulator to  a target real-world system.  The proposed approach considers using an ensemble of simulated source domains, along with adversarial training, to learn a robust policy that is able to generalize to several target domains.\n\nOverall, the paper tackles an interesting problem, and provides a reasonable solution.  The notion of adversarial training used here does not seem the same as other recent literature (e.g. on GANs).  It would be useful to add more details on a few components, as discussed in the question/response round.  I also encourage including the results with alternative policy gradient subroutines, even if they don\u2019t perform well (e.g. Reinforce), as well as results with and without the baseline on the value function. Such results are very useful to other researchers.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EPOpt: Learning Robust Neural Network Policies Using Model Ensembles", "abstract": "Sample complexity and safety are major challenges when learning policies with reinforcement learning for real-world tasks, especially when the policies are represented using rich function approximators like deep neural networks. Model-based methods where the real-world target domain is approximated using a simulated source domain provide an avenue to tackle the above challenges by augmenting real data with simulated data. However, discrepancies between the simulated source domain and the target domain pose a challenge for simulated training. We introduce the EPOpt algorithm, which uses an ensemble of simulated source domains and a form of adversarial training to learn policies that are robust and generalize to a broad range of possible target domains, including to unmodeled effects. Further, the probability distribution over source domains in the ensemble can be adapted using data from the target domain and approximate Bayesian methods, to progressively make it a better approximation. Thus, learning on a model ensemble, along with source domain adaptation, provides the benefit of both robustness and learning.", "pdf": "/pdf/96d9f09439812742c506b34c1a236e31029ed816.pdf", "TL;DR": "An ensemble optimization approach to help transfer neural network policies from simulated domains to real-world target domains.", "paperhash": "rajeswaran|epopt_learning_robust_neural_network_policies_using_model_ensembles", "keywords": ["Reinforcement Learning", "Applications"], "conflicts": ["washington.edu", "berkeley.edu", "iitm.ac.in"], "authors": ["Aravind Rajeswaran", "Sarvjeet Ghotra", "Balaraman Ravindran", "Sergey Levine"], "authorids": ["aravraj@cs.washington.edu", "sarvjeet.13it236@nitk.edu.in", "ravi@cse.iitm.ac.in", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512615535, "id": "ICLR.cc/2017/conference/-/paper352/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper352/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper352/AnonReviewer1", "ICLR.cc/2017/conference/paper352/AnonReviewer2", "ICLR.cc/2017/conference/paper352/AnonReviewer3"], "reply": {"forum": "SyWvgP5el", "replyto": "SyWvgP5el", "writers": {"values-regex": "ICLR.cc/2017/conference/paper352/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper352/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512615535}}}, {"tddate": null, "tmdate": 1481668885247, "tcdate": 1481668885241, "number": 2, "id": "Hk6fFxA7g", "invitation": "ICLR.cc/2017/conference/-/paper352/public/comment", "forum": "SyWvgP5el", "replyto": "SJKBzagXe", "signatures": ["~Aravind_Rajeswaran1"], "readers": ["everyone"], "writers": ["~Aravind_Rajeswaran1"], "content": {"title": "added results for different epsilon settings and discussed relationship to Wang et al. '10", "comment": "Thank you for reviewing our paper and for the useful pointer.\n\n\nQ1: We have added a citation to Wang et al. in the paper and briefly discussed the connections. We elaborate on the differences below:\n-- Wang et al. use a hand-engineered policy class similar to SIMBICON and optimize a small set of parameters using CMA-ES. EPOpt is a general policy gradient meta-algorithm for directly optimizing expressive policies.\n-- Wang et al. observed cautious policies with degraded performance, whereas we do not observe degradation in performance due to acquiring a robust policy. This is likely due to our use of a more expressive function approximator to represent the policy. We believe that this is an interesting empirical result that will be of interest to the deep RL community.\n-- Our end goal is to transfer to an unknown MDP whereas Wang et al. seek to optimize average performance on the ensemble. \n-- We show effectiveness of model adaptation to enable transfer in the absence of domain knowledge to pick informative source distributions. Wang et al. do not study adaptation.\n-- Our experiments show that the sub-sampling step (epsilon<1) we employ produces significantly more robust policies.\n-- We analyze transfer with unmodeled effects, which is important for transferring policies from simulations to real world.\n\n\nQ2: We have added results for different epsilon settings to the paper (appendix A.5). We have also added details about optimizing policies when using epsilon<1 to Section 3.1. There wasn\u2019t a marked difference between different epsilon settings when only one physical parameter was varied (Section 4.1). However, the difference was clearly visible when multiple parameters were varied (Section 4.2). We observed that as epsilon is decreased, the variance in performance decreases, with a small decrease in average performance."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EPOpt: Learning Robust Neural Network Policies Using Model Ensembles", "abstract": "Sample complexity and safety are major challenges when learning policies with reinforcement learning for real-world tasks, especially when the policies are represented using rich function approximators like deep neural networks. Model-based methods where the real-world target domain is approximated using a simulated source domain provide an avenue to tackle the above challenges by augmenting real data with simulated data. However, discrepancies between the simulated source domain and the target domain pose a challenge for simulated training. We introduce the EPOpt algorithm, which uses an ensemble of simulated source domains and a form of adversarial training to learn policies that are robust and generalize to a broad range of possible target domains, including to unmodeled effects. Further, the probability distribution over source domains in the ensemble can be adapted using data from the target domain and approximate Bayesian methods, to progressively make it a better approximation. Thus, learning on a model ensemble, along with source domain adaptation, provides the benefit of both robustness and learning.", "pdf": "/pdf/96d9f09439812742c506b34c1a236e31029ed816.pdf", "TL;DR": "An ensemble optimization approach to help transfer neural network policies from simulated domains to real-world target domains.", "paperhash": "rajeswaran|epopt_learning_robust_neural_network_policies_using_model_ensembles", "keywords": ["Reinforcement Learning", "Applications"], "conflicts": ["washington.edu", "berkeley.edu", "iitm.ac.in"], "authors": ["Aravind Rajeswaran", "Sarvjeet Ghotra", "Balaraman Ravindran", "Sergey Levine"], "authorids": ["aravraj@cs.washington.edu", "sarvjeet.13it236@nitk.edu.in", "ravi@cse.iitm.ac.in", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287612488, "id": "ICLR.cc/2017/conference/-/paper352/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SyWvgP5el", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper352/reviewers", "ICLR.cc/2017/conference/paper352/areachairs"], "cdate": 1485287612488}}}, {"tddate": null, "tmdate": 1481668506905, "tcdate": 1481668506896, "number": 1, "id": "ry7sDeAme", "invitation": "ICLR.cc/2017/conference/-/paper352/public/comment", "forum": "SyWvgP5el", "replyto": "SyHzcqJ7l", "signatures": ["~Aravind_Rajeswaran1"], "readers": ["everyone"], "writers": ["~Aravind_Rajeswaran1"], "content": {"title": "answers", "comment": "Thank you for reviewing our paper and for the interesting questions!\n\nWe use the same parametrization for the baseline as used in Duan et al. \u201816 and we have edited Section 3.1 to emphasize this. The parameters of the baseline are estimated using only the subset of trajectories with returns less than epsilon percentile of the return distribution.\n\nBatchPolOpt simply calls one step of an underlying policy gradient subroutine, which in our case was TRPO. We have edited Section 3.1 to clarify this. We emphasize that our method is not specific to TRPO and can be used with any batch policy optimization subroutine. We chose TRPO primarily because there are readily available open-source implementations, and the algorithm has been shown to work on a range of problems. The particular choice of policy gradient method is largely orthogonal to the main contribution of this paper.\nFollowing your comments, we ran experiments using REINFORCE with the same baseline parametrization as before. Our results indicate that REINFORCE is unable to produce robust policies, and has difficulty in finding a hopping gait even for a single MDP (i.e. without an ensemble). This mirrors the findings in Duan et al \u201816.\n\nWe have added a discussion of sample complexity to appendix A.2. The results in Figure 1 used 150 iterations with 240 trajectory samples per iteration, which is comparable to Schulman et al. \u201815 and Duan et al. \u201816. We tried a few different settings for epsilon, and finally chose 0.1 since it produced satisfactory results both qualitatively and quantitatively. We have summarized results for different epsilon settings in appendix A.5.\n\nThough Mordatch et al. \u201815 also use an ensemble of models, they do not train parametrized policies. Instead, they optimize individual open loop trajectories in simulation and use PD control to track those trajectories. While this is related to our approach in that ensembles are used, it is not a policy search method, and therefore cannot be directly compared to our method in regard to its ability to find robust policies.\nOur understanding is that Bayesian GP-TD is primarily used for policy evaluation [4,5], and hence is not directly connected to the main contribution of our paper. To our knowledge, GP based methods have not been applied successfully to high dimensional continuous locomotion tasks like those we study in this paper. \nWe note here that we show comparisons to reasonable alternatives like training a policy on a single maximum likelihood model (which is the most common practice in model-based control), and training a policy which optimizes for the average return over a model distribution (EPOpt with epsilon=1).\n\nEPOpt considers a distribution over models to learn the robust policy directly. Boosting on the other hand combines a set of weak learners (e.g instance level policies) to produce a strong learner (robust policy).\n\n[1] Duan et al., Benchmarking Deep Reinforcement Learning for Continuous Control, ICML 2016.\n[2] Schulman et al., Trust Region Policy Optimization, ICML 2015.\n[3] Mordatch et al., Ensemble-CIO: Full-body dynamic motion planning that transfers to physical humanoids, IROS 2015.\n[4] Engel et al., Reinforcement learning with Gaussian processes, ICML 2005.\n[5] Engel et al., GPTD Tutorial slides, ICML 2007."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EPOpt: Learning Robust Neural Network Policies Using Model Ensembles", "abstract": "Sample complexity and safety are major challenges when learning policies with reinforcement learning for real-world tasks, especially when the policies are represented using rich function approximators like deep neural networks. Model-based methods where the real-world target domain is approximated using a simulated source domain provide an avenue to tackle the above challenges by augmenting real data with simulated data. However, discrepancies between the simulated source domain and the target domain pose a challenge for simulated training. We introduce the EPOpt algorithm, which uses an ensemble of simulated source domains and a form of adversarial training to learn policies that are robust and generalize to a broad range of possible target domains, including to unmodeled effects. Further, the probability distribution over source domains in the ensemble can be adapted using data from the target domain and approximate Bayesian methods, to progressively make it a better approximation. Thus, learning on a model ensemble, along with source domain adaptation, provides the benefit of both robustness and learning.", "pdf": "/pdf/96d9f09439812742c506b34c1a236e31029ed816.pdf", "TL;DR": "An ensemble optimization approach to help transfer neural network policies from simulated domains to real-world target domains.", "paperhash": "rajeswaran|epopt_learning_robust_neural_network_policies_using_model_ensembles", "keywords": ["Reinforcement Learning", "Applications"], "conflicts": ["washington.edu", "berkeley.edu", "iitm.ac.in"], "authors": ["Aravind Rajeswaran", "Sarvjeet Ghotra", "Balaraman Ravindran", "Sergey Levine"], "authorids": ["aravraj@cs.washington.edu", "sarvjeet.13it236@nitk.edu.in", "ravi@cse.iitm.ac.in", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287612488, "id": "ICLR.cc/2017/conference/-/paper352/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SyWvgP5el", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper352/reviewers", "ICLR.cc/2017/conference/paper352/areachairs"], "cdate": 1485287612488}}}, {"tddate": null, "tmdate": 1480802880616, "tcdate": 1480802880611, "number": 2, "id": "SJKBzagXe", "invitation": "ICLR.cc/2017/conference/-/paper352/pre-review/question", "forum": "SyWvgP5el", "replyto": "SyWvgP5el", "signatures": ["ICLR.cc/2017/conference/paper352/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper352/AnonReviewer2"], "content": {"title": "Pre-review questions", "question": "1. The EPOpt algorithm in some abstract sense reminds me of:\n\nWang, J. M., Fleet, D. J., Hertzmann, A. Optimizing Walking Controllers for Uncertain Inputs and Environments. ACM Transactions on Graphics 29, 4 (Proceedings of SIGGRAPH 2010). \n\nCan you comment on the relationship? \n\n2. It would be interesting to see how the illustrated performance (particularly in Figure 1, right) changes as a function of \\epsilon, e.g., as epsilon is increased (or decreased) from 0.1 default. Has this been tried? Can the differences in performance be characterized?\n\n \n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EPOpt: Learning Robust Neural Network Policies Using Model Ensembles", "abstract": "Sample complexity and safety are major challenges when learning policies with reinforcement learning for real-world tasks, especially when the policies are represented using rich function approximators like deep neural networks. Model-based methods where the real-world target domain is approximated using a simulated source domain provide an avenue to tackle the above challenges by augmenting real data with simulated data. However, discrepancies between the simulated source domain and the target domain pose a challenge for simulated training. We introduce the EPOpt algorithm, which uses an ensemble of simulated source domains and a form of adversarial training to learn policies that are robust and generalize to a broad range of possible target domains, including to unmodeled effects. Further, the probability distribution over source domains in the ensemble can be adapted using data from the target domain and approximate Bayesian methods, to progressively make it a better approximation. Thus, learning on a model ensemble, along with source domain adaptation, provides the benefit of both robustness and learning.", "pdf": "/pdf/96d9f09439812742c506b34c1a236e31029ed816.pdf", "TL;DR": "An ensemble optimization approach to help transfer neural network policies from simulated domains to real-world target domains.", "paperhash": "rajeswaran|epopt_learning_robust_neural_network_policies_using_model_ensembles", "keywords": ["Reinforcement Learning", "Applications"], "conflicts": ["washington.edu", "berkeley.edu", "iitm.ac.in"], "authors": ["Aravind Rajeswaran", "Sarvjeet Ghotra", "Balaraman Ravindran", "Sergey Levine"], "authorids": ["aravraj@cs.washington.edu", "sarvjeet.13it236@nitk.edu.in", "ravi@cse.iitm.ac.in", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959328095, "id": "ICLR.cc/2017/conference/-/paper352/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper352/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper352/AnonReviewer1", "ICLR.cc/2017/conference/paper352/AnonReviewer2"], "reply": {"forum": "SyWvgP5el", "replyto": "SyWvgP5el", "writers": {"values-regex": "ICLR.cc/2017/conference/paper352/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper352/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959328095}}}, {"tddate": null, "tmdate": 1480727053128, "tcdate": 1480727053123, "number": 1, "id": "SyHzcqJ7l", "invitation": "ICLR.cc/2017/conference/-/paper352/pre-review/question", "forum": "SyWvgP5el", "replyto": "SyWvgP5el", "signatures": ["ICLR.cc/2017/conference/paper352/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper352/AnonReviewer1"], "content": {"title": "Questions", "question": "Can you add details about the baseline you use to stabilize the policy gradient estimate (bottom of p.3). Is this estimated from all trajectories, or model-specific?\n\nCan you add code for the BatchPolOpt subroutine (Algorithm 1)? What are the pros/cons of using TRPO for this, compared to other methods?  How do the results change if you use a different policy optimization method, e.g Reinforcement w/baseline?\n\nHow many samples are used for the Fig.1 results?  How many samples are necessary to pick a good setting for epsilon?\n\nI would expect to see comparison to the bayesian GP-TD approach.  What do these results show?  A comparison to Mordatch et al. would also be informative. Currently, there are no comparison to any other method.\n\nWhat \n\nWhat is the relation between your approach and boosting?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EPOpt: Learning Robust Neural Network Policies Using Model Ensembles", "abstract": "Sample complexity and safety are major challenges when learning policies with reinforcement learning for real-world tasks, especially when the policies are represented using rich function approximators like deep neural networks. Model-based methods where the real-world target domain is approximated using a simulated source domain provide an avenue to tackle the above challenges by augmenting real data with simulated data. However, discrepancies between the simulated source domain and the target domain pose a challenge for simulated training. We introduce the EPOpt algorithm, which uses an ensemble of simulated source domains and a form of adversarial training to learn policies that are robust and generalize to a broad range of possible target domains, including to unmodeled effects. Further, the probability distribution over source domains in the ensemble can be adapted using data from the target domain and approximate Bayesian methods, to progressively make it a better approximation. Thus, learning on a model ensemble, along with source domain adaptation, provides the benefit of both robustness and learning.", "pdf": "/pdf/96d9f09439812742c506b34c1a236e31029ed816.pdf", "TL;DR": "An ensemble optimization approach to help transfer neural network policies from simulated domains to real-world target domains.", "paperhash": "rajeswaran|epopt_learning_robust_neural_network_policies_using_model_ensembles", "keywords": ["Reinforcement Learning", "Applications"], "conflicts": ["washington.edu", "berkeley.edu", "iitm.ac.in"], "authors": ["Aravind Rajeswaran", "Sarvjeet Ghotra", "Balaraman Ravindran", "Sergey Levine"], "authorids": ["aravraj@cs.washington.edu", "sarvjeet.13it236@nitk.edu.in", "ravi@cse.iitm.ac.in", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959328095, "id": "ICLR.cc/2017/conference/-/paper352/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper352/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper352/AnonReviewer1", "ICLR.cc/2017/conference/paper352/AnonReviewer2"], "reply": {"forum": "SyWvgP5el", "replyto": "SyWvgP5el", "writers": {"values-regex": "ICLR.cc/2017/conference/paper352/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper352/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959328095}}}], "count": 10}