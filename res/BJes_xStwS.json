{"notes": [{"id": "3ItgKii3ng", "original": null, "number": 1, "cdate": 1580507445419, "ddate": null, "tcdate": 1580507445419, "tmdate": 1580507445419, "tddate": null, "forum": "BJes_xStwS", "replyto": "HJx9csHQ5H", "invitation": "ICLR.cc/2020/Conference/Paper2411/-/Public_Comment", "content": {"title": "To Reviewer2: What are the state-of-the-art methods? Could you cite the paper here?", "comment": "Hi Reviewer2,\n\nI would like to ask what research papers, in your opinion, addressing the same problem (learning graphs at scale)? Could you name some papers with the state-of-the-art methods here? \n\nMany Thanks"}, "signatures": ["~Stacy_X_Pu1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Stacy_X_Pu1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yongyuw@mtu.edu", "qzzhao@mtu.edu", "zfeng12@stevens.edu"], "title": "GRASPEL: GRAPH SPECTRAL LEARNING AT SCALE", "authors": ["Yongyu Wang", "Zhiqiang Zhao", "Zhuo Feng"], "pdf": "/pdf/4851c66a2c2f68226e9906f01c1642354bd63a3f.pdf", "TL;DR": "A spectral approach to scalable graph learning from data", "abstract": "Learning meaningful graphs from data plays important roles in many data mining and machine learning tasks, such as data representation and analysis, dimension reduction, data clustering, and visualization, etc. In this work, we present a  scalable spectral approach to graph learning from data. By limiting the precision matrix to be a graph Laplacian, our approach aims to estimate ultra-sparse weighted graphs and has a clear connection with the prior graphical Lasso method. By interleaving nearly-linear time spectral graph sparsification,  coarsening and embedding procedures, ultra-sparse yet spectrally-stable graphs can be iteratively constructed in a highly-scalable manner. Compared with prior graph learning approaches that do not scale to large problems, our approach is highly-scalable for constructing graphs that can immediately lead to substantially improved computing efficiency and solution quality for a variety of data mining and machine learning applications, such as spectral clustering (SC), and t-Distributed Stochastic Neighbor Embedding (t-SNE).  ", "keywords": ["Spectral graph theory", "graph learning", "data clustering", "t-SNE visualization"], "paperhash": "wang|graspel_graph_spectral_learning_at_scale", "original_pdf": "/attachment/9e562544ab881365c2a8e56288e2da9176752777.pdf", "_bibtex": "@misc{\nwang2020graspel,\ntitle={{\\{}GRASPEL{\\}}: {\\{}GRAPH{\\}} {\\{}SPECTRAL{\\}} {\\{}LEARNING{\\}} {\\{}AT{\\}} {\\{}SCALE{\\}}},\nauthor={Yongyu Wang and Zhiqiang Zhao and Zhuo Feng},\nyear={2020},\nurl={https://openreview.net/forum?id=BJes_xStwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJes_xStwS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504180668, "tmdate": 1576860584340, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2411/Authors", "ICLR.cc/2020/Conference/Paper2411/Reviewers", "ICLR.cc/2020/Conference/Paper2411/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2411/-/Public_Comment"}}}, {"id": "BJes_xStwS", "original": "BJx8VpxFvH", "number": 2411, "cdate": 1569439859294, "ddate": null, "tcdate": 1569439859294, "tmdate": 1577168240217, "tddate": null, "forum": "BJes_xStwS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["yongyuw@mtu.edu", "qzzhao@mtu.edu", "zfeng12@stevens.edu"], "title": "GRASPEL: GRAPH SPECTRAL LEARNING AT SCALE", "authors": ["Yongyu Wang", "Zhiqiang Zhao", "Zhuo Feng"], "pdf": "/pdf/4851c66a2c2f68226e9906f01c1642354bd63a3f.pdf", "TL;DR": "A spectral approach to scalable graph learning from data", "abstract": "Learning meaningful graphs from data plays important roles in many data mining and machine learning tasks, such as data representation and analysis, dimension reduction, data clustering, and visualization, etc. In this work, we present a  scalable spectral approach to graph learning from data. By limiting the precision matrix to be a graph Laplacian, our approach aims to estimate ultra-sparse weighted graphs and has a clear connection with the prior graphical Lasso method. By interleaving nearly-linear time spectral graph sparsification,  coarsening and embedding procedures, ultra-sparse yet spectrally-stable graphs can be iteratively constructed in a highly-scalable manner. Compared with prior graph learning approaches that do not scale to large problems, our approach is highly-scalable for constructing graphs that can immediately lead to substantially improved computing efficiency and solution quality for a variety of data mining and machine learning applications, such as spectral clustering (SC), and t-Distributed Stochastic Neighbor Embedding (t-SNE).  ", "keywords": ["Spectral graph theory", "graph learning", "data clustering", "t-SNE visualization"], "paperhash": "wang|graspel_graph_spectral_learning_at_scale", "original_pdf": "/attachment/9e562544ab881365c2a8e56288e2da9176752777.pdf", "_bibtex": "@misc{\nwang2020graspel,\ntitle={{\\{}GRASPEL{\\}}: {\\{}GRAPH{\\}} {\\{}SPECTRAL{\\}} {\\{}LEARNING{\\}} {\\{}AT{\\}} {\\{}SCALE{\\}}},\nauthor={Yongyu Wang and Zhiqiang Zhao and Zhuo Feng},\nyear={2020},\nurl={https://openreview.net/forum?id=BJes_xStwS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "63MySLlyvP", "original": null, "number": 1, "cdate": 1576798748478, "ddate": null, "tcdate": 1576798748478, "tmdate": 1576800887541, "tddate": null, "forum": "BJes_xStwS", "replyto": "BJes_xStwS", "invitation": "ICLR.cc/2020/Conference/Paper2411/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes a scalable approach for graph learning from data. The reviewers think the approach appears heuristic and it is not clear the algorithm is optimizing the proposed sparse graph recovery objective. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yongyuw@mtu.edu", "qzzhao@mtu.edu", "zfeng12@stevens.edu"], "title": "GRASPEL: GRAPH SPECTRAL LEARNING AT SCALE", "authors": ["Yongyu Wang", "Zhiqiang Zhao", "Zhuo Feng"], "pdf": "/pdf/4851c66a2c2f68226e9906f01c1642354bd63a3f.pdf", "TL;DR": "A spectral approach to scalable graph learning from data", "abstract": "Learning meaningful graphs from data plays important roles in many data mining and machine learning tasks, such as data representation and analysis, dimension reduction, data clustering, and visualization, etc. In this work, we present a  scalable spectral approach to graph learning from data. By limiting the precision matrix to be a graph Laplacian, our approach aims to estimate ultra-sparse weighted graphs and has a clear connection with the prior graphical Lasso method. By interleaving nearly-linear time spectral graph sparsification,  coarsening and embedding procedures, ultra-sparse yet spectrally-stable graphs can be iteratively constructed in a highly-scalable manner. Compared with prior graph learning approaches that do not scale to large problems, our approach is highly-scalable for constructing graphs that can immediately lead to substantially improved computing efficiency and solution quality for a variety of data mining and machine learning applications, such as spectral clustering (SC), and t-Distributed Stochastic Neighbor Embedding (t-SNE).  ", "keywords": ["Spectral graph theory", "graph learning", "data clustering", "t-SNE visualization"], "paperhash": "wang|graspel_graph_spectral_learning_at_scale", "original_pdf": "/attachment/9e562544ab881365c2a8e56288e2da9176752777.pdf", "_bibtex": "@misc{\nwang2020graspel,\ntitle={{\\{}GRASPEL{\\}}: {\\{}GRAPH{\\}} {\\{}SPECTRAL{\\}} {\\{}LEARNING{\\}} {\\{}AT{\\}} {\\{}SCALE{\\}}},\nauthor={Yongyu Wang and Zhiqiang Zhao and Zhuo Feng},\nyear={2020},\nurl={https://openreview.net/forum?id=BJes_xStwS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BJes_xStwS", "replyto": "BJes_xStwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795706832, "tmdate": 1576800254969, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2411/-/Decision"}}}, {"id": "HJx9csHQ5H", "original": null, "number": 3, "cdate": 1572195217764, "ddate": null, "tcdate": 1572195217764, "tmdate": 1574506246559, "tddate": null, "forum": "BJes_xStwS", "replyto": "BJes_xStwS", "invitation": "ICLR.cc/2020/Conference/Paper2411/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "In this paper, the authors present a method that transforms data into graph. They emphasize on the fact that the proposed method is scalable, using a spectral embedding to construct the graph.\n\nWe think that the paper is not of enough quality to be accepted in ICLR. Without going in detail in the derivations, we give below some major issues in this submitted paper.\n\nThe studied problem has been widely investigated in the literature. Many methods have been proposed within the same objective, including taking care of the scalability issue. The authors fail to provide the state of the art, as well as describe the contributions with respect to previous work. As a consequence, the contributions are not clear. Maybe the proposed framework is original, but the there has been plenty of methods that have considered the same problem.\n\nExperiments are poor and not convincing. The authors compare the proposed method to only two spectral clustering methods, which as the standard kNN and the Consensus kNN from 2013. These two methods are pretty old and many more recent methods have been introduced in the literature. Moreover, the results in Table 1 are somehow misleading, as the standard kNN is faster that the proposed method on 3 out of 4 datasets. Experiments in graph recovery are not clear, starting from the fact that the datasets are not defined (what are the Gaussian graph and ER graph?), neither the experimental setting (what is the problem at hand?). The same goes to the application of t-SNE which is also very weak.\n\n--------------\nReply to Rebuttal \n\nThe authors have modified the paper to take into consideration our previous comments and suggestions. However, we think that it is still of not sufficient quality. We give below some elements, without providing a thorough review.\n\nIt is pretty pretentious to say that \"this is the first work that introduces a spectral method for learning ultra-sparse (tree-like) graphs from data\", while not comparing to the state of the art. There have been many spectral methods in graph learning for large-scale datasets.\n\nIn experiments, the only added method is the one of Kalofolias and Perraudin (submitted in 2017 to ArXiv). However, results show that this method is the worst of all methods. It is even the worst compared to the simple standard knn. It is not clear how the authors get such results; It looks like something is wrong in experiments, or they are cherrypicking.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2411/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2411/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yongyuw@mtu.edu", "qzzhao@mtu.edu", "zfeng12@stevens.edu"], "title": "GRASPEL: GRAPH SPECTRAL LEARNING AT SCALE", "authors": ["Yongyu Wang", "Zhiqiang Zhao", "Zhuo Feng"], "pdf": "/pdf/4851c66a2c2f68226e9906f01c1642354bd63a3f.pdf", "TL;DR": "A spectral approach to scalable graph learning from data", "abstract": "Learning meaningful graphs from data plays important roles in many data mining and machine learning tasks, such as data representation and analysis, dimension reduction, data clustering, and visualization, etc. In this work, we present a  scalable spectral approach to graph learning from data. By limiting the precision matrix to be a graph Laplacian, our approach aims to estimate ultra-sparse weighted graphs and has a clear connection with the prior graphical Lasso method. By interleaving nearly-linear time spectral graph sparsification,  coarsening and embedding procedures, ultra-sparse yet spectrally-stable graphs can be iteratively constructed in a highly-scalable manner. Compared with prior graph learning approaches that do not scale to large problems, our approach is highly-scalable for constructing graphs that can immediately lead to substantially improved computing efficiency and solution quality for a variety of data mining and machine learning applications, such as spectral clustering (SC), and t-Distributed Stochastic Neighbor Embedding (t-SNE).  ", "keywords": ["Spectral graph theory", "graph learning", "data clustering", "t-SNE visualization"], "paperhash": "wang|graspel_graph_spectral_learning_at_scale", "original_pdf": "/attachment/9e562544ab881365c2a8e56288e2da9176752777.pdf", "_bibtex": "@misc{\nwang2020graspel,\ntitle={{\\{}GRASPEL{\\}}: {\\{}GRAPH{\\}} {\\{}SPECTRAL{\\}} {\\{}LEARNING{\\}} {\\{}AT{\\}} {\\{}SCALE{\\}}},\nauthor={Yongyu Wang and Zhiqiang Zhao and Zhuo Feng},\nyear={2020},\nurl={https://openreview.net/forum?id=BJes_xStwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJes_xStwS", "replyto": "BJes_xStwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2411/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2411/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575858261546, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2411/Reviewers"], "noninvitees": [], "tcdate": 1570237723203, "tmdate": 1575858261560, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2411/-/Official_Review"}}}, {"id": "Bkx5ok9sjr", "original": null, "number": 5, "cdate": 1573785505681, "ddate": null, "tcdate": 1573785505681, "tmdate": 1573785505681, "tddate": null, "forum": "BJes_xStwS", "replyto": "S1e5fAq6tS", "invitation": "ICLR.cc/2020/Conference/Paper2411/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "Q1. The authors demonstrate that their algorithm is scalable and faster than Laplacian-based methods requiring O(N^2). However, the proposed method also requires to compute eigenvectors of Laplacian thus it seems not to be faster compared to the previous algorithm. It would be better to provide the time complexity of each step in section 3.2 and that of the overall algorithm.\n\nA1: Thanks for pointing this out. Our spectral graph embedding leverages a recent spectral graph coarsening approach to achieve nearly-linear time complexity for computing the first few graph Laplacian eigenvalues and eigenvectors.  We note that this is the first work that introduces a spectral method for scalable graph learning from data by leveraging the latest results in spectral graph theory. Since in the proposed work all the kernel functions, such as spectral graph sparsification, spectral graph coarsening and spectral graph embedding methods are nearly-linear time algorithms, the entire spectral graph learning approach is also highly scalable. We have included more results to show the scalability of our method and stressed the above fact in the revised draft.\n\n\nQ2: It is unclear that the proposed algorithm (section 3.2) is optimized for the objective function in equation (9). And it is possible to theoretically guarantee that the algorithm finds a spectrally optimized graph?\n\nA2: This is a very good suggestion. In the revised paper, we have included a description of the connection between our algorithm and the optimization objective in (2). The original optimization objective function (9) includes three components: (a) log (det L) that corresponds to the sum of the Laplacian eigenvalues, (b) - \\alpha* X^T L X that corresponds to the smoothness of signals across the graph, and (c) - \\beta* |L|_0 that corresponds to graph sparsity. Our algorithm flow aims to iteratively identify and include the most spectrally-critical edges into the latest graph so that the first few Laplacian eigenvalues & eigenvectors can be most significantly perturbed with the minimum amount of edges. Since the inclusion of spectrally-critical edges will immediately improve distortion in the embedding space, the overall smoothness of graph signals will thus be significantly improved. In other words, the spectrally-critical edges will only impact the first few Laplacian eigenvalues and eigenvectors key to graph spectral properties, but not the largest few eigenvalues and eigenvectors-which will require adding much more edges to influence. It can be easily shown that including any additional edge into the graph will monotonically increase (a), but monotonically decrease (b) and (c). Specifically, when the spectra of the learned graph is not stable, adding spectrally-critical edges will dramatically increase  (a), while decreasing (b) and (c) at a much lower rate since the improved graph signal smoothness will only result in a slight change (increase) to  Tr(X^T L x). Consequently, the objective function in (2) will be effectively maximized by including only a small amount of spectrally-critical edges until the first few eigenvalues become sufficiently stable; when adding extra edges can no longer significantly perturb the first few eigenvalues, (b) and (c) will start to dominate the objective function value, indicating that the iterations should be terminated. The stopping condition can be controlled by properly setting an embedding distortion threshold for $\\eta $ or parameters $\\alpha$  and $\\beta$. We have included the above convergence analysis in the revised draft.\n\nQ3: For experiments, although the authors argue that the proposed algorithm is scalable, datasets that they used are not large-scale. And it is needed to provide runtimes of other algorithms for graph recovery tasks (section 4.2).\n\nA3: Thanks for the suggestion. We have compared our methods with state-of-the-art graph learning methods published in ICLR\u201919 paper \"Large scale graph learning from smooth signals.\" by Kalofolias, Vassilis, and Nathana\u00ebl Perraudin. As shown, our approach is over 400X faster for graph construction while achieving consistently much better accuracy in spectral clustering tasks. We also added a figure (Figure 3) showing the scalabilities of comparisons with state-of-the-art methods for graph recovery tasks. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2411/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2411/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yongyuw@mtu.edu", "qzzhao@mtu.edu", "zfeng12@stevens.edu"], "title": "GRASPEL: GRAPH SPECTRAL LEARNING AT SCALE", "authors": ["Yongyu Wang", "Zhiqiang Zhao", "Zhuo Feng"], "pdf": "/pdf/4851c66a2c2f68226e9906f01c1642354bd63a3f.pdf", "TL;DR": "A spectral approach to scalable graph learning from data", "abstract": "Learning meaningful graphs from data plays important roles in many data mining and machine learning tasks, such as data representation and analysis, dimension reduction, data clustering, and visualization, etc. In this work, we present a  scalable spectral approach to graph learning from data. By limiting the precision matrix to be a graph Laplacian, our approach aims to estimate ultra-sparse weighted graphs and has a clear connection with the prior graphical Lasso method. By interleaving nearly-linear time spectral graph sparsification,  coarsening and embedding procedures, ultra-sparse yet spectrally-stable graphs can be iteratively constructed in a highly-scalable manner. Compared with prior graph learning approaches that do not scale to large problems, our approach is highly-scalable for constructing graphs that can immediately lead to substantially improved computing efficiency and solution quality for a variety of data mining and machine learning applications, such as spectral clustering (SC), and t-Distributed Stochastic Neighbor Embedding (t-SNE).  ", "keywords": ["Spectral graph theory", "graph learning", "data clustering", "t-SNE visualization"], "paperhash": "wang|graspel_graph_spectral_learning_at_scale", "original_pdf": "/attachment/9e562544ab881365c2a8e56288e2da9176752777.pdf", "_bibtex": "@misc{\nwang2020graspel,\ntitle={{\\{}GRASPEL{\\}}: {\\{}GRAPH{\\}} {\\{}SPECTRAL{\\}} {\\{}LEARNING{\\}} {\\{}AT{\\}} {\\{}SCALE{\\}}},\nauthor={Yongyu Wang and Zhiqiang Zhao and Zhuo Feng},\nyear={2020},\nurl={https://openreview.net/forum?id=BJes_xStwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJes_xStwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2411/Authors", "ICLR.cc/2020/Conference/Paper2411/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2411/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2411/Reviewers", "ICLR.cc/2020/Conference/Paper2411/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2411/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2411/Authors|ICLR.cc/2020/Conference/Paper2411/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141769, "tmdate": 1576860551082, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2411/Authors", "ICLR.cc/2020/Conference/Paper2411/Reviewers", "ICLR.cc/2020/Conference/Paper2411/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2411/-/Official_Comment"}}}, {"id": "ryxCxkcjiB", "original": null, "number": 4, "cdate": 1573785333678, "ddate": null, "tcdate": 1573785333678, "tmdate": 1573785333678, "tddate": null, "forum": "BJes_xStwS", "replyto": "rkeVRxoCFB", "invitation": "ICLR.cc/2020/Conference/Paper2411/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "Q1. My major concern is about the experiments. The authors claim that the proposed graph learning approach is highly scalable. It would be more convincing if the authors can evaluate the proposed method on larger datasets.\n\nA1: Thanks for the suggestion. We have compared our methods with state-of-the-art graph learning methods published in ICLR\u201919 paper \"Large scale graph learning from smooth signals.\" by Kalofolias, Vassilis, and Nathana\u00ebl Perraudin. As shown, our approach is over 400X faster for graph learning while achieving consistently much better accuracy in spectral clustering tasks. We also added Figure 3 to show the runtime comparisons with state-of-the-art methods for graph recovery tasks.\n \nQ2. One of the tasks in experiments is t-SNE visualization. There are also some faster versions of t-SNE with a complexity of O(NlogN), such as [a]. For t-SNE, the authors may justify what's the advantage of using the proposed method over other fast t-SNE algorithms.\n[a] Accelerating t-SNE using Tree-Based Algorithms, JMLR 2014.\n\nA2: Thanks very much for the suggestion. Our results (standard t-SNE) reported in the paper are obtained by using the tree-based t-SNE algorithm that is a default option in Matlab. We have clarified this in the revised paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper2411/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2411/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yongyuw@mtu.edu", "qzzhao@mtu.edu", "zfeng12@stevens.edu"], "title": "GRASPEL: GRAPH SPECTRAL LEARNING AT SCALE", "authors": ["Yongyu Wang", "Zhiqiang Zhao", "Zhuo Feng"], "pdf": "/pdf/4851c66a2c2f68226e9906f01c1642354bd63a3f.pdf", "TL;DR": "A spectral approach to scalable graph learning from data", "abstract": "Learning meaningful graphs from data plays important roles in many data mining and machine learning tasks, such as data representation and analysis, dimension reduction, data clustering, and visualization, etc. In this work, we present a  scalable spectral approach to graph learning from data. By limiting the precision matrix to be a graph Laplacian, our approach aims to estimate ultra-sparse weighted graphs and has a clear connection with the prior graphical Lasso method. By interleaving nearly-linear time spectral graph sparsification,  coarsening and embedding procedures, ultra-sparse yet spectrally-stable graphs can be iteratively constructed in a highly-scalable manner. Compared with prior graph learning approaches that do not scale to large problems, our approach is highly-scalable for constructing graphs that can immediately lead to substantially improved computing efficiency and solution quality for a variety of data mining and machine learning applications, such as spectral clustering (SC), and t-Distributed Stochastic Neighbor Embedding (t-SNE).  ", "keywords": ["Spectral graph theory", "graph learning", "data clustering", "t-SNE visualization"], "paperhash": "wang|graspel_graph_spectral_learning_at_scale", "original_pdf": "/attachment/9e562544ab881365c2a8e56288e2da9176752777.pdf", "_bibtex": "@misc{\nwang2020graspel,\ntitle={{\\{}GRASPEL{\\}}: {\\{}GRAPH{\\}} {\\{}SPECTRAL{\\}} {\\{}LEARNING{\\}} {\\{}AT{\\}} {\\{}SCALE{\\}}},\nauthor={Yongyu Wang and Zhiqiang Zhao and Zhuo Feng},\nyear={2020},\nurl={https://openreview.net/forum?id=BJes_xStwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJes_xStwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2411/Authors", "ICLR.cc/2020/Conference/Paper2411/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2411/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2411/Reviewers", "ICLR.cc/2020/Conference/Paper2411/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2411/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2411/Authors|ICLR.cc/2020/Conference/Paper2411/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141769, "tmdate": 1576860551082, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2411/Authors", "ICLR.cc/2020/Conference/Paper2411/Reviewers", "ICLR.cc/2020/Conference/Paper2411/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2411/-/Official_Comment"}}}, {"id": "Skg4aRtssH", "original": null, "number": 3, "cdate": 1573785275595, "ddate": null, "tcdate": 1573785275595, "tmdate": 1573785275595, "tddate": null, "forum": "BJes_xStwS", "replyto": "HJx9csHQ5H", "invitation": "ICLR.cc/2020/Conference/Paper2411/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "Q1. The studied problem has been widely investigated in the literature. Many methods have been proposed within the same objective, including taking care of the scalability issue. The authors fail to provide the state of the art, as well as describe the contributions with respect to previous work. As a consequence, the contributions are not clear. Maybe the proposed framework is original, but there has been plenty of methods that have considered the same problem.\n\nA1: Thanks very much for pointing this out. We have added descriptions regarding our contribution to the abstract and introduction. Our contribution: this is the first work that introduces a spectral method for learning ultra-sparse (tree-like) graphs from data by leveraging the latest results in spectral graph theory, such as the nearly-linear-time spectral graph sparsification, spectral coarsening and spectral embedding techniques. Our framework is similar to the original graphical Lasso framework with the precision matrix replaced by a graph Laplacian matrix. This approach iteratively identifies and includes the most spectrally-critical edges into the latest graph, so that the first few Laplacian eigenvalues and eigenvectors can be most significantly perturbed by including the minimum amount of edges. The iterations will be terminated when the graph spectra become sufficiently stable (or graph signals become sufficiently smooth across the graph and lead to rather small Laplacian quadratic forms). High-quality estimation of attractive Gaussian Markov Random Fields (GMRFs) can be achieved for much larger datasets compared with state-of-the-art methods. The graphs learned from our approach allow obtaining much more accurate results more efficiently in spectral clustering tasks (due to the ultra-sparse tree-like structure) and faster performance for t-SNE visualization of large data sets. We have also included more details about the connection between our algorithm with the original optimization objective in (2) in the revised draft.\n\nQ2. Experiments are poor and not convincing. The authors compare the proposed method to only two spectral clustering methods, which as the standard kNN and the Consensus kNN from 2013. These two methods are pretty old and many more recent methods have been introduced in the literature. Moreover, the results in Table 1 are somehow misleading, as the standard kNN is faster that the proposed method on 3 out of 4 datasets. Experiments in graph recovery are not clear, starting from the fact that the datasets are not defined (what are the Gaussian graph and ER graph?), neither the experimental setting (what is the problem at hand?). The same goes to the application of t-SNE which is also very weak.\n\nA2: Thanks very much for the kind suggestion. GRASPEL indeed runs slightly slower than the standard kNN for very small datasets but much faster for larger ones. More importantly, the graphs learned by our approach have ultra-sparse tree-like structures (the edge to node ratio is between 1.1 to 1.3) and will result in significantly improved accuracy and efficiency in spectral clustering. As shown in Table 1 that includes substantially updated results, the spectral clustering time for the MNIST data set with standard kNN is over 6000 seconds but will be dramatically brought down to less than three seconds (over 2000X speedup) using the graph learned by our method (GRASPEL). We also have compared our methods with state-of-the-art graph learning methods published in ICLR\u201919, \"Large scale graph learning from smooth signals.\" by Kalofolias, Vassilis, and Nathana\u00ebl Perraudin. As shown, our approach is over 400X faster for graph construction while achieving consistently much better accuracy in spectral clustering tasks.  We also added Figure 3 to show the runtime comparisons with state-of-the-art methods for graph recovery tasks."}, "signatures": ["ICLR.cc/2020/Conference/Paper2411/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2411/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yongyuw@mtu.edu", "qzzhao@mtu.edu", "zfeng12@stevens.edu"], "title": "GRASPEL: GRAPH SPECTRAL LEARNING AT SCALE", "authors": ["Yongyu Wang", "Zhiqiang Zhao", "Zhuo Feng"], "pdf": "/pdf/4851c66a2c2f68226e9906f01c1642354bd63a3f.pdf", "TL;DR": "A spectral approach to scalable graph learning from data", "abstract": "Learning meaningful graphs from data plays important roles in many data mining and machine learning tasks, such as data representation and analysis, dimension reduction, data clustering, and visualization, etc. In this work, we present a  scalable spectral approach to graph learning from data. By limiting the precision matrix to be a graph Laplacian, our approach aims to estimate ultra-sparse weighted graphs and has a clear connection with the prior graphical Lasso method. By interleaving nearly-linear time spectral graph sparsification,  coarsening and embedding procedures, ultra-sparse yet spectrally-stable graphs can be iteratively constructed in a highly-scalable manner. Compared with prior graph learning approaches that do not scale to large problems, our approach is highly-scalable for constructing graphs that can immediately lead to substantially improved computing efficiency and solution quality for a variety of data mining and machine learning applications, such as spectral clustering (SC), and t-Distributed Stochastic Neighbor Embedding (t-SNE).  ", "keywords": ["Spectral graph theory", "graph learning", "data clustering", "t-SNE visualization"], "paperhash": "wang|graspel_graph_spectral_learning_at_scale", "original_pdf": "/attachment/9e562544ab881365c2a8e56288e2da9176752777.pdf", "_bibtex": "@misc{\nwang2020graspel,\ntitle={{\\{}GRASPEL{\\}}: {\\{}GRAPH{\\}} {\\{}SPECTRAL{\\}} {\\{}LEARNING{\\}} {\\{}AT{\\}} {\\{}SCALE{\\}}},\nauthor={Yongyu Wang and Zhiqiang Zhao and Zhuo Feng},\nyear={2020},\nurl={https://openreview.net/forum?id=BJes_xStwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJes_xStwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2411/Authors", "ICLR.cc/2020/Conference/Paper2411/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2411/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2411/Reviewers", "ICLR.cc/2020/Conference/Paper2411/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2411/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2411/Authors|ICLR.cc/2020/Conference/Paper2411/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141769, "tmdate": 1576860551082, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2411/Authors", "ICLR.cc/2020/Conference/Paper2411/Reviewers", "ICLR.cc/2020/Conference/Paper2411/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2411/-/Official_Comment"}}}, {"id": "r1l8rAKooH", "original": null, "number": 2, "cdate": 1573785150290, "ddate": null, "tcdate": 1573785150290, "tmdate": 1573785150290, "tddate": null, "forum": "BJes_xStwS", "replyto": "BJes_xStwS", "invitation": "ICLR.cc/2020/Conference/Paper2411/-/Official_Comment", "content": {"title": "Summary of our update", "comment": "Thanks for the comments from all three reviewers. We added additional experiments and clarification into our modified paper (marked in blue). Specifically, (1) we completely rewrote Sections 2 and 3, as well as modified Sections 1 and 4 to more clearly highlight our contribution and results; (2) convergence and complexity analysis has also been included into Section 3; (3) we added additional experimental results comparing with the ICLR\u201919 paper (\"Large scale graph learning from smooth signals.\") and included additional runtime results for spectral clustering tasks using the graphs learned (constructed) by different methods; (4) we also demonstrated GRASPEL\u2019s runtime scalability for graph recovery tasks in Figure 3 by comparing it with state-of-the-art methods on data sets of different sizes. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2411/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2411/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yongyuw@mtu.edu", "qzzhao@mtu.edu", "zfeng12@stevens.edu"], "title": "GRASPEL: GRAPH SPECTRAL LEARNING AT SCALE", "authors": ["Yongyu Wang", "Zhiqiang Zhao", "Zhuo Feng"], "pdf": "/pdf/4851c66a2c2f68226e9906f01c1642354bd63a3f.pdf", "TL;DR": "A spectral approach to scalable graph learning from data", "abstract": "Learning meaningful graphs from data plays important roles in many data mining and machine learning tasks, such as data representation and analysis, dimension reduction, data clustering, and visualization, etc. In this work, we present a  scalable spectral approach to graph learning from data. By limiting the precision matrix to be a graph Laplacian, our approach aims to estimate ultra-sparse weighted graphs and has a clear connection with the prior graphical Lasso method. By interleaving nearly-linear time spectral graph sparsification,  coarsening and embedding procedures, ultra-sparse yet spectrally-stable graphs can be iteratively constructed in a highly-scalable manner. Compared with prior graph learning approaches that do not scale to large problems, our approach is highly-scalable for constructing graphs that can immediately lead to substantially improved computing efficiency and solution quality for a variety of data mining and machine learning applications, such as spectral clustering (SC), and t-Distributed Stochastic Neighbor Embedding (t-SNE).  ", "keywords": ["Spectral graph theory", "graph learning", "data clustering", "t-SNE visualization"], "paperhash": "wang|graspel_graph_spectral_learning_at_scale", "original_pdf": "/attachment/9e562544ab881365c2a8e56288e2da9176752777.pdf", "_bibtex": "@misc{\nwang2020graspel,\ntitle={{\\{}GRASPEL{\\}}: {\\{}GRAPH{\\}} {\\{}SPECTRAL{\\}} {\\{}LEARNING{\\}} {\\{}AT{\\}} {\\{}SCALE{\\}}},\nauthor={Yongyu Wang and Zhiqiang Zhao and Zhuo Feng},\nyear={2020},\nurl={https://openreview.net/forum?id=BJes_xStwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJes_xStwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2411/Authors", "ICLR.cc/2020/Conference/Paper2411/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2411/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2411/Reviewers", "ICLR.cc/2020/Conference/Paper2411/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2411/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2411/Authors|ICLR.cc/2020/Conference/Paper2411/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141769, "tmdate": 1576860551082, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2411/Authors", "ICLR.cc/2020/Conference/Paper2411/Reviewers", "ICLR.cc/2020/Conference/Paper2411/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2411/-/Official_Comment"}}}, {"id": "S1e5fAq6tS", "original": null, "number": 1, "cdate": 1571823121605, "ddate": null, "tcdate": 1571823121605, "tmdate": 1572972341739, "tddate": null, "forum": "BJes_xStwS", "replyto": "BJes_xStwS", "invitation": "ICLR.cc/2020/Conference/Paper2411/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a scalable approach for graph learning from data. At a high-level, it begins with a k-NN graph construction, then node features are embedded to spectral space (embedding to space spanned by eigenvectors of Laplacian). Next, edges that have a large distortion are additionally added to the latest graph. And these steps are repeated until the output graph is stable (i.e., the embedding distortion becomes small). Experimental result for spectral clustering shows that the proposed method can achieve the best accuracy compared to kNN-based methods. For graph recovery, the algorithm also performs better than other Laplacian-based graph learning methods. In addition, the proposed approach runs up to 5 times faster for t-SNE.\n\nThe authors demonstrate that their algorithm is scalable and faster than Laplacian-based methods requiring O(N^2). However, the proposed method also requires to compute eigenvectors of Laplacian thus it seems not to be faster compared to the previous algorithm. It would be better to provide the time complexity of each step in section 3.2 and that of the overall algorithm.\n\nIt is unclear that the proposed algorithm (section 3.2) is optimized the objective function in equation (9). And it is possible to theoretically guarantee that the algorithm finds a spectrally optimized graph?\n\nFor experiments, although the authors argue that the proposed algorithm is scalable, datasets that they used are not large-scale. And it is needed to provide runtimes of other algorithms for graph recovery tasks (section 4.2).\n\nOverall, this paper develops a new approach, but its novelty and intuition are unclear. Moreover, it does not seem to be scalable under the bar of acceptance. \n\nMinor concerns:\nThere is no content in section 3.2.5.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2411/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2411/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yongyuw@mtu.edu", "qzzhao@mtu.edu", "zfeng12@stevens.edu"], "title": "GRASPEL: GRAPH SPECTRAL LEARNING AT SCALE", "authors": ["Yongyu Wang", "Zhiqiang Zhao", "Zhuo Feng"], "pdf": "/pdf/4851c66a2c2f68226e9906f01c1642354bd63a3f.pdf", "TL;DR": "A spectral approach to scalable graph learning from data", "abstract": "Learning meaningful graphs from data plays important roles in many data mining and machine learning tasks, such as data representation and analysis, dimension reduction, data clustering, and visualization, etc. In this work, we present a  scalable spectral approach to graph learning from data. By limiting the precision matrix to be a graph Laplacian, our approach aims to estimate ultra-sparse weighted graphs and has a clear connection with the prior graphical Lasso method. By interleaving nearly-linear time spectral graph sparsification,  coarsening and embedding procedures, ultra-sparse yet spectrally-stable graphs can be iteratively constructed in a highly-scalable manner. Compared with prior graph learning approaches that do not scale to large problems, our approach is highly-scalable for constructing graphs that can immediately lead to substantially improved computing efficiency and solution quality for a variety of data mining and machine learning applications, such as spectral clustering (SC), and t-Distributed Stochastic Neighbor Embedding (t-SNE).  ", "keywords": ["Spectral graph theory", "graph learning", "data clustering", "t-SNE visualization"], "paperhash": "wang|graspel_graph_spectral_learning_at_scale", "original_pdf": "/attachment/9e562544ab881365c2a8e56288e2da9176752777.pdf", "_bibtex": "@misc{\nwang2020graspel,\ntitle={{\\{}GRASPEL{\\}}: {\\{}GRAPH{\\}} {\\{}SPECTRAL{\\}} {\\{}LEARNING{\\}} {\\{}AT{\\}} {\\{}SCALE{\\}}},\nauthor={Yongyu Wang and Zhiqiang Zhao and Zhuo Feng},\nyear={2020},\nurl={https://openreview.net/forum?id=BJes_xStwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJes_xStwS", "replyto": "BJes_xStwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2411/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2411/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575858261546, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2411/Reviewers"], "noninvitees": [], "tcdate": 1570237723203, "tmdate": 1575858261560, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2411/-/Official_Review"}}}, {"id": "rkeVRxoCFB", "original": null, "number": 2, "cdate": 1571889356054, "ddate": null, "tcdate": 1571889356054, "tmdate": 1572972341690, "tddate": null, "forum": "BJes_xStwS", "replyto": "BJes_xStwS", "invitation": "ICLR.cc/2020/Conference/Paper2411/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a scalable spectral approach for graph learning. In particular, the authors use graph Laplacian as precision matrix, and show the connection between the proposed method and graphical Lasso. Three tasks, including spectral clustering, graph recovery and t-SNE visualization, are considered in experiments.\n\nPros.\n1. Scalable graph learning is an important research topic. This paper presents a practical solution to large-scale graph learning.\n2. The connection between the proposed method and graphical Lasso is discussed. Also, theoretical analysis on spectral criticality is provided.\n3. Overall the paper is well organized and clearly written. \n\nCons.\n1. My major concern is about the experiments. The authors claim that the proposed graph learning approach is highly scalable. It would be more convincing if the authors can evaluate the proposed method on larger datasets.\n2. One of the tasks in experiments is t-SNE visualization. There are also some faster versions of t-SNE with a complexity of O(NlogN), such as [a]. For t-SNE, the authors may justify what's the advantage of using the proposed method over other fast t-SNE algorithms.\n[a] Accelerating t-SNE using Tree-Based Algorithms, JMLR 2014."}, "signatures": ["ICLR.cc/2020/Conference/Paper2411/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2411/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yongyuw@mtu.edu", "qzzhao@mtu.edu", "zfeng12@stevens.edu"], "title": "GRASPEL: GRAPH SPECTRAL LEARNING AT SCALE", "authors": ["Yongyu Wang", "Zhiqiang Zhao", "Zhuo Feng"], "pdf": "/pdf/4851c66a2c2f68226e9906f01c1642354bd63a3f.pdf", "TL;DR": "A spectral approach to scalable graph learning from data", "abstract": "Learning meaningful graphs from data plays important roles in many data mining and machine learning tasks, such as data representation and analysis, dimension reduction, data clustering, and visualization, etc. In this work, we present a  scalable spectral approach to graph learning from data. By limiting the precision matrix to be a graph Laplacian, our approach aims to estimate ultra-sparse weighted graphs and has a clear connection with the prior graphical Lasso method. By interleaving nearly-linear time spectral graph sparsification,  coarsening and embedding procedures, ultra-sparse yet spectrally-stable graphs can be iteratively constructed in a highly-scalable manner. Compared with prior graph learning approaches that do not scale to large problems, our approach is highly-scalable for constructing graphs that can immediately lead to substantially improved computing efficiency and solution quality for a variety of data mining and machine learning applications, such as spectral clustering (SC), and t-Distributed Stochastic Neighbor Embedding (t-SNE).  ", "keywords": ["Spectral graph theory", "graph learning", "data clustering", "t-SNE visualization"], "paperhash": "wang|graspel_graph_spectral_learning_at_scale", "original_pdf": "/attachment/9e562544ab881365c2a8e56288e2da9176752777.pdf", "_bibtex": "@misc{\nwang2020graspel,\ntitle={{\\{}GRASPEL{\\}}: {\\{}GRAPH{\\}} {\\{}SPECTRAL{\\}} {\\{}LEARNING{\\}} {\\{}AT{\\}} {\\{}SCALE{\\}}},\nauthor={Yongyu Wang and Zhiqiang Zhao and Zhuo Feng},\nyear={2020},\nurl={https://openreview.net/forum?id=BJes_xStwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJes_xStwS", "replyto": "BJes_xStwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2411/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2411/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575858261546, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2411/Reviewers"], "noninvitees": [], "tcdate": 1570237723203, "tmdate": 1575858261560, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2411/-/Official_Review"}}}], "count": 10}