{"notes": [{"id": "ohz3OEhVcs", "original": "jNRJ6RmM-0Z", "number": 732, "cdate": 1601308085919, "ddate": null, "tcdate": 1601308085919, "tmdate": 1614985684864, "tddate": null, "forum": "ohz3OEhVcs", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Graph Autoencoders with Deconvolutional Networks", "authorids": ["~Jia_Li4", "jwyu@se.cuhk.edu.hk", "~Da-Cheng_Juan1", "~HAN_Zhichao1", "arjung@google.com", "~Hong_Cheng1", "~Andrew_Tomkins2"], "authors": ["Jia Li", "Jianwei Yu", "Da-Cheng Juan", "HAN Zhichao", "Arjun Gopalan", "Hong Cheng", "Andrew Tomkins"], "keywords": ["graph autoencoders", "graph deconvolutional networks"], "abstract": "Recent studies have indicated that Graph Convolutional Networks (GCNs) act as a $\\textit{low pass}$ filter in spectral domain and encode smoothed node representations.  In this paper, we consider their opposite, namely Graph Deconvolutional Networks (GDNs) that reconstruct graph signals from smoothed node representations. We motivate the design of Graph Deconvolutional Networks via a combination of inverse filters in spectral domain and de-noising layers in wavelet domain, as the inverse operation results in a $\\textit{high pass}$ filter and may amplify the noise.  Based on the proposed GDN, we further propose a graph autoencoder framework that first encodes smoothed graph representations with GCN and then decodes accurate graph signals with GDN.  We demonstrate the effectiveness of the proposed method on several tasks including unsupervised graph-level representation, social recommendation  and graph generation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|graph_autoencoders_with_deconvolutional_networks", "pdf": "/pdf/922581d7032f7f06d9f934717eb97d594762822c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=0WbTEwj6A3", "_bibtex": "@misc{\nli2021graph,\ntitle={Graph Autoencoders with Deconvolutional Networks},\nauthor={Jia Li and Jianwei Yu and Da-Cheng Juan and HAN Zhichao and Arjun Gopalan and Hong Cheng and Andrew Tomkins},\nyear={2021},\nurl={https://openreview.net/forum?id=ohz3OEhVcs}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "f4pPmTpT4qt", "original": null, "number": 1, "cdate": 1610040466708, "ddate": null, "tcdate": 1610040466708, "tmdate": 1610474070318, "tddate": null, "forum": "ohz3OEhVcs", "replyto": "ohz3OEhVcs", "invitation": "ICLR.cc/2021/Conference/Paper732/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The covered topic is timely and of potential impact for many application domains, such as drug design. The paper is well written and presentation is clear. The proposed approach seems to have some degree of originality. Experimental results seem to be generally good, and in the rebuttal the authors have provided further experimental support to their main claim.  There are however some issues that have not been solved by the author\u2019s rebuttal. I think two of them are the most important and related: \n\ni) significance of contribution: although the authors have tried in the rebuttal to explain how the proposed approach differs from related papers, it seems that there are still doubts about the amount of innovation introduced by the paper. This issue could have been mitigated by SOTA experimental results in presence of a proper model selection, that, however does not seem to be the case here (see next point);\n\nii) model selection: the authors did not clearly explain the model selection procedure in the rebuttal. This is an important issue since it is often easy to get good results by picking the best run a posteriori. Unfortunately in the literature there are highly cited papers where model selection is not performed in a proper way and reviewers very often reject papers just looking at numbers without looking at how the numbers were obtained. So, I believe it is important to accept only papers where model selection is properly done and properly explained, so to allow for reproducibility of experiments. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Autoencoders with Deconvolutional Networks", "authorids": ["~Jia_Li4", "jwyu@se.cuhk.edu.hk", "~Da-Cheng_Juan1", "~HAN_Zhichao1", "arjung@google.com", "~Hong_Cheng1", "~Andrew_Tomkins2"], "authors": ["Jia Li", "Jianwei Yu", "Da-Cheng Juan", "HAN Zhichao", "Arjun Gopalan", "Hong Cheng", "Andrew Tomkins"], "keywords": ["graph autoencoders", "graph deconvolutional networks"], "abstract": "Recent studies have indicated that Graph Convolutional Networks (GCNs) act as a $\\textit{low pass}$ filter in spectral domain and encode smoothed node representations.  In this paper, we consider their opposite, namely Graph Deconvolutional Networks (GDNs) that reconstruct graph signals from smoothed node representations. We motivate the design of Graph Deconvolutional Networks via a combination of inverse filters in spectral domain and de-noising layers in wavelet domain, as the inverse operation results in a $\\textit{high pass}$ filter and may amplify the noise.  Based on the proposed GDN, we further propose a graph autoencoder framework that first encodes smoothed graph representations with GCN and then decodes accurate graph signals with GDN.  We demonstrate the effectiveness of the proposed method on several tasks including unsupervised graph-level representation, social recommendation  and graph generation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|graph_autoencoders_with_deconvolutional_networks", "pdf": "/pdf/922581d7032f7f06d9f934717eb97d594762822c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=0WbTEwj6A3", "_bibtex": "@misc{\nli2021graph,\ntitle={Graph Autoencoders with Deconvolutional Networks},\nauthor={Jia Li and Jianwei Yu and Da-Cheng Juan and HAN Zhichao and Arjun Gopalan and Hong Cheng and Andrew Tomkins},\nyear={2021},\nurl={https://openreview.net/forum?id=ohz3OEhVcs}\n}"}, "tags": [], "invitation": {"reply": {"forum": "ohz3OEhVcs", "replyto": "ohz3OEhVcs", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040466695, "tmdate": 1610474070303, "id": "ICLR.cc/2021/Conference/Paper732/-/Decision"}}}, {"id": "1FvCXiLffh", "original": null, "number": 3, "cdate": 1603977873278, "ddate": null, "tcdate": 1603977873278, "tmdate": 1606754515108, "tddate": null, "forum": "ohz3OEhVcs", "replyto": "ohz3OEhVcs", "invitation": "ICLR.cc/2021/Conference/Paper732/-/Official_Review", "content": {"title": "The paper proposes a graph deconvolutional network to reconstruct the original graph signal from smoothed node representations obtained by graph convolutional networks.", "review": "Graph Autoencoders with Deconvolutional Networks\n\nThe paper proposes a graph deconvolutional network to reconstruct the original graph signal from smoothed node representations obtained by graph convolutional networks.\n\nThe proposed deconvolution incorporates a denoising component based on graph wavelet transforms.\n\nPros:\n-The idea of defining graph deconvolution operators is appealing and and may potentially lead to improvement in the performance of graph reconstruction/generation tasks.\n-The Visualization section of the paper shows an advantage of the proposed approach compared to other methods for reconstruction. It maintains information about high-frequency signal.\n\nCons:\n-The potential of a graph deconvolution operator are not fully exploited in the paper, mainly because of the considered tasks that do not require deconvolution because they are not intrinsically reconstruction tasks.\nThe paper applies the proposed approach to tasks of graph classification in Table 1 and social recommendation (matrix completion) in Table 2. While the comparison with other unsupervised learnimg methods looks favourable to the proposed approach, supervised learning methods are naturally more suited for the tasks in Table 1 and tend to perform slightly better (a comparison with supervised approaches would be appreciated).\nGraph Autoencoders are usually applied to tasks of graph generation such as molecule design, where they are one of the most suited approaches. Many works in literature face this problem. A comparison on the generation task would be interesting.\n-Considering the Ablation results, the improvement with respect to the ablation approaches seems marginal. Again, my opinion is that the considered tasks are not well suited for the proposed model.\n-Hyper-parameter selection: In Appendix A the hyper-parameter selection procedure is not sufficiently detailed. How do you choose the hyper-parameter values? You report the considered ranges but not the procedure you adopt to select them. \"parameters of downstream classifiers\", in my understanding it refers to the C parameter, or to other hyper-parameters as well?\n\nMinor:\nALATION-GCN -> ABLATION\n\n---Rebuttal--\nI acknowledge having checked the authors' response and the revised version of the manuscript, that has been improved since the first revision. Authors did not answer to my request for more details about the hyper parameter selection procedure that has been adopted. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper732/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper732/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Autoencoders with Deconvolutional Networks", "authorids": ["~Jia_Li4", "jwyu@se.cuhk.edu.hk", "~Da-Cheng_Juan1", "~HAN_Zhichao1", "arjung@google.com", "~Hong_Cheng1", "~Andrew_Tomkins2"], "authors": ["Jia Li", "Jianwei Yu", "Da-Cheng Juan", "HAN Zhichao", "Arjun Gopalan", "Hong Cheng", "Andrew Tomkins"], "keywords": ["graph autoencoders", "graph deconvolutional networks"], "abstract": "Recent studies have indicated that Graph Convolutional Networks (GCNs) act as a $\\textit{low pass}$ filter in spectral domain and encode smoothed node representations.  In this paper, we consider their opposite, namely Graph Deconvolutional Networks (GDNs) that reconstruct graph signals from smoothed node representations. We motivate the design of Graph Deconvolutional Networks via a combination of inverse filters in spectral domain and de-noising layers in wavelet domain, as the inverse operation results in a $\\textit{high pass}$ filter and may amplify the noise.  Based on the proposed GDN, we further propose a graph autoencoder framework that first encodes smoothed graph representations with GCN and then decodes accurate graph signals with GDN.  We demonstrate the effectiveness of the proposed method on several tasks including unsupervised graph-level representation, social recommendation  and graph generation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|graph_autoencoders_with_deconvolutional_networks", "pdf": "/pdf/922581d7032f7f06d9f934717eb97d594762822c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=0WbTEwj6A3", "_bibtex": "@misc{\nli2021graph,\ntitle={Graph Autoencoders with Deconvolutional Networks},\nauthor={Jia Li and Jianwei Yu and Da-Cheng Juan and HAN Zhichao and Arjun Gopalan and Hong Cheng and Andrew Tomkins},\nyear={2021},\nurl={https://openreview.net/forum?id=ohz3OEhVcs}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ohz3OEhVcs", "replyto": "ohz3OEhVcs", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper732/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538136380, "tmdate": 1606915791141, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper732/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper732/-/Official_Review"}}}, {"id": "QomLfEftgyu", "original": null, "number": 2, "cdate": 1603886457970, "ddate": null, "tcdate": 1603886457970, "tmdate": 1606720119927, "tddate": null, "forum": "ohz3OEhVcs", "replyto": "ohz3OEhVcs", "invitation": "ICLR.cc/2021/Conference/Paper732/-/Official_Review", "content": {"title": "The presentation of the paper is clear for the method part. Need clarification for the experiments.", "review": "The main contribution of this paper is that the authors design a graph deconvolutional network that combines inverse filters in the spectral domain and de-noising layers in the wavelet domain. Further graph autoencoders are proposed based on the graph convolutional networks and the graph deconvolutional networks. Many experiments are done and many previous methods are compared to show the effectiveness of the proposed networks.\n\nMy reason for the rating:\nBefore the experiments part, I think authors clearly present the methods and related works. The results also look quite good compared to other methods. However, I have some questions for the experimental setups that they present at the end of the paper, which somehow confuses me.\n\nPros:\n1. In general, this paper is well-written and the presentation is clear and easy to understand for the method parts.\n2. The results well demonstrate the proposed network as reported. \n\nCons:\nI have some comments and concerns for this paper before I made my final decision:\n1. It would be great to give a reference to the Maclaurin series approximation for the function of a matrix near (8), or give a simple derivation. Please mention that $n$ starts from 0.\n2. For the running time part, it would be great to quantitatively show the time, when choosing a baseline. $10$ and $15$  seem approximation numbers to the reviewer.\n3. I recommend the authors explicitly state the previous methods that you have the same setups in the experiments.\n4. I find that the authors use $\\lambda_{A} = 0$ in the experiments, which means that in (6), the loss function contains only the second term. Could you please clarify this setting?\n5. The authors mention that they use best average classification accuracy for all the methods. Could you clarify this part?\n\n---------------------------\nAfter rebuttal\n\nThank the authors' response to my comments. They also provided new results (i.e., graph generation tasks) to address the Q.4 in my comments. I was about to lower my score when the first reply came as graph generation tasks are just future works in the original paper, but I changed my mind with these results. Although other reviewers may still question the novelty and contributions, I think I would stay with my score according to the comparison and good results reported in the paper. Looking forward to seeing the code someday.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper732/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper732/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Autoencoders with Deconvolutional Networks", "authorids": ["~Jia_Li4", "jwyu@se.cuhk.edu.hk", "~Da-Cheng_Juan1", "~HAN_Zhichao1", "arjung@google.com", "~Hong_Cheng1", "~Andrew_Tomkins2"], "authors": ["Jia Li", "Jianwei Yu", "Da-Cheng Juan", "HAN Zhichao", "Arjun Gopalan", "Hong Cheng", "Andrew Tomkins"], "keywords": ["graph autoencoders", "graph deconvolutional networks"], "abstract": "Recent studies have indicated that Graph Convolutional Networks (GCNs) act as a $\\textit{low pass}$ filter in spectral domain and encode smoothed node representations.  In this paper, we consider their opposite, namely Graph Deconvolutional Networks (GDNs) that reconstruct graph signals from smoothed node representations. We motivate the design of Graph Deconvolutional Networks via a combination of inverse filters in spectral domain and de-noising layers in wavelet domain, as the inverse operation results in a $\\textit{high pass}$ filter and may amplify the noise.  Based on the proposed GDN, we further propose a graph autoencoder framework that first encodes smoothed graph representations with GCN and then decodes accurate graph signals with GDN.  We demonstrate the effectiveness of the proposed method on several tasks including unsupervised graph-level representation, social recommendation  and graph generation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|graph_autoencoders_with_deconvolutional_networks", "pdf": "/pdf/922581d7032f7f06d9f934717eb97d594762822c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=0WbTEwj6A3", "_bibtex": "@misc{\nli2021graph,\ntitle={Graph Autoencoders with Deconvolutional Networks},\nauthor={Jia Li and Jianwei Yu and Da-Cheng Juan and HAN Zhichao and Arjun Gopalan and Hong Cheng and Andrew Tomkins},\nyear={2021},\nurl={https://openreview.net/forum?id=ohz3OEhVcs}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ohz3OEhVcs", "replyto": "ohz3OEhVcs", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper732/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538136380, "tmdate": 1606915791141, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper732/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper732/-/Official_Review"}}}, {"id": "vWzNcE_s6o-", "original": null, "number": 6, "cdate": 1605605383797, "ddate": null, "tcdate": 1605605383797, "tmdate": 1606290200730, "tddate": null, "forum": "ohz3OEhVcs", "replyto": "zhtAN7JLQ2", "invitation": "ICLR.cc/2021/Conference/Paper732/-/Official_Comment", "content": {"title": "Response to Reviewer1(1/2)", "comment": "Thanks for your nice review and constructive feedback. Please see our detailed responses below.\n\nQ1. The contributions in this work are incremental. It seems most parts are based on the previous works. The deconvolution part follows the scheme defined in (Bianchi et al. 2020). The overall graph autoencoder also follows a very common settings by involving pooling and unpooling operations. In pooling part, two pooling methods are employed that are hard pooling and soft pooling. However, it is not clear to me how to combine these two methods since they are very different. The equation (3) shows the combination of these two methods but fails to make sense to me. I would like the authors to clarify this and provide detailed explanations for this part.\n\nA1:  Firstly we would like to highlight our main contribution. We study graph deconvolutional networks (GDN) from the perspective of inverse operations in spectral domain (same domain as in Kipf \\& Welling, 2017) and identity the inverse operations results in $\\textit{high pass}$ and may amplify the noise.  To solve the noise issue,  a de-nosing process based on graph wavelet are further studied. The study on GDN from the perspective of inverse filter, to the best of our knowledge, has not been studied before. Based on GDN, we further make a contribution to graph auto-encoders by reconstructing graph features with deconvolutional layers. Again, we would like to point out that the deconvolutional layer is included in graph auto-encoders for the first time in this work and there is no such component in Bianchi et al. (2020). As for the pooling operations, we don't claim the invention of this operation as we follow Ying et al. (2018). Nevertheless, we provide an explanation as follow: equation (3) strictly follows equation (3)(4) in Ying et al. (2018). Given a soft cluster assignment $S \\in \\mathbb{R}^{N \\times K}$ where $S_{ij}$ represents the probability of node $v_i$ to cluster $j$, $Z = S^{\\top}H \\in \\mathbb{R}^{K \\times v}$ derives the feature representations of the coarse graph where each row $Z_j$ represents the feature presentation of cluster $j$, and $A^{pool} = S^{\\top}AS$ derives the structure of the coarse graph where each row $A^{pool}_j$ denotes the connectivity of cluster $j$ to all $K$ clusters.\n\nQ2: In both Section 4.1 inverse of GCN and Section 4.2 wavelet de-noising, the authors talked about two methods but didn\u2019t explain why they are related to the proposed graph deconvolution networks. Also, most of the content in these two parts are following previous works like (Li et al. 2019b) and (Xu et al. 2019). I would recommend the authors to clearly point out what is the contribution and how these are related to the proposed methods.\n\nA2: We agree we need to clarify the relation of our components in GDN with previous works and thus have revised the paper accordingly. However, we must point out that our methods have substantial differences with previous works like Li et al. (2019b) and Xu et al. (2019). We address the differences as below: firstly we would like to highlight GDN is a hybrid deconvolutional network which consists of an inverse operator in spectral domain (inverse of GCN in Section 4.1) and de-noising in wavelet domain (wavelet de-noising in Section 4.2). For inverse GCN vs. Li et al. (2019b), the connection of inverse GCN and Li et al. (2019b) is that both follow an analysis framework that categorizes graph neural networks into two stages: a graph convolution stage and a feature transformation stage. Aside from this, inverse GCN and Li et al. (2019b) have distinct differences. Inverse GCN includes a high pass filter and is used to recover graph features from smoothed representations, while Li et al. (2019b) is within the scope of low pass filters and is used to encode smoothed representations. This point is quite obvious as the experiments in Li et al. (2019b) are designed to show it can retain useful representations rather than recovering the original features. For wavelet de-noising vs. Xu et al. (2019), the distinct difference is that we use Maclaurin series approximation to the heat kernel, which has explicit polynomial coefficients and can resemble the heat kernel well when $n =3$ (See Figure 2 in the revision). On the contrary, Xu et al. (2019), though tries to avoid eigen-decomposition by exploiting Chebyshev polynomials, still relies integral operations and there is no clear way that it can scale up."}, "signatures": ["ICLR.cc/2021/Conference/Paper732/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper732/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Autoencoders with Deconvolutional Networks", "authorids": ["~Jia_Li4", "jwyu@se.cuhk.edu.hk", "~Da-Cheng_Juan1", "~HAN_Zhichao1", "arjung@google.com", "~Hong_Cheng1", "~Andrew_Tomkins2"], "authors": ["Jia Li", "Jianwei Yu", "Da-Cheng Juan", "HAN Zhichao", "Arjun Gopalan", "Hong Cheng", "Andrew Tomkins"], "keywords": ["graph autoencoders", "graph deconvolutional networks"], "abstract": "Recent studies have indicated that Graph Convolutional Networks (GCNs) act as a $\\textit{low pass}$ filter in spectral domain and encode smoothed node representations.  In this paper, we consider their opposite, namely Graph Deconvolutional Networks (GDNs) that reconstruct graph signals from smoothed node representations. We motivate the design of Graph Deconvolutional Networks via a combination of inverse filters in spectral domain and de-noising layers in wavelet domain, as the inverse operation results in a $\\textit{high pass}$ filter and may amplify the noise.  Based on the proposed GDN, we further propose a graph autoencoder framework that first encodes smoothed graph representations with GCN and then decodes accurate graph signals with GDN.  We demonstrate the effectiveness of the proposed method on several tasks including unsupervised graph-level representation, social recommendation  and graph generation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|graph_autoencoders_with_deconvolutional_networks", "pdf": "/pdf/922581d7032f7f06d9f934717eb97d594762822c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=0WbTEwj6A3", "_bibtex": "@misc{\nli2021graph,\ntitle={Graph Autoencoders with Deconvolutional Networks},\nauthor={Jia Li and Jianwei Yu and Da-Cheng Juan and HAN Zhichao and Arjun Gopalan and Hong Cheng and Andrew Tomkins},\nyear={2021},\nurl={https://openreview.net/forum?id=ohz3OEhVcs}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ohz3OEhVcs", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper732/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper732/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper732/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper732/Authors|ICLR.cc/2021/Conference/Paper732/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper732/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867826, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper732/-/Official_Comment"}}}, {"id": "th_PKd23Fpl", "original": null, "number": 9, "cdate": 1605772261966, "ddate": null, "tcdate": 1605772261966, "tmdate": 1606201775172, "tddate": null, "forum": "ohz3OEhVcs", "replyto": "1FvCXiLffh", "invitation": "ICLR.cc/2021/Conference/Paper732/-/Official_Comment", "content": {"title": "Response to Reviewer3", "comment": "We sincerely thank you for the valuable comments. Our responses to the comments are listed below.\n\nQ1. The potential of a graph deconvolution operator are not fully exploited in the paper, mainly because of the considered tasks that do not require deconvolution because they are not intrinsically reconstruction tasks.  \n\nA1.  We agree more tasks can be done to fully exploit the potential of GDN and have added graph generation tasks in Section 5.3 in the revision. So far the effectiveness of GDN has been validated in three tasks including unsupervised graph representation, social recommendation and graph generation tasks. Meanwhile, we can't agree graph feature reconstructions (as the cases in unsupervised graph representation and social recommendation) are not reconstruction tasks. In fact, as most previous reconstruction works on graph data focus on topology reconstruction, the potential benefits of feature or signal reconstructions have been largely ignored.\n\nQ2. While the comparison with other unsupervised learnimg methods looks favourable to the proposed approach, supervised learning methods are naturally more suited for the tasks in Table 1 and tend to perform slightly better (a comparison with supervised approaches would be appreciated). \n\nA2.  We add the results of supervised approaches in Table 1. When compared with supervised graph classification models, ours beats the best supervised classification model GIN on IMDB-BIN, on-par with GIN on PROTEINS, IMDB-MULTI, and only loses on REDDIT. We also like to mention that it is unfair for the unsupervised methods to compare with supervised models, as our purpose here is to show that with GDN, graph autoencoders can effectively extract graph representations, and perform better than the other two unsupervised graph representation techniques, i.e., graph kernels and contrastive methods.\n\nQ3. Graph Autoencoders are usually applied to tasks of graph generation such as molecule design, where they are one of the most suited approaches. Many works in literature face this problem. A comparison on the generation task would be interesting. \n\nA3. Great suggestions. As our proposed graph autoencoder framework is not generative model and is not suitable for graph generation task, we validate the proposed GDN in two popular variational autoencoder framework including VGAE (Kipf \\& Welling, 2016) and Graphite (Grover et al., 2019). We reconstruct the graph structures using their default methods and the features using GDN. The two reconstructions share the same encoders and sample from the same latent distributions. As shown in Table 3 in the revision, GDN can improve the generative performance of both VGAE and Graphite in general, e.g., it improve the AP score of Graphite by 4.9% on MUTAG , the AUC score of VGAE by 19.4% on PTC-MR and $\\log p(A|Z)$ of VGAE by 5.4% on ZINC.\n\nQ4. Considering the Ablation results, the improvement with respect to the ablation approaches seems marginal.\n\nA4. Experimentally, GDN has 1.2% to 1.9% improvements over GCN on all five datasets, which is a very promising result as the competition on these benchmarks is pretty tough. Meanwhile, we would like to mention the results of GDN are the SOTAs of unsupervised graph classification on 4 out 5 datasets. From another perspective, We aim to demonstrate that GDN is better than GCN in reconstructing graph signals by not only retaining low frequency information but also some high frequency information, as gratefully pointed out by you.  Again, as the first work in designing graph deconvolutional networks from inverse filters in spectral domain, our proposed GDN initials a clear way to implement deconvolutional operations and produces a mechanism to prevent the potential noise problem. \n\nQ5. \"parameters of downstream classifiers\", in my understanding it refers to the C parameter, or to other hyper-parameters as well?\n\nA5.  Yes, \"parameters of downstream classifiers\" refers to the C parameter. And there is no other hyper-parameters involved."}, "signatures": ["ICLR.cc/2021/Conference/Paper732/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper732/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Autoencoders with Deconvolutional Networks", "authorids": ["~Jia_Li4", "jwyu@se.cuhk.edu.hk", "~Da-Cheng_Juan1", "~HAN_Zhichao1", "arjung@google.com", "~Hong_Cheng1", "~Andrew_Tomkins2"], "authors": ["Jia Li", "Jianwei Yu", "Da-Cheng Juan", "HAN Zhichao", "Arjun Gopalan", "Hong Cheng", "Andrew Tomkins"], "keywords": ["graph autoencoders", "graph deconvolutional networks"], "abstract": "Recent studies have indicated that Graph Convolutional Networks (GCNs) act as a $\\textit{low pass}$ filter in spectral domain and encode smoothed node representations.  In this paper, we consider their opposite, namely Graph Deconvolutional Networks (GDNs) that reconstruct graph signals from smoothed node representations. We motivate the design of Graph Deconvolutional Networks via a combination of inverse filters in spectral domain and de-noising layers in wavelet domain, as the inverse operation results in a $\\textit{high pass}$ filter and may amplify the noise.  Based on the proposed GDN, we further propose a graph autoencoder framework that first encodes smoothed graph representations with GCN and then decodes accurate graph signals with GDN.  We demonstrate the effectiveness of the proposed method on several tasks including unsupervised graph-level representation, social recommendation  and graph generation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|graph_autoencoders_with_deconvolutional_networks", "pdf": "/pdf/922581d7032f7f06d9f934717eb97d594762822c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=0WbTEwj6A3", "_bibtex": "@misc{\nli2021graph,\ntitle={Graph Autoencoders with Deconvolutional Networks},\nauthor={Jia Li and Jianwei Yu and Da-Cheng Juan and HAN Zhichao and Arjun Gopalan and Hong Cheng and Andrew Tomkins},\nyear={2021},\nurl={https://openreview.net/forum?id=ohz3OEhVcs}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ohz3OEhVcs", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper732/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper732/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper732/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper732/Authors|ICLR.cc/2021/Conference/Paper732/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper732/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867826, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper732/-/Official_Comment"}}}, {"id": "wwAqI5A-BND", "original": null, "number": 10, "cdate": 1605773825438, "ddate": null, "tcdate": 1605773825438, "tmdate": 1605773825438, "tddate": null, "forum": "ohz3OEhVcs", "replyto": "ohz3OEhVcs", "invitation": "ICLR.cc/2021/Conference/Paper732/-/Official_Comment", "content": {"title": "Summary of revisions made to the paper", "comment": "We would like to thank the reviewers for their insightful and helpful comments. We have revised the paper according to the comments. A summary of the major changes is given below:\n\n1. Re-organize the structure of ``Section 4 graph deconvolutional networks. Specifically, we have made the following changes: (1) establish the connection with related works on wavelet neural networks, the distinct difference is that we use Maclaurin series approximation to the heat kernel, which has explicit polynomial coefficients and can resemble the heat kernel well when $n =3$; (2) provide intuitive visualization in Figure 2 for low order Maclaurin Series approximations w.r.t.\\ wavelet kernel and inverse wavelet kernel; (3) provide detailed descriptions to the experimental settings of visualization in Section 4.3.  We would like to point out, compared with previous graph convolutional networks that encodes smoothed representations, graph deconvolutional network has substantially difference, and the corresponding solutions are not incremental, as concerned by reviewer1.  \n\n2. Add experiments about graph generation. Motivated by the reviewers2' suggestions, we add graph generation tasks in Section 5.3 in the revision. We validate the proposed GDN in two popular variational autoencoder framework including VGAE (Kipf \\& Welling, 2016) and Graphite (Grover et al., 2019). We reconstruct the graph structures using their default methods and the features using GDN. The two reconstructions share the same encoders and sample from the same latent distributions. As shown in Table 3 in the revision, GDN can improve the generative performance of both VGAE and Graphite in general, e.g., it improve the AP score of Graphite by 4.9% on MUTAG , the AUC score of VGAE by 19.4% on PTC-MR and $\\log p(A|Z)$ of VGAE by 5.4% on ZINC.\n\n3. Provide comparison with supervised graph classification models. We want to point out that comparing unsupervised methods to  with supervised models may not be an apple-to-apple comparison, as our purpose here is mainly to show graph autoencoders performs better in encoding graph representations with GDN in an unsupervised fashion than the other two unsupervised techniques, i.e., graph kernels and contrastive methods. Still, as the reviewer suggested, we include the results of popular supervised methods. The results confirm the proposed GDN outperforms GIN on IMDB-BIN, and is on-par with GIN on PROTEINS, IMDB-MULTI, and only loses on REDDIT. \n\nWe submitted a revised version including the aforementioned revisions. Thank you for all the efforts that help us improve the paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper732/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper732/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Autoencoders with Deconvolutional Networks", "authorids": ["~Jia_Li4", "jwyu@se.cuhk.edu.hk", "~Da-Cheng_Juan1", "~HAN_Zhichao1", "arjung@google.com", "~Hong_Cheng1", "~Andrew_Tomkins2"], "authors": ["Jia Li", "Jianwei Yu", "Da-Cheng Juan", "HAN Zhichao", "Arjun Gopalan", "Hong Cheng", "Andrew Tomkins"], "keywords": ["graph autoencoders", "graph deconvolutional networks"], "abstract": "Recent studies have indicated that Graph Convolutional Networks (GCNs) act as a $\\textit{low pass}$ filter in spectral domain and encode smoothed node representations.  In this paper, we consider their opposite, namely Graph Deconvolutional Networks (GDNs) that reconstruct graph signals from smoothed node representations. We motivate the design of Graph Deconvolutional Networks via a combination of inverse filters in spectral domain and de-noising layers in wavelet domain, as the inverse operation results in a $\\textit{high pass}$ filter and may amplify the noise.  Based on the proposed GDN, we further propose a graph autoencoder framework that first encodes smoothed graph representations with GCN and then decodes accurate graph signals with GDN.  We demonstrate the effectiveness of the proposed method on several tasks including unsupervised graph-level representation, social recommendation  and graph generation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|graph_autoencoders_with_deconvolutional_networks", "pdf": "/pdf/922581d7032f7f06d9f934717eb97d594762822c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=0WbTEwj6A3", "_bibtex": "@misc{\nli2021graph,\ntitle={Graph Autoencoders with Deconvolutional Networks},\nauthor={Jia Li and Jianwei Yu and Da-Cheng Juan and HAN Zhichao and Arjun Gopalan and Hong Cheng and Andrew Tomkins},\nyear={2021},\nurl={https://openreview.net/forum?id=ohz3OEhVcs}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ohz3OEhVcs", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper732/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper732/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper732/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper732/Authors|ICLR.cc/2021/Conference/Paper732/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper732/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867826, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper732/-/Official_Comment"}}}, {"id": "vqWAqLNTJtO", "original": null, "number": 8, "cdate": 1605756675287, "ddate": null, "tcdate": 1605756675287, "tmdate": 1605766534776, "tddate": null, "forum": "ohz3OEhVcs", "replyto": "QomLfEftgyu", "invitation": "ICLR.cc/2021/Conference/Paper732/-/Official_Comment", "content": {"title": "Response to Reviewer4", "comment": "We sincerely thank you for the valuable comments. Our responses to the comments are listed below.\n\nQ1. It would be great to give a reference to the Maclaurin series approximation for the function of a matrix near (8), or give a simple derivation. Please mention that  starts from 0.\n\nA1. Thank you for your suggestions. We put a simple derivation in Appendix C as: \n$g_c^{-1}(\\lambda_i)$ = $\\frac{1}{1-\\lambda_i}$ = $\\sum_{n=0}^{\\infty}\\frac{(-1)^{n}n!(-1)^n}{n!}\\lambda_i^n$=$\\sum_{n=0}^{\\infty} \\lambda_i^n$,  \nwhere n denotes the n-th order derivative. Thus, Eq.(8) can be obtained as \n$ g_{c}^{-1} * x = U\\text{diag}(\\sum_{n=0}^{\\infty} \\lambda_1^n,\\ldots,\\sum_{n=0}^{\\infty}\\lambda_N^n)U^\\top x=U \\sum_{n=0}^{\\infty} \\Lambda^n U^\\top x$.\n\nQ2. For the running time part, it would be great to quantitatively show the time.\n \nA2.  We appreciate the reviewer's suggestions and have revised as follows to address the concerns:  Our model takes 10s to train one epoch of PORTEINS on Tesla P40 24G, while INFOGRAPH needs 127s and MVGRL needs 193s.\n\nQ3. I recommend the authors explicitly state the previous methods that you have the same setups in the experiments.\n\nA3.  If we understand it correctly, \u201cthe same setups in the experiments\u201d refers to unsupervised graph-level representation. If so, we do state in the first sentence -- \u201cWe adopt the same procedure of previous works (Sun et al., 2020; Hassani \\& Khasahmadi,2020) and report the mean 10-fold cross validation accuracy with standard deviation after 5 runs using LIBSVM (Chang & Lin, 2011).\u201d\n\nQ4. $\\lambda_A = 0$ in the experiments.\n\nA4.  For the unsupervised graph-level representation and social recommendation, yes, we set $\\lambda_A = 0$ and directly use the original graph structure in the decoders of the proposed graph autoencoder framework, i.e., GDN takes the original graph structure as the filter to reconstruct the graph features. In this sense, we don't reconstruct the structure, and the consideration is that both unsupervised graph-level representation experiment and social recommendation can be empowered by reconstructing features. For graph generation tasks, we set $\\lambda_A = 1$ and $\\lambda_X = 1$.\n\nQ5. The authors mention that they use best average classification accuracy for all the methods. Could you clarify this part?}\n\nA5. In terms of the best average classification accuracy, it means from the combination of hyperparameters such as the dimension of the first layer, dimension of the second layer, the number of clusters K, we report the best results. This follows the settings in Sun et al., (2020) and Hassani \\& Khasahmadi,(2020)."}, "signatures": ["ICLR.cc/2021/Conference/Paper732/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper732/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Autoencoders with Deconvolutional Networks", "authorids": ["~Jia_Li4", "jwyu@se.cuhk.edu.hk", "~Da-Cheng_Juan1", "~HAN_Zhichao1", "arjung@google.com", "~Hong_Cheng1", "~Andrew_Tomkins2"], "authors": ["Jia Li", "Jianwei Yu", "Da-Cheng Juan", "HAN Zhichao", "Arjun Gopalan", "Hong Cheng", "Andrew Tomkins"], "keywords": ["graph autoencoders", "graph deconvolutional networks"], "abstract": "Recent studies have indicated that Graph Convolutional Networks (GCNs) act as a $\\textit{low pass}$ filter in spectral domain and encode smoothed node representations.  In this paper, we consider their opposite, namely Graph Deconvolutional Networks (GDNs) that reconstruct graph signals from smoothed node representations. We motivate the design of Graph Deconvolutional Networks via a combination of inverse filters in spectral domain and de-noising layers in wavelet domain, as the inverse operation results in a $\\textit{high pass}$ filter and may amplify the noise.  Based on the proposed GDN, we further propose a graph autoencoder framework that first encodes smoothed graph representations with GCN and then decodes accurate graph signals with GDN.  We demonstrate the effectiveness of the proposed method on several tasks including unsupervised graph-level representation, social recommendation  and graph generation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|graph_autoencoders_with_deconvolutional_networks", "pdf": "/pdf/922581d7032f7f06d9f934717eb97d594762822c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=0WbTEwj6A3", "_bibtex": "@misc{\nli2021graph,\ntitle={Graph Autoencoders with Deconvolutional Networks},\nauthor={Jia Li and Jianwei Yu and Da-Cheng Juan and HAN Zhichao and Arjun Gopalan and Hong Cheng and Andrew Tomkins},\nyear={2021},\nurl={https://openreview.net/forum?id=ohz3OEhVcs}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ohz3OEhVcs", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper732/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper732/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper732/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper732/Authors|ICLR.cc/2021/Conference/Paper732/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper732/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867826, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper732/-/Official_Comment"}}}, {"id": "xbKc8LXtkJt", "original": null, "number": 7, "cdate": 1605755746741, "ddate": null, "tcdate": 1605755746741, "tmdate": 1605755746741, "tddate": null, "forum": "ohz3OEhVcs", "replyto": "Cbc2Kg9t4B-", "invitation": "ICLR.cc/2021/Conference/Paper732/-/Official_Comment", "content": {"title": "Response to Reviewer2", "comment": "We sincerely thank you for the valuable comments. Our responses to the comments are listed below.\n\nQ1. Variational Graph Auto-Encoders as baselines. \n\nA1. We appreciate the reviewer's suggestions and have added VGAE (kipf \\& Welling, 2016) as baselines. We report the experimental results of VGAE with SUM functions (VGAE\\&SUM). VGAE\\&SUM achieves 64.9\\%, 38.9\\%, 72.4\\% and 76.3\\% on IMDB-BIN, IMDB-MULTI, PROTEINS and DD respectively; The results show our method outperforms VGAE\\&SUM on all datasets. In addition, we would like to highlight that our method can reconstruct both graph structures and graph features, while VGAE\\&SUM focuses on reconstructing structures. \n\nQ2. In addition, the authors should also mention how many pooling layers are used in encoder-decoder framework. And the experiments with different number of pooling layers can help understand the proposed method.\n \nA2.  We use one pooling layer and one unpooling layer in the framework. We are aware that in autoencoder framework that applies to images, a common practice is stacking multiple layers of convolution and pooling, like \u201cCNN-pooling-CNN-pooling...\u201d. However, current GCN still suffers from the problem of over-smoothing, which means increasing the number of pooling layers and GCN layers would deteriorate the overall performance (see [1]). In this sense, we just use one pooling layer.\n\n[1] Qimai Li, Zhichao Han, Xiao-Ming Wu. Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning, in AAAI 2018, 3538--3545."}, "signatures": ["ICLR.cc/2021/Conference/Paper732/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper732/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Autoencoders with Deconvolutional Networks", "authorids": ["~Jia_Li4", "jwyu@se.cuhk.edu.hk", "~Da-Cheng_Juan1", "~HAN_Zhichao1", "arjung@google.com", "~Hong_Cheng1", "~Andrew_Tomkins2"], "authors": ["Jia Li", "Jianwei Yu", "Da-Cheng Juan", "HAN Zhichao", "Arjun Gopalan", "Hong Cheng", "Andrew Tomkins"], "keywords": ["graph autoencoders", "graph deconvolutional networks"], "abstract": "Recent studies have indicated that Graph Convolutional Networks (GCNs) act as a $\\textit{low pass}$ filter in spectral domain and encode smoothed node representations.  In this paper, we consider their opposite, namely Graph Deconvolutional Networks (GDNs) that reconstruct graph signals from smoothed node representations. We motivate the design of Graph Deconvolutional Networks via a combination of inverse filters in spectral domain and de-noising layers in wavelet domain, as the inverse operation results in a $\\textit{high pass}$ filter and may amplify the noise.  Based on the proposed GDN, we further propose a graph autoencoder framework that first encodes smoothed graph representations with GCN and then decodes accurate graph signals with GDN.  We demonstrate the effectiveness of the proposed method on several tasks including unsupervised graph-level representation, social recommendation  and graph generation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|graph_autoencoders_with_deconvolutional_networks", "pdf": "/pdf/922581d7032f7f06d9f934717eb97d594762822c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=0WbTEwj6A3", "_bibtex": "@misc{\nli2021graph,\ntitle={Graph Autoencoders with Deconvolutional Networks},\nauthor={Jia Li and Jianwei Yu and Da-Cheng Juan and HAN Zhichao and Arjun Gopalan and Hong Cheng and Andrew Tomkins},\nyear={2021},\nurl={https://openreview.net/forum?id=ohz3OEhVcs}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ohz3OEhVcs", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper732/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper732/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper732/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper732/Authors|ICLR.cc/2021/Conference/Paper732/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper732/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867826, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper732/-/Official_Comment"}}}, {"id": "ZY0ItKOZ4n", "original": null, "number": 4, "cdate": 1605604853408, "ddate": null, "tcdate": 1605604853408, "tmdate": 1605605528642, "tddate": null, "forum": "ohz3OEhVcs", "replyto": "zhtAN7JLQ2", "invitation": "ICLR.cc/2021/Conference/Paper732/-/Official_Comment", "content": {"title": "Response to Reviewer1(2/2)", "comment": "Q3. The visualization section in 4.3 is confusion. Firstly, why the authors believe this is a contribution to the graph deconvolutional networks. To my understanding, there is no technical contribution for this paper. It would be better to show this in the experimental parts. Secondly, there is no description on the settings for the illustrations. I didn\u2019t understand how the image are converted into graph and how the inverse GCN and the proposed models are used to them. Thus, the authors need to clarify this and point out the technical contributions for this part.\n\nA3.  We appreciate this point and agree the settings for the visualization is not very clear. we revise this part and clarify as follow:  (1) the purpose, the experiment targets a traffic network in real service session and is used to provide an intuitive understanding of the difference of GCN, inverse GCN and GDN in reconstructing graph features. It serves as a visual ablation for GDN; and (2) the settings, we select 4438 sensor stations as the node set $V$ and collect their road average occupancy rates. We construct an adjacency matrix A by denoting $A_{ij} = 1$ if sensor station $v_j$ and $v_i$ are adjacent on a freeway. We use the proposed graph autoencoders to reconstruct road occupancy rates in each station, with different variants (GCN, inverse GCN and GDN) working as the decoders. We target traffic network because traffic network can be projected into Euclidean space, which facilitates the public's sense-making. In this vein, we would like to point out there is no conversion from image into graph and the map background is just used for sense-making.\n\nQ4. The experimental results are very limited. The four datasets used in table 1 are very small in terms of the number of graphs, which cannot provide comprehensive evaluations to the proposed results. The baseline methods are not well established. There are far better results on these datasets such as GIN. The authors claim that the unsupervised settings are used. However, it is weird to me why the unsupervised settings do not help the overall model performances.\n\nA4. (1) Comparison with supervised classification models. For the experimental settings of unsupervised graph-level representations, we closely follow the recent methods (Sun et al., 2020; Hassani \\& Khasahmadi, 2020) and evaluate the qualify of the unsupervised graph representations in the downstream classification tasks. Thus, it is unfair for these unsupervised methods to compare with supervised models, as our purpose here is to show that with GDN, graph autoencoders can perform better than the other two unsupervised techniques, i.e., graph kernels and contrastive methods.\nNevertheless, we include the results of popular methods in supervised fashion. The results show ours is better than GIN on IMDB-BIN, on-par with GIN on PROTEINS, IMDB-MULTI, and only loses on REDDIT. (2) The size of datasets. We fully agree the experiments should be done on larger datasets for a fair comparison. That's why we replace two small datasets MUTAG and PTC-MR used in Sun et al., (2020) and Hassani \\& Khasahmadi (2020) by PROTEINS and DD, which are larger in terms of both number of graphs and size of graphs. In fact, the largest graph size used in GIN (as raised by the review) is 508 nodes per graph of REDDIT, while the largest graph size used in our experiments is 715 of DD. \n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper732/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper732/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Autoencoders with Deconvolutional Networks", "authorids": ["~Jia_Li4", "jwyu@se.cuhk.edu.hk", "~Da-Cheng_Juan1", "~HAN_Zhichao1", "arjung@google.com", "~Hong_Cheng1", "~Andrew_Tomkins2"], "authors": ["Jia Li", "Jianwei Yu", "Da-Cheng Juan", "HAN Zhichao", "Arjun Gopalan", "Hong Cheng", "Andrew Tomkins"], "keywords": ["graph autoencoders", "graph deconvolutional networks"], "abstract": "Recent studies have indicated that Graph Convolutional Networks (GCNs) act as a $\\textit{low pass}$ filter in spectral domain and encode smoothed node representations.  In this paper, we consider their opposite, namely Graph Deconvolutional Networks (GDNs) that reconstruct graph signals from smoothed node representations. We motivate the design of Graph Deconvolutional Networks via a combination of inverse filters in spectral domain and de-noising layers in wavelet domain, as the inverse operation results in a $\\textit{high pass}$ filter and may amplify the noise.  Based on the proposed GDN, we further propose a graph autoencoder framework that first encodes smoothed graph representations with GCN and then decodes accurate graph signals with GDN.  We demonstrate the effectiveness of the proposed method on several tasks including unsupervised graph-level representation, social recommendation  and graph generation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|graph_autoencoders_with_deconvolutional_networks", "pdf": "/pdf/922581d7032f7f06d9f934717eb97d594762822c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=0WbTEwj6A3", "_bibtex": "@misc{\nli2021graph,\ntitle={Graph Autoencoders with Deconvolutional Networks},\nauthor={Jia Li and Jianwei Yu and Da-Cheng Juan and HAN Zhichao and Arjun Gopalan and Hong Cheng and Andrew Tomkins},\nyear={2021},\nurl={https://openreview.net/forum?id=ohz3OEhVcs}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ohz3OEhVcs", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper732/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper732/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper732/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper732/Authors|ICLR.cc/2021/Conference/Paper732/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper732/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867826, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper732/-/Official_Comment"}}}, {"id": "zhtAN7JLQ2", "original": null, "number": 4, "cdate": 1604115056957, "ddate": null, "tcdate": 1604115056957, "tmdate": 1605024620007, "tddate": null, "forum": "ohz3OEhVcs", "replyto": "ohz3OEhVcs", "invitation": "ICLR.cc/2021/Conference/Paper732/-/Official_Review", "content": {"title": "Incremental work", "review": "This work considers the graph deconvolution networks. It proposed a graph deconvolutional networks to reconstruct the graph signals. The concerns for this work are as below:\n\n1.\tThe contributions in this work are incremental. It seems most parts are based on the previous works. The deconvolution part follows the scheme defined in (Bianchi et al. 2020). The overall graph autoencoder also follows a very common settings by involving pooling and unpooling operations. In pooling part, two pooling methods are employed that are hard pooling and soft pooling. However, it is not clear to me how to combine these two methods since they are very different. The equation (3) shows the combination of these two methods but fails to make sense to me. I would like the authors to clarify this and provide detailed explanations for this part.\n2.\tIn both Section 4.1 inverse of GCN and Section 4.2 wavelet de-noising, the authors talked about two methods but didn\u2019t explain why they are related to the proposed graph deconvolution networks. Also, most of the content in these two parts are following previous works like (Li et al. 2019b) and (Xu et al. 2019). I would recommend the authors to clearly point out what is the contribution and how these are related to the proposed methods.\n3.\tThe visualization section in 4.3 is confusion. Firstly, why the authors believe this is a contribution to the graph deconvolutional networks. To my understanding, there is no technical contribution for this paper. It would be better to show this in the experimental parts. Secondly, there is no description on the settings for the illustrations. I didn\u2019t understand how the image are converted into graph and how the inverse GCN and the proposed models are used to them. Thus, the authors need to clarify this and point out the technical contributions for this part.\n4.\tThe experimental results are very limited. The four datasets used in table 1 are very small in terms of the number of graphs, which cannot provide comprehensive evaluations to the proposed results. The baseline methods are not well established. There are far better results on these datasets such as GIN. The authors claim that the unsupervised settings are used. However, it is weird to me why the unsupervised settings do not help the  overall model performances.\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper732/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper732/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Autoencoders with Deconvolutional Networks", "authorids": ["~Jia_Li4", "jwyu@se.cuhk.edu.hk", "~Da-Cheng_Juan1", "~HAN_Zhichao1", "arjung@google.com", "~Hong_Cheng1", "~Andrew_Tomkins2"], "authors": ["Jia Li", "Jianwei Yu", "Da-Cheng Juan", "HAN Zhichao", "Arjun Gopalan", "Hong Cheng", "Andrew Tomkins"], "keywords": ["graph autoencoders", "graph deconvolutional networks"], "abstract": "Recent studies have indicated that Graph Convolutional Networks (GCNs) act as a $\\textit{low pass}$ filter in spectral domain and encode smoothed node representations.  In this paper, we consider their opposite, namely Graph Deconvolutional Networks (GDNs) that reconstruct graph signals from smoothed node representations. We motivate the design of Graph Deconvolutional Networks via a combination of inverse filters in spectral domain and de-noising layers in wavelet domain, as the inverse operation results in a $\\textit{high pass}$ filter and may amplify the noise.  Based on the proposed GDN, we further propose a graph autoencoder framework that first encodes smoothed graph representations with GCN and then decodes accurate graph signals with GDN.  We demonstrate the effectiveness of the proposed method on several tasks including unsupervised graph-level representation, social recommendation  and graph generation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|graph_autoencoders_with_deconvolutional_networks", "pdf": "/pdf/922581d7032f7f06d9f934717eb97d594762822c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=0WbTEwj6A3", "_bibtex": "@misc{\nli2021graph,\ntitle={Graph Autoencoders with Deconvolutional Networks},\nauthor={Jia Li and Jianwei Yu and Da-Cheng Juan and HAN Zhichao and Arjun Gopalan and Hong Cheng and Andrew Tomkins},\nyear={2021},\nurl={https://openreview.net/forum?id=ohz3OEhVcs}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ohz3OEhVcs", "replyto": "ohz3OEhVcs", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper732/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538136380, "tmdate": 1606915791141, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper732/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper732/-/Official_Review"}}}, {"id": "Cbc2Kg9t4B-", "original": null, "number": 1, "cdate": 1603668581982, "ddate": null, "tcdate": 1603668581982, "tmdate": 1605024619947, "tddate": null, "forum": "ohz3OEhVcs", "replyto": "ohz3OEhVcs", "invitation": "ICLR.cc/2021/Conference/Paper732/-/Official_Review", "content": {"title": "The proposed GDN to learn high quality unsupervised graph embedding.", "review": "Summary:\n\nThe authors proposed graph deconvolution layers (GDNs) and employ GDNs to learn graph embedding in a encoder-decoder framework. The authors performs inverse signal recovery in spectral domain and then conducts a de-noising step in wavelet domain to remove the amplified noise. The proposed method can outperform baseline methods on graph classification and social recommendation. However, there are some issuses should be solved.\n\nPro:\nThe proposed method is very novel. Encoder-decoder network is firsted employed on graph data for unsupervised learning. The writing and organization of this paper is good. The authors provide a clear and detailed describtion for the proposed method. For the decoding part, the authors proposed a de-noising step in wavelet domain to remove the amplified noise.  \n\nCons:\nIn this paper, the authors employ encoder-decoder framework to learn graph embedding. Why encoder-decoder framework is needed for graph data? One option is that we can use Variational Graph Auto-Encoders[1] to learn node embedding and use SUM or AVG function to obtain graph embedding. This should be added as a baseline to show that encoder-decoder framework is better. In addition, the authors should also mention how many pooling layers are used in encoder-decoder framework. And the experiments with different number of pooling layers can help understand the proposed method. \n\nOverall: I vote to accept this paper considering the importance of the task and the novely method proposed.\n\n[1].Kipf, Thomas N., and Max Welling. \"Variational graph auto-encoders.\" arXiv preprint arXiv:1611.07308 (2016).", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper732/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper732/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Autoencoders with Deconvolutional Networks", "authorids": ["~Jia_Li4", "jwyu@se.cuhk.edu.hk", "~Da-Cheng_Juan1", "~HAN_Zhichao1", "arjung@google.com", "~Hong_Cheng1", "~Andrew_Tomkins2"], "authors": ["Jia Li", "Jianwei Yu", "Da-Cheng Juan", "HAN Zhichao", "Arjun Gopalan", "Hong Cheng", "Andrew Tomkins"], "keywords": ["graph autoencoders", "graph deconvolutional networks"], "abstract": "Recent studies have indicated that Graph Convolutional Networks (GCNs) act as a $\\textit{low pass}$ filter in spectral domain and encode smoothed node representations.  In this paper, we consider their opposite, namely Graph Deconvolutional Networks (GDNs) that reconstruct graph signals from smoothed node representations. We motivate the design of Graph Deconvolutional Networks via a combination of inverse filters in spectral domain and de-noising layers in wavelet domain, as the inverse operation results in a $\\textit{high pass}$ filter and may amplify the noise.  Based on the proposed GDN, we further propose a graph autoencoder framework that first encodes smoothed graph representations with GCN and then decodes accurate graph signals with GDN.  We demonstrate the effectiveness of the proposed method on several tasks including unsupervised graph-level representation, social recommendation  and graph generation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|graph_autoencoders_with_deconvolutional_networks", "pdf": "/pdf/922581d7032f7f06d9f934717eb97d594762822c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=0WbTEwj6A3", "_bibtex": "@misc{\nli2021graph,\ntitle={Graph Autoencoders with Deconvolutional Networks},\nauthor={Jia Li and Jianwei Yu and Da-Cheng Juan and HAN Zhichao and Arjun Gopalan and Hong Cheng and Andrew Tomkins},\nyear={2021},\nurl={https://openreview.net/forum?id=ohz3OEhVcs}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ohz3OEhVcs", "replyto": "ohz3OEhVcs", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper732/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538136380, "tmdate": 1606915791141, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper732/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper732/-/Official_Review"}}}], "count": 12}