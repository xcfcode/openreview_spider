{"notes": [{"id": "ohdw3t-8VCY", "original": "O-hVYMVcNZ7", "number": 2271, "cdate": 1601308250283, "ddate": null, "tcdate": 1601308250283, "tmdate": 1614985722692, "tddate": null, "forum": "ohdw3t-8VCY", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "CTRLsum: Towards Generic Controllable Text Summarization", "authorids": ["~Junxian_He1", "~Wojciech_Maciej_Kryscinski1", "~Bryan_McCann1", "~Nazneen_Rajani1", "~Caiming_Xiong1"], "authors": ["Junxian He", "Wojciech Maciej Kryscinski", "Bryan McCann", "Nazneen Rajani", "Caiming Xiong"], "keywords": ["controllable text summarization"], "abstract": "Current summarization systems yield generic summaries that are disconnected from users' preferences and expectations. To address this limitation, we present CTRLsum, a novel framework for controllable summarization. Our approach enables users to control multiple aspects of generated summaries by interacting with the summarization system through textual input in the form of a set of keywords or descriptive prompts. Using a single unified model, CTRLsum is able to achieve a broad scope of summary manipulation at inference time without requiring additional human annotations or pre-defining a set of control aspects during training. We quantitatively demonstrate the effectiveness of our approach on three domains of summarization datasets and five control aspects: 1) entity-centric and 2) length-controllable summarization, 3) contribution summarization on scientific papers, 4) invention purpose summarization on patent filings, and 5) question-guided summarization on news articles in a reading comprehension setting. Moreover, when used in a standard, uncontrolled summarization setting, CTRLsum achieves state-of-the-art results on the CNN/DailyMail dataset.", "one-sentence_summary": "We present CTRLsum, a generic framework for controllable summarization that is able to achieve a broad scope of summary manipulation", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "he|ctrlsum_towards_generic_controllable_text_summarization", "pdf": "/pdf/35ef9528b1e138d28d8091c04007052cdc70a5eb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=l4P2vEEBsY", "_bibtex": "@misc{\nhe2021ctrlsum,\ntitle={{\\{}CTRL{\\}}sum: Towards Generic Controllable Text Summarization},\nauthor={Junxian He and Wojciech Maciej Kryscinski and Bryan McCann and Nazneen Rajani and Caiming Xiong},\nyear={2021},\nurl={https://openreview.net/forum?id=ohdw3t-8VCY}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "jsbb21OXNj", "original": null, "number": 1, "cdate": 1610040418003, "ddate": null, "tcdate": 1610040418003, "tmdate": 1610474016376, "tddate": null, "forum": "ohdw3t-8VCY", "replyto": "ohdw3t-8VCY", "invitation": "ICLR.cc/2021/Conference/Paper2271/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "\u2028The paper attempts at controllable summarization in two dimensions: Length, and content. Authors try to achieve this through training data generation approach, where they provide a standard BART model with additional keywords (extracted using a BERT model) in training.\n\nThe paper's main motivation on controllable summarization is important and interesting, and despite simplicity, the results are generally positive on multiple datasets.\nHowever, despite positive results, reviewers raised several critical concerns, some of which remained unresolved after reviewer/author discussion period. Examples include concerns regarding lack of methodological novelty over prior work (R1, R2, R4), unfair/incomplete comparisons with prior work (R2, R4, R5), and not evaluating on a real user controlled setting instead of automatic keywords (R1, R4). Although the authors tried addressing human evaluation in their revision, some reviewers remained unconvinced. \nSome quotes from reviewer discussions:\n\n> I'm not convinced the human eval was done properly.\n\n> My concerns are not completely addressed and the score remains unchanged.\u00a0For human evaluation, I agreed with Reviewer X.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CTRLsum: Towards Generic Controllable Text Summarization", "authorids": ["~Junxian_He1", "~Wojciech_Maciej_Kryscinski1", "~Bryan_McCann1", "~Nazneen_Rajani1", "~Caiming_Xiong1"], "authors": ["Junxian He", "Wojciech Maciej Kryscinski", "Bryan McCann", "Nazneen Rajani", "Caiming Xiong"], "keywords": ["controllable text summarization"], "abstract": "Current summarization systems yield generic summaries that are disconnected from users' preferences and expectations. To address this limitation, we present CTRLsum, a novel framework for controllable summarization. Our approach enables users to control multiple aspects of generated summaries by interacting with the summarization system through textual input in the form of a set of keywords or descriptive prompts. Using a single unified model, CTRLsum is able to achieve a broad scope of summary manipulation at inference time without requiring additional human annotations or pre-defining a set of control aspects during training. We quantitatively demonstrate the effectiveness of our approach on three domains of summarization datasets and five control aspects: 1) entity-centric and 2) length-controllable summarization, 3) contribution summarization on scientific papers, 4) invention purpose summarization on patent filings, and 5) question-guided summarization on news articles in a reading comprehension setting. Moreover, when used in a standard, uncontrolled summarization setting, CTRLsum achieves state-of-the-art results on the CNN/DailyMail dataset.", "one-sentence_summary": "We present CTRLsum, a generic framework for controllable summarization that is able to achieve a broad scope of summary manipulation", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "he|ctrlsum_towards_generic_controllable_text_summarization", "pdf": "/pdf/35ef9528b1e138d28d8091c04007052cdc70a5eb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=l4P2vEEBsY", "_bibtex": "@misc{\nhe2021ctrlsum,\ntitle={{\\{}CTRL{\\}}sum: Towards Generic Controllable Text Summarization},\nauthor={Junxian He and Wojciech Maciej Kryscinski and Bryan McCann and Nazneen Rajani and Caiming Xiong},\nyear={2021},\nurl={https://openreview.net/forum?id=ohdw3t-8VCY}\n}"}, "tags": [], "invitation": {"reply": {"forum": "ohdw3t-8VCY", "replyto": "ohdw3t-8VCY", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040417989, "tmdate": 1610474016360, "id": "ICLR.cc/2021/Conference/Paper2271/-/Decision"}}}, {"id": "5Mc64UkeQFv", "original": null, "number": 1, "cdate": 1603847595467, "ddate": null, "tcdate": 1603847595467, "tmdate": 1606761436961, "tddate": null, "forum": "ohdw3t-8VCY", "replyto": "ohdw3t-8VCY", "invitation": "ICLR.cc/2021/Conference/Paper2271/-/Official_Review", "content": {"title": "Official Blind Review #1", "review": "Paper Summary:\n* This paper proposes a framework for controllable summarization, CTRLsum.  It is different from standard summarization models that CTRLsum uses a set of keywords extracted from the source text automatically or descriptive prompts to control the summary.  Experiments with three domains of summarization datasets and five control aspects.\n\nStrengthes:\n* The authors investigated the effectiveness of the proposed model through extensive experiments.\n\nWeaknesses:\n* The proposed method that uses keywords as an additional input text is almost the same as CIT (Saito et al., 2020), and the scores of CTRLsum on the CNNDM dataset reported in Table 7 does not outperformed those of CIT.  Also, it is not novel to use descriptive prompts to control natural language generation.\n   * Saito et al.: Abstractive Summarization with Combination of Pre-trained Sequence-to-Sequence and Saliency Models. CoRR abs/2003.13028 (2020)\n* I think that the author's claim, \"keywords and prompts are complementary\", is not evaluated fully.\n\nQuestions:\n\n* With respect to contribution summarization, did you evaluate CTRLsum(keyword without prompt) and CTRLsum(prompt without keywords)?  The control tokens \"the main contributions of this paper are : ( 1 )\" is far from the keywords used during training, and so I think that the keywords are not effective for contribution summarization.  In fact, BART that uses prompt worked well for contribution summarization.\n\n* Did you evaluate the ablation tests with respect to the special token \"|\" and keyword dropout?\n\n* Can CTRLsum control the generation with multiple aspects (length and entity control, length and QA control, etc.) simultaneously?  The length of  summaries generated by CTRLsum is strongly dependent the number of keywords, and so I think it is difficult to simultaneously control multiple aspects including length control.\n\nUpdate:\nThank you for the answers to my questions and additional experiments. ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2271/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2271/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CTRLsum: Towards Generic Controllable Text Summarization", "authorids": ["~Junxian_He1", "~Wojciech_Maciej_Kryscinski1", "~Bryan_McCann1", "~Nazneen_Rajani1", "~Caiming_Xiong1"], "authors": ["Junxian He", "Wojciech Maciej Kryscinski", "Bryan McCann", "Nazneen Rajani", "Caiming Xiong"], "keywords": ["controllable text summarization"], "abstract": "Current summarization systems yield generic summaries that are disconnected from users' preferences and expectations. To address this limitation, we present CTRLsum, a novel framework for controllable summarization. Our approach enables users to control multiple aspects of generated summaries by interacting with the summarization system through textual input in the form of a set of keywords or descriptive prompts. Using a single unified model, CTRLsum is able to achieve a broad scope of summary manipulation at inference time without requiring additional human annotations or pre-defining a set of control aspects during training. We quantitatively demonstrate the effectiveness of our approach on three domains of summarization datasets and five control aspects: 1) entity-centric and 2) length-controllable summarization, 3) contribution summarization on scientific papers, 4) invention purpose summarization on patent filings, and 5) question-guided summarization on news articles in a reading comprehension setting. Moreover, when used in a standard, uncontrolled summarization setting, CTRLsum achieves state-of-the-art results on the CNN/DailyMail dataset.", "one-sentence_summary": "We present CTRLsum, a generic framework for controllable summarization that is able to achieve a broad scope of summary manipulation", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "he|ctrlsum_towards_generic_controllable_text_summarization", "pdf": "/pdf/35ef9528b1e138d28d8091c04007052cdc70a5eb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=l4P2vEEBsY", "_bibtex": "@misc{\nhe2021ctrlsum,\ntitle={{\\{}CTRL{\\}}sum: Towards Generic Controllable Text Summarization},\nauthor={Junxian He and Wojciech Maciej Kryscinski and Bryan McCann and Nazneen Rajani and Caiming Xiong},\nyear={2021},\nurl={https://openreview.net/forum?id=ohdw3t-8VCY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ohdw3t-8VCY", "replyto": "ohdw3t-8VCY", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2271/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538100118, "tmdate": 1606915777636, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2271/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2271/-/Official_Review"}}}, {"id": "seX3Hn_4roa", "original": null, "number": 4, "cdate": 1604696768570, "ddate": null, "tcdate": 1604696768570, "tmdate": 1606753682106, "tddate": null, "forum": "ohdw3t-8VCY", "replyto": "ohdw3t-8VCY", "invitation": "ICLR.cc/2021/Conference/Paper2271/-/Official_Review", "content": {"title": "review", "review": "This paper proposes a two-stage summarization system where a document is provided along with (optionally) keywords or a prompt. This supplemental information helps to guide the summarization and possibly make it more user-specific. The keywords and prompt can also be guessed automatically by a BERT-base model, which seems to improve automatic metrics on CNN/daily mail.\n\nStrengths:\n* Moving beyond the conventional 'document/summary' framework of existing summarization approaches is a strength of this paper. This paper studies a few different ways that the summaries can be controlled: through prompts, entities, or one-sentence summaries of summaries (contribution and purpose summarization). These seem novel at least to this reviewer and could be helpful for future work.\n* When using oracle guidance, performance increases on several different datasets for slightly different forms of summarization (CNN/DM, arxiv, bigpatent).\n* The idea of using a two-stage approach (with a BERT-Base extractor to guess keywords to guide the summary) seems novel to this reviewer, and it seems to enable this approach to perform well even in an unconditional setting.\n\nWeaknesses:\n* The main weakness to this reviewer is that the evaluation might not be sufficiently convincing to test the key hypothesis: that these keywords/prompts can enable users to get summaries that are closer to their intent (like Figure 1). To this reviewer, this necessitates a human evaluation. Though testing factual correctness in Table 3 seems like a good start to this reviewer, measuring overall summarization quality (both conditional and unconditional on user intent) through a human evaluation seems necessary.\n* (minor) one possible reason why the two-stage approach might perform better on unconditional summarization is because there are more parameters when ensembling BERT-Base and BART. Possibly doing something multitask within a single BART model might be cleaner and could clearly test whether the gains come from more parameters/computation, or the keyword approach.\n\n\nOverall, to this reviewer, this paper seems like it would be strong if it had human evaluations of summarization quality. I would be willing to raise my score if those were provided.\n\n----\n\nUpdate: thanks for the additional human evaluation results! These help and the results on excluding unimportant entities seem strong to this reviewer. Perhaps it might be more helpful for the annotators themselves to try to interact with the summarizer in some way, but that's a more minor point.\n\nAnyways, I bumped up my score from 5->7.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2271/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2271/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CTRLsum: Towards Generic Controllable Text Summarization", "authorids": ["~Junxian_He1", "~Wojciech_Maciej_Kryscinski1", "~Bryan_McCann1", "~Nazneen_Rajani1", "~Caiming_Xiong1"], "authors": ["Junxian He", "Wojciech Maciej Kryscinski", "Bryan McCann", "Nazneen Rajani", "Caiming Xiong"], "keywords": ["controllable text summarization"], "abstract": "Current summarization systems yield generic summaries that are disconnected from users' preferences and expectations. To address this limitation, we present CTRLsum, a novel framework for controllable summarization. Our approach enables users to control multiple aspects of generated summaries by interacting with the summarization system through textual input in the form of a set of keywords or descriptive prompts. Using a single unified model, CTRLsum is able to achieve a broad scope of summary manipulation at inference time without requiring additional human annotations or pre-defining a set of control aspects during training. We quantitatively demonstrate the effectiveness of our approach on three domains of summarization datasets and five control aspects: 1) entity-centric and 2) length-controllable summarization, 3) contribution summarization on scientific papers, 4) invention purpose summarization on patent filings, and 5) question-guided summarization on news articles in a reading comprehension setting. Moreover, when used in a standard, uncontrolled summarization setting, CTRLsum achieves state-of-the-art results on the CNN/DailyMail dataset.", "one-sentence_summary": "We present CTRLsum, a generic framework for controllable summarization that is able to achieve a broad scope of summary manipulation", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "he|ctrlsum_towards_generic_controllable_text_summarization", "pdf": "/pdf/35ef9528b1e138d28d8091c04007052cdc70a5eb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=l4P2vEEBsY", "_bibtex": "@misc{\nhe2021ctrlsum,\ntitle={{\\{}CTRL{\\}}sum: Towards Generic Controllable Text Summarization},\nauthor={Junxian He and Wojciech Maciej Kryscinski and Bryan McCann and Nazneen Rajani and Caiming Xiong},\nyear={2021},\nurl={https://openreview.net/forum?id=ohdw3t-8VCY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ohdw3t-8VCY", "replyto": "ohdw3t-8VCY", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2271/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538100118, "tmdate": 1606915777636, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2271/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2271/-/Official_Review"}}}, {"id": "ltIhQFgxl1g", "original": null, "number": 2, "cdate": 1606177140728, "ddate": null, "tcdate": 1606177140728, "tmdate": 1606250376797, "tddate": null, "forum": "ohdw3t-8VCY", "replyto": "seX3Hn_4roa", "invitation": "ICLR.cc/2021/Conference/Paper2271/-/Official_Comment", "content": {"title": "We have added new results on human evaluation in Section 4.7.", "comment": "We thank the reviewer for the time and comments. Due to time limitations, we could only address major points, but we will try to reflect all advice in future revisions.\n\n> Q1: More human evaluation of the overall summarization quality\n\nThank you for the advice! We have added a new Section 4.7 to the paper to include more human evaluation results on both controlled and uncontrolled summarization. While the reviewer suggests evaluating \u201coverall summarization quality\u201d, we argue that more detailed evaluation over multiple dimensions can be more reliable, informative, and objective [1] (for example, some annotators may simply prefer more fluent and coherent summaries even though its factual correctness is worse than others if we only ask for an overall score). Therefore, we follow previous work [2,3] and ask the human annotators to score multiple dimensions of the summary such as factual consistency, relevance, fluency, and coherence. Please find details and results in the updated paper. Here we summarize the key findings:\n\n(1) For uncontrolled summarization, the quality of summaries from all systems across different datasets and dimensions is generally good, as most of the scores are higher than 4.0 out of 5.\n\n(2) For uncontrolled summarization, our significance test results suggest that both BART and CTRLsum (oracle) are *not* significantly different from CTRLsum (automatic), despite very different similarities against reference summaries in terms of ROUGE/BERTScore (e.g. CTRLsum with oracle keywords has much higher scores on automatic metrics than others).  This implies that the summary quality from different systems powered by strong pretrained models like BART has become difficult to be clearly distinguished by non-expert MTurkers. More expertise might be needed to pursue more reliable human evaluation for such strong systems. The reliability of crowdsourcing workers have been studied in [3,4], and how to obtain more reliable results from non-expert annotators for summarization is still an open research problem [1].\n\n(3) For controlled summarization, the control accuracy and control relevance (see definition in Section 4.7) are >= 3.5 out of 5 in all cases. The control accuracy for important entity control and purpose control are comparable between BART and CTRLsum without significant difference (p-value > 0.05), while CTRLsum shows significantly better control relevance overall by focusing solely on the \"user intent\". Also, the unconstrained BART are unable to generate unimportant-entity-related summaries and thus suffers from poor scores.\n\nWe would like to note that we didn\u2019t perform human eval for contribution summarization since it is difficult for the annotators to judge contributions of scientific papers from various domains. We hope these new results can address your concerns!\n.\n\n[1] Shapira et al. Crowdsourcing Lightweight Pyramids for Manual Summary Evaluation. NAACL 2019\n\n[2] Grusky et al. Newsroom: A Dataset of 1.3 Million Summaries with Diverse Extractive Strategies. NAACL 2018\n\n[3] Fabbri et al. Summeval: Re-evaluating summarization evaluation. arXiv 2020\n\n[4] Gillick et al. Non-expert evaluation of summarization systems is risky. NAACL 2010 Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical Turk.\n\n\n> Q2: The two-stage approach might perform better because there are more parameters when assembling BERT and BART.\n\nWe do agree that the two-stage model in sum has more parameters and can be a bit unfair for (uncontrolled summarization) comparison. Our method follows previous work in hybrid, extractive- abstractive/compressive summarization where an additional network is used for the extraction stage [5,6,7,8]. Such models were also benchmarked against single-model solutions. We share the reviewer\u2019s concerns on this line of work, and we think the reviewer's suggestion of using the same BART to perform both tagging and summarization with multi-task learning is a novel idea worth being explored in the future. \n\nWe would also like to emphasize that CTRLsum does not require a BERT in most of the control experiments (entity, contribution, purpose, QA), and thus the parameter size is the same as BART in those cases.\n\n\n[5] Chen et al. Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting. ACL 2018\n\n[6] Liu et al. Generating Wikipedia by Summarizing Long Sequences. ICLR 2018\n\n[7] Xu et al. Neural Extractive Text Summarization with Syntactic Compression. EMNLP 2019\n\n[8] Desai et al. Compressive Summarization with Plausibility and Salience Modeling. EMNLP 2020\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2271/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2271/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CTRLsum: Towards Generic Controllable Text Summarization", "authorids": ["~Junxian_He1", "~Wojciech_Maciej_Kryscinski1", "~Bryan_McCann1", "~Nazneen_Rajani1", "~Caiming_Xiong1"], "authors": ["Junxian He", "Wojciech Maciej Kryscinski", "Bryan McCann", "Nazneen Rajani", "Caiming Xiong"], "keywords": ["controllable text summarization"], "abstract": "Current summarization systems yield generic summaries that are disconnected from users' preferences and expectations. To address this limitation, we present CTRLsum, a novel framework for controllable summarization. Our approach enables users to control multiple aspects of generated summaries by interacting with the summarization system through textual input in the form of a set of keywords or descriptive prompts. Using a single unified model, CTRLsum is able to achieve a broad scope of summary manipulation at inference time without requiring additional human annotations or pre-defining a set of control aspects during training. We quantitatively demonstrate the effectiveness of our approach on three domains of summarization datasets and five control aspects: 1) entity-centric and 2) length-controllable summarization, 3) contribution summarization on scientific papers, 4) invention purpose summarization on patent filings, and 5) question-guided summarization on news articles in a reading comprehension setting. Moreover, when used in a standard, uncontrolled summarization setting, CTRLsum achieves state-of-the-art results on the CNN/DailyMail dataset.", "one-sentence_summary": "We present CTRLsum, a generic framework for controllable summarization that is able to achieve a broad scope of summary manipulation", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "he|ctrlsum_towards_generic_controllable_text_summarization", "pdf": "/pdf/35ef9528b1e138d28d8091c04007052cdc70a5eb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=l4P2vEEBsY", "_bibtex": "@misc{\nhe2021ctrlsum,\ntitle={{\\{}CTRL{\\}}sum: Towards Generic Controllable Text Summarization},\nauthor={Junxian He and Wojciech Maciej Kryscinski and Bryan McCann and Nazneen Rajani and Caiming Xiong},\nyear={2021},\nurl={https://openreview.net/forum?id=ohdw3t-8VCY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ohdw3t-8VCY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2271/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2271/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2271/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2271/Authors|ICLR.cc/2021/Conference/Paper2271/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2271/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850318, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2271/-/Official_Comment"}}}, {"id": "U9aIBgniCpY", "original": null, "number": 4, "cdate": 1606177580868, "ddate": null, "tcdate": 1606177580868, "tmdate": 1606245797627, "tddate": null, "forum": "ohdw3t-8VCY", "replyto": "zQuAplqoaA2", "invitation": "ICLR.cc/2021/Conference/Paper2271/-/Official_Comment", "content": {"title": "Cont'd part 1", "comment": "> Q4: It is unclear how well control works when not using oracle words or automatically extracted keywords, i.e. user-controlled. An MTurk experiment evaluating how well control works would be useful in assessing this.\n\nEvaluating CTRLsum as a fully interactive system with user-arbitrarily-specified keywords has many difficulties: (1) non-expert human evaluations can be unreliable and exhibit poor correlation with experts even for traditional summarization system [1,2], and how to obtain reliable human evaluation for summarization from non-expert MTurkers is still an active research problem [3], (2) when equipped with user input, human evaluation becomes even harder with more freedom to vary summaries for a single article -- the evaluation rubric becomes far more subjective due to diverse and ill-defined \"user intent\", (3) maybe more importantly, in the interactive case it is difficult to clearly define what aspect we are controlling and what the application scenario is, which brings difficulty to a clear problem formulation of this paper.\n\nWe emphasize that the difficulties above are exactly the reasons that we narrow down the freedom and focus on five well-defined applications in the first place. In contrast with unclear/ill-defined \"user intent\" in the case of arbitrary keywords without any constraints, our experiments simulate user intent in specific and well-defined tasks such as  entity control, length control, contribution summarization, etc., and this paper aims to have a focused contribution on such well-defined application scenarios. Therefore, despite the interesting potentials of CTRLsum conditioned on unconstrained arbitrary keywords, evaluation from such perspective is beyond the scope of this paper. \n\nHowever, we do share the concerns with the reviewer on evaluating control with human evaluations. Thus we added human evaluation results for entity control and purpose summarization in Section 4.7, Table 8. The human annotators were informed with the pre-defined user intent (generating entity-focused summary or purpose-focused summary) and asked to score control directly. Please find details and results in the updated paper. We also note that we didn\u2019t perform human eval for contribution summarization since it is difficult for the annotators to judge contributions of scientific papers from various domains. \n\nTo summarize the results, the control accuracy and control relevance (see definition in Section 4.7) in all cases are >= 3.5 out of 5. The control accuracy of important entity control and purpose control are comparable between BART and CTRLsum without significant difference (p-value > 0.05), while CTRLsum shows significantly better control relevance overall by focusing solely on the desired information. Also, the unconstrained BART are unable to generate unimportant-entity-related summaries and thus suffers from poor scores. \n\nWe hope these clarification and new results can address your concerns!\n\n\n[1] Fabbri et al. Summeval: Re-evaluating summarization evaluation. arXiv 2020\n\n[2] Gillick et al. Non-expert evaluation of summarization systems is risky. NAACL 2010 Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical Turk.\n\n[3] Shapira et al. Crowdsourcing Lightweight Pyramids for Manual Summary Evaluation. NAACL 2019\n\n> Q5: Comparing BART to BART+BERT-based models is a little unfair due to different total parameter sizes.\n\nWe agree this for uncontrolled summarization. Our method follows previous work in hybrid, extractive- abstractive/compressive summarization where an additional network is used for the extraction stage [4,5,6,7]. Such models were also benchmarked against single-model solutions. Yet, we do share the reviewer\u2019s concerns on this line of work, and we think the reviewer\u2019s suggested experiment is interesting to understand the effect of overall parameter size in this case, we\u2019ll consider running it in the future.\n\nWe would also like to emphasize that CTRLsum does not require a BERT in most of the control experiments (entity, contribution, purpose, QA), and thus the parameter size is the same as BART in those cases.\n\n\n[4] Chen et al. Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting. ACL 2018\n\n[5] Liu et al. Generating Wikipedia by Summarizing Long Sequences. ICLR 2018\n\n[6] Xu et al. Neural Extractive Text Summarization with Syntactic Compression. EMNLP 2019\n\n[7] Desai et al. Compressive Summarization with Plausibility and Salience Modeling. EMNLP 2020\n\n\n> Q6: For \"CONTRIBUTION AND PURPOSE SUMMARIZATION\", what is the fine-tuning process? Are the models fine-tuned on (paper/patent, keywords)->abstract task before testing on intro->contribution generation?\n\nYes, the modes are fine-tuned with (paper/patent, keywords) -> abstract. We just use the standard summarization dataset for training, and the training task/method for all datasets is always the same regardless of the test task.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2271/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2271/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CTRLsum: Towards Generic Controllable Text Summarization", "authorids": ["~Junxian_He1", "~Wojciech_Maciej_Kryscinski1", "~Bryan_McCann1", "~Nazneen_Rajani1", "~Caiming_Xiong1"], "authors": ["Junxian He", "Wojciech Maciej Kryscinski", "Bryan McCann", "Nazneen Rajani", "Caiming Xiong"], "keywords": ["controllable text summarization"], "abstract": "Current summarization systems yield generic summaries that are disconnected from users' preferences and expectations. To address this limitation, we present CTRLsum, a novel framework for controllable summarization. Our approach enables users to control multiple aspects of generated summaries by interacting with the summarization system through textual input in the form of a set of keywords or descriptive prompts. Using a single unified model, CTRLsum is able to achieve a broad scope of summary manipulation at inference time without requiring additional human annotations or pre-defining a set of control aspects during training. We quantitatively demonstrate the effectiveness of our approach on three domains of summarization datasets and five control aspects: 1) entity-centric and 2) length-controllable summarization, 3) contribution summarization on scientific papers, 4) invention purpose summarization on patent filings, and 5) question-guided summarization on news articles in a reading comprehension setting. Moreover, when used in a standard, uncontrolled summarization setting, CTRLsum achieves state-of-the-art results on the CNN/DailyMail dataset.", "one-sentence_summary": "We present CTRLsum, a generic framework for controllable summarization that is able to achieve a broad scope of summary manipulation", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "he|ctrlsum_towards_generic_controllable_text_summarization", "pdf": "/pdf/35ef9528b1e138d28d8091c04007052cdc70a5eb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=l4P2vEEBsY", "_bibtex": "@misc{\nhe2021ctrlsum,\ntitle={{\\{}CTRL{\\}}sum: Towards Generic Controllable Text Summarization},\nauthor={Junxian He and Wojciech Maciej Kryscinski and Bryan McCann and Nazneen Rajani and Caiming Xiong},\nyear={2021},\nurl={https://openreview.net/forum?id=ohdw3t-8VCY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ohdw3t-8VCY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2271/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2271/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2271/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2271/Authors|ICLR.cc/2021/Conference/Paper2271/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2271/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850318, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2271/-/Official_Comment"}}}, {"id": "UcgXeRXA8HK", "original": null, "number": 5, "cdate": 1606177633999, "ddate": null, "tcdate": 1606177633999, "tmdate": 1606196869050, "tddate": null, "forum": "ohdw3t-8VCY", "replyto": "U9aIBgniCpY", "invitation": "ICLR.cc/2021/Conference/Paper2271/-/Official_Comment", "content": {"title": "Cont'd part 2", "comment": "> Q7: How are entities randomly selected in the example decodes in the Appendix?\n\nWe identify all named entity mentions in the test article, remove duplicate entity mentions, and randomly sample 5 from the remains without replacement following a uniform distribution.\n\n> Q8: Are any of the prompts used in training or is it zero-shot?\n\nNone of the prompts are used in training and it is zero-shot. The training only uses article, summary, and keywords prepended to articles.\n\n> Q9: What is zero-shot state-of-the-art on the QA tasks? Please add to Table 5. GPT-3 zero-shot results would also be informative.\n\nAs far as we know, the zero-shot state-of-the-art reading comprehension is probably the giant GPT-2/GPT-3 language models. The GPT-3 paper does not report performance on NewsQA or SQuAD v1.1, and we don\u2019t have access to the GPT-3 model. However, we evaluated the GPT2-Large model on the two QA benchmarks and added the results to Table 5. We could not evaluate the GPT2-XL model with 1.5B parameters on our single GPU device.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2271/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2271/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CTRLsum: Towards Generic Controllable Text Summarization", "authorids": ["~Junxian_He1", "~Wojciech_Maciej_Kryscinski1", "~Bryan_McCann1", "~Nazneen_Rajani1", "~Caiming_Xiong1"], "authors": ["Junxian He", "Wojciech Maciej Kryscinski", "Bryan McCann", "Nazneen Rajani", "Caiming Xiong"], "keywords": ["controllable text summarization"], "abstract": "Current summarization systems yield generic summaries that are disconnected from users' preferences and expectations. To address this limitation, we present CTRLsum, a novel framework for controllable summarization. Our approach enables users to control multiple aspects of generated summaries by interacting with the summarization system through textual input in the form of a set of keywords or descriptive prompts. Using a single unified model, CTRLsum is able to achieve a broad scope of summary manipulation at inference time without requiring additional human annotations or pre-defining a set of control aspects during training. We quantitatively demonstrate the effectiveness of our approach on three domains of summarization datasets and five control aspects: 1) entity-centric and 2) length-controllable summarization, 3) contribution summarization on scientific papers, 4) invention purpose summarization on patent filings, and 5) question-guided summarization on news articles in a reading comprehension setting. Moreover, when used in a standard, uncontrolled summarization setting, CTRLsum achieves state-of-the-art results on the CNN/DailyMail dataset.", "one-sentence_summary": "We present CTRLsum, a generic framework for controllable summarization that is able to achieve a broad scope of summary manipulation", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "he|ctrlsum_towards_generic_controllable_text_summarization", "pdf": "/pdf/35ef9528b1e138d28d8091c04007052cdc70a5eb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=l4P2vEEBsY", "_bibtex": "@misc{\nhe2021ctrlsum,\ntitle={{\\{}CTRL{\\}}sum: Towards Generic Controllable Text Summarization},\nauthor={Junxian He and Wojciech Maciej Kryscinski and Bryan McCann and Nazneen Rajani and Caiming Xiong},\nyear={2021},\nurl={https://openreview.net/forum?id=ohdw3t-8VCY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ohdw3t-8VCY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2271/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2271/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2271/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2271/Authors|ICLR.cc/2021/Conference/Paper2271/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2271/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850318, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2271/-/Official_Comment"}}}, {"id": "zQuAplqoaA2", "original": null, "number": 3, "cdate": 1606177403187, "ddate": null, "tcdate": 1606177403187, "tmdate": 1606195131698, "tddate": null, "forum": "ohdw3t-8VCY", "replyto": "gg7-miWPeLQ", "invitation": "ICLR.cc/2021/Conference/Paper2271/-/Official_Comment", "content": {"title": "We have added new human eval results and clarified the difference between this work and Fan et al. ", "comment": "We thank the reviewer for the time and comments. Due to time limitations, we could only address major points, but we\u2019ll try to reflect all advice in future revisions.\n\n\n> Q1: For the prompt tests, how well does BART/PEGASUS do if the decoder is prompted? This baseline would be useful to have. That is, it\u2019s unclear how the p(y | x, z) improves over using p(y | x) by simply prompting the decoder.\n\nWe would like to clarify that in the submission version the BART baselines reported in all prompt tests (contribution, purpose, QA) were already prompted with the same prompts as CTRLsum, thus we believe that Table 5 and Table 6 exactly showed the improvement of p(y | x, z) over p(y | x) that simply prompts the decoder, as suggested by the reviewer. We did include this configuration detail in the submission version for contribution and purpose summarization (in both caption and model names from Table 6), now we added it to QA (in Table 5 caption) as well to make it clearer.\n\n\n> Q2: Contribution in methods over Fan et al + BART (Lewis et al)  is minimal.\n\nWe would like to highlight the difference between our work and that of Fan et al.\n\nMethodologically, (1) Fan et al. require to pre-define the test control aspects before training, and cannot generalize to unseen control aspects at test time. For example, if they train a length-control system, then the model is incapable of performing entity control at test time, and vice versa. Similarly, length-control or entity-control systems from Fan et al. may not perform purpose summarization or QA tasks well after we demonstrate the necessity of *non-entity* keywords in addition to prompts. We emphasize that our *generic* model does not require a prior notion of test control aspects at training time,  but is still able to perform multi-dimensional control at test time as the five example applications in this paper. \n\n(2) Moreover, the method proposed by Fan et al. requires additional, manual annotations collection of length and entities (which may not be easy to obtain in many domains) to train a length or entity control system. In contrast, our method only uses keywords for training that are automatically extracted based on articles and reference summaries. Also, the entity-control method from Fan et al. needs another baseline model to provide additional information for training, as quoted from their paper: \u201cat training time \u2026.. To ensure the entity request is informative, we provide an entity that is present in the ground-truth summary but not present in the summary generated by the baseline model\u201d.\n\nEmpirically, among five diverse control aspects that we demonstrated, the only overlapping experiments between this paper and Fan. et al. are entity and length control. For length control, we implemented the approach from Fan. et al. with BART (LengthCode in Table 4) and found that it fails to control length when the underlying architecture is strong like BART, with a Pearson correlation coefficient being zero between length control code and summary length. For entity control, it is true that our success rate is much higher because of BART (in the submission version we actually ran CTRLsum with convolutional seq2seq as Fan et al. and analyzed the effect of underlying architectures in Appendix B), we have rephrased the Results paragraph in Section 4.2 as well as the captions of Table 3 to make this clearer, stating that the CTRLsum numbers are not comparable to Fan et al. and we included their numbers only for reference point. We did not re-implement their entity-control approach with BART because the two approaches are not comparable anyway due to the requirement of entity annotations from their method.\n\nTherefore, both methodologically and empirically, there are substantial differences in contributions between our work and Fan et al.\n\n> Q3: Results in Fan et al were weak because the underlying model was much weaker than BART. \n\nWe believe that this concern is only regarding entity control results though the reviewer does not specify. To clarify, the length control approach from Fan et al does not work when applied with BART as we showed in Table 4. Regarding entity control, we agree with the reviewer\u2019s point and we rephrased descriptions in Section 4.2 as mentioned in response to #Q2 above. However, we would also like to emphasize that the methods of Fan et al. and CTRLsum are not comparable for entity control even with the same architecture because (1) Fan et al. require entity annotations for training while CTRLsum does not, and (2) their method is specifically designed for entity control and unable to generalize to unseen control aspects at test time while ours aims to be generic, as explained in response to Q2.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2271/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2271/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CTRLsum: Towards Generic Controllable Text Summarization", "authorids": ["~Junxian_He1", "~Wojciech_Maciej_Kryscinski1", "~Bryan_McCann1", "~Nazneen_Rajani1", "~Caiming_Xiong1"], "authors": ["Junxian He", "Wojciech Maciej Kryscinski", "Bryan McCann", "Nazneen Rajani", "Caiming Xiong"], "keywords": ["controllable text summarization"], "abstract": "Current summarization systems yield generic summaries that are disconnected from users' preferences and expectations. To address this limitation, we present CTRLsum, a novel framework for controllable summarization. Our approach enables users to control multiple aspects of generated summaries by interacting with the summarization system through textual input in the form of a set of keywords or descriptive prompts. Using a single unified model, CTRLsum is able to achieve a broad scope of summary manipulation at inference time without requiring additional human annotations or pre-defining a set of control aspects during training. We quantitatively demonstrate the effectiveness of our approach on three domains of summarization datasets and five control aspects: 1) entity-centric and 2) length-controllable summarization, 3) contribution summarization on scientific papers, 4) invention purpose summarization on patent filings, and 5) question-guided summarization on news articles in a reading comprehension setting. Moreover, when used in a standard, uncontrolled summarization setting, CTRLsum achieves state-of-the-art results on the CNN/DailyMail dataset.", "one-sentence_summary": "We present CTRLsum, a generic framework for controllable summarization that is able to achieve a broad scope of summary manipulation", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "he|ctrlsum_towards_generic_controllable_text_summarization", "pdf": "/pdf/35ef9528b1e138d28d8091c04007052cdc70a5eb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=l4P2vEEBsY", "_bibtex": "@misc{\nhe2021ctrlsum,\ntitle={{\\{}CTRL{\\}}sum: Towards Generic Controllable Text Summarization},\nauthor={Junxian He and Wojciech Maciej Kryscinski and Bryan McCann and Nazneen Rajani and Caiming Xiong},\nyear={2021},\nurl={https://openreview.net/forum?id=ohdw3t-8VCY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ohdw3t-8VCY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2271/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2271/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2271/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2271/Authors|ICLR.cc/2021/Conference/Paper2271/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2271/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850318, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2271/-/Official_Comment"}}}, {"id": "jUQy4cvD6hI", "original": null, "number": 6, "cdate": 1606177729467, "ddate": null, "tcdate": 1606177729467, "tmdate": 1606184079742, "tddate": null, "forum": "ohdw3t-8VCY", "replyto": "Qo366HRkgq_", "invitation": "ICLR.cc/2021/Conference/Paper2271/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "We are glad you like this paper and thank you for the encouraging comments!\n\n> Q1: the authors should say more about the differences in entity control of their method and Fan et al. 2018, which seem on their face to be similar.\n\nThank you for the advice! Fan et al. and our method in entity control look similar only at test time. At training time Fan et al. require entity annotations while ours do not. Also, Fan et al. has a more sophisticated procedure to train their entity-control model as quoted from their paper: \u201cat training time \u2026.. To ensure the entity request is informative, we provide an entity that is present in the ground-truth summary but not present in the summary generated by the baseline model\u201d. We have added the details about our difference into Section 4.2 Entity Control section.\n\n> Q2: Did the authors experiment with pairs of entities as entity controls? It would be especially interesting to see whether the model preserves the correct relationship between entities, \n\nThis is an interesting point! We added several randomly picked qualitative examples from paired entities into Appendix E.2. Please check the updated paper for details. We use one important entity and one unimportant entity as input as suggested by the reviewer, and the two entities do not appear in the same sentence in most cases. From our perspective, the output does look interesting -- it seems like the model performs reasonably well to preserve the correct relationship between entities, the model is also able to generate the \u201chidden entities\u201d correctly which are used to connect the input pair in some cases.\n\n>Q3: It would be interesting to see an evaluation that compared CTRLsum to BART with a constrained decoding method, such as dynamic beam allocation.\n\nThank you for the advice! We agree that comparing with constrained decoding methods is very interesting. We think the two are very different approaches which might share similar performance, the constrained decoding methods don\u2019t bother to train a new model but require a more complex and slower decoding process. Thus it is definitely worth exploring the tradeoff between these two, we don\u2019t have the time to do it for the rebuttal but will consider adding it in the future. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2271/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2271/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CTRLsum: Towards Generic Controllable Text Summarization", "authorids": ["~Junxian_He1", "~Wojciech_Maciej_Kryscinski1", "~Bryan_McCann1", "~Nazneen_Rajani1", "~Caiming_Xiong1"], "authors": ["Junxian He", "Wojciech Maciej Kryscinski", "Bryan McCann", "Nazneen Rajani", "Caiming Xiong"], "keywords": ["controllable text summarization"], "abstract": "Current summarization systems yield generic summaries that are disconnected from users' preferences and expectations. To address this limitation, we present CTRLsum, a novel framework for controllable summarization. Our approach enables users to control multiple aspects of generated summaries by interacting with the summarization system through textual input in the form of a set of keywords or descriptive prompts. Using a single unified model, CTRLsum is able to achieve a broad scope of summary manipulation at inference time without requiring additional human annotations or pre-defining a set of control aspects during training. We quantitatively demonstrate the effectiveness of our approach on three domains of summarization datasets and five control aspects: 1) entity-centric and 2) length-controllable summarization, 3) contribution summarization on scientific papers, 4) invention purpose summarization on patent filings, and 5) question-guided summarization on news articles in a reading comprehension setting. Moreover, when used in a standard, uncontrolled summarization setting, CTRLsum achieves state-of-the-art results on the CNN/DailyMail dataset.", "one-sentence_summary": "We present CTRLsum, a generic framework for controllable summarization that is able to achieve a broad scope of summary manipulation", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "he|ctrlsum_towards_generic_controllable_text_summarization", "pdf": "/pdf/35ef9528b1e138d28d8091c04007052cdc70a5eb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=l4P2vEEBsY", "_bibtex": "@misc{\nhe2021ctrlsum,\ntitle={{\\{}CTRL{\\}}sum: Towards Generic Controllable Text Summarization},\nauthor={Junxian He and Wojciech Maciej Kryscinski and Bryan McCann and Nazneen Rajani and Caiming Xiong},\nyear={2021},\nurl={https://openreview.net/forum?id=ohdw3t-8VCY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ohdw3t-8VCY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2271/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2271/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2271/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2271/Authors|ICLR.cc/2021/Conference/Paper2271/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2271/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850318, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2271/-/Official_Comment"}}}, {"id": "dvio7K6Lxxn", "original": null, "number": 8, "cdate": 1606178564749, "ddate": null, "tcdate": 1606178564749, "tmdate": 1606178564749, "tddate": null, "forum": "ohdw3t-8VCY", "replyto": "ohdw3t-8VCY", "invitation": "ICLR.cc/2021/Conference/Paper2271/-/Official_Comment", "content": {"title": "Revision Submitted", "comment": "We thank all the reviewers for their helpful comments! We have submitted a revised manuscript and made the following modifications to address the reviewers' major concerns:\n\n-- Added a human evaluation section (Section 4.7) which includes new human evaluation results for both controlled and uncontrolled summarization across multiple tasks and datasets (Reviewer #4, Reviewer #5).\n\n-- Added ablation results (Appendix C, Table 11) including CTRLsum (keyword only), CTRLsum (prompt only), and CTRLsum (keyword + prompt) on different control tasks (Reviewer #1).\n\n-- Added qualitative examples (Appendix E.2)  with paired entity control (Reviewer #2)\n\n-- Clarified several statements in the paper\n\nWhile limited by time in the response period, we do still plan to address all the reviewer\u2019s comments in future revisions. We also welcome any further feedbacks to improve this paper !"}, "signatures": ["ICLR.cc/2021/Conference/Paper2271/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2271/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CTRLsum: Towards Generic Controllable Text Summarization", "authorids": ["~Junxian_He1", "~Wojciech_Maciej_Kryscinski1", "~Bryan_McCann1", "~Nazneen_Rajani1", "~Caiming_Xiong1"], "authors": ["Junxian He", "Wojciech Maciej Kryscinski", "Bryan McCann", "Nazneen Rajani", "Caiming Xiong"], "keywords": ["controllable text summarization"], "abstract": "Current summarization systems yield generic summaries that are disconnected from users' preferences and expectations. To address this limitation, we present CTRLsum, a novel framework for controllable summarization. Our approach enables users to control multiple aspects of generated summaries by interacting with the summarization system through textual input in the form of a set of keywords or descriptive prompts. Using a single unified model, CTRLsum is able to achieve a broad scope of summary manipulation at inference time without requiring additional human annotations or pre-defining a set of control aspects during training. We quantitatively demonstrate the effectiveness of our approach on three domains of summarization datasets and five control aspects: 1) entity-centric and 2) length-controllable summarization, 3) contribution summarization on scientific papers, 4) invention purpose summarization on patent filings, and 5) question-guided summarization on news articles in a reading comprehension setting. Moreover, when used in a standard, uncontrolled summarization setting, CTRLsum achieves state-of-the-art results on the CNN/DailyMail dataset.", "one-sentence_summary": "We present CTRLsum, a generic framework for controllable summarization that is able to achieve a broad scope of summary manipulation", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "he|ctrlsum_towards_generic_controllable_text_summarization", "pdf": "/pdf/35ef9528b1e138d28d8091c04007052cdc70a5eb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=l4P2vEEBsY", "_bibtex": "@misc{\nhe2021ctrlsum,\ntitle={{\\{}CTRL{\\}}sum: Towards Generic Controllable Text Summarization},\nauthor={Junxian He and Wojciech Maciej Kryscinski and Bryan McCann and Nazneen Rajani and Caiming Xiong},\nyear={2021},\nurl={https://openreview.net/forum?id=ohdw3t-8VCY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ohdw3t-8VCY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2271/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2271/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2271/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2271/Authors|ICLR.cc/2021/Conference/Paper2271/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2271/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850318, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2271/-/Official_Comment"}}}, {"id": "ouf74IHvwCZ", "original": null, "number": 7, "cdate": 1606177978256, "ddate": null, "tcdate": 1606177978256, "tmdate": 1606178018878, "tddate": null, "forum": "ohdw3t-8VCY", "replyto": "5Mc64UkeQFv", "invitation": "ICLR.cc/2021/Conference/Paper2271/-/Official_Comment", "content": {"title": "We have added ablation results on the role of keywords and prompts for CTRLsum", "comment": "We thank the reviewer for the time and comments. Due to time limitations, we could only address major points, but we\u2019ll try to reflect all advice in future revisions.\n\n\n> Q1: The proposed method that uses keywords as an additional input text is almost the same as CIT (Saito et al., 2020)\n\nThank you for the pointer, we were not aware of this relatively recent paper. We have added the paper to the Related Works section in the updated version, however, the paper is still \"work in progress\" as noted by the original authors and was not published at a peer-reviewed venue, thus we will refrain from comparing our results with the mentioned work.\n\nWe argue that the main point of our paper is for controllable summarization and the strong performance on uncontrolled summarization is a side product of CTRLsum. The formulation of a framework that controls summarization through control tokens and quantitatively evaluating the effectiveness in five control dimensions stands as one of the main contributions of this paper.\n\n> Q2: the author's claim, \"keywords and prompts are complementary\", is not evaluated fully.\n\nWe have added new ablation results covering CTRLsum (keyword only), CTRLsum (prompt only), and CTRLsum (keyword + prompt) into Appendix C to provide stronger support to the claim, \u201ckeywords and prompts are complementary\u201d. Please check the paper for details. We summarize several evidence that backs this claim: (1) in the previous results, the BART baseline with prompts performs poorly for entity and length control (e.g. with full article entity success rate < 20%), comparably well to CTRLsum (keyword + prompt) for contribution summarization, greatly worse than CTRLsum (keyword + prompt) on purpose summarization and QA. (2) In the newly added results on entity control, contribution, and QA, using keywords alone is critical for entity control success, using prompts (either alone or with keywords) is important for contribution summarization, using prompts and keywords together is crucial for QA.  \n\nIn conclusion, although for a specific control task one of keywords and prompts might dominate over another, neither keywords nor prompts could succeed alone in all dimensions. Therefore, we think they are complementary for the sake of general and flexible controllable summarization. Hope these new results address your concerns!\n\n> Q3: With respect to contribution summarization, did you evaluate CTRLsum(keyword without prompt) and CTRLsum(prompt without keywords)? \n\nWe added this result in Appendix C, Table 11. You are correct that keywords are not effective for contribution summarization.\n\n> Q4: Did you evaluate the ablation tests with respect to the special token \"|\" and keyword dropout?\n\nWe had some experience with these two model variants in the early stage of this project but could not find these two model checkpoints, thus we cannot provide fully ablation results at this time due to time limitation. However, we did have some impressions about them which hopefully answer your questions a bit: (1) the special token \u201c|\u201d does not matter much and it does not show an obvious correlation with the summary. (2) keyword dropout training does not have evident effect on uncontrolled summarization scores with automatic keywords, but without keyword dropout we observed that the summary demonstrates a stronger correlation with the keywords and weaker dependence on the article. We were worried that this may hurt control since it is often desired that the summarization system can retrieve related information from the article related to the keywords, yet we didn\u2019t do further quantitative evaluation on this.\n\n> Q5: Can CTRLsum control the generation with multiple aspects\n\nThis is an interesting point. In our method, some control aspects can be difficult to be combined with others, for example, the QA and others are hard to combine given that we are using a reading-comprehension-style prompt. Combining entity and length is possible under the CTRLsum model, but that requires a more sophisticated control logic between users and control tokens. For instance, the current control logic of entity control is simply using that entity words as the control tokens, the control logic of length control is to vary the number of automatic keywords as control tokens given user length preference. Such control logic can be more complicated -- for example, identifying the related words for a given entity (such as the words which appear in the same sentences of the entity for simplicity) and selecting K of these words with the highest selection probability by the tagger (K depends on user length input) can probably do the job of combining entity and length. The underlying CTRLsum model keeps unchanged regardless of the test-time control logic. We think such directions are definitely worth exploring in the future for more flexible controllable summarization.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2271/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2271/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CTRLsum: Towards Generic Controllable Text Summarization", "authorids": ["~Junxian_He1", "~Wojciech_Maciej_Kryscinski1", "~Bryan_McCann1", "~Nazneen_Rajani1", "~Caiming_Xiong1"], "authors": ["Junxian He", "Wojciech Maciej Kryscinski", "Bryan McCann", "Nazneen Rajani", "Caiming Xiong"], "keywords": ["controllable text summarization"], "abstract": "Current summarization systems yield generic summaries that are disconnected from users' preferences and expectations. To address this limitation, we present CTRLsum, a novel framework for controllable summarization. Our approach enables users to control multiple aspects of generated summaries by interacting with the summarization system through textual input in the form of a set of keywords or descriptive prompts. Using a single unified model, CTRLsum is able to achieve a broad scope of summary manipulation at inference time without requiring additional human annotations or pre-defining a set of control aspects during training. We quantitatively demonstrate the effectiveness of our approach on three domains of summarization datasets and five control aspects: 1) entity-centric and 2) length-controllable summarization, 3) contribution summarization on scientific papers, 4) invention purpose summarization on patent filings, and 5) question-guided summarization on news articles in a reading comprehension setting. Moreover, when used in a standard, uncontrolled summarization setting, CTRLsum achieves state-of-the-art results on the CNN/DailyMail dataset.", "one-sentence_summary": "We present CTRLsum, a generic framework for controllable summarization that is able to achieve a broad scope of summary manipulation", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "he|ctrlsum_towards_generic_controllable_text_summarization", "pdf": "/pdf/35ef9528b1e138d28d8091c04007052cdc70a5eb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=l4P2vEEBsY", "_bibtex": "@misc{\nhe2021ctrlsum,\ntitle={{\\{}CTRL{\\}}sum: Towards Generic Controllable Text Summarization},\nauthor={Junxian He and Wojciech Maciej Kryscinski and Bryan McCann and Nazneen Rajani and Caiming Xiong},\nyear={2021},\nurl={https://openreview.net/forum?id=ohdw3t-8VCY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ohdw3t-8VCY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2271/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2271/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2271/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2271/Authors|ICLR.cc/2021/Conference/Paper2271/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2271/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850318, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2271/-/Official_Comment"}}}, {"id": "Qo366HRkgq_", "original": null, "number": 2, "cdate": 1603855368568, "ddate": null, "tcdate": 1603855368568, "tmdate": 1605024249701, "tddate": null, "forum": "ohdw3t-8VCY", "replyto": "ohdw3t-8VCY", "invitation": "ICLR.cc/2021/Conference/Paper2271/-/Official_Review", "content": {"title": "A simple but effective method of focusing abstractive summarization models.", "review": "The authors propose an abstractive document summarization model that can\ngenerate summaries that target a specific set of keywords or prompts. This is\nin contrast to generic summarization models that learn to summarize a document\nbut are difficult to control or direct. The authors propose a straightforward\nway of obtaining keywords from an article similar in spirit to Gerhmann et al.\n2018. Alternatively, \"ground truth\" keywords can be found using a reference\nsummary. In either case, the keywords are prepended to the input document and\na BART model is fine-tuned to generate summaries using both the document and\nkeyword content.\n\nThe authors go on to show how a model trained in such a way, which they refer\nto as CTRLsum, can generate entity-focused summaries, by using an entity name\nas the keyword prefix. Additionally, providing differing numbers of keywords\ncan be used to control the length of the generated summary.  The authors also\nshow that CTRLsum can respond sensibly to prompts, i.e. instead of providing\nkeywords, a question or initial phrase is provided.  Useful summarization\nbehavior can be achieved including zero shot question answering, or\nenumeration of a research paper's contributions or the purpose of an\ninvention.\n\nWhile the paper feels largely like an extension of Keskar et al. 2019, the\nevaluation of the proposed methods is very thorough on a variety of settings\nand domains. I especially enjoyed the break out of entity targeted summaries\nbased on whether the entity occurred in the lead and/or reference summary. The\nuse of prompts to obtain question answering and more focused\ncontributions/purpose summarization was also very interesting.\n\nI would be happy to see this paper accepted to ICLR. This paper offers a\nsimple method of obtaining a variety of focused or targeted summarization\nbehaviors from a BART summarization model. In general, I would like to see\nmore work like this exploring methods of controlling pretrained language\nmodels.  The evaluation of the correctness of the generated utterances\nsuggests that this method provides fairly reliable control.\n\nThere several areas where the paper could improve. The explanation of how\nlength control is achieved was not very clear. It would help to have examples\nlike those shown in the appendix present in the section introducing length\ncontrol. \n\nComparisons are to the standard BART model or to Fan et al. (2018) which\nsimilarly prepend important control information to the input. It would be\ninteresting to see an evaluation that compared CTRLsum to BART with a\nconstrained decoding method, such as dynamic beam allocation [1].\n\nAdditionally, the authors should say more about the differences in entity\ncontrol of their method and Fan et al. 2018, which seem on their face to be\nsimilar.\n\nDid the authors experiment with pairs of entities as entity controls? It would\nbe especially interesting to see whether the model preserves the correct\nrelationship between entities, especially for entities that didn't occur in\nthe same sentences in the original document, e.g. one important entity and one\nunimportant entity.\n\n[1] Matt Post and David Vilar. Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation. ACL. 2018.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2271/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2271/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CTRLsum: Towards Generic Controllable Text Summarization", "authorids": ["~Junxian_He1", "~Wojciech_Maciej_Kryscinski1", "~Bryan_McCann1", "~Nazneen_Rajani1", "~Caiming_Xiong1"], "authors": ["Junxian He", "Wojciech Maciej Kryscinski", "Bryan McCann", "Nazneen Rajani", "Caiming Xiong"], "keywords": ["controllable text summarization"], "abstract": "Current summarization systems yield generic summaries that are disconnected from users' preferences and expectations. To address this limitation, we present CTRLsum, a novel framework for controllable summarization. Our approach enables users to control multiple aspects of generated summaries by interacting with the summarization system through textual input in the form of a set of keywords or descriptive prompts. Using a single unified model, CTRLsum is able to achieve a broad scope of summary manipulation at inference time without requiring additional human annotations or pre-defining a set of control aspects during training. We quantitatively demonstrate the effectiveness of our approach on three domains of summarization datasets and five control aspects: 1) entity-centric and 2) length-controllable summarization, 3) contribution summarization on scientific papers, 4) invention purpose summarization on patent filings, and 5) question-guided summarization on news articles in a reading comprehension setting. Moreover, when used in a standard, uncontrolled summarization setting, CTRLsum achieves state-of-the-art results on the CNN/DailyMail dataset.", "one-sentence_summary": "We present CTRLsum, a generic framework for controllable summarization that is able to achieve a broad scope of summary manipulation", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "he|ctrlsum_towards_generic_controllable_text_summarization", "pdf": "/pdf/35ef9528b1e138d28d8091c04007052cdc70a5eb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=l4P2vEEBsY", "_bibtex": "@misc{\nhe2021ctrlsum,\ntitle={{\\{}CTRL{\\}}sum: Towards Generic Controllable Text Summarization},\nauthor={Junxian He and Wojciech Maciej Kryscinski and Bryan McCann and Nazneen Rajani and Caiming Xiong},\nyear={2021},\nurl={https://openreview.net/forum?id=ohdw3t-8VCY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ohdw3t-8VCY", "replyto": "ohdw3t-8VCY", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2271/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538100118, "tmdate": 1606915777636, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2271/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2271/-/Official_Review"}}}, {"id": "gg7-miWPeLQ", "original": null, "number": 3, "cdate": 1603920334670, "ddate": null, "tcdate": 1603920334670, "tmdate": 1605024249639, "tddate": null, "forum": "ohdw3t-8VCY", "replyto": "ohdw3t-8VCY", "invitation": "ICLR.cc/2021/Conference/Paper2271/-/Official_Review", "content": {"title": "Interesting techniques for some summarization tasks, but unclear contribution over Fan et al.", "review": "# Summary:\n\nBuilds/extends on Controllable Abstractive Summarization (Fan et al) using keywords and other prompts. There\u2019s two phases, and both phases are independent:\n\n1. Extract keywords, z,  using a BERT classifier/sequence-tagger trained to predict keywords\n2. Fine-tune BART (Lewis et al) to learn p(summary | document, z).\n\nOne can use automatic keywords using (1) and get uncontrolled generation, for which they present SOTA results on some summarization tasks. Two datasets are collected: (a) intro->contributions from arxiv papers; (b) patent->one-sentence summary, which are used to measure performance of prompts specific to those tasks.\n\n# Pros:\n1. Improves state-of-the-art results on some summarization benchmarks.\n2. Provides BERTScore results in addition to ROUGE.\n3. Results provided across multiple summarization datasets.\n4. Interesting new datasets for measuring document+prompt->summary performance\n5. Interesting zero-shot/transfer results from summarization to Question-answering.\n\n# Cons: \n1. Contribution in methods over Fan et al + BART (Lewis et al) is minimal. Results in Fan et al were weak because the underlying model was much weaker than BART, so it is unclear how much results here improved from simply using BART and adding control tokens from Fan et al, which would be a useful baseline to have that is omitted.\n2. Since the focus of the paper is on controlling generation, more results on this would be informative. It is unclear how well control works when not using oracle words or automatically extracted keywords, i.e. user-controlled. An MTurk experiment evaluating how well control works would be useful in assessing this.\n3. Comparing BART/PEGASUS to BART+BERT-based model is a little unfair since BERT is another large model in the system, i.e. the total amount of compute and number of parameters is much greater. A more fair comparison would be to compare using a smaller BART or PEGASUS model such that the model sizes are comparable. It is unclear whether the proposed system would do better than BART/PEGASUS scaled to the same amount of total parameters.\n4. For the prompt tests, how well does BART/PEGASUS do if the decoder is prompted? This baseline would be useful to have. That is, it\u2019s unclear how the p(y | x, z) improves over using p(y | x) by simply prompting the decoder.\n\n# Clarifications/questions:\n1. For \"CONTRIBUTION AND PURPOSE SUMMARIZATION\", what is the fine-tuning process? Are the models fine-tuned on (paper/patent, keywords)->abstract task before testing on intro->contribution generation? \n2. How are entities randomly selected in the example decodes in the Appendix?\n3. Are any of the prompts used in training or is it zero-shot?\n4. What is zero-shot state-of-the-art on the QA tasks? Please add to Table 5. GPT-3 zero-shot results would also be informative.", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2271/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2271/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CTRLsum: Towards Generic Controllable Text Summarization", "authorids": ["~Junxian_He1", "~Wojciech_Maciej_Kryscinski1", "~Bryan_McCann1", "~Nazneen_Rajani1", "~Caiming_Xiong1"], "authors": ["Junxian He", "Wojciech Maciej Kryscinski", "Bryan McCann", "Nazneen Rajani", "Caiming Xiong"], "keywords": ["controllable text summarization"], "abstract": "Current summarization systems yield generic summaries that are disconnected from users' preferences and expectations. To address this limitation, we present CTRLsum, a novel framework for controllable summarization. Our approach enables users to control multiple aspects of generated summaries by interacting with the summarization system through textual input in the form of a set of keywords or descriptive prompts. Using a single unified model, CTRLsum is able to achieve a broad scope of summary manipulation at inference time without requiring additional human annotations or pre-defining a set of control aspects during training. We quantitatively demonstrate the effectiveness of our approach on three domains of summarization datasets and five control aspects: 1) entity-centric and 2) length-controllable summarization, 3) contribution summarization on scientific papers, 4) invention purpose summarization on patent filings, and 5) question-guided summarization on news articles in a reading comprehension setting. Moreover, when used in a standard, uncontrolled summarization setting, CTRLsum achieves state-of-the-art results on the CNN/DailyMail dataset.", "one-sentence_summary": "We present CTRLsum, a generic framework for controllable summarization that is able to achieve a broad scope of summary manipulation", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "he|ctrlsum_towards_generic_controllable_text_summarization", "pdf": "/pdf/35ef9528b1e138d28d8091c04007052cdc70a5eb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=l4P2vEEBsY", "_bibtex": "@misc{\nhe2021ctrlsum,\ntitle={{\\{}CTRL{\\}}sum: Towards Generic Controllable Text Summarization},\nauthor={Junxian He and Wojciech Maciej Kryscinski and Bryan McCann and Nazneen Rajani and Caiming Xiong},\nyear={2021},\nurl={https://openreview.net/forum?id=ohdw3t-8VCY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ohdw3t-8VCY", "replyto": "ohdw3t-8VCY", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2271/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538100118, "tmdate": 1606915777636, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2271/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2271/-/Official_Review"}}}], "count": 13}