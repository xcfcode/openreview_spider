{"notes": [{"id": "Byg5flHFDr", "original": "rkx_lmlKwr", "number": 2183, "cdate": 1569439761817, "ddate": null, "tcdate": 1569439761817, "tmdate": 1577168274824, "tddate": null, "forum": "Byg5flHFDr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["changmin.wu@polytechnique.edu", "giannisnik@hotmail.com", "mvazirg@lix.polytechnique.fr"], "title": "EvoNet: A Neural Network for Predicting the Evolution of Dynamic Graphs", "authors": ["Changmin Wu", "Giannis Nikolentzos", "Michalis Vazirgiannis"], "pdf": "/pdf/d18adcbbd9667d537eeac3d834c059c183da5bdb.pdf", "TL;DR": "Combining graph neural networks and the RNN graph generative model, we propose a novel architecture that is able to learn from a sequence of evolving graphs and predict the graph topology evolution for the future timesteps", "abstract": "Neural networks for structured data like graphs have been studied extensively in recent years.\nTo date, the bulk of research activity has focused mainly on static graphs.\nHowever, most real-world networks are dynamic since their topology tends to change over time.\nPredicting the evolution of dynamic graphs is a task of high significance in the area of graph mining.\nDespite its practical importance, the task has not been explored in depth so far, mainly due to its challenging nature.\nIn this paper, we propose a model that predicts the evolution of dynamic graphs.\nSpecifically, we use a graph neural network along with a recurrent architecture to capture the temporal evolution patterns of dynamic graphs.\nThen, we employ a generative model which predicts the topology of the graph at the next time step and constructs a graph instance that corresponds to that topology.\nWe evaluate the proposed model on several artificial datasets following common network evolving dynamics, as well as on real-world datasets.\nResults demonstrate the effectiveness of the proposed model. ", "keywords": ["temporal graphs", "graph neural network", "graph generative model", "graph topology prediction"], "paperhash": "wu|evonet_a_neural_network_for_predicting_the_evolution_of_dynamic_graphs", "original_pdf": "/attachment/d18adcbbd9667d537eeac3d834c059c183da5bdb.pdf", "_bibtex": "@misc{\nwu2020evonet,\ntitle={EvoNet: A Neural Network for Predicting the Evolution of Dynamic Graphs},\nauthor={Changmin Wu and Giannis Nikolentzos and Michalis Vazirgiannis},\nyear={2020},\nurl={https://openreview.net/forum?id=Byg5flHFDr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "-jtws4dEnM", "original": null, "number": 1, "cdate": 1576798742631, "ddate": null, "tcdate": 1576798742631, "tmdate": 1576800893594, "tddate": null, "forum": "Byg5flHFDr", "replyto": "Byg5flHFDr", "invitation": "ICLR.cc/2020/Conference/Paper2183/-/Decision", "content": {"decision": "Reject", "comment": "The paper proposes a combination graph neural networks and graph generation model (GraphRNN) to model the evolution of dynamic graphs for predicting the topology of next graph given a sequence of graphs.\n\nThe problem to be addressed seems interesting, but lacks strong motivation. Therefore it would be better if some important applications can be specified.  \n\nThe proposed approach lacks novelty. It would be better to point out why the specific combination of two existing models is the most appropriate approach to address the task. \n\nThe experiments are not fully convincing. Bigger and comprehensive datasets (with the right motivating applications) should be used to test the effectiveness of the proposed model. \n\nIn short, the current version failed to raise excitement from readers due to the reasons above. A major revision addressing these issues could lead to a strong publication in the future. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["changmin.wu@polytechnique.edu", "giannisnik@hotmail.com", "mvazirg@lix.polytechnique.fr"], "title": "EvoNet: A Neural Network for Predicting the Evolution of Dynamic Graphs", "authors": ["Changmin Wu", "Giannis Nikolentzos", "Michalis Vazirgiannis"], "pdf": "/pdf/d18adcbbd9667d537eeac3d834c059c183da5bdb.pdf", "TL;DR": "Combining graph neural networks and the RNN graph generative model, we propose a novel architecture that is able to learn from a sequence of evolving graphs and predict the graph topology evolution for the future timesteps", "abstract": "Neural networks for structured data like graphs have been studied extensively in recent years.\nTo date, the bulk of research activity has focused mainly on static graphs.\nHowever, most real-world networks are dynamic since their topology tends to change over time.\nPredicting the evolution of dynamic graphs is a task of high significance in the area of graph mining.\nDespite its practical importance, the task has not been explored in depth so far, mainly due to its challenging nature.\nIn this paper, we propose a model that predicts the evolution of dynamic graphs.\nSpecifically, we use a graph neural network along with a recurrent architecture to capture the temporal evolution patterns of dynamic graphs.\nThen, we employ a generative model which predicts the topology of the graph at the next time step and constructs a graph instance that corresponds to that topology.\nWe evaluate the proposed model on several artificial datasets following common network evolving dynamics, as well as on real-world datasets.\nResults demonstrate the effectiveness of the proposed model. ", "keywords": ["temporal graphs", "graph neural network", "graph generative model", "graph topology prediction"], "paperhash": "wu|evonet_a_neural_network_for_predicting_the_evolution_of_dynamic_graphs", "original_pdf": "/attachment/d18adcbbd9667d537eeac3d834c059c183da5bdb.pdf", "_bibtex": "@misc{\nwu2020evonet,\ntitle={EvoNet: A Neural Network for Predicting the Evolution of Dynamic Graphs},\nauthor={Changmin Wu and Giannis Nikolentzos and Michalis Vazirgiannis},\nyear={2020},\nurl={https://openreview.net/forum?id=Byg5flHFDr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Byg5flHFDr", "replyto": "Byg5flHFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795725343, "tmdate": 1576800277207, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2183/-/Decision"}}}, {"id": "Bylnso_aKH", "original": null, "number": 1, "cdate": 1571814308193, "ddate": null, "tcdate": 1571814308193, "tmdate": 1572972372257, "tddate": null, "forum": "Byg5flHFDr", "replyto": "Byg5flHFDr", "invitation": "ICLR.cc/2020/Conference/Paper2183/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a system for predicting evolution of graphs. It makes use of three different known components - (a) Graph Neural Networks (GNN); (b) Recurrent Neural Networks (RNN); (c) Graph Generator. A significant portion of the paper is spent in explaining these known concepts. The contribution of the paper seems to be a system of combining these to achieve graph evolution prediction. As stated, this system is effectively a recurrent auto-encoder of sorts. \n\nThe main objection I have in this paper is that they have only used two real datasets (both of which are from the same domain). There are several only available datasets that have temporally annotated graph evolution. It is not possible to conclude the empirical superiority of a system based on such little evidence. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2183/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2183/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["changmin.wu@polytechnique.edu", "giannisnik@hotmail.com", "mvazirg@lix.polytechnique.fr"], "title": "EvoNet: A Neural Network for Predicting the Evolution of Dynamic Graphs", "authors": ["Changmin Wu", "Giannis Nikolentzos", "Michalis Vazirgiannis"], "pdf": "/pdf/d18adcbbd9667d537eeac3d834c059c183da5bdb.pdf", "TL;DR": "Combining graph neural networks and the RNN graph generative model, we propose a novel architecture that is able to learn from a sequence of evolving graphs and predict the graph topology evolution for the future timesteps", "abstract": "Neural networks for structured data like graphs have been studied extensively in recent years.\nTo date, the bulk of research activity has focused mainly on static graphs.\nHowever, most real-world networks are dynamic since their topology tends to change over time.\nPredicting the evolution of dynamic graphs is a task of high significance in the area of graph mining.\nDespite its practical importance, the task has not been explored in depth so far, mainly due to its challenging nature.\nIn this paper, we propose a model that predicts the evolution of dynamic graphs.\nSpecifically, we use a graph neural network along with a recurrent architecture to capture the temporal evolution patterns of dynamic graphs.\nThen, we employ a generative model which predicts the topology of the graph at the next time step and constructs a graph instance that corresponds to that topology.\nWe evaluate the proposed model on several artificial datasets following common network evolving dynamics, as well as on real-world datasets.\nResults demonstrate the effectiveness of the proposed model. ", "keywords": ["temporal graphs", "graph neural network", "graph generative model", "graph topology prediction"], "paperhash": "wu|evonet_a_neural_network_for_predicting_the_evolution_of_dynamic_graphs", "original_pdf": "/attachment/d18adcbbd9667d537eeac3d834c059c183da5bdb.pdf", "_bibtex": "@misc{\nwu2020evonet,\ntitle={EvoNet: A Neural Network for Predicting the Evolution of Dynamic Graphs},\nauthor={Changmin Wu and Giannis Nikolentzos and Michalis Vazirgiannis},\nyear={2020},\nurl={https://openreview.net/forum?id=Byg5flHFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Byg5flHFDr", "replyto": "Byg5flHFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2183/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2183/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575744130177, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2183/Reviewers"], "noninvitees": [], "tcdate": 1570237726512, "tmdate": 1575744130192, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2183/-/Official_Review"}}}, {"id": "rJl6gxxRYB", "original": null, "number": 2, "cdate": 1571844084725, "ddate": null, "tcdate": 1571844084725, "tmdate": 1572972372213, "tddate": null, "forum": "Byg5flHFDr", "replyto": "Byg5flHFDr", "invitation": "ICLR.cc/2020/Conference/Paper2183/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, the authors propose a new neural network architecture for predicting the next graph conditioned on a past graph sequence. It seems that the proposed model is the first deep learning model for graph sequence prediction. The model consists of three major components: a graph encoder that maps a graph to an encoding represented as a vector, an LSTM for graph sequence embedding, and a graph decoder for generating an affinity matrix. \n\nThere are two main concerns I have with this paper:\n- The model has some inherent limitations in the graph embedding step. First, the graph encoder embeds a graph into a feature vector that represents the topology of the graph. I assume that the feature vector has a small size, and it is hard to encode a large graph (i.e. 1000x1000). This representation is quite sub-optimal to me. The model will not be able to utilize the complete information of a large dense graph.  Second, the model only takes 10 graphs as input and ignores other graphs in the input graph sequences. This sounds suboptimal to me. \n\n- The performance of the proposed model is not satisfactory. The model does not output a graph with the right size for very simple synthetic graphs. The model completely fails for generating circles. A better model should be proposed to address this challenge. Evaluation is not convincing enough. Simply comparing the graph size between the output and ground truth is not sufficient. We can further predict where graph structure matches the ground truth exactly. I believe this can be done for simple graphs like circles, paths, and ladders.\n\nOther comments:\n- The authors claim that all the sequences in the datasets are fixed to 1000. However, in Figure 4, the graph index goes up to 1600.  Why?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2183/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2183/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["changmin.wu@polytechnique.edu", "giannisnik@hotmail.com", "mvazirg@lix.polytechnique.fr"], "title": "EvoNet: A Neural Network for Predicting the Evolution of Dynamic Graphs", "authors": ["Changmin Wu", "Giannis Nikolentzos", "Michalis Vazirgiannis"], "pdf": "/pdf/d18adcbbd9667d537eeac3d834c059c183da5bdb.pdf", "TL;DR": "Combining graph neural networks and the RNN graph generative model, we propose a novel architecture that is able to learn from a sequence of evolving graphs and predict the graph topology evolution for the future timesteps", "abstract": "Neural networks for structured data like graphs have been studied extensively in recent years.\nTo date, the bulk of research activity has focused mainly on static graphs.\nHowever, most real-world networks are dynamic since their topology tends to change over time.\nPredicting the evolution of dynamic graphs is a task of high significance in the area of graph mining.\nDespite its practical importance, the task has not been explored in depth so far, mainly due to its challenging nature.\nIn this paper, we propose a model that predicts the evolution of dynamic graphs.\nSpecifically, we use a graph neural network along with a recurrent architecture to capture the temporal evolution patterns of dynamic graphs.\nThen, we employ a generative model which predicts the topology of the graph at the next time step and constructs a graph instance that corresponds to that topology.\nWe evaluate the proposed model on several artificial datasets following common network evolving dynamics, as well as on real-world datasets.\nResults demonstrate the effectiveness of the proposed model. ", "keywords": ["temporal graphs", "graph neural network", "graph generative model", "graph topology prediction"], "paperhash": "wu|evonet_a_neural_network_for_predicting_the_evolution_of_dynamic_graphs", "original_pdf": "/attachment/d18adcbbd9667d537eeac3d834c059c183da5bdb.pdf", "_bibtex": "@misc{\nwu2020evonet,\ntitle={EvoNet: A Neural Network for Predicting the Evolution of Dynamic Graphs},\nauthor={Changmin Wu and Giannis Nikolentzos and Michalis Vazirgiannis},\nyear={2020},\nurl={https://openreview.net/forum?id=Byg5flHFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Byg5flHFDr", "replyto": "Byg5flHFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2183/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2183/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575744130177, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2183/Reviewers"], "noninvitees": [], "tcdate": 1570237726512, "tmdate": 1575744130192, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2183/-/Official_Review"}}}, {"id": "HylezQvAKB", "original": null, "number": 3, "cdate": 1571873544365, "ddate": null, "tcdate": 1571873544365, "tmdate": 1572972372165, "tddate": null, "forum": "Byg5flHFDr", "replyto": "Byg5flHFDr", "invitation": "ICLR.cc/2020/Conference/Paper2183/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes a framework to model the evolution of dynamic graphs for the task of predicting the topology of next graph given a sequence of graphs. Specifically, the paper uses a combination of recently proposed techniques in graph representation learning (Graph Neural Network) and Graph Generation (GraphRNN [You et. al. 2018]). Given a sequence of graphs as input, a GNN (to obtain low-dimensional representations of the graphs in this sequence) and LSTM (to model the sequence of these representations)  based encoder is used to compute a vector representation of the topology of next graph in the sequence. The learned vector is then used as input to a GraphRNN decoder to generate a graph that would serve as a predicted next graph in the sequence. The proposed approach is validated with experiments on three synthetic datasets and one real-world dataset (Bitcoin is same dataset from two different resources with little difference in characteristics) and compared against random graph models.\n\nThis paper should be rejected due to following reasons: \n(1) The authors do not justify/discuss the motivation and importance of the task and corresponding applications that would require to predict topology of complete graph in the next step. \n(2) The proposed techniques are an adhoc combination of existing techniques with major concerns (details below) but also with little novelty (if any) for achieving this combination.\n(3) The empirical efforts are very limited and does not provide enough evidence about the efficacy of the method, miss several details and does not serve as motivation for designing such a method in first place. Please note that negative results on cycle graphs has no role to play in this assessment. In fact,\nI appreciate the authors for reporting negative results as it provides a transparent insights into the effectiveness of model in different settings.\nOverall, the paper needs lot of work on all aspects - motivation, technique and experiments to make it fit for a conference publication.\n\nMajor Concerns:\n\n(a) Motivation: The authors do not discuss or motivate the problem and why it is important to the community. The authors mention that many existing work on dynamic graphs focus on learning representations. This is the case because learned representations can then be used for various downstream applications and even future event predictions. When such methods can be used to do future predictions required for most applications, why does one need to predict the topology of complete next graph? The authors need to provide concrete justification for the problem they address, instances where such a task would be useful and discussion on other techniques that can do similar tasks but lack in aspects that such a method can capture. For instance, as a preliminary step, can the authors explain how solving this problem would be helpful to bitcoin?\n\n(b) Technical: The technical contributions of this paper lack novelty and has several flows:\n\n- Figure 1 seems to show that graph only grows in size. While the authors do provide an experiment with removal process, that experiment does not seem to perform well. So, does the method is only good to support growing graphs?\n- Authors mention that the edge and node attributes are considered to be fixed. However, if the number of nodes and edges change, X and L should also change in terms of dimensions and adding values for new nodes/edges. so why should it not be considered time-varying?\n- What was the motivation for using GRU for update function in Eq 3? Was simple MLP tried and not useful? Was GRU used to capture some long term dependencies in structure? If so, the authors must explain how it is useful for this task.\n- Why Set2Set was used for ReadOut function? This seems to be a particularly adhoc and odd choice. when sum did not work well, jump to Set2Set is not justified. Can the authors provide an explanation for the same?\n- The authors claim that the embedding h_G_T incorporates topological information -- I find this claim highly unsubstantiated and needs justification. For instance, can you provide some rigorous analysis to demonstrate that this is the case? At the least, can the authors use this vector and pass it through a graph decoder to recover the original graph? \n- What is novel in 3.2.3 as compared to You et. al.? Infact, it is hard to see any novelty in the entire combination. Was it challenging to achieve this combination? If so, what was the challenging part? It is not clear what the authors contributed to address such a challenge. Was the training challenging? If so, please explain. If not, why is this a novel approach?\n\n(c) Empirical: The empirical efforts are inadequate and raises more questions than answers. \n\n- Synthetic datasets are simple and more datasets should be used e.g. You et. al. 2018 to validate the performance. Only one real-world dataset from two different sources is used. It is hard to understand author's motivation in doing so. Why not use various graph datasets available in papers that learn representations (e.g. cited by authors themselves) What is special about bitcoin dataset that makes it suitable for this task?\n\n- Node/edge attributes are chosen in adhoc manner and it is unclear what role they perform. Do they help with prediction? If not, would it be useful to first show experiments without them? Or does this method absolutely need attributes? It is not clear why it is useful to set all attributes for edge as 1.\n\n- How was window size of 10 chosen? Why is the same window size good for all graphs? What impact does window size has on performance?\n\n- What is the motivation for using Graph kernel for similarity? The authors borrow the decoder from You et. al. 2018 which also provides a principled method to compare graphs using MMD based on statistics. Why not employ the same?\n\n- GraphRNN (You et. al.) and other generative models can learn over multiple graphs? Did the authors try to feed the sequence of graphs to such models and then try to generate a new graph to see if they can produce similar results? It is true that those generative models do not specifically model temporal sequence, but such an experiment would help to distinguish the efficacy of the proposed method.\n\n-The technique of using MLP for generating predictions using random graph models seem to be highly unfair for the baselines. Can you elaborate more as it is difficult to understand why one should handicap those models by using learned information instead of data information?\n\n- A rigorous discussion on insights explaining the results is required. The authors show high performance on Bitcoin dataset. However,  it is not clear what part is contributing to the performance. Similarly, authors should dig deep into the failure cases and provide justification of why such a method would fail in particular cases and propose alternatives.\n\n- Why was Graph size used as a statistic to report? Two graphs of same size can be entirely different and I do not see any merit in using such a metric. Again, something like MMD based metric may be useful.\n\n\nImprovements that would make future revision strong but has not impacted current assessment:\n\nOverall, the presentation of the paper is very unpolished. The authors are missing many important details as described above while spending a lot of time in describing (repeating) known techniques verbatim as original works. This can be removed and condensed into very short preliminary section.\n\n- Notations: The authors must use clear notations. For instance, on Page 2, L is used to  describe edge attributes but then it is replaced by E in Page 3. Also, both X and L are shown to have dimension d. Are edge and node attributes of same dimension? w is used for window-size of sequence used as input and also as neighbor node. When modeling evolution of graphs where a sequence is available over time points 0...T, it is not useful to use T to also represent time step of GNN propagation. Infact, authors should avoid using time steps to signify GNN iterations.\n\n- Empirical details: The details provided for datasets and experimental setup is inadequate. Why are the two Bitcoin datasets different from each other? What does Pos. Edges in Table 1 mean? What does  Mean and 90th percentile in Table 2 signify? Authors only talk about train-test split but then mention\nvalidation set for hyper-param tuning. How was this validation set obtained? Also, what hyper-params were tuned and what was sensitivity of those hyper-params? Authors use GNN and multiple RNN's, what was the model capacity used and how it impacted the performance? Figure 5 (c) what is a circle graph?"}, "signatures": ["ICLR.cc/2020/Conference/Paper2183/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2183/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["changmin.wu@polytechnique.edu", "giannisnik@hotmail.com", "mvazirg@lix.polytechnique.fr"], "title": "EvoNet: A Neural Network for Predicting the Evolution of Dynamic Graphs", "authors": ["Changmin Wu", "Giannis Nikolentzos", "Michalis Vazirgiannis"], "pdf": "/pdf/d18adcbbd9667d537eeac3d834c059c183da5bdb.pdf", "TL;DR": "Combining graph neural networks and the RNN graph generative model, we propose a novel architecture that is able to learn from a sequence of evolving graphs and predict the graph topology evolution for the future timesteps", "abstract": "Neural networks for structured data like graphs have been studied extensively in recent years.\nTo date, the bulk of research activity has focused mainly on static graphs.\nHowever, most real-world networks are dynamic since their topology tends to change over time.\nPredicting the evolution of dynamic graphs is a task of high significance in the area of graph mining.\nDespite its practical importance, the task has not been explored in depth so far, mainly due to its challenging nature.\nIn this paper, we propose a model that predicts the evolution of dynamic graphs.\nSpecifically, we use a graph neural network along with a recurrent architecture to capture the temporal evolution patterns of dynamic graphs.\nThen, we employ a generative model which predicts the topology of the graph at the next time step and constructs a graph instance that corresponds to that topology.\nWe evaluate the proposed model on several artificial datasets following common network evolving dynamics, as well as on real-world datasets.\nResults demonstrate the effectiveness of the proposed model. ", "keywords": ["temporal graphs", "graph neural network", "graph generative model", "graph topology prediction"], "paperhash": "wu|evonet_a_neural_network_for_predicting_the_evolution_of_dynamic_graphs", "original_pdf": "/attachment/d18adcbbd9667d537eeac3d834c059c183da5bdb.pdf", "_bibtex": "@misc{\nwu2020evonet,\ntitle={EvoNet: A Neural Network for Predicting the Evolution of Dynamic Graphs},\nauthor={Changmin Wu and Giannis Nikolentzos and Michalis Vazirgiannis},\nyear={2020},\nurl={https://openreview.net/forum?id=Byg5flHFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Byg5flHFDr", "replyto": "Byg5flHFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2183/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2183/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575744130177, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2183/Reviewers"], "noninvitees": [], "tcdate": 1570237726512, "tmdate": 1575744130192, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2183/-/Official_Review"}}}], "count": 5}