{"notes": [{"id": "Pd_oMxH8IlF", "original": "3ukXfSBr8L6", "number": 2705, "cdate": 1601308299764, "ddate": null, "tcdate": 1601308299764, "tmdate": 1616022770952, "tddate": null, "forum": "Pd_oMxH8IlF", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Iterated learning for emergent systematicity in VQA", "authorids": ["~Ankit_Vani1", "~Max_Schwarzer1", "~Yuchen_Lu1", "eeshandhekane@gmail.com", "~Aaron_Courville3"], "authors": ["Ankit Vani", "Max Schwarzer", "Yuchen Lu", "Eeshan Dhekane", "Aaron Courville"], "keywords": ["iterated learning", "cultural transmission", "neural module network", "clevr", "shapes", "vqa", "visual question answering", "systematic generalization", "compositionality"], "abstract": "Although neural module networks have an architectural bias towards compositionality, they require gold standard layouts to generalize systematically in practice. When instead learning layouts and modules jointly, compositionality does not arise automatically and an explicit pressure is necessary for the emergence of layouts exhibiting the right structure. We propose to address this problem using iterated learning, a cognitive science theory of the emergence of compositional languages in nature that has primarily been applied to simple referential games in machine learning. Considering the layouts of module networks as samples from an emergent language, we use iterated learning to encourage the development of structure within this language. We show that the resulting layouts support systematic generalization in neural agents solving the more complex task of visual question-answering. Our regularized iterated learning method can outperform baselines without iterated learning on SHAPES-SyGeT (SHAPES Systematic Generalization Test), a new split of the SHAPES dataset we introduce to evaluate systematic generalization, and on CLOSURE, an extension of CLEVR also designed to test systematic generalization. We demonstrate superior performance in recovering ground-truth compositional program structure with limited supervision on both SHAPES-SyGeT and CLEVR.", "one-sentence_summary": "We use iterated learning to encourage the emergence of structure in the generated programs for neural module networks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vani|iterated_learning_for_emergent_systematicity_in_vqa", "supplementary_material": "", "pdf": "/pdf/62bee9dfb73bae4271c7f80e9d64eda7effacc43.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvani2021iterated,\ntitle={Iterated learning for emergent systematicity in {\\{}VQA{\\}}},\nauthor={Ankit Vani and Max Schwarzer and Yuchen Lu and Eeshan Dhekane and Aaron Courville},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Pd_oMxH8IlF}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "rUPIQH6EFC", "original": null, "number": 1, "cdate": 1610040369076, "ddate": null, "tcdate": 1610040369076, "tmdate": 1610473960247, "tddate": null, "forum": "Pd_oMxH8IlF", "replyto": "Pd_oMxH8IlF", "invitation": "ICLR.cc/2021/Conference/Paper2705/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Oral)", "comment": "This paper presents an original perspective on how to learn layouts and modules of neural module networks jointly, in a way that encourages the emergence of compositional solutions. In particular, layouts are treated as messages from an emergent language, and iterated learning is used to encourage the emergence of structure. The paper shows good performance in inducing compositional structure in two datasets.\n\nSummarizing the reviewers' doubts, one is that the idea is tested on relatively toyish data sets, and it is not clear how it would scale up. The second, coming from one reviewer, concerns a lack of originality that, honestly, I do not see. If anything, this is probably the most original paper in my pool.\n\nConcerning the first point, that is a fair objection, but I think that getting good results on program learning on datasets such as CLEVER is more than encouraging for a paper that is introducing quite a novel idea for the first time.\n\nFinally, the authors added new text and new experiments strenghtening their conclusion during the discussion.\n\nI am strongly in favour of accepting this paper.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterated learning for emergent systematicity in VQA", "authorids": ["~Ankit_Vani1", "~Max_Schwarzer1", "~Yuchen_Lu1", "eeshandhekane@gmail.com", "~Aaron_Courville3"], "authors": ["Ankit Vani", "Max Schwarzer", "Yuchen Lu", "Eeshan Dhekane", "Aaron Courville"], "keywords": ["iterated learning", "cultural transmission", "neural module network", "clevr", "shapes", "vqa", "visual question answering", "systematic generalization", "compositionality"], "abstract": "Although neural module networks have an architectural bias towards compositionality, they require gold standard layouts to generalize systematically in practice. When instead learning layouts and modules jointly, compositionality does not arise automatically and an explicit pressure is necessary for the emergence of layouts exhibiting the right structure. We propose to address this problem using iterated learning, a cognitive science theory of the emergence of compositional languages in nature that has primarily been applied to simple referential games in machine learning. Considering the layouts of module networks as samples from an emergent language, we use iterated learning to encourage the development of structure within this language. We show that the resulting layouts support systematic generalization in neural agents solving the more complex task of visual question-answering. Our regularized iterated learning method can outperform baselines without iterated learning on SHAPES-SyGeT (SHAPES Systematic Generalization Test), a new split of the SHAPES dataset we introduce to evaluate systematic generalization, and on CLOSURE, an extension of CLEVR also designed to test systematic generalization. We demonstrate superior performance in recovering ground-truth compositional program structure with limited supervision on both SHAPES-SyGeT and CLEVR.", "one-sentence_summary": "We use iterated learning to encourage the emergence of structure in the generated programs for neural module networks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vani|iterated_learning_for_emergent_systematicity_in_vqa", "supplementary_material": "", "pdf": "/pdf/62bee9dfb73bae4271c7f80e9d64eda7effacc43.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvani2021iterated,\ntitle={Iterated learning for emergent systematicity in {\\{}VQA{\\}}},\nauthor={Ankit Vani and Max Schwarzer and Yuchen Lu and Eeshan Dhekane and Aaron Courville},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Pd_oMxH8IlF}\n}"}, "tags": [], "invitation": {"reply": {"forum": "Pd_oMxH8IlF", "replyto": "Pd_oMxH8IlF", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040369061, "tmdate": 1610473960229, "id": "ICLR.cc/2021/Conference/Paper2705/-/Decision"}}}, {"id": "X2W0yhGNtsm", "original": null, "number": 3, "cdate": 1604751756805, "ddate": null, "tcdate": 1604751756805, "tmdate": 1606498219667, "tddate": null, "forum": "Pd_oMxH8IlF", "replyto": "Pd_oMxH8IlF", "invitation": "ICLR.cc/2021/Conference/Paper2705/-/Official_Review", "content": {"title": "Revised review", "review": "Review:\nThe authors address methods to encourage the emergence of the layout expression structures on the frameworks of neural module networks (NMN) for the visual QA problems. The methods are motivated from the works on language emergence for communication between multi-agents and the language acquisition of new-born babies from parents, which achieved with limited data. The methods, \u2018iterative learning\u2019 (IL) are designed as forming two agents (program generators and execution engines) to play VQA games. Basic architectures and learning methods seem to be very similar to the approach of semi-supervised learning introduced in [ICCV17]. \nThis paper deals with one of very interesting topics, the language emergence among cooperative multi-agent environments and the compositionality of human language as recent related studies are well-surveyed in related work. In particular, the main idea of problem formation, layout expressions in NMN as emergent languages is very fresh and interesting.\nThe main claims are as follows: (1) the proposed approach of IL improves generalization performance for visual QA, and it is shown experimentally by comparing the ablation results of IL. (2) the language structures in the ground-truth data are recovered with only limited supervisions and the superiority is validated on two datasets \u2013 SHAPES-SyGeT and CLOSURE. However, I believe that the evidences for their claims are insufficient. Specifically, the authors do not provide enough information of language structure such as the superiority compared to other methods and the structure similarity of recovery levels.\nI recommend 'ok, but not good enough \u2013 reject\u2019 for this paper. \n\n\nPros:\n- The authors propose novel interesting problem and their solutions. Arguably, it seems potentially to be on one of important research flows to make influence to lots of works for academia in the future.\n- They find and report good performance for out-of-distribution accuracies for visual QA datasets.\n\nConcerns:\n- It is not clear which parts in the proposed methods are novel with respect to previous works. Those make vague which parts are the authors\u2019 contributions.\n- I think IL should be clarified from semi-supervised learning approaches on them for visual QA. Also, for reproducibility, it should be specified which parts are with/without IL. What is \u2018learning bottleneck\u2019 on this approach? Also, it is not enough how program generators and execution engines are specified, even though some explanations are in appendix. I think it needs more links for reference or explanation.\n- As mentioned above, the supports for main claims are not appropriate or unclear. It needs to theoretically or experimentally show the results of comparison with other methods and the similarity of the recovery level of language structures.\n- Table 1 reports the result of comparative methods such as MAC and FiLM without program supervision. How are they configured in the experiments?\n- I think that it would be better understandable to show usability and superiority with the experimental results on realistic visual QA such as VQA and GQA.\n\nMinors:\n- In Section 3.1,  \u03b3. -> \u03b3,  \u03b2. -> \u03b2\n- It needs the description for operators and symbols for the formulae in Section 3.1\n\n[ICCV17] Johnson et al., Inferring and executing programs for visual reasoning, ICCV 2017.\n\n------------------------------------------------------------------------------------------------------------------------------\nAfter rebuttal\n\nFrom the revised version of this manuscript, the authors resolve my major concerns such as clarity/reproducibility of the method, differentiation from the previous works including semi-supervised learning, and scalability. So, I've raised my score to 6. \nThank you for the contributions!\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2705/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2705/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterated learning for emergent systematicity in VQA", "authorids": ["~Ankit_Vani1", "~Max_Schwarzer1", "~Yuchen_Lu1", "eeshandhekane@gmail.com", "~Aaron_Courville3"], "authors": ["Ankit Vani", "Max Schwarzer", "Yuchen Lu", "Eeshan Dhekane", "Aaron Courville"], "keywords": ["iterated learning", "cultural transmission", "neural module network", "clevr", "shapes", "vqa", "visual question answering", "systematic generalization", "compositionality"], "abstract": "Although neural module networks have an architectural bias towards compositionality, they require gold standard layouts to generalize systematically in practice. When instead learning layouts and modules jointly, compositionality does not arise automatically and an explicit pressure is necessary for the emergence of layouts exhibiting the right structure. We propose to address this problem using iterated learning, a cognitive science theory of the emergence of compositional languages in nature that has primarily been applied to simple referential games in machine learning. Considering the layouts of module networks as samples from an emergent language, we use iterated learning to encourage the development of structure within this language. We show that the resulting layouts support systematic generalization in neural agents solving the more complex task of visual question-answering. Our regularized iterated learning method can outperform baselines without iterated learning on SHAPES-SyGeT (SHAPES Systematic Generalization Test), a new split of the SHAPES dataset we introduce to evaluate systematic generalization, and on CLOSURE, an extension of CLEVR also designed to test systematic generalization. We demonstrate superior performance in recovering ground-truth compositional program structure with limited supervision on both SHAPES-SyGeT and CLEVR.", "one-sentence_summary": "We use iterated learning to encourage the emergence of structure in the generated programs for neural module networks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vani|iterated_learning_for_emergent_systematicity_in_vqa", "supplementary_material": "", "pdf": "/pdf/62bee9dfb73bae4271c7f80e9d64eda7effacc43.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvani2021iterated,\ntitle={Iterated learning for emergent systematicity in {\\{}VQA{\\}}},\nauthor={Ankit Vani and Max Schwarzer and Yuchen Lu and Eeshan Dhekane and Aaron Courville},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Pd_oMxH8IlF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Pd_oMxH8IlF", "replyto": "Pd_oMxH8IlF", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2705/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538090330, "tmdate": 1606915763777, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2705/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2705/-/Official_Review"}}}, {"id": "CYlURuY6hhS", "original": null, "number": 16, "cdate": 1606150509800, "ddate": null, "tcdate": 1606150509800, "tmdate": 1606180472282, "tddate": null, "forum": "Pd_oMxH8IlF", "replyto": "u-8AszcfBc6", "invitation": "ICLR.cc/2021/Conference/Paper2705/-/Official_Comment", "content": {"title": "Clarification on the \"iterated\" part of iterated learning", "comment": "We realize that due to the lack of a formal algorithm in our paper that there could be a misunderstanding regarding what is \"iterated\" in iterated learning (IL). Just in case, we clarify it here.\n\nOur IL approach follows four phases *in a single iteration*, and these phases are repeated many times during training. For brevity, we denote the interacting phase as P1, transmitting phase as P2, PG learning phase as P3, and EE learning phase as P4. Then, the **training of an IL model** goes as follows:  \nP1 -> P2 -> P3 -> P4 -> P1 -> P2 -> P3 -> P4 -> P1 -> ...\n\nIn contrast, the **training of a model without IL** proceeds through one long interacting phase. Viewed another way, it can be seen as:  \nP1 -> P1 -> P1 -> ...\n\nEvery time a PG learning phase is executed, a new program generator (PG) is initialized to represent a new generation. Similarly, a new execution engine (EE) is created in EE learning phases (it can either be initialized from scratch or be a clone of a previous EE).\n\nWe will add a formal algorithm describing our approach in the appendix."}, "signatures": ["ICLR.cc/2021/Conference/Paper2705/Authors"], "readers": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2705/Area_Chairs", "ICLR.cc/2021/Conference/Paper2705/Reviewers", "ICLR.cc/2021/Conference/Paper2705/Authors", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2705/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterated learning for emergent systematicity in VQA", "authorids": ["~Ankit_Vani1", "~Max_Schwarzer1", "~Yuchen_Lu1", "eeshandhekane@gmail.com", "~Aaron_Courville3"], "authors": ["Ankit Vani", "Max Schwarzer", "Yuchen Lu", "Eeshan Dhekane", "Aaron Courville"], "keywords": ["iterated learning", "cultural transmission", "neural module network", "clevr", "shapes", "vqa", "visual question answering", "systematic generalization", "compositionality"], "abstract": "Although neural module networks have an architectural bias towards compositionality, they require gold standard layouts to generalize systematically in practice. When instead learning layouts and modules jointly, compositionality does not arise automatically and an explicit pressure is necessary for the emergence of layouts exhibiting the right structure. We propose to address this problem using iterated learning, a cognitive science theory of the emergence of compositional languages in nature that has primarily been applied to simple referential games in machine learning. Considering the layouts of module networks as samples from an emergent language, we use iterated learning to encourage the development of structure within this language. We show that the resulting layouts support systematic generalization in neural agents solving the more complex task of visual question-answering. Our regularized iterated learning method can outperform baselines without iterated learning on SHAPES-SyGeT (SHAPES Systematic Generalization Test), a new split of the SHAPES dataset we introduce to evaluate systematic generalization, and on CLOSURE, an extension of CLEVR also designed to test systematic generalization. We demonstrate superior performance in recovering ground-truth compositional program structure with limited supervision on both SHAPES-SyGeT and CLEVR.", "one-sentence_summary": "We use iterated learning to encourage the emergence of structure in the generated programs for neural module networks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vani|iterated_learning_for_emergent_systematicity_in_vqa", "supplementary_material": "", "pdf": "/pdf/62bee9dfb73bae4271c7f80e9d64eda7effacc43.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvani2021iterated,\ntitle={Iterated learning for emergent systematicity in {\\{}VQA{\\}}},\nauthor={Ankit Vani and Max Schwarzer and Yuchen Lu and Eeshan Dhekane and Aaron Courville},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Pd_oMxH8IlF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Pd_oMxH8IlF", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2705/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2705/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2705/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2705/Authors|ICLR.cc/2021/Conference/Paper2705/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2705/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845319, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2705/-/Official_Comment"}}}, {"id": "4f4jRzn-fRU", "original": null, "number": 17, "cdate": 1606156050861, "ddate": null, "tcdate": 1606156050861, "tmdate": 1606156050861, "tddate": null, "forum": "Pd_oMxH8IlF", "replyto": "rJtgDxcpiKA", "invitation": "ICLR.cc/2021/Conference/Paper2705/-/Official_Comment", "content": {"title": "Thank you for the response!", "comment": "Thank you for the responses! I think it would be nice for the paper to use different language than \"toy\", purely because I worry that some readers will view the paper negatively on the grounds that SHAPES and CLEVR still are toy-ish (due to the subjective nature of the definition of \"toy\", as you noted). So I would recommend using different terms that are more objective, to prevent readers from misreading the paper in that way. (E.g., you could say that you have applied IL to datasets that are substantially more complex than those used in prior work - \"substantially more complex\" seems a fair statement to me, as it makes not claim that SHAPES and CLEVR are not toy datasets).\n\nBy the way, I very much support the use of synthetic datasets, as they are more controlled and therefore allow for more rigorous experiments. The only concern I had was that the current phrasing might make the paper seem like it is overclaiming.\n\nAnyway, this is a small enough issue that it has not affected the score I assigned to the paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper2705/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2705/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterated learning for emergent systematicity in VQA", "authorids": ["~Ankit_Vani1", "~Max_Schwarzer1", "~Yuchen_Lu1", "eeshandhekane@gmail.com", "~Aaron_Courville3"], "authors": ["Ankit Vani", "Max Schwarzer", "Yuchen Lu", "Eeshan Dhekane", "Aaron Courville"], "keywords": ["iterated learning", "cultural transmission", "neural module network", "clevr", "shapes", "vqa", "visual question answering", "systematic generalization", "compositionality"], "abstract": "Although neural module networks have an architectural bias towards compositionality, they require gold standard layouts to generalize systematically in practice. When instead learning layouts and modules jointly, compositionality does not arise automatically and an explicit pressure is necessary for the emergence of layouts exhibiting the right structure. We propose to address this problem using iterated learning, a cognitive science theory of the emergence of compositional languages in nature that has primarily been applied to simple referential games in machine learning. Considering the layouts of module networks as samples from an emergent language, we use iterated learning to encourage the development of structure within this language. We show that the resulting layouts support systematic generalization in neural agents solving the more complex task of visual question-answering. Our regularized iterated learning method can outperform baselines without iterated learning on SHAPES-SyGeT (SHAPES Systematic Generalization Test), a new split of the SHAPES dataset we introduce to evaluate systematic generalization, and on CLOSURE, an extension of CLEVR also designed to test systematic generalization. We demonstrate superior performance in recovering ground-truth compositional program structure with limited supervision on both SHAPES-SyGeT and CLEVR.", "one-sentence_summary": "We use iterated learning to encourage the emergence of structure in the generated programs for neural module networks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vani|iterated_learning_for_emergent_systematicity_in_vqa", "supplementary_material": "", "pdf": "/pdf/62bee9dfb73bae4271c7f80e9d64eda7effacc43.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvani2021iterated,\ntitle={Iterated learning for emergent systematicity in {\\{}VQA{\\}}},\nauthor={Ankit Vani and Max Schwarzer and Yuchen Lu and Eeshan Dhekane and Aaron Courville},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Pd_oMxH8IlF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Pd_oMxH8IlF", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2705/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2705/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2705/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2705/Authors|ICLR.cc/2021/Conference/Paper2705/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2705/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845319, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2705/-/Official_Comment"}}}, {"id": "4943jk488e_", "original": null, "number": 15, "cdate": 1606114550022, "ddate": null, "tcdate": 1606114550022, "tmdate": 1606114550022, "tddate": null, "forum": "Pd_oMxH8IlF", "replyto": "u-8AszcfBc6", "invitation": "ICLR.cc/2021/Conference/Paper2705/-/Official_Comment", "content": {"title": "Response to AnonReviewer5's reply", "comment": "We thank the reviewer for their response and thoughtful questions. For the convenience of the reviewer and other readers, we start by giving a short overview of our method. We will refer to this overview later when addressing the reviewer\u2019s specific concerns.\n\nOur iterated learning (IL) approach involves the following phases:\n1. *Interacting phase:* This is a standard self-play setting where the program generator (PG) and the execution engine (EE) are trained end-to-end to solve the VQA classification task. We use REINFORCE to estimate the gradients of the PG, and also provide direct supervision to it with a small subset of labeled programs. This phase is carried out for a fixed number of gradient updates specified by $T_i$. *In our experiments without IL, this is the only phase executed.*\n2. *Transmitting phase:* We collect samples from the PG to serve as targets for an imitation learning task in the next phase. We sample training questions $q$ and do a forward pass through the PG to produce predicted programs $\\hat{z}$. We add tuples $(q, \\hat{z})$ to the transmitting dataset $D$ to contain $T_t$ samples.\n3. *PG learning phase:* A new PG is initialized and trained to imitate the previous PG, using the transmitting dataset $D$. The new PG is trained to match $\\hat{z}$ when given $q$ as input, where $(q, \\hat{z})$ is sampled from $D$. This phase is carried out for a fixed number of gradient updates specified by $T_p$. *The value of $T_p$ controls the learning bottleneck by altering the new PG\u2019s tendency to underfit or overfit the previous PG\u2019s language.*\n4. *EE learning phase:* We initialize a new EE and train it to adapt to the new PG. We train the new EE to solve the VQA classification task using programs generated by the new PG, without modifying the new PG. Adapting the EE stabilizes training during the interacting phase. This phase is carried out for a fixed number of gradient updates specified by $T_e$.\n\nWe now address the reviewer's specific concerns.\n\n- **Learning bottleneck**:  \nIterated learning creates a new PG and a new EE for every generation, and a learning bottleneck controls the amount of information that passes from one generation to the next. We impose this learning bottleneck only by limiting the number of gradient updates $T_p$ performed during the PG learning phase. Based on the hypothesis that structured language is easier to fit for neural networks ([4], [5]), we expect systematic global linguistic rules to be acquired before any specific idiosyncrasies or non-compositional rules. A well-tuned $T_p$ would ideally stop the PG learning phase just before any idiosyncrasies are learned (for example, see Table 1 in [5]). Thus, the goal of the learning bottleneck is to allow structured rules to survive to the new generation, while eliminating non-compositional idiomatic rules from the layout language.  \nBy repeatedly applying this learning bottleneck, our IL algorithm pushes the language towards one that can solve the task while maintaining a structure that is easy-to-learn. It is this combination that aids systematic generalization. *Our algorithm therefore explores regularization that emerges from modifications to the learning process rather than structural constraints.*  \nAs we specify in Table 3 in the appendix, $T_p$ is 2000 in our experiments. If we initialize the new EE in the EE learning phase from scratch, $T_e$ is 250; if it is initialized as the EE after the previous EE learning phase, $T_e$ is 200; and if it is initialized as the EE after the previous interacting phase, $T_e$ is 50. We find $T_p$ to be the more important hyperparameter since it controls the learning bottleneck, while $T_e$ primarily affects the stability of training.\n\n- **Parts with/without IL**:  \nIn our runs without IL, the model is trained entirely in one long interacting phase, roughly analogous to previous works that have used NMNs on these tasks. In this case, there are no transmitting or learning phases, and the PG and the EE are never reset.  \nIn our plots, the x-axis denotes the number of gradient steps. This is straightforward to track in the runs without IL. In the case of IL, we maintain a global counter for gradient steps across all phases. This ensures that our plots fairly reflect the computational requirements of IL. The training task and program accuracies are measured only during the interacting phases, so one can see dips in performance between the end of one interacting phase and the start of the next interacting phase. Validation accuracies are measured at the end of interacting phases in the case of IL, and at regular intervals for the baselines without IL.\n\nWe will upload a revision of the paper shortly, updated to address some of the reviewer's concerns. As a reminder, we will not have the GQA results until the camera-ready version since these experiments are time-consuming and not necessary to convey the core message of our paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper2705/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2705/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterated learning for emergent systematicity in VQA", "authorids": ["~Ankit_Vani1", "~Max_Schwarzer1", "~Yuchen_Lu1", "eeshandhekane@gmail.com", "~Aaron_Courville3"], "authors": ["Ankit Vani", "Max Schwarzer", "Yuchen Lu", "Eeshan Dhekane", "Aaron Courville"], "keywords": ["iterated learning", "cultural transmission", "neural module network", "clevr", "shapes", "vqa", "visual question answering", "systematic generalization", "compositionality"], "abstract": "Although neural module networks have an architectural bias towards compositionality, they require gold standard layouts to generalize systematically in practice. When instead learning layouts and modules jointly, compositionality does not arise automatically and an explicit pressure is necessary for the emergence of layouts exhibiting the right structure. We propose to address this problem using iterated learning, a cognitive science theory of the emergence of compositional languages in nature that has primarily been applied to simple referential games in machine learning. Considering the layouts of module networks as samples from an emergent language, we use iterated learning to encourage the development of structure within this language. We show that the resulting layouts support systematic generalization in neural agents solving the more complex task of visual question-answering. Our regularized iterated learning method can outperform baselines without iterated learning on SHAPES-SyGeT (SHAPES Systematic Generalization Test), a new split of the SHAPES dataset we introduce to evaluate systematic generalization, and on CLOSURE, an extension of CLEVR also designed to test systematic generalization. We demonstrate superior performance in recovering ground-truth compositional program structure with limited supervision on both SHAPES-SyGeT and CLEVR.", "one-sentence_summary": "We use iterated learning to encourage the emergence of structure in the generated programs for neural module networks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vani|iterated_learning_for_emergent_systematicity_in_vqa", "supplementary_material": "", "pdf": "/pdf/62bee9dfb73bae4271c7f80e9d64eda7effacc43.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvani2021iterated,\ntitle={Iterated learning for emergent systematicity in {\\{}VQA{\\}}},\nauthor={Ankit Vani and Max Schwarzer and Yuchen Lu and Eeshan Dhekane and Aaron Courville},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Pd_oMxH8IlF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Pd_oMxH8IlF", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2705/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2705/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2705/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2705/Authors|ICLR.cc/2021/Conference/Paper2705/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2705/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845319, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2705/-/Official_Comment"}}}, {"id": "u-8AszcfBc6", "original": null, "number": 13, "cdate": 1606072016205, "ddate": null, "tcdate": 1606072016205, "tmdate": 1606072016205, "tddate": null, "forum": "Pd_oMxH8IlF", "replyto": "r_bxVPq9t9d", "invitation": "ICLR.cc/2021/Conference/Paper2705/-/Official_Comment", "content": {"title": "Reply to the Response", "comment": "Thank you for your detailed response. \nIt is helpful to understand your manuscript more, and it is nice to be ready with the result of realistic VQA datasets.\nBy the response, some of my concerns are resolved. \n\nHowever, there are still unclear parts remained as follows:\n(1) on the learning bottleneck: I'm still not sure that how does *the learning bottleneck* give structural constraints or sampling constraints on the training processes.  What is the meaning of limiting the length of learning phase? Is it for masking the training samples similar to [5]? Why is it helpful to improve the proposed method? So, how many iterations on T_p and T_e  are executed finally? 2000 (in Appendices)?\n\n(2) clarity of which parts are with/without IL: For without IL configuration, which components in IL are not performed? Most experiments include with/without IL cases along the x-axis of \"step\". It seems that the configuration of without IL also follows 3 phases of iterative learning. Is the learning bottleneck just blocked? It is not still clear."}, "signatures": ["ICLR.cc/2021/Conference/Paper2705/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2705/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterated learning for emergent systematicity in VQA", "authorids": ["~Ankit_Vani1", "~Max_Schwarzer1", "~Yuchen_Lu1", "eeshandhekane@gmail.com", "~Aaron_Courville3"], "authors": ["Ankit Vani", "Max Schwarzer", "Yuchen Lu", "Eeshan Dhekane", "Aaron Courville"], "keywords": ["iterated learning", "cultural transmission", "neural module network", "clevr", "shapes", "vqa", "visual question answering", "systematic generalization", "compositionality"], "abstract": "Although neural module networks have an architectural bias towards compositionality, they require gold standard layouts to generalize systematically in practice. When instead learning layouts and modules jointly, compositionality does not arise automatically and an explicit pressure is necessary for the emergence of layouts exhibiting the right structure. We propose to address this problem using iterated learning, a cognitive science theory of the emergence of compositional languages in nature that has primarily been applied to simple referential games in machine learning. Considering the layouts of module networks as samples from an emergent language, we use iterated learning to encourage the development of structure within this language. We show that the resulting layouts support systematic generalization in neural agents solving the more complex task of visual question-answering. Our regularized iterated learning method can outperform baselines without iterated learning on SHAPES-SyGeT (SHAPES Systematic Generalization Test), a new split of the SHAPES dataset we introduce to evaluate systematic generalization, and on CLOSURE, an extension of CLEVR also designed to test systematic generalization. We demonstrate superior performance in recovering ground-truth compositional program structure with limited supervision on both SHAPES-SyGeT and CLEVR.", "one-sentence_summary": "We use iterated learning to encourage the emergence of structure in the generated programs for neural module networks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vani|iterated_learning_for_emergent_systematicity_in_vqa", "supplementary_material": "", "pdf": "/pdf/62bee9dfb73bae4271c7f80e9d64eda7effacc43.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvani2021iterated,\ntitle={Iterated learning for emergent systematicity in {\\{}VQA{\\}}},\nauthor={Ankit Vani and Max Schwarzer and Yuchen Lu and Eeshan Dhekane and Aaron Courville},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Pd_oMxH8IlF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Pd_oMxH8IlF", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2705/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2705/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2705/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2705/Authors|ICLR.cc/2021/Conference/Paper2705/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2705/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845319, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2705/-/Official_Comment"}}}, {"id": "rJtgDxcpiKA", "original": null, "number": 8, "cdate": 1605507953301, "ddate": null, "tcdate": 1605507953301, "tmdate": 1605507953301, "tddate": null, "forum": "Pd_oMxH8IlF", "replyto": "njolLhrWLWe", "invitation": "ICLR.cc/2021/Conference/Paper2705/-/Official_Comment", "content": {"title": "Response to AnonReviewer4", "comment": "We thank the reviewer for taking the time to read our paper and for their valuable suggestions. We have incorporated the reviewer\u2019s minor comments and will make Section 3.2 easier to follow in the paper.\n\nWhile we agree that SHAPES is fairly toy-ish (244 unique questions in 12 question templates with 30x30 images laid out in a 3x3 grid), CLEVR is substantially more complex in comparison (700,000 unique questions in 90 question templates, with 3D-rendered 224x224 images). As a crude point of comparison, it takes about 30 minutes to train a SHAPES-SyGeT model to achieve decent generalization on a modern GPU, whereas it takes over two days to do the same on CLEVR. That said, CLEVR is indeed a synthetic dataset, although this has the advantage of allowing us to clearly demonstrate the systematic generalization gains of our method by removing other sources of error.\n\nAs we mention in the separate standalone comment, we understand the need to test methods such as ours on realistic data and are thus experimenting with the GQA dataset. These results are not necessary for the goal of our paper, and we will be unable to explicitly evaluate systematic generalization as we do in the case of SHAPES-SyGeT and CLEVR/CLOSURE. Thus, we plan to include the validation performances in our appendix rather than the paper\u2019s main body, which we will do before the camera-ready deadline.\n\nDue to the subjective definition of \u201ctoy\u201d tasks, we can modify that claim in the paper to specify that we open up IL to broader machine learning applications beyond the previously-explored scope of language emergence and preservation."}, "signatures": ["ICLR.cc/2021/Conference/Paper2705/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2705/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterated learning for emergent systematicity in VQA", "authorids": ["~Ankit_Vani1", "~Max_Schwarzer1", "~Yuchen_Lu1", "eeshandhekane@gmail.com", "~Aaron_Courville3"], "authors": ["Ankit Vani", "Max Schwarzer", "Yuchen Lu", "Eeshan Dhekane", "Aaron Courville"], "keywords": ["iterated learning", "cultural transmission", "neural module network", "clevr", "shapes", "vqa", "visual question answering", "systematic generalization", "compositionality"], "abstract": "Although neural module networks have an architectural bias towards compositionality, they require gold standard layouts to generalize systematically in practice. When instead learning layouts and modules jointly, compositionality does not arise automatically and an explicit pressure is necessary for the emergence of layouts exhibiting the right structure. We propose to address this problem using iterated learning, a cognitive science theory of the emergence of compositional languages in nature that has primarily been applied to simple referential games in machine learning. Considering the layouts of module networks as samples from an emergent language, we use iterated learning to encourage the development of structure within this language. We show that the resulting layouts support systematic generalization in neural agents solving the more complex task of visual question-answering. Our regularized iterated learning method can outperform baselines without iterated learning on SHAPES-SyGeT (SHAPES Systematic Generalization Test), a new split of the SHAPES dataset we introduce to evaluate systematic generalization, and on CLOSURE, an extension of CLEVR also designed to test systematic generalization. We demonstrate superior performance in recovering ground-truth compositional program structure with limited supervision on both SHAPES-SyGeT and CLEVR.", "one-sentence_summary": "We use iterated learning to encourage the emergence of structure in the generated programs for neural module networks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vani|iterated_learning_for_emergent_systematicity_in_vqa", "supplementary_material": "", "pdf": "/pdf/62bee9dfb73bae4271c7f80e9d64eda7effacc43.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvani2021iterated,\ntitle={Iterated learning for emergent systematicity in {\\{}VQA{\\}}},\nauthor={Ankit Vani and Max Schwarzer and Yuchen Lu and Eeshan Dhekane and Aaron Courville},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Pd_oMxH8IlF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Pd_oMxH8IlF", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2705/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2705/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2705/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2705/Authors|ICLR.cc/2021/Conference/Paper2705/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2705/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845319, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2705/-/Official_Comment"}}}, {"id": "UITGOHQXLZO", "original": null, "number": 4, "cdate": 1605332089218, "ddate": null, "tcdate": 1605332089218, "tmdate": 1605496485114, "tddate": null, "forum": "Pd_oMxH8IlF", "replyto": "Pd_oMxH8IlF", "invitation": "ICLR.cc/2021/Conference/Paper2705/-/Official_Comment", "content": {"title": "Our contributions, CLEVR update, and a note on datasets choice", "comment": "We highlight our main contributions here and will update the paper to reflect these more clearly. Additionally, we provide a note on some updated results and remark on the choice of SHAPES-SyGeT and CLEVR/CLOSURE for our work.\n\n- Main contributions:  \n  1. We present iterated learning (IL) as a more fundamental way of recovering structure in machine learning. To the best of our knowledge, IL has not been applied outside language emergence or preservation. We believe this is an important message to share with the machine learning community, where there is still a need and a lack of understanding of the emergence of compositional structure.\n  2. We propose an IL method to achieve higher systematic generalization (generalization to novel combinations of known concepts) in neural module networks (NMNs). Previously, NMNs have been shown to exhibit superior systematic generalization ([1], [2]) but only with gold-standard layouts. Subsequently, prior work has required supervision with a large number of ground-truth programs for CLEVR (18000 for [3], 1000 for [4]) to get NMNs to generalize well. In contrast, our method is significantly more data-efficient (only 100 ground-truth programs for supervision) by using IL to learn the program language.\n  3. We introduce the SHAPES-SyGeT dataset as a new split of the existing SHAPES dataset to evaluate systematic generalization as a lightweight alternative to CLEVR/CLOSURE.\n  4. Minor contribution: Tensor-FiLM-NMN module architecture to combine the parameter-efficient Vector-NMN with the spatial representational power of Tensor-NMN.\n\n- Updates to CLEVR results:  \nWhile we already presented clear advantages in recovering the correct programs, the CLEVR models in the first version of our paper did not completely converge. Thus, we have updated the paper with experiments that run twice as long and with 8 seeds instead of 3. With this setup, IL outperforms the baselines on all but one CLOSURE category for both Tensor-NMN and Vector-NMN, illustrating the superior systematic generalization of IL.\n\n- Note on our choice of datasets:  \nWe use SHAPES-SyGeT and CLEVR/CLOSURE for this work to clearly demonstrate the advantages of IL for systematic generalization. These diagnostic VQA datasets make it easy to analyze and split questions into templates, which can then be divided into training and testing templates. We train on questions generated from the train templates and evaluate on questions generated from templates never seen during training (SHAPES-SyGeT's Val-OOD and CLOSURE).  \nHowever, we agree with the reviewers that it is important to understand the scaling limitations of methods such as ours. Although not necessary for our paper's goals, we are trying our method on the GQA dataset ([5]) to analyze the gains of our method against the baselines on a large-scale non-synthetic VQA dataset. This dataset also provides programs for its questions, similar to the datasets we have used. We plan to include these results in the appendix before the camera-ready deadline.\n\nReferences:  \n[1] Dzmitry Bahdanau, Shikhar Murty, Michael Noukhovitch, Thien Huu Nguyen, Harm de Vries, and Aaron Courville. Systematic generalization: What is required and can it be learned? In International Conference on Learning Representations, 2019.  \n[2] Dzmitry Bahdanau, Harm de Vries, Timothy J O\u2019Donnell, Shikhar Murty, Philippe Beaudoin, Yoshua Bengio, and Aaron Courville. Closure: Assessing systematic generalization of clevr models. arXiv preprint arXiv:1912.05783, 2019.  \n[3] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Judy Hoffman, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Inferring and executing programs for visual reasoning. In Proceedings of the IEEE International Conference on Computer Vision, pp. 2989\u20132998, 2017.  \n[4] Ramakrishna Vedantam, Karan Desai, Stefan Lee, Marcus Rohrbach, Dhruv Batra, and Devi Parikh. Probabilistic neural-symbolic models for interpretable visual question answering. arXiv preprint arXiv:1902.07864, 2019.  \n[5] Drew A. Hudson and Christopher D. Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019."}, "signatures": ["ICLR.cc/2021/Conference/Paper2705/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2705/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterated learning for emergent systematicity in VQA", "authorids": ["~Ankit_Vani1", "~Max_Schwarzer1", "~Yuchen_Lu1", "eeshandhekane@gmail.com", "~Aaron_Courville3"], "authors": ["Ankit Vani", "Max Schwarzer", "Yuchen Lu", "Eeshan Dhekane", "Aaron Courville"], "keywords": ["iterated learning", "cultural transmission", "neural module network", "clevr", "shapes", "vqa", "visual question answering", "systematic generalization", "compositionality"], "abstract": "Although neural module networks have an architectural bias towards compositionality, they require gold standard layouts to generalize systematically in practice. When instead learning layouts and modules jointly, compositionality does not arise automatically and an explicit pressure is necessary for the emergence of layouts exhibiting the right structure. We propose to address this problem using iterated learning, a cognitive science theory of the emergence of compositional languages in nature that has primarily been applied to simple referential games in machine learning. Considering the layouts of module networks as samples from an emergent language, we use iterated learning to encourage the development of structure within this language. We show that the resulting layouts support systematic generalization in neural agents solving the more complex task of visual question-answering. Our regularized iterated learning method can outperform baselines without iterated learning on SHAPES-SyGeT (SHAPES Systematic Generalization Test), a new split of the SHAPES dataset we introduce to evaluate systematic generalization, and on CLOSURE, an extension of CLEVR also designed to test systematic generalization. We demonstrate superior performance in recovering ground-truth compositional program structure with limited supervision on both SHAPES-SyGeT and CLEVR.", "one-sentence_summary": "We use iterated learning to encourage the emergence of structure in the generated programs for neural module networks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vani|iterated_learning_for_emergent_systematicity_in_vqa", "supplementary_material": "", "pdf": "/pdf/62bee9dfb73bae4271c7f80e9d64eda7effacc43.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvani2021iterated,\ntitle={Iterated learning for emergent systematicity in {\\{}VQA{\\}}},\nauthor={Ankit Vani and Max Schwarzer and Yuchen Lu and Eeshan Dhekane and Aaron Courville},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Pd_oMxH8IlF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Pd_oMxH8IlF", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2705/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2705/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2705/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2705/Authors|ICLR.cc/2021/Conference/Paper2705/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2705/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845319, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2705/-/Official_Comment"}}}, {"id": "bQb768mfr7V", "original": null, "number": 7, "cdate": 1605419348046, "ddate": null, "tcdate": 1605419348046, "tmdate": 1605469282977, "tddate": null, "forum": "Pd_oMxH8IlF", "replyto": "hHAsr1Et-0k", "invitation": "ICLR.cc/2021/Conference/Paper2705/-/Official_Comment", "content": {"title": "Response to AnonReviewer1", "comment": "We thank the reviewer for taking the time to read our paper and for their comments.\n\n- Compute requirements of iterated learning (IL):  \nThe compute required for IL depends on the compute required to train the program generator (PG) and the execution engine (EE) independently and the frequency of re-initialization of these components. For CLEVR, the EE takes about a day to achieve high validation performance on a modern GPU, even with the ground-truth layouts. Thus, re-initializing the EE for every iteration becomes computationally intractable. Fortunately, the seq2seq PG is quick to train, and thus not reinitializing the EE drastically reduces the computational overhead of IL. With this setup for CLEVR, we find little difference in the compute between IL and non-IL models. This is because IL model training is dominated by the interacting phase, while non-IL models are essentially trained through one long interacting phase.  \nHowever, if the question and program languages are more complex and the PG is a larger model such as a transformer, re-training the PG can become a bottleneck. Making IL work with a harder-to-train PG without completely re-initializing it can be an interesting research direction. Seeded IL ([1]) is one possible strategy to avoid learning the PG from scratch at every iteration. Another idea could be using pruning or strong weight decay on the previous PG to produce an initialization point for the new PG.\n\n- Results for CLEVR/CLOSURE:  \nAs noted in a separate standalone comment, we have updated our CLEVR/CLOSURE results indicating stronger systematic generalization performance. Regarding in-distribution performance, the improvement on the CLEVR validation set is substantial for Tensor-NMN, which is the widely used NMN architecture from [2]. There is also a modest improvement in the case of Vector-NMN, but baseline performance on CLEVR validation here is already very high.\n\n- Artificial nature of SHAPES-SyGeT and CLEVR/CLOSURE:  \nPlease see the standalone comment.\n\n- Requirement of ground-truth programs:  \nOur method's requirement for ground-truth programs primarily arises from the optimization difficulties in jointly training the PG and the EE from scratch. The EE modules can specialize in performing specific roles only if the programs reuse the modules in appropriate ways. On the other hand, a good quality reinforcement signal from the EE to the PG relies on some amount of specialization of the modules. However, we note that only 100 programs is sufficient for us to achieve CLEVR validation performance close to that of previous NMN papers, which used 18000 ([2]) or 1000 ([3]) ground-truth programs. With a reduced need for supervision, it might become tractable for practitioners to achieve good systematic generalization in realistic VQA tasks by labeling programs for only a small set of questions.\n\nReferences:  \n[1] Yuchen Lu, Soumye Singhal, Florian Strub, Olivier Pietquin, and Aaron Courville. Countering language drift with seeded iterated learning. arXiv preprint arXiv:2003.12694, 2020.  \n[2] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Judy Hoffman, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Inferring and executing programs for visual reasoning. In Proceedings of the IEEE International Conference on Computer Vision, pp. 2989\u20132998, 2017.  \n[3] Ramakrishna Vedantam, Karan Desai, Stefan Lee, Marcus Rohrbach, Dhruv Batra, and Devi Parikh. Probabilistic neural-symbolic models for interpretable visual question answering. arXiv preprint arXiv:1902.07864, 2019."}, "signatures": ["ICLR.cc/2021/Conference/Paper2705/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2705/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterated learning for emergent systematicity in VQA", "authorids": ["~Ankit_Vani1", "~Max_Schwarzer1", "~Yuchen_Lu1", "eeshandhekane@gmail.com", "~Aaron_Courville3"], "authors": ["Ankit Vani", "Max Schwarzer", "Yuchen Lu", "Eeshan Dhekane", "Aaron Courville"], "keywords": ["iterated learning", "cultural transmission", "neural module network", "clevr", "shapes", "vqa", "visual question answering", "systematic generalization", "compositionality"], "abstract": "Although neural module networks have an architectural bias towards compositionality, they require gold standard layouts to generalize systematically in practice. When instead learning layouts and modules jointly, compositionality does not arise automatically and an explicit pressure is necessary for the emergence of layouts exhibiting the right structure. We propose to address this problem using iterated learning, a cognitive science theory of the emergence of compositional languages in nature that has primarily been applied to simple referential games in machine learning. Considering the layouts of module networks as samples from an emergent language, we use iterated learning to encourage the development of structure within this language. We show that the resulting layouts support systematic generalization in neural agents solving the more complex task of visual question-answering. Our regularized iterated learning method can outperform baselines without iterated learning on SHAPES-SyGeT (SHAPES Systematic Generalization Test), a new split of the SHAPES dataset we introduce to evaluate systematic generalization, and on CLOSURE, an extension of CLEVR also designed to test systematic generalization. We demonstrate superior performance in recovering ground-truth compositional program structure with limited supervision on both SHAPES-SyGeT and CLEVR.", "one-sentence_summary": "We use iterated learning to encourage the emergence of structure in the generated programs for neural module networks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vani|iterated_learning_for_emergent_systematicity_in_vqa", "supplementary_material": "", "pdf": "/pdf/62bee9dfb73bae4271c7f80e9d64eda7effacc43.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvani2021iterated,\ntitle={Iterated learning for emergent systematicity in {\\{}VQA{\\}}},\nauthor={Ankit Vani and Max Schwarzer and Yuchen Lu and Eeshan Dhekane and Aaron Courville},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Pd_oMxH8IlF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Pd_oMxH8IlF", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2705/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2705/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2705/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2705/Authors|ICLR.cc/2021/Conference/Paper2705/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2705/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845319, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2705/-/Official_Comment"}}}, {"id": "r_bxVPq9t9d", "original": null, "number": 6, "cdate": 1605365179883, "ddate": null, "tcdate": 1605365179883, "tmdate": 1605468973031, "tddate": null, "forum": "Pd_oMxH8IlF", "replyto": "uStfLc_cLAE", "invitation": "ICLR.cc/2021/Conference/Paper2705/-/Official_Comment", "content": {"title": "Response to AnonReviewer5 (cont.)", "comment": "- Recovery level of language structures:  \nOur understanding of the reviewer's concern regarding the \"recovery level of language structures\" is that they would like to see some measure of how much of the structure is recovered by our methods and baselines beyond just the program accuracy. We would appreciate it if the reviewer could clarify this. However, we respond here with our current understanding to argue that the program accuracy indeed largely gets to what we want for this work.  \nIn the context of NMNs, the program determines the computation to be performed on the input images. Small differences in the serialized representation of the computation graph, which the PG outputs, can result in trees that differ significantly in structure. Metrics that use n-gram statistics like the BLEU score can thus be misleading since they may provide high scores for programs that have similar serialized structure but vastly different tree representations once parsed. Since we have some amount of program supervision, there indeed is a notion of a \"correct\" program structure, which is one that agrees with the supervision programs. Finally, we note a very significant and clear difference in the program accuracy exhibited by IL and baselines without IL that correlates with the models' ability to systematically generalize, which supports our choice of discussing the recovery of structure in terms of program accuracy.\n\n- Theory:  \nFinally, while we do believe that a theoretical understanding of IL is necessary, we consider it a broad research problem that is out of scope for our study. A mathematical model from cognitive science is presented in [8], that relies on a Bayesian learner assumption. However, this theory does not sufficiently explain IL in deep learning or in the presence of a grounding task. Our paper offers contributions to encourage the research community to study IL in more detail, which will hopefully offer further theoretical insights.\n\nReferences:  \n[1] Dzmitry Bahdanau, Shikhar Murty, Michael Noukhovitch, Thien Huu Nguyen, Harm de Vries, and Aaron Courville. Systematic generalization: What is required and can it be learned? In International Conference on Learning Representations, 2019.  \n[2] Dzmitry Bahdanau, Harm de Vries, Timothy J O\u2019Donnell, Shikhar Murty, Philippe Beaudoin, Yoshua Bengio, and Aaron Courville. Closure: Assessing systematic generalization of clevr models. arXiv preprint arXiv:1912.05783, 2019.  \n[3] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Judy Hoffman, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Inferring and executing programs for visual reasoning. In Proceedings of the IEEE International Conference on Computer Vision, pp. 2989\u20132998, 2017.  \n[4] Fushan Li and Michael Bowling. Ease-of-teaching and language structure from emergent communication. In Advances in Neural Information Processing Systems, pp. 15851\u201315861, 2019.  \n[5] Yi Ren, Shangmin Guo, Matthieu Labeau, Shay B. Cohen, and Simon Kirby. Compositional languages emerge in a neural iterated learning model. In International Conference on Learning Representations, 2020.  \n[6] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.  \n[7] Drew Arad Hudson and Christopher D. Manning. Compositional attention networks for machine reasoning. In International Conference on Learning Representations, 2018.  \n[8] Thomas L. Griffiths and Michael L. Kalish. Language evolution by iterated learning with Bayesian agents. In Cognitive science 31.3, 2007."}, "signatures": ["ICLR.cc/2021/Conference/Paper2705/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2705/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterated learning for emergent systematicity in VQA", "authorids": ["~Ankit_Vani1", "~Max_Schwarzer1", "~Yuchen_Lu1", "eeshandhekane@gmail.com", "~Aaron_Courville3"], "authors": ["Ankit Vani", "Max Schwarzer", "Yuchen Lu", "Eeshan Dhekane", "Aaron Courville"], "keywords": ["iterated learning", "cultural transmission", "neural module network", "clevr", "shapes", "vqa", "visual question answering", "systematic generalization", "compositionality"], "abstract": "Although neural module networks have an architectural bias towards compositionality, they require gold standard layouts to generalize systematically in practice. When instead learning layouts and modules jointly, compositionality does not arise automatically and an explicit pressure is necessary for the emergence of layouts exhibiting the right structure. We propose to address this problem using iterated learning, a cognitive science theory of the emergence of compositional languages in nature that has primarily been applied to simple referential games in machine learning. Considering the layouts of module networks as samples from an emergent language, we use iterated learning to encourage the development of structure within this language. We show that the resulting layouts support systematic generalization in neural agents solving the more complex task of visual question-answering. Our regularized iterated learning method can outperform baselines without iterated learning on SHAPES-SyGeT (SHAPES Systematic Generalization Test), a new split of the SHAPES dataset we introduce to evaluate systematic generalization, and on CLOSURE, an extension of CLEVR also designed to test systematic generalization. We demonstrate superior performance in recovering ground-truth compositional program structure with limited supervision on both SHAPES-SyGeT and CLEVR.", "one-sentence_summary": "We use iterated learning to encourage the emergence of structure in the generated programs for neural module networks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vani|iterated_learning_for_emergent_systematicity_in_vqa", "supplementary_material": "", "pdf": "/pdf/62bee9dfb73bae4271c7f80e9d64eda7effacc43.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvani2021iterated,\ntitle={Iterated learning for emergent systematicity in {\\{}VQA{\\}}},\nauthor={Ankit Vani and Max Schwarzer and Yuchen Lu and Eeshan Dhekane and Aaron Courville},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Pd_oMxH8IlF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Pd_oMxH8IlF", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2705/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2705/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2705/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2705/Authors|ICLR.cc/2021/Conference/Paper2705/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2705/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845319, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2705/-/Official_Comment"}}}, {"id": "uStfLc_cLAE", "original": null, "number": 5, "cdate": 1605365141336, "ddate": null, "tcdate": 1605365141336, "tmdate": 1605468950008, "tddate": null, "forum": "Pd_oMxH8IlF", "replyto": "X2W0yhGNtsm", "invitation": "ICLR.cc/2021/Conference/Paper2705/-/Official_Comment", "content": {"title": "Response to AnonReviewer5", "comment": "We thank the reviewer for their insightful comments.\n\n- Contributions and note on other datasets:  \nWe have provided a clear set of contributions of this paper, as well as a note on datasets, as a standalone comment. We kindly ask the reviewer to please refer to that comment first.\n\n- Relation to semi-supervised approaches:  \nWe see iterated learning (IL) as more of a regularization technique than a semi-supervised learning method. IL in general can also be used in unsupervised settings. We happen to apply it in a semi-supervised learning setting for VQA, diverging from its previous scope of language emergence in the machine learning literature. That said, IL is indeed related to semi-supervised learning. Where semi-supervised learning deals with grounding a subset of its predictions in ground-truth labels, IL is supervised using a subset of the previous generation's utterances. The crucial difference is that in IL, the supervision set evolves, changing the language acquired through the learning bottleneck in every new generation until convergence.\n\n- Learning bottleneck:  \nIn our work, the learning bottleneck primarily arises from limiting the length of the learning phase. In the learning phase, a new program generator (PG) is trained on the previous PG's utterances for a limited number of gradient updates. This is related to early stopping, which is an effective strategy for regularization in machine learning. The central hypothesis for the success of this bottleneck is that structured language is easier to fit for neural networks ([4], [5]). Thus, easy-to-learn structure is acquired quickly by training the new PG for a limited number of steps. In contrast, idiomatic concepts that are harder to acquire (requiring additional learning time or resources) are not transmitted. Consequently, a student PG can learn a more structured language than its teacher. The repeated application of this bottleneck can push the language of programs towards one that is more structured. The ability to solve the task using programs exhibiting structure promotes systematic generalization. If the learning phase's length is too small or too large, the effectiveness of the learning bottleneck can decrease due to underfitting or overfitting the teacher. We will clarify this intuition in the paper.\n\n- Specifics of architecture:  \nSince our work's focus is on the IL algorithm, much of our architectural setup is based on previous works ([3] for Tensor-NMN, [2] for Vector-NMN and the PG), and we provide details for our Tensor-FiLM-NMN module architecture in Section 3.1. We will expand Section 3.1 to explain the notations and operational details better, as well as add more details for reproducibility in the appendix.\n\n- Clarity of which parts are with/without IL:  \nThe details of which parts of the architecture are modified when running experiments with and without IL are specified in Section 3.2. However, for clarity and ease of reference, we will add the algorithm (abstract pseudocode format) describing all the steps of our approach more precisely in the appendix.\n\n- Comparison to other methods and FiLM/MAC setting:  \nSince this work aims to demonstrate the effectiveness of IL for encouraging systematic generalization in NMNs, the comparisons of interest are the ones between NMNs with IL and without IL, which we have presented in Section 5. We present the results on FiLM and MAC, which achieve near-perfect accuracy on large datasets like CLEVR (see [6], [7]), not to claim state-of-the-art performance, but to provide a better understanding for readers as to where models without program supervision stand. We know from [1] and [2] that NMNs can systematically generalize better than models with generic deep architectures. However, there are stringent supervision requirements for this to happen in practice, which we address in this work. As with all our models, both FiLM and MAC are configured by finding the hyperparameters that perform the best on Val-IID.\n\n[continued in the next comment]"}, "signatures": ["ICLR.cc/2021/Conference/Paper2705/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2705/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterated learning for emergent systematicity in VQA", "authorids": ["~Ankit_Vani1", "~Max_Schwarzer1", "~Yuchen_Lu1", "eeshandhekane@gmail.com", "~Aaron_Courville3"], "authors": ["Ankit Vani", "Max Schwarzer", "Yuchen Lu", "Eeshan Dhekane", "Aaron Courville"], "keywords": ["iterated learning", "cultural transmission", "neural module network", "clevr", "shapes", "vqa", "visual question answering", "systematic generalization", "compositionality"], "abstract": "Although neural module networks have an architectural bias towards compositionality, they require gold standard layouts to generalize systematically in practice. When instead learning layouts and modules jointly, compositionality does not arise automatically and an explicit pressure is necessary for the emergence of layouts exhibiting the right structure. We propose to address this problem using iterated learning, a cognitive science theory of the emergence of compositional languages in nature that has primarily been applied to simple referential games in machine learning. Considering the layouts of module networks as samples from an emergent language, we use iterated learning to encourage the development of structure within this language. We show that the resulting layouts support systematic generalization in neural agents solving the more complex task of visual question-answering. Our regularized iterated learning method can outperform baselines without iterated learning on SHAPES-SyGeT (SHAPES Systematic Generalization Test), a new split of the SHAPES dataset we introduce to evaluate systematic generalization, and on CLOSURE, an extension of CLEVR also designed to test systematic generalization. We demonstrate superior performance in recovering ground-truth compositional program structure with limited supervision on both SHAPES-SyGeT and CLEVR.", "one-sentence_summary": "We use iterated learning to encourage the emergence of structure in the generated programs for neural module networks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vani|iterated_learning_for_emergent_systematicity_in_vqa", "supplementary_material": "", "pdf": "/pdf/62bee9dfb73bae4271c7f80e9d64eda7effacc43.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvani2021iterated,\ntitle={Iterated learning for emergent systematicity in {\\{}VQA{\\}}},\nauthor={Ankit Vani and Max Schwarzer and Yuchen Lu and Eeshan Dhekane and Aaron Courville},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Pd_oMxH8IlF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Pd_oMxH8IlF", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2705/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2705/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2705/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2705/Authors|ICLR.cc/2021/Conference/Paper2705/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2705/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845319, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2705/-/Official_Comment"}}}, {"id": "njolLhrWLWe", "original": null, "number": 1, "cdate": 1603871435190, "ddate": null, "tcdate": 1603871435190, "tmdate": 1605024149765, "tddate": null, "forum": "Pd_oMxH8IlF", "replyto": "Pd_oMxH8IlF", "invitation": "ICLR.cc/2021/Conference/Paper2705/-/Official_Review", "content": {"title": "Interesting and impressive application of iterated learning to VQA", "review": "The authors apply iterated learning - a procedure originating in CogSci analyses of how human languages might develop - to the training of neural module networks. The goal is for iterated learning to encourage these networks to develop compositional structures that support systematic generalization without requiring explicit pressures for compositional structures (in past work, such explicit pressures have generally been necessary). The proposed approach brings substantial improvements in systematic generalization across two datasets, SHAPES and CLEVR.\n\nStrengths:\n\n1. The approach is well-motivated, including impressive coverage of prior literature in both ML and CogSci.\n\n2. The approach brings impressive gains in an area that is one of the major weaknesses of current ML systems, namely systematic generalization. \n\n3. In addition to the gains in accuracy, one particularly impressive benefit of this approach is the decreased amount of supervision that it requires compared to past approaches.\n\n4. The paper is generally well-written and easy to follow. \n\nWeaknesses:\n\n1. One of the motivations is to expand the use of iterated learning beyond toy datasets. While SHAPES and CLEVR may be not as toy-ish as datasets used in the past, they still are pretty toy-ish, so I\u2019m not sure if this paper can reasonably claim that one of its contributions is to expand iterated learning to realistic domains.\n\n2. Though the paper in general was very clear, I found Section 3.2 to be a bit hard to follow, and that section is important as it is the part that describes the structure of the iterated learning framing. I think this section would benefit from starting each subpart with a more high-level, intuitive description of what that stage accomplished, before diving into the details.\n\nMinor comments:\n\n1. Fodor et al only has 2 authors - Fodor and Pylyshyn\n\n2. Page 3: \u201cAlthough, the Gumbel straight-through estimator\u201d: this use of \u201calthough\u201d is usually frowned upon - better to use \u201cHowever\u201d\n\n3. Page 5: typo: \u201cminimzing\u201d\n\n4. In general, for the bibliography, check to see if a paper has been published at a conference or journal; if so, cite that version instead of the arXiv version. E.g., \u201cNeural machine translation by jointly learning to align and translate.\u201d was published in ICLR 2015, and \u201cSystematic generalization: what is required and can it be learned?\u201d was published at ICLR 2019.\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2705/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2705/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterated learning for emergent systematicity in VQA", "authorids": ["~Ankit_Vani1", "~Max_Schwarzer1", "~Yuchen_Lu1", "eeshandhekane@gmail.com", "~Aaron_Courville3"], "authors": ["Ankit Vani", "Max Schwarzer", "Yuchen Lu", "Eeshan Dhekane", "Aaron Courville"], "keywords": ["iterated learning", "cultural transmission", "neural module network", "clevr", "shapes", "vqa", "visual question answering", "systematic generalization", "compositionality"], "abstract": "Although neural module networks have an architectural bias towards compositionality, they require gold standard layouts to generalize systematically in practice. When instead learning layouts and modules jointly, compositionality does not arise automatically and an explicit pressure is necessary for the emergence of layouts exhibiting the right structure. We propose to address this problem using iterated learning, a cognitive science theory of the emergence of compositional languages in nature that has primarily been applied to simple referential games in machine learning. Considering the layouts of module networks as samples from an emergent language, we use iterated learning to encourage the development of structure within this language. We show that the resulting layouts support systematic generalization in neural agents solving the more complex task of visual question-answering. Our regularized iterated learning method can outperform baselines without iterated learning on SHAPES-SyGeT (SHAPES Systematic Generalization Test), a new split of the SHAPES dataset we introduce to evaluate systematic generalization, and on CLOSURE, an extension of CLEVR also designed to test systematic generalization. We demonstrate superior performance in recovering ground-truth compositional program structure with limited supervision on both SHAPES-SyGeT and CLEVR.", "one-sentence_summary": "We use iterated learning to encourage the emergence of structure in the generated programs for neural module networks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vani|iterated_learning_for_emergent_systematicity_in_vqa", "supplementary_material": "", "pdf": "/pdf/62bee9dfb73bae4271c7f80e9d64eda7effacc43.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvani2021iterated,\ntitle={Iterated learning for emergent systematicity in {\\{}VQA{\\}}},\nauthor={Ankit Vani and Max Schwarzer and Yuchen Lu and Eeshan Dhekane and Aaron Courville},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Pd_oMxH8IlF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Pd_oMxH8IlF", "replyto": "Pd_oMxH8IlF", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2705/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538090330, "tmdate": 1606915763777, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2705/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2705/-/Official_Review"}}}, {"id": "hHAsr1Et-0k", "original": null, "number": 2, "cdate": 1604019882799, "ddate": null, "tcdate": 1604019882799, "tmdate": 1605024149702, "tddate": null, "forum": "Pd_oMxH8IlF", "replyto": "Pd_oMxH8IlF", "invitation": "ICLR.cc/2021/Conference/Paper2705/-/Official_Review", "content": {"title": "Review", "review": "This paper proposes to combine iterated learning (the process of repeated language transmission from a \u2018parent\u2019 agent to a \u2018child\u2019 agent) with neural module networks (NMNs), in order to emerge NMN layouts that perform better at systematic generalization. The paper evaluates on their new variant of the SHAPES dataset that tests systematic generalization, and on CLEVR / CLOSURE, showing improved systematic generalization performance while requiring a small amount of ground-truth layout supervision.\n\nPros:\n- I think the idea of combining IL with NMNs is really clever (heh). Treating the program generator and execution engine as two agents that need to coordinate through a shared language (NMN layouts) is really interesting. If it were to work without layout supervision, it could open up new doors to applying IL + NMNs to many other tasks\n\n- The paper is quite well-written, and easy to follow\n\n- The related work section is thorough\n\n- The experiments on SHAPES-SyGeT, show that IL helps significantly for generalization of NMN models \n\n- I appreciate the ablations in the Appendix. \n\n\nCons: \n- One of the main questions I have about this approach is whether it will provide any benefit on more complex problems (e.g. large-scale VQA). There are a few reasons to think it might not be able to do so:\n1) As alluded to in the paper, the IL procedure requires a lot of compute, which could be used to train larger models on more pre-training data\n2) The improvement in validation performance on the more complex CLEVR dataset is fairly modest (though the program accuracy increase is large). While CLEVR is more complex than SHAPES, it is still a fairly artificial dataset targeted \u2018compositional\u2019 in nature, compared to general VQA. This suggests that it might be hard to get IL+NMN to work well on harder problems.\n3) The method still requires (a small amount of) ground-truth layout supervision, which is not obtainable in general VQA or most other tasks.\n\n- I would also like to have seen a bit more analysis / description of why the method currently fails without any ground-truth layout supervision. I think this would improve the paper a lot, as it would help other researchers improve upon the method to address this problem.\n\n\n\nOverall:\nI think the ideas in this paper are interesting enough, and the execution good enough, to warrant acceptance. While I have some concerns about whether this approach will scale, these questions will have to be answered in subsequent works and with further research.\n\n\nSmall typos:\n\u201ceach new-born child need\u201d -> needs\n\n\u201cRecently machine learning community also show\u201d -> the machine learning community also shows\n\n\u201cMinimzing\u201d -> minimizing \n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2705/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2705/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterated learning for emergent systematicity in VQA", "authorids": ["~Ankit_Vani1", "~Max_Schwarzer1", "~Yuchen_Lu1", "eeshandhekane@gmail.com", "~Aaron_Courville3"], "authors": ["Ankit Vani", "Max Schwarzer", "Yuchen Lu", "Eeshan Dhekane", "Aaron Courville"], "keywords": ["iterated learning", "cultural transmission", "neural module network", "clevr", "shapes", "vqa", "visual question answering", "systematic generalization", "compositionality"], "abstract": "Although neural module networks have an architectural bias towards compositionality, they require gold standard layouts to generalize systematically in practice. When instead learning layouts and modules jointly, compositionality does not arise automatically and an explicit pressure is necessary for the emergence of layouts exhibiting the right structure. We propose to address this problem using iterated learning, a cognitive science theory of the emergence of compositional languages in nature that has primarily been applied to simple referential games in machine learning. Considering the layouts of module networks as samples from an emergent language, we use iterated learning to encourage the development of structure within this language. We show that the resulting layouts support systematic generalization in neural agents solving the more complex task of visual question-answering. Our regularized iterated learning method can outperform baselines without iterated learning on SHAPES-SyGeT (SHAPES Systematic Generalization Test), a new split of the SHAPES dataset we introduce to evaluate systematic generalization, and on CLOSURE, an extension of CLEVR also designed to test systematic generalization. We demonstrate superior performance in recovering ground-truth compositional program structure with limited supervision on both SHAPES-SyGeT and CLEVR.", "one-sentence_summary": "We use iterated learning to encourage the emergence of structure in the generated programs for neural module networks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vani|iterated_learning_for_emergent_systematicity_in_vqa", "supplementary_material": "", "pdf": "/pdf/62bee9dfb73bae4271c7f80e9d64eda7effacc43.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvani2021iterated,\ntitle={Iterated learning for emergent systematicity in {\\{}VQA{\\}}},\nauthor={Ankit Vani and Max Schwarzer and Yuchen Lu and Eeshan Dhekane and Aaron Courville},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Pd_oMxH8IlF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Pd_oMxH8IlF", "replyto": "Pd_oMxH8IlF", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2705/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538090330, "tmdate": 1606915763777, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2705/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2705/-/Official_Review"}}}], "count": 14}