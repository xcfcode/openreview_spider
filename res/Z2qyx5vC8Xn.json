{"notes": [{"id": "Z2qyx5vC8Xn", "original": "ZCVpf8KLt2J", "number": 497, "cdate": 1601308062368, "ddate": null, "tcdate": 1601308062368, "tmdate": 1614985707616, "tddate": null, "forum": "Z2qyx5vC8Xn", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Temporal Difference Uncertainties as a Signal for Exploration", "authorids": ["~Sebastian_Flennerhag1", "~Jane_X_Wang1", "~Pablo_Sprechmann1", "~Francesco_Visin1", "~Alexandre_Galashov1", "~Steven_Kapturowski1", "~Diana_L_Borsa1", "~Nicolas_Heess1", "~Andre_Barreto1", "~Razvan_Pascanu1"], "authors": ["Sebastian Flennerhag", "Jane X Wang", "Pablo Sprechmann", "Francesco Visin", "Alexandre Galashov", "Steven Kapturowski", "Diana L Borsa", "Nicolas Heess", "Andre Barreto", "Razvan Pascanu"], "keywords": ["deep reinforcement learning", "deep-rl", "exploration"], "abstract": "An effective approach to exploration in reinforcement learning is to rely on an agent's uncertainty over the optimal policy, which can yield near-optimal exploration strategies in tabular settings. However, in non-tabular settings that involve function approximators, obtaining accurate uncertainty estimates is almost as challenging as the exploration problem itself. In this paper, we highlight that value estimates are easily biased and temporally inconsistent. In light of this, we propose a novel method for estimating uncertainty over the value function that relies on inducing a distribution over temporal difference errors. This exploration signal controls for state-action transitions so as to isolate uncertainty in value that is due to uncertainty over the agent's parameters. Because our measure of uncertainty conditions on state-action transitions, we cannot act on this measure directly. Instead, we incorporate it as an intrinsic reward and treat exploration as a separate learning problem, induced by the agent's temporal difference uncertainties. We introduce a distinct exploration policy that learns to collect data with high estimated uncertainty, which gives rise to a curriculum that smoothly changes throughout learning and vanishes in the limit of perfect value estimates. We evaluate our method on hard exploration tasks, including Deep Sea and Atari 2600 environments and find that our proposed form of exploration facilitates efficient exploration.", "one-sentence_summary": "A method for exploration based on learning from uncertainty over the agent's value-function.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "flennerhag|temporal_difference_uncertainties_as_a_signal_for_exploration", "pdf": "/pdf/b1f081d47000b34d557d4a38a25e4f35a83ae58a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=kbcQqAnHa", "_bibtex": "@misc{\nflennerhag2021temporal,\ntitle={Temporal Difference Uncertainties as a Signal for Exploration},\nauthor={Sebastian Flennerhag and Jane X Wang and Pablo Sprechmann and Francesco Visin and Alexandre Galashov and Steven Kapturowski and Diana L Borsa and Nicolas Heess and Andre Barreto and Razvan Pascanu},\nyear={2021},\nurl={https://openreview.net/forum?id=Z2qyx5vC8Xn}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "f_KymhueUWb", "original": null, "number": 1, "cdate": 1610040436073, "ddate": null, "tcdate": 1610040436073, "tmdate": 1610474036680, "tddate": null, "forum": "Z2qyx5vC8Xn", "replyto": "Z2qyx5vC8Xn", "invitation": "ICLR.cc/2021/Conference/Paper497/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The submitted paper contains interesting theoretical insights into common approaches for exploration and proposes a new way for deriving intrinsic rewards for exploration which is evaluated in several benchmark environments. While all reviewers appreciate these aspects, there are concerns about whether the paper is ready for publication. In particular, the authors\u2019 response did not clarify all open questions and concerns (although the authors already improved the paper a lot by updating the submitted paper according to recommendations/questions of the reviewers). After discussions and author feedback, 3 knowledgable reviewers suggest (weak) rejection of the paper and 1 reviewer suggested acceptance of the paper. Considering this, I recommend to reject the paper but I would like to encourage the authors to consider the comments of the reviewers to revise their paper accordingly, as I expect the paper to then turn into a strong and impactful one."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Temporal Difference Uncertainties as a Signal for Exploration", "authorids": ["~Sebastian_Flennerhag1", "~Jane_X_Wang1", "~Pablo_Sprechmann1", "~Francesco_Visin1", "~Alexandre_Galashov1", "~Steven_Kapturowski1", "~Diana_L_Borsa1", "~Nicolas_Heess1", "~Andre_Barreto1", "~Razvan_Pascanu1"], "authors": ["Sebastian Flennerhag", "Jane X Wang", "Pablo Sprechmann", "Francesco Visin", "Alexandre Galashov", "Steven Kapturowski", "Diana L Borsa", "Nicolas Heess", "Andre Barreto", "Razvan Pascanu"], "keywords": ["deep reinforcement learning", "deep-rl", "exploration"], "abstract": "An effective approach to exploration in reinforcement learning is to rely on an agent's uncertainty over the optimal policy, which can yield near-optimal exploration strategies in tabular settings. However, in non-tabular settings that involve function approximators, obtaining accurate uncertainty estimates is almost as challenging as the exploration problem itself. In this paper, we highlight that value estimates are easily biased and temporally inconsistent. In light of this, we propose a novel method for estimating uncertainty over the value function that relies on inducing a distribution over temporal difference errors. This exploration signal controls for state-action transitions so as to isolate uncertainty in value that is due to uncertainty over the agent's parameters. Because our measure of uncertainty conditions on state-action transitions, we cannot act on this measure directly. Instead, we incorporate it as an intrinsic reward and treat exploration as a separate learning problem, induced by the agent's temporal difference uncertainties. We introduce a distinct exploration policy that learns to collect data with high estimated uncertainty, which gives rise to a curriculum that smoothly changes throughout learning and vanishes in the limit of perfect value estimates. We evaluate our method on hard exploration tasks, including Deep Sea and Atari 2600 environments and find that our proposed form of exploration facilitates efficient exploration.", "one-sentence_summary": "A method for exploration based on learning from uncertainty over the agent's value-function.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "flennerhag|temporal_difference_uncertainties_as_a_signal_for_exploration", "pdf": "/pdf/b1f081d47000b34d557d4a38a25e4f35a83ae58a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=kbcQqAnHa", "_bibtex": "@misc{\nflennerhag2021temporal,\ntitle={Temporal Difference Uncertainties as a Signal for Exploration},\nauthor={Sebastian Flennerhag and Jane X Wang and Pablo Sprechmann and Francesco Visin and Alexandre Galashov and Steven Kapturowski and Diana L Borsa and Nicolas Heess and Andre Barreto and Razvan Pascanu},\nyear={2021},\nurl={https://openreview.net/forum?id=Z2qyx5vC8Xn}\n}"}, "tags": [], "invitation": {"reply": {"forum": "Z2qyx5vC8Xn", "replyto": "Z2qyx5vC8Xn", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040436060, "tmdate": 1610474036665, "id": "ICLR.cc/2021/Conference/Paper497/-/Decision"}}}, {"id": "-ShCtRgNyfm", "original": null, "number": 3, "cdate": 1603939290807, "ddate": null, "tcdate": 1603939290807, "tmdate": 1607313722086, "tddate": null, "forum": "Z2qyx5vC8Xn", "replyto": "Z2qyx5vC8Xn", "invitation": "ICLR.cc/2021/Conference/Paper497/-/Official_Review", "content": {"title": "Official Review for Reviewer 1", "review": "This paper proposes to use an intrinsic reward based on uncertainties calculated from temporal difference errors. The approach, called Temporal Difference Uncertainties (TDU), estimates the variance of td errors across multiple (bootstrapped) parameters, for a given state, action, next state and reward, where variability is due only to variance in parameters. The other addition is to learn a separate set of action-values that use this intrinsic reward, from the bootstrap set. Actions are then taken by randomly sampling an action-value function from the combined set. \n\nThe idea of using td uncertainties as an intrinsic reward is interesting, and should help avoid the fact that bootstrapping value functions alone likely provides insufficient exploration. However, the paper in its current form is not yet ready for publication for two main reasons. First, there are significant gaps in motivating and detailing the TDU technique. In sections 1 and 2, TDU is motivated as an exploration method deriving \u201can intrinsic reward from the agent\u2019s uncertainty over the value function.\u201d  In particular, it is heavily implied that TDU\u2019s uncertainty estimate does not suffer the bias indicated by Lemma 1.  But it is not detailed how the quantity \\sigma(\\tau) estimates value function uncertainty, nor how it is unbiased. Why is it a better choice than, for example, directly using the bootstrapped set of action-values? There is some intuition provided, as well as an informal argument following from the distribution p(\\delta|\\tau) to uncertainty over value function parameters \\theta.  But this intuition and lack of formality is at odds with the formality highlighting that estimating value function uncertainty is hard.\n\nSecond, the empirical results appear to be obtained with 3 runs and do not provide significant evidence of improvements. Considering the apparent variance in all techniques, no meaningful conclusions may be drawn from comparison between the average of so few samples. The reported shading is also not explained, though I suspect it is standard errors. For only 3 samples, standard errors are not reliable, and in either case here are quite large. It would be better to run on few environments, and try to provide a stronger claim about the role of tdu. Even better would be to also highlight if results change significantly with changes to hyperparameters.   \n \nBelow, more detailed comments are given about the paper. \n\nIt would be useful to better discuss the use of Bootstrapped DQN (BDQN), and why this approach improves on BDQN. In Janz et al. 2019, there is a thoughtful and detailed analysis of one particular failing of BDQN methods (cf. section 5.3) including with a specific demonstrative environment (the binary tree MDP with randomized actions.) While this TDU paper does make a compelling argument that TDU addresses certain challenges of BDQN methods, TDU\u2019s performance on the binary tree MDP (or one sufficiently similar) is not included.  This is a nontrivial omission.  If TDU succeeds on the environment, then a significant challenge is overcome. And if TDU does not succeed, then it is still worthy of publication, just with some discussion.\n\nThe theoretical contribution seems like it could be interesting, but it is not fully clear. Is this a statement about all methods that try to estimate some form of value uncertainty? Or those based on PSRL? Lemma 1 is summarized as \u201cthe harder it is to generalise, the more likely we are to observe a bias\u201d in value function estimation, which is helpful, but warrants further detail.  A worked, demonstrative example may be edifying here.\n\nIt would be useful to discuss more why estimating TDUs is easy (as per the title of Section 3). It looks like the quantity in Equation 4 relies on p(theta), which was stated to be difficult to maintain. A bootstrapped distribution is used to get, as described in Section 4, but Section 3 did not make it clear to me why estimating sigma(tau) was straightforward. There are two additional statements here that could use clarification:\n1. Why does using the standard deviation put this bonus on the same scale as the reward?\n2. The paper says: \u201cWe compare TDU to (a) a version where \u03c3 is defined as standard deviation over Q and (b) where \u03c3(Q) is used as an upper confidence bound in the policy, instead of as an intrinsic reward (Figure 2).\u201d  These choices are very interesting, but should be explained in more detail.  Especially why their results indicate that \u201cthe key to TDU\u2019s success is that the intrinsic reward relies on a distribution of TD-errors to control for environment uncertainty.\u201c\n\nMinor comments:\n1. \u201cneural networks, which are prone to overfitting and tend to generalise through interpolation (e.g. Li et al., 2020; Liu et al., 2020; Belkin et al., 2019)\u201d  This is not an accurate representation of the cited results.  Those papers do not argue that neural networks are prone to overfitting.  Indeed, they argue the opposite: neural networks do not seem to overfit in the overparameterized limit, despite their tendency to interpolate the training set.  That is, generalization is maintained, where high generalization is characterized as per convention: low prediction risk w.r.t. an independently sampled test set.  While this is a significant mischaracterization, it does not appear to be critical to the paper, so we choose not to include it in the rejection justification.\n\n2. The colored bar chart to render parameter sweeps (Fig 2 left, center left, and center right) is a bit distracting. The use of color does not seem to be helping here. \n\n3. The related work section describes and cites some of the same work as the introduction, and is hence some of it is a bit redundant.  There is no need to duplicate that part of the survey in this section.\n\n4. There is an incomplete sentence: \u201cWhile this can be effective in sparse reward settings, ... it can also lead to arbitrarily bad as the exploration (see analysis in Osband et al., 2019).\u201d\n\n5. In appendix section D.1, there appears to be a missing citation: \u201cFinally, instead of n-step returns we utilize Peng\u2019s Q(\u03bb) as was done in (Value-driven Hindsight Modelling: citation needed).\u201d\n\n6. This work looks at computing variances directly, and using that variance for uncertainty, rather than using the bootstrapped action-values. This citation seems relevant: \"Context-Dependent Upper-Confidence Bounds for Directed Exploration\", Kumaraswamy et al., 2018.  \n\n------- Update\nThe response and update was a huge improvement. I now much better understand the goals, and the authors added some content that I think made the paper stronger and clearer. One outstanding issue is still that I do not understand why E_M[Q_M^pi] is the gold standard, and why bias is measured relative to that quantity. I explain this more below, but first mention some of the additions I really liked. For an upcoming paper, if this issue is remedied, I think this will be a good paper. \n\nThank you for introducing Proposition 2 and the explanation about bias beforehand. This very much clarifies the motivation for using TD errors. The result itself highlights that using TDU will have a lower bias to the true expected TD error if the bias for the action-values for (s,a) and (s\u2019,a\u2019) are both in the same direction (both positive or both negative). Otherwise, however, it looks like the bias of TDU is strictly worse. One issue though with this result is that the magnitudes of these quantities could be different. The comparison between bias for TDU and the bias for Q seem a bit like comparing apples and oranges, and I would in fact expect the TD error to be smaller in magnitude and so naturally have a smaller bias. How much is due to this and how much to relative bias reductions? I suspect there is a real bias reduction here, but clarifying this would help.\n\nFor the variance result, it seems better to directly report 47, and the discuss ramifications, rather than writing that they all need to be approximately equal. By the way, it is too bad that the result is a bit weaker for the variance, which is precisely the quantity you care about for defining your rewards. But nonetheless this formalization is helpful and provides solid insight.\n\nI like that the empirical work was improved, including adding some comments about significance. I also very much appreciate the ablation, where you use the variance directly from the bootstrapped action-values as an intrinsic reward. My only concern here is that the magnitude of rewards might be quite different, since the TD error should be smaller than the action-values themselves. This might mean different beta are needed.\n\nHowever, I remain unsure about the importance of this bias that TDU mitigates. You state: \u201cOur analysis shows that biased estimates arise because uncertainty estimates require an integration over unknown future state visitations.\u201c It remains a strong statement that uncertainty estimates require integration over unknown future states. As one example where this does not seem to be true is the Kumaraswamy paper you have cited. They show that if you use LSTD to get estimates of the action-values for a fixed policy, then you can get an estimate of the variance of the parameters. Maybe this setting assumes too much, and so it does not invalidate your result. But, I do believe a more clear argument is needed in this section for this result, as I expand on below.\n\n\u201cMethods that rely on posterior sampling under function approximators assume that the induced distribution, p(Q\u03b8 ), is an accurate estimate of the agent\u2019s uncertainty over its value function, p(Q\u03c0 ), so that sampling Q\u03b8 \u223c p(Q\u03b8 ) is approximately equivalent to sampling from Q\u03c0 \u223c p(Q\u03c0 ). \u201d This is a strong statement. I do not see why it is true. Are you suggesting that the agent must have the true Qpi? Is it not enough to use uncertainty estimates (epistemic uncertainty) for a function approximator? This result seems to show: if we want to mimic uncertainty estimates over true action-values for different models, then this is not possible under function approximation. But, that is maybe reasonable. Instead, shouldn\u2019t we ask: how can we mimic uncertainty over approximate action-values for different models? (i.e., relative to our function class)\n\nAdditionally, you call E_theta[Q_theta] an estimator and discuss it\u2019s bias. But, isn\u2019t that quantity not random? I presume E_theta[Q_theta] is the expectation, and the difference to E_M[Q_pi] is the bias. But, then what is the estimator that is biased?\n\nMinor comments:\n\n\u201cHowever, in non-tabular settings that involve function approximators, obtaining accurate uncertainty estimates is almost as challenging as the exploration problem itself. \u201c What does this mean?\n\u201cstate-action transitions \u201d what is that? I think you mean just transition, since you condition on the whole thing\nIn the proof of Lemma 1, the Variance would remove the expectation over r term. Your result still holds, but the proof itself looks like it should be separated into the two cases.\n\u201cWhile Proposition 1 states that we cannot remove this bias unless we are willing to maintain a full posterior p(\u03b8), \u201d It is not clear how this result shows this. What if I maintained a full Gaussian posterior over theta? Would that solve the problem? What is a partial posterior?\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper497/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper497/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Temporal Difference Uncertainties as a Signal for Exploration", "authorids": ["~Sebastian_Flennerhag1", "~Jane_X_Wang1", "~Pablo_Sprechmann1", "~Francesco_Visin1", "~Alexandre_Galashov1", "~Steven_Kapturowski1", "~Diana_L_Borsa1", "~Nicolas_Heess1", "~Andre_Barreto1", "~Razvan_Pascanu1"], "authors": ["Sebastian Flennerhag", "Jane X Wang", "Pablo Sprechmann", "Francesco Visin", "Alexandre Galashov", "Steven Kapturowski", "Diana L Borsa", "Nicolas Heess", "Andre Barreto", "Razvan Pascanu"], "keywords": ["deep reinforcement learning", "deep-rl", "exploration"], "abstract": "An effective approach to exploration in reinforcement learning is to rely on an agent's uncertainty over the optimal policy, which can yield near-optimal exploration strategies in tabular settings. However, in non-tabular settings that involve function approximators, obtaining accurate uncertainty estimates is almost as challenging as the exploration problem itself. In this paper, we highlight that value estimates are easily biased and temporally inconsistent. In light of this, we propose a novel method for estimating uncertainty over the value function that relies on inducing a distribution over temporal difference errors. This exploration signal controls for state-action transitions so as to isolate uncertainty in value that is due to uncertainty over the agent's parameters. Because our measure of uncertainty conditions on state-action transitions, we cannot act on this measure directly. Instead, we incorporate it as an intrinsic reward and treat exploration as a separate learning problem, induced by the agent's temporal difference uncertainties. We introduce a distinct exploration policy that learns to collect data with high estimated uncertainty, which gives rise to a curriculum that smoothly changes throughout learning and vanishes in the limit of perfect value estimates. We evaluate our method on hard exploration tasks, including Deep Sea and Atari 2600 environments and find that our proposed form of exploration facilitates efficient exploration.", "one-sentence_summary": "A method for exploration based on learning from uncertainty over the agent's value-function.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "flennerhag|temporal_difference_uncertainties_as_a_signal_for_exploration", "pdf": "/pdf/b1f081d47000b34d557d4a38a25e4f35a83ae58a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=kbcQqAnHa", "_bibtex": "@misc{\nflennerhag2021temporal,\ntitle={Temporal Difference Uncertainties as a Signal for Exploration},\nauthor={Sebastian Flennerhag and Jane X Wang and Pablo Sprechmann and Francesco Visin and Alexandre Galashov and Steven Kapturowski and Diana L Borsa and Nicolas Heess and Andre Barreto and Razvan Pascanu},\nyear={2021},\nurl={https://openreview.net/forum?id=Z2qyx5vC8Xn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Z2qyx5vC8Xn", "replyto": "Z2qyx5vC8Xn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper497/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538141829, "tmdate": 1606915782663, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper497/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper497/-/Official_Review"}}}, {"id": "UQ7ohw8s2kP", "original": null, "number": 4, "cdate": 1604113217711, "ddate": null, "tcdate": 1604113217711, "tmdate": 1607305340092, "tddate": null, "forum": "Z2qyx5vC8Xn", "replyto": "Z2qyx5vC8Xn", "invitation": "ICLR.cc/2021/Conference/Paper497/-/Official_Review", "content": {"title": "Interesting work on exploration in RL, needs a clarification on transition function", "review": "This paper proposes a method for exploration in reinforcement learning by using the uncertainty over the value function as an intrinsic reward. It also offers an interesting theoretical analysis on the problem of estimating uncertainty over the value function. The paper is clearly written in general and has mentioned the related methods sufficiently. Moreover, the authors have compared their method with state-of-the-art models. In the experimental results section, the authors have shown that their model works well both in deterministic and stochastic environment. The noise in this stochastic environment, however, is very low (To be more precise the entropy of transition function is low). This is actually my main question/concern about the paper. In the beginning of section 3, the authors have mentioned that they \"fix\" the transition function in calculation of uncertainty over the value function. Does this mean that in that part, p(s'|a, s) is set to 1 for the most probable state and 0 for others? If this is the case, the method might not work well when the entropy of the transition function is high (e.g. the agent goes to one state 50% of the time, and to another 50% with an action), or in continuous state space environments where the actions are noisy. In the mentioned cases, sampling cases might actually work better because at least they consider those states in the future. I think it will improve the paper a lot if the authors add such environments to their analysis and discuss such problems.  \n\nUpdate: Thanks the authors for their response. Based on the other reviews and authors' response I decrease my score by 1 point.  ", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper497/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper497/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Temporal Difference Uncertainties as a Signal for Exploration", "authorids": ["~Sebastian_Flennerhag1", "~Jane_X_Wang1", "~Pablo_Sprechmann1", "~Francesco_Visin1", "~Alexandre_Galashov1", "~Steven_Kapturowski1", "~Diana_L_Borsa1", "~Nicolas_Heess1", "~Andre_Barreto1", "~Razvan_Pascanu1"], "authors": ["Sebastian Flennerhag", "Jane X Wang", "Pablo Sprechmann", "Francesco Visin", "Alexandre Galashov", "Steven Kapturowski", "Diana L Borsa", "Nicolas Heess", "Andre Barreto", "Razvan Pascanu"], "keywords": ["deep reinforcement learning", "deep-rl", "exploration"], "abstract": "An effective approach to exploration in reinforcement learning is to rely on an agent's uncertainty over the optimal policy, which can yield near-optimal exploration strategies in tabular settings. However, in non-tabular settings that involve function approximators, obtaining accurate uncertainty estimates is almost as challenging as the exploration problem itself. In this paper, we highlight that value estimates are easily biased and temporally inconsistent. In light of this, we propose a novel method for estimating uncertainty over the value function that relies on inducing a distribution over temporal difference errors. This exploration signal controls for state-action transitions so as to isolate uncertainty in value that is due to uncertainty over the agent's parameters. Because our measure of uncertainty conditions on state-action transitions, we cannot act on this measure directly. Instead, we incorporate it as an intrinsic reward and treat exploration as a separate learning problem, induced by the agent's temporal difference uncertainties. We introduce a distinct exploration policy that learns to collect data with high estimated uncertainty, which gives rise to a curriculum that smoothly changes throughout learning and vanishes in the limit of perfect value estimates. We evaluate our method on hard exploration tasks, including Deep Sea and Atari 2600 environments and find that our proposed form of exploration facilitates efficient exploration.", "one-sentence_summary": "A method for exploration based on learning from uncertainty over the agent's value-function.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "flennerhag|temporal_difference_uncertainties_as_a_signal_for_exploration", "pdf": "/pdf/b1f081d47000b34d557d4a38a25e4f35a83ae58a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=kbcQqAnHa", "_bibtex": "@misc{\nflennerhag2021temporal,\ntitle={Temporal Difference Uncertainties as a Signal for Exploration},\nauthor={Sebastian Flennerhag and Jane X Wang and Pablo Sprechmann and Francesco Visin and Alexandre Galashov and Steven Kapturowski and Diana L Borsa and Nicolas Heess and Andre Barreto and Razvan Pascanu},\nyear={2021},\nurl={https://openreview.net/forum?id=Z2qyx5vC8Xn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Z2qyx5vC8Xn", "replyto": "Z2qyx5vC8Xn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper497/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538141829, "tmdate": 1606915782663, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper497/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper497/-/Official_Review"}}}, {"id": "eLVCx-cNs7", "original": null, "number": 1, "cdate": 1603202893293, "ddate": null, "tcdate": 1603202893293, "tmdate": 1606679248274, "tddate": null, "forum": "Z2qyx5vC8Xn", "replyto": "Z2qyx5vC8Xn", "invitation": "ICLR.cc/2021/Conference/Paper497/-/Official_Review", "content": {"title": "Focused and sound", "review": "Summary:\nThe paper proposes a novel heuristic approach to estimate the uncertainty of the value function of an agent. Since this heuristic for estimating the uncertainty can be applied not only to tabular RL, but also when using function approximation, it is an interesting approach.\n\nStrong points:\nThe proposed heuristic can be used to estimate the uncertainty not only for tabular RL, but also when using function approximation, without relying on the ability of the function approximators to specify a value for the uncertainty.\n\nWeak points:\nThe investigated benchmarks are essentially deterministic and cover a very limited range of the spectrum of RL in general.\n\nRecommendation:\nThe paper is well written and clearly structured. It introduces a new approach that is likely to be beneficial in a relevant number of problems. I recommend to accept the paper.\n\nAdditional feedback with the aim to improve the paper:\n\u201eHowever [\u2026] obtaining accurate uncertainty estimates is almost as challenging a problem.\u201c As challenging as what?\nIt remains unclear, what is meant by \"diverse and deep exploration\".\nAt (\\beta >> 0), please use a proper LaTeX command for >>.\nPlease do correct missing capital letters in the bibliography: e.g.  bayesian, q-\n\n\n-----------------\n(Nov 26.) After reading Review1 I lower my Confidence.\n(Nov 29.) Taking into account the other reviews, the authors' responses to these reviews and the discussion, I now think the paper is not quite ready for publication and lower the score to 5.\n\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper497/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper497/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Temporal Difference Uncertainties as a Signal for Exploration", "authorids": ["~Sebastian_Flennerhag1", "~Jane_X_Wang1", "~Pablo_Sprechmann1", "~Francesco_Visin1", "~Alexandre_Galashov1", "~Steven_Kapturowski1", "~Diana_L_Borsa1", "~Nicolas_Heess1", "~Andre_Barreto1", "~Razvan_Pascanu1"], "authors": ["Sebastian Flennerhag", "Jane X Wang", "Pablo Sprechmann", "Francesco Visin", "Alexandre Galashov", "Steven Kapturowski", "Diana L Borsa", "Nicolas Heess", "Andre Barreto", "Razvan Pascanu"], "keywords": ["deep reinforcement learning", "deep-rl", "exploration"], "abstract": "An effective approach to exploration in reinforcement learning is to rely on an agent's uncertainty over the optimal policy, which can yield near-optimal exploration strategies in tabular settings. However, in non-tabular settings that involve function approximators, obtaining accurate uncertainty estimates is almost as challenging as the exploration problem itself. In this paper, we highlight that value estimates are easily biased and temporally inconsistent. In light of this, we propose a novel method for estimating uncertainty over the value function that relies on inducing a distribution over temporal difference errors. This exploration signal controls for state-action transitions so as to isolate uncertainty in value that is due to uncertainty over the agent's parameters. Because our measure of uncertainty conditions on state-action transitions, we cannot act on this measure directly. Instead, we incorporate it as an intrinsic reward and treat exploration as a separate learning problem, induced by the agent's temporal difference uncertainties. We introduce a distinct exploration policy that learns to collect data with high estimated uncertainty, which gives rise to a curriculum that smoothly changes throughout learning and vanishes in the limit of perfect value estimates. We evaluate our method on hard exploration tasks, including Deep Sea and Atari 2600 environments and find that our proposed form of exploration facilitates efficient exploration.", "one-sentence_summary": "A method for exploration based on learning from uncertainty over the agent's value-function.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "flennerhag|temporal_difference_uncertainties_as_a_signal_for_exploration", "pdf": "/pdf/b1f081d47000b34d557d4a38a25e4f35a83ae58a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=kbcQqAnHa", "_bibtex": "@misc{\nflennerhag2021temporal,\ntitle={Temporal Difference Uncertainties as a Signal for Exploration},\nauthor={Sebastian Flennerhag and Jane X Wang and Pablo Sprechmann and Francesco Visin and Alexandre Galashov and Steven Kapturowski and Diana L Borsa and Nicolas Heess and Andre Barreto and Razvan Pascanu},\nyear={2021},\nurl={https://openreview.net/forum?id=Z2qyx5vC8Xn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Z2qyx5vC8Xn", "replyto": "Z2qyx5vC8Xn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper497/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538141829, "tmdate": 1606915782663, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper497/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper497/-/Official_Review"}}}, {"id": "aH_lAgKDB8Z", "original": null, "number": 7, "cdate": 1606142032992, "ddate": null, "tcdate": 1606142032992, "tmdate": 1606142032992, "tddate": null, "forum": "Z2qyx5vC8Xn", "replyto": "eLVCx-cNs7", "invitation": "ICLR.cc/2021/Conference/Paper497/-/Official_Comment", "content": {"title": "Thank you for your review.", "comment": "Thank you for your review, we are pleased to hear that you find our paper interesting! \n\nIn response to your point about experiments, we chose to produce rigorous analyses on Deep Sea and Atari, while also including full results on Bsuite. While Atari is indeed largely deterministic, please do note that the stochastic version of Deep Sea is not. The stochastic version of Deep Sea adds noise to the transition kernel by generating the \u2018wrong\u2019 transition with a probability 1 / N. This is a relatively high degree of uncertainty given that if the agent suffers any \u2018wrong\u2019 transitions, it cannot get any reward in that episode. Moreover, even if Atari is deterministic, it does pose a significant exploration challenge and the distributed nature of R2D2 injects substantial noise into the learning progress. \n\nThank you for your additional comments, these have been addressed in the revised manuscript."}, "signatures": ["ICLR.cc/2021/Conference/Paper497/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper497/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Temporal Difference Uncertainties as a Signal for Exploration", "authorids": ["~Sebastian_Flennerhag1", "~Jane_X_Wang1", "~Pablo_Sprechmann1", "~Francesco_Visin1", "~Alexandre_Galashov1", "~Steven_Kapturowski1", "~Diana_L_Borsa1", "~Nicolas_Heess1", "~Andre_Barreto1", "~Razvan_Pascanu1"], "authors": ["Sebastian Flennerhag", "Jane X Wang", "Pablo Sprechmann", "Francesco Visin", "Alexandre Galashov", "Steven Kapturowski", "Diana L Borsa", "Nicolas Heess", "Andre Barreto", "Razvan Pascanu"], "keywords": ["deep reinforcement learning", "deep-rl", "exploration"], "abstract": "An effective approach to exploration in reinforcement learning is to rely on an agent's uncertainty over the optimal policy, which can yield near-optimal exploration strategies in tabular settings. However, in non-tabular settings that involve function approximators, obtaining accurate uncertainty estimates is almost as challenging as the exploration problem itself. In this paper, we highlight that value estimates are easily biased and temporally inconsistent. In light of this, we propose a novel method for estimating uncertainty over the value function that relies on inducing a distribution over temporal difference errors. This exploration signal controls for state-action transitions so as to isolate uncertainty in value that is due to uncertainty over the agent's parameters. Because our measure of uncertainty conditions on state-action transitions, we cannot act on this measure directly. Instead, we incorporate it as an intrinsic reward and treat exploration as a separate learning problem, induced by the agent's temporal difference uncertainties. We introduce a distinct exploration policy that learns to collect data with high estimated uncertainty, which gives rise to a curriculum that smoothly changes throughout learning and vanishes in the limit of perfect value estimates. We evaluate our method on hard exploration tasks, including Deep Sea and Atari 2600 environments and find that our proposed form of exploration facilitates efficient exploration.", "one-sentence_summary": "A method for exploration based on learning from uncertainty over the agent's value-function.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "flennerhag|temporal_difference_uncertainties_as_a_signal_for_exploration", "pdf": "/pdf/b1f081d47000b34d557d4a38a25e4f35a83ae58a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=kbcQqAnHa", "_bibtex": "@misc{\nflennerhag2021temporal,\ntitle={Temporal Difference Uncertainties as a Signal for Exploration},\nauthor={Sebastian Flennerhag and Jane X Wang and Pablo Sprechmann and Francesco Visin and Alexandre Galashov and Steven Kapturowski and Diana L Borsa and Nicolas Heess and Andre Barreto and Razvan Pascanu},\nyear={2021},\nurl={https://openreview.net/forum?id=Z2qyx5vC8Xn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Z2qyx5vC8Xn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper497/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper497/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper497/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper497/Authors|ICLR.cc/2021/Conference/Paper497/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper497/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870330, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper497/-/Official_Comment"}}}, {"id": "1jImYg2CKdj", "original": null, "number": 6, "cdate": 1606141624299, "ddate": null, "tcdate": 1606141624299, "tmdate": 1606141624299, "tddate": null, "forum": "Z2qyx5vC8Xn", "replyto": "0ckDMF5mIeq", "invitation": "ICLR.cc/2021/Conference/Paper497/-/Official_Comment", "content": {"title": "Thank you for your review and support!", "comment": "Thank you for your review and feedback; we are glad that you appreciated our paper! Please find answers to your questions below. We use **\"lorem ipsum\"** to reference parts of your review.\n\n**\u201cI am not convinced that the proposed bootstrap method for uncertainty estimation has a clear Bayesian interpretation as a form of approximate inference.\u201d**\n\nThank you for pointing this out. We use Bayesian terminology to describe prior works (such as BDQN). We have carefully revised the presentation of TDU to avoid implying that TDU itself is Bayesian. While it relies on uncertainty estimates to prioritise exploration and we do retain BDQNs posterior sampling mechanism, it is not immediately clear that introducing TDU\u2019s intrinsic reward still admits a Bayesian interpretation and establishing this is beyond the scope of our paper.\n  \n**\u201cthe author claims that since reward uncertainty estimation is biased then networks cannot meaningfully generalize.\u201d**\n\nOur claim is related to Lemma 1 and Proposition 1. Proposition 1 establishes that a bias results from having more unique state-action representations \u03c8(s,a) than degrees of freedom in the final layer w. From Lemma 1, this means that estimates of uncertainty are biased. In other words, to have unbiased estimates of uncertainty, the final layer w must have as many parameters as there are unique state-action pairs in the MDP, so must be able to produce a tabular representation. By definition, this means that estimators that are subject to Proposition 1 *cannot* generalise uncertainty estimates across state-action pairs, they are only unbiased when they are tabular, which implies no generalisation on the part of the network. \n\nWith that said, we do not claim that methods that rely on biased estimates of uncertainty cannot do meaningful exploration. Several of our baselines do perform well on the tasks that we consider, but our experimental results also demonstrates that estimators with smaller bias can yield better exploration.\n\n**\"It would have been informative to have a wider range of problems in the experiments section\"**\n\nWe chose to produce rigorous analyses on Deep Sea as well as the full Bsuite, along with several in-depth analyses of games from Atari and results for all 57 games. It is unclear what types of additional tasks the reviewer is referring to, but we have further added the binary tree MDP in response to reviewer 1."}, "signatures": ["ICLR.cc/2021/Conference/Paper497/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper497/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Temporal Difference Uncertainties as a Signal for Exploration", "authorids": ["~Sebastian_Flennerhag1", "~Jane_X_Wang1", "~Pablo_Sprechmann1", "~Francesco_Visin1", "~Alexandre_Galashov1", "~Steven_Kapturowski1", "~Diana_L_Borsa1", "~Nicolas_Heess1", "~Andre_Barreto1", "~Razvan_Pascanu1"], "authors": ["Sebastian Flennerhag", "Jane X Wang", "Pablo Sprechmann", "Francesco Visin", "Alexandre Galashov", "Steven Kapturowski", "Diana L Borsa", "Nicolas Heess", "Andre Barreto", "Razvan Pascanu"], "keywords": ["deep reinforcement learning", "deep-rl", "exploration"], "abstract": "An effective approach to exploration in reinforcement learning is to rely on an agent's uncertainty over the optimal policy, which can yield near-optimal exploration strategies in tabular settings. However, in non-tabular settings that involve function approximators, obtaining accurate uncertainty estimates is almost as challenging as the exploration problem itself. In this paper, we highlight that value estimates are easily biased and temporally inconsistent. In light of this, we propose a novel method for estimating uncertainty over the value function that relies on inducing a distribution over temporal difference errors. This exploration signal controls for state-action transitions so as to isolate uncertainty in value that is due to uncertainty over the agent's parameters. Because our measure of uncertainty conditions on state-action transitions, we cannot act on this measure directly. Instead, we incorporate it as an intrinsic reward and treat exploration as a separate learning problem, induced by the agent's temporal difference uncertainties. We introduce a distinct exploration policy that learns to collect data with high estimated uncertainty, which gives rise to a curriculum that smoothly changes throughout learning and vanishes in the limit of perfect value estimates. We evaluate our method on hard exploration tasks, including Deep Sea and Atari 2600 environments and find that our proposed form of exploration facilitates efficient exploration.", "one-sentence_summary": "A method for exploration based on learning from uncertainty over the agent's value-function.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "flennerhag|temporal_difference_uncertainties_as_a_signal_for_exploration", "pdf": "/pdf/b1f081d47000b34d557d4a38a25e4f35a83ae58a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=kbcQqAnHa", "_bibtex": "@misc{\nflennerhag2021temporal,\ntitle={Temporal Difference Uncertainties as a Signal for Exploration},\nauthor={Sebastian Flennerhag and Jane X Wang and Pablo Sprechmann and Francesco Visin and Alexandre Galashov and Steven Kapturowski and Diana L Borsa and Nicolas Heess and Andre Barreto and Razvan Pascanu},\nyear={2021},\nurl={https://openreview.net/forum?id=Z2qyx5vC8Xn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Z2qyx5vC8Xn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper497/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper497/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper497/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper497/Authors|ICLR.cc/2021/Conference/Paper497/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper497/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870330, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper497/-/Official_Comment"}}}, {"id": "vGh8I3KbcbX", "original": null, "number": 5, "cdate": 1606141297659, "ddate": null, "tcdate": 1606141297659, "tmdate": 1606141297659, "tddate": null, "forum": "Z2qyx5vC8Xn", "replyto": "UQ7ohw8s2kP", "invitation": "ICLR.cc/2021/Conference/Paper497/-/Official_Comment", "content": {"title": "Thank you for your review.", "comment": "Thank you for your review and feedback, we\u2019re glad you liked our paper! \n\nThank you for your question. By \u2018fix\u2019, we mean that we measure uncertainty over each (s, a, r, s\u2019). In contrast, prior methods estimate uncertainty over each (s, a). In both cases, the estimated uncertainty is obtained by training a (set of) neural network(s) by sampling mini-batches from a replay memory. \n\nTo illustrate the difference, consider an example where the agent is taking an action a in a state s and the transition kernel produces a new state s\u2019 with some probability otherwise a new state s\u2019\u2019. In prior methods, the estimated uncertainty would be over the action-value of the point (s, a), so would integrate over the two outcomes. If there is no uncertainty over the outcome s\u2019 and high uncertainty over the outcome s\u2019\u2019, these methods would not be able to differentiate between the two. In contrast, TDU estimates two separate quantities, the uncertainty over the transition (s, a, r\u2019, s\u2019) and over  (s, a, r\u2019\u2019, s\u2019\u2019). Consequently, if there is no uncertainty over (s, a, r\u2019, s\u2019), TDU would be able to make that distinction and produce the appropriate exploration signal. This is important for credit assignment: as we are using action-value functions, the exploration bonus should be attributed to the transition over which the agent is uncertain, rather than a state from which some transitions may induce uncertainty.\n\nAs for environment stochasticity, please do note that the stochastic version of Deep Sea adds noise to the transition kernel by generating the \u2018wrong\u2019 transition with a probability 1 / N, precisely as in the above example. For Deep Sea, this is a relatively high degree of uncertainty given that if the agent suffers any \u2018wrong\u2019 transitions, it cannot get any reward in that episode. We have added a discussion of this point in the revised version of the paper.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper497/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper497/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Temporal Difference Uncertainties as a Signal for Exploration", "authorids": ["~Sebastian_Flennerhag1", "~Jane_X_Wang1", "~Pablo_Sprechmann1", "~Francesco_Visin1", "~Alexandre_Galashov1", "~Steven_Kapturowski1", "~Diana_L_Borsa1", "~Nicolas_Heess1", "~Andre_Barreto1", "~Razvan_Pascanu1"], "authors": ["Sebastian Flennerhag", "Jane X Wang", "Pablo Sprechmann", "Francesco Visin", "Alexandre Galashov", "Steven Kapturowski", "Diana L Borsa", "Nicolas Heess", "Andre Barreto", "Razvan Pascanu"], "keywords": ["deep reinforcement learning", "deep-rl", "exploration"], "abstract": "An effective approach to exploration in reinforcement learning is to rely on an agent's uncertainty over the optimal policy, which can yield near-optimal exploration strategies in tabular settings. However, in non-tabular settings that involve function approximators, obtaining accurate uncertainty estimates is almost as challenging as the exploration problem itself. In this paper, we highlight that value estimates are easily biased and temporally inconsistent. In light of this, we propose a novel method for estimating uncertainty over the value function that relies on inducing a distribution over temporal difference errors. This exploration signal controls for state-action transitions so as to isolate uncertainty in value that is due to uncertainty over the agent's parameters. Because our measure of uncertainty conditions on state-action transitions, we cannot act on this measure directly. Instead, we incorporate it as an intrinsic reward and treat exploration as a separate learning problem, induced by the agent's temporal difference uncertainties. We introduce a distinct exploration policy that learns to collect data with high estimated uncertainty, which gives rise to a curriculum that smoothly changes throughout learning and vanishes in the limit of perfect value estimates. We evaluate our method on hard exploration tasks, including Deep Sea and Atari 2600 environments and find that our proposed form of exploration facilitates efficient exploration.", "one-sentence_summary": "A method for exploration based on learning from uncertainty over the agent's value-function.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "flennerhag|temporal_difference_uncertainties_as_a_signal_for_exploration", "pdf": "/pdf/b1f081d47000b34d557d4a38a25e4f35a83ae58a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=kbcQqAnHa", "_bibtex": "@misc{\nflennerhag2021temporal,\ntitle={Temporal Difference Uncertainties as a Signal for Exploration},\nauthor={Sebastian Flennerhag and Jane X Wang and Pablo Sprechmann and Francesco Visin and Alexandre Galashov and Steven Kapturowski and Diana L Borsa and Nicolas Heess and Andre Barreto and Razvan Pascanu},\nyear={2021},\nurl={https://openreview.net/forum?id=Z2qyx5vC8Xn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Z2qyx5vC8Xn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper497/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper497/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper497/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper497/Authors|ICLR.cc/2021/Conference/Paper497/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper497/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870330, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper497/-/Official_Comment"}}}, {"id": "LvtmVvs6ODR", "original": null, "number": 4, "cdate": 1606141119547, "ddate": null, "tcdate": 1606141119547, "tmdate": 1606141119547, "tddate": null, "forum": "Z2qyx5vC8Xn", "replyto": "Grj1X5hcAHg", "invitation": "ICLR.cc/2021/Conference/Paper497/-/Official_Comment", "content": {"title": "Rebuttal (continued)", "comment": "**\u201cTDU\u2019s performance on the binary tree MDP (or one sufficiently similar) is not included. This is a nontrivial omission.\u201d**\n\nThe binary tree MDP is a simplified version of Deep Sea where the episode terminates immediately after the agent takes a sub-optimal action. This makes exploration much easier, because the agent cannot explore further once an incorrect action has been taken. Because we conducted a rigorous analysis on Deep Sea, to satisfy space constraints we did not include this experiment. However, to appease reviewers, we have included results for the binary tree MDP in appendix. TDU improves the scaling factor significantly compared to BDQN. Moreover, TDU removes the need for prior functions in the bootstrap on this domain. Our results demonstrate conclusively that TDU enjoys a significantly superior scaling factor over BDQN.\n\n**\u201cThe theoretical contribution seems like it could be interesting, but it is not fully clear. Is this a statement about all methods that try to estimate some form of value uncertainty?\u201d**\n\nYes, this is a statement about uncertainty estimation (it does not say anything about how they might be used for exploration, it just so happens that PSRL methods rely heavily on uncertainty over the action-value function). Lemma 1 states that unbiased estimates of uncertainty over the action-value function requires Eqs 1. and 2. to hold. Proposition 1 states that, unless p(\\theta) is a full probability distribution (full covariance matrix), then Eqs 1. and 2. will not hold unless the final linear layer has more parameters than there are state-action pairs. This is a statement about the structure of p(\\theta) and holds for any method of estimation.\n\n**\u201cIt would be useful to discuss more why estimating TDUs is easy\u201d**\n\nIn response to this feedback, we have changed this section title to avoid confusion or to inflate claims. Uncertainty estimation over TD-errors is likely to have a smaller bias than uncertainty estimation over the action-value function, which is the main motivation for our proposed metric.\n\n**\"Why does using the standard deviation put this bonus on the same scale as the reward?\"**\n\nThe statement in the text is that it is \u201capproximately on the same scale\u201d. This follows because TD-error are approximately on the same scale as the reward: If Q(s, a) has not yet seen a given reward, then the TD-error is approximately equal to the reward, hence on the same order of magnitude as the reward. We take the standard deviation because it is known to be on the same unit of measure as its variable (which is not true for the variance), which for the TD-error means approximately on the same scale as the reward.\n\n**\u201cWe compare TDU to (a) a version where \u03c3 is defined as standard deviation over Q and (b) where \u03c3(Q) is used as an upper confidence bound in the policy, instead of as an intrinsic reward (Figure 2).\u201d These choices are very interesting, but should be explained in more detail. \u201c**\n\nWe apologise for failing to reference the Appendix, C.2, where we have a substantial analysis that explains these experiments in detail. We have added a reference to the manuscript.\n\n**\u201cneural networks, which are prone to overfitting and tend to generalise through interpolation (e.g. Li et al., 2020; Liu et al., 2020; Belkin et al., 2019). This is not an accurate representation of the cited results.\"**\n\nThank you for this comment. These references are with respect to that neural networks tend to generalise through interpolation, but we understand that it can be read in another way. Our point here refers to strong generalization, as in out-of-distribution generalization, which is extensively discussed in Li et. al. (2020). In particular while they show that neural networks can strongly generalize, this property is by no means a given. We have rephrased the reference to only refer to their tendency to interpolate over training data.\n\nFinally, thank you for pointing out the missing references and minor clarification issues. We have rectified these in the revised manuscript."}, "signatures": ["ICLR.cc/2021/Conference/Paper497/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper497/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Temporal Difference Uncertainties as a Signal for Exploration", "authorids": ["~Sebastian_Flennerhag1", "~Jane_X_Wang1", "~Pablo_Sprechmann1", "~Francesco_Visin1", "~Alexandre_Galashov1", "~Steven_Kapturowski1", "~Diana_L_Borsa1", "~Nicolas_Heess1", "~Andre_Barreto1", "~Razvan_Pascanu1"], "authors": ["Sebastian Flennerhag", "Jane X Wang", "Pablo Sprechmann", "Francesco Visin", "Alexandre Galashov", "Steven Kapturowski", "Diana L Borsa", "Nicolas Heess", "Andre Barreto", "Razvan Pascanu"], "keywords": ["deep reinforcement learning", "deep-rl", "exploration"], "abstract": "An effective approach to exploration in reinforcement learning is to rely on an agent's uncertainty over the optimal policy, which can yield near-optimal exploration strategies in tabular settings. However, in non-tabular settings that involve function approximators, obtaining accurate uncertainty estimates is almost as challenging as the exploration problem itself. In this paper, we highlight that value estimates are easily biased and temporally inconsistent. In light of this, we propose a novel method for estimating uncertainty over the value function that relies on inducing a distribution over temporal difference errors. This exploration signal controls for state-action transitions so as to isolate uncertainty in value that is due to uncertainty over the agent's parameters. Because our measure of uncertainty conditions on state-action transitions, we cannot act on this measure directly. Instead, we incorporate it as an intrinsic reward and treat exploration as a separate learning problem, induced by the agent's temporal difference uncertainties. We introduce a distinct exploration policy that learns to collect data with high estimated uncertainty, which gives rise to a curriculum that smoothly changes throughout learning and vanishes in the limit of perfect value estimates. We evaluate our method on hard exploration tasks, including Deep Sea and Atari 2600 environments and find that our proposed form of exploration facilitates efficient exploration.", "one-sentence_summary": "A method for exploration based on learning from uncertainty over the agent's value-function.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "flennerhag|temporal_difference_uncertainties_as_a_signal_for_exploration", "pdf": "/pdf/b1f081d47000b34d557d4a38a25e4f35a83ae58a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=kbcQqAnHa", "_bibtex": "@misc{\nflennerhag2021temporal,\ntitle={Temporal Difference Uncertainties as a Signal for Exploration},\nauthor={Sebastian Flennerhag and Jane X Wang and Pablo Sprechmann and Francesco Visin and Alexandre Galashov and Steven Kapturowski and Diana L Borsa and Nicolas Heess and Andre Barreto and Razvan Pascanu},\nyear={2021},\nurl={https://openreview.net/forum?id=Z2qyx5vC8Xn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Z2qyx5vC8Xn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper497/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper497/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper497/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper497/Authors|ICLR.cc/2021/Conference/Paper497/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper497/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870330, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper497/-/Official_Comment"}}}, {"id": "Grj1X5hcAHg", "original": null, "number": 3, "cdate": 1606141049274, "ddate": null, "tcdate": 1606141049274, "tmdate": 1606141049274, "tddate": null, "forum": "Z2qyx5vC8Xn", "replyto": "-ShCtRgNyfm", "invitation": "ICLR.cc/2021/Conference/Paper497/-/Official_Comment", "content": {"title": "Thank you for your review; we have made significant revisions to address all your concerns.", "comment": "Thank you for your detailed review and feedback. We appreciate that you found our proposed method interesting! We have made major revisions to fully address all your concerns, please see below for full details. We use **\"lorem ipsum\"**  to reference parts of your review. \n\n**\u201cit is heavily implied that TDU\u2019s uncertainty estimate does not suffer the bias indicated by Lemma 1\u201d**\n\nIt is not our intention to imply that TDU solves the bias that arises in Lemma 1. Proposition 1 states that the only solution to this bias is to use a full posterior p(\\theta). As this is computationally infeasible for all but small MPDs, our proposal is to estimate uncertainty over a different quantity that mitigates this bias. Here, we use TD-errors. As TD-errors rely on Q_\\theta, the same bias arises in TDU (indeed any uncertainty estimate that involves Q_\\theta will be subject to the identified in Lemma 1), but because it conditions on a full state-transition it can limit the extent of the bias. In response to your review, we have added a formal analysis to show the conditions under which TDU is an improvement. In brief, \\sigma(\\tau) has a smaller bias if the bias in E[Q_\\theta] is temporally consistent, for instance by persistent overestimation of the value function. \n\n**\u201cit is not detailed how the quantity \\sigma(\\tau) estimates value function uncertainty, nor how it is unbiased\u201d**\n\n\\sigma(\\tau) estimates uncertainty over TD-errors, which can be seen as a first-difference estimator of uncertainty over Q. As such, it expresses uncertainty over the value of a policy, but it is not a direct measure of uncertainty over Q_\\theta (but rather \\Delta Q_\\theta. We have updated our wording to clarify this point and avoid further confusion. We did not claim that \\sigma is unbiased, but understand that the text might have implied such. In response to your review, we have added a formal analysis to detail how TDU is impacted by a bias in the estimator E[Q] / V[Q]. \n\n**\u201cWhy is it a better choice than, for example, directly using the bootstrapped set of action-values?\u201d**\n\nThe motivation behind TDU is to provide a clearer signal for exploration by better capturing uncertainty in the value function that is due to uncertainty over the agent\u2019s parameters. Our new formal analysis details the scenarios under which this can happen, and shows that, if the bias is temporally consistent, then TDU will have a smaller bias than uncertainty directly over action-values. This is empirically demonstrated on Deep Sea. Because the *only* difference between BDQN and TDU is \\sigma, the difference in performance is directly attributable to \\sigma. Hence, that BDQN fails to solve the stochastic version of Deep Sea while TDU performs almost as well as in the deterministic case empirically verifies that \\sigma provides a signal for exploration that goes beyond sampling from a bootstrapped set of action values. This is then further validated on Atari, where TDU performs significantly better than BDQN on hard exploration games such as Montezuma\u2019s Revenge.\n\n**\u201c3 runs and do not provide significant evidence of improvements [...The reported shading is also not explained]\u201d**\n\nWe would like to point out that this refers only to Atari. We run the full Bsuite setup as designed by its authors. In particular, for Deep Sea, we report results from over 100 experiments. \n\nFor Atari, we have updated results for a subset of games, including hard exploration games with results from 8 seeds. We also ran these for 20 billion steps to highlight that BDQN saturates while TDU does not. Shading depicts standard deviation, we apologise for this omission, which has been corrected. Our results are statistically significant, as calculated by an ANOVA analysis comparing TDU vs non-TDU methods (BDQN and R2D2), controlling for Atari level (8 seeds per level, 11 levels, F = 8.17, p = 0.0045). We have added these statistics to the revised version of our paper.\n\n**\u201cEven better would be to also highlight if results change significantly with changes to hyperparameters.\u201d**\n\nWe provide an ablation of all TDU-specific hyper-parameters for all experiments. On Deep Sea, we have ablations in the main text, Figure 2. We provide further ablations in Appendix, section C.2. Moreover, for Atari, we have a full section on hyper-parameter sensitivity in section D.3, with ablations in Figure 6 and 7. \n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper497/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper497/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Temporal Difference Uncertainties as a Signal for Exploration", "authorids": ["~Sebastian_Flennerhag1", "~Jane_X_Wang1", "~Pablo_Sprechmann1", "~Francesco_Visin1", "~Alexandre_Galashov1", "~Steven_Kapturowski1", "~Diana_L_Borsa1", "~Nicolas_Heess1", "~Andre_Barreto1", "~Razvan_Pascanu1"], "authors": ["Sebastian Flennerhag", "Jane X Wang", "Pablo Sprechmann", "Francesco Visin", "Alexandre Galashov", "Steven Kapturowski", "Diana L Borsa", "Nicolas Heess", "Andre Barreto", "Razvan Pascanu"], "keywords": ["deep reinforcement learning", "deep-rl", "exploration"], "abstract": "An effective approach to exploration in reinforcement learning is to rely on an agent's uncertainty over the optimal policy, which can yield near-optimal exploration strategies in tabular settings. However, in non-tabular settings that involve function approximators, obtaining accurate uncertainty estimates is almost as challenging as the exploration problem itself. In this paper, we highlight that value estimates are easily biased and temporally inconsistent. In light of this, we propose a novel method for estimating uncertainty over the value function that relies on inducing a distribution over temporal difference errors. This exploration signal controls for state-action transitions so as to isolate uncertainty in value that is due to uncertainty over the agent's parameters. Because our measure of uncertainty conditions on state-action transitions, we cannot act on this measure directly. Instead, we incorporate it as an intrinsic reward and treat exploration as a separate learning problem, induced by the agent's temporal difference uncertainties. We introduce a distinct exploration policy that learns to collect data with high estimated uncertainty, which gives rise to a curriculum that smoothly changes throughout learning and vanishes in the limit of perfect value estimates. We evaluate our method on hard exploration tasks, including Deep Sea and Atari 2600 environments and find that our proposed form of exploration facilitates efficient exploration.", "one-sentence_summary": "A method for exploration based on learning from uncertainty over the agent's value-function.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "flennerhag|temporal_difference_uncertainties_as_a_signal_for_exploration", "pdf": "/pdf/b1f081d47000b34d557d4a38a25e4f35a83ae58a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=kbcQqAnHa", "_bibtex": "@misc{\nflennerhag2021temporal,\ntitle={Temporal Difference Uncertainties as a Signal for Exploration},\nauthor={Sebastian Flennerhag and Jane X Wang and Pablo Sprechmann and Francesco Visin and Alexandre Galashov and Steven Kapturowski and Diana L Borsa and Nicolas Heess and Andre Barreto and Razvan Pascanu},\nyear={2021},\nurl={https://openreview.net/forum?id=Z2qyx5vC8Xn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Z2qyx5vC8Xn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper497/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper497/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper497/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper497/Authors|ICLR.cc/2021/Conference/Paper497/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper497/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870330, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper497/-/Official_Comment"}}}, {"id": "0ckDMF5mIeq", "original": null, "number": 2, "cdate": 1603877082346, "ddate": null, "tcdate": 1603877082346, "tmdate": 1605024675578, "tddate": null, "forum": "Z2qyx5vC8Xn", "replyto": "Z2qyx5vC8Xn", "invitation": "ICLR.cc/2021/Conference/Paper497/-/Official_Review", "content": {"title": "Interesting exploration approach", "review": "Summary:\nThe authors introduce the use of value function variance (conditioned on state transition) as auxiliary reward promoting exploration during training. The variance is estimated using the bootstrap DQN approach. The main difference with similar methods is that the value uncertainty is not used in a Thompson sampling scheme but it is instead use to provide exploration reward. \n\nRelevance:\nThe paper address a central problem in deep RL. \n\nOriginality:\nThe central idea is rather original and it elegantly combine ideas the value uncertainty estimation approach with the auxiliary exploration reward strategy. This new form of auxiliary reward is rather attractive as it automatically scales down during training as the model becomes more certain.\n\n\nScientific quality:\n- The new method is generally well motivated. However, while the author use a Bayesian terminology (e.g. prior, posterior), I am not convinced that the proposed bootstrap method for uncertainty estimation has a clear Bayesian interpretation as a form of approximate inference. Boostrap resampling can lead to proper Bayesian posteriors when the likelihood is used to weight the samples but this does not seem to be the case in this work. \n- The experiment section contain an interesting set of experiments and compare with several relevant baselines. \n-The paper is clear and very well written. However, I am not convinced by some of the claims. For example, the author claims that since reward uncertainty estimation is biased then networks cannot meaningfully generalize. While the claim can definitely be true, I do not think it follows from their premises. I would like to see the authors to expand this argument. \n\nPros:\n- Simple and elegant new method\n- Very good performance when compared with a large number of relevant baselines\n\nCons:\n- Some claims are not completely justified\n- It would have been informative to have a wider range of problems in the experiments section", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper497/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper497/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Temporal Difference Uncertainties as a Signal for Exploration", "authorids": ["~Sebastian_Flennerhag1", "~Jane_X_Wang1", "~Pablo_Sprechmann1", "~Francesco_Visin1", "~Alexandre_Galashov1", "~Steven_Kapturowski1", "~Diana_L_Borsa1", "~Nicolas_Heess1", "~Andre_Barreto1", "~Razvan_Pascanu1"], "authors": ["Sebastian Flennerhag", "Jane X Wang", "Pablo Sprechmann", "Francesco Visin", "Alexandre Galashov", "Steven Kapturowski", "Diana L Borsa", "Nicolas Heess", "Andre Barreto", "Razvan Pascanu"], "keywords": ["deep reinforcement learning", "deep-rl", "exploration"], "abstract": "An effective approach to exploration in reinforcement learning is to rely on an agent's uncertainty over the optimal policy, which can yield near-optimal exploration strategies in tabular settings. However, in non-tabular settings that involve function approximators, obtaining accurate uncertainty estimates is almost as challenging as the exploration problem itself. In this paper, we highlight that value estimates are easily biased and temporally inconsistent. In light of this, we propose a novel method for estimating uncertainty over the value function that relies on inducing a distribution over temporal difference errors. This exploration signal controls for state-action transitions so as to isolate uncertainty in value that is due to uncertainty over the agent's parameters. Because our measure of uncertainty conditions on state-action transitions, we cannot act on this measure directly. Instead, we incorporate it as an intrinsic reward and treat exploration as a separate learning problem, induced by the agent's temporal difference uncertainties. We introduce a distinct exploration policy that learns to collect data with high estimated uncertainty, which gives rise to a curriculum that smoothly changes throughout learning and vanishes in the limit of perfect value estimates. We evaluate our method on hard exploration tasks, including Deep Sea and Atari 2600 environments and find that our proposed form of exploration facilitates efficient exploration.", "one-sentence_summary": "A method for exploration based on learning from uncertainty over the agent's value-function.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "flennerhag|temporal_difference_uncertainties_as_a_signal_for_exploration", "pdf": "/pdf/b1f081d47000b34d557d4a38a25e4f35a83ae58a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=kbcQqAnHa", "_bibtex": "@misc{\nflennerhag2021temporal,\ntitle={Temporal Difference Uncertainties as a Signal for Exploration},\nauthor={Sebastian Flennerhag and Jane X Wang and Pablo Sprechmann and Francesco Visin and Alexandre Galashov and Steven Kapturowski and Diana L Borsa and Nicolas Heess and Andre Barreto and Razvan Pascanu},\nyear={2021},\nurl={https://openreview.net/forum?id=Z2qyx5vC8Xn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Z2qyx5vC8Xn", "replyto": "Z2qyx5vC8Xn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper497/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538141829, "tmdate": 1606915782663, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper497/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper497/-/Official_Review"}}}], "count": 11}