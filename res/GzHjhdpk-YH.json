{"notes": [{"id": "GzHjhdpk-YH", "original": "8PYooEPLZ3o", "number": 651, "cdate": 1601308077482, "ddate": null, "tcdate": 1601308077482, "tmdate": 1614985706949, "tddate": null, "forum": "GzHjhdpk-YH", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "The Unbalanced Gromov Wasserstein Distance: Conic Formulation and Relaxation", "authorids": ["~Thibault_Sejourne2", "~Fran\u00e7ois-Xavier_Vialard2", "~Gabriel_Peyr\u00e92"], "authors": ["Thibault Sejourne", "Fran\u00e7ois-Xavier Vialard", "Gabriel Peyr\u00e9"], "keywords": ["Gromov-Wasserstein", "Non-convex optimization", "Optimal Transport", "Partial matching"], "abstract": "Comparing metric measure spaces (i.e. a metric space endowed with a probability distribution) is at the heart of many machine learning problems. This includes for instance predicting properties of molecules in quantum chemistry or generating graphs with varying connectivity. The most popular distance between such metric measure spaces is the Gromov-Wasserstein (GW) distance, which is the solution of a quadratic assignment problem. This distance has been successfully applied to supervised learning and generative modeling, for applications as diverse as quantum chemistry or natural language processing. The GW distance is however limited to the comparison of metric measure spaces endowed with a probability distribution. This strong limitation is problematic for many applications in ML where there is no a priori natural normalization on the total mass of the data. Furthermore, imposing an exact conservation of mass across spaces is not robust to outliers and often leads to irregular matching. To alleviate these issues, we introduce two Unbalanced Gromov-Wasserstein formulations: a distance and a more tractable upper-bounding relaxation. They both allow the comparison of metric spaces equipped with arbitrary positive measures up to isometries. The first formulation is a positive and definite divergence based on a relaxation of the mass conservation constraint using a novel type of quadratically-homogeneous divergence.This divergence works hand in hand with the entropic regularization approach which is popular to solve large scale optimal transport problems. We show that the underlying non-convex optimization problem can be efficiently tackled using a highly parallelizable and GPU-friendly iterative scheme. The second formulation is a distance between mm-spaces up to isometries based on a conic lifting. Lastly, we provide numerical simulations to highlight the salient features of the unbalanced divergence and its potential applications in ML.", "one-sentence_summary": "It is the generalization of the Gromov-Wasserstein distance inspired from unbalanced optimal transport, which is proved to define a distance and to be computable via GPU routines.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sejourne|the_unbalanced_gromov_wasserstein_distance_conic_formulation_and_relaxation", "pdf": "/pdf/85a7cbe50f7781f24692916b1e62cc214ff5e7d7.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=oHdCTBJMND", "_bibtex": "@misc{\nsejourne2021the,\ntitle={The Unbalanced Gromov Wasserstein Distance: Conic Formulation and Relaxation},\nauthor={Thibault Sejourne and Fran{\\c{c}}ois-Xavier Vialard and Gabriel Peyr{\\'e}},\nyear={2021},\nurl={https://openreview.net/forum?id=GzHjhdpk-YH}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "KA6q3c-zbnV", "original": null, "number": 1, "cdate": 1610040436644, "ddate": null, "tcdate": 1610040436644, "tmdate": 1610474037323, "tddate": null, "forum": "GzHjhdpk-YH", "replyto": "GzHjhdpk-YH", "invitation": "ICLR.cc/2021/Conference/Paper651/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper present novel formulations to address the problem of unbalanced Gromov. The Conic formulation is very interesting but stays theoretical until optimization algorithms are available. The Unbalanced Gromov is a nice extension of Gromov and comes with relatively efficient solvers. Some very limited numerical experiment show the proposed UGW used between 2D distributions (two moons) and graphs.\n\nThe paper had some mixed reviews with reviewers acknowledging the novelty of the approach (albeit an extension similar to unbalanced OT) and of the theoretical results. The detailed a very well written response to the reviewers comment has been appreciated. But all reviewers also noted a lack of numerical experiments outside of the very simple illustrations in the paper. This paper is a very nice contribution to the theory of optimal transport but fails at illustrating its relevance to the ML community.  Despite acknowledging the theoretical contributions of the paper, the  AC recommends a reject but strongly encourages the authors to complete the experimental section with some ML applications or at least proof of concepts (graph classification, domain adaptation, ...). \n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Unbalanced Gromov Wasserstein Distance: Conic Formulation and Relaxation", "authorids": ["~Thibault_Sejourne2", "~Fran\u00e7ois-Xavier_Vialard2", "~Gabriel_Peyr\u00e92"], "authors": ["Thibault Sejourne", "Fran\u00e7ois-Xavier Vialard", "Gabriel Peyr\u00e9"], "keywords": ["Gromov-Wasserstein", "Non-convex optimization", "Optimal Transport", "Partial matching"], "abstract": "Comparing metric measure spaces (i.e. a metric space endowed with a probability distribution) is at the heart of many machine learning problems. This includes for instance predicting properties of molecules in quantum chemistry or generating graphs with varying connectivity. The most popular distance between such metric measure spaces is the Gromov-Wasserstein (GW) distance, which is the solution of a quadratic assignment problem. This distance has been successfully applied to supervised learning and generative modeling, for applications as diverse as quantum chemistry or natural language processing. The GW distance is however limited to the comparison of metric measure spaces endowed with a probability distribution. This strong limitation is problematic for many applications in ML where there is no a priori natural normalization on the total mass of the data. Furthermore, imposing an exact conservation of mass across spaces is not robust to outliers and often leads to irregular matching. To alleviate these issues, we introduce two Unbalanced Gromov-Wasserstein formulations: a distance and a more tractable upper-bounding relaxation. They both allow the comparison of metric spaces equipped with arbitrary positive measures up to isometries. The first formulation is a positive and definite divergence based on a relaxation of the mass conservation constraint using a novel type of quadratically-homogeneous divergence.This divergence works hand in hand with the entropic regularization approach which is popular to solve large scale optimal transport problems. We show that the underlying non-convex optimization problem can be efficiently tackled using a highly parallelizable and GPU-friendly iterative scheme. The second formulation is a distance between mm-spaces up to isometries based on a conic lifting. Lastly, we provide numerical simulations to highlight the salient features of the unbalanced divergence and its potential applications in ML.", "one-sentence_summary": "It is the generalization of the Gromov-Wasserstein distance inspired from unbalanced optimal transport, which is proved to define a distance and to be computable via GPU routines.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sejourne|the_unbalanced_gromov_wasserstein_distance_conic_formulation_and_relaxation", "pdf": "/pdf/85a7cbe50f7781f24692916b1e62cc214ff5e7d7.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=oHdCTBJMND", "_bibtex": "@misc{\nsejourne2021the,\ntitle={The Unbalanced Gromov Wasserstein Distance: Conic Formulation and Relaxation},\nauthor={Thibault Sejourne and Fran{\\c{c}}ois-Xavier Vialard and Gabriel Peyr{\\'e}},\nyear={2021},\nurl={https://openreview.net/forum?id=GzHjhdpk-YH}\n}"}, "tags": [], "invitation": {"reply": {"forum": "GzHjhdpk-YH", "replyto": "GzHjhdpk-YH", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040436631, "tmdate": 1610474037307, "id": "ICLR.cc/2021/Conference/Paper651/-/Decision"}}}, {"id": "qJFteJhbEv", "original": null, "number": 1, "cdate": 1603757036033, "ddate": null, "tcdate": 1603757036033, "tmdate": 1606727722050, "tddate": null, "forum": "GzHjhdpk-YH", "replyto": "GzHjhdpk-YH", "invitation": "ICLR.cc/2021/Conference/Paper651/-/Official_Review", "content": {"title": "Nice theoretical results, poor experimental Section", "review": "The paper introduces a novel unbalanced Gromov-Wasserstein type problem. The Gromov-Wasserstein distance is very useful in practice for comparing probability distributions that do not lie in the same metric spaces. It has recently found several successful applications in ML for computational chemistry, graphs comparisons or NLP. Following previous works on unbalanced optimal transport (i.e. soft constraints over marginals enforcement of the coupling matrix), and the rationale that disposing of unbalanced versions of transport problems can alleviate in some ways presence of outliers or noise in the distributions, the authors propose two \u2018unbalanced\u2019 variants of the Gromov-Wasserstein (GW) problem, that allow comparison of metric spaces with arbitrary positive measures up to isometries (I.e. rigid transformations). \n\nThe paper is fairly well written, original, and the related works is particularly complete. The theoretical part of the paper is sound, rigorous and well-motivated, and I learnt many things from it. The idea of using a quadratic \\phi-divergence is neat and particularly clever. Yet, the paper requires, to some extent, very good notions in (unbalanced) optimal transport and probability, and I wonder, if some notations could have been eased a little bit (for instance, it seems that D_\\phi is usually chosen as KL), but I guess the choice was made to be as general as possible.  While I did not check in details all the proofs in the appendix Section, I believe the work is solid. The algorithm parts is a little less satisfactory, as it amounts to optimize a upper bound  of the described problem, following existing work from [M\u00e9moli11]. Though appealing, this upper bound is known to lose some properties of the original GW distance (for instance, two close points in the source can be matched to two distant points in the target if they share a very similar \u2018distance profile\u2019 or distance distributions wrt. the other samples). It seems that this effect is observable in Figure 2, where parts of a moon are \u2018flipped\u2019 in the matching. Combined with entropy and unbalanced formulation, I guess the final result can be very hard to interpret in a practical setting. \nFinally, the weakest part of the paper is the experimental Section. Only two toy examples are presented. While the method could have been used in many settings (graph classification, embedding matching in NLP or  even graph matching, for which many algorithms exist, etc.), it is very hard to conclude about the practical interest of the method. \nWhile I guess this is not a problem if one focuses on the original contribution of this novel unbalanced distance, it is more problematic, in the reviewers perception, for a machine learning venue such as ICLR.\n\nFor this reason, I will solely suggest a \u2018weak accept\u2019 decision, while I really believe the theoretical sections have a lot of merits. \n\nMinor comments.\n\n- On page 1, it is said that \u00ab\u00a0the paper defines for the first time a class of distances between the [aforementioned] objects\u00a0\u00bb. This claim is a bit strong knowing that GW has already been used in several (cited) applications. I guess authors could be a little bit more precise on the meaning of this sentence;\n- Page 4, just after definition 1, it seems (at least with my PDF reader), that there is a problem in the symbol used for \\phi on the last line of the paragraph; \n- The claim that a cost of O(n^3) in time and O(n^2) in memory allows to scale to large problems is a bit strong. In practice, what is the maximum size of the problem that can be addressed in reasonable time with this method ? I expect that handling graphs that have more than 10k nodes for instance in very difficult. \n- The final rescaling of \\gamma (line 8 of Algorithm 1), is a little bit difficult to understand from the alternating minimization point of view. \n- In the experimental Section, which algorithm is used to compute GW ? Also, I guess Figure 3 could present the original graphs (without scaling the dots), to gain a better understanding of the original problem\n\n\nEDIT after rebuttal period\n---------------------------------------\nmany thanks to the authors for taking into consideration my comments. I decided not to change my note because I still believe the paper lacks of a significative experimental Section.  ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper651/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper651/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Unbalanced Gromov Wasserstein Distance: Conic Formulation and Relaxation", "authorids": ["~Thibault_Sejourne2", "~Fran\u00e7ois-Xavier_Vialard2", "~Gabriel_Peyr\u00e92"], "authors": ["Thibault Sejourne", "Fran\u00e7ois-Xavier Vialard", "Gabriel Peyr\u00e9"], "keywords": ["Gromov-Wasserstein", "Non-convex optimization", "Optimal Transport", "Partial matching"], "abstract": "Comparing metric measure spaces (i.e. a metric space endowed with a probability distribution) is at the heart of many machine learning problems. This includes for instance predicting properties of molecules in quantum chemistry or generating graphs with varying connectivity. The most popular distance between such metric measure spaces is the Gromov-Wasserstein (GW) distance, which is the solution of a quadratic assignment problem. This distance has been successfully applied to supervised learning and generative modeling, for applications as diverse as quantum chemistry or natural language processing. The GW distance is however limited to the comparison of metric measure spaces endowed with a probability distribution. This strong limitation is problematic for many applications in ML where there is no a priori natural normalization on the total mass of the data. Furthermore, imposing an exact conservation of mass across spaces is not robust to outliers and often leads to irregular matching. To alleviate these issues, we introduce two Unbalanced Gromov-Wasserstein formulations: a distance and a more tractable upper-bounding relaxation. They both allow the comparison of metric spaces equipped with arbitrary positive measures up to isometries. The first formulation is a positive and definite divergence based on a relaxation of the mass conservation constraint using a novel type of quadratically-homogeneous divergence.This divergence works hand in hand with the entropic regularization approach which is popular to solve large scale optimal transport problems. We show that the underlying non-convex optimization problem can be efficiently tackled using a highly parallelizable and GPU-friendly iterative scheme. The second formulation is a distance between mm-spaces up to isometries based on a conic lifting. Lastly, we provide numerical simulations to highlight the salient features of the unbalanced divergence and its potential applications in ML.", "one-sentence_summary": "It is the generalization of the Gromov-Wasserstein distance inspired from unbalanced optimal transport, which is proved to define a distance and to be computable via GPU routines.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sejourne|the_unbalanced_gromov_wasserstein_distance_conic_formulation_and_relaxation", "pdf": "/pdf/85a7cbe50f7781f24692916b1e62cc214ff5e7d7.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=oHdCTBJMND", "_bibtex": "@misc{\nsejourne2021the,\ntitle={The Unbalanced Gromov Wasserstein Distance: Conic Formulation and Relaxation},\nauthor={Thibault Sejourne and Fran{\\c{c}}ois-Xavier Vialard and Gabriel Peyr{\\'e}},\nyear={2021},\nurl={https://openreview.net/forum?id=GzHjhdpk-YH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "GzHjhdpk-YH", "replyto": "GzHjhdpk-YH", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper651/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538138300, "tmdate": 1606915782832, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper651/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper651/-/Official_Review"}}}, {"id": "eSAfQz1IWiu", "original": null, "number": 9, "cdate": 1606259775429, "ddate": null, "tcdate": 1606259775429, "tmdate": 1606259775429, "tddate": null, "forum": "GzHjhdpk-YH", "replyto": "0L_oCO24wLx", "invitation": "ICLR.cc/2021/Conference/Paper651/-/Official_Comment", "content": {"title": "Thanks for the response", "comment": "I would like to thank the authors for spending time responding to my comments. \n\nFrankly speaking, I am still not convinced about the introduction of CGW and in fact get confused about its point in the paper. If CGW is expensive and hard to implement, I still do not understand why I should care about it (though I agree that it seems to be quite interesting from the theoretical perspective). Furthermore, as other reviewers pointed out, the current experiments are quite limited. I feel that the paper still needs more works in terms of experiments to fully explore the potential of unbalanced Gromov-Wasserstein divergence in machine learning and deep learning applications. \n\nFor the above reason, I decide to keep my current score with the paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper651/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper651/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Unbalanced Gromov Wasserstein Distance: Conic Formulation and Relaxation", "authorids": ["~Thibault_Sejourne2", "~Fran\u00e7ois-Xavier_Vialard2", "~Gabriel_Peyr\u00e92"], "authors": ["Thibault Sejourne", "Fran\u00e7ois-Xavier Vialard", "Gabriel Peyr\u00e9"], "keywords": ["Gromov-Wasserstein", "Non-convex optimization", "Optimal Transport", "Partial matching"], "abstract": "Comparing metric measure spaces (i.e. a metric space endowed with a probability distribution) is at the heart of many machine learning problems. This includes for instance predicting properties of molecules in quantum chemistry or generating graphs with varying connectivity. The most popular distance between such metric measure spaces is the Gromov-Wasserstein (GW) distance, which is the solution of a quadratic assignment problem. This distance has been successfully applied to supervised learning and generative modeling, for applications as diverse as quantum chemistry or natural language processing. The GW distance is however limited to the comparison of metric measure spaces endowed with a probability distribution. This strong limitation is problematic for many applications in ML where there is no a priori natural normalization on the total mass of the data. Furthermore, imposing an exact conservation of mass across spaces is not robust to outliers and often leads to irregular matching. To alleviate these issues, we introduce two Unbalanced Gromov-Wasserstein formulations: a distance and a more tractable upper-bounding relaxation. They both allow the comparison of metric spaces equipped with arbitrary positive measures up to isometries. The first formulation is a positive and definite divergence based on a relaxation of the mass conservation constraint using a novel type of quadratically-homogeneous divergence.This divergence works hand in hand with the entropic regularization approach which is popular to solve large scale optimal transport problems. We show that the underlying non-convex optimization problem can be efficiently tackled using a highly parallelizable and GPU-friendly iterative scheme. The second formulation is a distance between mm-spaces up to isometries based on a conic lifting. Lastly, we provide numerical simulations to highlight the salient features of the unbalanced divergence and its potential applications in ML.", "one-sentence_summary": "It is the generalization of the Gromov-Wasserstein distance inspired from unbalanced optimal transport, which is proved to define a distance and to be computable via GPU routines.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sejourne|the_unbalanced_gromov_wasserstein_distance_conic_formulation_and_relaxation", "pdf": "/pdf/85a7cbe50f7781f24692916b1e62cc214ff5e7d7.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=oHdCTBJMND", "_bibtex": "@misc{\nsejourne2021the,\ntitle={The Unbalanced Gromov Wasserstein Distance: Conic Formulation and Relaxation},\nauthor={Thibault Sejourne and Fran{\\c{c}}ois-Xavier Vialard and Gabriel Peyr{\\'e}},\nyear={2021},\nurl={https://openreview.net/forum?id=GzHjhdpk-YH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "GzHjhdpk-YH", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper651/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper651/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper651/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper651/Authors|ICLR.cc/2021/Conference/Paper651/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper651/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923868648, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper651/-/Official_Comment"}}}, {"id": "8VILPF0YT3x", "original": null, "number": 8, "cdate": 1606257506264, "ddate": null, "tcdate": 1606257506264, "tmdate": 1606257506264, "tddate": null, "forum": "GzHjhdpk-YH", "replyto": "vkR9n84oGoW", "invitation": "ICLR.cc/2021/Conference/Paper651/-/Official_Comment", "content": {"title": "Thank you for the answers", "comment": "I thank the authors for the detailed answers to my questions. They confirm my appreciation of the theoretical contributions of this paper and my recommendation for acceptance, but given the (still mostly unaddressed) limitations in the experimental evaluation, I am inclined to maintain my current score.  "}, "signatures": ["ICLR.cc/2021/Conference/Paper651/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper651/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Unbalanced Gromov Wasserstein Distance: Conic Formulation and Relaxation", "authorids": ["~Thibault_Sejourne2", "~Fran\u00e7ois-Xavier_Vialard2", "~Gabriel_Peyr\u00e92"], "authors": ["Thibault Sejourne", "Fran\u00e7ois-Xavier Vialard", "Gabriel Peyr\u00e9"], "keywords": ["Gromov-Wasserstein", "Non-convex optimization", "Optimal Transport", "Partial matching"], "abstract": "Comparing metric measure spaces (i.e. a metric space endowed with a probability distribution) is at the heart of many machine learning problems. This includes for instance predicting properties of molecules in quantum chemistry or generating graphs with varying connectivity. The most popular distance between such metric measure spaces is the Gromov-Wasserstein (GW) distance, which is the solution of a quadratic assignment problem. This distance has been successfully applied to supervised learning and generative modeling, for applications as diverse as quantum chemistry or natural language processing. The GW distance is however limited to the comparison of metric measure spaces endowed with a probability distribution. This strong limitation is problematic for many applications in ML where there is no a priori natural normalization on the total mass of the data. Furthermore, imposing an exact conservation of mass across spaces is not robust to outliers and often leads to irregular matching. To alleviate these issues, we introduce two Unbalanced Gromov-Wasserstein formulations: a distance and a more tractable upper-bounding relaxation. They both allow the comparison of metric spaces equipped with arbitrary positive measures up to isometries. The first formulation is a positive and definite divergence based on a relaxation of the mass conservation constraint using a novel type of quadratically-homogeneous divergence.This divergence works hand in hand with the entropic regularization approach which is popular to solve large scale optimal transport problems. We show that the underlying non-convex optimization problem can be efficiently tackled using a highly parallelizable and GPU-friendly iterative scheme. The second formulation is a distance between mm-spaces up to isometries based on a conic lifting. Lastly, we provide numerical simulations to highlight the salient features of the unbalanced divergence and its potential applications in ML.", "one-sentence_summary": "It is the generalization of the Gromov-Wasserstein distance inspired from unbalanced optimal transport, which is proved to define a distance and to be computable via GPU routines.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sejourne|the_unbalanced_gromov_wasserstein_distance_conic_formulation_and_relaxation", "pdf": "/pdf/85a7cbe50f7781f24692916b1e62cc214ff5e7d7.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=oHdCTBJMND", "_bibtex": "@misc{\nsejourne2021the,\ntitle={The Unbalanced Gromov Wasserstein Distance: Conic Formulation and Relaxation},\nauthor={Thibault Sejourne and Fran{\\c{c}}ois-Xavier Vialard and Gabriel Peyr{\\'e}},\nyear={2021},\nurl={https://openreview.net/forum?id=GzHjhdpk-YH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "GzHjhdpk-YH", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper651/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper651/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper651/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper651/Authors|ICLR.cc/2021/Conference/Paper651/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper651/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923868648, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper651/-/Official_Comment"}}}, {"id": "nyzBS1cuuu1", "original": null, "number": 7, "cdate": 1606181049620, "ddate": null, "tcdate": 1606181049620, "tmdate": 1606181049620, "tddate": null, "forum": "GzHjhdpk-YH", "replyto": "vjfh6kz1OFb", "invitation": "ICLR.cc/2021/Conference/Paper651/-/Official_Comment", "content": {"title": "Thanks for your clarification", "comment": "Thank you for your clarification.\nI am happy to increase my score (6 --> 7)."}, "signatures": ["ICLR.cc/2021/Conference/Paper651/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper651/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Unbalanced Gromov Wasserstein Distance: Conic Formulation and Relaxation", "authorids": ["~Thibault_Sejourne2", "~Fran\u00e7ois-Xavier_Vialard2", "~Gabriel_Peyr\u00e92"], "authors": ["Thibault Sejourne", "Fran\u00e7ois-Xavier Vialard", "Gabriel Peyr\u00e9"], "keywords": ["Gromov-Wasserstein", "Non-convex optimization", "Optimal Transport", "Partial matching"], "abstract": "Comparing metric measure spaces (i.e. a metric space endowed with a probability distribution) is at the heart of many machine learning problems. This includes for instance predicting properties of molecules in quantum chemistry or generating graphs with varying connectivity. The most popular distance between such metric measure spaces is the Gromov-Wasserstein (GW) distance, which is the solution of a quadratic assignment problem. This distance has been successfully applied to supervised learning and generative modeling, for applications as diverse as quantum chemistry or natural language processing. The GW distance is however limited to the comparison of metric measure spaces endowed with a probability distribution. This strong limitation is problematic for many applications in ML where there is no a priori natural normalization on the total mass of the data. Furthermore, imposing an exact conservation of mass across spaces is not robust to outliers and often leads to irregular matching. To alleviate these issues, we introduce two Unbalanced Gromov-Wasserstein formulations: a distance and a more tractable upper-bounding relaxation. They both allow the comparison of metric spaces equipped with arbitrary positive measures up to isometries. The first formulation is a positive and definite divergence based on a relaxation of the mass conservation constraint using a novel type of quadratically-homogeneous divergence.This divergence works hand in hand with the entropic regularization approach which is popular to solve large scale optimal transport problems. We show that the underlying non-convex optimization problem can be efficiently tackled using a highly parallelizable and GPU-friendly iterative scheme. The second formulation is a distance between mm-spaces up to isometries based on a conic lifting. Lastly, we provide numerical simulations to highlight the salient features of the unbalanced divergence and its potential applications in ML.", "one-sentence_summary": "It is the generalization of the Gromov-Wasserstein distance inspired from unbalanced optimal transport, which is proved to define a distance and to be computable via GPU routines.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sejourne|the_unbalanced_gromov_wasserstein_distance_conic_formulation_and_relaxation", "pdf": "/pdf/85a7cbe50f7781f24692916b1e62cc214ff5e7d7.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=oHdCTBJMND", "_bibtex": "@misc{\nsejourne2021the,\ntitle={The Unbalanced Gromov Wasserstein Distance: Conic Formulation and Relaxation},\nauthor={Thibault Sejourne and Fran{\\c{c}}ois-Xavier Vialard and Gabriel Peyr{\\'e}},\nyear={2021},\nurl={https://openreview.net/forum?id=GzHjhdpk-YH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "GzHjhdpk-YH", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper651/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper651/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper651/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper651/Authors|ICLR.cc/2021/Conference/Paper651/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper651/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923868648, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper651/-/Official_Comment"}}}, {"id": "ubzjMspIFIv", "original": null, "number": 3, "cdate": 1603892644370, "ddate": null, "tcdate": 1603892644370, "tmdate": 1606180951305, "tddate": null, "forum": "GzHjhdpk-YH", "replyto": "GzHjhdpk-YH", "invitation": "ICLR.cc/2021/Conference/Paper651/-/Official_Review", "content": {"title": "The Unbalanced Gromov Wasserstein Distance: Conic Formulation and Relaxation", "review": "The authors consider the Gromov Wasserstein (GW) problem for metric measure spaces having different masses (i.e., Unbalanced GW). Similar to the ideas of unbalanced optimal transport (UOT), they proposed to use a quadratic divergence to relax the marginal constraints (instead of divergence as in UOT). When divergence is KL, the authors derive a GPU-friendly algorithm for the UGW  which relies on the unbalanced Sinkhorn algorithm. Additionally, the authors also propose a variant of UGW, namely Conic Gromov-Wasserstein (CGW) to address the different masses of metric measure spaces. The authors propose that CGW has nice properties (Theorem 1). However, there is no algorithm to solve the CGW yet.\n\n+ It is easy to follow the paper. The authors provide rigorous details for the unbalanced GW.\n\n+ For the UGW with divergence, it seems that it is not surprised for this idea by extending the ideas of entropic OT for unbalanced OT into entropic GW for unbalanced GW (e.g., using Sinkhorn iterations).\n\n+ The proposed conic GW is theoretically interesting since it comes up with no algorithm yet. The authors also draw a connection between CGW and UGW.\n\n+ It seems that the experiments are quite simple (with some toy examples). It seems better if the authors use UGW in some applications (e.g., some motivating applications for the unbalance case).\n\nSome of my other concerns are as follows:\n+ The authors propose to use quadratic divergence in UGW. Could the authors give more motivation/explanation about this approach, why not just divergence as in unbalanced OT (Although the GW is a quadratic assignment, the unbalanced problem for GW comes from the marginal constraints, why not just simply use the divergence between the marginal and measures)?\n+ The authors proved nice properties for the proposed CGW and draw its connection to UGW. However, CGW has no algorithm yet. I wonder whether there exist some special instances of UGW (with some specific divergence, e.g., total variation or KL), one may have some interesting properties as in CGW? \n\nOverall, I lean on the positive side. (i) The ideas of UGW may not be a surprise (together with its algorithm). (ii) The proposed CGW has good properties for GW problems for metric measure spaces having different masses, however, there is no efficient algorithm for it yet as (a trade-off). (iii) It seems better in case the authors improve experiments (e.g., use UGW in some motivating applications for the unbalanced case).\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper651/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper651/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Unbalanced Gromov Wasserstein Distance: Conic Formulation and Relaxation", "authorids": ["~Thibault_Sejourne2", "~Fran\u00e7ois-Xavier_Vialard2", "~Gabriel_Peyr\u00e92"], "authors": ["Thibault Sejourne", "Fran\u00e7ois-Xavier Vialard", "Gabriel Peyr\u00e9"], "keywords": ["Gromov-Wasserstein", "Non-convex optimization", "Optimal Transport", "Partial matching"], "abstract": "Comparing metric measure spaces (i.e. a metric space endowed with a probability distribution) is at the heart of many machine learning problems. This includes for instance predicting properties of molecules in quantum chemistry or generating graphs with varying connectivity. The most popular distance between such metric measure spaces is the Gromov-Wasserstein (GW) distance, which is the solution of a quadratic assignment problem. This distance has been successfully applied to supervised learning and generative modeling, for applications as diverse as quantum chemistry or natural language processing. The GW distance is however limited to the comparison of metric measure spaces endowed with a probability distribution. This strong limitation is problematic for many applications in ML where there is no a priori natural normalization on the total mass of the data. Furthermore, imposing an exact conservation of mass across spaces is not robust to outliers and often leads to irregular matching. To alleviate these issues, we introduce two Unbalanced Gromov-Wasserstein formulations: a distance and a more tractable upper-bounding relaxation. They both allow the comparison of metric spaces equipped with arbitrary positive measures up to isometries. The first formulation is a positive and definite divergence based on a relaxation of the mass conservation constraint using a novel type of quadratically-homogeneous divergence.This divergence works hand in hand with the entropic regularization approach which is popular to solve large scale optimal transport problems. We show that the underlying non-convex optimization problem can be efficiently tackled using a highly parallelizable and GPU-friendly iterative scheme. The second formulation is a distance between mm-spaces up to isometries based on a conic lifting. Lastly, we provide numerical simulations to highlight the salient features of the unbalanced divergence and its potential applications in ML.", "one-sentence_summary": "It is the generalization of the Gromov-Wasserstein distance inspired from unbalanced optimal transport, which is proved to define a distance and to be computable via GPU routines.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sejourne|the_unbalanced_gromov_wasserstein_distance_conic_formulation_and_relaxation", "pdf": "/pdf/85a7cbe50f7781f24692916b1e62cc214ff5e7d7.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=oHdCTBJMND", "_bibtex": "@misc{\nsejourne2021the,\ntitle={The Unbalanced Gromov Wasserstein Distance: Conic Formulation and Relaxation},\nauthor={Thibault Sejourne and Fran{\\c{c}}ois-Xavier Vialard and Gabriel Peyr{\\'e}},\nyear={2021},\nurl={https://openreview.net/forum?id=GzHjhdpk-YH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "GzHjhdpk-YH", "replyto": "GzHjhdpk-YH", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper651/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538138300, "tmdate": 1606915782832, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper651/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper651/-/Official_Review"}}}, {"id": "vkR9n84oGoW", "original": null, "number": 6, "cdate": 1605479745343, "ddate": null, "tcdate": 1605479745343, "tmdate": 1605482078606, "tddate": null, "forum": "GzHjhdpk-YH", "replyto": "XGndPvBQ2ta", "invitation": "ICLR.cc/2021/Conference/Paper651/-/Official_Comment", "content": {"title": "Personnal answer to Reviewer 4", "comment": "We would like to thank the reviewer for the careful reading of our submission. We wrote a common answer addressed to all reviewers in a comment above, and provide here a personal answer to the reviewer\u2019s comments.\n\nWeaknesses:\n- *\u201cThe novelty of this paper is arguably limited\u201d:* \nWe respectfully disagree with such a statement, and respond to this in the common answers. \n- *\u201cThe experimental section is limited to very simple settings\u201d:* \nWe refer the reviewer to the common answers for a response to this remark.\n\nOther comments:\n- *\u201cIt would be useful if the paper discussed the assumptions that go into Prop 1\u201d:* \nThey are satisfied for most common f-divergences (TV, KL) and with bounded sets (e.g. point clouds or graphs). We will clarify this in the paper.\n- *\u201cI understand that Lemma 1 is a tool to draw a connection between the two approaches, but I find its introduction dry and abrupt.\u201d:*\nWe agree it is abrupt at first, and we will update the paper to improve its understanding. We had to trade brevity for clarity, but left details available in appendix A. The role of Lemma 1 is to introduce the cost $L_c$ which separates the pure creation penalty and includes the rest (partial transport, partial creation) in a new \u2018transport\u2019 cost.\n- *\u201cThe presentation of the conic formulation needs more hand-holding. What's the motivation for this cumbersome the conic formulation?\u201d:*\nThe goal is to express variational unbalanced problems as classical (balanced problems) over a lifted conic space. Similarly to the unbalanced OT framework (although the problem proofs are different, as detailed in the common answers), this lifting is pivotal to prove distance properties. \n- *\u201cI would have appreciated a discussion on the tightness of the bound of Theorem 1.\u201d:* \nAs detailed in the common answers, it is an open question we wish to solve in future works.\n- *\u201cI don't think $\\gamma_1$ and $\\gamma_2$ in page 6 were formally defined anywhere\u201d:* \nThis corresponds to the marginals of a transport plan $\\gamma$, which is defined below Equation (1). We will recall this in the final version.\n- *\u201cThe experimental section is limited to very simple settings, that either consider measures on the same space (defeating the purpose of GW) \u201c:*\nWe believe that despite its apparent simplicity, GW over Euclidean spaces is arguably the one which is the most used in practice (for e.g. shape matching in imaging [1] or word embeddings comparison in NLP  [2, 3]). \n- *\u201cThere is no comparison to other unbalanced methods (either OT or GW) except for one setting in Fig 3. I suspect most of these alternatives would produce similar results in Fig 1, so what is the competitive advantage of this method? \u201c:* \nWe compare UGW and UOT in Figure 2, to highlight the isometry invariance of UGW. The comparison of UGW with GW highlights the ability to deal with outliers (Fig 2) and unbalanced masses (Fig 1). We acknowledge that our simulations lack a comparison with other divergences, and in particular TV (and [Chapel et al.] which also makes use of a TV fidelity), which is expected (similar to UOT) to produce significantly different results. We will add these simulations in the final version of the paper.\n- *\u201cthere should be either a formal complexity analysis, or an empirical runtime comparison.\u201c:* \nOur only competitor is [Chapel et al.], which uses a Frank-Wolfe algorithm, so we believe the comparison would be unfair since F-W does not run on GPU and does not offer an epsilon-approximation mechanism. Besides partial GW is a different optimization problem, making the comparison difficult. In the revised version, we will show a visual comparison of the matchings obtained by partial GW and UGW (sharp vs. soft transitions).\n\nQuestions:\n- *\u201cWhat is meant by the last sentence in the first paragraph?\u201d:* \nWe will reformulate to explain (as detailed in the common answers) that our formulation is the first which yields both theoretical results and a computable algorithm at the same time. Note that the distance in [De Ponti et al.] appeared online at the same time as our submission, and is not amenable to a finite dimensional optimization problem when considering finite spaces.\n- *\u201cIs there a reference for the quadratic $\\phi$ divergences introduced in 2.1?\u201d:* \nTo the best of our knowledge we found no prior reference using such quadratic divergence.\n- *\u201cDo any of the experiments use the debiased UGW mentioned in page 7?\u201d:* \nIn all the experiments we compute UGW for a small regularization parameter epsilon, so that debiasing is not required. We will move this idea in the perspective section to avoid confusion.\n\nReferences:\n\n[1] Rodola et al., A game-theoretic approach to deformable shape matching.\n\n[2] Alvarez-Melis et al., Gromov-wasserstein alignment of word embedding spaces.\n\n[3] Grave et al., Unsupervised align-ment of embeddings with wasserstein procrustes\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper651/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper651/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Unbalanced Gromov Wasserstein Distance: Conic Formulation and Relaxation", "authorids": ["~Thibault_Sejourne2", "~Fran\u00e7ois-Xavier_Vialard2", "~Gabriel_Peyr\u00e92"], "authors": ["Thibault Sejourne", "Fran\u00e7ois-Xavier Vialard", "Gabriel Peyr\u00e9"], "keywords": ["Gromov-Wasserstein", "Non-convex optimization", "Optimal Transport", "Partial matching"], "abstract": "Comparing metric measure spaces (i.e. a metric space endowed with a probability distribution) is at the heart of many machine learning problems. This includes for instance predicting properties of molecules in quantum chemistry or generating graphs with varying connectivity. The most popular distance between such metric measure spaces is the Gromov-Wasserstein (GW) distance, which is the solution of a quadratic assignment problem. This distance has been successfully applied to supervised learning and generative modeling, for applications as diverse as quantum chemistry or natural language processing. The GW distance is however limited to the comparison of metric measure spaces endowed with a probability distribution. This strong limitation is problematic for many applications in ML where there is no a priori natural normalization on the total mass of the data. Furthermore, imposing an exact conservation of mass across spaces is not robust to outliers and often leads to irregular matching. To alleviate these issues, we introduce two Unbalanced Gromov-Wasserstein formulations: a distance and a more tractable upper-bounding relaxation. They both allow the comparison of metric spaces equipped with arbitrary positive measures up to isometries. The first formulation is a positive and definite divergence based on a relaxation of the mass conservation constraint using a novel type of quadratically-homogeneous divergence.This divergence works hand in hand with the entropic regularization approach which is popular to solve large scale optimal transport problems. We show that the underlying non-convex optimization problem can be efficiently tackled using a highly parallelizable and GPU-friendly iterative scheme. The second formulation is a distance between mm-spaces up to isometries based on a conic lifting. Lastly, we provide numerical simulations to highlight the salient features of the unbalanced divergence and its potential applications in ML.", "one-sentence_summary": "It is the generalization of the Gromov-Wasserstein distance inspired from unbalanced optimal transport, which is proved to define a distance and to be computable via GPU routines.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sejourne|the_unbalanced_gromov_wasserstein_distance_conic_formulation_and_relaxation", "pdf": "/pdf/85a7cbe50f7781f24692916b1e62cc214ff5e7d7.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=oHdCTBJMND", "_bibtex": "@misc{\nsejourne2021the,\ntitle={The Unbalanced Gromov Wasserstein Distance: Conic Formulation and Relaxation},\nauthor={Thibault Sejourne and Fran{\\c{c}}ois-Xavier Vialard and Gabriel Peyr{\\'e}},\nyear={2021},\nurl={https://openreview.net/forum?id=GzHjhdpk-YH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "GzHjhdpk-YH", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper651/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper651/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper651/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper651/Authors|ICLR.cc/2021/Conference/Paper651/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper651/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923868648, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper651/-/Official_Comment"}}}, {"id": "0L_oCO24wLx", "original": null, "number": 5, "cdate": 1605476999422, "ddate": null, "tcdate": 1605476999422, "tmdate": 1605481899599, "tddate": null, "forum": "GzHjhdpk-YH", "replyto": "aQTYzjP0clK", "invitation": "ICLR.cc/2021/Conference/Paper651/-/Official_Comment", "content": {"title": "Personnal answer to reviewer 3", "comment": "We would like to thank the reviewer for the careful reading of our submission. We wrote a common answer addressed to all reviewers in a comment above, and provide here a personal answer to the reviewer\u2019s comments.\n\n- *\u201cThey are simply extensions of the framework of UOT to the framework of Gromov-Wasserstein. \u201c:* \nWe respectfully disagree with the reviewer on this point. In the \u201ccommon answers\u201d we explain why we believe our contributions are significant both with respect to previous attempts to address unbalanced GW and with respect to unbalanced OT. \n- *\u201cCGW is quite weak and does not add extra practical value\u201d:* \nWe disagree with this statement.  [De Ponti et al.] and our paper appeared at the same time and are the only valid construction of unbalanced distances. None are implementable, but our is controlled (upper-bounded) by a valid divergence. \n- *\u201c [...] the authors can provide some thoughts on how to practically implement CGW in the rebuttal [...]\u201d : *\nUnfortunately, the conic formulation seems much more challenging to solve, in particular it cannot be cast as a finite dimensional optimization. A workaround is to approximate the lifted measure using a particle system. We will mention this in an updated version of the paper, but will leave these extensions for future works. \n- *\u201cThe theories (Proposition 1,2 and Lemma 1) are fine and not hard to obtain given those from UOT\u201d:* \nWe respectfully disagree with the reviewer on this point. We provided in the common answers some illustrative examples showcasing the difference in the proof of UOT and UGW theoretical results.\n- *\u201cIn the revision, I think the authors should give some examples of satisfying the assumptions in Propositions 1 and 2 to help the readers to understand these results better.\u201d:* \nThe assumptions are satisfied for all common entropies such as KL and TV. The compactness assumption holds in cases of practical interest, when working on bounded (and in particular finite) sets (e.g. graphs, pointclouds). We will add more details on this in a revised manuscript.\n- *\u201cIn Lemma 1, the authors should provide some intuition about $L_c$ as it is a key term used later to define the conic discrepancies D in the definition of CGW\u201d:* \nWe acknowledge that most of the information regarding $L_c$ and $H_c$ are buried in the supplementary material. $L_c$ appears naturally when re-writing the initial optimization problem using the inverse of the marginal densities (denoted f and g) which itself is useful to separate the pure transport of mass (measured by $L_c$) and the pure creation/destruction (measured by the term multiplied by $\\psi_\\infty$ in (4)). $H_c$ is the 1-homogeneous transformation of $L_c$, which is required in order to perform a conic lifting of the problem. We will update the final version of the manuscript to include additional intuition about these functionals. \n- *\u201cThere is an interesting algorithmic perspective of using unbalanced version over the balanced version. \u201c* \nWe thank the reviewer for pointing this to us. This provides an additional motivation (convergence speed) to use unbalanced methods in GW.  We will add this reference in the paper.\n- *\u201cThe literature of using UOT as a robust version of OT in practical applications of deep generative models and domain adaptation has been considered recently\u201d:* \nWe thank the reviewer for this very relevant reference which will be added to the paper.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper651/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper651/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Unbalanced Gromov Wasserstein Distance: Conic Formulation and Relaxation", "authorids": ["~Thibault_Sejourne2", "~Fran\u00e7ois-Xavier_Vialard2", "~Gabriel_Peyr\u00e92"], "authors": ["Thibault Sejourne", "Fran\u00e7ois-Xavier Vialard", "Gabriel Peyr\u00e9"], "keywords": ["Gromov-Wasserstein", "Non-convex optimization", "Optimal Transport", "Partial matching"], "abstract": "Comparing metric measure spaces (i.e. a metric space endowed with a probability distribution) is at the heart of many machine learning problems. This includes for instance predicting properties of molecules in quantum chemistry or generating graphs with varying connectivity. The most popular distance between such metric measure spaces is the Gromov-Wasserstein (GW) distance, which is the solution of a quadratic assignment problem. This distance has been successfully applied to supervised learning and generative modeling, for applications as diverse as quantum chemistry or natural language processing. The GW distance is however limited to the comparison of metric measure spaces endowed with a probability distribution. This strong limitation is problematic for many applications in ML where there is no a priori natural normalization on the total mass of the data. Furthermore, imposing an exact conservation of mass across spaces is not robust to outliers and often leads to irregular matching. To alleviate these issues, we introduce two Unbalanced Gromov-Wasserstein formulations: a distance and a more tractable upper-bounding relaxation. They both allow the comparison of metric spaces equipped with arbitrary positive measures up to isometries. The first formulation is a positive and definite divergence based on a relaxation of the mass conservation constraint using a novel type of quadratically-homogeneous divergence.This divergence works hand in hand with the entropic regularization approach which is popular to solve large scale optimal transport problems. We show that the underlying non-convex optimization problem can be efficiently tackled using a highly parallelizable and GPU-friendly iterative scheme. The second formulation is a distance between mm-spaces up to isometries based on a conic lifting. Lastly, we provide numerical simulations to highlight the salient features of the unbalanced divergence and its potential applications in ML.", "one-sentence_summary": "It is the generalization of the Gromov-Wasserstein distance inspired from unbalanced optimal transport, which is proved to define a distance and to be computable via GPU routines.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sejourne|the_unbalanced_gromov_wasserstein_distance_conic_formulation_and_relaxation", "pdf": "/pdf/85a7cbe50f7781f24692916b1e62cc214ff5e7d7.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=oHdCTBJMND", "_bibtex": "@misc{\nsejourne2021the,\ntitle={The Unbalanced Gromov Wasserstein Distance: Conic Formulation and Relaxation},\nauthor={Thibault Sejourne and Fran{\\c{c}}ois-Xavier Vialard and Gabriel Peyr{\\'e}},\nyear={2021},\nurl={https://openreview.net/forum?id=GzHjhdpk-YH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "GzHjhdpk-YH", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper651/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper651/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper651/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper651/Authors|ICLR.cc/2021/Conference/Paper651/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper651/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923868648, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper651/-/Official_Comment"}}}, {"id": "ZFuCc8AmP5f", "original": null, "number": 4, "cdate": 1605476530716, "ddate": null, "tcdate": 1605476530716, "tmdate": 1605481790756, "tddate": null, "forum": "GzHjhdpk-YH", "replyto": "qJFteJhbEv", "invitation": "ICLR.cc/2021/Conference/Paper651/-/Official_Comment", "content": {"title": "Personnal answer to reviewer 2", "comment": "We would like to thank the reviewer for his careful reading of our submission. We wrote a common answer addressed to all reviewers in a comment above, and provide here a personal answer to the reviewer\u2019s comments.\n\n- *\u201cI wonder, if some notations could have been eased a little bit\u201d:* \nWe acknowledge that the generality of the approach makes the paper notation-heavy. However, the degree of freedom in choosing general divergences makes the method versatile and will likely be useful for future applications. To showcase the difference with KL, we will add in the revised version a simulation using TV or the method of [Chapel et al.] which is connected to our TV setting.\n- *\u201cThe algorithm parts is a little less satisfactory, as it amounts to optimize an upper bound of the described problem, following existing work from [M\u00e9moli11].\u201d:* \nFirst note that we do not solve for an upper bound, but a lower bound when introducing a pair ($\\gamma$,$\\pi$). This approach is the de-facto standard approach to solve this type of bilinear problem. A theoretical justification is provided in [1] which proves that when the functional is concave (which is the case for GW on Euclidean space) then the bound is tight. Extending this to general cases is unclear, but in practice, we found that the algorithm always converges to $\\gamma$=$\\pi$, thus showing that the bound is tight. We will further explain this in the revised version. \n- *\u201cIt seems that this effect is observable in Figure 2, where parts of a moon are \u2018flipped\u2019 in the matching.\u201d:* \nin this specific case, the  flip is not due to a suboptimal matching, but rather to local symmetries of the pointcloud (so that the loss function has several almost global minimizers).\n- *\u201cThe claim that a cost of O(n^3) in time and O(n^2) in memory allows to scale to large problems is a bit strong\u201d:* \nIndeed, we will lower our claim, our method only scales to medium size problems (~10K). Extending this to larger problems would require using kernel compression methods such as hierarchical factorization [2] or Nystrom approximations of the cost [3]. We will mention this, but left the study of these approximation methods to future works.\n- *\u201cThe final rescaling of $\\gamma$ (line 8 of Algorithm 1), is a little bit difficult to understand from the alternating minimization point of view\u201d:* \nThis is only to remove the invariance of the problem by rescaling ($\\pi$,$\\gamma$)->($\\pi$ *c, $\\gamma$ /c), which is important to ensure that one hopefully gets $\\pi$=$\\gamma$ at convergence (which is observed in practice), but up to fixing this invariance it has no impact.\n- *\u201cIn the experimental Section, which algorithm is used to compute GW ?\u201d:* \nWe use the implementation of GW available in the POT library, which is equivalent to our algorithm when using $\\epsilon$=0 (no entropy regularization) and no mass relaxation (balanced case). \n- *\u201cAlso, I guess Figure 3 could present the original graphs (without scaling the dots), to gain a better understanding of the original problem\u201d:*  \nThe graph plotted for GW in Figure 3 should be taken as reference since there is no rescaling due to mass creation.\n\nReferences:\n\n[1] Konno, H. (1976). Maximization of a convex quadratic function under linear constraints.\n\n[2] Altschuler, J., Bach, F., Rudi, A., & Niles-Weed, J. (2019). Massively scalable Sinkhorn distances via the Nystr\u00f6m method.\n\n[3] Xu, H., Luo, D., & Carin, L. (2019). Scalable Gromov-Wasserstein learning for graph partitioning and matching."}, "signatures": ["ICLR.cc/2021/Conference/Paper651/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper651/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Unbalanced Gromov Wasserstein Distance: Conic Formulation and Relaxation", "authorids": ["~Thibault_Sejourne2", "~Fran\u00e7ois-Xavier_Vialard2", "~Gabriel_Peyr\u00e92"], "authors": ["Thibault Sejourne", "Fran\u00e7ois-Xavier Vialard", "Gabriel Peyr\u00e9"], "keywords": ["Gromov-Wasserstein", "Non-convex optimization", "Optimal Transport", "Partial matching"], "abstract": "Comparing metric measure spaces (i.e. a metric space endowed with a probability distribution) is at the heart of many machine learning problems. This includes for instance predicting properties of molecules in quantum chemistry or generating graphs with varying connectivity. The most popular distance between such metric measure spaces is the Gromov-Wasserstein (GW) distance, which is the solution of a quadratic assignment problem. This distance has been successfully applied to supervised learning and generative modeling, for applications as diverse as quantum chemistry or natural language processing. The GW distance is however limited to the comparison of metric measure spaces endowed with a probability distribution. This strong limitation is problematic for many applications in ML where there is no a priori natural normalization on the total mass of the data. Furthermore, imposing an exact conservation of mass across spaces is not robust to outliers and often leads to irregular matching. To alleviate these issues, we introduce two Unbalanced Gromov-Wasserstein formulations: a distance and a more tractable upper-bounding relaxation. They both allow the comparison of metric spaces equipped with arbitrary positive measures up to isometries. The first formulation is a positive and definite divergence based on a relaxation of the mass conservation constraint using a novel type of quadratically-homogeneous divergence.This divergence works hand in hand with the entropic regularization approach which is popular to solve large scale optimal transport problems. We show that the underlying non-convex optimization problem can be efficiently tackled using a highly parallelizable and GPU-friendly iterative scheme. The second formulation is a distance between mm-spaces up to isometries based on a conic lifting. Lastly, we provide numerical simulations to highlight the salient features of the unbalanced divergence and its potential applications in ML.", "one-sentence_summary": "It is the generalization of the Gromov-Wasserstein distance inspired from unbalanced optimal transport, which is proved to define a distance and to be computable via GPU routines.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sejourne|the_unbalanced_gromov_wasserstein_distance_conic_formulation_and_relaxation", "pdf": "/pdf/85a7cbe50f7781f24692916b1e62cc214ff5e7d7.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=oHdCTBJMND", "_bibtex": "@misc{\nsejourne2021the,\ntitle={The Unbalanced Gromov Wasserstein Distance: Conic Formulation and Relaxation},\nauthor={Thibault Sejourne and Fran{\\c{c}}ois-Xavier Vialard and Gabriel Peyr{\\'e}},\nyear={2021},\nurl={https://openreview.net/forum?id=GzHjhdpk-YH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "GzHjhdpk-YH", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper651/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper651/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper651/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper651/Authors|ICLR.cc/2021/Conference/Paper651/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper651/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923868648, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper651/-/Official_Comment"}}}, {"id": "-t96efchbTA", "original": null, "number": 2, "cdate": 1605475313745, "ddate": null, "tcdate": 1605475313745, "tmdate": 1605481493887, "tddate": null, "forum": "GzHjhdpk-YH", "replyto": "GzHjhdpk-YH", "invitation": "ICLR.cc/2021/Conference/Paper651/-/Official_Comment", "content": {"title": "Common answers to the reviewers", "comment": "We would like to thank all the reviewers for their careful reading of our manuscript. We would like to start by addressing some common remarks made by the reviewers.\n\n**Novelty with respect to previous works:**\nOnly 2 previous works address the computation of unbalanced GW : (i) [Chapel et al.] precedes ours but relies on a Frank-Wolfe solver which is not easily parallelizable on GPUs ; furthermore, it does not define a distance ; (ii)  [De Ponti et al.] appeared on arxiv concomitantly to this submission and also defines a distance as our conic formulation, but is not amenable to a finite dimensional optimization problem when considering finite spaces (does not come with a fast algorithm in contrast to our relaxed upper-bounding formulation). We believe our contributions are the first one to provide a distance which is upper-bounded by a computable divergence. \n\n**Novelty with respect to unbalanced OT:**\nWe disagree with the statement that our contributions are simple extensions of the existing unbalanced OT theory. The switch from comparing measures to comparing measure spaces requires the use of different proof techniques. The most striking difference is that the associated conic formulations are not on the same cone (UOT considers the cone X x R_+ while we use the 2-D cone R  x R_+). As an example of difference, the proof of definiteness of CGW (appendix C.1.1) requires to take into account  the conic constraints (Eq. (5)), prove its equivalence with a quadratic form of (Eq. (5)), and handle the isometry invariance, which differs radically from UOT. Another illustrative example of difference is when proving the inequality UGW>=CGW: it requires the design of an admissible plan (Eq. (24)) of the latter program. In sharp contrast with the UOT case, proving its admissibility requires taking into account both the conic framework (Eq 25 and computations below) and the quadratic structure of GW (Eq 23) at the same time.\n\n**Use of quadratic f-divergences:**\nIn addition to these theoretical novelties with respect to unbalanced OT is the use of quadratic divergences (which to the best of our knowledge, is the first apparition of such divergences). Using classical divergences would result in non-homogeneous UGW distances, which would thus degenerate if one considers measures with small or large total mass. Maintaining this 2-homogeneity is crucial and requires new theoretical proof techniques. In the final version, we will further expose these important novelties. \n\nHere is a more precise statement on the degeneracy. Writing $UGW$ and $\\tilde{UGW}$ the programs using respectively quadratic and standard f-divergences, we have:\n$\\lambda^{-2}UGW(\\lambda\\mu,\\lambda\\nu) = UGW(\\mu,\\nu),$\n$\\lim_{\\lambda\\rightarrow\\infty}\\lambda^{-2}\\tilde{UGW}(\\lambda\\mu,\\lambda\\nu) = 0.$\n$\\lim_{\\lambda\\rightarrow\\infty}\\lambda^{-1}\\tilde{UGW}(\\lambda\\mu,\\lambda\\nu) = +\\infty.$\nIn other terms, the behaviours differ depending on the scale of the mass, while our formulation remains consistent with respect to rescaling.\n\n**Experiments on real applications:**\nWe understand the criticism that we did not provide numerical illustrations on real applications. But the point of the paper is to make the case that unbalanced methods make lots of sense for most GW applications. We believe that our work is a key milestone in building efficient and theoretically sound divergences between unbalanced metric measure spaces. Although it is possible to address this by some ad-hoc normalization, we propose the first approach (conic distance) which is theoretically sound and which also comes with a simple to compute upper-bound (relaxed divergence). The purpose of our numerical simulations is rather to illustrate qualitatively the impact of mass relaxation. In the final manuscript, we will show a comparison between KL (already shown) and TV divergence or the methods of [Chapel et al.] to further exemplify the versatility of our approach and the impact of the mass relaxation on the solution. \n\n**Tightness of the upper-bound:**\nWe believe the upper bound UGW>=CGW is not an equality when the spaces are different, for reasons due to the quadratic structure of the problem. We expect the perspective transform $H_c$ (Eq (10)) not to be tight because the scaling $\\theta$ depends on $(x,x\u2019,y,y\u2019,r,r\u2019,s,s\u2019)$, while tightness would require a tensorized dependence in $(x,y,r,s)$ and $(x\u2019,y\u2019,r\u2019,s\u2019)$, in order to be compatible with the structure of GW problem. This being said, even in simple cases (like 2 points spaces), we were not able to find an explicit counter example, and we leave this study for future works."}, "signatures": ["ICLR.cc/2021/Conference/Paper651/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper651/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Unbalanced Gromov Wasserstein Distance: Conic Formulation and Relaxation", "authorids": ["~Thibault_Sejourne2", "~Fran\u00e7ois-Xavier_Vialard2", "~Gabriel_Peyr\u00e92"], "authors": ["Thibault Sejourne", "Fran\u00e7ois-Xavier Vialard", "Gabriel Peyr\u00e9"], "keywords": ["Gromov-Wasserstein", "Non-convex optimization", "Optimal Transport", "Partial matching"], "abstract": "Comparing metric measure spaces (i.e. a metric space endowed with a probability distribution) is at the heart of many machine learning problems. This includes for instance predicting properties of molecules in quantum chemistry or generating graphs with varying connectivity. The most popular distance between such metric measure spaces is the Gromov-Wasserstein (GW) distance, which is the solution of a quadratic assignment problem. This distance has been successfully applied to supervised learning and generative modeling, for applications as diverse as quantum chemistry or natural language processing. The GW distance is however limited to the comparison of metric measure spaces endowed with a probability distribution. This strong limitation is problematic for many applications in ML where there is no a priori natural normalization on the total mass of the data. Furthermore, imposing an exact conservation of mass across spaces is not robust to outliers and often leads to irregular matching. To alleviate these issues, we introduce two Unbalanced Gromov-Wasserstein formulations: a distance and a more tractable upper-bounding relaxation. They both allow the comparison of metric spaces equipped with arbitrary positive measures up to isometries. The first formulation is a positive and definite divergence based on a relaxation of the mass conservation constraint using a novel type of quadratically-homogeneous divergence.This divergence works hand in hand with the entropic regularization approach which is popular to solve large scale optimal transport problems. We show that the underlying non-convex optimization problem can be efficiently tackled using a highly parallelizable and GPU-friendly iterative scheme. The second formulation is a distance between mm-spaces up to isometries based on a conic lifting. Lastly, we provide numerical simulations to highlight the salient features of the unbalanced divergence and its potential applications in ML.", "one-sentence_summary": "It is the generalization of the Gromov-Wasserstein distance inspired from unbalanced optimal transport, which is proved to define a distance and to be computable via GPU routines.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sejourne|the_unbalanced_gromov_wasserstein_distance_conic_formulation_and_relaxation", "pdf": "/pdf/85a7cbe50f7781f24692916b1e62cc214ff5e7d7.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=oHdCTBJMND", "_bibtex": "@misc{\nsejourne2021the,\ntitle={The Unbalanced Gromov Wasserstein Distance: Conic Formulation and Relaxation},\nauthor={Thibault Sejourne and Fran{\\c{c}}ois-Xavier Vialard and Gabriel Peyr{\\'e}},\nyear={2021},\nurl={https://openreview.net/forum?id=GzHjhdpk-YH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "GzHjhdpk-YH", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper651/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper651/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper651/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper651/Authors|ICLR.cc/2021/Conference/Paper651/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper651/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923868648, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper651/-/Official_Comment"}}}, {"id": "vjfh6kz1OFb", "original": null, "number": 3, "cdate": 1605475860176, "ddate": null, "tcdate": 1605475860176, "tmdate": 1605476172303, "tddate": null, "forum": "GzHjhdpk-YH", "replyto": "ubzjMspIFIv", "invitation": "ICLR.cc/2021/Conference/Paper651/-/Official_Comment", "content": {"title": "Personnal answer to Reviewer 1", "comment": "We would like to thank the reviewer for the careful reading of our submission. We wrote a common answer addressed to all reviewers in a comment above, and provide here a personal answer to the reviewer\u2019s comments.\n\n- *\u201cIt seems better if the authors use UGW in some applications\u201d:* \nas detailed in the common answers, our goal in the numerical simulations was rather to give some intuitions about the relevance and impact of mass relaxation. \n- *\u201cCould the authors give more motivation/explanation about this approach, why not just divergence as in unbalanced OT\u201d:* \nas detailed in the common answers, using \u201cclassical\u201d divergences makes little sense because losing mass-homogeneity would lead to degenerate loss functions for measures with very small or very large total mass.\n- *\u201cThe proposed conic GW is theoretically interesting since it comes up with no algorithm yet.[...]  However, CGW has no algorithm yet.\u201d:* \nUnfortunately, the conic formulation seems much more challenging to solve, in particular it cannot be cast as a finite dimensional optimization. A workaround is to approximate the lifted measure using a particle system. We will mention this in an updated version of the paper, but will leave these extensions for future works. \n- *\u201cI wonder whether there exist some special instances of UGW (with some specific divergence, e.g., total variation or KL), one may have some interesting properties as in CGW?\u201d:* \nas detailed in the common answers, this is a very relevant but difficult question, which we leave open for future works, and will mention in the perspectives of the paper.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper651/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper651/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Unbalanced Gromov Wasserstein Distance: Conic Formulation and Relaxation", "authorids": ["~Thibault_Sejourne2", "~Fran\u00e7ois-Xavier_Vialard2", "~Gabriel_Peyr\u00e92"], "authors": ["Thibault Sejourne", "Fran\u00e7ois-Xavier Vialard", "Gabriel Peyr\u00e9"], "keywords": ["Gromov-Wasserstein", "Non-convex optimization", "Optimal Transport", "Partial matching"], "abstract": "Comparing metric measure spaces (i.e. a metric space endowed with a probability distribution) is at the heart of many machine learning problems. This includes for instance predicting properties of molecules in quantum chemistry or generating graphs with varying connectivity. The most popular distance between such metric measure spaces is the Gromov-Wasserstein (GW) distance, which is the solution of a quadratic assignment problem. This distance has been successfully applied to supervised learning and generative modeling, for applications as diverse as quantum chemistry or natural language processing. The GW distance is however limited to the comparison of metric measure spaces endowed with a probability distribution. This strong limitation is problematic for many applications in ML where there is no a priori natural normalization on the total mass of the data. Furthermore, imposing an exact conservation of mass across spaces is not robust to outliers and often leads to irregular matching. To alleviate these issues, we introduce two Unbalanced Gromov-Wasserstein formulations: a distance and a more tractable upper-bounding relaxation. They both allow the comparison of metric spaces equipped with arbitrary positive measures up to isometries. The first formulation is a positive and definite divergence based on a relaxation of the mass conservation constraint using a novel type of quadratically-homogeneous divergence.This divergence works hand in hand with the entropic regularization approach which is popular to solve large scale optimal transport problems. We show that the underlying non-convex optimization problem can be efficiently tackled using a highly parallelizable and GPU-friendly iterative scheme. The second formulation is a distance between mm-spaces up to isometries based on a conic lifting. Lastly, we provide numerical simulations to highlight the salient features of the unbalanced divergence and its potential applications in ML.", "one-sentence_summary": "It is the generalization of the Gromov-Wasserstein distance inspired from unbalanced optimal transport, which is proved to define a distance and to be computable via GPU routines.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sejourne|the_unbalanced_gromov_wasserstein_distance_conic_formulation_and_relaxation", "pdf": "/pdf/85a7cbe50f7781f24692916b1e62cc214ff5e7d7.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=oHdCTBJMND", "_bibtex": "@misc{\nsejourne2021the,\ntitle={The Unbalanced Gromov Wasserstein Distance: Conic Formulation and Relaxation},\nauthor={Thibault Sejourne and Fran{\\c{c}}ois-Xavier Vialard and Gabriel Peyr{\\'e}},\nyear={2021},\nurl={https://openreview.net/forum?id=GzHjhdpk-YH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "GzHjhdpk-YH", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper651/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper651/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper651/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper651/Authors|ICLR.cc/2021/Conference/Paper651/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper651/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923868648, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper651/-/Official_Comment"}}}, {"id": "aQTYzjP0clK", "original": null, "number": 2, "cdate": 1603778684615, "ddate": null, "tcdate": 1603778684615, "tmdate": 1605024638107, "tddate": null, "forum": "GzHjhdpk-YH", "replyto": "GzHjhdpk-YH", "invitation": "ICLR.cc/2021/Conference/Paper651/-/Official_Review", "content": {"title": "The paper is quite interesting but lacks novelty.", "review": "In the paper, the authors propose two versions of Gromov-Wasserstein distance when the weights of measures do not need to sum up to one. The first version, named Unbalanced Gromov-Wasserstein (UGW), is a direct application of unbalanced optimal transport (UOT) to the setting of Gromov-Wassertstein. The second version, named Conic Gromov-Wasserstein (CGW), is an extension of the conic formulation of UOT to the setting of Gromov-Wasserstein. The authors also show that CGW is a distance in the metric-measure spaces and is an lower bound of the UGW. Finally, the authors also provide some experiments with their proposed divergences.\n\nIn my opinion, the two proposed versions of Gromov-Wasserstein are quite interesting but lack novelty. They are simply extensions of the framework of UOT to the framework of Gromov-Wasserstein. Here are my other comments with the paper:\n\n(1) The CGW is introduced as a lower bound of UGW; however, it is computationally expensive and practically out-of-reach as the authors admitted in the paper. Therefore, I am a bit confused with the proposal of CGW in the paper as it is not useful in practice. The theory of CGW also does not appear deep enough for me to appreciate the contribution. Unless the authors can provide some thoughts on how to practically implement CGW in the rebuttal, I feel that the inclusion of CGW is quite weak and does not add extra practical value to the paper.\n\n(2) Going back to the UGW, it is a direct extension of the framework from UOT to that of Gromov-Wasserstein. The theories (Proposition 1,2 and Lemma 1) are fine and not hard to obtain given those from UOT. In the revision, I think the authors should give some examples of $\\varphi$ satisfying the assumptions in Propositions 1 and 2 to help the readers to understand these results better. In Lemma 1, the authors should provide some intuition about $L_{c}(a, b)$ as it is a key term used later to define the conic discrepancies $\\mathcal{D}$ in the definition of CGW.\n\n(3) There is an interesting algorithmic perspective of using unbalanced version over the balanced version. For the setting of OT, the recent paper of [1] shows that solving the UOT (by Sinkhorn algorithm) when the divergence is KL has the complexity of $n^2/ \\varepsilon$, which is near optimal, where $\\varepsilon$ denotes the desired accuracy to approximate the UOT by the entropic UOT. On the other hand, the complexity of using Sinkhorn algorithm for solving the OT is at the order of $n^2/ \\varepsilon^2$. Therefore, by considering the relaxation over constraints, solving UOT is in fact algorithmically favorable than that of solving OT. \n\nGoing back to the UGW, the relaxation of constraints, in my opinion, may also help the convergence of algorithm (specifically Algorithm 1 in the paper) comparing to that of the GW (via Sinkhorn algorithm) though the rigorous theoretical analyses of these algorithms can be very tricky in general. However, I think the authors also can provide some numerical experiments to see whether this phenomenon holds true. \n\n(4) The literature of using UOT as a robust version of OT in practical applications of deep generative models and domain adaptation has been considered recently; see for example the paper [2]. The authors may consider adding this reference in the related works and contributions.\n\n\nReferences:\n\n[1] K Pham, K Le, N Ho, T Pham, H Bui. On Unbalanced Optimal Transport: An Analysis of Sinkhorn Algorithm. ICML, 2020\n\n[2] Y. Balaji, R. Chellappa, S. Feizi. Robust Optimal Transport with Applications in Generative Modeling and Domain Adaptation. NeurIPS, 2020\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper651/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper651/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Unbalanced Gromov Wasserstein Distance: Conic Formulation and Relaxation", "authorids": ["~Thibault_Sejourne2", "~Fran\u00e7ois-Xavier_Vialard2", "~Gabriel_Peyr\u00e92"], "authors": ["Thibault Sejourne", "Fran\u00e7ois-Xavier Vialard", "Gabriel Peyr\u00e9"], "keywords": ["Gromov-Wasserstein", "Non-convex optimization", "Optimal Transport", "Partial matching"], "abstract": "Comparing metric measure spaces (i.e. a metric space endowed with a probability distribution) is at the heart of many machine learning problems. This includes for instance predicting properties of molecules in quantum chemistry or generating graphs with varying connectivity. The most popular distance between such metric measure spaces is the Gromov-Wasserstein (GW) distance, which is the solution of a quadratic assignment problem. This distance has been successfully applied to supervised learning and generative modeling, for applications as diverse as quantum chemistry or natural language processing. The GW distance is however limited to the comparison of metric measure spaces endowed with a probability distribution. This strong limitation is problematic for many applications in ML where there is no a priori natural normalization on the total mass of the data. Furthermore, imposing an exact conservation of mass across spaces is not robust to outliers and often leads to irregular matching. To alleviate these issues, we introduce two Unbalanced Gromov-Wasserstein formulations: a distance and a more tractable upper-bounding relaxation. They both allow the comparison of metric spaces equipped with arbitrary positive measures up to isometries. The first formulation is a positive and definite divergence based on a relaxation of the mass conservation constraint using a novel type of quadratically-homogeneous divergence.This divergence works hand in hand with the entropic regularization approach which is popular to solve large scale optimal transport problems. We show that the underlying non-convex optimization problem can be efficiently tackled using a highly parallelizable and GPU-friendly iterative scheme. The second formulation is a distance between mm-spaces up to isometries based on a conic lifting. Lastly, we provide numerical simulations to highlight the salient features of the unbalanced divergence and its potential applications in ML.", "one-sentence_summary": "It is the generalization of the Gromov-Wasserstein distance inspired from unbalanced optimal transport, which is proved to define a distance and to be computable via GPU routines.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sejourne|the_unbalanced_gromov_wasserstein_distance_conic_formulation_and_relaxation", "pdf": "/pdf/85a7cbe50f7781f24692916b1e62cc214ff5e7d7.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=oHdCTBJMND", "_bibtex": "@misc{\nsejourne2021the,\ntitle={The Unbalanced Gromov Wasserstein Distance: Conic Formulation and Relaxation},\nauthor={Thibault Sejourne and Fran{\\c{c}}ois-Xavier Vialard and Gabriel Peyr{\\'e}},\nyear={2021},\nurl={https://openreview.net/forum?id=GzHjhdpk-YH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "GzHjhdpk-YH", "replyto": "GzHjhdpk-YH", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper651/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538138300, "tmdate": 1606915782832, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper651/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper651/-/Official_Review"}}}, {"id": "XGndPvBQ2ta", "original": null, "number": 4, "cdate": 1603927231766, "ddate": null, "tcdate": 1603927231766, "tmdate": 1605024637909, "tddate": null, "forum": "GzHjhdpk-YH", "replyto": "GzHjhdpk-YH", "invitation": "ICLR.cc/2021/Conference/Paper651/-/Official_Review", "content": {"title": " A nice theoretical contribution that extends the Gromov-Wasserstein distance to the unbalanced setting, with limited experimental validation", "review": "Summary and overall recommendation:\n\nThis paper introduces two generalizations of the Gromov-Wasserstein distance to the case where the measures are unbalanced (i.e., not necessarily probability measures). While one of these is a proper distance, it is computationally infeasible. The other one, an upper bound relaxation, is not a proper distance but is much more computationally feasible. The main contribution of the paper is to prove some fundamental properties of these objects: positivity and definiteness. The paper proposes an algorithm for solving the upper-bound version, and shows proof-of-point experiments on very simple 2D settings. \n\nDespite some shortcomings in novelty, experimental evaluation, and presentation detailed below, I find the theoretical contribution of this paper to be just enough to carry it, so I'm (weakly) recommending acceptance. \n\nStrengths:\n\n1. Albeit relying on ideas that are already quite popular in the OT literature (Gromov-Wasserstein distance, Unbalanced OT), this paper manages to weave them together in a very compelling way \n2. The paper has sound theoretical footing, and a makes a solid theoretical contribution.\n3. The writing and exposition are mostly clear, intuitive and well developed\n\nWeaknesses:\n1. The flip side of strength (1) is that the novelty of this paper is arguably limited, considering it builds on known techinques, and is addressing a problem that has been tackled in at least two recent works (De Ponti & Mondino 2020, Chapel et al. 2020).\n2. Some of the assumptions and results are not discussed in enough detail\n3. The experimental section is limited to very simple settings, that either conisder measures on the same space (defeating the purpose of GW) (Fig 1,2), or do so for very simple synthetic data (Fig3). In particular, none of the motivating applications mentioned in the introduction (NLP, chemistry, graph matching). \n\nOther comments:\n1. It would be useful if the paper discussed the assumptions that go into Prop 1 (superlinearity, compact level sets) in more detail. What families of $\\phi$-divergences satisfy these?\n2. I understand that Lemma 1 is a tool to draw a connection between the two approaches, but I find its introduction dry and abrupt. It would help to provide some discussion on the intuition on introducing such a perspective transform.\n3. The presentation of the conic formulation needs fmore hand-holding. What's the motivation for this cumbersome the conic formulation? Are there other prior examples of such conic distances (even if not on mm spaces)?\n4. I would have appreciated a discussion on the tightness of the bound of Theorem 1. Without this, the purported possibility to use one as a computationally feasible drop-in for the other is not fully supported. \n5. I don't think $\\gamma_1$ and $\\gamma_2$ in page 6 were formally defined anywhere\n6. There is no comparison to other unbalanced methods (either OT or GW) except for one setting in Fig 3. I suspect most of these alternatives would produce similar results in Fig 1, so what is the competitive advantage of this method? If it is complexity/runtime (vs other unbalanced GW methods), then there should be either a formal complexity analysis, or a empirical runtime comparison. Without these, the paper feels incomplete. \n\nQuestions:\n1. What is meant by the last sentence in the first paragraph? This reads to be as if this paper claims to propose the first distance between the objects tackled in those works, which is certainly not the case for all of those which use GW or variations of the Wasserstein distance. \n2. Is there a reference for the quadratic $\\phi$ divergences introduced in 2.1?\n3. Do any of the experiments use the debiased UGW mentioned in page 7?\n\nMinor comments and typos:\n* pg 3 shed some lights\n* missing \"is\" in first line of Prop 1\n* pg 8 \"close to be isometric\"", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper651/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper651/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Unbalanced Gromov Wasserstein Distance: Conic Formulation and Relaxation", "authorids": ["~Thibault_Sejourne2", "~Fran\u00e7ois-Xavier_Vialard2", "~Gabriel_Peyr\u00e92"], "authors": ["Thibault Sejourne", "Fran\u00e7ois-Xavier Vialard", "Gabriel Peyr\u00e9"], "keywords": ["Gromov-Wasserstein", "Non-convex optimization", "Optimal Transport", "Partial matching"], "abstract": "Comparing metric measure spaces (i.e. a metric space endowed with a probability distribution) is at the heart of many machine learning problems. This includes for instance predicting properties of molecules in quantum chemistry or generating graphs with varying connectivity. The most popular distance between such metric measure spaces is the Gromov-Wasserstein (GW) distance, which is the solution of a quadratic assignment problem. This distance has been successfully applied to supervised learning and generative modeling, for applications as diverse as quantum chemistry or natural language processing. The GW distance is however limited to the comparison of metric measure spaces endowed with a probability distribution. This strong limitation is problematic for many applications in ML where there is no a priori natural normalization on the total mass of the data. Furthermore, imposing an exact conservation of mass across spaces is not robust to outliers and often leads to irregular matching. To alleviate these issues, we introduce two Unbalanced Gromov-Wasserstein formulations: a distance and a more tractable upper-bounding relaxation. They both allow the comparison of metric spaces equipped with arbitrary positive measures up to isometries. The first formulation is a positive and definite divergence based on a relaxation of the mass conservation constraint using a novel type of quadratically-homogeneous divergence.This divergence works hand in hand with the entropic regularization approach which is popular to solve large scale optimal transport problems. We show that the underlying non-convex optimization problem can be efficiently tackled using a highly parallelizable and GPU-friendly iterative scheme. The second formulation is a distance between mm-spaces up to isometries based on a conic lifting. Lastly, we provide numerical simulations to highlight the salient features of the unbalanced divergence and its potential applications in ML.", "one-sentence_summary": "It is the generalization of the Gromov-Wasserstein distance inspired from unbalanced optimal transport, which is proved to define a distance and to be computable via GPU routines.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sejourne|the_unbalanced_gromov_wasserstein_distance_conic_formulation_and_relaxation", "pdf": "/pdf/85a7cbe50f7781f24692916b1e62cc214ff5e7d7.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=oHdCTBJMND", "_bibtex": "@misc{\nsejourne2021the,\ntitle={The Unbalanced Gromov Wasserstein Distance: Conic Formulation and Relaxation},\nauthor={Thibault Sejourne and Fran{\\c{c}}ois-Xavier Vialard and Gabriel Peyr{\\'e}},\nyear={2021},\nurl={https://openreview.net/forum?id=GzHjhdpk-YH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "GzHjhdpk-YH", "replyto": "GzHjhdpk-YH", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper651/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538138300, "tmdate": 1606915782832, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper651/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper651/-/Official_Review"}}}], "count": 14}