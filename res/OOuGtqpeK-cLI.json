{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1363039740000, "tcdate": 1363039740000, "number": 4, "id": "8PUQYHnMEx8CL", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "OOuGtqpeK-cLI", "replyto": "OOuGtqpeK-cLI", "signatures": ["Tommi Vatanen, Tapani Raiko, Harri Valpola, Yann LeCun"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "First of all we would like to thank you for your informed, thorough and kind comments. We realize that there is major overlap with our previous paper [10]. We hope that these two papers could be combined in a journal paper later on. It was mentioned that we use some text verbatim from [10]. There is some basic methodology which is necessary to explain before going to deeper explanations and we felt that it is not a big violation to use our own text. However, we have now modified the sections in question with your comments and proposals in mind. If you feel that it is necessary to check every sentence for verbatim, please consider conditional acceptance with this condition.\r\n\r\nWe agree that the evidence supporting the use of the third transformation is rather weak. We have tried to report our findings as honestly as possible and also express our doubts in the paper (see, e.g.,  end of Section 4).\r\n\r\nTo reviewer 'Anonymous 1567':\r\n\r\nYou argue that Eqs. (12-17) are slightly accurate. However, we have computed the expectation over epsilon with pen and paper and epsilon does vanish from the Eqs. Thus, the Eqs. in question are exact. Would you think that we should still write down the gradients explicitly?\r\n\r\nWe had considered using the df/dx notation, but decided to use the f' notation, since the derivative is taken with respect to Bx and using the df/dx notation would require us to define a new variable u = Bx and denote df/du. We think, this would further clutter the equations. Would you think this is acceptable?\r\n\r\nWe tried to clarify the meaning of 'transforming the model instead of the gradient ...' in Discussion.\r\n\r\nTo reviewer 'Anonymous b670':\r\n\r\nWe have now explained the relationship to Schraudolph's method in more detail. We provide an example and refer to Discussion of [10].\r\n\r\nWhen writing about 'many competing paths' and 'linear dependencies', we have added explanations with equations in the updated version.\r\n\r\nThe question, whether the arguments in Section 3 can be applied to networks with more than one hidden layer: We have presented the theory with this simplified case in order to convey the understanding to the reader. We assume that the idea could be formulated in the general (deep) case, but writing it out would substantially complicate the equations. Our experimental results support this assumption.\r\n\r\nAbout the uncorrelatedness assumption, we have added the following explanation: 'Naturally, it is unrealistic to assume that inputs $x_t$, nonlinear activations $f(cdot)$, and their slopes $f^prime(cdot)$ are all uncorrelated, so the goodness of this approximation is empirically evaluated in the next section.'\r\n\r\nWe do realize that it is possible and more elegant to compute exact solution for the Hessian matrix. However, as being more error prone, it would require careful checking by, e.g., some approximative method. As the mere approximation suits our needs well, we refrained from doing the extra work for the exact solution. We have also acknowledged this in the paper.\r\n\r\nRegarding mu in Eq. 19: Thanks for this remark. We have reformulated the text surrounding Eq. 19. Could you kindly provide further suggestions and/or references if you still find it unsatisfactory.\r\n\r\nExperiments on eigenvalue distribution: Fig. 1(a) suggest that there is no clear difference between the eigenvalue distributions with our without gamma (the vertical position of the plot is irrelevant since it corresponds to choosing a different learning rate).\r\n\r\nWe show histogram of diagonal elements in order to distinguish between weights. For instance, the colors in Fig.2 could not have been used otherwise.\r\n\r\nFisher Information vs. Hessian matrix: This is a relevant point for the future work. The Hessian describes the curvature of the actual optimization problem. We chose Fisher information matrix in the theoretical part simply because it has more compact equations. As we note in the paper, 'the hessian matrix is closely related to the Fisher information matrix, but it does depend on the output data and contains more term'. We argue that the terms present in Fisher information matrix will make our point clear and adding the other terms included in the Hessian would just be additional clutter.\r\n\r\nTommi Vatanen and Tapani Raiko"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Pushing Stochastic Gradient towards Second-Order Methods -- Backpropagation Learning with Transformations in Nonlinearities", "decision": "conferencePoster-iclr2013-workshop", "abstract": "Recently, we proposed to transform the outputs of each hidden neuron in a multi-layer perceptron network to have zero output and zero slope on average, and use separate shortcut connections to model the linear dependencies instead. We continue the work by firstly introducing a third transformation to normalize the scale of the outputs of each hidden neuron, and secondly by analyzing the connections to second order optimization methods. We show that the transformations make a simple stochastic gradient behave closer to second-order optimization methods and thus speed up learning. This is shown both in theory and with experiments. The experiments on the third transformation show that while it further increases the speed of learning, it can also hurt performance by converging to a worse local optimum, where both the inputs and outputs of many hidden neurons are close to zero.", "pdf": "https://arxiv.org/abs/1301.3476", "paperhash": "vatanen|pushing_stochastic_gradient_towards_secondorder_methods_backpropagation_learning_with_transformations_in_nonlinearities", "keywords": [], "conflicts": [], "authors": ["Tommi Vatanen", "Tapani Raiko", "Harri Valpola", "Yann LeCun"], "authorids": ["tommivat@gmail.com", "tapani.raiko@gmail.com", "harri.valpola@aalto.fi", "ylecun@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362399720000, "tcdate": 1362399720000, "number": 1, "id": "og9azR3sTxoul", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "OOuGtqpeK-cLI", "replyto": "OOuGtqpeK-cLI", "signatures": ["anonymous reviewer b670"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Pushing Stochastic Gradient towards Second-Order Methods -- Backpropagation Learning with Transformations in Nonlinearities", "review": "This paper builds on previous work by the same authors that looks at performing dynamic reparameterizations of neural networks to improve training efficiency.  The previously published approach is augmented with an additional parameter (gamma) which, although it is argued should help in theory, doesn't seem to in practice.  Theoretical arguments for why the standard gradient computed under this reparameterization will be closer to a 2nd-order update are made, and experiments are conducted.   While the theoretical arguments are pretty weak in my opinion (see detailed comments below), the experiments that looks at eigenvalues of the Hessian are somewhat more convincing, although they indicate that the originally published approach, without the gamma modification, is doing a better job.\r\n\r\n\r\nPros:\r\n- reasonably well written\r\n- experiments looking at eigenvalue distributions are interesting\r\n\r\nCons:\r\n- actual method is similar to authors' previous work in [10] and the older method of Schraudolph [12]\r\n- the new modification doesn't seem to improve training efficiency, and even makes the eigenvalue distribution worse\r\n- there seem to be problems with the theoretical analysis (maybe the authors can address this in their response?)\r\n\r\n\r\n///// Detailed comments \\\\\r\n\r\nBecause it sounds similar to what you're doing, I think it would be helpful to give a slightly more detailed description of Schraudolph's 'gradient factor centering'.   Does it correspond exactly to what you are doing in the case of neural nets?  And if so, could you give an interesting example of how to apply your method to other models where Schraudolph's method would no longer apply? \r\n\r\nI don't understand what you mean by 'many competing paths' at the bottom of page 2.  \r\n\r\nAnd when talking about 'linear dependencies' from x to y, what exactly do you mean?  Do you mean the 1st-order components of the Taylor series of the true mapping or something else?   Also, you might want to use affine when discussing functions that are linear + constant to be more technically precise.\r\n\r\nCan the arguments in section 3 be applied to network with more than 1 hidden layer?\r\n\r\nA concern I have with the analysis in section 3 is that, while assuming uncorrelated hidden unit outputs might be somewhat sensible (although I feel that our intuitions about how neural networks model certain mappings - such as 'representing different things' may be inaccurate), it seems less reasonable to assume that inputs (x) are uncorrelated with the outputs of the units, which seems to be needed to show that off-diagonal terms are zero (other than for eqn 12).   You also seem to assume that certain 1st-derivatives of unit outputs are uncorrelated with various quantities (inputs, other unit outputs, and unit derivatives), which I don't think follows from the assumptions about the outputs of the units being uncorrelated with each other (but if this is indeed true, you should prove it or provide a reference).  I think you should apply more rigor to these arguments for them to be convincing.\r\n\r\nI would recommend using an exact method to compute the Hessian.  For example, you can compute it using n matrix-vector products, and tools for computing these automatically for any computational graph are widely available, as are particular formulae for neural networks.  Such a method would be no more costly than what you are doing now, which involves n gradient computations.\r\n\r\nThe discussion surrounding equation 19 is an somewhat inaccurate and oversimplified account of the role that a constant like mu has in a second-order update rule like eqn. 19.  This is a well studied and highly complex problem which doesn't really have to do with issues surrounding the inversion of the Hessian 'blowing up' so much as the problems of break-downs in model trust that occur when computing proposals based on local quadratic models of the objective. \r\n\r\nYour experiments seem to suggest that the eigenvalues are more even when you leave out the gamma parameter.  How do you reconcile this with your theoretical analysis?\r\n\r\nWhy do you show a histogram of diagonal elements as opposed to eigenvalues in figure 2?  I would argue that the concentration of the eigenvalues is a much better indicator of how close the Hessian matrix is to the identity (and hence how close the gradient is to being the same as a 2nd-order update) than what the diagonal entries look like.  The diagonal entries of a highly non-diagonal matrix aren't particularly meaningful to look at.\r\n\r\nAlso, since your analysis was done using the Fisher, why not examine this matrix instead of the Hessian in your experiments?"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Pushing Stochastic Gradient towards Second-Order Methods -- Backpropagation Learning with Transformations in Nonlinearities", "decision": "conferencePoster-iclr2013-workshop", "abstract": "Recently, we proposed to transform the outputs of each hidden neuron in a multi-layer perceptron network to have zero output and zero slope on average, and use separate shortcut connections to model the linear dependencies instead. We continue the work by firstly introducing a third transformation to normalize the scale of the outputs of each hidden neuron, and secondly by analyzing the connections to second order optimization methods. We show that the transformations make a simple stochastic gradient behave closer to second-order optimization methods and thus speed up learning. This is shown both in theory and with experiments. The experiments on the third transformation show that while it further increases the speed of learning, it can also hurt performance by converging to a worse local optimum, where both the inputs and outputs of many hidden neurons are close to zero.", "pdf": "https://arxiv.org/abs/1301.3476", "paperhash": "vatanen|pushing_stochastic_gradient_towards_secondorder_methods_backpropagation_learning_with_transformations_in_nonlinearities", "keywords": [], "conflicts": [], "authors": ["Tommi Vatanen", "Tapani Raiko", "Harri Valpola", "Yann LeCun"], "authorids": ["tommivat@gmail.com", "tapani.raiko@gmail.com", "harri.valpola@aalto.fi", "ylecun@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362387060000, "tcdate": 1362387060000, "number": 3, "id": "Id_EI3kn5mX4i", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "OOuGtqpeK-cLI", "replyto": "OOuGtqpeK-cLI", "signatures": ["anonymous reviewer c3d4"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Pushing Stochastic Gradient towards Second-Order Methods -- Backpropagation Learning with Transformations in Nonlinearities", "review": "* A brief summary of the paper's contributions, in the context of prior work.\r\n\r\nThis paper extends the authors' previous work on making sure that the hidden units in a neural net have zero output and slope on average, by also using direct connections that model explicitly the linear dependencies. The extension introduces another transformation which changes the scale of the outputs of the hidden units: essentially, they try to normalize both the scale and the slope of the outputs to one. This is done (essentially) by introducing a regularization parameter that encourages the geometric mean of the scale and the slope to be one.\r\n\r\nThe paper's contributions are also to give a theoretical analysis of the effect of the proposed transformations. The already proposed tricks are shown to make the non-diagonal elements of the Fisher information matrix closer to zero. The new transformation makes the diagonal elements closer to each other in scale, which is interesting as it's similar to what natural gradient does.\r\n\r\nThe authors also provide an empirical analysis of how the proposed method is close to what a second-order method would do (albeit on a small neural net). The experiment with the angle between the gradient and the second-order update is quite nice (I think such an experiment should be part of any paper that proposes new optimization tricks for training neural nets).\r\n\r\n* An assessment of novelty and quality.\r\n\r\nGenerally, this is a well-written and clear paper that extends naturally the authors' previous work. I think that the analysis is interesting and quite readable. I don't think that these particular transformations have been considered before in the literature and I like that they are not simply fixed transformations of the data, but something which integrates naturally into the learning algorithm.\r\n\r\n* A list of pros and cons (reasons to accept/reject).\r\n\r\nThe proposed scaling transformation makes sense in theory, but I'm not sure I agree with the authors' statement (end of Section 5) that the method's complexity is 'minimal regularization' compared to dropouts (maybe in theory, but honestly implementing dropout in a neural net learning system is considerably easier). The paper also doesn't show significant improvements (beyond analytical ones) over the previous transformations; based on the empirical results only I wouldn't necessarily use the scaling transformation."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Pushing Stochastic Gradient towards Second-Order Methods -- Backpropagation Learning with Transformations in Nonlinearities", "decision": "conferencePoster-iclr2013-workshop", "abstract": "Recently, we proposed to transform the outputs of each hidden neuron in a multi-layer perceptron network to have zero output and zero slope on average, and use separate shortcut connections to model the linear dependencies instead. We continue the work by firstly introducing a third transformation to normalize the scale of the outputs of each hidden neuron, and secondly by analyzing the connections to second order optimization methods. We show that the transformations make a simple stochastic gradient behave closer to second-order optimization methods and thus speed up learning. This is shown both in theory and with experiments. The experiments on the third transformation show that while it further increases the speed of learning, it can also hurt performance by converging to a worse local optimum, where both the inputs and outputs of many hidden neurons are close to zero.", "pdf": "https://arxiv.org/abs/1301.3476", "paperhash": "vatanen|pushing_stochastic_gradient_towards_secondorder_methods_backpropagation_learning_with_transformations_in_nonlinearities", "keywords": [], "conflicts": [], "authors": ["Tommi Vatanen", "Tapani Raiko", "Harri Valpola", "Yann LeCun"], "authorids": ["tommivat@gmail.com", "tapani.raiko@gmail.com", "harri.valpola@aalto.fi", "ylecun@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362183240000, "tcdate": 1362183240000, "number": 2, "id": "cAqVvWr0KLv0U", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "OOuGtqpeK-cLI", "replyto": "OOuGtqpeK-cLI", "signatures": ["anonymous reviewer 1567"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Pushing Stochastic Gradient towards Second-Order Methods -- Backpropagation Learning with Transformations in Nonlinearities", "review": "In [10], the authors had previously proposed modifying the network\r\nparametrization, in order to ensure zero-mean hidden unit activations across training examples (activity centering) and zero-mean derivatives (slope centering). This was achieved by introducing skip-connections between layers l-1 and l+1 and adding linear components to the non-linearity of layer l: these new parameters aren't learnt however, but instead are adjusted deterministically to enforce activity and slope centering.  These ideas had initially been proposed by Schraudolph in earlier work, with [10] showing that these tricks significantly improved convergence of deep networks while also making the connection to second order methods.\r\n\r\nIn this work, the authors proposed adding an extra scaling parameter to the  non-linearity, which is adjusted in order to make the digonal terms of the  Hessian / Fisher Information matrix closer to unity. The authors study the  effect of these 3 transformations by: \r\n(1) measuring properties of the Hessian matrix with and without transformations, as well as angular distance of the resulting gradients to 2nd order gradients;\r\n(2) comparing the overall classification convergence speed for a 2 and 3 layer MLPs on MNIST and finally;\r\n(3) studying its effect on a deep auto-encoder.\r\n\r\nWhile I find this research direction particularly interesting, I find the \r\noverlap between this paper and [10] to be rather troubling. While their analysis of slope / activity centering is new (and a more direct test of their \r\nhypothesis), I feel that the case for these transformations had already been\r\nmade in [10]. More importantly, evidence for the 3rd transformation is rather weak: it seems to slightly help convergence of 3-layer models and also helps in making the diagonal elements of the Hessian more unimodal. However, including  gamma seem to rotate gradients *away* from 2nd order gradients. Also, their method did not seem to help in the deep auto-encoder setting: using gamma in the encoder network did not improve convergence speed, while using gamma in both encoders/decoders led to gamma either blowing-up or going to zero. While you would expect a diagonal approximation to a second-order method to help with the problem of dead-units, adding gamma did not seem to help in this respect. \r\n\r\nSimilarities between this paper and [10] are also evident in the writing itself. Large portions of Sections 1, 2 and 3 appear verbatim in [10]. This needs to be addressed prior to publication. The math of Section 3 could also be simplified by writing out gradients of log p (for each parameter \theta) and then simply stating the general form of the FIM as E_eps[ dlogp/dtheta^T dlogp / dtheta]. As it stands Eqs. (12-17) are slightly inaccurate, as elements of the FIM should include an expectation over epsilon.\r\n\r\nSummary: I find the direction promising but the conclusion to be somewhat confusing / disappointing. The premise for gamma seemed well motivated and I expected more concrete evidence explaining the need for this transformation. Unfortunately, I am left wondering where things went wrong: some missing theoretical insight, wrong update rule on gamma or other ?\r\n\r\nOther:\r\n* Authors should consider using df/dx instead of the more ambiguous f' notation.\r\n* Could the authors clarify what they mean by: 'transforming the model instead of the gradient makes it easier to generalize to other contexts such as variational Bayes ?'  One downside I see to transforming the model instead of the gradients is that it obfuscates the link to second order methods and might thus hide useful insights.\r\n* Section 4: 'algorith' -> algorithm"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Pushing Stochastic Gradient towards Second-Order Methods -- Backpropagation Learning with Transformations in Nonlinearities", "decision": "conferencePoster-iclr2013-workshop", "abstract": "Recently, we proposed to transform the outputs of each hidden neuron in a multi-layer perceptron network to have zero output and zero slope on average, and use separate shortcut connections to model the linear dependencies instead. We continue the work by firstly introducing a third transformation to normalize the scale of the outputs of each hidden neuron, and secondly by analyzing the connections to second order optimization methods. We show that the transformations make a simple stochastic gradient behave closer to second-order optimization methods and thus speed up learning. This is shown both in theory and with experiments. The experiments on the third transformation show that while it further increases the speed of learning, it can also hurt performance by converging to a worse local optimum, where both the inputs and outputs of many hidden neurons are close to zero.", "pdf": "https://arxiv.org/abs/1301.3476", "paperhash": "vatanen|pushing_stochastic_gradient_towards_secondorder_methods_backpropagation_learning_with_transformations_in_nonlinearities", "keywords": [], "conflicts": [], "authors": ["Tommi Vatanen", "Tapani Raiko", "Harri Valpola", "Yann LeCun"], "authorids": ["tommivat@gmail.com", "tapani.raiko@gmail.com", "harri.valpola@aalto.fi", "ylecun@gmail.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1358347500000, "tcdate": 1358347500000, "number": 35, "id": "OOuGtqpeK-cLI", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "OOuGtqpeK-cLI", "signatures": ["tommivat@gmail.com"], "readers": ["everyone"], "content": {"title": "Pushing Stochastic Gradient towards Second-Order Methods -- Backpropagation Learning with Transformations in Nonlinearities", "decision": "conferencePoster-iclr2013-workshop", "abstract": "Recently, we proposed to transform the outputs of each hidden neuron in a multi-layer perceptron network to have zero output and zero slope on average, and use separate shortcut connections to model the linear dependencies instead. We continue the work by firstly introducing a third transformation to normalize the scale of the outputs of each hidden neuron, and secondly by analyzing the connections to second order optimization methods. We show that the transformations make a simple stochastic gradient behave closer to second-order optimization methods and thus speed up learning. This is shown both in theory and with experiments. The experiments on the third transformation show that while it further increases the speed of learning, it can also hurt performance by converging to a worse local optimum, where both the inputs and outputs of many hidden neurons are close to zero.", "pdf": "https://arxiv.org/abs/1301.3476", "paperhash": "vatanen|pushing_stochastic_gradient_towards_secondorder_methods_backpropagation_learning_with_transformations_in_nonlinearities", "keywords": [], "conflicts": [], "authors": ["Tommi Vatanen", "Tapani Raiko", "Harri Valpola", "Yann LeCun"], "authorids": ["tommivat@gmail.com", "tapani.raiko@gmail.com", "harri.valpola@aalto.fi", "ylecun@gmail.com"]}, "writers": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 5}