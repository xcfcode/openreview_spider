{"notes": [{"id": "3UDSdyIcBDA", "original": "-LQ3_wx-bE5", "number": 2889, "cdate": 1601308320419, "ddate": null, "tcdate": 1601308320419, "tmdate": 1616061673721, "tddate": null, "forum": "3UDSdyIcBDA", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "RMSprop converges with proper hyper-parameter", "authorids": ["~Naichen_Shi1", "~Dawei_Li3", "~Mingyi_Hong1", "~Ruoyu_Sun1"], "authors": ["Naichen Shi", "Dawei Li", "Mingyi Hong", "Ruoyu Sun"], "keywords": ["RMSprop", "convergence", "hyperparameter"], "abstract": "Despite the existence of divergence examples, RMSprop remains \none of the most popular algorithms in machine learning. Towards closing the gap between theory and practice, we prove that RMSprop converges with proper choice of hyper-parameters under certain conditions. More specifically, we prove that when the hyper-parameter $\\beta_2$ is close enough to $1$, RMSprop and its random shuffling version converge to a bounded region in general, and to critical points in the interpolation regime. It is worth mentioning that our results do not depend on  ``bounded gradient\"  assumption, which is often the key assumption utilized by existing theoretical work for Adam-type adaptive gradient method. Removing this assumption allows us to establish a phase transition from divergence to non-divergence for RMSprop. \n\nFinally, based on our theory, we conjecture that in practice there is a critical threshold $\\sf{\\beta_2^*}$, such that RMSprop generates reasonably good results only if $1>\\beta_2\\ge \\sf{\\beta_2^*}$. We provide empirical evidence for such a phase transition in our numerical experiments.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|rmsprop_converges_with_proper_hyperparameter", "supplementary_material": "/attachment/faa87d1692c51a62939e2fce04044f31625ee90f.zip", "pdf": "/pdf/551ff96cadef8a1f8bf77f1227bff8c8d9987cca.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshi2021rmsprop,\ntitle={{\\{}RMS{\\}}prop converges with proper hyper-parameter},\nauthor={Naichen Shi and Dawei Li and Mingyi Hong and Ruoyu Sun},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3UDSdyIcBDA}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "lk6aGdq_Yx", "original": null, "number": 2, "cdate": 1611063544306, "ddate": null, "tcdate": 1611063544306, "tmdate": 1611662579353, "tddate": null, "forum": "3UDSdyIcBDA", "replyto": "oj3cI3OT7GJ", "invitation": "ICLR.cc/2021/Conference/Paper2889/-/Comment", "content": {"title": "Thanks for your comment", "comment": "Hi Juntang:\n\nThank you for your comments! Below are some preliminary responses:\n\n(1) Thanks for pointing that out. We modified the code accordingly in the repository and replotted figure 1 (which will be updated soon). The actual range of convergence is indeed smaller than the what was shown in the initial version of figure 1. But still, for every C, there is a range of $\\beta_2$ that makes RMSprop converge. Specifically, for the case you mentioned, C=30 and $\\beta_2=0.999$, if you run RMSprop for 200k iterations using diminishing stepsize $\\frac{1}{\\sqrt{t}}$, you will find that $|x-x^\\star|$ is converging (in the last few iterations, this gap is below 0.01 and decreasing).  Nevertheless, if only run 20k iterations, then the error is still quite large, so it looks like it is not converging. \n\n(2) It is FigA.4 that corresponds to the toy-example in A.4; thanks for pointing out the typo. We will correct it in the revised version.\n\n  For the concern whether the experiments are enough to show RMSprop does not converge and SGD converges: thank you for pointing this out. First, SGD provably converge with diminishing stepsize. Nevertheless, since \"convergence\" does not mean fast convergence, it may take many iterations for $|x-x^\\star|$ to reach small error, which is the case for our example. Second, RMSprop also moves slowly for our example, and we suspect it may take more than $10^{12}$ iterations for RMSprop to achieve error larger than SGD. We did not run that many iterations but speculate according to the trend based on the results of $10^7$ iterations. The slow training of the two methods is the reason why SGD does not achieve smaller error than RMSprop. \n\n  Previously, we thought the trend of the two methods (RMSProp goes up, and SGD goes down) is enough. After your comment, we realized that we could have made the experiments more convincing. \n\n  The slow training is largely because we chose $a =10000$. There are two ways to alleviate the concern: i) run $10^{12}$ iterations for the original example; ii) still run $10^7$ iterations, but for a simpler problem. We choose the second way: we modify the problem from $a = 10000$ to $a=100$. Then the experiments show what we expect: SGD achieves error smaller than RMSProp after $4\\times 10^6$ iterations and is further going down, while RMSProp error is still going up after $10^7$ iterations.  The related code is in our repository now; you can also simply replace $a=10^4$ by $a=10^2$. The modified experiments and changes will be updated into our revised version.\n\n  Again, thank you for your comments which help improve the paper. Feel free to let us know any questions."}, "signatures": ["ICLR.cc/2021/Conference/Paper2889/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2889/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RMSprop converges with proper hyper-parameter", "authorids": ["~Naichen_Shi1", "~Dawei_Li3", "~Mingyi_Hong1", "~Ruoyu_Sun1"], "authors": ["Naichen Shi", "Dawei Li", "Mingyi Hong", "Ruoyu Sun"], "keywords": ["RMSprop", "convergence", "hyperparameter"], "abstract": "Despite the existence of divergence examples, RMSprop remains \none of the most popular algorithms in machine learning. Towards closing the gap between theory and practice, we prove that RMSprop converges with proper choice of hyper-parameters under certain conditions. More specifically, we prove that when the hyper-parameter $\\beta_2$ is close enough to $1$, RMSprop and its random shuffling version converge to a bounded region in general, and to critical points in the interpolation regime. It is worth mentioning that our results do not depend on  ``bounded gradient\"  assumption, which is often the key assumption utilized by existing theoretical work for Adam-type adaptive gradient method. Removing this assumption allows us to establish a phase transition from divergence to non-divergence for RMSprop. \n\nFinally, based on our theory, we conjecture that in practice there is a critical threshold $\\sf{\\beta_2^*}$, such that RMSprop generates reasonably good results only if $1>\\beta_2\\ge \\sf{\\beta_2^*}$. We provide empirical evidence for such a phase transition in our numerical experiments.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|rmsprop_converges_with_proper_hyperparameter", "supplementary_material": "/attachment/faa87d1692c51a62939e2fce04044f31625ee90f.zip", "pdf": "/pdf/551ff96cadef8a1f8bf77f1227bff8c8d9987cca.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshi2021rmsprop,\ntitle={{\\{}RMS{\\}}prop converges with proper hyper-parameter},\nauthor={Naichen Shi and Dawei Li and Mingyi Hong and Ruoyu Sun},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3UDSdyIcBDA}\n}"}, "tags": [], "invitation": {"reply": {"forum": "3UDSdyIcBDA", "readers": {"values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2021/Conference/Paper2889/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2889/Authors|ICLR.cc/2021/Conference/Paper2889/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs"}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}}, "multiReply": true, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["everyone"], "tcdate": 1610649441630, "tmdate": 1610649441630, "id": "ICLR.cc/2021/Conference/Paper2889/-/Comment"}}}, {"id": "nSCQ01cxbc", "original": null, "number": 3, "cdate": 1611520710917, "ddate": null, "tcdate": 1611520710917, "tmdate": 1611520710917, "tddate": null, "forum": "3UDSdyIcBDA", "replyto": "lk6aGdq_Yx", "invitation": "ICLR.cc/2021/Conference/Paper2889/-/Comment", "content": {"title": "Thanks for clarification", "comment": "Thanks for clarification. Looking forward to some more discussion regarding the second question."}, "signatures": ["~Juntang_Zhuang1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Juntang_Zhuang1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RMSprop converges with proper hyper-parameter", "authorids": ["~Naichen_Shi1", "~Dawei_Li3", "~Mingyi_Hong1", "~Ruoyu_Sun1"], "authors": ["Naichen Shi", "Dawei Li", "Mingyi Hong", "Ruoyu Sun"], "keywords": ["RMSprop", "convergence", "hyperparameter"], "abstract": "Despite the existence of divergence examples, RMSprop remains \none of the most popular algorithms in machine learning. Towards closing the gap between theory and practice, we prove that RMSprop converges with proper choice of hyper-parameters under certain conditions. More specifically, we prove that when the hyper-parameter $\\beta_2$ is close enough to $1$, RMSprop and its random shuffling version converge to a bounded region in general, and to critical points in the interpolation regime. It is worth mentioning that our results do not depend on  ``bounded gradient\"  assumption, which is often the key assumption utilized by existing theoretical work for Adam-type adaptive gradient method. Removing this assumption allows us to establish a phase transition from divergence to non-divergence for RMSprop. \n\nFinally, based on our theory, we conjecture that in practice there is a critical threshold $\\sf{\\beta_2^*}$, such that RMSprop generates reasonably good results only if $1>\\beta_2\\ge \\sf{\\beta_2^*}$. We provide empirical evidence for such a phase transition in our numerical experiments.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|rmsprop_converges_with_proper_hyperparameter", "supplementary_material": "/attachment/faa87d1692c51a62939e2fce04044f31625ee90f.zip", "pdf": "/pdf/551ff96cadef8a1f8bf77f1227bff8c8d9987cca.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshi2021rmsprop,\ntitle={{\\{}RMS{\\}}prop converges with proper hyper-parameter},\nauthor={Naichen Shi and Dawei Li and Mingyi Hong and Ruoyu Sun},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3UDSdyIcBDA}\n}"}, "tags": [], "invitation": {"reply": {"forum": "3UDSdyIcBDA", "readers": {"values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2021/Conference/Paper2889/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2889/Authors|ICLR.cc/2021/Conference/Paper2889/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs"}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}}, "multiReply": true, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["everyone"], "tcdate": 1610649441630, "tmdate": 1610649441630, "id": "ICLR.cc/2021/Conference/Paper2889/-/Comment"}}}, {"id": "oj3cI3OT7GJ", "original": null, "number": 1, "cdate": 1610817680808, "ddate": null, "tcdate": 1610817680808, "tmdate": 1610819412149, "tddate": null, "forum": "3UDSdyIcBDA", "replyto": "3UDSdyIcBDA", "invitation": "ICLR.cc/2021/Conference/Paper2889/-/Comment", "content": {"title": "Questions regarding toy examples", "comment": "Thanks for the nice paper. Very good theoretical analysis, however I'm somehow confused by the toy examples.\n\n(1) Figure 1 corresponds to the example $f_(x) = Cx\\  \\textit{if t mod C}==1$, but when I check the code, you are actually using $t mod C <=1$ in the reddieexample.m function, and for each period of C the function observes two gradients of C rather than 1. The actual region of convergence is much smaller than it shows in Figure 1. Please clarify if I misunderstand it.\n\nFurthermore, when I change the code to t mode C ==1, and I found for a large region, say C >=15, I found it not converge even beta2 is set as 1-10^{-4}. Is this a numerical issue, or does this imply that there exists some C, such that no matter how close beta2 to 1, RMSProp just won't converge?\n\n(2) For Fig.A4 in appendix A.4, should it correspond to the non-realizable toy-example in A.4? In text it's said \"FigA5\". Furthermore, I'm confused by the text that \"SGD converges, AMSgrad converges, RMSprop fail to converge\". If you look at the y-axis, which is log(1-x^*), SGD result is around 0.5, which is much worse than RMSProp (-0.8) and AMSGrad (-1.5), though there's a trend that RMSProp increases the distance, SGD and AMSGrad decreases the distance, but can we safely draw the conclusion that \"RMSProp fail to converge, SGD converges\" here?\n\nThanks a lot in advance"}, "signatures": ["~Juntang_Zhuang1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Juntang_Zhuang1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RMSprop converges with proper hyper-parameter", "authorids": ["~Naichen_Shi1", "~Dawei_Li3", "~Mingyi_Hong1", "~Ruoyu_Sun1"], "authors": ["Naichen Shi", "Dawei Li", "Mingyi Hong", "Ruoyu Sun"], "keywords": ["RMSprop", "convergence", "hyperparameter"], "abstract": "Despite the existence of divergence examples, RMSprop remains \none of the most popular algorithms in machine learning. Towards closing the gap between theory and practice, we prove that RMSprop converges with proper choice of hyper-parameters under certain conditions. More specifically, we prove that when the hyper-parameter $\\beta_2$ is close enough to $1$, RMSprop and its random shuffling version converge to a bounded region in general, and to critical points in the interpolation regime. It is worth mentioning that our results do not depend on  ``bounded gradient\"  assumption, which is often the key assumption utilized by existing theoretical work for Adam-type adaptive gradient method. Removing this assumption allows us to establish a phase transition from divergence to non-divergence for RMSprop. \n\nFinally, based on our theory, we conjecture that in practice there is a critical threshold $\\sf{\\beta_2^*}$, such that RMSprop generates reasonably good results only if $1>\\beta_2\\ge \\sf{\\beta_2^*}$. We provide empirical evidence for such a phase transition in our numerical experiments.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|rmsprop_converges_with_proper_hyperparameter", "supplementary_material": "/attachment/faa87d1692c51a62939e2fce04044f31625ee90f.zip", "pdf": "/pdf/551ff96cadef8a1f8bf77f1227bff8c8d9987cca.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshi2021rmsprop,\ntitle={{\\{}RMS{\\}}prop converges with proper hyper-parameter},\nauthor={Naichen Shi and Dawei Li and Mingyi Hong and Ruoyu Sun},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3UDSdyIcBDA}\n}"}, "tags": [], "invitation": {"reply": {"forum": "3UDSdyIcBDA", "readers": {"values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2021/Conference/Paper2889/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2889/Authors|ICLR.cc/2021/Conference/Paper2889/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs"}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}}, "multiReply": true, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["everyone"], "tcdate": 1610649441630, "tmdate": 1610649441630, "id": "ICLR.cc/2021/Conference/Paper2889/-/Comment"}}}, {"id": "x57idlx9YKX", "original": null, "number": 1, "cdate": 1610040350330, "ddate": null, "tcdate": 1610040350330, "tmdate": 1610473939313, "tddate": null, "forum": "3UDSdyIcBDA", "replyto": "3UDSdyIcBDA", "invitation": "ICLR.cc/2021/Conference/Paper2889/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Spotlight)", "comment": "The paper shows convergence results for RMSprop in certain regimes. The reviews are uniformly positive about this paper and I recommend acceptance."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RMSprop converges with proper hyper-parameter", "authorids": ["~Naichen_Shi1", "~Dawei_Li3", "~Mingyi_Hong1", "~Ruoyu_Sun1"], "authors": ["Naichen Shi", "Dawei Li", "Mingyi Hong", "Ruoyu Sun"], "keywords": ["RMSprop", "convergence", "hyperparameter"], "abstract": "Despite the existence of divergence examples, RMSprop remains \none of the most popular algorithms in machine learning. Towards closing the gap between theory and practice, we prove that RMSprop converges with proper choice of hyper-parameters under certain conditions. More specifically, we prove that when the hyper-parameter $\\beta_2$ is close enough to $1$, RMSprop and its random shuffling version converge to a bounded region in general, and to critical points in the interpolation regime. It is worth mentioning that our results do not depend on  ``bounded gradient\"  assumption, which is often the key assumption utilized by existing theoretical work for Adam-type adaptive gradient method. Removing this assumption allows us to establish a phase transition from divergence to non-divergence for RMSprop. \n\nFinally, based on our theory, we conjecture that in practice there is a critical threshold $\\sf{\\beta_2^*}$, such that RMSprop generates reasonably good results only if $1>\\beta_2\\ge \\sf{\\beta_2^*}$. We provide empirical evidence for such a phase transition in our numerical experiments.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|rmsprop_converges_with_proper_hyperparameter", "supplementary_material": "/attachment/faa87d1692c51a62939e2fce04044f31625ee90f.zip", "pdf": "/pdf/551ff96cadef8a1f8bf77f1227bff8c8d9987cca.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshi2021rmsprop,\ntitle={{\\{}RMS{\\}}prop converges with proper hyper-parameter},\nauthor={Naichen Shi and Dawei Li and Mingyi Hong and Ruoyu Sun},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3UDSdyIcBDA}\n}"}, "tags": [], "invitation": {"reply": {"forum": "3UDSdyIcBDA", "replyto": "3UDSdyIcBDA", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040350312, "tmdate": 1610473939294, "id": "ICLR.cc/2021/Conference/Paper2889/-/Decision"}}}, {"id": "N3S5c7KUmA", "original": null, "number": 3, "cdate": 1606277539709, "ddate": null, "tcdate": 1606277539709, "tmdate": 1606307506705, "tddate": null, "forum": "3UDSdyIcBDA", "replyto": "dpuRDjWPy3D", "invitation": "ICLR.cc/2021/Conference/Paper2889/-/Official_Comment", "content": {"title": "Thank you for the detailed and constructive comments!", "comment": "Thank you for the detailed and constructive comments! Below we provide the responses to specific comments. \n\n1. RMSProp and SGD\n\nThanks for the suggestion. Our convergence rate in high $\\beta_2$ regime is $O\\left(\\frac{\\log T}{\\sqrt{T}}\\right)$, and under the nonconvex setting, the best proved rate of SGD is also $O\\left(\\frac{\\log T}{\\sqrt{T}}\\right)$ (Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming). Thus, the two rates are theoretically comparable.    \n\nWe agree that proving \u201cadaptive gradient methods have improved convergence properties compared to SGD\u201d is a very interesting problem.  In our paper and other previous works such as Mnih et al., 2016, Seward et al., 2020; Yaz\u0131c\u0131 et al., 2019,  the advantage of RMSprop over SGD is mainly demonstrated through experiments. The intuition of the advantage of adaptive gradient methods is that by using adaptive learning rate, we can better utilize the local structure of the problem and thus achieve faster convergence, which is similar to the idea of Quasi-Newton method. Nevertheless, even for convex problems, the theoretical advantage of the Quasi-Newton method was not well understood (a possible benefit is asymptotic superlinear convergence, but such a benefit is not easy to prove for stochastic versions). Thus it seems that providing a strong theoretical justification for the advantage of Adam-type methods is challenging and requires much more work. We hope to explore this problem in the future.\n\n2. Exigesis on condition (4):\n\nThanks for the comment. The estimate of $\\beta_2$ by condition (4) in theorem 4.3 is $1-\\beta_2\\le O\\left(\\frac{1}{n^{3.5}}\\right)$. We did not try to optimize this bound before as we focus on proving the existence of such a threshold. We agree that explaining the bound will help readers understand RMSProp better, so we add some discussions on $\\beta_2$ below and in the revised version. The bound indicates that the transition point is roughly $\\beta_2 = 1 - 1/poly(n)$.  In our experiments on Resnet18 with batchsize 8 and n $\\approx$ 5000, the transition point of $\\beta_2$ is between $0.95$ and $0.99$; for n $\\approx 2500$ (batch size 16), the transition point lies in [0.9,0.95];\nfor $n\\approx 1200$, the transition point lies in [0.8,0.9]. This indicates that the transition point is decreasing over $n$, which qualitatively matches our bound of $\\beta_2$. In this experiment, the practical size of the $\\beta_2$-threshold \nseems to be somewhere between $1 - O(1/\\sqrt{n})$ and $1 - O(1/n)$. Note that we do not have extensively studied the order of the transition point, so this is a preliminary empirical estimate. If the empirical estimate is reasonable, then there is a gap between our theoretical bound and this empirical estimate.\n\nIn an effort to reduce the gap, we revisited our proof, and find that our proof only requires $1-\\beta_2\\le O\\left(\\frac{1}{n\\rho_1\\rho_2\\rho_3}\\right)$, where $\\rho_1$, $\\rho_2$, and $\\rho_3$ are three quantities that characterize the distribution of the gradients; see equations (14)-(16), equation (37) and the remark after it in the appendix of the revised version. The upper-bounds of $\\rho_1$, $\\rho_2$, and $\\rho_3$ are $\\sqrt{n}$, $n$, and $n$ respectively, thus in the worst case the bound is $1-\\beta_2\\le O\\left(\\frac{1}{n^{3.5}}\\right)$. This is why we obtain this bound in the original condition (4). The benefit of introducing the quantities $\\rho_1$, $\\rho_2$,  and $\\rho_3$ is that the threshold of $\\beta_2$ may be smaller than the worst-case estimate if $\\rho_1$, $\\rho_2$,  and $\\rho_3$ are small in practice. For instance, if $\\rho_i = O(1)$, then  $1 - \\beta_2 \\approx O(1/n). $ We checked the size of $\\rho_i$'s in a MNIST experiment, and find that $\\rho_1 \\rho_2 \\rho_3$ roughly lies in $[ 1, n ]$, in which case the bound of $\\beta_2$ is somewhere between $1- O\\left(\\frac{1}{n^{1}}\\right)$ and $1- O\\left(\\frac{1}{n^{2}}\\right)$. This is closer to the emprical transition point we observe. Anyhow, it remains a question to provide a more precise estimate of $\\beta_2$. \n\nWe add a remark (Remark 2 after Theorem 4.3) in the revised version to explain the size of $\\beta_2$ and the stronger bound based on $\\rho_i$'s; we also provide a preliminary empirical estimate of $\\rho_i's$ in Appendix A.5."}, "signatures": ["ICLR.cc/2021/Conference/Paper2889/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2889/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RMSprop converges with proper hyper-parameter", "authorids": ["~Naichen_Shi1", "~Dawei_Li3", "~Mingyi_Hong1", "~Ruoyu_Sun1"], "authors": ["Naichen Shi", "Dawei Li", "Mingyi Hong", "Ruoyu Sun"], "keywords": ["RMSprop", "convergence", "hyperparameter"], "abstract": "Despite the existence of divergence examples, RMSprop remains \none of the most popular algorithms in machine learning. Towards closing the gap between theory and practice, we prove that RMSprop converges with proper choice of hyper-parameters under certain conditions. More specifically, we prove that when the hyper-parameter $\\beta_2$ is close enough to $1$, RMSprop and its random shuffling version converge to a bounded region in general, and to critical points in the interpolation regime. It is worth mentioning that our results do not depend on  ``bounded gradient\"  assumption, which is often the key assumption utilized by existing theoretical work for Adam-type adaptive gradient method. Removing this assumption allows us to establish a phase transition from divergence to non-divergence for RMSprop. \n\nFinally, based on our theory, we conjecture that in practice there is a critical threshold $\\sf{\\beta_2^*}$, such that RMSprop generates reasonably good results only if $1>\\beta_2\\ge \\sf{\\beta_2^*}$. We provide empirical evidence for such a phase transition in our numerical experiments.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|rmsprop_converges_with_proper_hyperparameter", "supplementary_material": "/attachment/faa87d1692c51a62939e2fce04044f31625ee90f.zip", "pdf": "/pdf/551ff96cadef8a1f8bf77f1227bff8c8d9987cca.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshi2021rmsprop,\ntitle={{\\{}RMS{\\}}prop converges with proper hyper-parameter},\nauthor={Naichen Shi and Dawei Li and Mingyi Hong and Ruoyu Sun},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3UDSdyIcBDA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3UDSdyIcBDA", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2889/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2889/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2889/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2889/Authors|ICLR.cc/2021/Conference/Paper2889/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2889/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843423, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2889/-/Official_Comment"}}}, {"id": "LK0CPBimjq", "original": null, "number": 4, "cdate": 1606277837285, "ddate": null, "tcdate": 1606277837285, "tmdate": 1606285279092, "tddate": null, "forum": "3UDSdyIcBDA", "replyto": "hkya5mHmDvE", "invitation": "ICLR.cc/2021/Conference/Paper2889/-/Official_Comment", "content": {"title": "Thank you for your comments!", "comment": "Thank you for your comments! Below we provide the responses to specific comments.\n\n1. Definition of $\\beta_2$:\n\nThat\u2019s a good suggestion! We now modified the structure as follows: we added an explanation that \u03b22 is from Algorithm 1 in the beginning of the introduction, as suggested by the reviewer.\n\n2. Formal definition of realizable and non-realizable problems \n\nThanks for the suggestion. We moved the formal definitions from the section on the main result to the end of the introduction section. As stated there, we say a problem is realizable if it satisfies condition (3) with $D_0=0$, and non-realizable if it satisfies condition (3) with $D_0>0$.\n\n3. Figure illustrating performance of algorithms during training\n\nThank you for the suggestion. We followed the suggestion and now included a figure in Appendix A.1; see Figure A1 in the modified version of the paper.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2889/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2889/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RMSprop converges with proper hyper-parameter", "authorids": ["~Naichen_Shi1", "~Dawei_Li3", "~Mingyi_Hong1", "~Ruoyu_Sun1"], "authors": ["Naichen Shi", "Dawei Li", "Mingyi Hong", "Ruoyu Sun"], "keywords": ["RMSprop", "convergence", "hyperparameter"], "abstract": "Despite the existence of divergence examples, RMSprop remains \none of the most popular algorithms in machine learning. Towards closing the gap between theory and practice, we prove that RMSprop converges with proper choice of hyper-parameters under certain conditions. More specifically, we prove that when the hyper-parameter $\\beta_2$ is close enough to $1$, RMSprop and its random shuffling version converge to a bounded region in general, and to critical points in the interpolation regime. It is worth mentioning that our results do not depend on  ``bounded gradient\"  assumption, which is often the key assumption utilized by existing theoretical work for Adam-type adaptive gradient method. Removing this assumption allows us to establish a phase transition from divergence to non-divergence for RMSprop. \n\nFinally, based on our theory, we conjecture that in practice there is a critical threshold $\\sf{\\beta_2^*}$, such that RMSprop generates reasonably good results only if $1>\\beta_2\\ge \\sf{\\beta_2^*}$. We provide empirical evidence for such a phase transition in our numerical experiments.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|rmsprop_converges_with_proper_hyperparameter", "supplementary_material": "/attachment/faa87d1692c51a62939e2fce04044f31625ee90f.zip", "pdf": "/pdf/551ff96cadef8a1f8bf77f1227bff8c8d9987cca.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshi2021rmsprop,\ntitle={{\\{}RMS{\\}}prop converges with proper hyper-parameter},\nauthor={Naichen Shi and Dawei Li and Mingyi Hong and Ruoyu Sun},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3UDSdyIcBDA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3UDSdyIcBDA", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2889/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2889/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2889/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2889/Authors|ICLR.cc/2021/Conference/Paper2889/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2889/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843423, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2889/-/Official_Comment"}}}, {"id": "yg0Z0BPJhnE", "original": null, "number": 2, "cdate": 1606276850929, "ddate": null, "tcdate": 1606276850929, "tmdate": 1606284897131, "tddate": null, "forum": "3UDSdyIcBDA", "replyto": "cjfcGuf98pp", "invitation": "ICLR.cc/2021/Conference/Paper2889/-/Official_Comment", "content": {"title": "Thank you for your supportive and encouraging comments.", "comment": "Thank you for your supportive and encouraging comments! Below we provide the responses to specific suggestions. \n\n1. The work Defossez et al. 2019 On the convergence of Adam and Adagrad\uff1a\n\nThank you for the suggestion. That is a representative paper and we now added the discussion of it in the related work section: \u201cThe work (Defossez et al. 2019) establishes a clean convergence result and also provides some insights on the momentum mechanisms by improving the dependence of $1-\\beta_1$ in the convergence rate. However the work still assumes bounded gradient and non-zero $\\epsilon$.\u201d\n\n2. x-label of figure 1\uff1a\n\nThanks a lot for your suggestion! We have replotted Figure 1 according to your suggestion.\n\n3. Typo\uff1a\n\nThanks for pointing it out. We now changed it to \u201ccases of non-divergence\u201d.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2889/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2889/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RMSprop converges with proper hyper-parameter", "authorids": ["~Naichen_Shi1", "~Dawei_Li3", "~Mingyi_Hong1", "~Ruoyu_Sun1"], "authors": ["Naichen Shi", "Dawei Li", "Mingyi Hong", "Ruoyu Sun"], "keywords": ["RMSprop", "convergence", "hyperparameter"], "abstract": "Despite the existence of divergence examples, RMSprop remains \none of the most popular algorithms in machine learning. Towards closing the gap between theory and practice, we prove that RMSprop converges with proper choice of hyper-parameters under certain conditions. More specifically, we prove that when the hyper-parameter $\\beta_2$ is close enough to $1$, RMSprop and its random shuffling version converge to a bounded region in general, and to critical points in the interpolation regime. It is worth mentioning that our results do not depend on  ``bounded gradient\"  assumption, which is often the key assumption utilized by existing theoretical work for Adam-type adaptive gradient method. Removing this assumption allows us to establish a phase transition from divergence to non-divergence for RMSprop. \n\nFinally, based on our theory, we conjecture that in practice there is a critical threshold $\\sf{\\beta_2^*}$, such that RMSprop generates reasonably good results only if $1>\\beta_2\\ge \\sf{\\beta_2^*}$. We provide empirical evidence for such a phase transition in our numerical experiments.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|rmsprop_converges_with_proper_hyperparameter", "supplementary_material": "/attachment/faa87d1692c51a62939e2fce04044f31625ee90f.zip", "pdf": "/pdf/551ff96cadef8a1f8bf77f1227bff8c8d9987cca.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshi2021rmsprop,\ntitle={{\\{}RMS{\\}}prop converges with proper hyper-parameter},\nauthor={Naichen Shi and Dawei Li and Mingyi Hong and Ruoyu Sun},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3UDSdyIcBDA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3UDSdyIcBDA", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2889/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2889/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2889/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2889/Authors|ICLR.cc/2021/Conference/Paper2889/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2889/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843423, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2889/-/Official_Comment"}}}, {"id": "hkya5mHmDvE", "original": null, "number": 1, "cdate": 1603734607996, "ddate": null, "tcdate": 1603734607996, "tmdate": 1605024109887, "tddate": null, "forum": "3UDSdyIcBDA", "replyto": "3UDSdyIcBDA", "invitation": "ICLR.cc/2021/Conference/Paper2889/-/Official_Review", "content": {"title": "The paper studies one of the most popular algorithms in machine learning: RMSprop. The proposed problem is practical. Overall, I vote for accepting the paper. ", "review": "Summary:\n\nThe paper studies one of the most popular algorithms in machine learning: RMSprop. More specifically, it investigates the relation between the hyper-parameters and the convergence of the algorithm. By proving the convergence without using \"bounded gradient\" assumption, the authors establish a phase transition from divergence to non-divergence for RMSProp.\n\nPros:\n\n1. The paper concerns one of the most important algorithms in machine learning. In my opinion, the problem is practical and of interest in machine learning community.\n\n2. The results of the paper provide explicit conditions for the hyper-parameters of RMSprop/Adam that ensure the convergence of the algorithms. These results provide basic guidelines for tuning hyper-parameters of the algorithms in practice.\n\nCons:\n\nApart from the strong points, I still have some concerns about the clarity of the paper. I hope the authors can address my concerns to improve the quality of the paper.\n\n1. The parameter $\\beta_2$ is the most important subject of the paper. Until algorithm 1, the paper discusses $\\beta_2$ without defining it clearly. It would be more clear if $\\beta_2$ is mentioned from the beginning of the paper that it comes form algorithm 1.\n\n2. The authors divide the problems into 2 sub-classes to investigate: realizable and non-realizable, which are not clearly defined. It would be better if the authors can define these 2 sub-classes more formally.\n\n3. The experiments supporting the theoretical results are comprehensible. However, I would suggest the authors provide a figure with x-axis to be epochs and y-axis to be accuracy so that the readers can have better idea upon how SGD and RMSProp behave during training.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2889/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2889/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RMSprop converges with proper hyper-parameter", "authorids": ["~Naichen_Shi1", "~Dawei_Li3", "~Mingyi_Hong1", "~Ruoyu_Sun1"], "authors": ["Naichen Shi", "Dawei Li", "Mingyi Hong", "Ruoyu Sun"], "keywords": ["RMSprop", "convergence", "hyperparameter"], "abstract": "Despite the existence of divergence examples, RMSprop remains \none of the most popular algorithms in machine learning. Towards closing the gap between theory and practice, we prove that RMSprop converges with proper choice of hyper-parameters under certain conditions. More specifically, we prove that when the hyper-parameter $\\beta_2$ is close enough to $1$, RMSprop and its random shuffling version converge to a bounded region in general, and to critical points in the interpolation regime. It is worth mentioning that our results do not depend on  ``bounded gradient\"  assumption, which is often the key assumption utilized by existing theoretical work for Adam-type adaptive gradient method. Removing this assumption allows us to establish a phase transition from divergence to non-divergence for RMSprop. \n\nFinally, based on our theory, we conjecture that in practice there is a critical threshold $\\sf{\\beta_2^*}$, such that RMSprop generates reasonably good results only if $1>\\beta_2\\ge \\sf{\\beta_2^*}$. We provide empirical evidence for such a phase transition in our numerical experiments.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|rmsprop_converges_with_proper_hyperparameter", "supplementary_material": "/attachment/faa87d1692c51a62939e2fce04044f31625ee90f.zip", "pdf": "/pdf/551ff96cadef8a1f8bf77f1227bff8c8d9987cca.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshi2021rmsprop,\ntitle={{\\{}RMS{\\}}prop converges with proper hyper-parameter},\nauthor={Naichen Shi and Dawei Li and Mingyi Hong and Ruoyu Sun},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3UDSdyIcBDA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "3UDSdyIcBDA", "replyto": "3UDSdyIcBDA", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2889/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538086582, "tmdate": 1606915758452, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2889/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2889/-/Official_Review"}}}, {"id": "dpuRDjWPy3D", "original": null, "number": 2, "cdate": 1603882374588, "ddate": null, "tcdate": 1603882374588, "tmdate": 1605024109826, "tddate": null, "forum": "3UDSdyIcBDA", "replyto": "3UDSdyIcBDA", "invitation": "ICLR.cc/2021/Conference/Paper2889/-/Official_Review", "content": {"title": "Interesting analysis of the role of the beta2 parameter in RMSProp convergence behaviour", "review": "The paper starts off from the recent realization that there exists divergent examples for any set of hyperparameters for algorithms in the Adam family, such as RMSProp. It sets out to study the effect of the beta2 parameter on convergence for a fixed specific problem. The analysis shows that there exists a beta2 < 1 that leads to convergence for realizable problems, and to convergence to a bounded region of interest for non-realizable problems, without requiring a bounded-gradient assumption. Experiments confirm this new theory.\n\nOverall, the paper is well-written, clear and easy to read. One of its strongest points is how well the analysis and the relevance of the results is motived. For instance, the importance of removing the assumption on the bounds on the gradient because it effectively removes one of the convergence/divergence regimes is well executed.\nThere is also significant efforts on providing clear simplified examples from rather complex theorems, which is very appreciated (e.g. Corollary 4.1).\nFurther, there is a real effort to contrast the results with the previous work, and to explain how it complements them, resolving clearly what initially appears as direct contradictions.\n\nThe results are relevant, both from the point of view of the theory, where it adds to a body of work explaining how and why the Adam family of algorithm performs well on modern machine learning taskloads, and from the point of view of the practitioner, outlining what hyperparameter tuning is necessary to achieve convergence. They are also original, in the sense that they provide novel insights, while removing problematic assumptions that permeate most of the related work.\n\nA couple of things could be improved:\n- as pointed out in the paper, if beta2 = 1, the algorithm degenerates to SGD. While there is a remark explaining why as long as beta2 < 1 the two algorithms differ, it would be informative to compare the convergence regimes with high beta2 to SGD directly, to validate that there exists a set of hyperparameters that not only provide convergence, but improved convergence properties compared to SGD (otherwise the results are a lot less relevant), as well as give an order of magnitude of what value is typically necessary for beta2.\n- condition (4) in theorem 4.3 is quite difficult to apprehend, with a slightly worrying beta2^n term. More exegesis would be beneficial for reader comprehension.\n\nOverall, this is a nice, well-written and relevant paper that clears the bar for publication in its current version.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2889/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2889/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RMSprop converges with proper hyper-parameter", "authorids": ["~Naichen_Shi1", "~Dawei_Li3", "~Mingyi_Hong1", "~Ruoyu_Sun1"], "authors": ["Naichen Shi", "Dawei Li", "Mingyi Hong", "Ruoyu Sun"], "keywords": ["RMSprop", "convergence", "hyperparameter"], "abstract": "Despite the existence of divergence examples, RMSprop remains \none of the most popular algorithms in machine learning. Towards closing the gap between theory and practice, we prove that RMSprop converges with proper choice of hyper-parameters under certain conditions. More specifically, we prove that when the hyper-parameter $\\beta_2$ is close enough to $1$, RMSprop and its random shuffling version converge to a bounded region in general, and to critical points in the interpolation regime. It is worth mentioning that our results do not depend on  ``bounded gradient\"  assumption, which is often the key assumption utilized by existing theoretical work for Adam-type adaptive gradient method. Removing this assumption allows us to establish a phase transition from divergence to non-divergence for RMSprop. \n\nFinally, based on our theory, we conjecture that in practice there is a critical threshold $\\sf{\\beta_2^*}$, such that RMSprop generates reasonably good results only if $1>\\beta_2\\ge \\sf{\\beta_2^*}$. We provide empirical evidence for such a phase transition in our numerical experiments.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|rmsprop_converges_with_proper_hyperparameter", "supplementary_material": "/attachment/faa87d1692c51a62939e2fce04044f31625ee90f.zip", "pdf": "/pdf/551ff96cadef8a1f8bf77f1227bff8c8d9987cca.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshi2021rmsprop,\ntitle={{\\{}RMS{\\}}prop converges with proper hyper-parameter},\nauthor={Naichen Shi and Dawei Li and Mingyi Hong and Ruoyu Sun},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3UDSdyIcBDA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "3UDSdyIcBDA", "replyto": "3UDSdyIcBDA", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2889/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538086582, "tmdate": 1606915758452, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2889/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2889/-/Official_Review"}}}, {"id": "cjfcGuf98pp", "original": null, "number": 3, "cdate": 1603908950780, "ddate": null, "tcdate": 1603908950780, "tmdate": 1605024109751, "tddate": null, "forum": "3UDSdyIcBDA", "replyto": "3UDSdyIcBDA", "invitation": "ICLR.cc/2021/Conference/Paper2889/-/Official_Review", "content": {"title": "Very interesting work on classical adaptive methods", "review": "This work revisits a famous counterexample on the convergence of Adam (originally presented in Reddi 2018). The authors show that, if the EMA parameter beta2 in RMSprop and Adam is chosen high enough, then both methods converge to a bounded region in the stochastic setting. In addition, the authors provide some results for the full-batch case. Crucially, and differently from many other papers on the topic, the gradients are not assumed to be bounded and the beta2 hyperparameter is not chosen to increase to 1.\n\nThe paper is well written and the logic of it is convincing. I like the introduction and Figure 1 (this nicely illustrates the relevance of this paper). It is also very well organized. Unfortunately, I did not have the time I wish I had to dig into the proofs (just had a quick check), but the methodology of the authors and the results are convincing.\n\nThis is overall a very nice paper, with clean and easy to read results, that clarifies an important point: it is misleading to claim that \u201cAdam does not converge\u201d (which was pointed out in Reddi 2018 to introduce AMSgrad). I have heard this (wrong) claim many times in the optimization community \u2013 hence I think this paper deserves attention (therefore my clear accept). This work truly does merge the gap between theory and practice in non-convex stochastic optimization.\n\nJust a few suggestions: I think the authors should cite and discuss the results in Defossez et al. 2019 (On the convergence of Adam and Adagrad). Also, I think Figure 1 deserves better quality. It's done in matlab so in the xlabel command you can put 'interpreter','latex' and 'fontsize',20. Finally, I spotted 1 typo: in Remark2 \u201ccases of non-divergence cases\u201d.\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2889/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2889/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RMSprop converges with proper hyper-parameter", "authorids": ["~Naichen_Shi1", "~Dawei_Li3", "~Mingyi_Hong1", "~Ruoyu_Sun1"], "authors": ["Naichen Shi", "Dawei Li", "Mingyi Hong", "Ruoyu Sun"], "keywords": ["RMSprop", "convergence", "hyperparameter"], "abstract": "Despite the existence of divergence examples, RMSprop remains \none of the most popular algorithms in machine learning. Towards closing the gap between theory and practice, we prove that RMSprop converges with proper choice of hyper-parameters under certain conditions. More specifically, we prove that when the hyper-parameter $\\beta_2$ is close enough to $1$, RMSprop and its random shuffling version converge to a bounded region in general, and to critical points in the interpolation regime. It is worth mentioning that our results do not depend on  ``bounded gradient\"  assumption, which is often the key assumption utilized by existing theoretical work for Adam-type adaptive gradient method. Removing this assumption allows us to establish a phase transition from divergence to non-divergence for RMSprop. \n\nFinally, based on our theory, we conjecture that in practice there is a critical threshold $\\sf{\\beta_2^*}$, such that RMSprop generates reasonably good results only if $1>\\beta_2\\ge \\sf{\\beta_2^*}$. We provide empirical evidence for such a phase transition in our numerical experiments.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|rmsprop_converges_with_proper_hyperparameter", "supplementary_material": "/attachment/faa87d1692c51a62939e2fce04044f31625ee90f.zip", "pdf": "/pdf/551ff96cadef8a1f8bf77f1227bff8c8d9987cca.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshi2021rmsprop,\ntitle={{\\{}RMS{\\}}prop converges with proper hyper-parameter},\nauthor={Naichen Shi and Dawei Li and Mingyi Hong and Ruoyu Sun},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3UDSdyIcBDA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "3UDSdyIcBDA", "replyto": "3UDSdyIcBDA", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2889/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538086582, "tmdate": 1606915758452, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2889/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2889/-/Official_Review"}}}], "count": 11}