{"notes": [{"id": "d_Ue2glvcY8", "original": "JwqSm-bqrF", "number": 1519, "cdate": 1601308168731, "ddate": null, "tcdate": 1601308168731, "tmdate": 1614985633744, "tddate": null, "forum": "d_Ue2glvcY8", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Structure Controllable Text Generation", "authorids": ["~Liming_DENG1", "wanglong137@pingan.com.cn", "wangbinzhu86@gmail.com", "~Jiang_Qian1", "~Bojin_Zhuang2", "~Shaojun_Wang1", "~Jing_Xiao3"], "authors": ["Liming DENG", "Long WANG", "Binzhu WANG", "Jiang Qian", "Bojin Zhuang", "Shaojun Wang", "Jing Xiao"], "keywords": ["Natural language generation", "structure representation", "structure controlling", "conditional language model", "structure aware transformer"], "abstract": "Controlling the presented forms (or structures) of generated text are as important as controlling the generated contents during neural text generation. It helps to reduce the uncertainty and improve the interpretability of generated text. However, the structures and contents are entangled together and realized simultaneously during text generation, which is challenging for the structure controlling. In this paper, we propose an efficient, straightforward generation framework to control the structure of generated text. A structure-aware transformer (SAT) is proposed to explicitly incorporate multiple types of multi-granularity structure information to guide the text generation with corresponding structure. The structure information is extracted from given sequence template by auxiliary model, and the type of structure for the given template can be learned, represented and imitated. Extensive experiments have been conducted on both Chinese lyrics corpus and English Penn Treebank dataset. Both automatic evaluation metrics and human judgement demonstrate the superior capability of our model in controlling the structure of generated text, and the quality ( like Fluency and Meaningfulness) of the generated text is even better than the state-of-the-arts model.\n", "one-sentence_summary": "A straightforward, interpretable structure controlling text generation framework is proposed, which is capable of learning and controlling multigranularity sequence structure from character-level to sentence-level structure.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deng|structure_controllable_text_generation", "pdf": "/pdf/4cd5ec24342ae550985ea70ee63de828c7614431.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4rmZ6aXk1r", "_bibtex": "@misc{\ndeng2021structure,\ntitle={Structure Controllable Text Generation},\nauthor={Liming DENG and Long WANG and Binzhu WANG and Jiang Qian and Bojin Zhuang and Shaojun Wang and Jing Xiao},\nyear={2021},\nurl={https://openreview.net/forum?id=d_Ue2glvcY8}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "0qOsCs8fAcn", "original": null, "number": 1, "cdate": 1610040527416, "ddate": null, "tcdate": 1610040527416, "tmdate": 1610474136610, "tddate": null, "forum": "d_Ue2glvcY8", "replyto": "d_Ue2glvcY8", "invitation": "ICLR.cc/2021/Conference/Paper1519/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper proposes a controllable text generation model conditioned on desired structures, converting a text into structure information such as part of speech (POS) and participial construction (PC). It proposes a \u201cStructure Aware Transformer\u201d (SAT) to generate text and claims better  PPL and BLEU compared with GPT-2. Reviewers pointed out that limited novelty of this paper - SAT is essentially a transformer run on multiple sequences of structure information, with sums of structure embeddings as input embeddings - \u00a0the proposed method essentially infuses structure information as features, rather than \u201ccontrolling\u201d text generation. Some references are also missing, most prominently: \n\n1. Zhang X, Yang Y, Yuan S, et al. Syntax-infused variational autoencoder for text generation[J]. arXiv preprint arXiv:1906.02181, 2019.\n2. Casas N, Fonollosa J A R, Costa-juss\u00e0 M R. Syntax-driven Iterative Expansion Language Models for Controllable Text Generation[J]. arXiv preprint arXiv:2004.02211, 2020.\n3. Wu S, Zhou M, Zhang D. Improved Neural Machine Translation with Source Syntax[C]//IJCAI. 2017: 4179-4185.\n\nUnfortunately, no answers are provided by the authors to the questions asked by the reviewers, which makes me recommend rejection."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Structure Controllable Text Generation", "authorids": ["~Liming_DENG1", "wanglong137@pingan.com.cn", "wangbinzhu86@gmail.com", "~Jiang_Qian1", "~Bojin_Zhuang2", "~Shaojun_Wang1", "~Jing_Xiao3"], "authors": ["Liming DENG", "Long WANG", "Binzhu WANG", "Jiang Qian", "Bojin Zhuang", "Shaojun Wang", "Jing Xiao"], "keywords": ["Natural language generation", "structure representation", "structure controlling", "conditional language model", "structure aware transformer"], "abstract": "Controlling the presented forms (or structures) of generated text are as important as controlling the generated contents during neural text generation. It helps to reduce the uncertainty and improve the interpretability of generated text. However, the structures and contents are entangled together and realized simultaneously during text generation, which is challenging for the structure controlling. In this paper, we propose an efficient, straightforward generation framework to control the structure of generated text. A structure-aware transformer (SAT) is proposed to explicitly incorporate multiple types of multi-granularity structure information to guide the text generation with corresponding structure. The structure information is extracted from given sequence template by auxiliary model, and the type of structure for the given template can be learned, represented and imitated. Extensive experiments have been conducted on both Chinese lyrics corpus and English Penn Treebank dataset. Both automatic evaluation metrics and human judgement demonstrate the superior capability of our model in controlling the structure of generated text, and the quality ( like Fluency and Meaningfulness) of the generated text is even better than the state-of-the-arts model.\n", "one-sentence_summary": "A straightforward, interpretable structure controlling text generation framework is proposed, which is capable of learning and controlling multigranularity sequence structure from character-level to sentence-level structure.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deng|structure_controllable_text_generation", "pdf": "/pdf/4cd5ec24342ae550985ea70ee63de828c7614431.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4rmZ6aXk1r", "_bibtex": "@misc{\ndeng2021structure,\ntitle={Structure Controllable Text Generation},\nauthor={Liming DENG and Long WANG and Binzhu WANG and Jiang Qian and Bojin Zhuang and Shaojun Wang and Jing Xiao},\nyear={2021},\nurl={https://openreview.net/forum?id=d_Ue2glvcY8}\n}"}, "tags": [], "invitation": {"reply": {"forum": "d_Ue2glvcY8", "replyto": "d_Ue2glvcY8", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040527403, "tmdate": 1610474136594, "id": "ICLR.cc/2021/Conference/Paper1519/-/Decision"}}}, {"id": "27Qv8GUONiK", "original": null, "number": 1, "cdate": 1603799759210, "ddate": null, "tcdate": 1603799759210, "tmdate": 1605024423363, "tddate": null, "forum": "d_Ue2glvcY8", "replyto": "d_Ue2glvcY8", "invitation": "ICLR.cc/2021/Conference/Paper1519/-/Official_Review", "content": {"title": "Official Blind Review #4", "review": "Review of Paper: Structure Controllable Text Generation\n\nSummary:\nThis paper proposes a structure-aware Transformer (SAT) by incorporating multiple types of multi-granularity structure information to control the text generation. Their method can extract structure information given sequence template by auxiliary model and learn the structure representations.  \n\nStrengths:\n\u2022\tThe proposed model is able to control multiple or multi-granularity types sequence structure from character-level to sentence-level structure by explicitly incorporating the corresponding structure information. \n\u2022\tThe model can simultaneously learn the structure representation and token representation.\n\nWeakness:\n\u2022\tThe paper does not clearly define the input content of this task. \nWhat\u2019s the input of the task? It seems the input should be the \u201cprompt\u201d (prefix of the poem), according to figure 2 in appendix C.\n\nIf so, why not use the content of \u201cprompt\u201d to predict the following tokens, which is more important for generation rather than the structure information? \n\nIf the input of this task is the structure information of the whole poem, the model can access future information (the structure of the sequence to be generated) during inference. It is unfair to compare this model with GPT-2, because it\u2019s not a standard language modeling task. Instead, it should compare with structure controlled text generation task like SongNet (Li, P.), but the paper does not conduct such a comparison. \n\n\u2022\tThe baseline is not solid and suitable for the datasets they use. \nGPT-2 is suitable for large-scale training corpus. Training GPT-2 on a small-scale dataset may result in lower performance, so GPT-2 cannot act as a strong baseline on a small-scale dataset (e.g. Chinese lyrics corpus). However, GPT-2 is the only one baseline in this paper. \n\nAs a large-scale text generation dataset is not hard to acquire, why not try it on a large-scale dataset?\n\n\n\n\n\nReference:\n[Li, P.] Li, P., Zhang, H., Liu, X., & Shi, S. (2020). Rigid Formats Controlled Text Generation. arXiv preprint arXiv:2004.08022.\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1519/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1519/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Structure Controllable Text Generation", "authorids": ["~Liming_DENG1", "wanglong137@pingan.com.cn", "wangbinzhu86@gmail.com", "~Jiang_Qian1", "~Bojin_Zhuang2", "~Shaojun_Wang1", "~Jing_Xiao3"], "authors": ["Liming DENG", "Long WANG", "Binzhu WANG", "Jiang Qian", "Bojin Zhuang", "Shaojun Wang", "Jing Xiao"], "keywords": ["Natural language generation", "structure representation", "structure controlling", "conditional language model", "structure aware transformer"], "abstract": "Controlling the presented forms (or structures) of generated text are as important as controlling the generated contents during neural text generation. It helps to reduce the uncertainty and improve the interpretability of generated text. However, the structures and contents are entangled together and realized simultaneously during text generation, which is challenging for the structure controlling. In this paper, we propose an efficient, straightforward generation framework to control the structure of generated text. A structure-aware transformer (SAT) is proposed to explicitly incorporate multiple types of multi-granularity structure information to guide the text generation with corresponding structure. The structure information is extracted from given sequence template by auxiliary model, and the type of structure for the given template can be learned, represented and imitated. Extensive experiments have been conducted on both Chinese lyrics corpus and English Penn Treebank dataset. Both automatic evaluation metrics and human judgement demonstrate the superior capability of our model in controlling the structure of generated text, and the quality ( like Fluency and Meaningfulness) of the generated text is even better than the state-of-the-arts model.\n", "one-sentence_summary": "A straightforward, interpretable structure controlling text generation framework is proposed, which is capable of learning and controlling multigranularity sequence structure from character-level to sentence-level structure.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deng|structure_controllable_text_generation", "pdf": "/pdf/4cd5ec24342ae550985ea70ee63de828c7614431.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4rmZ6aXk1r", "_bibtex": "@misc{\ndeng2021structure,\ntitle={Structure Controllable Text Generation},\nauthor={Liming DENG and Long WANG and Binzhu WANG and Jiang Qian and Bojin Zhuang and Shaojun Wang and Jing Xiao},\nyear={2021},\nurl={https://openreview.net/forum?id=d_Ue2glvcY8}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "d_Ue2glvcY8", "replyto": "d_Ue2glvcY8", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1519/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538116753, "tmdate": 1606915807786, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1519/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1519/-/Official_Review"}}}, {"id": "W3-NBJteV86", "original": null, "number": 3, "cdate": 1603890193724, "ddate": null, "tcdate": 1603890193724, "tmdate": 1605024423305, "tddate": null, "forum": "d_Ue2glvcY8", "replyto": "d_Ue2glvcY8", "invitation": "ICLR.cc/2021/Conference/Paper1519/-/Official_Review", "content": {"title": "Recommendation to Reject", "review": "Summary:\nThe paper proposes a variation of the conditioned Transformer-based language model. The authors use POS labels (for English and Chinese) and participial construction labels (only for Chinese) to control the output of the decoder and show the results are better than an unconditioned generation with GPT-2 in terms of several metrics.\n\nCons and questions:\n- The paper describes a generic conditioned language model approach, I can see no novelty of methods or results here.\n- No definition of \"structures\" given. Instead, the authors use fuzzy formulations like \"multiple types of multi-granularity structure information\" or \"the auxiliary model can be any credible model or tool that can extract soundable structure information from the template.\" In fact, the paper provides the results of experiments with POS labels and participial construction labels (only for Chinese).\n- No comparisons with other known syntax-aware language models are done (consider [arXiv:1909.02273], DOI:10.18653/v1/N19-1118, DOI:10.1109/IALP48816.2019.9037672, etc).\n- No exact details on the structure encoder or decoder pre-training or finetuning process are given, although they can represent some practical interest.\n- No clear description of the resulting architecture is provided (\"We only modified the input representation and few parameters of transformer\").\n- The reason for using a Transformer encoder is ambiguous (\"both the transformer encoder and decoder are adopted here is that we want each token in sequence to aware its local and global structure information\"), the logic of the number of Encoder layers selection is hard to check without details of the training process.\n- The Structure Matching evaluation was done through human assessment. However, no clear rules of the process are given.\n\nMinor comments:\nThe paper has a lot of typos and grammar issues. Here are some samples:\n(1) \"importan-t\" -- an incorrect word-wrap.\n(2) \"quality ( like Fluency\" -- a space after the open parenthesis\n(3) \"extracted structure information are regarded\"\n(4) \"word can aware the global structures\"\n(5) \"which can be any precede words for continue generation\"\n(6)\t\"reagrded\"\n\nThere are many more, I suggest at least to use some automatic spellchecking tools.\n", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1519/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1519/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Structure Controllable Text Generation", "authorids": ["~Liming_DENG1", "wanglong137@pingan.com.cn", "wangbinzhu86@gmail.com", "~Jiang_Qian1", "~Bojin_Zhuang2", "~Shaojun_Wang1", "~Jing_Xiao3"], "authors": ["Liming DENG", "Long WANG", "Binzhu WANG", "Jiang Qian", "Bojin Zhuang", "Shaojun Wang", "Jing Xiao"], "keywords": ["Natural language generation", "structure representation", "structure controlling", "conditional language model", "structure aware transformer"], "abstract": "Controlling the presented forms (or structures) of generated text are as important as controlling the generated contents during neural text generation. It helps to reduce the uncertainty and improve the interpretability of generated text. However, the structures and contents are entangled together and realized simultaneously during text generation, which is challenging for the structure controlling. In this paper, we propose an efficient, straightforward generation framework to control the structure of generated text. A structure-aware transformer (SAT) is proposed to explicitly incorporate multiple types of multi-granularity structure information to guide the text generation with corresponding structure. The structure information is extracted from given sequence template by auxiliary model, and the type of structure for the given template can be learned, represented and imitated. Extensive experiments have been conducted on both Chinese lyrics corpus and English Penn Treebank dataset. Both automatic evaluation metrics and human judgement demonstrate the superior capability of our model in controlling the structure of generated text, and the quality ( like Fluency and Meaningfulness) of the generated text is even better than the state-of-the-arts model.\n", "one-sentence_summary": "A straightforward, interpretable structure controlling text generation framework is proposed, which is capable of learning and controlling multigranularity sequence structure from character-level to sentence-level structure.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deng|structure_controllable_text_generation", "pdf": "/pdf/4cd5ec24342ae550985ea70ee63de828c7614431.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4rmZ6aXk1r", "_bibtex": "@misc{\ndeng2021structure,\ntitle={Structure Controllable Text Generation},\nauthor={Liming DENG and Long WANG and Binzhu WANG and Jiang Qian and Bojin Zhuang and Shaojun Wang and Jing Xiao},\nyear={2021},\nurl={https://openreview.net/forum?id=d_Ue2glvcY8}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "d_Ue2glvcY8", "replyto": "d_Ue2glvcY8", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1519/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538116753, "tmdate": 1606915807786, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1519/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1519/-/Official_Review"}}}, {"id": "T4qv5ktf3g1", "original": null, "number": 2, "cdate": 1603802566785, "ddate": null, "tcdate": 1603802566785, "tmdate": 1605024423236, "tddate": null, "forum": "d_Ue2glvcY8", "replyto": "d_Ue2glvcY8", "invitation": "ICLR.cc/2021/Conference/Paper1519/-/Official_Review", "content": {"title": "Official Blind Review #2", "review": "SUMMARY\n\nThis paper presents a text generation model conditioned on desired structures. The proposed method is essentially a translation model from structure information (represented with multiple sequences of tokens) to a text. This study converts a text into structure information such as part of speech (POS) and participial construction (PC). Then, this paper proposes Structure Aware Transformer (SAT), which is essentially the same as the Transformer architecture. The experiments use datasets of Chinese lyrics and English Penn Treebank. This paper reports that giving structure information improved the performance in PPL and BLEU compared with GPT-2.\n\nPROS\n\nIt was nice to confirm that we can control language generation from POS and PC.\n\nCONS\n\nThe proposed method presented in Section 3.3 is identical to Transformer except that:\n\n+ An input consists of multiple sequences of structure information (e.g., pos and pc)\n\n+ Input embeddings are sums of structure embeddings (Equation 7)\n\nFor this reason, I do not think Structure Aware Transformer (SAT) is a novel proposal. If this explanation is sufficient, I think that the descriptions in Section 3.1 and 3.3 are redundant.\n\nBecause structure sequences (POS and PC) are obtained from sentences in the test set, it is not surprising to see performance improvements of language models. In other words, predicting word sequences with some hints (POS and PC) is much easier than doing without any hint (GPT-2). For this reason, the findings in this paper are not convincing.\n\nQUESTIONS\n\nCould you explain the major difference between the proposed method and Transformer (excluding minor differences in how input embedings are computed and hyper-parameters such as the number of layers)?", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1519/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1519/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Structure Controllable Text Generation", "authorids": ["~Liming_DENG1", "wanglong137@pingan.com.cn", "wangbinzhu86@gmail.com", "~Jiang_Qian1", "~Bojin_Zhuang2", "~Shaojun_Wang1", "~Jing_Xiao3"], "authors": ["Liming DENG", "Long WANG", "Binzhu WANG", "Jiang Qian", "Bojin Zhuang", "Shaojun Wang", "Jing Xiao"], "keywords": ["Natural language generation", "structure representation", "structure controlling", "conditional language model", "structure aware transformer"], "abstract": "Controlling the presented forms (or structures) of generated text are as important as controlling the generated contents during neural text generation. It helps to reduce the uncertainty and improve the interpretability of generated text. However, the structures and contents are entangled together and realized simultaneously during text generation, which is challenging for the structure controlling. In this paper, we propose an efficient, straightforward generation framework to control the structure of generated text. A structure-aware transformer (SAT) is proposed to explicitly incorporate multiple types of multi-granularity structure information to guide the text generation with corresponding structure. The structure information is extracted from given sequence template by auxiliary model, and the type of structure for the given template can be learned, represented and imitated. Extensive experiments have been conducted on both Chinese lyrics corpus and English Penn Treebank dataset. Both automatic evaluation metrics and human judgement demonstrate the superior capability of our model in controlling the structure of generated text, and the quality ( like Fluency and Meaningfulness) of the generated text is even better than the state-of-the-arts model.\n", "one-sentence_summary": "A straightforward, interpretable structure controlling text generation framework is proposed, which is capable of learning and controlling multigranularity sequence structure from character-level to sentence-level structure.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deng|structure_controllable_text_generation", "pdf": "/pdf/4cd5ec24342ae550985ea70ee63de828c7614431.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4rmZ6aXk1r", "_bibtex": "@misc{\ndeng2021structure,\ntitle={Structure Controllable Text Generation},\nauthor={Liming DENG and Long WANG and Binzhu WANG and Jiang Qian and Bojin Zhuang and Shaojun Wang and Jing Xiao},\nyear={2021},\nurl={https://openreview.net/forum?id=d_Ue2glvcY8}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "d_Ue2glvcY8", "replyto": "d_Ue2glvcY8", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1519/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538116753, "tmdate": 1606915807786, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1519/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1519/-/Official_Review"}}}, {"id": "vlVBbgRebL_", "original": null, "number": 4, "cdate": 1603972323274, "ddate": null, "tcdate": 1603972323274, "tmdate": 1605024423169, "tddate": null, "forum": "d_Ue2glvcY8", "replyto": "d_Ue2glvcY8", "invitation": "ICLR.cc/2021/Conference/Paper1519/-/Official_Review", "content": {"title": "The paper proposed a text generation method which can utilize the language structures from character-level, word-level to sentence-level structure.", "review": "The paper proposed a text generation method which can utilize the language structures from character-level, word-level to sentence-level structure.  The proposed model, structure-aware transformer (SAT), explicitly incorporates multiple types of multi-granularity structure information to guide the text generation with corresponding structure. \n\nPros:\n1. The paper is clearly written. The experiments show the effectiveness of the proposed method.\n2. The  proposed method can explicitly incorporate multiple types of multi-granularity structure information to guide the text generation.\n3. The proposed method shows its advantages in structure control.\n\nCons:\n\n1. The method just incorporates structure information in the encoder part rather than the decoder. It's hard to guarantee the quality of structure control. Moreover, this way to incorporate structure information is just adding extra features and not a new framework.\n2. The structure information used in the paper is only segmentation and POS information. Other information, such as syntax, also should be considered.  Segmentation and POS information are just shallow structure information.\n3. BLEU score for POS or BMES is not suitable. Since the structure information is given, the generated text should be evaluated with other measures, such as F1 score.\n4. The fluency could be due to the pre-trained language model GPT2. An experiment should be performed without PLMs.\n5. Some related references are missing.\n\nQuestions:\n1. The title of paper is \"Structure Controllable Text Generation\", but the proposed method is just to infuse structure information as features. Therefore, the proposed method is more like \"structure-infused\" rather than \"structure controllable\".\n\nMissing References:\n1. Zhang X, Yang Y, Yuan S, et al. Syntax-infused variational autoencoder for text generation[J]. arXiv preprint arXiv:1906.02181, 2019.\n2. Casas N, Fonollosa J A R, Costa-juss\u00e0 M R. Syntax-driven Iterative Expansion Language Models for Controllable Text Generation[J]. arXiv preprint arXiv:2004.02211, 2020.\n3. Wu S, Zhou M, Zhang D. Improved Neural Machine Translation with Source Syntax[C]//IJCAI. 2017: 4179-4185.\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1519/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1519/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Structure Controllable Text Generation", "authorids": ["~Liming_DENG1", "wanglong137@pingan.com.cn", "wangbinzhu86@gmail.com", "~Jiang_Qian1", "~Bojin_Zhuang2", "~Shaojun_Wang1", "~Jing_Xiao3"], "authors": ["Liming DENG", "Long WANG", "Binzhu WANG", "Jiang Qian", "Bojin Zhuang", "Shaojun Wang", "Jing Xiao"], "keywords": ["Natural language generation", "structure representation", "structure controlling", "conditional language model", "structure aware transformer"], "abstract": "Controlling the presented forms (or structures) of generated text are as important as controlling the generated contents during neural text generation. It helps to reduce the uncertainty and improve the interpretability of generated text. However, the structures and contents are entangled together and realized simultaneously during text generation, which is challenging for the structure controlling. In this paper, we propose an efficient, straightforward generation framework to control the structure of generated text. A structure-aware transformer (SAT) is proposed to explicitly incorporate multiple types of multi-granularity structure information to guide the text generation with corresponding structure. The structure information is extracted from given sequence template by auxiliary model, and the type of structure for the given template can be learned, represented and imitated. Extensive experiments have been conducted on both Chinese lyrics corpus and English Penn Treebank dataset. Both automatic evaluation metrics and human judgement demonstrate the superior capability of our model in controlling the structure of generated text, and the quality ( like Fluency and Meaningfulness) of the generated text is even better than the state-of-the-arts model.\n", "one-sentence_summary": "A straightforward, interpretable structure controlling text generation framework is proposed, which is capable of learning and controlling multigranularity sequence structure from character-level to sentence-level structure.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deng|structure_controllable_text_generation", "pdf": "/pdf/4cd5ec24342ae550985ea70ee63de828c7614431.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4rmZ6aXk1r", "_bibtex": "@misc{\ndeng2021structure,\ntitle={Structure Controllable Text Generation},\nauthor={Liming DENG and Long WANG and Binzhu WANG and Jiang Qian and Bojin Zhuang and Shaojun Wang and Jing Xiao},\nyear={2021},\nurl={https://openreview.net/forum?id=d_Ue2glvcY8}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "d_Ue2glvcY8", "replyto": "d_Ue2glvcY8", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1519/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538116753, "tmdate": 1606915807786, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1519/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1519/-/Official_Review"}}}], "count": 6}