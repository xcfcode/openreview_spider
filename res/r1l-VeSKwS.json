{"notes": [{"id": "r1l-VeSKwS", "original": "ByeFxLlKvr", "number": 2237, "cdate": 1569439784884, "ddate": null, "tcdate": 1569439784884, "tmdate": 1577168255762, "tddate": null, "forum": "r1l-VeSKwS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["haonanqiu@link.cuhk.edu.cn", "xiaocw@umich.edu", "yl016@ie.cuhk.edu.hk", "xcyan@umich.edu", "honglak@eecs.umich.edu", "lxbosky@gmail.com"], "title": "SemanticAdv: Generating Adversarial Examples via Attribute-Conditional Image Editing", "authors": ["Haonan Qiu", "Chaowei Xiao", "Lei Yang", "Xinchen Yan", "HongLak Lee", "Bo Li"], "pdf": "/pdf/f3fd4f5a5545c1c4236d566d6cdb91e5ad06d729.pdf", "TL;DR": "A novel and effective semantic adversarial attack method.", "abstract": "Deep neural networks (DNNs) have achieved great success in various applications due to their strong expressive power. However, recent studies have shown that DNNs are vulnerable to adversarial examples which are manipulated instances targeting to mislead DNNs to make incorrect predictions. Currently, most such adversarial examples try to guarantee \u201csubtle perturbation\" by limiting the Lp norm of the perturbation. In this paper, we aim to explore the impact of semantic manipulation on DNNs predictions by manipulating the semantic attributes of images and generate \u201cunrestricted adversarial examples\". Such semantic based perturbation is more practical compared with the Lp bounded perturbation. In particular, we propose an algorithm SemanticAdv which leverages disentangled semantic factors to generate adversarial perturbation by altering controlled semantic attributes to fool the learner towards various \u201cadversarial\" targets. We conduct extensive experiments to show that the semantic based adversarial examples can not only fool different learning tasks such as face verification and landmark detection, but also achieve high targeted attack success rate against real-world black-box services such as Azure face verification service based on transferability. To further demonstrate the applicability of SemanticAdv beyond face recognition domain, we also generate semantic perturbations on street-view images. Such adversarial examples with controlled semantic manipulation can shed light on further understanding about vulnerabilities of DNNs as well as potential defensive approaches.", "keywords": ["adversarial examples", "semantic attack"], "paperhash": "qiu|semanticadv_generating_adversarial_examples_via_attributeconditional_image_editing", "original_pdf": "/attachment/463f0dbb711de6a22e2b2925a653210743eb4913.pdf", "_bibtex": "@misc{\nqiu2020semanticadv,\ntitle={SemanticAdv: Generating Adversarial Examples via Attribute-Conditional Image Editing},\nauthor={Haonan Qiu and Chaowei Xiao and Lei Yang and Xinchen Yan and HongLak Lee and Bo Li},\nyear={2020},\nurl={https://openreview.net/forum?id=r1l-VeSKwS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 15, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "ctV0cFEYcE", "original": null, "number": 1, "cdate": 1576798743969, "ddate": null, "tcdate": 1576798743969, "tmdate": 1576800892181, "tddate": null, "forum": "r1l-VeSKwS", "replyto": "r1l-VeSKwS", "invitation": "ICLR.cc/2020/Conference/Paper2237/-/Decision", "content": {"decision": "Reject", "comment": "I had a little bit of difficulty with my recommendation here, but in the end I don't feel confident in recommending this paper for acceptance, with my concerns largely boiling down to the lack of clear description of the overall motivation.\n\nStandard adversarial attacks are meant to be *imperceptible* changes that do not change the underlying semantics of the input to the human eye. In other words, the goal of the current work, generating \"semantically meaningful\" perturbations goes against the standard definition of adversarial attacks. This left me with two questions:\n\n1. Under the definition of semantic adversarial attacks, what is to prevent someone from swapping out the current image with an entirely different image? From what I saw in the evaluation measures utilized in the paper, such a method would be judged as having performed a successful attack, and given no constraints there is nothing stopping this.\n\n2. In what situation would such an attack method would be practically useful?\n\nEven the reviewers who reviewed the paper favorably were not able to provide answers to these questions, and I was not able to resolve this from my reading of the paper as well. I do understand that there is a challenge on this by Google. In my opinion, even this contest is somewhat ill-defined, but it also features extensive human evaluation to evaluate the validity of the perturbations, which is not featured in the experimental evaluation here.\n\nWhile I think this work is potentially interesting, it seems that there are too many open questions that are not resolved yet to recommend acceptance at this time, but I would encourage the authors to tighten up the argumentation/evaluation in this regard and revise the paper to be better accordingly!", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["haonanqiu@link.cuhk.edu.cn", "xiaocw@umich.edu", "yl016@ie.cuhk.edu.hk", "xcyan@umich.edu", "honglak@eecs.umich.edu", "lxbosky@gmail.com"], "title": "SemanticAdv: Generating Adversarial Examples via Attribute-Conditional Image Editing", "authors": ["Haonan Qiu", "Chaowei Xiao", "Lei Yang", "Xinchen Yan", "HongLak Lee", "Bo Li"], "pdf": "/pdf/f3fd4f5a5545c1c4236d566d6cdb91e5ad06d729.pdf", "TL;DR": "A novel and effective semantic adversarial attack method.", "abstract": "Deep neural networks (DNNs) have achieved great success in various applications due to their strong expressive power. However, recent studies have shown that DNNs are vulnerable to adversarial examples which are manipulated instances targeting to mislead DNNs to make incorrect predictions. Currently, most such adversarial examples try to guarantee \u201csubtle perturbation\" by limiting the Lp norm of the perturbation. In this paper, we aim to explore the impact of semantic manipulation on DNNs predictions by manipulating the semantic attributes of images and generate \u201cunrestricted adversarial examples\". Such semantic based perturbation is more practical compared with the Lp bounded perturbation. In particular, we propose an algorithm SemanticAdv which leverages disentangled semantic factors to generate adversarial perturbation by altering controlled semantic attributes to fool the learner towards various \u201cadversarial\" targets. We conduct extensive experiments to show that the semantic based adversarial examples can not only fool different learning tasks such as face verification and landmark detection, but also achieve high targeted attack success rate against real-world black-box services such as Azure face verification service based on transferability. To further demonstrate the applicability of SemanticAdv beyond face recognition domain, we also generate semantic perturbations on street-view images. Such adversarial examples with controlled semantic manipulation can shed light on further understanding about vulnerabilities of DNNs as well as potential defensive approaches.", "keywords": ["adversarial examples", "semantic attack"], "paperhash": "qiu|semanticadv_generating_adversarial_examples_via_attributeconditional_image_editing", "original_pdf": "/attachment/463f0dbb711de6a22e2b2925a653210743eb4913.pdf", "_bibtex": "@misc{\nqiu2020semanticadv,\ntitle={SemanticAdv: Generating Adversarial Examples via Attribute-Conditional Image Editing},\nauthor={Haonan Qiu and Chaowei Xiao and Lei Yang and Xinchen Yan and HongLak Lee and Bo Li},\nyear={2020},\nurl={https://openreview.net/forum?id=r1l-VeSKwS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "r1l-VeSKwS", "replyto": "r1l-VeSKwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795711699, "tmdate": 1576800260947, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2237/-/Decision"}}}, {"id": "rJxyQVhiiB", "original": null, "number": 18, "cdate": 1573794839472, "ddate": null, "tcdate": 1573794839472, "tmdate": 1573794839472, "tddate": null, "forum": "r1l-VeSKwS", "replyto": "r1l-VeSKwS", "invitation": "ICLR.cc/2020/Conference/Paper2237/-/Official_Comment", "content": {"title": "Responses and Revisions", "comment": "\nWe thank all reviewers for their valuable comments and suggestions.  We appreciate the reviewers recognizing our work interesting (R1, R2, R3), technically sound with concrete experiment results (R2, R3), broadening the study of adversarial examples and encouraging a good deal of follow-up research (R3). Based on the reviewers\u2019 suggestions, we have made the following changes in our revision. \n    1. Adding StawnMan baseline proposed by R3 in Table H and I. \n    2. Selecting additional different layer\u2019s feature map for interpolation and evaluating the results. (Table F)\n    3. Changing the notations of the equations in Section 3.\n    4. Fixing some typos.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2237/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2237/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["haonanqiu@link.cuhk.edu.cn", "xiaocw@umich.edu", "yl016@ie.cuhk.edu.hk", "xcyan@umich.edu", "honglak@eecs.umich.edu", "lxbosky@gmail.com"], "title": "SemanticAdv: Generating Adversarial Examples via Attribute-Conditional Image Editing", "authors": ["Haonan Qiu", "Chaowei Xiao", "Lei Yang", "Xinchen Yan", "HongLak Lee", "Bo Li"], "pdf": "/pdf/f3fd4f5a5545c1c4236d566d6cdb91e5ad06d729.pdf", "TL;DR": "A novel and effective semantic adversarial attack method.", "abstract": "Deep neural networks (DNNs) have achieved great success in various applications due to their strong expressive power. However, recent studies have shown that DNNs are vulnerable to adversarial examples which are manipulated instances targeting to mislead DNNs to make incorrect predictions. Currently, most such adversarial examples try to guarantee \u201csubtle perturbation\" by limiting the Lp norm of the perturbation. In this paper, we aim to explore the impact of semantic manipulation on DNNs predictions by manipulating the semantic attributes of images and generate \u201cunrestricted adversarial examples\". Such semantic based perturbation is more practical compared with the Lp bounded perturbation. In particular, we propose an algorithm SemanticAdv which leverages disentangled semantic factors to generate adversarial perturbation by altering controlled semantic attributes to fool the learner towards various \u201cadversarial\" targets. We conduct extensive experiments to show that the semantic based adversarial examples can not only fool different learning tasks such as face verification and landmark detection, but also achieve high targeted attack success rate against real-world black-box services such as Azure face verification service based on transferability. To further demonstrate the applicability of SemanticAdv beyond face recognition domain, we also generate semantic perturbations on street-view images. Such adversarial examples with controlled semantic manipulation can shed light on further understanding about vulnerabilities of DNNs as well as potential defensive approaches.", "keywords": ["adversarial examples", "semantic attack"], "paperhash": "qiu|semanticadv_generating_adversarial_examples_via_attributeconditional_image_editing", "original_pdf": "/attachment/463f0dbb711de6a22e2b2925a653210743eb4913.pdf", "_bibtex": "@misc{\nqiu2020semanticadv,\ntitle={SemanticAdv: Generating Adversarial Examples via Attribute-Conditional Image Editing},\nauthor={Haonan Qiu and Chaowei Xiao and Lei Yang and Xinchen Yan and HongLak Lee and Bo Li},\nyear={2020},\nurl={https://openreview.net/forum?id=r1l-VeSKwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1l-VeSKwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2237/Authors", "ICLR.cc/2020/Conference/Paper2237/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2237/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2237/Reviewers", "ICLR.cc/2020/Conference/Paper2237/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2237/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2237/Authors|ICLR.cc/2020/Conference/Paper2237/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144327, "tmdate": 1576860545034, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2237/Authors", "ICLR.cc/2020/Conference/Paper2237/Reviewers", "ICLR.cc/2020/Conference/Paper2237/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2237/-/Official_Comment"}}}, {"id": "rJlh8NU5sB", "original": null, "number": 11, "cdate": 1573704788355, "ddate": null, "tcdate": 1573704788355, "tmdate": 1573707267331, "tddate": null, "forum": "r1l-VeSKwS", "replyto": "H1lWBP4oFH", "invitation": "ICLR.cc/2020/Conference/Paper2237/-/Official_Comment", "content": {"title": "Response to Review #1", "comment": "\nWe really appreciate the reviewer\u2019s precious comments. Sorry for the potential confusion. We would like to answer your questions as follows and we have added them in our revision. \n\nQ1: assume M is an oracle-- what is the impact of this?\nA1: Thanks for pointing this out and we will remove this notation in the revision to avoid confusion. Basically, M here is used to obtain the corresponding label related to data x, and we actually do not need to use this assumption in our experiments (we can assume the ground-truth label is given). But we see this assumption introducing the confusion and we will remove this statement by using the ground truth label y directly. \n\nQ2: \u201cThe results in Table C don't look good.\u201d \nA2: We believe the \u201cTable C don\u2019t look good\u201d refers to the results with \u201cworst\u201d and \u201caverage\u201d metrics. In Table C, the \u201cbest\u201d metric of SemanticAdv should be served as a fair comparison to CW, where both methods achieve 100% attack success rate. Therefore, our result is good. The detailed reasons are as follows. \n\nFor each victim image, our SemanticAdv generates a total of 17 adversarial images by augmenting one semantic attribute each time (e.g., we have 17 attributes to manipulate). However, CW generates a single adversarial example regardless of attributes, which can be viewed as instance-level generation. Therefore, we compare CW with our SemanticAdv on the instance-level which corresponds to the \u201cbest\u201d metric.  \n\nIn addition, we report the performance using the \u201caverage\u201d and \u201cworst\u201d metric, which actually provides additional insights into the robustness of face verification models across different attributes. Combining the results from Table C in our appendix and Figure 3, we understand that the face verification models used in our experiments have different levels of robustness across attributes. For example, face verification models are more robust against local shape variations than color variations, e.g., pale skin has higher attack success rate than mouth open. We believe these discoveries will help the community further understand the properties of face verification models.\n\nTo summarize, our *semantic* adversarial examples not only achieves attack success rate comparable to traditional $L_p$-norm bounded CW attacks, but also enables us to investigate the model robustness under different semantic attributes. We will make the description of Table C clearer in the revised manuscript.\n\n\n(To be continued.)\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2237/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2237/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["haonanqiu@link.cuhk.edu.cn", "xiaocw@umich.edu", "yl016@ie.cuhk.edu.hk", "xcyan@umich.edu", "honglak@eecs.umich.edu", "lxbosky@gmail.com"], "title": "SemanticAdv: Generating Adversarial Examples via Attribute-Conditional Image Editing", "authors": ["Haonan Qiu", "Chaowei Xiao", "Lei Yang", "Xinchen Yan", "HongLak Lee", "Bo Li"], "pdf": "/pdf/f3fd4f5a5545c1c4236d566d6cdb91e5ad06d729.pdf", "TL;DR": "A novel and effective semantic adversarial attack method.", "abstract": "Deep neural networks (DNNs) have achieved great success in various applications due to their strong expressive power. However, recent studies have shown that DNNs are vulnerable to adversarial examples which are manipulated instances targeting to mislead DNNs to make incorrect predictions. Currently, most such adversarial examples try to guarantee \u201csubtle perturbation\" by limiting the Lp norm of the perturbation. In this paper, we aim to explore the impact of semantic manipulation on DNNs predictions by manipulating the semantic attributes of images and generate \u201cunrestricted adversarial examples\". Such semantic based perturbation is more practical compared with the Lp bounded perturbation. In particular, we propose an algorithm SemanticAdv which leverages disentangled semantic factors to generate adversarial perturbation by altering controlled semantic attributes to fool the learner towards various \u201cadversarial\" targets. We conduct extensive experiments to show that the semantic based adversarial examples can not only fool different learning tasks such as face verification and landmark detection, but also achieve high targeted attack success rate against real-world black-box services such as Azure face verification service based on transferability. To further demonstrate the applicability of SemanticAdv beyond face recognition domain, we also generate semantic perturbations on street-view images. Such adversarial examples with controlled semantic manipulation can shed light on further understanding about vulnerabilities of DNNs as well as potential defensive approaches.", "keywords": ["adversarial examples", "semantic attack"], "paperhash": "qiu|semanticadv_generating_adversarial_examples_via_attributeconditional_image_editing", "original_pdf": "/attachment/463f0dbb711de6a22e2b2925a653210743eb4913.pdf", "_bibtex": "@misc{\nqiu2020semanticadv,\ntitle={SemanticAdv: Generating Adversarial Examples via Attribute-Conditional Image Editing},\nauthor={Haonan Qiu and Chaowei Xiao and Lei Yang and Xinchen Yan and HongLak Lee and Bo Li},\nyear={2020},\nurl={https://openreview.net/forum?id=r1l-VeSKwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1l-VeSKwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2237/Authors", "ICLR.cc/2020/Conference/Paper2237/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2237/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2237/Reviewers", "ICLR.cc/2020/Conference/Paper2237/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2237/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2237/Authors|ICLR.cc/2020/Conference/Paper2237/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144327, "tmdate": 1576860545034, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2237/Authors", "ICLR.cc/2020/Conference/Paper2237/Reviewers", "ICLR.cc/2020/Conference/Paper2237/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2237/-/Official_Comment"}}}, {"id": "rkg5HFI5sS", "original": null, "number": 17, "cdate": 1573706049772, "ddate": null, "tcdate": 1573706049772, "tmdate": 1573707176584, "tddate": null, "forum": "r1l-VeSKwS", "replyto": "rkxZTuIcoH", "invitation": "ICLR.cc/2020/Conference/Paper2237/-/Official_Comment", "content": {"title": "Continue #2", "comment": "\nQ5. Notations. \nQ5a: The difference between $x^{tgt}$ and $x^{adv}$, or between $x^{new}$ and $x^{*}$.\nA5a: We consider the adversarial attack in the targeted setting, where $x$ is the original image and $x^{tgt}$ is the image with the target label we aim at misclassifying $x^{adv}$ to.\n$x^{new}$ is the intermediate image we produced by the attribute-conditional image editing from the original $x$ (without adversarial attack).\n$x^{*}$ represents the intermediate image in the optimization step (e.g., $x^{*}$ equals to $x^{adv}$ at the end of optimization). \n\nQ5b: Equation 3. \nA5b: Thanks for the suggestion. We will improve the equation 3 denoting with optimization variable alpha.\n\nQ5c: $M(x^{tgt}) = y^{tgt}$\nA5c: Thanks for pointing this out and we realize it causes confusion and will remove this notation in the revision. Basically, this notation is used to guarantee the unperturbed instances evaluated by our algorithms can be predicted correctly by $M$ otherwise it would be a challenge to distinguish the source of the error of the generated instances. \n\nQ5d: Position of y.\nA5d: Thanks for pointing it out! It is a typo, it should be $y^{*}$. We will fix it in the revision.\n\nQ5e: Missing argument of $L_{smooth}$.\nA5e: We admit that this is an abbreviated form of $L_{smooth}(\\alpha)$, which has been defined in Equation (3). We will update this in our revision.\n\nReferences\n[a1] \u201cSpatially transformed adversarial examples.\u201d Xiao et al. In ICLR 2018.\n[a2] \u201cTesting robustness against unforeseen adversaries.\u201d Kang et al. arXiv preprint arXiv:1908.08016.\n[a3] \u201cWasserstein Adversarial Examples via Projected Sinkhorn Iterations.\u201d Wong et al. In ICML 2019\n[a4] \u201cUnrestricted adversarial examples.\u201d Brown et al. arXiv preprint arXiv:1809.08352.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2237/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2237/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["haonanqiu@link.cuhk.edu.cn", "xiaocw@umich.edu", "yl016@ie.cuhk.edu.hk", "xcyan@umich.edu", "honglak@eecs.umich.edu", "lxbosky@gmail.com"], "title": "SemanticAdv: Generating Adversarial Examples via Attribute-Conditional Image Editing", "authors": ["Haonan Qiu", "Chaowei Xiao", "Lei Yang", "Xinchen Yan", "HongLak Lee", "Bo Li"], "pdf": "/pdf/f3fd4f5a5545c1c4236d566d6cdb91e5ad06d729.pdf", "TL;DR": "A novel and effective semantic adversarial attack method.", "abstract": "Deep neural networks (DNNs) have achieved great success in various applications due to their strong expressive power. However, recent studies have shown that DNNs are vulnerable to adversarial examples which are manipulated instances targeting to mislead DNNs to make incorrect predictions. Currently, most such adversarial examples try to guarantee \u201csubtle perturbation\" by limiting the Lp norm of the perturbation. In this paper, we aim to explore the impact of semantic manipulation on DNNs predictions by manipulating the semantic attributes of images and generate \u201cunrestricted adversarial examples\". Such semantic based perturbation is more practical compared with the Lp bounded perturbation. In particular, we propose an algorithm SemanticAdv which leverages disentangled semantic factors to generate adversarial perturbation by altering controlled semantic attributes to fool the learner towards various \u201cadversarial\" targets. We conduct extensive experiments to show that the semantic based adversarial examples can not only fool different learning tasks such as face verification and landmark detection, but also achieve high targeted attack success rate against real-world black-box services such as Azure face verification service based on transferability. To further demonstrate the applicability of SemanticAdv beyond face recognition domain, we also generate semantic perturbations on street-view images. Such adversarial examples with controlled semantic manipulation can shed light on further understanding about vulnerabilities of DNNs as well as potential defensive approaches.", "keywords": ["adversarial examples", "semantic attack"], "paperhash": "qiu|semanticadv_generating_adversarial_examples_via_attributeconditional_image_editing", "original_pdf": "/attachment/463f0dbb711de6a22e2b2925a653210743eb4913.pdf", "_bibtex": "@misc{\nqiu2020semanticadv,\ntitle={SemanticAdv: Generating Adversarial Examples via Attribute-Conditional Image Editing},\nauthor={Haonan Qiu and Chaowei Xiao and Lei Yang and Xinchen Yan and HongLak Lee and Bo Li},\nyear={2020},\nurl={https://openreview.net/forum?id=r1l-VeSKwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1l-VeSKwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2237/Authors", "ICLR.cc/2020/Conference/Paper2237/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2237/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2237/Reviewers", "ICLR.cc/2020/Conference/Paper2237/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2237/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2237/Authors|ICLR.cc/2020/Conference/Paper2237/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144327, "tmdate": 1576860545034, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2237/Authors", "ICLR.cc/2020/Conference/Paper2237/Reviewers", "ICLR.cc/2020/Conference/Paper2237/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2237/-/Official_Comment"}}}, {"id": "rkxZTuIcoH", "original": null, "number": 16, "cdate": 1573705913400, "ddate": null, "tcdate": 1573705913400, "tmdate": 1573707153476, "tddate": null, "forum": "r1l-VeSKwS", "replyto": "Skx6qOI9iH", "invitation": "ICLR.cc/2020/Conference/Paper2237/-/Official_Comment", "content": {"title": "Continue #1", "comment": "\nQ2. Is the argument that this is a more powerful attack surface, so adversaries should take note (and defenders should figure out how to defend against this)?\nA2: Thanks for the interesting question. There are several definitions for \u201cmore powerful attack\u201d. For instance, in terms of the magnitude of perturbation, SemanticAdv is more powerful as it is able to tolerate a larger perturbation. In terms of attack success rate, both SemanticAdv and other $L_p$ norm based pixel level attacks can achieve almost 100% attack success rate. Therefore, we believe semantic based adversarial examples are important mainly because it explores different properties that traditional $L_p$ based ones have missed and provide diverse adversarial attacks.\n\nQ3. Capture a more realistic part of the data distribution over all natural images. \nA3: Thanks for the very interesting point. At this point, we can only show that such semantic based attacks do provide diverse adversarial examples in addition to existing ones, but whether it actually captures more realistic part of the data distribution is challenging to verify and we will definitely explore it as the future work by proposing different evaluation process and metrics for the benign and adversarial data distributions.\n\nBased on the reviewer\u2019s suggestion, we conduct the StawnMan baseline. We generate adversarial examples by using StawnMan baseline. It shows 100% attack success rate under the white-box setting. We further evaluate its performance of query-free black-box API attacks and transferability. The results are shown below. We can observe there is a noticeable gap between our proposed SemanticAdv and the StawnMan baseline in terms of performance. This result justifies the argument that our SemanticAdv is able to produce novel adversarial examples that cannot be simply achieved by combining attribute-conditional image editing model with $L_p$ bounded perturbation. \n\nTable R3T1: Transferability of StawnMan\n+\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500+\u2500\u2500\u2500\u2500\u2500\u2500\u2500+\n| M_test / M_opt   | R-101-S             |\n+\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500+\u2500\u2500\u2500\u2500\u2500\u2500\u2500+\n| R-50-S                  | 0.035 (0.108)    |\n| R-101-S                | 1.000 (1.000)    |\n| R-50-C                  | 0.145 (0.202)    |\n| R-101-C                | 0.085 (0.236)    |\n+\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500+\u2500\u2500\u2500\u2500\u2500\u2500\u2500+\n(G-FPR = $10^{-3}$, T-FPR = $10^{-3}$, SemanticAdv in blankets)\n\n+\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500+\u2500\u2500\u2500\u2500\u2500\u2500\u2500+\n| M_test / M_opt   | R-101-S             |\n+\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500+\u2500\u2500\u2500\u2500\u2500\u2500\u2500+\n| R-50-S                  | 0.615 (0.862)    |\n| R-101-S                | 1.000 (1.000)    |\n| R-50-C                  | 0.570 (0.837)    |\n| R-101-C                | 0.695 (0.888)    |\n+\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500+\u2500\u2500\u2500\u2500\u2500\u2500\u2500+\n(G-FPR = $10^{-4}$, T-FPR = $10^{-3}$, SemanticAdv in blankets)\n\nTable R3T2: Quantitative analysis on query-free black-box attack of StawMan\n\n+\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500+\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500+\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500+\n| API name                                   |                   Face++                       |                     AliYun                     |\n+\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500+\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500+\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500+\n| Attacker / Evaluation Metric   | T-FPR = $10^{-3}$  | T-FPR = $10^{-4}$ | T-FPR = $10^{-3}$  | T-FPR = $10^{-4}$ |\n+\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500+\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500+\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500+\n| StawnMan (G-FPR = 1e-3)        | 10.71                | 4.08                 | 3.00                  | 0.00                 |\n| SemanticAdv (G-FPR = 1e\u22123)   | 27.32                | 9.79                 | 7.50                  | 2.00                 |\n| StawnMan (G-FPR = 3e-4)        | 21.32                | 9.14                 | 7.50                  | 1.50                 |\n| SemanticAdv (G-FPR = 3e\u22124)   | 57.22                | 38.66               | 29.50                | 17.50               |\n| StawnMan (G-FPR < 1e-4)        | 27.69                | 15.38               | 10.00                | 3.00                 |\n| SemanticAdv (G-FPR < 1e\u22124)   | 64.63                | 42.69               | 35.50                | 22.17               |   \n+\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500+\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500+\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500+\n\n\n\n\nQ4. It is just good to be able to generate examples that models get wrong? If so, why, and why is this method better than other methods?\n\nA4: According to the evaluation in our paper and the addition StawnMan baseline results, it shows that SemanticAdv is not only effective to generate adversarial examples different with $L_p$ based attacks but also indeed contain unique properties (e.g. different from applying $L_p$ perturbation on the manipulated image guided by semantic attributes). One of the potential reasons for the good performance is that SemanticAdv is able to explore the stronger adversarial space which can achieve higher transferability. For more detailed motivation of the semantidAdv please refer to A1.\n\n\n(To be continued.)\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2237/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2237/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["haonanqiu@link.cuhk.edu.cn", "xiaocw@umich.edu", "yl016@ie.cuhk.edu.hk", "xcyan@umich.edu", "honglak@eecs.umich.edu", "lxbosky@gmail.com"], "title": "SemanticAdv: Generating Adversarial Examples via Attribute-Conditional Image Editing", "authors": ["Haonan Qiu", "Chaowei Xiao", "Lei Yang", "Xinchen Yan", "HongLak Lee", "Bo Li"], "pdf": "/pdf/f3fd4f5a5545c1c4236d566d6cdb91e5ad06d729.pdf", "TL;DR": "A novel and effective semantic adversarial attack method.", "abstract": "Deep neural networks (DNNs) have achieved great success in various applications due to their strong expressive power. However, recent studies have shown that DNNs are vulnerable to adversarial examples which are manipulated instances targeting to mislead DNNs to make incorrect predictions. Currently, most such adversarial examples try to guarantee \u201csubtle perturbation\" by limiting the Lp norm of the perturbation. In this paper, we aim to explore the impact of semantic manipulation on DNNs predictions by manipulating the semantic attributes of images and generate \u201cunrestricted adversarial examples\". Such semantic based perturbation is more practical compared with the Lp bounded perturbation. In particular, we propose an algorithm SemanticAdv which leverages disentangled semantic factors to generate adversarial perturbation by altering controlled semantic attributes to fool the learner towards various \u201cadversarial\" targets. We conduct extensive experiments to show that the semantic based adversarial examples can not only fool different learning tasks such as face verification and landmark detection, but also achieve high targeted attack success rate against real-world black-box services such as Azure face verification service based on transferability. To further demonstrate the applicability of SemanticAdv beyond face recognition domain, we also generate semantic perturbations on street-view images. Such adversarial examples with controlled semantic manipulation can shed light on further understanding about vulnerabilities of DNNs as well as potential defensive approaches.", "keywords": ["adversarial examples", "semantic attack"], "paperhash": "qiu|semanticadv_generating_adversarial_examples_via_attributeconditional_image_editing", "original_pdf": "/attachment/463f0dbb711de6a22e2b2925a653210743eb4913.pdf", "_bibtex": "@misc{\nqiu2020semanticadv,\ntitle={SemanticAdv: Generating Adversarial Examples via Attribute-Conditional Image Editing},\nauthor={Haonan Qiu and Chaowei Xiao and Lei Yang and Xinchen Yan and HongLak Lee and Bo Li},\nyear={2020},\nurl={https://openreview.net/forum?id=r1l-VeSKwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1l-VeSKwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2237/Authors", "ICLR.cc/2020/Conference/Paper2237/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2237/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2237/Reviewers", "ICLR.cc/2020/Conference/Paper2237/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2237/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2237/Authors|ICLR.cc/2020/Conference/Paper2237/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144327, "tmdate": 1576860545034, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2237/Authors", "ICLR.cc/2020/Conference/Paper2237/Reviewers", "ICLR.cc/2020/Conference/Paper2237/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2237/-/Official_Comment"}}}, {"id": "Skx6qOI9iH", "original": null, "number": 15, "cdate": 1573705876731, "ddate": null, "tcdate": 1573705876731, "tmdate": 1573707141494, "tddate": null, "forum": "r1l-VeSKwS", "replyto": "SkxDlf-ttr", "invitation": "ICLR.cc/2020/Conference/Paper2237/-/Official_Comment", "content": {"title": "Response to Review #3", "comment": "\nWe appreciate the reviewer\u2019s precious comments and suggestions. We thank the reviewer for recognizing our work as helpful to broaden the study of adversarial examples and encourage a good deal of follow-up research. We will first provide the high-level motivation of why we need to generate adversarial examples and then answer the individual questions. We have revised the notations and equations in our updated manuscript.\n\nQ1. Why it is important to generate adversarial examples in the way they do?\n\nA1: Thanks for the question, the reasons/motivations are described below. \n\nDeep Neural Networks (DNNs) have achieved great success in a variety of applications. However, various security threats are emerging with the deployment of machine learning models. Without a deep understanding of how neural networks fail under attacks, it would be concerning to apply them in security-critical systems such as face verification and autonomous driving systems. Additionally, learning systems are usually required to be immune to *reasonable variations* of the input. \n\nSo far, such *variations* have been focused on imperceptible perturbation added to the given inputs whose magnitude is bounded by pixel-space $L_p$-norm. Some works have discussed the limitations of only measuring and evaluating the $L_p$ bounded perturbation [a1,a3,a4]. Therefore, it is important to explore other non-$L_p$ bounded perturbation, especially semantically meaningful perturbation, and more detailed reasons are listed below.\n\nFirst, the semantic based perturbation is new and interesting, which contains different intrinsic properties compared with the traditional $L_p$ bounded attacks. For instance, the semantic perturbation could be very large to cover the other side of $L_p$ bounded perturbation.\n\nSecond, in our proposed semantic based adversarial examples, we can explicitly control the desired editing attribute (e.g. hair color), and successfully preserve the high perceptual quality of the generated images as shown in Figure 4. This would help to explore the vulnerability/sensitivity of different semantic attributes. \n\nThird, various methods have been proposed to defend against adversarial attacks. Adversarial training based methods are currently the most efficient. Currently most adversarial training methods are only effective against a small set of seen attacks [a1], and researchers (e.g., Kang, et. al. [a2]) have shown that generating diverse attacks can help improve adversarial training performance against unseen attacks. Therefore, we believe that our semantic adversarial examples can potentially benefit adversarial training to improve model robustness by providing diverse unseen adversarial examples. \n\nIn addition, partially based on the reasons above, Brown, et. al.[a4] proposed the unrestricted adversarial example challenge to encourage the community to explore the adversarial space beyond $L_p$, which would potentially benefit the adversarial learning research, and we do hope SemanticAdv can contribute as well. \n\n\n(To be continued.)\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2237/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2237/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["haonanqiu@link.cuhk.edu.cn", "xiaocw@umich.edu", "yl016@ie.cuhk.edu.hk", "xcyan@umich.edu", "honglak@eecs.umich.edu", "lxbosky@gmail.com"], "title": "SemanticAdv: Generating Adversarial Examples via Attribute-Conditional Image Editing", "authors": ["Haonan Qiu", "Chaowei Xiao", "Lei Yang", "Xinchen Yan", "HongLak Lee", "Bo Li"], "pdf": "/pdf/f3fd4f5a5545c1c4236d566d6cdb91e5ad06d729.pdf", "TL;DR": "A novel and effective semantic adversarial attack method.", "abstract": "Deep neural networks (DNNs) have achieved great success in various applications due to their strong expressive power. However, recent studies have shown that DNNs are vulnerable to adversarial examples which are manipulated instances targeting to mislead DNNs to make incorrect predictions. Currently, most such adversarial examples try to guarantee \u201csubtle perturbation\" by limiting the Lp norm of the perturbation. In this paper, we aim to explore the impact of semantic manipulation on DNNs predictions by manipulating the semantic attributes of images and generate \u201cunrestricted adversarial examples\". Such semantic based perturbation is more practical compared with the Lp bounded perturbation. In particular, we propose an algorithm SemanticAdv which leverages disentangled semantic factors to generate adversarial perturbation by altering controlled semantic attributes to fool the learner towards various \u201cadversarial\" targets. We conduct extensive experiments to show that the semantic based adversarial examples can not only fool different learning tasks such as face verification and landmark detection, but also achieve high targeted attack success rate against real-world black-box services such as Azure face verification service based on transferability. To further demonstrate the applicability of SemanticAdv beyond face recognition domain, we also generate semantic perturbations on street-view images. Such adversarial examples with controlled semantic manipulation can shed light on further understanding about vulnerabilities of DNNs as well as potential defensive approaches.", "keywords": ["adversarial examples", "semantic attack"], "paperhash": "qiu|semanticadv_generating_adversarial_examples_via_attributeconditional_image_editing", "original_pdf": "/attachment/463f0dbb711de6a22e2b2925a653210743eb4913.pdf", "_bibtex": "@misc{\nqiu2020semanticadv,\ntitle={SemanticAdv: Generating Adversarial Examples via Attribute-Conditional Image Editing},\nauthor={Haonan Qiu and Chaowei Xiao and Lei Yang and Xinchen Yan and HongLak Lee and Bo Li},\nyear={2020},\nurl={https://openreview.net/forum?id=r1l-VeSKwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1l-VeSKwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2237/Authors", "ICLR.cc/2020/Conference/Paper2237/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2237/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2237/Reviewers", "ICLR.cc/2020/Conference/Paper2237/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2237/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2237/Authors|ICLR.cc/2020/Conference/Paper2237/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144327, "tmdate": 1576860545034, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2237/Authors", "ICLR.cc/2020/Conference/Paper2237/Reviewers", "ICLR.cc/2020/Conference/Paper2237/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2237/-/Official_Comment"}}}, {"id": "ryx3grU9iS", "original": null, "number": 13, "cdate": 1573704947908, "ddate": null, "tcdate": 1573704947908, "tmdate": 1573706441748, "tddate": null, "forum": "r1l-VeSKwS", "replyto": "rJlh8NU5sB", "invitation": "ICLR.cc/2020/Conference/Paper2237/-/Official_Comment", "content": {"title": "Continue", "comment": "\nQ3a: Is there a way to evaluate the merits of semantic modification (beyond attack success) in addition to \u201cdoes it look reasonable\u201d? \nA3a: As far as we know, user study has been widely used in the literature when it comes to the qualitative measurement of adversarial examples [a1]. Other measurement such as $L_p$ bound has been known drawbacks and limitations as discussed in [a1, a2, a3]. We admit it is non-trivial to devise perceptual metrics to measure the perceptual quality of adversarial samples in a systematic manner, and it is truly a challenging open problem in the vision and learning community. \n\nQ3b:The authors mention attribute-based modifications are more practical, how can this be evaluated? \nA3b: Thanks for the question and sorry for the confusion. In our scenario, \u201cmore practical\u201d means it is relatively easier for someone to realize semantic attributes in practice than perform $L_p$ based perturbation. For instance, one can realize the semantic attribute editing to the faceID system by wearing a pair of glasses or have the hair dyed with a different color.\n\nQ3c: If attribute-based attacks are better, is there a cost to this?\nA3c: Thanks for the interesting question! The extra costs of SemanticAdv are from two sources: (1) we need the corresponding attribute annotation for each image or a pre-trained attribute classifier to predict the attribute labels; and (2) we need to train a generative model to conduct attribute-conditional image editing.\nThese two problems happen to be popular research topics in the vision and learning community with tremendous progress in the past few years, which we can leverage.\n\nQ3d: How easy is it to make attribute-based attacks compared to low-level ones?\nA3d: First, we observe that generating the attribute-based attacks is as efficient as the low-level ones. We conduct additional experiments to evaluate the running time. The detailed setting can be found in Section A. It takes  on average 0.30s for CW to generate a single adversarial example on single GTX 1080Ti while the running time is 0.32s for our SemanticAdv. Besides the efficiency, we believe the model optimization of SemanticAdv is as easy as CW attack, except that SemanticAdv requires a pre-trained attribute-conditional image generation model available (see Q3c).\n\nQ4: Impact of \u201cselecting successfully attacked example\u201d to evaluate the transferability. \nA4: This is the standard setting in the literature [a1] when it comes to attack transferability evaluation. We will make this clear in the revision. \n\nQ5: The advantage of the proposed method.\nA5: Thanks for pointing this out. Our proposed method has three major advantages: \n(1) SemanticAdv helps identify specific semantic-based adversarial examples for a machine learning model (e.g., face verification network, scene segmentation network) to further explore corner cases in the representation; (2) as the reviewer points out, such semantic-based attacks can enlarge the diversity of seen adversarial examples and therefore help improve model robustness by training with them against unseen ones as discussed in [a4]; and (3) analyzing the defense effectiveness with SemanticAdv by modifying different attributes could help better understand the model vulnerabilities from semantic perspective. \n\nReferences\n[a1] \u201cSpatially transformed adversarial examples.\u201d Xiao et al, In ICLR 2018.\n[a2] \u201cWasserstein Adversarial Examples via Projected Sinkhorn Iterations.\u201d Wong et al. In ICML 2019\n[a3] \u201cUnrestricted adversarial examples.\u201d Brown et al. arXiv preprint arXiv:1809.08352 2018.\n[a4] \u201cTesting robustness against unforeseen adversaries.\u201d Kang et al. arXiv preprint arXiv:1908.08016 2019.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2237/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2237/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["haonanqiu@link.cuhk.edu.cn", "xiaocw@umich.edu", "yl016@ie.cuhk.edu.hk", "xcyan@umich.edu", "honglak@eecs.umich.edu", "lxbosky@gmail.com"], "title": "SemanticAdv: Generating Adversarial Examples via Attribute-Conditional Image Editing", "authors": ["Haonan Qiu", "Chaowei Xiao", "Lei Yang", "Xinchen Yan", "HongLak Lee", "Bo Li"], "pdf": "/pdf/f3fd4f5a5545c1c4236d566d6cdb91e5ad06d729.pdf", "TL;DR": "A novel and effective semantic adversarial attack method.", "abstract": "Deep neural networks (DNNs) have achieved great success in various applications due to their strong expressive power. However, recent studies have shown that DNNs are vulnerable to adversarial examples which are manipulated instances targeting to mislead DNNs to make incorrect predictions. Currently, most such adversarial examples try to guarantee \u201csubtle perturbation\" by limiting the Lp norm of the perturbation. In this paper, we aim to explore the impact of semantic manipulation on DNNs predictions by manipulating the semantic attributes of images and generate \u201cunrestricted adversarial examples\". Such semantic based perturbation is more practical compared with the Lp bounded perturbation. In particular, we propose an algorithm SemanticAdv which leverages disentangled semantic factors to generate adversarial perturbation by altering controlled semantic attributes to fool the learner towards various \u201cadversarial\" targets. We conduct extensive experiments to show that the semantic based adversarial examples can not only fool different learning tasks such as face verification and landmark detection, but also achieve high targeted attack success rate against real-world black-box services such as Azure face verification service based on transferability. To further demonstrate the applicability of SemanticAdv beyond face recognition domain, we also generate semantic perturbations on street-view images. Such adversarial examples with controlled semantic manipulation can shed light on further understanding about vulnerabilities of DNNs as well as potential defensive approaches.", "keywords": ["adversarial examples", "semantic attack"], "paperhash": "qiu|semanticadv_generating_adversarial_examples_via_attributeconditional_image_editing", "original_pdf": "/attachment/463f0dbb711de6a22e2b2925a653210743eb4913.pdf", "_bibtex": "@misc{\nqiu2020semanticadv,\ntitle={SemanticAdv: Generating Adversarial Examples via Attribute-Conditional Image Editing},\nauthor={Haonan Qiu and Chaowei Xiao and Lei Yang and Xinchen Yan and HongLak Lee and Bo Li},\nyear={2020},\nurl={https://openreview.net/forum?id=r1l-VeSKwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1l-VeSKwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2237/Authors", "ICLR.cc/2020/Conference/Paper2237/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2237/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2237/Reviewers", "ICLR.cc/2020/Conference/Paper2237/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2237/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2237/Authors|ICLR.cc/2020/Conference/Paper2237/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144327, "tmdate": 1576860545034, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2237/Authors", "ICLR.cc/2020/Conference/Paper2237/Reviewers", "ICLR.cc/2020/Conference/Paper2237/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2237/-/Official_Comment"}}}, {"id": "rke1SHLqiB", "original": null, "number": 14, "cdate": 1573705015038, "ddate": null, "tcdate": 1573705015038, "tmdate": 1573706123463, "tddate": null, "forum": "r1l-VeSKwS", "replyto": "rkxCcRlYtH", "invitation": "ICLR.cc/2020/Conference/Paper2237/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "\nWe thank the reviewer for the constructive suggestions and comments, and we have conducted additional experiments based on the comments.\n\nQ1. Effectiveness of using other layers?\n\nWe have tested another two feature maps ($f_{1}$, $f_{2}$) after the first/second up-sampling operations as shown in Table E (see Section D in our appendix) in the submitted paper; and we also conducted additional experiments on two extra feature maps ($f_{-2}$, $f_{-1}$) based on the suggestions. $f_{-2}$ indicates the first feature map after the last down-sampling operations and $f_{-1}$ represents the feature map after $f_{-2}$. The full results are shown in the revision Table E and F.\nWe also present the results as below. The result shows that samples generated by interpolating on our selected layer ($f_0$) achieve the highest attack success rate. \n\n+\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\n| T-FPR(G-FPR)              |          $10^{\u22123}(10^{\u22123})$       |  $3\\times10^{\u22123}(3\\times10^{\u22123})$ |         $10^{\u22124}(10^{\u22124})$        |\n+\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\n| Layer(f)                        | $f_{-2}$    | $f_{-1}$    | $f_0$      | $f_{-2}$    | $f_{-1}$    |   $f_{0}$    | $f_{-2}$    | $f_{-1}$    | $f_{0}$      |\n+\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\n| Attack Success Rate  | 49.4   | 92.09 | 99.29  | 30.44 | 81.87 | 97.35 | 6.66   | 45.46 | 76.64 |\n+\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2237/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2237/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["haonanqiu@link.cuhk.edu.cn", "xiaocw@umich.edu", "yl016@ie.cuhk.edu.hk", "xcyan@umich.edu", "honglak@eecs.umich.edu", "lxbosky@gmail.com"], "title": "SemanticAdv: Generating Adversarial Examples via Attribute-Conditional Image Editing", "authors": ["Haonan Qiu", "Chaowei Xiao", "Lei Yang", "Xinchen Yan", "HongLak Lee", "Bo Li"], "pdf": "/pdf/f3fd4f5a5545c1c4236d566d6cdb91e5ad06d729.pdf", "TL;DR": "A novel and effective semantic adversarial attack method.", "abstract": "Deep neural networks (DNNs) have achieved great success in various applications due to their strong expressive power. However, recent studies have shown that DNNs are vulnerable to adversarial examples which are manipulated instances targeting to mislead DNNs to make incorrect predictions. Currently, most such adversarial examples try to guarantee \u201csubtle perturbation\" by limiting the Lp norm of the perturbation. In this paper, we aim to explore the impact of semantic manipulation on DNNs predictions by manipulating the semantic attributes of images and generate \u201cunrestricted adversarial examples\". Such semantic based perturbation is more practical compared with the Lp bounded perturbation. In particular, we propose an algorithm SemanticAdv which leverages disentangled semantic factors to generate adversarial perturbation by altering controlled semantic attributes to fool the learner towards various \u201cadversarial\" targets. We conduct extensive experiments to show that the semantic based adversarial examples can not only fool different learning tasks such as face verification and landmark detection, but also achieve high targeted attack success rate against real-world black-box services such as Azure face verification service based on transferability. To further demonstrate the applicability of SemanticAdv beyond face recognition domain, we also generate semantic perturbations on street-view images. Such adversarial examples with controlled semantic manipulation can shed light on further understanding about vulnerabilities of DNNs as well as potential defensive approaches.", "keywords": ["adversarial examples", "semantic attack"], "paperhash": "qiu|semanticadv_generating_adversarial_examples_via_attributeconditional_image_editing", "original_pdf": "/attachment/463f0dbb711de6a22e2b2925a653210743eb4913.pdf", "_bibtex": "@misc{\nqiu2020semanticadv,\ntitle={SemanticAdv: Generating Adversarial Examples via Attribute-Conditional Image Editing},\nauthor={Haonan Qiu and Chaowei Xiao and Lei Yang and Xinchen Yan and HongLak Lee and Bo Li},\nyear={2020},\nurl={https://openreview.net/forum?id=r1l-VeSKwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1l-VeSKwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2237/Authors", "ICLR.cc/2020/Conference/Paper2237/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2237/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2237/Reviewers", "ICLR.cc/2020/Conference/Paper2237/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2237/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2237/Authors|ICLR.cc/2020/Conference/Paper2237/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144327, "tmdate": 1576860545034, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2237/Authors", "ICLR.cc/2020/Conference/Paper2237/Reviewers", "ICLR.cc/2020/Conference/Paper2237/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2237/-/Official_Comment"}}}, {"id": "SkxDlf-ttr", "original": null, "number": 2, "cdate": 1571521007248, "ddate": null, "tcdate": 1571521007248, "tmdate": 1572972365077, "tddate": null, "forum": "r1l-VeSKwS", "replyto": "r1l-VeSKwS", "invitation": "ICLR.cc/2020/Conference/Paper2237/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The authors describe a method for adversarially modifying a given (test) example that 1) still retains the correct label on the example, but 2) causes a model to make an incorrect prediction on it. The novelty of their proposed method is that their adversarial modifications are along a provided semantic axis (e.g., changing the color of someone's skin in a face recognition task) instead of the standard $L_p$ perturbations that the existing literature has focused on (e.g., making a very small change to each individual pixel). The adversarial examples that the authors construct, experimentally, are impressive and striking. I'd especially like to acknowledge the work that the authors put in to construct an anonymous link where they showcase results from their experiments. Thank you!\n\nOverall, I think that this is interesting work that can help to broaden the study of adversarial examples and make them more applicable even in non-adversarial settings (e.g., by making models more robust to the changes in semantic attributes that the authors consider). There has been quite a bit of interest in the community in adversarial examples that are not just $L_p$ perturbations, and I believe that the authors' approach will encourage a good deal of follow-up research. \n\nHowever, my main concern with the paper is that in my opinion, it does not sufficiently address why it is important to generate adversarial examples in the way they do. For example:\n\n1) Is the argument that this is a more powerful attack surface, so adversaries should take note (and defenders should figure out how to defend against this)? If that is the case, what is the attack model under which these attacks are realistic? For example, the original $L_\\infty$ attacks are motivated in the sense that the adversarial examples are visually imperceptible, so they might not be noticed by the end-user. What is the equivalent argument for these semantic attacks?\n\n2) Is the argument that these semantic attacks somehow capture a more realistic part of the data distribution over all natural images, and therefore it is good to have models that perform well on these semantic adversarial examples even if we're not concerned about an adversary (e.g., because the model might generalize better to other tasks or be more causally correct)? If that's the case, then I think this needs to be explored more. For example, what about the following straw man baseline: use a controllable semantic-attribute-based generator to generate semantically different images without any notion of an adversarial attack, and then do standard $L_p$ attacks on that generated image? How would that be better or worse than the proposed method?\n\n3) Or is the argument that it is just good to be able to generate examples that models get wrong? If so, why, and why is this method better than other methods?\n\nI think the paper would be significantly stronger if the importance and implications of their work were explicated along the above lines. For this reason, my current assessment is a weak reject, though I'd be open to changing this assessment.\n\n=== Less critical comments, no need to respond or fix right away ===\n\nWhile the overall concept and approach was clear, I generally found the notation and mathematical exposition difficult to follow. Please be more precise. Here is a non-exhaustive list of examples from section 3:\n\na) I'm not sure what's the difference between $x^\\text{tgt}$ and $x^\\text{adv}$, or between $x^\\text{new}$ and  $x^*$. These seem to be used somewhat interchangeably?\n\nb) Equation 3 is the central optimization problem in the paper, and should be written out explicitly using $\\alpha$ as the optimization variable, instead of referring to equations 1 and 2 (in which $x^*$ doesn't even appear).\n\nc) I didn't understand equation 4. What does assuming $M(x^\\text{tgt}) = y^\\text{tgt}$ mean? What happens when that is not true?\n\nd) Equation 5: Why is $y$ in the right hand side by not in the left?\n\ne) Equation 6: $L_\\text{smooth}$ is missing an argument.\n\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2237/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2237/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["haonanqiu@link.cuhk.edu.cn", "xiaocw@umich.edu", "yl016@ie.cuhk.edu.hk", "xcyan@umich.edu", "honglak@eecs.umich.edu", "lxbosky@gmail.com"], "title": "SemanticAdv: Generating Adversarial Examples via Attribute-Conditional Image Editing", "authors": ["Haonan Qiu", "Chaowei Xiao", "Lei Yang", "Xinchen Yan", "HongLak Lee", "Bo Li"], "pdf": "/pdf/f3fd4f5a5545c1c4236d566d6cdb91e5ad06d729.pdf", "TL;DR": "A novel and effective semantic adversarial attack method.", "abstract": "Deep neural networks (DNNs) have achieved great success in various applications due to their strong expressive power. However, recent studies have shown that DNNs are vulnerable to adversarial examples which are manipulated instances targeting to mislead DNNs to make incorrect predictions. Currently, most such adversarial examples try to guarantee \u201csubtle perturbation\" by limiting the Lp norm of the perturbation. In this paper, we aim to explore the impact of semantic manipulation on DNNs predictions by manipulating the semantic attributes of images and generate \u201cunrestricted adversarial examples\". Such semantic based perturbation is more practical compared with the Lp bounded perturbation. In particular, we propose an algorithm SemanticAdv which leverages disentangled semantic factors to generate adversarial perturbation by altering controlled semantic attributes to fool the learner towards various \u201cadversarial\" targets. We conduct extensive experiments to show that the semantic based adversarial examples can not only fool different learning tasks such as face verification and landmark detection, but also achieve high targeted attack success rate against real-world black-box services such as Azure face verification service based on transferability. To further demonstrate the applicability of SemanticAdv beyond face recognition domain, we also generate semantic perturbations on street-view images. Such adversarial examples with controlled semantic manipulation can shed light on further understanding about vulnerabilities of DNNs as well as potential defensive approaches.", "keywords": ["adversarial examples", "semantic attack"], "paperhash": "qiu|semanticadv_generating_adversarial_examples_via_attributeconditional_image_editing", "original_pdf": "/attachment/463f0dbb711de6a22e2b2925a653210743eb4913.pdf", "_bibtex": "@misc{\nqiu2020semanticadv,\ntitle={SemanticAdv: Generating Adversarial Examples via Attribute-Conditional Image Editing},\nauthor={Haonan Qiu and Chaowei Xiao and Lei Yang and Xinchen Yan and HongLak Lee and Bo Li},\nyear={2020},\nurl={https://openreview.net/forum?id=r1l-VeSKwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1l-VeSKwS", "replyto": "r1l-VeSKwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2237/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2237/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575408058459, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2237/Reviewers"], "noninvitees": [], "tcdate": 1570237725734, "tmdate": 1575408058483, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2237/-/Official_Review"}}}, {"id": "rkxCcRlYtH", "original": null, "number": 1, "cdate": 1571520150320, "ddate": null, "tcdate": 1571520150320, "tmdate": 1572972365043, "tddate": null, "forum": "r1l-VeSKwS", "replyto": "r1l-VeSKwS", "invitation": "ICLR.cc/2020/Conference/Paper2237/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary:\nThis paper proposes to generate \"unrestricted adversarial examples\" via attribute-conditional image editing. Their method, SemanticAdv, leverages disentangled semantic factors and interpolates feature-map with higher freedom than attribute-space. Their adversarial optimization objectives combine both attack effectiveness and interpolation smoothness. They conduct extensive experiments for several tasks compared with CW-attack, showing broad applicability of the proposed method.\n\nThe paper is well written and technically sound with concrete experimental results. I'm glad to suggest accepting the paper.\n\nWith the help of attribute-conditional StarGAN, SemanticAdv generates adversarial examples by interpolating feature-maps conditioned on attributes. They design adversarial optimization objectives with specific attack objectives for identity verification and structured prediction tasks. They provide experiments showing the effectiveness of SemanticAdv; analysis on attributes, attack transferability, black-box attack, and robustness against defenses; as well as user study with subjective. The qualitative results also look nice and the code base is open-sourced.\n\nA question out of curiosity, the last conv layer in the generator is used as the feature-map. How is the attack effectiveness of using other layers?"}, "signatures": ["ICLR.cc/2020/Conference/Paper2237/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2237/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["haonanqiu@link.cuhk.edu.cn", "xiaocw@umich.edu", "yl016@ie.cuhk.edu.hk", "xcyan@umich.edu", "honglak@eecs.umich.edu", "lxbosky@gmail.com"], "title": "SemanticAdv: Generating Adversarial Examples via Attribute-Conditional Image Editing", "authors": ["Haonan Qiu", "Chaowei Xiao", "Lei Yang", "Xinchen Yan", "HongLak Lee", "Bo Li"], "pdf": "/pdf/f3fd4f5a5545c1c4236d566d6cdb91e5ad06d729.pdf", "TL;DR": "A novel and effective semantic adversarial attack method.", "abstract": "Deep neural networks (DNNs) have achieved great success in various applications due to their strong expressive power. However, recent studies have shown that DNNs are vulnerable to adversarial examples which are manipulated instances targeting to mislead DNNs to make incorrect predictions. Currently, most such adversarial examples try to guarantee \u201csubtle perturbation\" by limiting the Lp norm of the perturbation. In this paper, we aim to explore the impact of semantic manipulation on DNNs predictions by manipulating the semantic attributes of images and generate \u201cunrestricted adversarial examples\". Such semantic based perturbation is more practical compared with the Lp bounded perturbation. In particular, we propose an algorithm SemanticAdv which leverages disentangled semantic factors to generate adversarial perturbation by altering controlled semantic attributes to fool the learner towards various \u201cadversarial\" targets. We conduct extensive experiments to show that the semantic based adversarial examples can not only fool different learning tasks such as face verification and landmark detection, but also achieve high targeted attack success rate against real-world black-box services such as Azure face verification service based on transferability. To further demonstrate the applicability of SemanticAdv beyond face recognition domain, we also generate semantic perturbations on street-view images. Such adversarial examples with controlled semantic manipulation can shed light on further understanding about vulnerabilities of DNNs as well as potential defensive approaches.", "keywords": ["adversarial examples", "semantic attack"], "paperhash": "qiu|semanticadv_generating_adversarial_examples_via_attributeconditional_image_editing", "original_pdf": "/attachment/463f0dbb711de6a22e2b2925a653210743eb4913.pdf", "_bibtex": "@misc{\nqiu2020semanticadv,\ntitle={SemanticAdv: Generating Adversarial Examples via Attribute-Conditional Image Editing},\nauthor={Haonan Qiu and Chaowei Xiao and Lei Yang and Xinchen Yan and HongLak Lee and Bo Li},\nyear={2020},\nurl={https://openreview.net/forum?id=r1l-VeSKwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1l-VeSKwS", "replyto": "r1l-VeSKwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2237/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2237/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575408058459, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2237/Reviewers"], "noninvitees": [], "tcdate": 1570237725734, "tmdate": 1575408058483, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2237/-/Official_Review"}}}, {"id": "H1lWBP4oFH", "original": null, "number": 3, "cdate": 1571665720735, "ddate": null, "tcdate": 1571665720735, "tmdate": 1572972365008, "tddate": null, "forum": "r1l-VeSKwS", "replyto": "r1l-VeSKwS", "invitation": "ICLR.cc/2020/Conference/Paper2237/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposes adversarial attacks by modifying semantic properties of the image. Rather than modifying low-level pixels, it modifies mid-level attributes. The authors show that the proposed method is effective and achieves stronger results than the pixel-level attack method (CW) in terms of attacking capability transferring to other architectures. Importantly, the authors show results on a variety of tasks, e.g. landmark detection and segmentation in addition to classification/identification. The most related work is Joshi 2019 and the authors show that the method used in that work (modification in attribute space) is inferior to modification in feature space still via attributes, as the authors proposed. However, I have a few comments and concerns:\n1) The authors mention on page 3 they assume M is an oracle-- what is the impact of this?\n2) The results in Table C don't look good-- the proposed method can *at best* (in a generous setup) equal the results of CW-- maybe I missed something but more discussion would be helpful.\n3) Is there a way to evaluate the merits of semantic modification (beyond attack success) in addition to \"does it look reasonable\"? The authors mention attribute-based modifications are more practical, how can this be evaluated? If attribute-based attacks are better, is there a cost to this? How easy is it to make attribute-based attacks compared to low-level ones?\n4) The authors mention that for their transferrability results, they \"select the successfully attacked...\" (page 7). What is the impact of this, as opposed to selecting non-successfully attacked samples?\n5) Re: behavior with defense methods, is the advantage of the proposed method a matter of training the defense methods in a tailored way, so they're aware of attribute-based attacks?"}, "signatures": ["ICLR.cc/2020/Conference/Paper2237/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2237/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["haonanqiu@link.cuhk.edu.cn", "xiaocw@umich.edu", "yl016@ie.cuhk.edu.hk", "xcyan@umich.edu", "honglak@eecs.umich.edu", "lxbosky@gmail.com"], "title": "SemanticAdv: Generating Adversarial Examples via Attribute-Conditional Image Editing", "authors": ["Haonan Qiu", "Chaowei Xiao", "Lei Yang", "Xinchen Yan", "HongLak Lee", "Bo Li"], "pdf": "/pdf/f3fd4f5a5545c1c4236d566d6cdb91e5ad06d729.pdf", "TL;DR": "A novel and effective semantic adversarial attack method.", "abstract": "Deep neural networks (DNNs) have achieved great success in various applications due to their strong expressive power. However, recent studies have shown that DNNs are vulnerable to adversarial examples which are manipulated instances targeting to mislead DNNs to make incorrect predictions. Currently, most such adversarial examples try to guarantee \u201csubtle perturbation\" by limiting the Lp norm of the perturbation. In this paper, we aim to explore the impact of semantic manipulation on DNNs predictions by manipulating the semantic attributes of images and generate \u201cunrestricted adversarial examples\". Such semantic based perturbation is more practical compared with the Lp bounded perturbation. In particular, we propose an algorithm SemanticAdv which leverages disentangled semantic factors to generate adversarial perturbation by altering controlled semantic attributes to fool the learner towards various \u201cadversarial\" targets. We conduct extensive experiments to show that the semantic based adversarial examples can not only fool different learning tasks such as face verification and landmark detection, but also achieve high targeted attack success rate against real-world black-box services such as Azure face verification service based on transferability. To further demonstrate the applicability of SemanticAdv beyond face recognition domain, we also generate semantic perturbations on street-view images. Such adversarial examples with controlled semantic manipulation can shed light on further understanding about vulnerabilities of DNNs as well as potential defensive approaches.", "keywords": ["adversarial examples", "semantic attack"], "paperhash": "qiu|semanticadv_generating_adversarial_examples_via_attributeconditional_image_editing", "original_pdf": "/attachment/463f0dbb711de6a22e2b2925a653210743eb4913.pdf", "_bibtex": "@misc{\nqiu2020semanticadv,\ntitle={SemanticAdv: Generating Adversarial Examples via Attribute-Conditional Image Editing},\nauthor={Haonan Qiu and Chaowei Xiao and Lei Yang and Xinchen Yan and HongLak Lee and Bo Li},\nyear={2020},\nurl={https://openreview.net/forum?id=r1l-VeSKwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1l-VeSKwS", "replyto": "r1l-VeSKwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2237/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2237/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575408058459, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2237/Reviewers"], "noninvitees": [], "tcdate": 1570237725734, "tmdate": 1575408058483, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2237/-/Official_Review"}}}, {"id": "S1e7sXjM5H", "original": null, "number": 3, "cdate": 1572152218681, "ddate": null, "tcdate": 1572152218681, "tmdate": 1572805918875, "tddate": null, "forum": "r1l-VeSKwS", "replyto": "Hke-61rAFH", "invitation": "ICLR.cc/2020/Conference/Paper2237/-/Official_Comment", "content": {"title": "Reply to \"generalization of semantic perturbations\".", "comment": "Thank you for your interest!\n\nIn our experiments, we observe that our proposed semantic perturbation is generalizable to some extent. Specifically, the same type of semantic perturbation can be applied to attack different samples (faces). Also, feel free to check our anonymous website where we show good examples of applying the same type of perturbations (e.g., young --> senior, regular skin --> pale skin) to synthesize adversarial face images against the verification model (see Figure 2 on the anonymous website).\n\nhttps://sites.google.com/view/generate-semantic-adv-example\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2237/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2237/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["haonanqiu@link.cuhk.edu.cn", "xiaocw@umich.edu", "yl016@ie.cuhk.edu.hk", "xcyan@umich.edu", "honglak@eecs.umich.edu", "lxbosky@gmail.com"], "title": "SemanticAdv: Generating Adversarial Examples via Attribute-Conditional Image Editing", "authors": ["Haonan Qiu", "Chaowei Xiao", "Lei Yang", "Xinchen Yan", "HongLak Lee", "Bo Li"], "pdf": "/pdf/f3fd4f5a5545c1c4236d566d6cdb91e5ad06d729.pdf", "TL;DR": "A novel and effective semantic adversarial attack method.", "abstract": "Deep neural networks (DNNs) have achieved great success in various applications due to their strong expressive power. However, recent studies have shown that DNNs are vulnerable to adversarial examples which are manipulated instances targeting to mislead DNNs to make incorrect predictions. Currently, most such adversarial examples try to guarantee \u201csubtle perturbation\" by limiting the Lp norm of the perturbation. In this paper, we aim to explore the impact of semantic manipulation on DNNs predictions by manipulating the semantic attributes of images and generate \u201cunrestricted adversarial examples\". Such semantic based perturbation is more practical compared with the Lp bounded perturbation. In particular, we propose an algorithm SemanticAdv which leverages disentangled semantic factors to generate adversarial perturbation by altering controlled semantic attributes to fool the learner towards various \u201cadversarial\" targets. We conduct extensive experiments to show that the semantic based adversarial examples can not only fool different learning tasks such as face verification and landmark detection, but also achieve high targeted attack success rate against real-world black-box services such as Azure face verification service based on transferability. To further demonstrate the applicability of SemanticAdv beyond face recognition domain, we also generate semantic perturbations on street-view images. Such adversarial examples with controlled semantic manipulation can shed light on further understanding about vulnerabilities of DNNs as well as potential defensive approaches.", "keywords": ["adversarial examples", "semantic attack"], "paperhash": "qiu|semanticadv_generating_adversarial_examples_via_attributeconditional_image_editing", "original_pdf": "/attachment/463f0dbb711de6a22e2b2925a653210743eb4913.pdf", "_bibtex": "@misc{\nqiu2020semanticadv,\ntitle={SemanticAdv: Generating Adversarial Examples via Attribute-Conditional Image Editing},\nauthor={Haonan Qiu and Chaowei Xiao and Lei Yang and Xinchen Yan and HongLak Lee and Bo Li},\nyear={2020},\nurl={https://openreview.net/forum?id=r1l-VeSKwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1l-VeSKwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2237/Authors", "ICLR.cc/2020/Conference/Paper2237/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2237/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2237/Reviewers", "ICLR.cc/2020/Conference/Paper2237/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2237/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2237/Authors|ICLR.cc/2020/Conference/Paper2237/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144327, "tmdate": 1576860545034, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2237/Authors", "ICLR.cc/2020/Conference/Paper2237/Reviewers", "ICLR.cc/2020/Conference/Paper2237/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2237/-/Official_Comment"}}}, {"id": "Hke-61rAFH", "original": null, "number": 2, "cdate": 1571864504609, "ddate": null, "tcdate": 1571864504609, "tmdate": 1571864597388, "tddate": null, "forum": "r1l-VeSKwS", "replyto": "r1l-VeSKwS", "invitation": "ICLR.cc/2020/Conference/Paper2237/-/Public_Comment", "content": {"title": "Interesting work", "comment": "Hi, \n\nThe idea of semantic manipulation is pretty interesting. I wonder how these perturbations generalize? For example in Figure 1 Right, the perturbation is supposed to make a face pale and fool a face verification system. Would the same perturbation still work for a different face?"}, "signatures": ["~Zhenhua_Chen1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Zhenhua_Chen1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["haonanqiu@link.cuhk.edu.cn", "xiaocw@umich.edu", "yl016@ie.cuhk.edu.hk", "xcyan@umich.edu", "honglak@eecs.umich.edu", "lxbosky@gmail.com"], "title": "SemanticAdv: Generating Adversarial Examples via Attribute-Conditional Image Editing", "authors": ["Haonan Qiu", "Chaowei Xiao", "Lei Yang", "Xinchen Yan", "HongLak Lee", "Bo Li"], "pdf": "/pdf/f3fd4f5a5545c1c4236d566d6cdb91e5ad06d729.pdf", "TL;DR": "A novel and effective semantic adversarial attack method.", "abstract": "Deep neural networks (DNNs) have achieved great success in various applications due to their strong expressive power. However, recent studies have shown that DNNs are vulnerable to adversarial examples which are manipulated instances targeting to mislead DNNs to make incorrect predictions. Currently, most such adversarial examples try to guarantee \u201csubtle perturbation\" by limiting the Lp norm of the perturbation. In this paper, we aim to explore the impact of semantic manipulation on DNNs predictions by manipulating the semantic attributes of images and generate \u201cunrestricted adversarial examples\". Such semantic based perturbation is more practical compared with the Lp bounded perturbation. In particular, we propose an algorithm SemanticAdv which leverages disentangled semantic factors to generate adversarial perturbation by altering controlled semantic attributes to fool the learner towards various \u201cadversarial\" targets. We conduct extensive experiments to show that the semantic based adversarial examples can not only fool different learning tasks such as face verification and landmark detection, but also achieve high targeted attack success rate against real-world black-box services such as Azure face verification service based on transferability. To further demonstrate the applicability of SemanticAdv beyond face recognition domain, we also generate semantic perturbations on street-view images. Such adversarial examples with controlled semantic manipulation can shed light on further understanding about vulnerabilities of DNNs as well as potential defensive approaches.", "keywords": ["adversarial examples", "semantic attack"], "paperhash": "qiu|semanticadv_generating_adversarial_examples_via_attributeconditional_image_editing", "original_pdf": "/attachment/463f0dbb711de6a22e2b2925a653210743eb4913.pdf", "_bibtex": "@misc{\nqiu2020semanticadv,\ntitle={SemanticAdv: Generating Adversarial Examples via Attribute-Conditional Image Editing},\nauthor={Haonan Qiu and Chaowei Xiao and Lei Yang and Xinchen Yan and HongLak Lee and Bo Li},\nyear={2020},\nurl={https://openreview.net/forum?id=r1l-VeSKwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1l-VeSKwS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504183166, "tmdate": 1576860578398, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2237/Authors", "ICLR.cc/2020/Conference/Paper2237/Reviewers", "ICLR.cc/2020/Conference/Paper2237/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2237/-/Public_Comment"}}}, {"id": "SJeZgu5juS", "original": null, "number": 2, "cdate": 1570641896727, "ddate": null, "tcdate": 1570641896727, "tmdate": 1570641896727, "tddate": null, "forum": "r1l-VeSKwS", "replyto": "ByeBgbotOr", "invitation": "ICLR.cc/2020/Conference/Paper2237/-/Official_Comment", "content": {"comment": "Thanks for the nice reference! \nWe mainly valued the mentioned paper based on its physical attack effectiveness, but we will definitely add this interesting discussion about it!\n\n", "title": "reply to \"A closely related paper\""}, "signatures": ["ICLR.cc/2020/Conference/Paper2237/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2237/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["haonanqiu@link.cuhk.edu.cn", "xiaocw@umich.edu", "yl016@ie.cuhk.edu.hk", "xcyan@umich.edu", "honglak@eecs.umich.edu", "lxbosky@gmail.com"], "title": "SemanticAdv: Generating Adversarial Examples via Attribute-Conditional Image Editing", "authors": ["Haonan Qiu", "Chaowei Xiao", "Lei Yang", "Xinchen Yan", "HongLak Lee", "Bo Li"], "pdf": "/pdf/f3fd4f5a5545c1c4236d566d6cdb91e5ad06d729.pdf", "TL;DR": "A novel and effective semantic adversarial attack method.", "abstract": "Deep neural networks (DNNs) have achieved great success in various applications due to their strong expressive power. However, recent studies have shown that DNNs are vulnerable to adversarial examples which are manipulated instances targeting to mislead DNNs to make incorrect predictions. Currently, most such adversarial examples try to guarantee \u201csubtle perturbation\" by limiting the Lp norm of the perturbation. In this paper, we aim to explore the impact of semantic manipulation on DNNs predictions by manipulating the semantic attributes of images and generate \u201cunrestricted adversarial examples\". Such semantic based perturbation is more practical compared with the Lp bounded perturbation. In particular, we propose an algorithm SemanticAdv which leverages disentangled semantic factors to generate adversarial perturbation by altering controlled semantic attributes to fool the learner towards various \u201cadversarial\" targets. We conduct extensive experiments to show that the semantic based adversarial examples can not only fool different learning tasks such as face verification and landmark detection, but also achieve high targeted attack success rate against real-world black-box services such as Azure face verification service based on transferability. To further demonstrate the applicability of SemanticAdv beyond face recognition domain, we also generate semantic perturbations on street-view images. Such adversarial examples with controlled semantic manipulation can shed light on further understanding about vulnerabilities of DNNs as well as potential defensive approaches.", "keywords": ["adversarial examples", "semantic attack"], "paperhash": "qiu|semanticadv_generating_adversarial_examples_via_attributeconditional_image_editing", "original_pdf": "/attachment/463f0dbb711de6a22e2b2925a653210743eb4913.pdf", "_bibtex": "@misc{\nqiu2020semanticadv,\ntitle={SemanticAdv: Generating Adversarial Examples via Attribute-Conditional Image Editing},\nauthor={Haonan Qiu and Chaowei Xiao and Lei Yang and Xinchen Yan and HongLak Lee and Bo Li},\nyear={2020},\nurl={https://openreview.net/forum?id=r1l-VeSKwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1l-VeSKwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2237/Authors", "ICLR.cc/2020/Conference/Paper2237/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2237/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2237/Reviewers", "ICLR.cc/2020/Conference/Paper2237/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2237/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2237/Authors|ICLR.cc/2020/Conference/Paper2237/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144327, "tmdate": 1576860545034, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2237/Authors", "ICLR.cc/2020/Conference/Paper2237/Reviewers", "ICLR.cc/2020/Conference/Paper2237/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2237/-/Official_Comment"}}}, {"id": "ByeBgbotOr", "original": null, "number": 1, "cdate": 1570513132910, "ddate": null, "tcdate": 1570513132910, "tmdate": 1570513132910, "tddate": null, "forum": "r1l-VeSKwS", "replyto": "r1l-VeSKwS", "invitation": "ICLR.cc/2020/Conference/Paper2237/-/Public_Comment", "content": {"comment": "Great work and I really enjoy reading it.\n\nHowever, previous work has also studied the semantic attack to fool models. Please check out this paper [1]. For the attack of semantic attributes, to my knowledge, [1] is the first work to perform the semantic attack to fool DNNs by designing specific eyeglasses.\n\nIn my opinion, a discussion/comparison seems due.\n\n[1] Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition\n", "title": "A closely related paper "}, "signatures": ["~Anthony_Wittmer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Anthony_Wittmer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["haonanqiu@link.cuhk.edu.cn", "xiaocw@umich.edu", "yl016@ie.cuhk.edu.hk", "xcyan@umich.edu", "honglak@eecs.umich.edu", "lxbosky@gmail.com"], "title": "SemanticAdv: Generating Adversarial Examples via Attribute-Conditional Image Editing", "authors": ["Haonan Qiu", "Chaowei Xiao", "Lei Yang", "Xinchen Yan", "HongLak Lee", "Bo Li"], "pdf": "/pdf/f3fd4f5a5545c1c4236d566d6cdb91e5ad06d729.pdf", "TL;DR": "A novel and effective semantic adversarial attack method.", "abstract": "Deep neural networks (DNNs) have achieved great success in various applications due to their strong expressive power. However, recent studies have shown that DNNs are vulnerable to adversarial examples which are manipulated instances targeting to mislead DNNs to make incorrect predictions. Currently, most such adversarial examples try to guarantee \u201csubtle perturbation\" by limiting the Lp norm of the perturbation. In this paper, we aim to explore the impact of semantic manipulation on DNNs predictions by manipulating the semantic attributes of images and generate \u201cunrestricted adversarial examples\". Such semantic based perturbation is more practical compared with the Lp bounded perturbation. In particular, we propose an algorithm SemanticAdv which leverages disentangled semantic factors to generate adversarial perturbation by altering controlled semantic attributes to fool the learner towards various \u201cadversarial\" targets. We conduct extensive experiments to show that the semantic based adversarial examples can not only fool different learning tasks such as face verification and landmark detection, but also achieve high targeted attack success rate against real-world black-box services such as Azure face verification service based on transferability. To further demonstrate the applicability of SemanticAdv beyond face recognition domain, we also generate semantic perturbations on street-view images. Such adversarial examples with controlled semantic manipulation can shed light on further understanding about vulnerabilities of DNNs as well as potential defensive approaches.", "keywords": ["adversarial examples", "semantic attack"], "paperhash": "qiu|semanticadv_generating_adversarial_examples_via_attributeconditional_image_editing", "original_pdf": "/attachment/463f0dbb711de6a22e2b2925a653210743eb4913.pdf", "_bibtex": "@misc{\nqiu2020semanticadv,\ntitle={SemanticAdv: Generating Adversarial Examples via Attribute-Conditional Image Editing},\nauthor={Haonan Qiu and Chaowei Xiao and Lei Yang and Xinchen Yan and HongLak Lee and Bo Li},\nyear={2020},\nurl={https://openreview.net/forum?id=r1l-VeSKwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1l-VeSKwS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504183166, "tmdate": 1576860578398, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2237/Authors", "ICLR.cc/2020/Conference/Paper2237/Reviewers", "ICLR.cc/2020/Conference/Paper2237/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2237/-/Public_Comment"}}}], "count": 16}