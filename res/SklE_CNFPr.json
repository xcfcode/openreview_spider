{"notes": [{"id": "SklE_CNFPr", "original": "BkxIOhD_Dr", "number": 1207, "cdate": 1569439339775, "ddate": null, "tcdate": 1569439339775, "tmdate": 1577168232389, "tddate": null, "forum": "SklE_CNFPr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["jimwang@gatech.edu", "xl374@scarletmail.rutgers.edu", "liping11@baidu.com"], "title": "Zeroth Order Optimization by a Mixture of Evolution Strategies", "authors": ["Jun-Kun Wang", "Xiaoyun Li", "Ping Li"], "pdf": "/pdf/f333e090161c8ee9f3feb4124758a76dc30ef134.pdf", "abstract": "Evolution strategies or zeroth-order optimization algorithms have become popular in some areas of optimization and machine learning where only the oracle of function value evaluations is available. The central idea in the design of the algorithms is by querying function values of some perturbed points in the neighborhood of the current update and constructing a pseudo-gradient using the function values. In recent years, there is a growing interest in developing new ways of perturbation. Though the new perturbation methods are well motivating, most of them are criticized for lack of convergence guarantees even when the underlying function is convex. Perhaps the only methods that enjoy convergence guarantees are the ones that sample the perturbed points uniformly from a unit sphere or from a multivariate Gaussian distribution with an isotropic covariance. In this work, we tackle the non-convergence issue and propose sampling perturbed points from a mixture of distributions. Experiments show that our proposed method can identify the best perturbation scheme for the convergence and might also help to leverage the complementariness of different perturbation schemes.\n", "code": "https://www.dropbox.com/s/e3pzv5581e3x8ku/codes.zip?dl=0", "keywords": [], "paperhash": "wang|zeroth_order_optimization_by_a_mixture_of_evolution_strategies", "original_pdf": "/attachment/f333e090161c8ee9f3feb4124758a76dc30ef134.pdf", "_bibtex": "@misc{\nwang2020zeroth,\ntitle={Zeroth Order Optimization by a Mixture of Evolution Strategies},\nauthor={Jun-Kun Wang and Xiaoyun Li and Ping Li},\nyear={2020},\nurl={https://openreview.net/forum?id=SklE_CNFPr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "PN7vqUphQ6", "original": null, "number": 1, "cdate": 1576798717411, "ddate": null, "tcdate": 1576798717411, "tmdate": 1576800919127, "tddate": null, "forum": "SklE_CNFPr", "replyto": "SklE_CNFPr", "invitation": "ICLR.cc/2020/Conference/Paper1207/-/Decision", "content": {"decision": "Reject", "comment": "The paper proposes an adaptive sampling mechanism for zeroth order optimization that samples perturbed points from a mixture distribution with asymptotic convergence guarantees. The reviewers raised issues regarding the clarity of presentation, potential problems with the proofs, and simplicity of the experimental setup. The authors did not provide a response. Overall, the reviewers agree that the quality of the paper is not sufficient for publishing, and therefore I recommend rejection.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jimwang@gatech.edu", "xl374@scarletmail.rutgers.edu", "liping11@baidu.com"], "title": "Zeroth Order Optimization by a Mixture of Evolution Strategies", "authors": ["Jun-Kun Wang", "Xiaoyun Li", "Ping Li"], "pdf": "/pdf/f333e090161c8ee9f3feb4124758a76dc30ef134.pdf", "abstract": "Evolution strategies or zeroth-order optimization algorithms have become popular in some areas of optimization and machine learning where only the oracle of function value evaluations is available. The central idea in the design of the algorithms is by querying function values of some perturbed points in the neighborhood of the current update and constructing a pseudo-gradient using the function values. In recent years, there is a growing interest in developing new ways of perturbation. Though the new perturbation methods are well motivating, most of them are criticized for lack of convergence guarantees even when the underlying function is convex. Perhaps the only methods that enjoy convergence guarantees are the ones that sample the perturbed points uniformly from a unit sphere or from a multivariate Gaussian distribution with an isotropic covariance. In this work, we tackle the non-convergence issue and propose sampling perturbed points from a mixture of distributions. Experiments show that our proposed method can identify the best perturbation scheme for the convergence and might also help to leverage the complementariness of different perturbation schemes.\n", "code": "https://www.dropbox.com/s/e3pzv5581e3x8ku/codes.zip?dl=0", "keywords": [], "paperhash": "wang|zeroth_order_optimization_by_a_mixture_of_evolution_strategies", "original_pdf": "/attachment/f333e090161c8ee9f3feb4124758a76dc30ef134.pdf", "_bibtex": "@misc{\nwang2020zeroth,\ntitle={Zeroth Order Optimization by a Mixture of Evolution Strategies},\nauthor={Jun-Kun Wang and Xiaoyun Li and Ping Li},\nyear={2020},\nurl={https://openreview.net/forum?id=SklE_CNFPr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SklE_CNFPr", "replyto": "SklE_CNFPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795727441, "tmdate": 1576800279671, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1207/-/Decision"}}}, {"id": "BylC0IXsYr", "original": null, "number": 1, "cdate": 1571661525956, "ddate": null, "tcdate": 1571661525956, "tmdate": 1572972498967, "tddate": null, "forum": "SklE_CNFPr", "replyto": "SklE_CNFPr", "invitation": "ICLR.cc/2020/Conference/Paper1207/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The submission is proposing an adaptive derivative-free optimization method. In this method, the sampling covariances are adapted between different covariance adaptation heuristics. The main intuition is seeing each algorithm as an arm in multi-armed bandit (MAB) setting. Moreover, authors use EXP3.P as an online learning mechanism.\n\nThe idea and intuition is definitely interesting. Although the idea of adapting DFO using MAB has been already explored, using MAB for choosing covariance adaptation scheme is definitely novel and interesting. However, the submission has various issues which need to be fixed.\n\nThe claim that the existing adaptation mechanisms have no guarantees is not correct. (Ye et al, 2019) provides convergence guarantees. Moreover, most of the existing adaptive optimization method convergence guarantees can be extended to DFO in a straightforward manner.\n\nThe main claim (\"we can show that our method converges with a high probability when the underlying function is convex\") is not completely substantiated. The high-probability no-regret guarantee (Theorem 2) the authors state is using the iterates (w_t) as an input. It shows that given adversarially chosen iterates (w_t), the Exp3.p has no regret. However, this says nothing about the convergence of these iterates (w_1,...,w_t). I think it is not hard to prove the main claim; however, the reasoning provided in the paper is incomplete. More importantly, both Thm.1 and Thm.2 are specifying the sample complexity of the methods. Whereas, authors only give rather asymptotic argument. Authors should 1) rigorously prove the convergence guarantee as an additional theorem. And, 2) give sample complexity of the proposed method.\n\nAnother issue is the empirical validation. It is clear from the results that the adam-style Hessian method outperforms the proposed method. This is counter-intuitive and no explanation (theoretical or empirical) is given for this discrepancy. \n\nAs a minor question: Why do you only update one method after each gradient update in Line 9 of Alg5? It is an unbiased estimate of the gradient, hence all sampling oracles can update itself using this estimate?"}, "signatures": ["ICLR.cc/2020/Conference/Paper1207/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1207/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jimwang@gatech.edu", "xl374@scarletmail.rutgers.edu", "liping11@baidu.com"], "title": "Zeroth Order Optimization by a Mixture of Evolution Strategies", "authors": ["Jun-Kun Wang", "Xiaoyun Li", "Ping Li"], "pdf": "/pdf/f333e090161c8ee9f3feb4124758a76dc30ef134.pdf", "abstract": "Evolution strategies or zeroth-order optimization algorithms have become popular in some areas of optimization and machine learning where only the oracle of function value evaluations is available. The central idea in the design of the algorithms is by querying function values of some perturbed points in the neighborhood of the current update and constructing a pseudo-gradient using the function values. In recent years, there is a growing interest in developing new ways of perturbation. Though the new perturbation methods are well motivating, most of them are criticized for lack of convergence guarantees even when the underlying function is convex. Perhaps the only methods that enjoy convergence guarantees are the ones that sample the perturbed points uniformly from a unit sphere or from a multivariate Gaussian distribution with an isotropic covariance. In this work, we tackle the non-convergence issue and propose sampling perturbed points from a mixture of distributions. Experiments show that our proposed method can identify the best perturbation scheme for the convergence and might also help to leverage the complementariness of different perturbation schemes.\n", "code": "https://www.dropbox.com/s/e3pzv5581e3x8ku/codes.zip?dl=0", "keywords": [], "paperhash": "wang|zeroth_order_optimization_by_a_mixture_of_evolution_strategies", "original_pdf": "/attachment/f333e090161c8ee9f3feb4124758a76dc30ef134.pdf", "_bibtex": "@misc{\nwang2020zeroth,\ntitle={Zeroth Order Optimization by a Mixture of Evolution Strategies},\nauthor={Jun-Kun Wang and Xiaoyun Li and Ping Li},\nyear={2020},\nurl={https://openreview.net/forum?id=SklE_CNFPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SklE_CNFPr", "replyto": "SklE_CNFPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1207/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1207/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575471341113, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1207/Reviewers"], "noninvitees": [], "tcdate": 1570237740754, "tmdate": 1575471341125, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1207/-/Official_Review"}}}, {"id": "H1gVIxe6KS", "original": null, "number": 2, "cdate": 1571778636118, "ddate": null, "tcdate": 1571778636118, "tmdate": 1572972498934, "tddate": null, "forum": "SklE_CNFPr", "replyto": "SklE_CNFPr", "invitation": "ICLR.cc/2020/Conference/Paper1207/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes to change the sampling scheme of evolutionary strategies to sample from a mixture of distributions (instead of the standard way of sampling from one Gaussian).\nAlthough this seems to provide slightly better performance, the contribution is incremental and more importantly the paper lacks clarity, especially regarding the theory part. The experiments are also not on par with what\u2019s typically expected in the literature. Overall, this paper is not ready for acceptance at a venue such as ICLR.\n\nEXP3.P algorithm\nThis is one of the key components of the algorithm presented by the authors but it is not properly explained in the paper. In fact, the algorithm implemented by the authors is very unclear to me. Below I quote the description given by the authors:\n\u201cOne can view that EXP3.P runs synchronously with Algorithm 5 in the way that step 4 of EXP3.P implements step 5 of Algorithm 5 and that step 5 and the subsequent steps of EXP3.P are conducted after step 8 of Algorithm 5 is finished\u201c\nThis is a very poor description of the algorithm. The authors should give a clear pseudo-code to explain the algorithm.\n\nTheoretical guarantees\n1) Again, the paper is very unclear on what algorithm is being implemented. The authors claim they can directly use the guarantees provided by Bubeck et al but this is unclear to me given that the algorithm is never fully stated. This needs to be clarified.\n2) The authors claim asymptotic convergence, specifically they claim \u201cTherefore, one might show that the proposed algorithm converges with a high probability.\u201c. This proof should be provided in the paper, not left as an exercise to the reader...\n3) Does Theorem 2 (Theorem 3.3 from Bubeck) require convexity of the function? Can you comment on guarantees for non-convex functions?\n\nPrior work\nThe authors do not give a clear description of prior work. There is a large literature on proving convergence guarantees under weaker assumptions than discussed in the paper, see e.g.\nY. Diouane, S. Gratton, and L. N. Vicente. Globally convergent evolution strategies. Math.\nProgram., 152:467\u2013490, 2015.\nY. Diouane, S. Gratton, and L. N. Vicente. Globally convergent evolution strategies for\nconstrained optimization. Comput. Optim. Appl., 62:323\u2013346, 2015.\nVincente. Worst case complexity of direct search\n\nExperiments\nThis is also a weak part of the paper.\n1) The authors only run experiments on very small datasets. Regarding the non-convex task, it would be much more interesting to consider a reinforcement learning task where evolutionary methods typically perform well.\n2) You should include stronger baselines too, e.g. https://arxiv.org/abs/1806.10230.\n3) An interesting question to look at would be to analyze the behavior of the algorithm as a function of the number of mixture components. Also, what is the computational increase in terms of the number of components? Consider giving plots in terms of computation time.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1207/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1207/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jimwang@gatech.edu", "xl374@scarletmail.rutgers.edu", "liping11@baidu.com"], "title": "Zeroth Order Optimization by a Mixture of Evolution Strategies", "authors": ["Jun-Kun Wang", "Xiaoyun Li", "Ping Li"], "pdf": "/pdf/f333e090161c8ee9f3feb4124758a76dc30ef134.pdf", "abstract": "Evolution strategies or zeroth-order optimization algorithms have become popular in some areas of optimization and machine learning where only the oracle of function value evaluations is available. The central idea in the design of the algorithms is by querying function values of some perturbed points in the neighborhood of the current update and constructing a pseudo-gradient using the function values. In recent years, there is a growing interest in developing new ways of perturbation. Though the new perturbation methods are well motivating, most of them are criticized for lack of convergence guarantees even when the underlying function is convex. Perhaps the only methods that enjoy convergence guarantees are the ones that sample the perturbed points uniformly from a unit sphere or from a multivariate Gaussian distribution with an isotropic covariance. In this work, we tackle the non-convergence issue and propose sampling perturbed points from a mixture of distributions. Experiments show that our proposed method can identify the best perturbation scheme for the convergence and might also help to leverage the complementariness of different perturbation schemes.\n", "code": "https://www.dropbox.com/s/e3pzv5581e3x8ku/codes.zip?dl=0", "keywords": [], "paperhash": "wang|zeroth_order_optimization_by_a_mixture_of_evolution_strategies", "original_pdf": "/attachment/f333e090161c8ee9f3feb4124758a76dc30ef134.pdf", "_bibtex": "@misc{\nwang2020zeroth,\ntitle={Zeroth Order Optimization by a Mixture of Evolution Strategies},\nauthor={Jun-Kun Wang and Xiaoyun Li and Ping Li},\nyear={2020},\nurl={https://openreview.net/forum?id=SklE_CNFPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SklE_CNFPr", "replyto": "SklE_CNFPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1207/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1207/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575471341113, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1207/Reviewers"], "noninvitees": [], "tcdate": 1570237740754, "tmdate": 1575471341125, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1207/-/Official_Review"}}}, {"id": "SyggtDwptr", "original": null, "number": 3, "cdate": 1571809144285, "ddate": null, "tcdate": 1571809144285, "tmdate": 1572972498891, "tddate": null, "forum": "SklE_CNFPr", "replyto": "SklE_CNFPr", "invitation": "ICLR.cc/2020/Conference/Paper1207/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper is motivated to improve the performance of zeroth order stochastic search by incorporating a gaussian mixture as the candidate solution sampler. It is (almost) not possible to improve the state of the art zeroth order optimizer in all aspects, hence I expect a description in which situation the authors try to improve the performance by the proposed technique. However, it is not mentioned in the paper. \n\nThe experiments are far away from convincing. The authors pick only two very simple functions, convex one and non-convex but very simple one. The experimental results are compared only with the two baselines of the proposed algorithm. I have to say that the experimental design of this paper must be reconsidered to conclude and derive the goodness of the proposed algorithm.\n\nFirst of all, the effect of introducing non-identity covariance matrix is usually to tackle the ill-conditioning of the objective function, as is well discussed in papers addressing the covariance matrix adaptation in evolution strategies (CMA-ES) by Hansen and his co-authors. However, the objective function doesn't seem to be so. Therefore, this effect can not be tested from this experiments. It is not at all clear what the authors want to claim from these experiments. \n\nIf the non-isotropic part is not important but the scaling factor of the covariance matrix matters (for the step-size adaptation effect), one must compare algorithms using isotropic covariance matrix but with step-size adaptation mechanisms. For example, random pursuit (Stich, SIOPT 2013) is a hill-climbing algorithm with randomly sampled direction with a line search. It also has a convergence results on convex functions. (1+1)-evolution strategy is another random direction hill-climbing algorithm with a step-size control mechanism. (Morinaga and Akimoto, FOGA 2019) has shown its linear convergence of this algorithm on strongly convex functions and beyond. These algorithms works reasonably well without tuning the hyper-parameters. In the proposed approach, there are several hyper-parameters that needs to be tuned to guarantee the convergence and perform reasonably. What is then the goodness of the proposed algorithm compared with above mentioned algorithms?\n\nTheorems provided in the paper doesn't show the goodness of the proposed algorithm, but it tells that the idea of having a mixture model is not a bad idea. I do not see from these arguments the reason why a mixture model leads to a better performance than the baseline. Nonetheless, the theoretical analysis of this work is not rigorous, and if I understand correctly, the convergence (and its rate) of the algorithm is not formally proved. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1207/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1207/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jimwang@gatech.edu", "xl374@scarletmail.rutgers.edu", "liping11@baidu.com"], "title": "Zeroth Order Optimization by a Mixture of Evolution Strategies", "authors": ["Jun-Kun Wang", "Xiaoyun Li", "Ping Li"], "pdf": "/pdf/f333e090161c8ee9f3feb4124758a76dc30ef134.pdf", "abstract": "Evolution strategies or zeroth-order optimization algorithms have become popular in some areas of optimization and machine learning where only the oracle of function value evaluations is available. The central idea in the design of the algorithms is by querying function values of some perturbed points in the neighborhood of the current update and constructing a pseudo-gradient using the function values. In recent years, there is a growing interest in developing new ways of perturbation. Though the new perturbation methods are well motivating, most of them are criticized for lack of convergence guarantees even when the underlying function is convex. Perhaps the only methods that enjoy convergence guarantees are the ones that sample the perturbed points uniformly from a unit sphere or from a multivariate Gaussian distribution with an isotropic covariance. In this work, we tackle the non-convergence issue and propose sampling perturbed points from a mixture of distributions. Experiments show that our proposed method can identify the best perturbation scheme for the convergence and might also help to leverage the complementariness of different perturbation schemes.\n", "code": "https://www.dropbox.com/s/e3pzv5581e3x8ku/codes.zip?dl=0", "keywords": [], "paperhash": "wang|zeroth_order_optimization_by_a_mixture_of_evolution_strategies", "original_pdf": "/attachment/f333e090161c8ee9f3feb4124758a76dc30ef134.pdf", "_bibtex": "@misc{\nwang2020zeroth,\ntitle={Zeroth Order Optimization by a Mixture of Evolution Strategies},\nauthor={Jun-Kun Wang and Xiaoyun Li and Ping Li},\nyear={2020},\nurl={https://openreview.net/forum?id=SklE_CNFPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SklE_CNFPr", "replyto": "SklE_CNFPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1207/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1207/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575471341113, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1207/Reviewers"], "noninvitees": [], "tcdate": 1570237740754, "tmdate": 1575471341125, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1207/-/Official_Review"}}}], "count": 5}