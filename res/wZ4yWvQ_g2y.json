{"notes": [{"id": "wZ4yWvQ_g2y", "original": "-jI6tPtY2nP", "number": 1743, "cdate": 1601308192505, "ddate": null, "tcdate": 1601308192505, "tmdate": 1614985687589, "tddate": null, "forum": "wZ4yWvQ_g2y", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Task-Agnostic and Adaptive-Size BERT Compression", "authorids": ["~Jin_Xu5", "~Xu_Tan1", "~Renqian_Luo1", "~Kaitao_Song1", "~Li_Jian1", "~Tao_Qin1", "~Tie-Yan_Liu1"], "authors": ["Jin Xu", "Xu Tan", "Renqian Luo", "Kaitao Song", "Li Jian", "Tao Qin", "Tie-Yan Liu"], "keywords": ["BERT compression", "neural architecture search", "adaptive sizes", "across tasks", "knowledge distillation"], "abstract": "While pre-trained language models such as BERT and RoBERTa have achieved impressive results on various natural language processing tasks, they have huge numbers of parameters and suffer from huge computational and memory costs, which make them difficult for real-world deployment. Hence, model compression should be performed in order to reduce the computation and memory cost of pre-trained models. In this work, we aim to compress BERT and address the following two challenging practical issues: (1) The compression algorithm should be able to output multiple compressed models with different sizes and latencies, so as to support devices with different kinds of memory and latency limitations; (2) the algorithm should be downstream task agnostic, so that the compressed models are generally applicable for different downstream tasks.  We leverage techniques in neural architecture search (NAS) and propose NAS-BERT, an efficient method for BERT compression. NAS-BERT trains a big supernet on a carefully designed search space containing various architectures and outputs multiple compressed models with adaptive sizes and latency. Furthermore, the training of NAS-BERT is conducted on standard self-supervised pre-training tasks (e.g., masked language model) and does not depend on specific downstream tasks. Thus, the models it produces can be used across various downstream tasks. The technical challenge of NAS-BERT is that training a big supernet on the pre-training task is extremely costly. We employ several techniques including block-wise search, search space pruning, and performance approximation to improve search efficiency and accuracy. Extensive experiments on GLUE benchmark datasets demonstrate that NAS-BERT can find lightweight models with better accuracy than previous approaches, and can be directly applied to different downstream tasks with adaptive model sizes for different requirements of memory or latency.", "one-sentence_summary": "we propose NAS-BERT, which leverages neural architecture search for BERT compression with adaptive model sizes and across downstream tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|taskagnostic_and_adaptivesize_bert_compression", "pdf": "/pdf/b9075151b7b155f2bc523a3e13c7717f7cff6256.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8SFXuIpUMM", "_bibtex": "@misc{\nxu2021taskagnostic,\ntitle={Task-Agnostic and Adaptive-Size {\\{}BERT{\\}} Compression},\nauthor={Jin Xu and Xu Tan and Renqian Luo and Kaitao Song and Li Jian and Tao Qin and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=wZ4yWvQ_g2y}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 17, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "5fqmhdAJhNC", "original": null, "number": 1, "cdate": 1610040463136, "ddate": null, "tcdate": 1610040463136, "tmdate": 1610474066195, "tddate": null, "forum": "wZ4yWvQ_g2y", "replyto": "wZ4yWvQ_g2y", "invitation": "ICLR.cc/2021/Conference/Paper1743/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "Compressing BERT is a practically important research direction. Our main concern on this submission is on its practical value. Comparing with MobileBERT in the literature, NAS-BERT does not show advantages on any aspect: latency, prediction performance, or model size (less important), while being much more costly to build because of NAS. MobileBERT just simply narrowed the original BERT models (8x narrower than BERT large). So it is hard to convince the readers that adaptive-size or NAS is interesting or matters. On the research side, this paper have some interesting points on designing the search space, but overall the novelty of this paper is limited, as all of the reviewers pointed out. It is also worth noticing that the claim of \"task agonistic\" in this paper does not fully hold: in the downstream tasks, the soft labels of the teacher model are required to train the compressed model. To be fully \"task agonistic\", the results on downstream tasks should be solely based on training with the ground truth labels, as in the MobileBERT paper. Once following the exact task agnostic experimental protocol, the reported performance in this paper may be significantly lower. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Task-Agnostic and Adaptive-Size BERT Compression", "authorids": ["~Jin_Xu5", "~Xu_Tan1", "~Renqian_Luo1", "~Kaitao_Song1", "~Li_Jian1", "~Tao_Qin1", "~Tie-Yan_Liu1"], "authors": ["Jin Xu", "Xu Tan", "Renqian Luo", "Kaitao Song", "Li Jian", "Tao Qin", "Tie-Yan Liu"], "keywords": ["BERT compression", "neural architecture search", "adaptive sizes", "across tasks", "knowledge distillation"], "abstract": "While pre-trained language models such as BERT and RoBERTa have achieved impressive results on various natural language processing tasks, they have huge numbers of parameters and suffer from huge computational and memory costs, which make them difficult for real-world deployment. Hence, model compression should be performed in order to reduce the computation and memory cost of pre-trained models. In this work, we aim to compress BERT and address the following two challenging practical issues: (1) The compression algorithm should be able to output multiple compressed models with different sizes and latencies, so as to support devices with different kinds of memory and latency limitations; (2) the algorithm should be downstream task agnostic, so that the compressed models are generally applicable for different downstream tasks.  We leverage techniques in neural architecture search (NAS) and propose NAS-BERT, an efficient method for BERT compression. NAS-BERT trains a big supernet on a carefully designed search space containing various architectures and outputs multiple compressed models with adaptive sizes and latency. Furthermore, the training of NAS-BERT is conducted on standard self-supervised pre-training tasks (e.g., masked language model) and does not depend on specific downstream tasks. Thus, the models it produces can be used across various downstream tasks. The technical challenge of NAS-BERT is that training a big supernet on the pre-training task is extremely costly. We employ several techniques including block-wise search, search space pruning, and performance approximation to improve search efficiency and accuracy. Extensive experiments on GLUE benchmark datasets demonstrate that NAS-BERT can find lightweight models with better accuracy than previous approaches, and can be directly applied to different downstream tasks with adaptive model sizes for different requirements of memory or latency.", "one-sentence_summary": "we propose NAS-BERT, which leverages neural architecture search for BERT compression with adaptive model sizes and across downstream tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|taskagnostic_and_adaptivesize_bert_compression", "pdf": "/pdf/b9075151b7b155f2bc523a3e13c7717f7cff6256.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8SFXuIpUMM", "_bibtex": "@misc{\nxu2021taskagnostic,\ntitle={Task-Agnostic and Adaptive-Size {\\{}BERT{\\}} Compression},\nauthor={Jin Xu and Xu Tan and Renqian Luo and Kaitao Song and Li Jian and Tao Qin and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=wZ4yWvQ_g2y}\n}"}, "tags": [], "invitation": {"reply": {"forum": "wZ4yWvQ_g2y", "replyto": "wZ4yWvQ_g2y", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040463121, "tmdate": 1610474066177, "id": "ICLR.cc/2021/Conference/Paper1743/-/Decision"}}}, {"id": "mMhsFT_vAAw", "original": null, "number": 2, "cdate": 1603892139391, "ddate": null, "tcdate": 1603892139391, "tmdate": 1606715975429, "tddate": null, "forum": "wZ4yWvQ_g2y", "replyto": "wZ4yWvQ_g2y", "invitation": "ICLR.cc/2021/Conference/Paper1743/-/Official_Review", "content": {"title": "Interesting work on NAS for BERT", "review": "Summary:\nThis paper proposes to search architectures of BERT model under various memory and latency contraints. The search algorithm is conducted by pretraining a big supernet that contains the all the sub-network structures, where the optimal models for different requirements are selected from it. Once an architecture is found, it is re-trained through pretraining-finetuning or two-stage distillation for each specific task. Several approaches (block-wise training and search, progressive shrinking, performance approximation) are proposed to improve the search efficiency. Experiments on GLUE benchmark shows the models found by proposed methods can achieve better accuracy than some of the previous compressed BERT models. The paper (together with the appendix) is clearly presented, and the idea is new and interesting to me. The experiments are detailed and comprehensive.\n\nPros:\nThe paper is well presented. The architecture of the superent and the candidate operations are carefully designed and selected. It seems that the SpeConv operation is particularly effective when the model size is small. The search algorithm including the block-wise training, progressive shrinking can remove less-optimal structures quickly and significantly reduce the search space. The performance of NAS-BERT models are generally better than those of the compressed BERT models with similar model size, although the comparisons may not be completely fair.\n\nConcerns:\n1. The organization of the paper can be further improved. The paper may not be easy to follow if the appendix is skipped, especially for the readers who are not familiar with NAS or related work. Many of the important information can only be found in appendix.\n2. The novelty of the paper is unclear to me. Although this work may be new on search BERT-like language model, it seems many of the ideas such as block-wise search and distillation are borrowed from existing work. Please the author clarify the main novelties and technical contribution of this work, especially to the field of neural architecture search or more broadly, AutoML . Moreover, some of the proposed techniques such as progressive shrinking are merely empirical practices and are lack of theory or insight showing how accurate the approximation would be.\n3. It is usually more illustrative (and also space saving) to plot accuracy versus latency/#parameters of different models in the same figure. Some of the well noted models such as MobileBERT and TinyBERT are not included in comparison. For DynaBERT, there are multiple configurations but only one is included. AdaBERT, which adopts NAS for each specific task, should also be included if possible. Again, since there are of many models with different size and latency, it may be better to have a plot for clear comparison. \n4. HAT (Wang et al. HAT: Hardware-Aware Transformers for Efficient Natural Language Processing. ACL 2020.) is not mentioned in the paper, which share similarities (training supernet) and differences (search algorithm) with this work from technical point of view. It will be better if the author can explain and compare the proposed search algorithm to evolutionary search.\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1743/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1743/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Task-Agnostic and Adaptive-Size BERT Compression", "authorids": ["~Jin_Xu5", "~Xu_Tan1", "~Renqian_Luo1", "~Kaitao_Song1", "~Li_Jian1", "~Tao_Qin1", "~Tie-Yan_Liu1"], "authors": ["Jin Xu", "Xu Tan", "Renqian Luo", "Kaitao Song", "Li Jian", "Tao Qin", "Tie-Yan Liu"], "keywords": ["BERT compression", "neural architecture search", "adaptive sizes", "across tasks", "knowledge distillation"], "abstract": "While pre-trained language models such as BERT and RoBERTa have achieved impressive results on various natural language processing tasks, they have huge numbers of parameters and suffer from huge computational and memory costs, which make them difficult for real-world deployment. Hence, model compression should be performed in order to reduce the computation and memory cost of pre-trained models. In this work, we aim to compress BERT and address the following two challenging practical issues: (1) The compression algorithm should be able to output multiple compressed models with different sizes and latencies, so as to support devices with different kinds of memory and latency limitations; (2) the algorithm should be downstream task agnostic, so that the compressed models are generally applicable for different downstream tasks.  We leverage techniques in neural architecture search (NAS) and propose NAS-BERT, an efficient method for BERT compression. NAS-BERT trains a big supernet on a carefully designed search space containing various architectures and outputs multiple compressed models with adaptive sizes and latency. Furthermore, the training of NAS-BERT is conducted on standard self-supervised pre-training tasks (e.g., masked language model) and does not depend on specific downstream tasks. Thus, the models it produces can be used across various downstream tasks. The technical challenge of NAS-BERT is that training a big supernet on the pre-training task is extremely costly. We employ several techniques including block-wise search, search space pruning, and performance approximation to improve search efficiency and accuracy. Extensive experiments on GLUE benchmark datasets demonstrate that NAS-BERT can find lightweight models with better accuracy than previous approaches, and can be directly applied to different downstream tasks with adaptive model sizes for different requirements of memory or latency.", "one-sentence_summary": "we propose NAS-BERT, which leverages neural architecture search for BERT compression with adaptive model sizes and across downstream tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|taskagnostic_and_adaptivesize_bert_compression", "pdf": "/pdf/b9075151b7b155f2bc523a3e13c7717f7cff6256.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8SFXuIpUMM", "_bibtex": "@misc{\nxu2021taskagnostic,\ntitle={Task-Agnostic and Adaptive-Size {\\{}BERT{\\}} Compression},\nauthor={Jin Xu and Xu Tan and Renqian Luo and Kaitao Song and Li Jian and Tao Qin and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=wZ4yWvQ_g2y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "wZ4yWvQ_g2y", "replyto": "wZ4yWvQ_g2y", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1743/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538111634, "tmdate": 1606915790228, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1743/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1743/-/Official_Review"}}}, {"id": "sW0RBsZJwAJ", "original": null, "number": 13, "cdate": 1606237100657, "ddate": null, "tcdate": 1606237100657, "tmdate": 1606237100657, "tddate": null, "forum": "wZ4yWvQ_g2y", "replyto": "vfI0Uj0ZgfL", "invitation": "ICLR.cc/2021/Conference/Paper1743/-/Official_Comment", "content": {"title": "The results of SQuAD v1.0 and v2.0", "comment": "We get results of SQuAD 1.1 and SQuAD 2.0 are as follows:\n\n|Model|\\|| Params   |\\| | SQuAD v1.1| \\|| SQuAD v2.0  | \n|:----|:-:|:---: |:--:|:----:|:----: |:---: |\n|========= |\\||=======|\\||==EM/F1== |\\||==EM/ F1==|\n|Teacher     | \\||110M|\\||81.8/88.9|\\||74.5/77.9|\n|DistilBERT |\\||66M|\\||79.1/86.9|\\||-/-|\n|BERT-PKD |\\||66M|\\||77.1/85.3|\\||66.3/69.8|\n|MiniLM\u2020    |\\||66M|\\||-/-|\\||-/76.4  |\n|TinyBERT   |\\||66M|\\||79.7/87.5|\\||69.9/73.4|\n|NAS-BERT  |\\||60M|\\||80.5/88.0|\\||73.2/76.3|\n|NAS-BERT\u2020|\\||60M|\\||**81.2**/**88.4**|\\||**73.9**/**77.1**| \n\n**Title**: MiniLM\u2020 means that MiniLM in the original paper [1] was trained with more computations (batch size 1024 * 400,000 steps). Thus, we also trained the NAS-BERT\u2020 with the same computations (batch size 2048 * 200,000 steps) for a fair comparison.\n\n We can find that our NAS-BERT, without using sophisticated distillation techniques, outperforms previous works with even slightly more parameters on both SQuAD v1.1 and v2.0.\n\n[1] Wang W, Wei F, Dong L, et al. Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers[J]. arXiv preprint arXiv:2002.10957, 2020."}, "signatures": ["ICLR.cc/2021/Conference/Paper1743/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1743/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Task-Agnostic and Adaptive-Size BERT Compression", "authorids": ["~Jin_Xu5", "~Xu_Tan1", "~Renqian_Luo1", "~Kaitao_Song1", "~Li_Jian1", "~Tao_Qin1", "~Tie-Yan_Liu1"], "authors": ["Jin Xu", "Xu Tan", "Renqian Luo", "Kaitao Song", "Li Jian", "Tao Qin", "Tie-Yan Liu"], "keywords": ["BERT compression", "neural architecture search", "adaptive sizes", "across tasks", "knowledge distillation"], "abstract": "While pre-trained language models such as BERT and RoBERTa have achieved impressive results on various natural language processing tasks, they have huge numbers of parameters and suffer from huge computational and memory costs, which make them difficult for real-world deployment. Hence, model compression should be performed in order to reduce the computation and memory cost of pre-trained models. In this work, we aim to compress BERT and address the following two challenging practical issues: (1) The compression algorithm should be able to output multiple compressed models with different sizes and latencies, so as to support devices with different kinds of memory and latency limitations; (2) the algorithm should be downstream task agnostic, so that the compressed models are generally applicable for different downstream tasks.  We leverage techniques in neural architecture search (NAS) and propose NAS-BERT, an efficient method for BERT compression. NAS-BERT trains a big supernet on a carefully designed search space containing various architectures and outputs multiple compressed models with adaptive sizes and latency. Furthermore, the training of NAS-BERT is conducted on standard self-supervised pre-training tasks (e.g., masked language model) and does not depend on specific downstream tasks. Thus, the models it produces can be used across various downstream tasks. The technical challenge of NAS-BERT is that training a big supernet on the pre-training task is extremely costly. We employ several techniques including block-wise search, search space pruning, and performance approximation to improve search efficiency and accuracy. Extensive experiments on GLUE benchmark datasets demonstrate that NAS-BERT can find lightweight models with better accuracy than previous approaches, and can be directly applied to different downstream tasks with adaptive model sizes for different requirements of memory or latency.", "one-sentence_summary": "we propose NAS-BERT, which leverages neural architecture search for BERT compression with adaptive model sizes and across downstream tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|taskagnostic_and_adaptivesize_bert_compression", "pdf": "/pdf/b9075151b7b155f2bc523a3e13c7717f7cff6256.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8SFXuIpUMM", "_bibtex": "@misc{\nxu2021taskagnostic,\ntitle={Task-Agnostic and Adaptive-Size {\\{}BERT{\\}} Compression},\nauthor={Jin Xu and Xu Tan and Renqian Luo and Kaitao Song and Li Jian and Tao Qin and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=wZ4yWvQ_g2y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "wZ4yWvQ_g2y", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1743/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1743/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1743/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1743/Authors|ICLR.cc/2021/Conference/Paper1743/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1743/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856205, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1743/-/Official_Comment"}}}, {"id": "dEmgbf1omy", "original": null, "number": 1, "cdate": 1603781793073, "ddate": null, "tcdate": 1603781793073, "tmdate": 1606185647036, "tddate": null, "forum": "wZ4yWvQ_g2y", "replyto": "wZ4yWvQ_g2y", "invitation": "ICLR.cc/2021/Conference/Paper1743/-/Official_Review", "content": {"title": "Paper on applying NAS to pre-training tasks", "review": "This paper presents an effective NAS method for pre-trained language models at the pre-training stage, so the selected models can be applied to various downstream tasks with fine-tuning. To achieve better performances the widely used two-stage distillation and data augmentation are applied to the selected models from super-net.  \n\nThe main contribution of this work lies in the designed search space and the proposed three strategies (block-wise search, progressive shrinking and performance approximation) for improving search efficiency and accuracy. Although the novelty of this work is quite limited, training a big supernet for BERT at the pre-training stage is not trivial, which is useful for industry applications. The authors evaluate their approach on GLUE datasets and compare it to other state-of-the-art models.\n\nThe paper is well-written and organized, the experiments are thorough. However, I have several concerns:\n\n1)  In the Table 1 under the KD setting, \u201ctwo-stage distillation\u201d is conducted on the selected models from supernet to further improve the performances, it would be interesting to add another two settings: a) only conducting the distillation at the pre-training stage, b) continuing to pre-training on large scale unlabeled data to finally obtain better task-agnostic models. \n\n2) The models are evaluated on the GLUE dataset, more experiments on challenging QA tasks should be added. \n\n3) In the Table 1, the comparison to MobileBERT and TinyBERT should be added, and the FLOPs or the inference time on CPU/GPU can be provided.  \n\n4) Some important related work should be included, [1] HAT: Hardware-Aware Transformers for Efficient Natural Language Processing (although this work focuses on the machine translation task) [2] Finding Fast Transformers: One-Shot Neural Architecture Search by Component Composition.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1743/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1743/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Task-Agnostic and Adaptive-Size BERT Compression", "authorids": ["~Jin_Xu5", "~Xu_Tan1", "~Renqian_Luo1", "~Kaitao_Song1", "~Li_Jian1", "~Tao_Qin1", "~Tie-Yan_Liu1"], "authors": ["Jin Xu", "Xu Tan", "Renqian Luo", "Kaitao Song", "Li Jian", "Tao Qin", "Tie-Yan Liu"], "keywords": ["BERT compression", "neural architecture search", "adaptive sizes", "across tasks", "knowledge distillation"], "abstract": "While pre-trained language models such as BERT and RoBERTa have achieved impressive results on various natural language processing tasks, they have huge numbers of parameters and suffer from huge computational and memory costs, which make them difficult for real-world deployment. Hence, model compression should be performed in order to reduce the computation and memory cost of pre-trained models. In this work, we aim to compress BERT and address the following two challenging practical issues: (1) The compression algorithm should be able to output multiple compressed models with different sizes and latencies, so as to support devices with different kinds of memory and latency limitations; (2) the algorithm should be downstream task agnostic, so that the compressed models are generally applicable for different downstream tasks.  We leverage techniques in neural architecture search (NAS) and propose NAS-BERT, an efficient method for BERT compression. NAS-BERT trains a big supernet on a carefully designed search space containing various architectures and outputs multiple compressed models with adaptive sizes and latency. Furthermore, the training of NAS-BERT is conducted on standard self-supervised pre-training tasks (e.g., masked language model) and does not depend on specific downstream tasks. Thus, the models it produces can be used across various downstream tasks. The technical challenge of NAS-BERT is that training a big supernet on the pre-training task is extremely costly. We employ several techniques including block-wise search, search space pruning, and performance approximation to improve search efficiency and accuracy. Extensive experiments on GLUE benchmark datasets demonstrate that NAS-BERT can find lightweight models with better accuracy than previous approaches, and can be directly applied to different downstream tasks with adaptive model sizes for different requirements of memory or latency.", "one-sentence_summary": "we propose NAS-BERT, which leverages neural architecture search for BERT compression with adaptive model sizes and across downstream tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|taskagnostic_and_adaptivesize_bert_compression", "pdf": "/pdf/b9075151b7b155f2bc523a3e13c7717f7cff6256.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8SFXuIpUMM", "_bibtex": "@misc{\nxu2021taskagnostic,\ntitle={Task-Agnostic and Adaptive-Size {\\{}BERT{\\}} Compression},\nauthor={Jin Xu and Xu Tan and Renqian Luo and Kaitao Song and Li Jian and Tao Qin and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=wZ4yWvQ_g2y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "wZ4yWvQ_g2y", "replyto": "wZ4yWvQ_g2y", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1743/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538111634, "tmdate": 1606915790228, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1743/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1743/-/Official_Review"}}}, {"id": "vfI0Uj0ZgfL", "original": null, "number": 12, "cdate": 1606185632213, "ddate": null, "tcdate": 1606185632213, "tmdate": 1606185632213, "tddate": null, "forum": "wZ4yWvQ_g2y", "replyto": "_n7HSBTMfR", "invitation": "ICLR.cc/2021/Conference/Paper1743/-/Official_Comment", "content": {"title": "My concerns have been addressed", "comment": "Thanks for your reply. Your explanation well addressed my concerns, so I have updated my score to 6. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1743/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1743/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Task-Agnostic and Adaptive-Size BERT Compression", "authorids": ["~Jin_Xu5", "~Xu_Tan1", "~Renqian_Luo1", "~Kaitao_Song1", "~Li_Jian1", "~Tao_Qin1", "~Tie-Yan_Liu1"], "authors": ["Jin Xu", "Xu Tan", "Renqian Luo", "Kaitao Song", "Li Jian", "Tao Qin", "Tie-Yan Liu"], "keywords": ["BERT compression", "neural architecture search", "adaptive sizes", "across tasks", "knowledge distillation"], "abstract": "While pre-trained language models such as BERT and RoBERTa have achieved impressive results on various natural language processing tasks, they have huge numbers of parameters and suffer from huge computational and memory costs, which make them difficult for real-world deployment. Hence, model compression should be performed in order to reduce the computation and memory cost of pre-trained models. In this work, we aim to compress BERT and address the following two challenging practical issues: (1) The compression algorithm should be able to output multiple compressed models with different sizes and latencies, so as to support devices with different kinds of memory and latency limitations; (2) the algorithm should be downstream task agnostic, so that the compressed models are generally applicable for different downstream tasks.  We leverage techniques in neural architecture search (NAS) and propose NAS-BERT, an efficient method for BERT compression. NAS-BERT trains a big supernet on a carefully designed search space containing various architectures and outputs multiple compressed models with adaptive sizes and latency. Furthermore, the training of NAS-BERT is conducted on standard self-supervised pre-training tasks (e.g., masked language model) and does not depend on specific downstream tasks. Thus, the models it produces can be used across various downstream tasks. The technical challenge of NAS-BERT is that training a big supernet on the pre-training task is extremely costly. We employ several techniques including block-wise search, search space pruning, and performance approximation to improve search efficiency and accuracy. Extensive experiments on GLUE benchmark datasets demonstrate that NAS-BERT can find lightweight models with better accuracy than previous approaches, and can be directly applied to different downstream tasks with adaptive model sizes for different requirements of memory or latency.", "one-sentence_summary": "we propose NAS-BERT, which leverages neural architecture search for BERT compression with adaptive model sizes and across downstream tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|taskagnostic_and_adaptivesize_bert_compression", "pdf": "/pdf/b9075151b7b155f2bc523a3e13c7717f7cff6256.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8SFXuIpUMM", "_bibtex": "@misc{\nxu2021taskagnostic,\ntitle={Task-Agnostic and Adaptive-Size {\\{}BERT{\\}} Compression},\nauthor={Jin Xu and Xu Tan and Renqian Luo and Kaitao Song and Li Jian and Tao Qin and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=wZ4yWvQ_g2y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "wZ4yWvQ_g2y", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1743/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1743/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1743/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1743/Authors|ICLR.cc/2021/Conference/Paper1743/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1743/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856205, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1743/-/Official_Comment"}}}, {"id": "jTbAn1eo8fa", "original": null, "number": 4, "cdate": 1605889823359, "ddate": null, "tcdate": 1605889823359, "tmdate": 1606117123292, "tddate": null, "forum": "wZ4yWvQ_g2y", "replyto": "hh3O5Rj3d--", "invitation": "ICLR.cc/2021/Conference/Paper1743/-/Official_Comment", "content": {"title": "Response to AnonReviewer3 (Part 3) ", "comment": "**[About the fair comparison with previous works]**\n\nIn Table 2 in the paper, we fairly compare NAS-BERT with the BERT baselines with exactly the same training configurations, which demonstrates the effectiveness of our method. Yes, as you say, when compared with previous work, there are many implementation differences, but we try our best to reduce the impact of these differences for fair comparison as follows:\n- Different teacher models. We have already compared our teacher models with those used in other works in Table 6 in the Appendix A.2 of the paper and clarified this kind of unfairness in the submitted version. Even without considering RTE and CoLA on which our teacher model achieves better accuracy, NAS-BERT still outperforms existing works as shown in Table 3.  \n- Batch size and training steps. For the student training, DistilBERT uses a larger batch size (4096 batch size * 62,500 steps) and MiniLM uses more computations (1024 batch size * 400,000 steps). We pre-train the NAS-BERT from scratch with computations (2048 batch size * 125,000 steps), which has the same computations as BERT and most of the related works (256 batch size * 1,000,000 steps). Although MiniLM uses more computations, NAS-BERT still outperforms MiniLM in the 60M model setting.  \n- Sophisticated distillation techniques. Hidden layer distillation (DynaBERT, TinyBERT, etc), attention score distillation (DynaBERT, TinyBERT), probe classifiers (AdaBERT) can be used to boost the performance. These techniques are efficient and are complementary to NAS-BERT. Our work mainly focuses on finding efficient and lightweight models for task-agnostic and adaptive-size BERT compression, but not aims to incorporate each sophisticated technique to train the searched architectures.\n\nWe try our best to make the comparison with other works as fair as possible, without using sophisticated distillation skills, more computations, or a much better teacher. Even without sophisticated distillation techniques, NAS-BERT outperforms previous works as shown in Table 3 in the paper, which demonstrates the effectiveness of our searched architectures.  \n\n\n[1] Li C, Peng J, Yuan L, et al. Block-wisely Supervised Neural Architecture Search with Knowledge Distillation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 1989-1998.  \n[2] Cai H, Gan C, Wang T, et al. Once-for-All: Train One Network and Specialize it for Efficient Deployment[C]//International Conference on Learning Representations. 2019.  \n[3] Yu J, Jin P, Liu H, et al. Bignas: Scaling up neural architecture search with big single-stage models[J]. arXiv preprint arXiv:2003.11142, 2020.  \n[4] Devlin J, Chang M W, Lee K, et al. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding[C]//NAACL-HLT (1). 2019."}, "signatures": ["ICLR.cc/2021/Conference/Paper1743/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1743/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Task-Agnostic and Adaptive-Size BERT Compression", "authorids": ["~Jin_Xu5", "~Xu_Tan1", "~Renqian_Luo1", "~Kaitao_Song1", "~Li_Jian1", "~Tao_Qin1", "~Tie-Yan_Liu1"], "authors": ["Jin Xu", "Xu Tan", "Renqian Luo", "Kaitao Song", "Li Jian", "Tao Qin", "Tie-Yan Liu"], "keywords": ["BERT compression", "neural architecture search", "adaptive sizes", "across tasks", "knowledge distillation"], "abstract": "While pre-trained language models such as BERT and RoBERTa have achieved impressive results on various natural language processing tasks, they have huge numbers of parameters and suffer from huge computational and memory costs, which make them difficult for real-world deployment. Hence, model compression should be performed in order to reduce the computation and memory cost of pre-trained models. In this work, we aim to compress BERT and address the following two challenging practical issues: (1) The compression algorithm should be able to output multiple compressed models with different sizes and latencies, so as to support devices with different kinds of memory and latency limitations; (2) the algorithm should be downstream task agnostic, so that the compressed models are generally applicable for different downstream tasks.  We leverage techniques in neural architecture search (NAS) and propose NAS-BERT, an efficient method for BERT compression. NAS-BERT trains a big supernet on a carefully designed search space containing various architectures and outputs multiple compressed models with adaptive sizes and latency. Furthermore, the training of NAS-BERT is conducted on standard self-supervised pre-training tasks (e.g., masked language model) and does not depend on specific downstream tasks. Thus, the models it produces can be used across various downstream tasks. The technical challenge of NAS-BERT is that training a big supernet on the pre-training task is extremely costly. We employ several techniques including block-wise search, search space pruning, and performance approximation to improve search efficiency and accuracy. Extensive experiments on GLUE benchmark datasets demonstrate that NAS-BERT can find lightweight models with better accuracy than previous approaches, and can be directly applied to different downstream tasks with adaptive model sizes for different requirements of memory or latency.", "one-sentence_summary": "we propose NAS-BERT, which leverages neural architecture search for BERT compression with adaptive model sizes and across downstream tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|taskagnostic_and_adaptivesize_bert_compression", "pdf": "/pdf/b9075151b7b155f2bc523a3e13c7717f7cff6256.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8SFXuIpUMM", "_bibtex": "@misc{\nxu2021taskagnostic,\ntitle={Task-Agnostic and Adaptive-Size {\\{}BERT{\\}} Compression},\nauthor={Jin Xu and Xu Tan and Renqian Luo and Kaitao Song and Li Jian and Tao Qin and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=wZ4yWvQ_g2y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "wZ4yWvQ_g2y", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1743/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1743/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1743/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1743/Authors|ICLR.cc/2021/Conference/Paper1743/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1743/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856205, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1743/-/Official_Comment"}}}, {"id": "UFUuFRphoY", "original": null, "number": 5, "cdate": 1605889955510, "ddate": null, "tcdate": 1605889955510, "tmdate": 1605934225188, "tddate": null, "forum": "wZ4yWvQ_g2y", "replyto": "nhZqyL8xXv6", "invitation": "ICLR.cc/2021/Conference/Paper1743/-/Official_Comment", "content": {"title": "Response to AnonReviewer2 (Part 1)", "comment": "Thanks for your constructive review comments. Below are our responses to your concerns.\n\n**[clarify if we save the model parameters corresponding to all the possible architectural choices]**\n\nNAS-BERT can deliver various architectures with different model sizes and latency, which can save model parameters in different degrees. According to the given constraints (params, latency), we can select an architecture to meet these requirements. Then the selected model is pre-trained from scratch independently without inheriting the parameters from the supernet.  \n\n**[Analysis on why SepConv is preferred in small models]**\n\nWe have conducted the analysis before the search space design. To demonstrate why SepConv is preferred in small models, we show the parameters and latency of MHA, FFN and SepConv as follows:\n\n| Operation (hidden size 384) |\\||  Params |\\||  Latency (s) |\\|| Latency/Params |\n|-----------------------------|:--:|:-------:|:--:|:--------:|:--:|:--------------:|\n| Multi-Head Attention        |\\||  592128 |\\|| 0.006146 |\\||    1.00E-08    |\n| Feed-Forward Network        |\\|| 1182336 |\\|| 0.005555 |\\||    4.70E-09    |\n| Separable Conv 3         |\\||  298760 |\\|| 0.002272 |\\||    7.60E-09    |\n| Separable Conv 5            |\\||  300300 |\\|| 0.002285 |\\||    7.60E-09    |\n| Separable Conv 7            |\\||  301840 |\\|| 0.002297 |\\||    7.60E-09    |\n\nCompared with MHA, we can find that Latency/Params ratio of SepConv is smaller. Thus, the latency of SepConv is low when using the same number of parameters. Compared with FFN,  given constraints of the same number of parameters, we can stack more layers without adding much latency (usually better representation ability). For small models with strict latency and parameter constraints, SepConv can be stacked with more layers than FFN and MHA, and do not add too much latency. Thus SepConv is preferred. However, the network completely composed of SepConv layers cannot get good results as explained in the next point below.\n\n**[Can the network completely composed of SepConv layers get better results than the searched model?]** \n\nIn our preliminary study, we build a network completely composed of SepConv layers, which cannot get good performance. Specifically, we build two 10M models with a full stack of SepConv layers: 1) SepConv with kernel size 3 and hidden size 384; 2) SepConv with kernel size 7 and hidden size 384 to compare with $\\text{NAS-BERT}_{10}$. The results are as follows:\n\n| Setting            |\\|| MNLI |\\||  QQP |\\|| QNLI |\\|| CoLA |\\|| SST-2 |\\|| STS-B |\\||  RTE |\\|| MRPC |\\||  AVG |\n|--------------------|:-:|:----:|:-:|:----:|:-:|:----:|:-:|:----:|:-:|:-----:|:-:|:-----:|:-:|:----:|:-:|:----:|:-:|:----:|\n| $\\text{NAS-BERT}_{10}$         |\\|| 76.4 |\\|| 88.5 |\\|| 86.3 |\\||  34.0  |\\||  88.6 |\\||  84.8 |\\|| 66.6 |\\|| 79.1 |\\|| **75.5** |\n| $\\text{BERT}_{10}$             |\\|| 74.4 |\\|| 87.8 |\\|| 85.7 |\\|| 32.5 |\\||  86.6 |\\||  85.2 |\\|| 66.9 |\\|| 77.9 |\\|| 74.6 |\n| SepConv (Kernel 3) |\\|| 58.1 |\\|| 80.9 |\\|| 63.0 |\\|| 38.4 |\\||  83.3 |\\||  45.2 |\\|| 53.0 |\\|| 70.7 |\\|| 61.6 |\n| SepConv (Kernel 7) |\\|| 64.7 |\\|| 82.8 |\\|| 67.8 |\\|| 40.7 |\\||  85.4 |\\||  51.2 |\\|| 54.7 |\\|| 73.8 |\\|| 65.1 |\n\nWe can observe that SepConv network gets much worse performance on the paired sentence tasks (MNLI, QQP, QNLI, STS-B, RTE and MRPC) compared with NAS-BERT and BERT baseline. We analyze that the paired sentence tasks typically need a large reception field to efficiently extract the information between two sentences. However, network composed of only SepConv has limited reception field given normal kernel size and limited layers, which cannot efficiently model the global information. We can find that SepConv network with kernel size 7 gets better scores than that with kernel size 3 but still has a large gap with BERT baseline and NAS-BERT. We hypothesis that the advantages of NAS-BERT rely on the novel stacking between MHA, SepConv and FFN. MHA can extract the global information and SepConv or FFN can refine the information between adjacent positions (feed-forward network can be viewed as a convolutional network with kernel size 1)."}, "signatures": ["ICLR.cc/2021/Conference/Paper1743/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1743/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Task-Agnostic and Adaptive-Size BERT Compression", "authorids": ["~Jin_Xu5", "~Xu_Tan1", "~Renqian_Luo1", "~Kaitao_Song1", "~Li_Jian1", "~Tao_Qin1", "~Tie-Yan_Liu1"], "authors": ["Jin Xu", "Xu Tan", "Renqian Luo", "Kaitao Song", "Li Jian", "Tao Qin", "Tie-Yan Liu"], "keywords": ["BERT compression", "neural architecture search", "adaptive sizes", "across tasks", "knowledge distillation"], "abstract": "While pre-trained language models such as BERT and RoBERTa have achieved impressive results on various natural language processing tasks, they have huge numbers of parameters and suffer from huge computational and memory costs, which make them difficult for real-world deployment. Hence, model compression should be performed in order to reduce the computation and memory cost of pre-trained models. In this work, we aim to compress BERT and address the following two challenging practical issues: (1) The compression algorithm should be able to output multiple compressed models with different sizes and latencies, so as to support devices with different kinds of memory and latency limitations; (2) the algorithm should be downstream task agnostic, so that the compressed models are generally applicable for different downstream tasks.  We leverage techniques in neural architecture search (NAS) and propose NAS-BERT, an efficient method for BERT compression. NAS-BERT trains a big supernet on a carefully designed search space containing various architectures and outputs multiple compressed models with adaptive sizes and latency. Furthermore, the training of NAS-BERT is conducted on standard self-supervised pre-training tasks (e.g., masked language model) and does not depend on specific downstream tasks. Thus, the models it produces can be used across various downstream tasks. The technical challenge of NAS-BERT is that training a big supernet on the pre-training task is extremely costly. We employ several techniques including block-wise search, search space pruning, and performance approximation to improve search efficiency and accuracy. Extensive experiments on GLUE benchmark datasets demonstrate that NAS-BERT can find lightweight models with better accuracy than previous approaches, and can be directly applied to different downstream tasks with adaptive model sizes for different requirements of memory or latency.", "one-sentence_summary": "we propose NAS-BERT, which leverages neural architecture search for BERT compression with adaptive model sizes and across downstream tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|taskagnostic_and_adaptivesize_bert_compression", "pdf": "/pdf/b9075151b7b155f2bc523a3e13c7717f7cff6256.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8SFXuIpUMM", "_bibtex": "@misc{\nxu2021taskagnostic,\ntitle={Task-Agnostic and Adaptive-Size {\\{}BERT{\\}} Compression},\nauthor={Jin Xu and Xu Tan and Renqian Luo and Kaitao Song and Li Jian and Tao Qin and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=wZ4yWvQ_g2y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "wZ4yWvQ_g2y", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1743/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1743/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1743/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1743/Authors|ICLR.cc/2021/Conference/Paper1743/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1743/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856205, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1743/-/Official_Comment"}}}, {"id": "mIO2WZIaXF3", "original": null, "number": 10, "cdate": 1605891253663, "ddate": null, "tcdate": 1605891253663, "tmdate": 1605933720281, "tddate": null, "forum": "wZ4yWvQ_g2y", "replyto": "dEmgbf1omy", "invitation": "ICLR.cc/2021/Conference/Paper1743/-/Official_Comment", "content": {"title": "Response to AnonReviewer1 (Part 1)", "comment": "Thanks for your constructive review comments. Below are our responses to your concerns.\n\n**[Ablation study of two-stage distillation]**\n\nTo study the performance of distillation at the pre-training or fine-tuning stage, we conduct experiments on two settings: 1) only using the distillation on the pre-training stage; 2) only using the distillation on the fine-tuning stage. The results are shown as follows: \n\n| Setting    |\\|| PD |\\|| KD |\\|| MNLI |\\||  QQP |\\|| QNLI |\\|| CoLA |\\|| SST-2 |\\|| STS-B |\\||  RTE |\\|| MRPC |\\||  AVG |\n|------------|:-:|:--:|:-:|:--:|:-:|:----:|:-:|:----:|:-:|:----:|:-:|:----:|:-:|:-----:|:-:|:-----:|:-:|:----:|:-:|:----:|:-:|:----:|\n| $\\text{BERT}_{60}$     |\\||  \u221a |\\||  \u221a |\\|| 83.2 |\\|| 90.5 |\\|| 90.2 |\\|| 56.3 |\\||  91.8 |\\||  88.8 |\\|| 78.5 |\\|| 88.5 |\\|| 83.5 |\n| $\\text{NAS-BERT}_{60}$ |\\||  \u221a |\\||  \u221a |\\|| 84.1 |\\|| 91.0 |\\|| 91.3 |\\|| 58.1 |\\||  92.1 |\\||  89.4 |\\|| 79.2 |\\|| 88.5 |\\|| **84.2** |\n| $\\text{BERT}_{60}$     |\\||  \u221a |\\||    |\\|| 83.2 |\\|| 90.3 |\\|| 89.5 |\\|| 55.0 |\\||  91.6 |\\||  88.6 |\\|| 77.8 |\\|| 87.3 |\\|| 82.9 |\n| $\\text{NAS-BERT}_{60}$ |\\||  \u221a |\\||    |\\|| 83.3 |\\|| 90.9 |\\|| 91.3 |\\|| 55.6 |\\||  92.0 |\\||  88.6 |\\|| 78.5 |\\|| 87.5 |\\|| **83.5** |\n| $\\text{BERT}_{60}$     |\\||    |\\||  \u221a |\\|| 83.1 |\\|| 90.4 |\\|| 90.4 |\\|| 54.3 |\\||  91.2 |\\||  88.7 |\\|| 75.6 |\\|| 87.0 |\\|| 82.6 |\n| $\\text{NAS-BERT}_{60}$ |\\||    |\\||  \u221a |\\|| 83.7 |\\|| 90.8 |\\|| 91.0 |\\|| 54.2 |\\||  92.1 |\\||  89.4 |\\|| 76.0 |\\|| 87.5 |\\|| **83.1** |  \n\n**Title**: \u201cPD\u201d or \u201cFD\u201d means to add knowledge distillation on the pre-training or the fine-tuning stage.\n\nWe have two observations from the results: 1) NAS-BERT outperforms the BERT baseline at different settings; 2) distillation on either pre-training or fine-tuning stage can improve the scores, and two-stage distillation can further get better performance. \n\n**[Continuing to pre-training on large scale unlabeled data]**\n\nWe are now continuing to pre-train on the large-scale dataset (160GB)  to obtain a better model. Due to the huge training cost, we will update the results when it is finished.    \n\n**[More results on the challenging QA tasks]**\n\nWe follow your suggestions and are conducting experiments on SQuAD 1.1 and SQuAD 2.0. We will update the results when it is finished."}, "signatures": ["ICLR.cc/2021/Conference/Paper1743/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1743/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Task-Agnostic and Adaptive-Size BERT Compression", "authorids": ["~Jin_Xu5", "~Xu_Tan1", "~Renqian_Luo1", "~Kaitao_Song1", "~Li_Jian1", "~Tao_Qin1", "~Tie-Yan_Liu1"], "authors": ["Jin Xu", "Xu Tan", "Renqian Luo", "Kaitao Song", "Li Jian", "Tao Qin", "Tie-Yan Liu"], "keywords": ["BERT compression", "neural architecture search", "adaptive sizes", "across tasks", "knowledge distillation"], "abstract": "While pre-trained language models such as BERT and RoBERTa have achieved impressive results on various natural language processing tasks, they have huge numbers of parameters and suffer from huge computational and memory costs, which make them difficult for real-world deployment. Hence, model compression should be performed in order to reduce the computation and memory cost of pre-trained models. In this work, we aim to compress BERT and address the following two challenging practical issues: (1) The compression algorithm should be able to output multiple compressed models with different sizes and latencies, so as to support devices with different kinds of memory and latency limitations; (2) the algorithm should be downstream task agnostic, so that the compressed models are generally applicable for different downstream tasks.  We leverage techniques in neural architecture search (NAS) and propose NAS-BERT, an efficient method for BERT compression. NAS-BERT trains a big supernet on a carefully designed search space containing various architectures and outputs multiple compressed models with adaptive sizes and latency. Furthermore, the training of NAS-BERT is conducted on standard self-supervised pre-training tasks (e.g., masked language model) and does not depend on specific downstream tasks. Thus, the models it produces can be used across various downstream tasks. The technical challenge of NAS-BERT is that training a big supernet on the pre-training task is extremely costly. We employ several techniques including block-wise search, search space pruning, and performance approximation to improve search efficiency and accuracy. Extensive experiments on GLUE benchmark datasets demonstrate that NAS-BERT can find lightweight models with better accuracy than previous approaches, and can be directly applied to different downstream tasks with adaptive model sizes for different requirements of memory or latency.", "one-sentence_summary": "we propose NAS-BERT, which leverages neural architecture search for BERT compression with adaptive model sizes and across downstream tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|taskagnostic_and_adaptivesize_bert_compression", "pdf": "/pdf/b9075151b7b155f2bc523a3e13c7717f7cff6256.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8SFXuIpUMM", "_bibtex": "@misc{\nxu2021taskagnostic,\ntitle={Task-Agnostic and Adaptive-Size {\\{}BERT{\\}} Compression},\nauthor={Jin Xu and Xu Tan and Renqian Luo and Kaitao Song and Li Jian and Tao Qin and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=wZ4yWvQ_g2y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "wZ4yWvQ_g2y", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1743/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1743/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1743/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1743/Authors|ICLR.cc/2021/Conference/Paper1743/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1743/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856205, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1743/-/Official_Comment"}}}, {"id": "kkBhgG8GFsn", "original": null, "number": 8, "cdate": 1605890827616, "ddate": null, "tcdate": 1605890827616, "tmdate": 1605933616793, "tddate": null, "forum": "wZ4yWvQ_g2y", "replyto": "1ALXJuPFZJ8", "invitation": "ICLR.cc/2021/Conference/Paper1743/-/Official_Comment", "content": {"title": "Response to AnonReviewer4 (Part 2)", "comment": "**[Comparison with DynaBERT, TinyBERT, MobileBERT and AdaBERT]** \n\n**DynaBERT**\n\nDynaBERT has multiple configurations (40M-110M) for each specific task and has an overlap (40M-60M) compared with NAS-BERT. Thus we use $\\text{NAS-BERT}_{30,60}$ to compare with DynaBERT as follows: \n \n| Model     |\\|| Params |\\|| MNLI |\\||  QQP |\\|| QNLI |\\|| CoLA |\\|| SST-2 |\\|| STS-B |\\||  RTE |\\|| MRPC |\\||  AVG |\n|-----------|:-:|:------:|:-:|:----:|:-:|:----:|:-:|:----:|:-:|:----:|:-:|:-----:|:-:|:-----:|:-:|:----:|:-:|:----:|:-:|:----:|\n| DynaBERT* |\\||   60M  |\\|| 84.2 |\\|| 91.2 |\\|| 91.5 |\\|| 56.8 |\\||  92.7 |\\||  89.2 |\\|| 72.2 |\\|| 84.1 |\\|| 82.7 |\n| NAS-BERT  |\\||   60M  |\\|| 84.1 |\\||  91.0  |\\|| 91.3 |\\|| 58.1 |\\||  92.1 |\\||  89.4 |\\|| 79.2 |\\|| 88.5 |\\|| 84.2 |\n| NSA-BERT* |\\||   60M  |\\|| 84.8 |\\|| 91.2 |\\|| 91.9 |\\|| 58.7 |\\||  93.1 |\\||  89.9 |\\|| 79.8 |\\|| 88.9 |\\|| **84.8** |\n| DynaBERT* |\\||   40M  |\\||  82.0  |\\|| 90.4 |\\|| 88.5 |\\|| 43.7 |\\||   92.0  |\\||   87.0 |\\|| 63.2 |\\|| 81.4 |\\|| 78.5 |\n| NAS-BERT  |\\||   30M  |\\||  81.0  |\\|| 90.2 |\\|| 88.4 |\\|| 48.7 |\\||  90.5 |\\||  87.6 |\\|| 71.8 |\\|| 84.6 |\\|| 80.3 |\n| NAS-BERT* |\\||   30M  |\\||  82.0  |\\|| 90.4 |\\||  89.0  |\\|| 46.3 |\\||  92.1 |\\||  88.9 |\\|| 73.5 |\\|| 88.7 |\\|| **81.4** |\n\n'*' means using data augmentation.\n\nWe can find that our models achieve better performance than DynaBERT and especially, NAS-BERT with only 30M parameters outperforms DynaBERT with 40M parameters by 2.9 average GLUE score. \n\n**TinyBERT**\n\nNote that TinyBERT used sophisticated distillation techniques such as hidden layer distillation, attention score distillation. Even so, our NAS-BERT (60M) outperforms TinyBERT (66M) on GLUE tasks in terms of the average score as follows. We also update the results in the revised paper. \n\n| Model    |\\|| Params |\\|| MNLI |\\||  QQP |\\|| QNLI |\\|| CoLA |\\|| SST-2 |\\|| STS-B |\\||  RTE |\\|| MRPC |\\||  AVG |\n|----------|:-:|:------:|:-:|:----:|:-:|:----:|:-:|:----:|:-:|:----:|:-:|:-----:|:-:|:-----:|:-:|:----:|:-:|:----:|:-:|:----:|\n| TinyBERT |\\||   66M  |\\|| 84.6 |\\|| 89.1 |\\|| 90.4 |\\|| 51.1 |\\||  93.1 |\\||  83.7 |\\|| 70.0 |\\|| 82.6 |\\|| 80.6 |\n| NAS-BERT |\\||   60M  |\\|| 84.1 |\\|| 88.8 |\\|| 91.2 |\\|| 50.5 |\\||  92.6 |\\||  86.9 |\\|| 72.7 |\\|| 86.4 |\\|| **81.7** |\n\n**MobileBERT**\n\nFor MobileBERT, it is unfair to compare with it because the teacher model (IB-BERT) of MobileBERT achieves the performance close to the $\\text{BERT}_{\\text{large}}$, which is much better than our teacher model and those used in most related works such as TinyBERT, DynaBERT, DistilBERT. Anyway, we will also use a teacher model with similar performance for our NAS-BERT and compare with MobileBERT, which we leave for future work.\n\n**AdaBERT**\n\nFor AdaBERT [5], it searches a task-specific architecture (6M - 10M) for each task and also introduces the special distillation techniques (e.g., using probe classifiers to hierarchically decompose the task-useful knowledge from the teacher model) and data augmentation. For fair comparison, we compare NAS-BERT (5M, without two-stage distillation) with AdaBERT (without data augmentation and probe classifiers, i.e., the results from Table 4 in [5]) as follows. \n\n| Setting  |\\|| QNLI/Params |\\|| MRPC/Params |\\|| RTE/Params |\n|----------|:-:|:-------------:|:-:|:-------------:|:-:|:------------:|\n| AdaBERT  |\\|| 82.0/7.9M   |\\|| 77.2/7.5M   |\\|| 56.7/8.6M  |\n| NAS-BERT |\\|| 83.9/5.0M   |\\|| 80.0/5.0M   |\\|| 67.0/5.0M  | \n\nWe can find that NAS-BERT (5M) outperforms AdaBERT with even slightly more parameters. For future work, we will also compare NAS-BERT with AdaBERT using its special distillation techniques and data augmentation."}, "signatures": ["ICLR.cc/2021/Conference/Paper1743/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1743/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Task-Agnostic and Adaptive-Size BERT Compression", "authorids": ["~Jin_Xu5", "~Xu_Tan1", "~Renqian_Luo1", "~Kaitao_Song1", "~Li_Jian1", "~Tao_Qin1", "~Tie-Yan_Liu1"], "authors": ["Jin Xu", "Xu Tan", "Renqian Luo", "Kaitao Song", "Li Jian", "Tao Qin", "Tie-Yan Liu"], "keywords": ["BERT compression", "neural architecture search", "adaptive sizes", "across tasks", "knowledge distillation"], "abstract": "While pre-trained language models such as BERT and RoBERTa have achieved impressive results on various natural language processing tasks, they have huge numbers of parameters and suffer from huge computational and memory costs, which make them difficult for real-world deployment. Hence, model compression should be performed in order to reduce the computation and memory cost of pre-trained models. In this work, we aim to compress BERT and address the following two challenging practical issues: (1) The compression algorithm should be able to output multiple compressed models with different sizes and latencies, so as to support devices with different kinds of memory and latency limitations; (2) the algorithm should be downstream task agnostic, so that the compressed models are generally applicable for different downstream tasks.  We leverage techniques in neural architecture search (NAS) and propose NAS-BERT, an efficient method for BERT compression. NAS-BERT trains a big supernet on a carefully designed search space containing various architectures and outputs multiple compressed models with adaptive sizes and latency. Furthermore, the training of NAS-BERT is conducted on standard self-supervised pre-training tasks (e.g., masked language model) and does not depend on specific downstream tasks. Thus, the models it produces can be used across various downstream tasks. The technical challenge of NAS-BERT is that training a big supernet on the pre-training task is extremely costly. We employ several techniques including block-wise search, search space pruning, and performance approximation to improve search efficiency and accuracy. Extensive experiments on GLUE benchmark datasets demonstrate that NAS-BERT can find lightweight models with better accuracy than previous approaches, and can be directly applied to different downstream tasks with adaptive model sizes for different requirements of memory or latency.", "one-sentence_summary": "we propose NAS-BERT, which leverages neural architecture search for BERT compression with adaptive model sizes and across downstream tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|taskagnostic_and_adaptivesize_bert_compression", "pdf": "/pdf/b9075151b7b155f2bc523a3e13c7717f7cff6256.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8SFXuIpUMM", "_bibtex": "@misc{\nxu2021taskagnostic,\ntitle={Task-Agnostic and Adaptive-Size {\\{}BERT{\\}} Compression},\nauthor={Jin Xu and Xu Tan and Renqian Luo and Kaitao Song and Li Jian and Tao Qin and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=wZ4yWvQ_g2y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "wZ4yWvQ_g2y", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1743/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1743/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1743/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1743/Authors|ICLR.cc/2021/Conference/Paper1743/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1743/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856205, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1743/-/Official_Comment"}}}, {"id": "_n7HSBTMfR", "original": null, "number": 11, "cdate": 1605891456022, "ddate": null, "tcdate": 1605891456022, "tmdate": 1605891456022, "tddate": null, "forum": "wZ4yWvQ_g2y", "replyto": "mIO2WZIaXF3", "invitation": "ICLR.cc/2021/Conference/Paper1743/-/Official_Comment", "content": {"title": "Response to AnonReviewer1 (Part 2)", "comment": "**[Comparison with MobileBERT and TinyBERT; Adding FLOPs and Latency in Table 1]**\n\n**TinyBERT**\n\nNote that TinyBERT used sophisticated distillation techniques such as hidden layer distillation, attention score distillation. Even so, our NAS-BERT (60M) outperforms TinyBERT (66M) on GLUE tasks in terms of the average score as follows. We also update the result in the revised paper. \n\n| Model    |\\|| Params |\\|| MNLI |\\||  QQP |\\|| QNLI |\\|| CoLA |\\|| SST-2 |\\|| STS-B |\\||  RTE |\\|| MRPC |\\||  AVG |\n|----------|:-:|:------:|:-:|:----:|:-:|:----:|:-:|:----:|:-:|:----:|:-:|:-----:|:-:|:-----:|:-:|:----:|:-:|:----:|:-:|:----:|\n| TinyBERT |\\||   66M  |\\|| 84.6 |\\|| 89.1 |\\|| 90.4 |\\|| 51.1 |\\||  93.1 |\\||  83.7 |\\|| 70.0 |\\|| 82.6 |\\|| 80.6 |\n| NAS-BERT |\\||   60M  |\\|| 84.1 |\\|| 88.8 |\\|| 91.2 |\\|| 50.5 |\\||  92.6 |\\||  86.9 |\\|| 72.7 |\\|| 86.4 |\\|| **81.7** |\n\n**MobileBERT**\n\nFor MobileBERT, it is unfair to compare with it because the teacher model (IB-BERT) of MobileBERT achieves the performance close to $\\text{BERT}_{\\text{large}}$, which is much better than our teacher model and those used in most related works such as TinyBERT, DynaBERT, DistilBERT. We will also use a teacher model with similar performance for our NAS-BERT and compare it with MobileBERT, which we leave for future work.\n\n**FLOPs and Latency**\n\nWe have added the latency and FLOPs in Table 2  in the revised version (i.e., Table 1 in the submitted version).    \n\n**[More discussion with related work]**\n\nThanks for our suggestions! We have cited the two papers [1,2] and made discussions in our updated version.\n\n\n\n---------Work in Progress---------\n\n-Pre-train on the large-scale dataset (160GB)  to obtain better model. \n\n-Experiments on the SQuAD 1.1 and SQuAD 2.0     \n\n[1] HAT: Hardware-Aware Transformers for Efficient Natural Language Processing  \n[2] Finding Fast Transformers: One-Shot Neural Architecture Search by Component Composition and provided some discussion between these methods."}, "signatures": ["ICLR.cc/2021/Conference/Paper1743/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1743/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Task-Agnostic and Adaptive-Size BERT Compression", "authorids": ["~Jin_Xu5", "~Xu_Tan1", "~Renqian_Luo1", "~Kaitao_Song1", "~Li_Jian1", "~Tao_Qin1", "~Tie-Yan_Liu1"], "authors": ["Jin Xu", "Xu Tan", "Renqian Luo", "Kaitao Song", "Li Jian", "Tao Qin", "Tie-Yan Liu"], "keywords": ["BERT compression", "neural architecture search", "adaptive sizes", "across tasks", "knowledge distillation"], "abstract": "While pre-trained language models such as BERT and RoBERTa have achieved impressive results on various natural language processing tasks, they have huge numbers of parameters and suffer from huge computational and memory costs, which make them difficult for real-world deployment. Hence, model compression should be performed in order to reduce the computation and memory cost of pre-trained models. In this work, we aim to compress BERT and address the following two challenging practical issues: (1) The compression algorithm should be able to output multiple compressed models with different sizes and latencies, so as to support devices with different kinds of memory and latency limitations; (2) the algorithm should be downstream task agnostic, so that the compressed models are generally applicable for different downstream tasks.  We leverage techniques in neural architecture search (NAS) and propose NAS-BERT, an efficient method for BERT compression. NAS-BERT trains a big supernet on a carefully designed search space containing various architectures and outputs multiple compressed models with adaptive sizes and latency. Furthermore, the training of NAS-BERT is conducted on standard self-supervised pre-training tasks (e.g., masked language model) and does not depend on specific downstream tasks. Thus, the models it produces can be used across various downstream tasks. The technical challenge of NAS-BERT is that training a big supernet on the pre-training task is extremely costly. We employ several techniques including block-wise search, search space pruning, and performance approximation to improve search efficiency and accuracy. Extensive experiments on GLUE benchmark datasets demonstrate that NAS-BERT can find lightweight models with better accuracy than previous approaches, and can be directly applied to different downstream tasks with adaptive model sizes for different requirements of memory or latency.", "one-sentence_summary": "we propose NAS-BERT, which leverages neural architecture search for BERT compression with adaptive model sizes and across downstream tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|taskagnostic_and_adaptivesize_bert_compression", "pdf": "/pdf/b9075151b7b155f2bc523a3e13c7717f7cff6256.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8SFXuIpUMM", "_bibtex": "@misc{\nxu2021taskagnostic,\ntitle={Task-Agnostic and Adaptive-Size {\\{}BERT{\\}} Compression},\nauthor={Jin Xu and Xu Tan and Renqian Luo and Kaitao Song and Li Jian and Tao Qin and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=wZ4yWvQ_g2y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "wZ4yWvQ_g2y", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1743/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1743/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1743/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1743/Authors|ICLR.cc/2021/Conference/Paper1743/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1743/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856205, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1743/-/Official_Comment"}}}, {"id": "H9qQrNhWefv", "original": null, "number": 9, "cdate": 1605890915181, "ddate": null, "tcdate": 1605890915181, "tmdate": 1605890915181, "tddate": null, "forum": "wZ4yWvQ_g2y", "replyto": "kkBhgG8GFsn", "invitation": "ICLR.cc/2021/Conference/Paper1743/-/Official_Comment", "content": {"title": "Response to AnonReviewer4 (Part 3)", "comment": "**[Discussion with related work - HAT]** \n\nHAT [6] also trains a big supernet to generate various architectures. There are three differences between NAS-BERT and HAT: \n- From the perspective of tasks, HAT mainly focuses on the machine translation task but NAS-BERT focuses on the BERT pre-training task. More importantly, BERT pre-training is extremely heavy.  Therefore, we propose block-wise search, progressive shrinking and performance approximation to handle it.  \n- From the perspective of the delivered architectures, HAT aims to generate models with different widths and depths, while NAS-BERT aims to generate novel architectures including width, depth, and novel combination of operations. \n- From the perspective of search algorithms, HAT uses an evolutionary search algorithm (a heuristic algorithm) with the help of a latency predictor, considering it is exhausting to search over all the models in the supernet. Different from HAT, NAS-BERT progressively prunes the search space during the supernet training stage and leaves more computation resources to the promising architectures, considering the huge cost of BERT pre-training task, and most architectures are not good enough and not necessary to be kept till the model selection stage. In the model selection stage, we only have limited but good architectures and can directly build a look-up table and select an architecture under given constraints. \nWe also add the discussion and comparison with HAT [6] in our revised paper.\n\n[1] Li C, Peng J, Yuan L, et al. Block-wisely Supervised Neural Architecture Search with Knowledge Distillation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 1989-1998.\n[2] Pham H, Guan M Y, Zoph B, et al. Efficient Neural Architecture Search via Parameter Sharing[C]//ICML. 2018.  \n[3] Cai H, Zhu L, Han S. ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware[C]//International Conference on Learning Representations. 2018.  \n[4] Xu Y, Xie L, Zhang X, et al. Pc-darts: Partial channel connections for memory-efficient differentiable architecture search[J]. arXiv preprint arXiv:1907.05737, 2019.  \n[5] Chen D, Li Y, Qiu M, et al. Adabert: Task-adaptive bert compression with differentiable neural architecture search[J]. arXiv preprint arXiv:2001.04246, 2020.  \n[6] HAT: Hardware-Aware Transformers for Efficient Natural Language Processing "}, "signatures": ["ICLR.cc/2021/Conference/Paper1743/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1743/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Task-Agnostic and Adaptive-Size BERT Compression", "authorids": ["~Jin_Xu5", "~Xu_Tan1", "~Renqian_Luo1", "~Kaitao_Song1", "~Li_Jian1", "~Tao_Qin1", "~Tie-Yan_Liu1"], "authors": ["Jin Xu", "Xu Tan", "Renqian Luo", "Kaitao Song", "Li Jian", "Tao Qin", "Tie-Yan Liu"], "keywords": ["BERT compression", "neural architecture search", "adaptive sizes", "across tasks", "knowledge distillation"], "abstract": "While pre-trained language models such as BERT and RoBERTa have achieved impressive results on various natural language processing tasks, they have huge numbers of parameters and suffer from huge computational and memory costs, which make them difficult for real-world deployment. Hence, model compression should be performed in order to reduce the computation and memory cost of pre-trained models. In this work, we aim to compress BERT and address the following two challenging practical issues: (1) The compression algorithm should be able to output multiple compressed models with different sizes and latencies, so as to support devices with different kinds of memory and latency limitations; (2) the algorithm should be downstream task agnostic, so that the compressed models are generally applicable for different downstream tasks.  We leverage techniques in neural architecture search (NAS) and propose NAS-BERT, an efficient method for BERT compression. NAS-BERT trains a big supernet on a carefully designed search space containing various architectures and outputs multiple compressed models with adaptive sizes and latency. Furthermore, the training of NAS-BERT is conducted on standard self-supervised pre-training tasks (e.g., masked language model) and does not depend on specific downstream tasks. Thus, the models it produces can be used across various downstream tasks. The technical challenge of NAS-BERT is that training a big supernet on the pre-training task is extremely costly. We employ several techniques including block-wise search, search space pruning, and performance approximation to improve search efficiency and accuracy. Extensive experiments on GLUE benchmark datasets demonstrate that NAS-BERT can find lightweight models with better accuracy than previous approaches, and can be directly applied to different downstream tasks with adaptive model sizes for different requirements of memory or latency.", "one-sentence_summary": "we propose NAS-BERT, which leverages neural architecture search for BERT compression with adaptive model sizes and across downstream tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|taskagnostic_and_adaptivesize_bert_compression", "pdf": "/pdf/b9075151b7b155f2bc523a3e13c7717f7cff6256.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8SFXuIpUMM", "_bibtex": "@misc{\nxu2021taskagnostic,\ntitle={Task-Agnostic and Adaptive-Size {\\{}BERT{\\}} Compression},\nauthor={Jin Xu and Xu Tan and Renqian Luo and Kaitao Song and Li Jian and Tao Qin and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=wZ4yWvQ_g2y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "wZ4yWvQ_g2y", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1743/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1743/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1743/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1743/Authors|ICLR.cc/2021/Conference/Paper1743/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1743/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856205, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1743/-/Official_Comment"}}}, {"id": "1ALXJuPFZJ8", "original": null, "number": 7, "cdate": 1605890299826, "ddate": null, "tcdate": 1605890299826, "tmdate": 1605890299826, "tddate": null, "forum": "wZ4yWvQ_g2y", "replyto": "mMhsFT_vAAw", "invitation": "ICLR.cc/2021/Conference/Paper1743/-/Official_Comment", "content": {"title": "Response to AnonReviewer4 (Part 1)", "comment": "Thanks for your helpful review comments. Below are our responses to your concerns.\n\n**[Organization of the paper]**\n\nThanks for your advice. We have re-organized in the new version as follows: 1) We move the detailed description of search space design from the Appendix to Section 3.1. 2) To enable better comparison of accuracy versus latency/#parameters of different models, we add more information in the Table 2 in the revised paper. 3) We move the experiment description of progreive shrinking from the Appendix to Section 4.2. 4) We add the discussions with other works including HAT in the related work part. \n    \n**[About the novelty of NAS-BERT]**\n\n- To achieve task-agnostic and adaptive-size BERT compression, we conduct neural architecture search by training the big supernet on the pre-training stage, which is extremely costly. Our proposed techniques including block-wise search, progressive shrinking, and performance approximation all target for alleviating the pain of heavy training cost and improving the search efficiency. \n    - Different from block-wise search in [1], we add identity operation to automatically search models with adaptive depths in each block. [1] achieves this by manually training different depths to get several models, which is acceptable in lightweight image tasks, but is extremely costly in BERT pre-training task and cannot support devices with different kinds of memory and latency limitations. \n    - Besides block-wise search, our newly proposed progressive shrinking and performance approximation are critical for efficiency and effectiveness. Specifically, for progressive shrinking, we split the models into different bins to prevent the smaller models from being pruned and guarantee that we can get various architectures to meet different constraints. \n- We think our work is not simply using existing techniques, but developing the existing techniques (block-wise search) and designing new techniques (progressive shrinking, performance approximation, novel search space for BERT) to solve the unique challenges in task-agnostic and adaptive-size BERT compression. \n- From the field of neural architecture search, most algorithms [2,3,4] are proposed to improve the performance on the computer vision tasks and light-weight NLP tasks (e.g., PTB). We extend NAS to the much heavy BERT pre-training task and propose a series of techniques to improve the efficiency, which may inspire a lot of following works on NAS for heavy tasks."}, "signatures": ["ICLR.cc/2021/Conference/Paper1743/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1743/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Task-Agnostic and Adaptive-Size BERT Compression", "authorids": ["~Jin_Xu5", "~Xu_Tan1", "~Renqian_Luo1", "~Kaitao_Song1", "~Li_Jian1", "~Tao_Qin1", "~Tie-Yan_Liu1"], "authors": ["Jin Xu", "Xu Tan", "Renqian Luo", "Kaitao Song", "Li Jian", "Tao Qin", "Tie-Yan Liu"], "keywords": ["BERT compression", "neural architecture search", "adaptive sizes", "across tasks", "knowledge distillation"], "abstract": "While pre-trained language models such as BERT and RoBERTa have achieved impressive results on various natural language processing tasks, they have huge numbers of parameters and suffer from huge computational and memory costs, which make them difficult for real-world deployment. Hence, model compression should be performed in order to reduce the computation and memory cost of pre-trained models. In this work, we aim to compress BERT and address the following two challenging practical issues: (1) The compression algorithm should be able to output multiple compressed models with different sizes and latencies, so as to support devices with different kinds of memory and latency limitations; (2) the algorithm should be downstream task agnostic, so that the compressed models are generally applicable for different downstream tasks.  We leverage techniques in neural architecture search (NAS) and propose NAS-BERT, an efficient method for BERT compression. NAS-BERT trains a big supernet on a carefully designed search space containing various architectures and outputs multiple compressed models with adaptive sizes and latency. Furthermore, the training of NAS-BERT is conducted on standard self-supervised pre-training tasks (e.g., masked language model) and does not depend on specific downstream tasks. Thus, the models it produces can be used across various downstream tasks. The technical challenge of NAS-BERT is that training a big supernet on the pre-training task is extremely costly. We employ several techniques including block-wise search, search space pruning, and performance approximation to improve search efficiency and accuracy. Extensive experiments on GLUE benchmark datasets demonstrate that NAS-BERT can find lightweight models with better accuracy than previous approaches, and can be directly applied to different downstream tasks with adaptive model sizes for different requirements of memory or latency.", "one-sentence_summary": "we propose NAS-BERT, which leverages neural architecture search for BERT compression with adaptive model sizes and across downstream tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|taskagnostic_and_adaptivesize_bert_compression", "pdf": "/pdf/b9075151b7b155f2bc523a3e13c7717f7cff6256.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8SFXuIpUMM", "_bibtex": "@misc{\nxu2021taskagnostic,\ntitle={Task-Agnostic and Adaptive-Size {\\{}BERT{\\}} Compression},\nauthor={Jin Xu and Xu Tan and Renqian Luo and Kaitao Song and Li Jian and Tao Qin and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=wZ4yWvQ_g2y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "wZ4yWvQ_g2y", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1743/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1743/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1743/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1743/Authors|ICLR.cc/2021/Conference/Paper1743/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1743/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856205, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1743/-/Official_Comment"}}}, {"id": "OUrjJ-SAFJg", "original": null, "number": 6, "cdate": 1605890103931, "ddate": null, "tcdate": 1605890103931, "tmdate": 1605890103931, "tddate": null, "forum": "wZ4yWvQ_g2y", "replyto": "UFUuFRphoY", "invitation": "ICLR.cc/2021/Conference/Paper1743/-/Official_Comment", "content": {"title": "Response to AnonReviewer2 (Part 2)", "comment": "**[About the novelty of NAS-BERT]**\n\n- Applying neural architecture search directly on the heavy pre-training task is challenging. Our proposed techniques including block-wise search, progressive shrinking, and performance approximation all target for alleviating the pain of heavy training cost and improving the search efficiency. \n- Different from block-wise search in [1], we add identity operation to automatically search models with adaptive depths in each block, which is not supported by [1], and thus [1] cannot automatically search models with adaptive depths. [1] solves this problem by manually training different depths to get several models, which is acceptable in lightweight image tasks, but is extremely costly in BERT pre-training task and cannot support devices with different kinds of memory and latency limitations. \n- Note that besides block-wise search, our newly proposed progressive shrinking and performance approximation are critical for efficiency and effectiveness. Specifically, for progressive shrinking, we split the models into different bins to prevent the smaller models from being pruned and guarantee that we can get various architectures to meet different constraints.  \n \nWe think our work is not simply using existing techniques, but developing the existing techniques and designing new techniques to solve the unique challenges in task-agnostic and adaptive-size BERT compression. \n\n[1] Li C, Peng J, Yuan L, et al. Block-wisely Supervised Neural Architecture Search with Knowledge Distillation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 1989-1998."}, "signatures": ["ICLR.cc/2021/Conference/Paper1743/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1743/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Task-Agnostic and Adaptive-Size BERT Compression", "authorids": ["~Jin_Xu5", "~Xu_Tan1", "~Renqian_Luo1", "~Kaitao_Song1", "~Li_Jian1", "~Tao_Qin1", "~Tie-Yan_Liu1"], "authors": ["Jin Xu", "Xu Tan", "Renqian Luo", "Kaitao Song", "Li Jian", "Tao Qin", "Tie-Yan Liu"], "keywords": ["BERT compression", "neural architecture search", "adaptive sizes", "across tasks", "knowledge distillation"], "abstract": "While pre-trained language models such as BERT and RoBERTa have achieved impressive results on various natural language processing tasks, they have huge numbers of parameters and suffer from huge computational and memory costs, which make them difficult for real-world deployment. Hence, model compression should be performed in order to reduce the computation and memory cost of pre-trained models. In this work, we aim to compress BERT and address the following two challenging practical issues: (1) The compression algorithm should be able to output multiple compressed models with different sizes and latencies, so as to support devices with different kinds of memory and latency limitations; (2) the algorithm should be downstream task agnostic, so that the compressed models are generally applicable for different downstream tasks.  We leverage techniques in neural architecture search (NAS) and propose NAS-BERT, an efficient method for BERT compression. NAS-BERT trains a big supernet on a carefully designed search space containing various architectures and outputs multiple compressed models with adaptive sizes and latency. Furthermore, the training of NAS-BERT is conducted on standard self-supervised pre-training tasks (e.g., masked language model) and does not depend on specific downstream tasks. Thus, the models it produces can be used across various downstream tasks. The technical challenge of NAS-BERT is that training a big supernet on the pre-training task is extremely costly. We employ several techniques including block-wise search, search space pruning, and performance approximation to improve search efficiency and accuracy. Extensive experiments on GLUE benchmark datasets demonstrate that NAS-BERT can find lightweight models with better accuracy than previous approaches, and can be directly applied to different downstream tasks with adaptive model sizes for different requirements of memory or latency.", "one-sentence_summary": "we propose NAS-BERT, which leverages neural architecture search for BERT compression with adaptive model sizes and across downstream tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|taskagnostic_and_adaptivesize_bert_compression", "pdf": "/pdf/b9075151b7b155f2bc523a3e13c7717f7cff6256.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8SFXuIpUMM", "_bibtex": "@misc{\nxu2021taskagnostic,\ntitle={Task-Agnostic and Adaptive-Size {\\{}BERT{\\}} Compression},\nauthor={Jin Xu and Xu Tan and Renqian Luo and Kaitao Song and Li Jian and Tao Qin and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=wZ4yWvQ_g2y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "wZ4yWvQ_g2y", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1743/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1743/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1743/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1743/Authors|ICLR.cc/2021/Conference/Paper1743/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1743/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856205, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1743/-/Official_Comment"}}}, {"id": "hh3O5Rj3d--", "original": null, "number": 3, "cdate": 1605889776210, "ddate": null, "tcdate": 1605889776210, "tmdate": 1605889776210, "tddate": null, "forum": "wZ4yWvQ_g2y", "replyto": "FE5Cbb5aWoL", "invitation": "ICLR.cc/2021/Conference/Paper1743/-/Official_Comment", "content": {"title": "Response to AnonReviewer3 (Part 2)", "comment": "**[About the novelty]** \n\nWe respectfully disagree with your comment on novelty and would like to emphasize that our method is not simply a combination of existing ideas. \n\n- The key challenge for task-agnostic and adaptive-size BERT compression is computational complexity, as both neural architecture search and pre-training are extremely costly. Our main innovation is to reduce complexity for neural architecture search in pre-training, and we introduce three techniques: block-wise search, progressive shrinking, and performance approximation, to reduce the training cost, improve search efficiency and achieve strong experiment results. \n- Note that besides block-wise search, our newly proposed progressive shrinking and performance approximation are critical for efficiency and effectiveness. Specifically, for progressive shrinking, we split the models into different bins to prevent the smaller models from being pruned and guarantee that we can get various architectures to meet different constraints.\n- Even if for block-wise search, our method has the following key differences and main advantages compared with block-wise search. We add identity operation to allow an adaptive depth of searched architectures, which is not considered in [1], and thus [1] cannot automatically search models with adaptive depths. [1] solves this problem by manually training different depths to get several models, which is acceptable in lightweight image tasks, but is extremely costly in BERT pre-training task and cannot support devices with different kinds of memory and latency limitations.\n- Once-for-all is a general idea to train a supernet with multiple architectures, and different works adopt different methods to achieve this purpose. For original once-for-all [2] and BigNAS [3], they rely on special designs (elastic kernel size, etc.) to train an adaptive network, which cannot explore novel architectures.  However, NAS-BERT can search for novel architectures. Specifically, we design search space including MHA, FFN, and especially the lightweight and efficient separable convolution operation, which demonstrate to be very helpful for BERT compression.\n\n**[About whether NAS-BERT is a good way to obtain the desired task-agnostic compressed models]**\n\n- For the first comment: the excellence of these models are tied to the specific set of shared parameters obtained. Note that after obtaining the searched model, we pre-train it from scratch without inheriting weight from the supernet and thus the excellence of NAS-BERT is not tied to the specific set of shared parameters.\n- For the second comment: the excellence of these models is tied to the pre-training task. Note that the pre-training task in BERT [4] is designed to learn general representation, which can generalize well on downstream language understanding tasks, and is downstream task agnostic. The search of NAS-BERT is conducted on the pre-training stage. As long as a downstream task is well supported by BERT, it can be also well supported by NAS-BERT. Therefore, our NAS-BERT is task-agnostic from this point. The results in Table 2 and Table 3 in the paper demonstrate the effectiveness of NAS-BERT on GLUE tasks."}, "signatures": ["ICLR.cc/2021/Conference/Paper1743/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1743/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Task-Agnostic and Adaptive-Size BERT Compression", "authorids": ["~Jin_Xu5", "~Xu_Tan1", "~Renqian_Luo1", "~Kaitao_Song1", "~Li_Jian1", "~Tao_Qin1", "~Tie-Yan_Liu1"], "authors": ["Jin Xu", "Xu Tan", "Renqian Luo", "Kaitao Song", "Li Jian", "Tao Qin", "Tie-Yan Liu"], "keywords": ["BERT compression", "neural architecture search", "adaptive sizes", "across tasks", "knowledge distillation"], "abstract": "While pre-trained language models such as BERT and RoBERTa have achieved impressive results on various natural language processing tasks, they have huge numbers of parameters and suffer from huge computational and memory costs, which make them difficult for real-world deployment. Hence, model compression should be performed in order to reduce the computation and memory cost of pre-trained models. In this work, we aim to compress BERT and address the following two challenging practical issues: (1) The compression algorithm should be able to output multiple compressed models with different sizes and latencies, so as to support devices with different kinds of memory and latency limitations; (2) the algorithm should be downstream task agnostic, so that the compressed models are generally applicable for different downstream tasks.  We leverage techniques in neural architecture search (NAS) and propose NAS-BERT, an efficient method for BERT compression. NAS-BERT trains a big supernet on a carefully designed search space containing various architectures and outputs multiple compressed models with adaptive sizes and latency. Furthermore, the training of NAS-BERT is conducted on standard self-supervised pre-training tasks (e.g., masked language model) and does not depend on specific downstream tasks. Thus, the models it produces can be used across various downstream tasks. The technical challenge of NAS-BERT is that training a big supernet on the pre-training task is extremely costly. We employ several techniques including block-wise search, search space pruning, and performance approximation to improve search efficiency and accuracy. Extensive experiments on GLUE benchmark datasets demonstrate that NAS-BERT can find lightweight models with better accuracy than previous approaches, and can be directly applied to different downstream tasks with adaptive model sizes for different requirements of memory or latency.", "one-sentence_summary": "we propose NAS-BERT, which leverages neural architecture search for BERT compression with adaptive model sizes and across downstream tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|taskagnostic_and_adaptivesize_bert_compression", "pdf": "/pdf/b9075151b7b155f2bc523a3e13c7717f7cff6256.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8SFXuIpUMM", "_bibtex": "@misc{\nxu2021taskagnostic,\ntitle={Task-Agnostic and Adaptive-Size {\\{}BERT{\\}} Compression},\nauthor={Jin Xu and Xu Tan and Renqian Luo and Kaitao Song and Li Jian and Tao Qin and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=wZ4yWvQ_g2y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "wZ4yWvQ_g2y", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1743/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1743/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1743/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1743/Authors|ICLR.cc/2021/Conference/Paper1743/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1743/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856205, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1743/-/Official_Comment"}}}, {"id": "FE5Cbb5aWoL", "original": null, "number": 2, "cdate": 1605889607152, "ddate": null, "tcdate": 1605889607152, "tmdate": 1605889607152, "tddate": null, "forum": "wZ4yWvQ_g2y", "replyto": "uezBS89xMhk", "invitation": "ICLR.cc/2021/Conference/Paper1743/-/Official_Comment", "content": {"title": "Response to AnonReviewer3 (Part 1)", "comment": "Thanks for your constructive review comments. Below are our responses to your comments.\n\n**[Whether block-wise search is also used in student training and thus lead to unfair comparison with baselines]**\n\nOur pipeline consists of search stage and evaluation stage. The block-wise distillation is only conducted in the search stage to reduce the search space and improve the search efficiency. In the evaluation stage, the NAS-BERT model searched by our algorithm is pre-trained from scratch (without inheriting weights from the supernet) and then fine-tuned for a fair comparison with baseline. Thus, the improvement of our NAS-BERT comes from the discovered architectures. \n\nWe also conduct experiments on NAS-BERT and the baseline using block-wise distillation in the pre-training in evaluation stage. We split the student model into different blocks following Fig. 2 in the paper. Each block is trained under the supervision of the corresponding teacher block. Since block-wise distillation does not jointly optimize different blocks, we also use standard distillation as used in NAS-BERT in the second half of training steps. The results are shown as follows: \n\n| Model          |\\|| MNLI |\\||  QQP |\\|| QNLI |\\|| CoLA |\\|| SST-2 |\\|| STS-B |\\||  RTE |\\|| MRPC |\\||  AVG |\n|-------------------|:-:|:----:|:-:|:----:|:-:|:----:|:-:|:----:|:-:|:-----:|:-:|:-----:|:-:|:----:|:-:|:----:|:-:|:----:|\n| $\\text{BERT}_{60}$          |\\|| 83.2 |\\|| 90.5 |\\|| 90.2 |\\|| 56.3 |\\||  91.8 |\\||  88.8 |\\|| 78.5 |\\|| 88.5 |\\|| 83.5 |\n| $\\text{NAS-BERT}_{60}$        |\\|| 84.1 |\\||  91.0  |\\|| 91.3 |\\|| 58.1 |\\||  92.1 |\\||  89.4 |\\|| 79.2 |\\|| 88.5 |\\|| **84.2** |\n| $\\text{BERT}_{60}+\\text{Block}$    |\\|| 82.2 |\\|| 90.4 |\\||  90.0  |\\|| 54.4 |\\||   92.0  |\\||  88.7 |\\|| 74.9 |\\|| 87.7 |\\|| 82.5 |\n| $\\text{NAS-BERT}_{60}+\\text{Block}$|\\|| 83.1 |\\|| 90.8 |\\|| 90.3 |\\|| 54.8 |\\||  92.2 |\\||  89.2 |\\||  77.0  |\\||  87.0  |\\|| **83.1** |\n\n**Title**: Comparison between block-wise distillation and two-stage distillation. \"+Block\" means that the student model is trained with half of the training steps and continues to be trained with distillation as used in NAS-BERT with the remaining training steps.\n\nWe find that block-wise distillation performs worse than the normal distillation in both BERT baseline and NAS-BERT. In the block-wise distillation setting, NAS-BERT still outperforms BERT baseline."}, "signatures": ["ICLR.cc/2021/Conference/Paper1743/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1743/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Task-Agnostic and Adaptive-Size BERT Compression", "authorids": ["~Jin_Xu5", "~Xu_Tan1", "~Renqian_Luo1", "~Kaitao_Song1", "~Li_Jian1", "~Tao_Qin1", "~Tie-Yan_Liu1"], "authors": ["Jin Xu", "Xu Tan", "Renqian Luo", "Kaitao Song", "Li Jian", "Tao Qin", "Tie-Yan Liu"], "keywords": ["BERT compression", "neural architecture search", "adaptive sizes", "across tasks", "knowledge distillation"], "abstract": "While pre-trained language models such as BERT and RoBERTa have achieved impressive results on various natural language processing tasks, they have huge numbers of parameters and suffer from huge computational and memory costs, which make them difficult for real-world deployment. Hence, model compression should be performed in order to reduce the computation and memory cost of pre-trained models. In this work, we aim to compress BERT and address the following two challenging practical issues: (1) The compression algorithm should be able to output multiple compressed models with different sizes and latencies, so as to support devices with different kinds of memory and latency limitations; (2) the algorithm should be downstream task agnostic, so that the compressed models are generally applicable for different downstream tasks.  We leverage techniques in neural architecture search (NAS) and propose NAS-BERT, an efficient method for BERT compression. NAS-BERT trains a big supernet on a carefully designed search space containing various architectures and outputs multiple compressed models with adaptive sizes and latency. Furthermore, the training of NAS-BERT is conducted on standard self-supervised pre-training tasks (e.g., masked language model) and does not depend on specific downstream tasks. Thus, the models it produces can be used across various downstream tasks. The technical challenge of NAS-BERT is that training a big supernet on the pre-training task is extremely costly. We employ several techniques including block-wise search, search space pruning, and performance approximation to improve search efficiency and accuracy. Extensive experiments on GLUE benchmark datasets demonstrate that NAS-BERT can find lightweight models with better accuracy than previous approaches, and can be directly applied to different downstream tasks with adaptive model sizes for different requirements of memory or latency.", "one-sentence_summary": "we propose NAS-BERT, which leverages neural architecture search for BERT compression with adaptive model sizes and across downstream tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|taskagnostic_and_adaptivesize_bert_compression", "pdf": "/pdf/b9075151b7b155f2bc523a3e13c7717f7cff6256.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8SFXuIpUMM", "_bibtex": "@misc{\nxu2021taskagnostic,\ntitle={Task-Agnostic and Adaptive-Size {\\{}BERT{\\}} Compression},\nauthor={Jin Xu and Xu Tan and Renqian Luo and Kaitao Song and Li Jian and Tao Qin and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=wZ4yWvQ_g2y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "wZ4yWvQ_g2y", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1743/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1743/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1743/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1743/Authors|ICLR.cc/2021/Conference/Paper1743/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1743/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856205, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1743/-/Official_Comment"}}}, {"id": "nhZqyL8xXv6", "original": null, "number": 3, "cdate": 1603896311855, "ddate": null, "tcdate": 1603896311855, "tmdate": 1605024367911, "tddate": null, "forum": "wZ4yWvQ_g2y", "replyto": "wZ4yWvQ_g2y", "invitation": "ICLR.cc/2021/Conference/Paper1743/-/Official_Review", "content": {"title": "Official Review ", "review": "Summary\n\nThe paper develops a new method to compress the BERT model with varying model sizes depending on the underlying usage. They use block-wise neural architecture search to choose the best set of submodules for each of the blocks. To reduce the size of the exponential search space they progressively remove the architectural configurations that yields high loss. Over-all the paper is well written and nicely presented.\n\n+ve\n\n- The NAS-BERT can produce pertained models with varying model sizes which is better than DistilBERT and BERT-PKD that requires pre-training every time the number of layers are varied.\n- The paper conducts rigorous experiments to demonstrate the effectiveness of NAS-BERT. The baseline methods used for comparison covers most of the state-of-the-art methods used for model compression for BERT.\n\nConcerns:\n \n- Can the authors clarify if they save the model parameters corresponding to all the possible architectural choices? Or they find out the best configuration matching the model size and latency requirements and then do the pre-training again with those architectural choices.\n\n- In appendix A.7 the paper demonstrates some of the architectures used by NAS-BERT. For lower model size ( < 20M ) it can be observed that the NAS-BERT ends up choosing SepConv layers most of the times. Do the authors do any analysis on why SepConv layer works better than the self-attention layer and the feed forward network. How does the network perform if it is composed of all SepConv layers? Have the authors tried to use only SepConv layers and see if that itself gives good accuracy rather than doing the architecture search.\n\n- In terms of original ideas, although the concepts of block-wise architecture search, using SepConv layer for NLP tasks and using block-wise knowledge distillation are not novel by themselves but this paper has efficiently made use of the available techniques (along with efficient engineering work like progressively reducing the search space) to develop a method that gives good performance on NLP tasks.  ", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1743/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1743/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Task-Agnostic and Adaptive-Size BERT Compression", "authorids": ["~Jin_Xu5", "~Xu_Tan1", "~Renqian_Luo1", "~Kaitao_Song1", "~Li_Jian1", "~Tao_Qin1", "~Tie-Yan_Liu1"], "authors": ["Jin Xu", "Xu Tan", "Renqian Luo", "Kaitao Song", "Li Jian", "Tao Qin", "Tie-Yan Liu"], "keywords": ["BERT compression", "neural architecture search", "adaptive sizes", "across tasks", "knowledge distillation"], "abstract": "While pre-trained language models such as BERT and RoBERTa have achieved impressive results on various natural language processing tasks, they have huge numbers of parameters and suffer from huge computational and memory costs, which make them difficult for real-world deployment. Hence, model compression should be performed in order to reduce the computation and memory cost of pre-trained models. In this work, we aim to compress BERT and address the following two challenging practical issues: (1) The compression algorithm should be able to output multiple compressed models with different sizes and latencies, so as to support devices with different kinds of memory and latency limitations; (2) the algorithm should be downstream task agnostic, so that the compressed models are generally applicable for different downstream tasks.  We leverage techniques in neural architecture search (NAS) and propose NAS-BERT, an efficient method for BERT compression. NAS-BERT trains a big supernet on a carefully designed search space containing various architectures and outputs multiple compressed models with adaptive sizes and latency. Furthermore, the training of NAS-BERT is conducted on standard self-supervised pre-training tasks (e.g., masked language model) and does not depend on specific downstream tasks. Thus, the models it produces can be used across various downstream tasks. The technical challenge of NAS-BERT is that training a big supernet on the pre-training task is extremely costly. We employ several techniques including block-wise search, search space pruning, and performance approximation to improve search efficiency and accuracy. Extensive experiments on GLUE benchmark datasets demonstrate that NAS-BERT can find lightweight models with better accuracy than previous approaches, and can be directly applied to different downstream tasks with adaptive model sizes for different requirements of memory or latency.", "one-sentence_summary": "we propose NAS-BERT, which leverages neural architecture search for BERT compression with adaptive model sizes and across downstream tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|taskagnostic_and_adaptivesize_bert_compression", "pdf": "/pdf/b9075151b7b155f2bc523a3e13c7717f7cff6256.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8SFXuIpUMM", "_bibtex": "@misc{\nxu2021taskagnostic,\ntitle={Task-Agnostic and Adaptive-Size {\\{}BERT{\\}} Compression},\nauthor={Jin Xu and Xu Tan and Renqian Luo and Kaitao Song and Li Jian and Tao Qin and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=wZ4yWvQ_g2y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "wZ4yWvQ_g2y", "replyto": "wZ4yWvQ_g2y", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1743/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538111634, "tmdate": 1606915790228, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1743/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1743/-/Official_Review"}}}, {"id": "uezBS89xMhk", "original": null, "number": 4, "cdate": 1603931074555, "ddate": null, "tcdate": 1603931074555, "tmdate": 1605024367849, "tddate": null, "forum": "wZ4yWvQ_g2y", "replyto": "wZ4yWvQ_g2y", "invitation": "ICLR.cc/2021/Conference/Paper1743/-/Official_Review", "content": {"title": "Reasonable techniques and decent performance but with quite some complication", "review": "First of all, I believe the paper is looking into a very important question that attracts lots of attention recently. The set of techniques proposed in the work are also reasonable and practical, where the proposed progressive space pruning seems to work very well. Empirically, the obtained models do perform better compared to standard Transformer baselines. As for the comparison with previous methods, since there are too many implementation details that can affect the fairness of the comparison (e.g. length of pretraining, batch size, teacher performances, etc), it's hard to judge the actual scale of the gain.\n\nThere are also a few concerns. \n- Firstly, when the block-wise search is used, it feels like the NAS-BERT is trained in a way that is more similar to a variant of distillation that additionally utilizes intermediate hidden states. As this signal is not used in the standard BERT baseline, some improvement could actually come from this factor besides a better model (architecture+param). A better baseline could be a Transformer trained in a similar way.\n- Secondly, in terms of novelty, this work is more like the combination of existing ideas, namely \"once-for-all\" and \"block-wise search\". One general issue with \"once-for-all\" is after the search, although you obtain multiple models of different sizes, the excellence these models are tied to the (1) specific set of shared parameters obtained and (2) the pre-training task. So, whether this is really a good way to obtain the desired task-agnostic compressed models is still questionable.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1743/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1743/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Task-Agnostic and Adaptive-Size BERT Compression", "authorids": ["~Jin_Xu5", "~Xu_Tan1", "~Renqian_Luo1", "~Kaitao_Song1", "~Li_Jian1", "~Tao_Qin1", "~Tie-Yan_Liu1"], "authors": ["Jin Xu", "Xu Tan", "Renqian Luo", "Kaitao Song", "Li Jian", "Tao Qin", "Tie-Yan Liu"], "keywords": ["BERT compression", "neural architecture search", "adaptive sizes", "across tasks", "knowledge distillation"], "abstract": "While pre-trained language models such as BERT and RoBERTa have achieved impressive results on various natural language processing tasks, they have huge numbers of parameters and suffer from huge computational and memory costs, which make them difficult for real-world deployment. Hence, model compression should be performed in order to reduce the computation and memory cost of pre-trained models. In this work, we aim to compress BERT and address the following two challenging practical issues: (1) The compression algorithm should be able to output multiple compressed models with different sizes and latencies, so as to support devices with different kinds of memory and latency limitations; (2) the algorithm should be downstream task agnostic, so that the compressed models are generally applicable for different downstream tasks.  We leverage techniques in neural architecture search (NAS) and propose NAS-BERT, an efficient method for BERT compression. NAS-BERT trains a big supernet on a carefully designed search space containing various architectures and outputs multiple compressed models with adaptive sizes and latency. Furthermore, the training of NAS-BERT is conducted on standard self-supervised pre-training tasks (e.g., masked language model) and does not depend on specific downstream tasks. Thus, the models it produces can be used across various downstream tasks. The technical challenge of NAS-BERT is that training a big supernet on the pre-training task is extremely costly. We employ several techniques including block-wise search, search space pruning, and performance approximation to improve search efficiency and accuracy. Extensive experiments on GLUE benchmark datasets demonstrate that NAS-BERT can find lightweight models with better accuracy than previous approaches, and can be directly applied to different downstream tasks with adaptive model sizes for different requirements of memory or latency.", "one-sentence_summary": "we propose NAS-BERT, which leverages neural architecture search for BERT compression with adaptive model sizes and across downstream tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|taskagnostic_and_adaptivesize_bert_compression", "pdf": "/pdf/b9075151b7b155f2bc523a3e13c7717f7cff6256.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8SFXuIpUMM", "_bibtex": "@misc{\nxu2021taskagnostic,\ntitle={Task-Agnostic and Adaptive-Size {\\{}BERT{\\}} Compression},\nauthor={Jin Xu and Xu Tan and Renqian Luo and Kaitao Song and Li Jian and Tao Qin and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=wZ4yWvQ_g2y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "wZ4yWvQ_g2y", "replyto": "wZ4yWvQ_g2y", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1743/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538111634, "tmdate": 1606915790228, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1743/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1743/-/Official_Review"}}}], "count": 18}