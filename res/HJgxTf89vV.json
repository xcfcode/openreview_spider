{"notes": [{"id": "HJgxTf89vV", "original": "H1leRHsKDN", "number": 5, "cdate": 1552732856447, "ddate": null, "tcdate": 1552732856447, "tmdate": 1562082913649, "tddate": null, "forum": "HJgxTf89vV", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/drlStructPred/-/Blind_Submission", "content": {"title": "Learning proposals for sequential importance samplers using reinforced variational inference", "authors": ["Zafarali Ahmed", "Arjun Karuvally", "Doina Precup", "Simon Gravel"], "authorids": ["zafarali.ahmed@mail.mcgill.ca", "akaruvally@cs.umass.edu", "dprecup@cs.mcgill.ca", "simon.gravel@mcgill.ca"], "keywords": ["variational inference", "reinforcement learning", "monte carlo methods", "stochastic processes"], "abstract": "The problem of inferring unobserved values in a partially observed trajectory from a stochastic process can be considered as a structured prediction problem. Traditionally inference is conducted using heuristic-based Monte Carlo methods. This work considers learning heuristics by leveraging a connection between policy optimization reinforcement learning and approximate inference. In particular, we learn proposal distributions used in importance samplers by casting it as a variational inference problem. We then rewrite the variational lower bound as a policy optimization problem similar to Weber et al. (2015) allowing us to transfer techniques from reinforcement learning. We apply this technique to a simple stochastic process as a proof-of-concept and show that while it is viable, it will require more engineering effort to scale inference for rare observations", "pdf": "/pdf/760d87e3cc1d14c32a0a9a208025d5c5ba0b0fb9.pdf", "paperhash": "ahmed|learning_proposals_for_sequential_importance_samplers_using_reinforced_variational_inference"}, "signatures": ["ICLR.cc/2019/Workshop/drlStructPred"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/drlStructPred"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/drlStructPred/-/Blind_Submission", "cdate": 1552732853551, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": ".*", "values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2019/Workshop/drlStructPred"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/drlStructPred"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1552732853551, "tmdate": 1554911328507, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/drlStructPred"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/drlStructPred"]}}, "tauthor": "OpenReview.net"}, {"id": "r1lSBzD_Y4", "original": null, "number": 1, "cdate": 1554702909467, "ddate": null, "tcdate": 1554702909467, "tmdate": 1554910464624, "tddate": null, "forum": "HJgxTf89vV", "replyto": "HJgxTf89vV", "invitation": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper5/Decision", "content": {"title": "Acceptance Decision", "decision": "Accept"}, "signatures": ["ICLR.cc/2019/Workshop/drlStructPred/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/drlStructPred/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning proposals for sequential importance samplers using reinforced variational inference", "authors": ["Zafarali Ahmed", "Arjun Karuvally", "Doina Precup", "Simon Gravel"], "authorids": ["zafarali.ahmed@mail.mcgill.ca", "akaruvally@cs.umass.edu", "dprecup@cs.mcgill.ca", "simon.gravel@mcgill.ca"], "keywords": ["variational inference", "reinforcement learning", "monte carlo methods", "stochastic processes"], "abstract": "The problem of inferring unobserved values in a partially observed trajectory from a stochastic process can be considered as a structured prediction problem. Traditionally inference is conducted using heuristic-based Monte Carlo methods. This work considers learning heuristics by leveraging a connection between policy optimization reinforcement learning and approximate inference. In particular, we learn proposal distributions used in importance samplers by casting it as a variational inference problem. We then rewrite the variational lower bound as a policy optimization problem similar to Weber et al. (2015) allowing us to transfer techniques from reinforcement learning. We apply this technique to a simple stochastic process as a proof-of-concept and show that while it is viable, it will require more engineering effort to scale inference for rare observations", "pdf": "/pdf/760d87e3cc1d14c32a0a9a208025d5c5ba0b0fb9.pdf", "paperhash": "ahmed|learning_proposals_for_sequential_importance_samplers_using_reinforced_variational_inference"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper5/Decision", "cdate": 1554496488653, "reply": {"forum": "HJgxTf89vV", "replyto": "HJgxTf89vV", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/drlStructPred/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/drlStructPred/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1554496488653, "tmdate": 1554910461314, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/drlStructPred"], "invitees": ["ICLR.cc/2019/Workshop/drlStructPred/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/drlStructPred"]}}}, {"id": "SygW6LlPtN", "original": null, "number": 4, "cdate": 1554609849095, "ddate": null, "tcdate": 1554609849095, "tmdate": 1554910459591, "tddate": null, "forum": "HJgxTf89vV", "replyto": "HJgxTf89vV", "invitation": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper5/Official_Review", "content": {"title": "interesting proof of concept paper", "review": "In this paper, the authors propose the use of REINFORCE for selecting proposals in importance sampling. The idea is to write  the problem as a variational inference task, and then establish the link between policy optimization and variational lower bound. The paper is relatively well-written, and it seems like it's above the bar of the workshop to me.\n\nA few items:\n\n1. The experiments are a bit weak, in the sense that the dataset and experimental settings descriptions are a bit vague, and it does not seem that the authors have tried real-world datasets such as vision, games, and language problems. \n\n2. It's widely known that MCMC method could be very inefficient when sampling from complex distributions. That's why some people prefer likelihood-free methods such as neural variational inference. It would be great to add some discussions and comparisons. I would appreciate some analysis on runtime since REINFORCE is also bad at sample complexity. \n\n3. I haven't seen much interesting theoretical results from this paper. Maybe you can ask more interesting theoretical questions? Such as the sample complexity? Regret bounds? \n\nAnyway, I think it's a nice paper showing the connections among VI, REINFORCE, and importance sampling, but the theoretical and empirical results could be strengthened in many different ways.", "rating": "4: Top 50% of accepted papers, clear accept", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/drlStructPred/Paper5/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/drlStructPred/Paper5/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning proposals for sequential importance samplers using reinforced variational inference", "authors": ["Zafarali Ahmed", "Arjun Karuvally", "Doina Precup", "Simon Gravel"], "authorids": ["zafarali.ahmed@mail.mcgill.ca", "akaruvally@cs.umass.edu", "dprecup@cs.mcgill.ca", "simon.gravel@mcgill.ca"], "keywords": ["variational inference", "reinforcement learning", "monte carlo methods", "stochastic processes"], "abstract": "The problem of inferring unobserved values in a partially observed trajectory from a stochastic process can be considered as a structured prediction problem. Traditionally inference is conducted using heuristic-based Monte Carlo methods. This work considers learning heuristics by leveraging a connection between policy optimization reinforcement learning and approximate inference. In particular, we learn proposal distributions used in importance samplers by casting it as a variational inference problem. We then rewrite the variational lower bound as a policy optimization problem similar to Weber et al. (2015) allowing us to transfer techniques from reinforcement learning. We apply this technique to a simple stochastic process as a proof-of-concept and show that while it is viable, it will require more engineering effort to scale inference for rare observations", "pdf": "/pdf/760d87e3cc1d14c32a0a9a208025d5c5ba0b0fb9.pdf", "paperhash": "ahmed|learning_proposals_for_sequential_importance_samplers_using_reinforced_variational_inference"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper5/Official_Review", "cdate": 1553778228723, "expdate": 1554526740000, "duedate": 1554526740000, "reply": {"forum": "HJgxTf89vV", "replyto": "HJgxTf89vV", "readers": {"values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/drlStructPred/Paper5/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/drlStructPred/Paper5/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553778228723, "tmdate": 1554911862501, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/drlStructPred"], "invitees": ["ICLR.cc/2019/Workshop/drlStructPred/Paper5/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/drlStructPred"]}}}, {"id": "ByxqW0LIK4", "original": null, "number": 3, "cdate": 1554570754193, "ddate": null, "tcdate": 1554570754193, "tmdate": 1554910458259, "tddate": null, "forum": "HJgxTf89vV", "replyto": "HJgxTf89vV", "invitation": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper5/Official_Review", "content": {"title": "Improvements in inferred unobserved values in partially observed trajectory", "review": "This paper is well written and describes their goals and reasons for their decision making clearly. They clearly state their objectives and use of variational inference in as a reinforcement learning problem to improve trajectories. They provide comparisons to Random Monte Carlo methods as well as ISSoft demonstrating improvements of MC methods and in some instances similar performance to ISSoft. The pros are their clear evidence of being able to use variational inference as a means of matching hand crafted baselines as well as a clear reinforcement learning objective to improve the quality of trajectories. The cons are that their proposed method can learn proposals only for a simple stochastic process and do note that they recieve poor results on rare instances from their X distribution as well as their approach not being able to improve the quality  of pre-trained proposals.", "rating": "3: Marginally above acceptance threshold", "confidence": "1: The reviewer's evaluation is an educated guess"}, "signatures": ["ICLR.cc/2019/Workshop/drlStructPred/Paper5/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/drlStructPred/Paper5/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning proposals for sequential importance samplers using reinforced variational inference", "authors": ["Zafarali Ahmed", "Arjun Karuvally", "Doina Precup", "Simon Gravel"], "authorids": ["zafarali.ahmed@mail.mcgill.ca", "akaruvally@cs.umass.edu", "dprecup@cs.mcgill.ca", "simon.gravel@mcgill.ca"], "keywords": ["variational inference", "reinforcement learning", "monte carlo methods", "stochastic processes"], "abstract": "The problem of inferring unobserved values in a partially observed trajectory from a stochastic process can be considered as a structured prediction problem. Traditionally inference is conducted using heuristic-based Monte Carlo methods. This work considers learning heuristics by leveraging a connection between policy optimization reinforcement learning and approximate inference. In particular, we learn proposal distributions used in importance samplers by casting it as a variational inference problem. We then rewrite the variational lower bound as a policy optimization problem similar to Weber et al. (2015) allowing us to transfer techniques from reinforcement learning. We apply this technique to a simple stochastic process as a proof-of-concept and show that while it is viable, it will require more engineering effort to scale inference for rare observations", "pdf": "/pdf/760d87e3cc1d14c32a0a9a208025d5c5ba0b0fb9.pdf", "paperhash": "ahmed|learning_proposals_for_sequential_importance_samplers_using_reinforced_variational_inference"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper5/Official_Review", "cdate": 1553778228723, "expdate": 1554526740000, "duedate": 1554526740000, "reply": {"forum": "HJgxTf89vV", "replyto": "HJgxTf89vV", "readers": {"values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/drlStructPred/Paper5/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/drlStructPred/Paper5/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553778228723, "tmdate": 1554911862501, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/drlStructPred"], "invitees": ["ICLR.cc/2019/Workshop/drlStructPred/Paper5/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/drlStructPred"]}}}, {"id": "rklpFLlUY4", "original": null, "number": 2, "cdate": 1554544260861, "ddate": null, "tcdate": 1554544260861, "tmdate": 1554910457589, "tddate": null, "forum": "HJgxTf89vV", "replyto": "HJgxTf89vV", "invitation": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper5/Official_Review", "content": {"title": "Empirical performance of reinforced variational inference", "review": "This paper proposes an empirical validation of the reinforced variation inference method proposed by Weber et al. 2015, as a  good candidate for transferring existing policy optimization techniques from variational inference.\nThe results, however, are not consistent, this work reports reasonable results on qualitative experiments, but quantitative experiments suggest that this approach does not perform better than handcrafted proposals on experiments with random walks. Furthermore, experiments with Chi-squared showed that is not a better alternative than KL-divergence.\n\nThis paper provides preliminary results on an interesting topic but with inconclusive remarks, I believe further investigation should be done, in particular regarding the effect of performing RVI on real VI problems and comparing against different VI approaches not only random MC. It would be interesting to understand where does this approach fail, for which cases? It is also not clear why the finetuned approach fails when compared with the simple pretrained model. How different is the proposed approach from Weber 2015? I believe a remark in the related work would better help understand the differences.\nIs RVI leveraging techniques from policy optimization, such as GAE and A3C, it would be interesting to see whether this is helping in the future. \n\nThis paper is written clearly and provides a thorough explanation of results, although some details could be clarified, such as more information on how the model is pretrained and fine tuned?  why would this objective be a wrong objective function? (5.4), and what are the family of proposal distributions  (q_theta) adopted. \n\nI believe this paper provides \n\nMinor revisions:\nequation equation -> equation (2.1)\namd -> and (5.4)\naas -> as (FigureS1)", "rating": "4: Top 50% of accepted papers, clear accept", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/drlStructPred/Paper5/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/drlStructPred/Paper5/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning proposals for sequential importance samplers using reinforced variational inference", "authors": ["Zafarali Ahmed", "Arjun Karuvally", "Doina Precup", "Simon Gravel"], "authorids": ["zafarali.ahmed@mail.mcgill.ca", "akaruvally@cs.umass.edu", "dprecup@cs.mcgill.ca", "simon.gravel@mcgill.ca"], "keywords": ["variational inference", "reinforcement learning", "monte carlo methods", "stochastic processes"], "abstract": "The problem of inferring unobserved values in a partially observed trajectory from a stochastic process can be considered as a structured prediction problem. Traditionally inference is conducted using heuristic-based Monte Carlo methods. This work considers learning heuristics by leveraging a connection between policy optimization reinforcement learning and approximate inference. In particular, we learn proposal distributions used in importance samplers by casting it as a variational inference problem. We then rewrite the variational lower bound as a policy optimization problem similar to Weber et al. (2015) allowing us to transfer techniques from reinforcement learning. We apply this technique to a simple stochastic process as a proof-of-concept and show that while it is viable, it will require more engineering effort to scale inference for rare observations", "pdf": "/pdf/760d87e3cc1d14c32a0a9a208025d5c5ba0b0fb9.pdf", "paperhash": "ahmed|learning_proposals_for_sequential_importance_samplers_using_reinforced_variational_inference"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper5/Official_Review", "cdate": 1553778228723, "expdate": 1554526740000, "duedate": 1554526740000, "reply": {"forum": "HJgxTf89vV", "replyto": "HJgxTf89vV", "readers": {"values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/drlStructPred/Paper5/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/drlStructPred/Paper5/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553778228723, "tmdate": 1554911862501, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/drlStructPred"], "invitees": ["ICLR.cc/2019/Workshop/drlStructPred/Paper5/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/drlStructPred"]}}}, {"id": "SJlZfp6BF4", "original": null, "number": 1, "cdate": 1554533640755, "ddate": null, "tcdate": 1554533640755, "tmdate": 1554910457364, "tddate": null, "forum": "HJgxTf89vV", "replyto": "HJgxTf89vV", "invitation": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper5/Official_Review", "content": {"title": "Preliminary investigation of RVI on toy task", "review": "This work investigates the connection between RL and variational inference on a toy task. Building upon the RVI  (Weber et al., 2015), off-the-shelf RL methods like policy gradient methods can be applied to the variational inference reformulated as RL. The authors attempted to apply this idea to learn a proposal distribution in an importance sampler. Based on RVI, they also proposed alternative objective that replaces the KL divergence with chi-square divergence. An inference problem in simple 1D random walk is used for evaluation. The proposed approach is shown to outperform simple MC baseline, but still worse than a hand-crafted baseline. And the objective using chi-square divergence isn't performing well. \n\nThe paper is relatively well-written and easy to follow. This work is an empirical investigation of RVI on a toy task, and proposes the use of chi-square divergence as an alternative to KL-divergence. \n\nAlthough the experiment is quite preliminary and the novelty is limited, this is an interesting work in progress and I recommend acceptance. I would suggest exploring some real inference tasks in graphical model as additional benchmark to make the experiments more solid. \n\nMinor issues:\nPage 1 paragraph 4 \"Variational inference (VI, Blei et al. (2006; 2017)) is an alternate technique\" --> \"alternative technique\"\nThe equation (6) should have \"x_T\" instead of \"x_t\". \n", "rating": "3: Marginally above acceptance threshold", "confidence": "3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Workshop/drlStructPred/Paper5/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/drlStructPred/Paper5/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning proposals for sequential importance samplers using reinforced variational inference", "authors": ["Zafarali Ahmed", "Arjun Karuvally", "Doina Precup", "Simon Gravel"], "authorids": ["zafarali.ahmed@mail.mcgill.ca", "akaruvally@cs.umass.edu", "dprecup@cs.mcgill.ca", "simon.gravel@mcgill.ca"], "keywords": ["variational inference", "reinforcement learning", "monte carlo methods", "stochastic processes"], "abstract": "The problem of inferring unobserved values in a partially observed trajectory from a stochastic process can be considered as a structured prediction problem. Traditionally inference is conducted using heuristic-based Monte Carlo methods. This work considers learning heuristics by leveraging a connection between policy optimization reinforcement learning and approximate inference. In particular, we learn proposal distributions used in importance samplers by casting it as a variational inference problem. We then rewrite the variational lower bound as a policy optimization problem similar to Weber et al. (2015) allowing us to transfer techniques from reinforcement learning. We apply this technique to a simple stochastic process as a proof-of-concept and show that while it is viable, it will require more engineering effort to scale inference for rare observations", "pdf": "/pdf/760d87e3cc1d14c32a0a9a208025d5c5ba0b0fb9.pdf", "paperhash": "ahmed|learning_proposals_for_sequential_importance_samplers_using_reinforced_variational_inference"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper5/Official_Review", "cdate": 1553778228723, "expdate": 1554526740000, "duedate": 1554526740000, "reply": {"forum": "HJgxTf89vV", "replyto": "HJgxTf89vV", "readers": {"values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/drlStructPred/Paper5/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/drlStructPred/Paper5/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553778228723, "tmdate": 1554911862501, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/drlStructPred"], "invitees": ["ICLR.cc/2019/Workshop/drlStructPred/Paper5/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/drlStructPred"]}}}], "count": 6}