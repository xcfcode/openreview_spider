{"notes": [{"id": "HJej6jR5Fm", "original": "rkxwdqp5tX", "number": 834, "cdate": 1538087875035, "ddate": null, "tcdate": 1538087875035, "tmdate": 1545355430102, "tddate": null, "forum": "HJej6jR5Fm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Meta-Learning to Guide Segmentation", "abstract": "There are myriad kinds of segmentation, and ultimately the `\"right\" segmentation of a given scene is in the eye of the annotator. Standard approaches require large amounts of labeled data to learn just one particular kind of segmentation. As a first step towards relieving this annotation burden, we propose the problem of guided segmentation: given varying amounts of pixel-wise labels, segment unannotated pixels by propagating supervision locally (within an image) and non-locally (across images). We propose guided networks, which extract a latent task representation---guidance---from variable amounts and classes (categories, instances, etc.) of pixel supervision and optimize our architecture end-to-end for fast, accurate, and data-efficient segmentation by meta-learning. To span the few-shot and many-shot learning regimes, we examine guidance from as little as one pixel per concept to as much as 1000+ images, and compare to full gradient optimization at both extremes. To explore generalization, we analyze guidance as a bridge between different levels of supervision to segment classes as the union of instances. Our segmentor concentrates different amounts of supervision of different types of classes into an efficient latent representation, non-locally propagates this supervision across images, and can be updated quickly and cumulatively when given more supervision.", "keywords": ["meta-learning", "few-shot learning", "visual segmentation"], "authorids": ["rakelly@eecs.berkeley.edu", "shelhamer@cs.berkeley.edu", "trevor@eecs.berkeley.edu", "efros@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "authors": ["Kate Rakelly*", "Evan Shelhamer*", "Trevor Darrell", "Alexei A. Efros", "Sergey Levine"], "TL;DR": "We propose a meta-learning approach for guiding visual segmentation tasks from varying amounts of supervision.", "pdf": "/pdf/d766595ec941c688a1a97864210f3d40f86aedc0.pdf", "paperhash": "rakelly|metalearning_to_guide_segmentation", "_bibtex": "@misc{\nrakelly*2019metalearning,\ntitle={Meta-Learning to Guide Segmentation},\nauthor={Kate Rakelly* and Evan Shelhamer* and Trevor Darrell and Alexei A. Efros and Sergey Levine},\nyear={2019},\nurl={https://openreview.net/forum?id=HJej6jR5Fm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "r1e-IYtNeE", "original": null, "number": 1, "cdate": 1545013577490, "ddate": null, "tcdate": 1545013577490, "tmdate": 1545354486182, "tddate": null, "forum": "HJej6jR5Fm", "replyto": "HJej6jR5Fm", "invitation": "ICLR.cc/2019/Conference/-/Paper834/Meta_Review", "content": {"metareview": "Paper proposes a meta-learning approach to interactive segmentation. After the author response, R2 and R3 recommend rejecting this paper citing concerns of limited novelty and insufficient experimental evaluation (given the popularity of this topic in computer vision). R1 does not seem be familiar with the extensive literature on interactive segmentation and their positive recommendation has been discounted. The AC finds no basis for accepting this paper. \n", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "meta-review"}, "signatures": ["ICLR.cc/2019/Conference/Paper834/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper834/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning to Guide Segmentation", "abstract": "There are myriad kinds of segmentation, and ultimately the `\"right\" segmentation of a given scene is in the eye of the annotator. Standard approaches require large amounts of labeled data to learn just one particular kind of segmentation. As a first step towards relieving this annotation burden, we propose the problem of guided segmentation: given varying amounts of pixel-wise labels, segment unannotated pixels by propagating supervision locally (within an image) and non-locally (across images). We propose guided networks, which extract a latent task representation---guidance---from variable amounts and classes (categories, instances, etc.) of pixel supervision and optimize our architecture end-to-end for fast, accurate, and data-efficient segmentation by meta-learning. To span the few-shot and many-shot learning regimes, we examine guidance from as little as one pixel per concept to as much as 1000+ images, and compare to full gradient optimization at both extremes. To explore generalization, we analyze guidance as a bridge between different levels of supervision to segment classes as the union of instances. Our segmentor concentrates different amounts of supervision of different types of classes into an efficient latent representation, non-locally propagates this supervision across images, and can be updated quickly and cumulatively when given more supervision.", "keywords": ["meta-learning", "few-shot learning", "visual segmentation"], "authorids": ["rakelly@eecs.berkeley.edu", "shelhamer@cs.berkeley.edu", "trevor@eecs.berkeley.edu", "efros@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "authors": ["Kate Rakelly*", "Evan Shelhamer*", "Trevor Darrell", "Alexei A. Efros", "Sergey Levine"], "TL;DR": "We propose a meta-learning approach for guiding visual segmentation tasks from varying amounts of supervision.", "pdf": "/pdf/d766595ec941c688a1a97864210f3d40f86aedc0.pdf", "paperhash": "rakelly|metalearning_to_guide_segmentation", "_bibtex": "@misc{\nrakelly*2019metalearning,\ntitle={Meta-Learning to Guide Segmentation},\nauthor={Kate Rakelly* and Evan Shelhamer* and Trevor Darrell and Alexei A. Efros and Sergey Levine},\nyear={2019},\nurl={https://openreview.net/forum?id=HJej6jR5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper834/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353069518, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJej6jR5Fm", "replyto": "HJej6jR5Fm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper834/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper834/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper834/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353069518}}}, {"id": "rJemqdpaRm", "original": null, "number": 6, "cdate": 1543522443042, "ddate": null, "tcdate": 1543522443042, "tmdate": 1543522443042, "tddate": null, "forum": "HJej6jR5Fm", "replyto": "rkx0CjV96Q", "invitation": "ICLR.cc/2019/Conference/-/Paper834/Official_Comment", "content": {"title": "Experimental Setting", "comment": "Thank you for the detailed explanation about the experimental setting and clarification.\nHowever, I'm still not convinced whether proposed model could learn diverse user's intent in case of interactive image segmentation.\nI think there should be significant amount of ambiguity given few sparse guidance for segmentation.\nFor example, in case guidance is given on the chest of a person in a image, does it mean every person in the image should be segmented? only one person should be segmented? t-shirt should be segmented? or some region in a chest with similar pattern should be segmented. \nIn the experimental setting, it is hard to see how the proposed method is dealing with this ambiguity, and whether model's output is biased to class label in case guidance is clearly point some reason of image.\nFor example, guidance is pointing glasses on a face, does the model correctly segment glasses only? or does it segment whole face, or person.\nI think these perspectives cannot be analyses just by comparing overall numbers with simple baselines.\nI would recommend to present more analysis on the characteristics of the model by showing qualitative examples showing that the model correctly handle ambiguity and not bias. \nOne potential way to quantitatively evaluate this aspect would be using segmentation dataset containing classes with different granularity. \nFor example, one can repurpose MSCOCO dataset and include additional classes including subset of classes (e.g vegetable, food, fruit , etc) and designing an setting that ambiguity should be resolved. \nI believe PASCAL VOC is somewhat limited to evaluate model's behaviour on such cases properly.\n\nI still believe this type of detailed analysis of the algorithm is essential to give acceptance to this paper. "}, "signatures": ["ICLR.cc/2019/Conference/Paper834/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper834/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper834/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning to Guide Segmentation", "abstract": "There are myriad kinds of segmentation, and ultimately the `\"right\" segmentation of a given scene is in the eye of the annotator. Standard approaches require large amounts of labeled data to learn just one particular kind of segmentation. As a first step towards relieving this annotation burden, we propose the problem of guided segmentation: given varying amounts of pixel-wise labels, segment unannotated pixels by propagating supervision locally (within an image) and non-locally (across images). We propose guided networks, which extract a latent task representation---guidance---from variable amounts and classes (categories, instances, etc.) of pixel supervision and optimize our architecture end-to-end for fast, accurate, and data-efficient segmentation by meta-learning. To span the few-shot and many-shot learning regimes, we examine guidance from as little as one pixel per concept to as much as 1000+ images, and compare to full gradient optimization at both extremes. To explore generalization, we analyze guidance as a bridge between different levels of supervision to segment classes as the union of instances. Our segmentor concentrates different amounts of supervision of different types of classes into an efficient latent representation, non-locally propagates this supervision across images, and can be updated quickly and cumulatively when given more supervision.", "keywords": ["meta-learning", "few-shot learning", "visual segmentation"], "authorids": ["rakelly@eecs.berkeley.edu", "shelhamer@cs.berkeley.edu", "trevor@eecs.berkeley.edu", "efros@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "authors": ["Kate Rakelly*", "Evan Shelhamer*", "Trevor Darrell", "Alexei A. Efros", "Sergey Levine"], "TL;DR": "We propose a meta-learning approach for guiding visual segmentation tasks from varying amounts of supervision.", "pdf": "/pdf/d766595ec941c688a1a97864210f3d40f86aedc0.pdf", "paperhash": "rakelly|metalearning_to_guide_segmentation", "_bibtex": "@misc{\nrakelly*2019metalearning,\ntitle={Meta-Learning to Guide Segmentation},\nauthor={Kate Rakelly* and Evan Shelhamer* and Trevor Darrell and Alexei A. Efros and Sergey Levine},\nyear={2019},\nurl={https://openreview.net/forum?id=HJej6jR5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper834/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609294, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJej6jR5Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper834/Authors", "ICLR.cc/2019/Conference/Paper834/Reviewers", "ICLR.cc/2019/Conference/Paper834/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper834/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper834/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper834/Authors|ICLR.cc/2019/Conference/Paper834/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper834/Reviewers", "ICLR.cc/2019/Conference/Paper834/Authors", "ICLR.cc/2019/Conference/Paper834/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609294}}}, {"id": "rkx0CjV96Q", "original": null, "number": 4, "cdate": 1542241238378, "ddate": null, "tcdate": 1542241238378, "tmdate": 1542241238378, "tddate": null, "forum": "HJej6jR5Fm", "replyto": "S1lxeZrq37", "invitation": "ICLR.cc/2019/Conference/-/Paper834/Official_Comment", "content": {"title": "contributions, results metric, and interpretation of experiments", "comment": "Thank you for the review and the attention to our architecture and results. Here we detail how our architectural choices lead to key differences from prior work, clarify the metric in our experiments, and discuss the interpretation of our experiments. We would appreciate if the reviewer can comment on how these points affect their views on the novelty, strength of results, and interpretation of our work and reconsider their rating.\n\n> the only differences with the referenced paper (Shaban et al., 2017) is how the support is fused and how multiple guidance could be handled, which can be done by averaging.\n\nOur work differs in architecture, optimization, and scope.\n\nFor architecture, we factorize the approach into (1) extracting the task representation and (2) guiding inference by the representation: this takes the form of our novel late fusion architecture in contrast to the early fusion of Shaban et al. While this difference might appear minor, it has several important consequences. Late fusion allows for parameter sharing between guide and inference branches that makes optimization converge sooner. Given new support annotations, inference by our model updates an order of magnitude faster because only the late stage is recomputed, unlike the full recomputation of the net required by early fusion. For multi-class segmentation, our method only requires a single pass to compute a guide for each class, while Shaban et al. inefficiently require a forward pass per class since their early fusion is only defined for binary tasks.\n\nFor optimization we meta-train on sparse annotations, not dense, and do not require per-branch learning rate tuning (since our parameters are shared). For tasks with sparsely labeled supports, we achieve an an accuracy improvement of ~50% relative over Shaban et al. for only two points per image; see Figure 5 (right).\n\nFor scope, we formulate the more general problem of guided segmentation, and we agree with the reviewer that \"learning a single segmentation algorithm to solve various segmentation problem is an interesting problem that worth exploring.\"  However Shaban et al. restrict their scope to one-shot semantic segmentation from densely labeled support. We hope that a unified meta-learning framework for varied types of segmentation leads to further progress on the accuracy of such methods over those that require more specialization.\n\n>  absolute performance looks bad compared to existing algorithms exploiting prior knowledge for each of the tasks\n> 0.45 IOU in PASCAL VOC dataset, while many existing algorithms could achieve more than 0.8 mean IOU in this dataset\n\nPlease note that we score all methods with positive IU for consistency across tasks (section 5, paragraph 2), which is not equivalent to class-wise mean IU! We will further highlight and explain our choice of metric in the revision to resolve the commented on confusion, thank you.\n\nOur reported 0.45 positive IU oracle for few-shot semantic segmentation corresponds to 0.62 mean IU, which is expected for our FCN architecture based on VGG-16. The referred to methods that score more than 0.8 mean IU on PASCAL VOC require outside segmentation data, deeper architectures, longer optimization schedules, aggressive data augmentation, test-time post-processing, and more. These extensions are orthogonal to our scientific question comparing our general meta-learning method with the specialized methods for each of the tasks in a common experimental framework with the same base architecture.\n\n> I question whether foreground / background baseline is reasonable baseline for all these tasks\n\nThe foreground-background baseline is surprisingly strong for video because DAVIS clips are biased towards containing one salient object per frame. To reduce the severity of this issue, our work evalutes on DAVIS'17 (Section 5.1, Figure 6) which includes some multi-object tasks instead of the simpler DAVIS'16.\n\n> In 4.3, authors argue that the model is trained with S=1, but could operate with different (S, P)\n> increasing S does not necessarily increase the performance.\n\nWhile the semantic-trained guided segmentor struggles to effectively aggregate larger supports, the instance-trained segmentor performs better with increasing S; see Section 5.2 for a discussion. Likewise our guided segmentor for video object segmentation improves with increasing S; see Section 5.1, Figure 6 (right).\n\n> In 5.3, this paper investigated whether the model trained with instances could be used for semantic segmentation.\n> but this might be just because there are many images with single instance in each image\n\nIt\u2019s true that PASCAL includes many images containing a single class. However, the semantic-guided instance-trained segmentor significantly outperforms the foreground-background baseline, which should do just as well on single-class images and single-instance images, so the accuracy of our guided segmentor cannot be entirely explained away by these kinds of images."}, "signatures": ["ICLR.cc/2019/Conference/Paper834/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper834/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper834/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning to Guide Segmentation", "abstract": "There are myriad kinds of segmentation, and ultimately the `\"right\" segmentation of a given scene is in the eye of the annotator. Standard approaches require large amounts of labeled data to learn just one particular kind of segmentation. As a first step towards relieving this annotation burden, we propose the problem of guided segmentation: given varying amounts of pixel-wise labels, segment unannotated pixels by propagating supervision locally (within an image) and non-locally (across images). We propose guided networks, which extract a latent task representation---guidance---from variable amounts and classes (categories, instances, etc.) of pixel supervision and optimize our architecture end-to-end for fast, accurate, and data-efficient segmentation by meta-learning. To span the few-shot and many-shot learning regimes, we examine guidance from as little as one pixel per concept to as much as 1000+ images, and compare to full gradient optimization at both extremes. To explore generalization, we analyze guidance as a bridge between different levels of supervision to segment classes as the union of instances. Our segmentor concentrates different amounts of supervision of different types of classes into an efficient latent representation, non-locally propagates this supervision across images, and can be updated quickly and cumulatively when given more supervision.", "keywords": ["meta-learning", "few-shot learning", "visual segmentation"], "authorids": ["rakelly@eecs.berkeley.edu", "shelhamer@cs.berkeley.edu", "trevor@eecs.berkeley.edu", "efros@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "authors": ["Kate Rakelly*", "Evan Shelhamer*", "Trevor Darrell", "Alexei A. Efros", "Sergey Levine"], "TL;DR": "We propose a meta-learning approach for guiding visual segmentation tasks from varying amounts of supervision.", "pdf": "/pdf/d766595ec941c688a1a97864210f3d40f86aedc0.pdf", "paperhash": "rakelly|metalearning_to_guide_segmentation", "_bibtex": "@misc{\nrakelly*2019metalearning,\ntitle={Meta-Learning to Guide Segmentation},\nauthor={Kate Rakelly* and Evan Shelhamer* and Trevor Darrell and Alexei A. Efros and Sergey Levine},\nyear={2019},\nurl={https://openreview.net/forum?id=HJej6jR5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper834/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609294, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJej6jR5Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper834/Authors", "ICLR.cc/2019/Conference/Paper834/Reviewers", "ICLR.cc/2019/Conference/Paper834/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper834/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper834/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper834/Authors|ICLR.cc/2019/Conference/Paper834/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper834/Reviewers", "ICLR.cc/2019/Conference/Paper834/Authors", "ICLR.cc/2019/Conference/Paper834/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609294}}}, {"id": "HJlEPLVqpQ", "original": null, "number": 3, "cdate": 1542239836046, "ddate": null, "tcdate": 1542239836046, "tmdate": 1542239836046, "tddate": null, "forum": "HJej6jR5Fm", "replyto": "H1g_Df45aX", "invitation": "ICLR.cc/2019/Conference/-/Paper834/Official_Comment", "content": {"title": "meta-learning setting, method novelty, and comparisons (2/2)", "comment": "> it is not clear what the problem setting of this paper is, as it seems to have two sets of training data of fully-annotated images (for training) and the combined set of point-wise annotated images and unannotated images (guidance images)\n\nOur problem setting is meta-learning for segmentation. Meta-learning seeks to learn a learning algorithm that can learn a new task, often from little supervision. In our case, a task consists of a support set of (sparsely) labeled images and a query set of unlabeled images to be segmented. In the standard terminology of few-shot learning, the \"point-wise annotated images\" are the labeled supports and the \"unannotated images\" are the queries to be segmented according to the labeled support. \n\nWe divide the set of tasks into sets for meta-training and meta-testing. We optimize the parameters of our model to perform learning on tasks drawn from the meta-training set, and evaluate on tasks drawn from meta-test. For our guided nets, learning a task corresponds to inference in the model, which we call guidance: extracting the task representation from the supports and guided inference to segment the queries. Meta-training optimizes the model parameters to improve guidance, and once meta-training is complete the model parameters are fixed and only the task representation changes as a function of the support. For meta-testing we evaluate on heldout instances, classes, or videos in our interactive, semantic, and video object segmentation results respectively. \n\n> It is not clear whether authors generate the second dataset out of the first one, or they have separate datasets for these two.\n\nThis kind of dataset division is a common approach to few-shot learning for image classification (e.g., Omniglot from Lake et al. 2015, miniImageNet from Vinyals et al. 2016) that we adapt to pixel-wise tasks.\n\nWe generate our sparse meta-learning datasets from the standard, fully-annotated segmentation datasets by sampling different tasks (e.g., segment a particular bear in all the frames of the video) and subsampling the annotations. A task consists of a support set of (sparsely) labeled images and a query set of unlabeled images to be segmented. Tasks are synthesized from a densely labeled dataset such as PASCAL by binarizing and sparsifying dense masks, as illustrated in Section 4.3 Figure 4. During training, the query set is given as input to the model without labels, and the dense ground truth labels for the query set used to define the loss. We are revising section 4.3 to clearly explain this process.\n\n> it is not clear how the authors incorporate the unannotated images for training (guidance images)\n\nOur method is trained by meta-learning through episodic optimization: during meta-training, the unnannotated images are given as queries to be segmented by the model, the model infers an output segmentation, and these are compared against the true segmentation of the queries (known only during meta-training). Please see figure 4 and section 4.3. Are queries what was meant by guidance images?\n\n> what is the dataset used for the evaluation of the first paragraph in section 5.1? How do you split the Pascal VOC data to exclusive sets?\n\nThe dataset used in the first paragraph of Section 5.1 is PASCAL VOC/SBD, as used in Xu et al., which we compare against (we are correcting this omission in a revision of the text\u2014thank you for noticing it). For few-shot semantic segmentation, we follow the experimental protocol of Shaban et al., as stated in the second to last paragraph of Section 5.1, which tests few-shot performance on held-out classes by dividing the 20 classes of PASCAL into 4 sets of 5, then reports the average performance across these sets for the 5 held-out classes after training on the remaining 15. Images that contain both held-out and training classes are placed in the held-out set.\n\n> How do you sample point-wise annotation from dense mask labels? How does the sampling procedure affect the performance? \n\nThe dense ground truth labels are sparsified via uniform random sampling. We found random sampling to perform about equal to more complex sampling strategies explored in previous work, such as Xu et al. 2016. We are adding these details to the paper appendix.\n\n> The performance of the guided semantic segmentation is also quite low\n\nThe performance of our method is in some cases lower than the performance of task-specific methods (video object segmentation and 5-shot semantic segmentation). However, a main contribution of our work is to present a first general meta-learning framework for structured output tasks. A compensating advantage of our proposed late fusion architecture is that it is quicker to update than Shaban et al. and Caelles et al., making it more practical for interactive use.\n\n> Please revise the notations in equations.\n\nThank you for noticing these typsetting errors! We are correcting them in a revision of the text."}, "signatures": ["ICLR.cc/2019/Conference/Paper834/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper834/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper834/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning to Guide Segmentation", "abstract": "There are myriad kinds of segmentation, and ultimately the `\"right\" segmentation of a given scene is in the eye of the annotator. Standard approaches require large amounts of labeled data to learn just one particular kind of segmentation. As a first step towards relieving this annotation burden, we propose the problem of guided segmentation: given varying amounts of pixel-wise labels, segment unannotated pixels by propagating supervision locally (within an image) and non-locally (across images). We propose guided networks, which extract a latent task representation---guidance---from variable amounts and classes (categories, instances, etc.) of pixel supervision and optimize our architecture end-to-end for fast, accurate, and data-efficient segmentation by meta-learning. To span the few-shot and many-shot learning regimes, we examine guidance from as little as one pixel per concept to as much as 1000+ images, and compare to full gradient optimization at both extremes. To explore generalization, we analyze guidance as a bridge between different levels of supervision to segment classes as the union of instances. Our segmentor concentrates different amounts of supervision of different types of classes into an efficient latent representation, non-locally propagates this supervision across images, and can be updated quickly and cumulatively when given more supervision.", "keywords": ["meta-learning", "few-shot learning", "visual segmentation"], "authorids": ["rakelly@eecs.berkeley.edu", "shelhamer@cs.berkeley.edu", "trevor@eecs.berkeley.edu", "efros@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "authors": ["Kate Rakelly*", "Evan Shelhamer*", "Trevor Darrell", "Alexei A. Efros", "Sergey Levine"], "TL;DR": "We propose a meta-learning approach for guiding visual segmentation tasks from varying amounts of supervision.", "pdf": "/pdf/d766595ec941c688a1a97864210f3d40f86aedc0.pdf", "paperhash": "rakelly|metalearning_to_guide_segmentation", "_bibtex": "@misc{\nrakelly*2019metalearning,\ntitle={Meta-Learning to Guide Segmentation},\nauthor={Kate Rakelly* and Evan Shelhamer* and Trevor Darrell and Alexei A. Efros and Sergey Levine},\nyear={2019},\nurl={https://openreview.net/forum?id=HJej6jR5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper834/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609294, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJej6jR5Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper834/Authors", "ICLR.cc/2019/Conference/Paper834/Reviewers", "ICLR.cc/2019/Conference/Paper834/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper834/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper834/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper834/Authors|ICLR.cc/2019/Conference/Paper834/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper834/Reviewers", "ICLR.cc/2019/Conference/Paper834/Authors", "ICLR.cc/2019/Conference/Paper834/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609294}}}, {"id": "H1g_Df45aX", "original": null, "number": 2, "cdate": 1542238815684, "ddate": null, "tcdate": 1542238815684, "tmdate": 1542238815684, "tddate": null, "forum": "HJej6jR5Fm", "replyto": "HkxrUnYc2Q", "invitation": "ICLR.cc/2019/Conference/-/Paper834/Official_Comment", "content": {"title": "meta-learning setting, method novelty, and comparisons (1/2)", "comment": "Thank you for your review, especially the comments regarding the clarity of the method description and experimental setting, which are helping us to revise the text to depend less on familiarity with meta-learning approaches and few-shot learning setups. In the meantime we offer clarifications here, and in particular address the problem setting, architecture and optimization novelty, and experimental comparisons. We will make a follow-up post once the revision is uploaded. Please let us know if the method and experiments are now clear, and how these details impact your evaluation of the submission's originality, significance, and experiments.\n\n> This paper proposed a few-shot learning approach for interactive segmentation\n\nWe would like to clarify that our work is an extension of interactive segmentation. Our meta-learning learning approach, guided segmentation, generalizes the usual problem statement of interactive segmentation. Given an image with partial annotations, an interactive segmentor fully segments that image, but it cannot segment a new image without any annotations. That is, for an interactive segmentor, annotations on one image do not inform the segmentation of another image. On the other hand, our guided segmentor extracts a latent representation of the pixel-wise annotations and conditions on it to inform the segmentation of all images, and additional annotations on any image affect the segmentation of all of them.\n\n> I do not see many novel contributions in terms of both network architecture and learning perspective.\n\n\nPrior work is limited to binary segmentation of a single image (interactive segmentation by Xu et al. 2016), two-class tasks supervised by dense annotations from a single image (one-shot semantic segmentation by Shaban et al. 2017), and slow optimization that fails for sparse annotations (video object segmentation through fine-tuning by Caelles et al. 2017). Our novel choices for architecture and optimization are key to addressing these issues:\n\n- Our novel late-fusion architecture (Section 4.1 and Figure 3) is necessary for efficient representation and segmentation from annotations that are multi-shot (multi-image, multi-pixel) and multi-way (multi-class). Xu et al. and Shaban et al., with their early fusion architectures, are limited to one image and two classes at a time. When annotations change, they must re-compute the entire network as the annotations are fused early at the input, while we update in constant time w.r.t. the full network time since only the late stage is re-computed. For multi-class segmentation, our model simply and efficiently fuses shared image features with the annotations for each class (end of Section 4.1), while Xu et al. and Shaban et al. inefficiently have to do a forward pass for each class.\n- With optimization by meta-learning, our model learns to handle sparse annotations that the Caelles et al. approach of optimization by fine-tuning fails on. While Shaban et al. likewise optimize by meta-learning, they require dense annotations, and we show more than 50% relative improvement for accuracy in the sparse regime.\n- Our novel contributions to meta-learning optimization (Section 4.3) are (1) sampling tasks with different shot (number of labels) and way (number of classes) per episode of optimization for better generalization to different amounts of supervision and (2) investigating transfer learning when meta-learning one kind of task, instances, then meta-testing on a different kind of task, semantics.\n\nFor novelty in experiments, our work is the first to show results on this set of tasks with a unified model.\n\n> The method is compared against only a few (not popular) interactive segmentation methods, although there exist many recent works addressing the same task (e.g. Xu et al. 2016)\n\nFor comparison we chose popular, state-of-the-art at publication methods: DIOS (Xu et al. 2016) for interactive segmentation and OSVOS (Caelles et al.) for video object segmentation. To the best of our knowledge Shaban et al. 2017 is the first and only few-shot semantic segmentor prior to our work. Furthermore, these methods were chosen for fair comparison since their architectures and ours are all derived from a VGG-16 backbone and are free from confounding differences in post-processing, data augmentation, and so forth. Our work shows results on few-shot semantic segmentation, video object segmentation, and interactive instance segmentation (as mentioned above, guided segmentation is not simply interactive segmentation, as evidenced by this set of tasks).\n\nWe ask that the reviewer please be specific about alternative comparisons.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper834/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper834/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper834/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning to Guide Segmentation", "abstract": "There are myriad kinds of segmentation, and ultimately the `\"right\" segmentation of a given scene is in the eye of the annotator. Standard approaches require large amounts of labeled data to learn just one particular kind of segmentation. As a first step towards relieving this annotation burden, we propose the problem of guided segmentation: given varying amounts of pixel-wise labels, segment unannotated pixels by propagating supervision locally (within an image) and non-locally (across images). We propose guided networks, which extract a latent task representation---guidance---from variable amounts and classes (categories, instances, etc.) of pixel supervision and optimize our architecture end-to-end for fast, accurate, and data-efficient segmentation by meta-learning. To span the few-shot and many-shot learning regimes, we examine guidance from as little as one pixel per concept to as much as 1000+ images, and compare to full gradient optimization at both extremes. To explore generalization, we analyze guidance as a bridge between different levels of supervision to segment classes as the union of instances. Our segmentor concentrates different amounts of supervision of different types of classes into an efficient latent representation, non-locally propagates this supervision across images, and can be updated quickly and cumulatively when given more supervision.", "keywords": ["meta-learning", "few-shot learning", "visual segmentation"], "authorids": ["rakelly@eecs.berkeley.edu", "shelhamer@cs.berkeley.edu", "trevor@eecs.berkeley.edu", "efros@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "authors": ["Kate Rakelly*", "Evan Shelhamer*", "Trevor Darrell", "Alexei A. Efros", "Sergey Levine"], "TL;DR": "We propose a meta-learning approach for guiding visual segmentation tasks from varying amounts of supervision.", "pdf": "/pdf/d766595ec941c688a1a97864210f3d40f86aedc0.pdf", "paperhash": "rakelly|metalearning_to_guide_segmentation", "_bibtex": "@misc{\nrakelly*2019metalearning,\ntitle={Meta-Learning to Guide Segmentation},\nauthor={Kate Rakelly* and Evan Shelhamer* and Trevor Darrell and Alexei A. Efros and Sergey Levine},\nyear={2019},\nurl={https://openreview.net/forum?id=HJej6jR5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper834/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609294, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJej6jR5Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper834/Authors", "ICLR.cc/2019/Conference/Paper834/Reviewers", "ICLR.cc/2019/Conference/Paper834/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper834/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper834/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper834/Authors|ICLR.cc/2019/Conference/Paper834/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper834/Reviewers", "ICLR.cc/2019/Conference/Paper834/Authors", "ICLR.cc/2019/Conference/Paper834/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609294}}}, {"id": "SygL0b496Q", "original": null, "number": 1, "cdate": 1542238669741, "ddate": null, "tcdate": 1542238669741, "tmdate": 1542238669741, "tddate": null, "forum": "HJej6jR5Fm", "replyto": "HkxNQtn937", "invitation": "ICLR.cc/2019/Conference/-/Paper834/Official_Comment", "content": {"title": "related work, interactive segmentation, and our latent task representation z", "comment": "Thank you for the review and your enthusiasm for applying few-shot learning to richer visual tasks like segmentation! We provide a few clarifications and address the questions listed in your review. Given our response here, we would appreciate it if you could comment further regarding\n\n- novelty with respect to the one existing few-shot segmentation method we cite\n- clarity of our figure summarizing interactive segmentation and the other segmentation tasks we address (Figure 2)\n\nWe agree that few-shot learning need not be limited to image classification and should address higher-level tasks such as different types of segmentation as we show in this work. We hope that our work inspires more progress on few-shot learning for structured output tasks for which labels are even more costly and scarce than image-level supervision.\n\nOur work is not the first to consider few-shot learning for structured output, but we do significantly generalize the problem scope and extend the approach. Shaban et al. (2017) consider one-shot semantic segmentation. We consider a wider range of tasks (instance, semantic, and video object segmentation), experiment with varying shot and way (from one-shot to 1000+ shot and 2-20 way) beyond the prior 1-5 shot and fixed 2-way of Shaban et al., and propose a novel late fusion architecture (that is faster to update during inference).\n\n> what is interactive segmentation?\n\nInteractive segmentation is the task of inferring dense segmentation masks from sparse pixel-wise labels within the same image (see middle panel of Figure 2 and our references Kass et al. 1998, Boykov and Jolly 2001, and Xu et al. 2016). Guided segmentation is our extension to interactive segmentation that can propagate pixel labels across images and not just within images. Guided segmentation is necessary to (1) cumulatively incorporate labels across inputs to keep improving the segmentation and (2) increase data efficiency by not requiring annotations on every input.\n\n> is there any constraint on z? Like Gaussian distributions like what z is like in VAE models\n\nz is the latent task encoding extracted by the guide branch g (see Figure 1 and Sections 4 & 4.1). We do not enforce a distribution over z, although this is a possible extension of our work for regularization or sampling diverse segmentations. We are revising the text to make it clear that there is no constraint on the value of z.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper834/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper834/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper834/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning to Guide Segmentation", "abstract": "There are myriad kinds of segmentation, and ultimately the `\"right\" segmentation of a given scene is in the eye of the annotator. Standard approaches require large amounts of labeled data to learn just one particular kind of segmentation. As a first step towards relieving this annotation burden, we propose the problem of guided segmentation: given varying amounts of pixel-wise labels, segment unannotated pixels by propagating supervision locally (within an image) and non-locally (across images). We propose guided networks, which extract a latent task representation---guidance---from variable amounts and classes (categories, instances, etc.) of pixel supervision and optimize our architecture end-to-end for fast, accurate, and data-efficient segmentation by meta-learning. To span the few-shot and many-shot learning regimes, we examine guidance from as little as one pixel per concept to as much as 1000+ images, and compare to full gradient optimization at both extremes. To explore generalization, we analyze guidance as a bridge between different levels of supervision to segment classes as the union of instances. Our segmentor concentrates different amounts of supervision of different types of classes into an efficient latent representation, non-locally propagates this supervision across images, and can be updated quickly and cumulatively when given more supervision.", "keywords": ["meta-learning", "few-shot learning", "visual segmentation"], "authorids": ["rakelly@eecs.berkeley.edu", "shelhamer@cs.berkeley.edu", "trevor@eecs.berkeley.edu", "efros@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "authors": ["Kate Rakelly*", "Evan Shelhamer*", "Trevor Darrell", "Alexei A. Efros", "Sergey Levine"], "TL;DR": "We propose a meta-learning approach for guiding visual segmentation tasks from varying amounts of supervision.", "pdf": "/pdf/d766595ec941c688a1a97864210f3d40f86aedc0.pdf", "paperhash": "rakelly|metalearning_to_guide_segmentation", "_bibtex": "@misc{\nrakelly*2019metalearning,\ntitle={Meta-Learning to Guide Segmentation},\nauthor={Kate Rakelly* and Evan Shelhamer* and Trevor Darrell and Alexei A. Efros and Sergey Levine},\nyear={2019},\nurl={https://openreview.net/forum?id=HJej6jR5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper834/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609294, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJej6jR5Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper834/Authors", "ICLR.cc/2019/Conference/Paper834/Reviewers", "ICLR.cc/2019/Conference/Paper834/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper834/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper834/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper834/Authors|ICLR.cc/2019/Conference/Paper834/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper834/Reviewers", "ICLR.cc/2019/Conference/Paper834/Authors", "ICLR.cc/2019/Conference/Paper834/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609294}}}, {"id": "HkxNQtn937", "original": null, "number": 3, "cdate": 1541224732433, "ddate": null, "tcdate": 1541224732433, "tmdate": 1541533652037, "tddate": null, "forum": "HJej6jR5Fm", "replyto": "HJej6jR5Fm", "invitation": "ICLR.cc/2019/Conference/-/Paper834/Official_Review", "content": {"title": "review", "review": "To my knowledge, this paper is probably the first one to apply few-shot learning concept into high-level computer vision tasks. In this paper's sense, segmentation. It proposes a general framework to few from the very few sample, extract a latent representation z, and apply it to do segmentation on a query. Cases of semantic, interactive and video segmentation are applied. Experiments are very thorough.\n\nWe see too many variants of few-shot learning papers on mini-imagenet or omniglot. For the reason of applying to high-level segmentation, the paper already deserves an acceptance for the first work. I believe this work would inspire many follow-ups in related domain (especially for high-level vision tasks)\n\nComments:\n\n- what is interactive segmentation? I looked through the related work, it just mentioned some previous work without defining or describing it.\n\n- z is the network output of g? is there any constraint on z? Like Gaussian distributions like what z is like in VAE models. \n\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper834/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning to Guide Segmentation", "abstract": "There are myriad kinds of segmentation, and ultimately the `\"right\" segmentation of a given scene is in the eye of the annotator. Standard approaches require large amounts of labeled data to learn just one particular kind of segmentation. As a first step towards relieving this annotation burden, we propose the problem of guided segmentation: given varying amounts of pixel-wise labels, segment unannotated pixels by propagating supervision locally (within an image) and non-locally (across images). We propose guided networks, which extract a latent task representation---guidance---from variable amounts and classes (categories, instances, etc.) of pixel supervision and optimize our architecture end-to-end for fast, accurate, and data-efficient segmentation by meta-learning. To span the few-shot and many-shot learning regimes, we examine guidance from as little as one pixel per concept to as much as 1000+ images, and compare to full gradient optimization at both extremes. To explore generalization, we analyze guidance as a bridge between different levels of supervision to segment classes as the union of instances. Our segmentor concentrates different amounts of supervision of different types of classes into an efficient latent representation, non-locally propagates this supervision across images, and can be updated quickly and cumulatively when given more supervision.", "keywords": ["meta-learning", "few-shot learning", "visual segmentation"], "authorids": ["rakelly@eecs.berkeley.edu", "shelhamer@cs.berkeley.edu", "trevor@eecs.berkeley.edu", "efros@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "authors": ["Kate Rakelly*", "Evan Shelhamer*", "Trevor Darrell", "Alexei A. Efros", "Sergey Levine"], "TL;DR": "We propose a meta-learning approach for guiding visual segmentation tasks from varying amounts of supervision.", "pdf": "/pdf/d766595ec941c688a1a97864210f3d40f86aedc0.pdf", "paperhash": "rakelly|metalearning_to_guide_segmentation", "_bibtex": "@misc{\nrakelly*2019metalearning,\ntitle={Meta-Learning to Guide Segmentation},\nauthor={Kate Rakelly* and Evan Shelhamer* and Trevor Darrell and Alexei A. Efros and Sergey Levine},\nyear={2019},\nurl={https://openreview.net/forum?id=HJej6jR5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper834/Official_Review", "cdate": 1542234366240, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJej6jR5Fm", "replyto": "HJej6jR5Fm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper834/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335813084, "tmdate": 1552335813084, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper834/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HkxrUnYc2Q", "original": null, "number": 2, "cdate": 1541213260765, "ddate": null, "tcdate": 1541213260765, "tmdate": 1541533651821, "tddate": null, "forum": "HJej6jR5Fm", "replyto": "HJej6jR5Fm", "invitation": "ICLR.cc/2019/Conference/-/Paper834/Official_Review", "content": {"title": "Unclear presentations, limited novelty.", "review": "Summary:\nThis paper proposed a few-shot learning approach for interactive segmentation. Given a set of user-annotated points, the proposed model learns to generate dense segmentation masks of objects. To incorporate the point-wise annotation, the guidance network is introduced. The proposed idea is applied to guided image segmentation, semantic segmentation, and video segmentation.\n\nClarity:\nOverall, the presentation of the paper can be significantly improved. First of all, it is not clear what the problem setting of this paper is, as it seems to have two sets of training data of fully-annotated images (for training) and the combined set of point-wise annotated images and unannotated images (guidance images T in the first equation); It is not clear whether authors generate the second dataset out of the first one, or they have separate datasets for these two. Also, it is not clear how the authors incorporate the unannotated images for training. \n\nThe descriptions on model architecture are also not quite clear, as it involves two components (g and f) but start discussing with g without providing a clear overview of the combined model (I would suggest changing the order of Section 4.1 and Section 4.2 to make it clearer). The loss functions are introduced in the last part of the method, which makes it also very difficult to understand. \n\nOriginality and significance:\nThe technical contribution of the paper is very limited. I do not see many novel contributions in terms of both network architecture and learning perspective.\n\nExperiment:\nOverall, I am not quite convinced with the experiment results. The method is compared against only a few (not popular) interactive segmentation methods, although there exist many recent works addressing the same task (e.g. Xu et al. 2016). \n\nThe experiment settings are also not clearly presented. For instance, what is the dataset used for the evaluation of the first paragraph in section 5.1? How do you split the Pascal VOC data to exclusive sets? How do you sample point-wise annotation from dense mask labels? How does the sampling procedure affect the performance? \n\nThe performance of the guided semantic segmentation is also quite low, limiting the practical usefulness of the method. Finally, the paper does not present qualitative results, which are essential to understanding the performance of the segmentation system. \n\nMinor comments:\n1. There are a lot of grammar issues. Please revise your draft.\n2. Please revise the notations in equations. For instance, \n    T = {{(x_1, L_1),...} \\cup {\\bar{x}_1,...}\n    L_s = {(p_j,l_j):j\\in{1,...,P}, l\\in{1,...,K}\\cup{\\emptyset}}\n    Also, in the next equation, j\\in\\bar{x}_q} -> p_ j\\in\\bar{x}_q} (j is an index of pixel)\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper834/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning to Guide Segmentation", "abstract": "There are myriad kinds of segmentation, and ultimately the `\"right\" segmentation of a given scene is in the eye of the annotator. Standard approaches require large amounts of labeled data to learn just one particular kind of segmentation. As a first step towards relieving this annotation burden, we propose the problem of guided segmentation: given varying amounts of pixel-wise labels, segment unannotated pixels by propagating supervision locally (within an image) and non-locally (across images). We propose guided networks, which extract a latent task representation---guidance---from variable amounts and classes (categories, instances, etc.) of pixel supervision and optimize our architecture end-to-end for fast, accurate, and data-efficient segmentation by meta-learning. To span the few-shot and many-shot learning regimes, we examine guidance from as little as one pixel per concept to as much as 1000+ images, and compare to full gradient optimization at both extremes. To explore generalization, we analyze guidance as a bridge between different levels of supervision to segment classes as the union of instances. Our segmentor concentrates different amounts of supervision of different types of classes into an efficient latent representation, non-locally propagates this supervision across images, and can be updated quickly and cumulatively when given more supervision.", "keywords": ["meta-learning", "few-shot learning", "visual segmentation"], "authorids": ["rakelly@eecs.berkeley.edu", "shelhamer@cs.berkeley.edu", "trevor@eecs.berkeley.edu", "efros@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "authors": ["Kate Rakelly*", "Evan Shelhamer*", "Trevor Darrell", "Alexei A. Efros", "Sergey Levine"], "TL;DR": "We propose a meta-learning approach for guiding visual segmentation tasks from varying amounts of supervision.", "pdf": "/pdf/d766595ec941c688a1a97864210f3d40f86aedc0.pdf", "paperhash": "rakelly|metalearning_to_guide_segmentation", "_bibtex": "@misc{\nrakelly*2019metalearning,\ntitle={Meta-Learning to Guide Segmentation},\nauthor={Kate Rakelly* and Evan Shelhamer* and Trevor Darrell and Alexei A. Efros and Sergey Levine},\nyear={2019},\nurl={https://openreview.net/forum?id=HJej6jR5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper834/Official_Review", "cdate": 1542234366240, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJej6jR5Fm", "replyto": "HJej6jR5Fm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper834/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335813084, "tmdate": 1552335813084, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper834/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1lxeZrq37", "original": null, "number": 1, "cdate": 1541193960241, "ddate": null, "tcdate": 1541193960241, "tmdate": 1541533651614, "tddate": null, "forum": "HJej6jR5Fm", "replyto": "HJej6jR5Fm", "invitation": "ICLR.cc/2019/Conference/-/Paper834/Official_Review", "content": {"title": "Incremental idea and weak analysis", "review": "Summary\nThis paper proposes to formulate diverse segmentation problems as a guided segmentation, whose task is defined by the guiding annotations.\nThe main idea of this paper is using meta-learning to train a single neural network performing guidance segmentation.\nSpecifically, they encode S annotated support image into a task representation and use it to perform binary segmentation.\nBy performing episodic optimisation, the model's guidance to segmentation output is defined by the task distribution.\n\nStrength\nLearning a single segmentation algorithm to solve various segmentation problem is an interesting problem that worth exploring.\nThis paper tackles this problem and showed results on various segmentation problems.\n\nWeakness\nThe proposed method, including the architecture and training strategy, is relatively simple and very closely related to existing approach. Especially, the only differences with the referenced paper (Shaban et al., 2017) is how the support is fused and how multiple guidance could be handled, which can be done by averaging. These differences are relatively minor, so I question the novelty of this paper.\n\nThis paper performs experiments on diverse tasks but the method is compared with relatively weak baselines absolute performance looks bad compared to existing algorithms exploiting prior knowledge for each of the tasks.\nFor example, the oracle performance in semantic segmentation (fully supervised method) is 0.45 IOU in PASCAL VOC dataset, while many existing algorithms could achieve more than 0.8 mean IOU in this dataset. \nIn addition, I question whether foreground / background baseline is reasonable baseline for all these tasks, because a little domain knowledge might already give very strong result on various segmentation tasks.\nFor example, in terms of video segmentation, one trivial baseline might include propagating ground truth labels in the first frame with color and spatial location similarity, which might be already stronger than the foreground / background baseline.\n\nThere are some strong arguments that require further justification. \n- In 4.3, authors argue that the model is trained with S=1, but could operate with different (S, P).\nHowever, it's suspicious whether this would be really true, because it requires generalisation to out-of-distribution examples, which is very difficult machine learning problem. The performance in Figure 5 (right) might support the difficulty of this generalisation, because increasing S does not necessarily increase the performance.\n- In 5.3, this paper investigated whether the model trained with instances could be used for semantic segmentation. I think performing semantic segmentation with model trained for instance segmentation in the same dataset might show reasonable performance, but this might be just because there are many images with single instance in each image and because instance annotations in this dataset are based on semantic classes. So the argument that training with instance segmentation lead to semantic segmentation should be more carefully made.\n\nOverall comment\nI believe the method proposed in this paper is rather incremental and analysis is not supporting the main arguments of this paper and strength of the proposed method. \nEspecially, simple performance comparison with weak baselines give no clues about the property of the method and advantage of using this method compared to other existing approaches.\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper834/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning to Guide Segmentation", "abstract": "There are myriad kinds of segmentation, and ultimately the `\"right\" segmentation of a given scene is in the eye of the annotator. Standard approaches require large amounts of labeled data to learn just one particular kind of segmentation. As a first step towards relieving this annotation burden, we propose the problem of guided segmentation: given varying amounts of pixel-wise labels, segment unannotated pixels by propagating supervision locally (within an image) and non-locally (across images). We propose guided networks, which extract a latent task representation---guidance---from variable amounts and classes (categories, instances, etc.) of pixel supervision and optimize our architecture end-to-end for fast, accurate, and data-efficient segmentation by meta-learning. To span the few-shot and many-shot learning regimes, we examine guidance from as little as one pixel per concept to as much as 1000+ images, and compare to full gradient optimization at both extremes. To explore generalization, we analyze guidance as a bridge between different levels of supervision to segment classes as the union of instances. Our segmentor concentrates different amounts of supervision of different types of classes into an efficient latent representation, non-locally propagates this supervision across images, and can be updated quickly and cumulatively when given more supervision.", "keywords": ["meta-learning", "few-shot learning", "visual segmentation"], "authorids": ["rakelly@eecs.berkeley.edu", "shelhamer@cs.berkeley.edu", "trevor@eecs.berkeley.edu", "efros@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "authors": ["Kate Rakelly*", "Evan Shelhamer*", "Trevor Darrell", "Alexei A. Efros", "Sergey Levine"], "TL;DR": "We propose a meta-learning approach for guiding visual segmentation tasks from varying amounts of supervision.", "pdf": "/pdf/d766595ec941c688a1a97864210f3d40f86aedc0.pdf", "paperhash": "rakelly|metalearning_to_guide_segmentation", "_bibtex": "@misc{\nrakelly*2019metalearning,\ntitle={Meta-Learning to Guide Segmentation},\nauthor={Kate Rakelly* and Evan Shelhamer* and Trevor Darrell and Alexei A. Efros and Sergey Levine},\nyear={2019},\nurl={https://openreview.net/forum?id=HJej6jR5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper834/Official_Review", "cdate": 1542234366240, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJej6jR5Fm", "replyto": "HJej6jR5Fm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper834/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335813084, "tmdate": 1552335813084, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper834/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 10}