{"notes": [{"id": "rJl3S2A9t7", "original": "Skg-0h5tt7", "number": 1577, "cdate": 1538088003615, "ddate": null, "tcdate": 1538088003615, "tmdate": 1545355402311, "tddate": null, "forum": "rJl3S2A9t7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Policy Optimization via Stochastic Recursive Gradient Algorithm", "abstract": "In this paper, we propose the StochAstic Recursive grAdient Policy Optimization (SARAPO) algorithm which is a novel variance reduction method on Trust Region Policy Optimization (TRPO). The algorithm incorporates the StochAstic Recursive grAdient algoritHm(SARAH) into the TRPO framework. Compared with the existing Stochastic Variance Reduced Policy Optimization (SVRPO), our algorithm is more stable in the variance. Furthermore, by theoretical analysis the ordinary differential equation and the stochastic differential equation (ODE/SDE) of SARAH, we analyze its convergence property and stability. Our experiments demonstrate its performance on a variety of benchmark tasks. We show that our algorithm gets better improvement in each iteration and matches or even outperforms SVRPO and TRPO.\n", "keywords": ["reinforcement learning", "policy gradient", "variance reduction", "stochastic recursive gradient algorithm"], "authorids": ["yuanhz@pku.edu.cn", "junchi.li.duke@gmail.com", "yuhaotang97@gmail.com", "yuren.zhou@duke.edu"], "authors": ["Huizhuo Yuan", "Chris Junchi Li", "Yuhao Tang", "Yuren Zhou"], "TL;DR": "This paper proposes the StochAstic Recursive Gradient Policy Optimization (SARAPO) algorithm based on the novel SARAH method, and exemplifies its advantages over existing policy gradient methods from both theory and experiments.", "pdf": "/pdf/7d40e2c6a08eaad3b498c61952a6fbfabc34581e.pdf", "paperhash": "yuan|policy_optimization_via_stochastic_recursive_gradient_algorithm", "_bibtex": "@misc{\nyuan2019policy,\ntitle={Policy Optimization via Stochastic Recursive Gradient Algorithm},\nauthor={Huizhuo Yuan and Chris Junchi Li and Yuhao Tang and Yuren Zhou},\nyear={2019},\nurl={https://openreview.net/forum?id=rJl3S2A9t7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "Hyett17leV", "original": null, "number": 1, "cdate": 1544724353300, "ddate": null, "tcdate": 1544724353300, "tmdate": 1545354510591, "tddate": null, "forum": "rJl3S2A9t7", "replyto": "rJl3S2A9t7", "invitation": "ICLR.cc/2019/Conference/-/Paper1577/Meta_Review", "content": {"metareview": "The use of SARAH for Policy optimization in RL is novel, with some theoretical analysis to demonstrate convergence of this approach. However, concerns were raised in terms of clarity of the paper, empirical results and in placement of this theory relative to a previous variance reduction algorithm called SVRPG. The author response similarly did not explain the novelty of the theory beyond the convergence results of what was given by the paper on SVRPG.  By incorporating some of the reviewer comments, this paper could be a meaningful and useful contribution.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Interesting idea that needs a bit more work"}, "signatures": ["ICLR.cc/2019/Conference/Paper1577/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1577/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Policy Optimization via Stochastic Recursive Gradient Algorithm", "abstract": "In this paper, we propose the StochAstic Recursive grAdient Policy Optimization (SARAPO) algorithm which is a novel variance reduction method on Trust Region Policy Optimization (TRPO). The algorithm incorporates the StochAstic Recursive grAdient algoritHm(SARAH) into the TRPO framework. Compared with the existing Stochastic Variance Reduced Policy Optimization (SVRPO), our algorithm is more stable in the variance. Furthermore, by theoretical analysis the ordinary differential equation and the stochastic differential equation (ODE/SDE) of SARAH, we analyze its convergence property and stability. Our experiments demonstrate its performance on a variety of benchmark tasks. We show that our algorithm gets better improvement in each iteration and matches or even outperforms SVRPO and TRPO.\n", "keywords": ["reinforcement learning", "policy gradient", "variance reduction", "stochastic recursive gradient algorithm"], "authorids": ["yuanhz@pku.edu.cn", "junchi.li.duke@gmail.com", "yuhaotang97@gmail.com", "yuren.zhou@duke.edu"], "authors": ["Huizhuo Yuan", "Chris Junchi Li", "Yuhao Tang", "Yuren Zhou"], "TL;DR": "This paper proposes the StochAstic Recursive Gradient Policy Optimization (SARAPO) algorithm based on the novel SARAH method, and exemplifies its advantages over existing policy gradient methods from both theory and experiments.", "pdf": "/pdf/7d40e2c6a08eaad3b498c61952a6fbfabc34581e.pdf", "paperhash": "yuan|policy_optimization_via_stochastic_recursive_gradient_algorithm", "_bibtex": "@misc{\nyuan2019policy,\ntitle={Policy Optimization via Stochastic Recursive Gradient Algorithm},\nauthor={Huizhuo Yuan and Chris Junchi Li and Yuhao Tang and Yuren Zhou},\nyear={2019},\nurl={https://openreview.net/forum?id=rJl3S2A9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1577/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352787556, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJl3S2A9t7", "replyto": "rJl3S2A9t7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1577/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1577/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1577/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352787556}}}, {"id": "SkxZyFDtA7", "original": null, "number": 4, "cdate": 1543235800756, "ddate": null, "tcdate": 1543235800756, "tmdate": 1543235800756, "tddate": null, "forum": "rJl3S2A9t7", "replyto": "rJl3S2A9t7", "invitation": "ICLR.cc/2019/Conference/-/Paper1577/Official_Comment", "content": {"title": "Respond to all reviewers", "comment": "We sincerely thank all reviewers for the valuable remarks!\n\nWe would like to emphasize that our paper is not an incremental one. We believe that variance reduced (VR) gradient methods (SARAH and SVRG) serve as potential alternatives to incorporate into the TRPO framework [Xu 2017], which might significantly outperform the PG-type algorithms accompanied by importance sampling [Papini 2018]. We aimed to provide the first theoretical analysis in order to support the experiments of VR gradient methods [Xu 2017], and the differential equation approximation for VR is a novel and powerful tool to analyze such.\n\nDespite saying that, we agree that our experiments might not be sufficient to convince some of our proposal. This is partly due to the limited time for running large-scale experiments. Following reviewers' remarks, will try to work more smaller test experiments to support our proposal and fix all the clarity/presentation issues and typos in our next submission."}, "signatures": ["ICLR.cc/2019/Conference/Paper1577/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1577/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1577/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Policy Optimization via Stochastic Recursive Gradient Algorithm", "abstract": "In this paper, we propose the StochAstic Recursive grAdient Policy Optimization (SARAPO) algorithm which is a novel variance reduction method on Trust Region Policy Optimization (TRPO). The algorithm incorporates the StochAstic Recursive grAdient algoritHm(SARAH) into the TRPO framework. Compared with the existing Stochastic Variance Reduced Policy Optimization (SVRPO), our algorithm is more stable in the variance. Furthermore, by theoretical analysis the ordinary differential equation and the stochastic differential equation (ODE/SDE) of SARAH, we analyze its convergence property and stability. Our experiments demonstrate its performance on a variety of benchmark tasks. We show that our algorithm gets better improvement in each iteration and matches or even outperforms SVRPO and TRPO.\n", "keywords": ["reinforcement learning", "policy gradient", "variance reduction", "stochastic recursive gradient algorithm"], "authorids": ["yuanhz@pku.edu.cn", "junchi.li.duke@gmail.com", "yuhaotang97@gmail.com", "yuren.zhou@duke.edu"], "authors": ["Huizhuo Yuan", "Chris Junchi Li", "Yuhao Tang", "Yuren Zhou"], "TL;DR": "This paper proposes the StochAstic Recursive Gradient Policy Optimization (SARAPO) algorithm based on the novel SARAH method, and exemplifies its advantages over existing policy gradient methods from both theory and experiments.", "pdf": "/pdf/7d40e2c6a08eaad3b498c61952a6fbfabc34581e.pdf", "paperhash": "yuan|policy_optimization_via_stochastic_recursive_gradient_algorithm", "_bibtex": "@misc{\nyuan2019policy,\ntitle={Policy Optimization via Stochastic Recursive Gradient Algorithm},\nauthor={Huizhuo Yuan and Chris Junchi Li and Yuhao Tang and Yuren Zhou},\nyear={2019},\nurl={https://openreview.net/forum?id=rJl3S2A9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1577/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621621696, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJl3S2A9t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1577/Authors", "ICLR.cc/2019/Conference/Paper1577/Reviewers", "ICLR.cc/2019/Conference/Paper1577/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1577/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1577/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1577/Authors|ICLR.cc/2019/Conference/Paper1577/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1577/Reviewers", "ICLR.cc/2019/Conference/Paper1577/Authors", "ICLR.cc/2019/Conference/Paper1577/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621621696}}}, {"id": "ByeyPs1YCm", "original": null, "number": 3, "cdate": 1543203671203, "ddate": null, "tcdate": 1543203671203, "tmdate": 1543203671203, "tddate": null, "forum": "rJl3S2A9t7", "replyto": "rJl3S2A9t7", "invitation": "ICLR.cc/2019/Conference/-/Paper1577/Official_Review", "content": {"title": "TRPO with SARAH optimization: great idea, but inconclusive results", "review": "This paper investigates how the SARAH stochastic recursive gradient algorithm can be applied to Trust Region Policy Optimization. The authors analyze the SARAH algorithm using its approximating ordinary and stochastic differential equations. The empirical performance of SARAPO is then compared with SVRPO and TRPO on several benchmark problems.\n\nAlthough the idea of applying SARAH to reduce the variance of gradient estimates in policy gradient algorithms is interesting and potentially quite significant (variance of gradient estimates is a major problem in policy gradient algorithms), I recommend rejecting this paper at the present time due to issues with clarity and quality, particularly of the experiments.\n\nNot enough of the possible values for experimental settings were tested to say anything conclusive about the performance of the algorithms being compared. For the values that were tested, no measures of the variability of performance or statistical significance of the results were given. This is important because the performance of the algorithms is similar on many of the environments, and it is important to know if the improved performance of SARAPO observed on some of the environments is statistically significant or simply due to the small sample size.\n\nThe paper also needs improvements in clarity. Grammatical errors and sentence fragments make it challenging to understand at times. Section 2.3 seemed very brief, and did not include enough discussion of design decisions made in the algorithm. For example, the authors say ``\"the Fisher Information Matrix can be approximated by Hessian matrix of the KL divergence when the current distribution exactly matches that of the base distribution\" but then suggest using the Hessian of the KL of the old parameters and the new parameters which are not the same. What are the consequences of this approximation? Are there alternative approaches?\n\nThe analysis in section 3 is interesting, but the technique has been applied to SGD before and the results only seem to confirm findings from the original SARAH paper.\n\nTo improve the paper, I would suggest moving section 3 to an appendix and using the extra space to further explain details and conduct additional simpler experiments. Additional experiments on simpler environments and policy gradient algorithms (REINFORCE, REINFORCE with baseline) would allow the authors to try more possible values for experimental settings and do enough runs to obtain more conclusive results about performance. Then the authors can present their results applying SARAH to TRPO with some measure of statistical significance.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1577/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Policy Optimization via Stochastic Recursive Gradient Algorithm", "abstract": "In this paper, we propose the StochAstic Recursive grAdient Policy Optimization (SARAPO) algorithm which is a novel variance reduction method on Trust Region Policy Optimization (TRPO). The algorithm incorporates the StochAstic Recursive grAdient algoritHm(SARAH) into the TRPO framework. Compared with the existing Stochastic Variance Reduced Policy Optimization (SVRPO), our algorithm is more stable in the variance. Furthermore, by theoretical analysis the ordinary differential equation and the stochastic differential equation (ODE/SDE) of SARAH, we analyze its convergence property and stability. Our experiments demonstrate its performance on a variety of benchmark tasks. We show that our algorithm gets better improvement in each iteration and matches or even outperforms SVRPO and TRPO.\n", "keywords": ["reinforcement learning", "policy gradient", "variance reduction", "stochastic recursive gradient algorithm"], "authorids": ["yuanhz@pku.edu.cn", "junchi.li.duke@gmail.com", "yuhaotang97@gmail.com", "yuren.zhou@duke.edu"], "authors": ["Huizhuo Yuan", "Chris Junchi Li", "Yuhao Tang", "Yuren Zhou"], "TL;DR": "This paper proposes the StochAstic Recursive Gradient Policy Optimization (SARAPO) algorithm based on the novel SARAH method, and exemplifies its advantages over existing policy gradient methods from both theory and experiments.", "pdf": "/pdf/7d40e2c6a08eaad3b498c61952a6fbfabc34581e.pdf", "paperhash": "yuan|policy_optimization_via_stochastic_recursive_gradient_algorithm", "_bibtex": "@misc{\nyuan2019policy,\ntitle={Policy Optimization via Stochastic Recursive Gradient Algorithm},\nauthor={Huizhuo Yuan and Chris Junchi Li and Yuhao Tang and Yuren Zhou},\nyear={2019},\nurl={https://openreview.net/forum?id=rJl3S2A9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1577/Official_Review", "cdate": 1542234199769, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJl3S2A9t7", "replyto": "rJl3S2A9t7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1577/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335976579, "tmdate": 1552335976579, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1577/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "r1eYIN9phX", "original": null, "number": 2, "cdate": 1541411921186, "ddate": null, "tcdate": 1541411921186, "tmdate": 1541533018737, "tddate": null, "forum": "rJl3S2A9t7", "replyto": "rJl3S2A9t7", "invitation": "ICLR.cc/2019/Conference/-/Paper1577/Official_Review", "content": {"title": "Sarah applied to policy optimization with comparable performance to SVRG", "review": "The paper extends Sarah to policy optimization with theoretical analysis and experimental study. \n\n1) The theoretical analysis under certain assumption seems novel. But the significance is unknown compared to similar analysis. \n\n2) The analysis demonstrates the advantage of Sarah over SVRG, as noted in Remark 1. It would be better to give explicit equations for SVRG in order for comparison.\n\n3) Experimental results seem to show empirically that the SARAH is only comparable to SVRG.\n\n4) Presentation needs to be improved. ", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper1577/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Policy Optimization via Stochastic Recursive Gradient Algorithm", "abstract": "In this paper, we propose the StochAstic Recursive grAdient Policy Optimization (SARAPO) algorithm which is a novel variance reduction method on Trust Region Policy Optimization (TRPO). The algorithm incorporates the StochAstic Recursive grAdient algoritHm(SARAH) into the TRPO framework. Compared with the existing Stochastic Variance Reduced Policy Optimization (SVRPO), our algorithm is more stable in the variance. Furthermore, by theoretical analysis the ordinary differential equation and the stochastic differential equation (ODE/SDE) of SARAH, we analyze its convergence property and stability. Our experiments demonstrate its performance on a variety of benchmark tasks. We show that our algorithm gets better improvement in each iteration and matches or even outperforms SVRPO and TRPO.\n", "keywords": ["reinforcement learning", "policy gradient", "variance reduction", "stochastic recursive gradient algorithm"], "authorids": ["yuanhz@pku.edu.cn", "junchi.li.duke@gmail.com", "yuhaotang97@gmail.com", "yuren.zhou@duke.edu"], "authors": ["Huizhuo Yuan", "Chris Junchi Li", "Yuhao Tang", "Yuren Zhou"], "TL;DR": "This paper proposes the StochAstic Recursive Gradient Policy Optimization (SARAPO) algorithm based on the novel SARAH method, and exemplifies its advantages over existing policy gradient methods from both theory and experiments.", "pdf": "/pdf/7d40e2c6a08eaad3b498c61952a6fbfabc34581e.pdf", "paperhash": "yuan|policy_optimization_via_stochastic_recursive_gradient_algorithm", "_bibtex": "@misc{\nyuan2019policy,\ntitle={Policy Optimization via Stochastic Recursive Gradient Algorithm},\nauthor={Huizhuo Yuan and Chris Junchi Li and Yuhao Tang and Yuren Zhou},\nyear={2019},\nurl={https://openreview.net/forum?id=rJl3S2A9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1577/Official_Review", "cdate": 1542234199769, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJl3S2A9t7", "replyto": "rJl3S2A9t7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1577/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335976579, "tmdate": 1552335976579, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1577/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "r1eJPAn52Q", "original": null, "number": 1, "cdate": 1541226070551, "ddate": null, "tcdate": 1541226070551, "tmdate": 1541533018536, "tddate": null, "forum": "rJl3S2A9t7", "replyto": "rJl3S2A9t7", "invitation": "ICLR.cc/2019/Conference/-/Paper1577/Official_Review", "content": {"title": "Advantages of the proposed method over SVRG + policy gradient method are unclear.", "review": "This paper proposes a new policy gradient method for reinforcement learning.\nThe method essentially combines SARAH and trust region method using Fisher information matrix.\nThe effectiveness of the proposed method is verified in experiments.\n\nSARAH is a variance reduction method developed in stochastic optimization literature, which significantly accelerates convergence speed of stochastic gradient descent.\nSince the policy gradient often suffers from high variance during the training, a combination with variance reduction methods is quite reasonable.\nHowever, this work seems to be rather incremental compared to a previous method adopting another variance reduction method (SVRG) [Xu+2017, Papini+2018].\nMoreover, the advantage of the proposed method over SVRPG (SVRG + policy gradient) is unclear both theoretically and experimentally.\n[Papini+2018] provided a convergence guarantee with its convergence rate, while this paper does not give such a result.\nIt would be nice if the authors could clarify theoretical advantages over SVRPG.\n\nMinor comment:\n- The description of SVRG updates in page 2 is wrong.\n- The notation of H in Section 3.1 (\"ODE analysis\") is not defined at this time.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1577/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Policy Optimization via Stochastic Recursive Gradient Algorithm", "abstract": "In this paper, we propose the StochAstic Recursive grAdient Policy Optimization (SARAPO) algorithm which is a novel variance reduction method on Trust Region Policy Optimization (TRPO). The algorithm incorporates the StochAstic Recursive grAdient algoritHm(SARAH) into the TRPO framework. Compared with the existing Stochastic Variance Reduced Policy Optimization (SVRPO), our algorithm is more stable in the variance. Furthermore, by theoretical analysis the ordinary differential equation and the stochastic differential equation (ODE/SDE) of SARAH, we analyze its convergence property and stability. Our experiments demonstrate its performance on a variety of benchmark tasks. We show that our algorithm gets better improvement in each iteration and matches or even outperforms SVRPO and TRPO.\n", "keywords": ["reinforcement learning", "policy gradient", "variance reduction", "stochastic recursive gradient algorithm"], "authorids": ["yuanhz@pku.edu.cn", "junchi.li.duke@gmail.com", "yuhaotang97@gmail.com", "yuren.zhou@duke.edu"], "authors": ["Huizhuo Yuan", "Chris Junchi Li", "Yuhao Tang", "Yuren Zhou"], "TL;DR": "This paper proposes the StochAstic Recursive Gradient Policy Optimization (SARAPO) algorithm based on the novel SARAH method, and exemplifies its advantages over existing policy gradient methods from both theory and experiments.", "pdf": "/pdf/7d40e2c6a08eaad3b498c61952a6fbfabc34581e.pdf", "paperhash": "yuan|policy_optimization_via_stochastic_recursive_gradient_algorithm", "_bibtex": "@misc{\nyuan2019policy,\ntitle={Policy Optimization via Stochastic Recursive Gradient Algorithm},\nauthor={Huizhuo Yuan and Chris Junchi Li and Yuhao Tang and Yuren Zhou},\nyear={2019},\nurl={https://openreview.net/forum?id=rJl3S2A9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1577/Official_Review", "cdate": 1542234199769, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJl3S2A9t7", "replyto": "rJl3S2A9t7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1577/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335976579, "tmdate": 1552335976579, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1577/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 6}