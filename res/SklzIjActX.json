{"notes": [{"id": "SklzIjActX", "original": "rJxtmNxcKX", "number": 153, "cdate": 1538087753711, "ddate": null, "tcdate": 1538087753711, "tmdate": 1545355426691, "tddate": null, "forum": "SklzIjActX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "HIGHLY EFFICIENT 8-BIT LOW PRECISION INFERENCE OF CONVOLUTIONAL NEURAL NETWORKS", "abstract": "High throughput and low latency inference of deep neural networks are critical for the deployment of deep learning applications. This paper presents a general technique toward 8-bit low precision inference of convolutional neural networks, including 1) channel-wise scale factors of weights, especially for depthwise convolution, 2) Winograd convolution, and 3) topology-wise 8-bit support. We experiment the techniques on top of a widely-used deep learning framework. The 8-bit optimized model is automatically generated with a calibration process from FP32 model without the need of fine-tuning or retraining. We perform a systematical and comprehensive study on 18 widely-used convolutional neural networks and demonstrate the effectiveness of 8-bit low precision inference across a wide range of applications and use cases, including image classification, object detection, image segmentation, and super resolution. We show that the inference throughput\nand latency are improved by 1.6X and 1.5X respectively with minimal within 0.6%1to no loss in accuracy from FP32 baseline. We believe the methodology can provide the guidance and reference design of 8-bit low precision inference for other frameworks. All the code and models will be publicly available soon.", "keywords": ["8-bit low precision inference", "convolutional neural networks", "statistical accuracy", "8-bit Winograd convolution"], "authorids": ["haihao.shen@intel.com", "jiong.gong@intel.com", "xiaoli.liu@intel.com", "guoming.zhang@intel.com", "ge.jin@intel.com", "eric.lin@intel.com"], "authors": ["Haihao Shen", "Jiong Gong", "Xiaoli Liu", "Guoming Zhang", "Ge Jin", "and Eric Lin"], "TL;DR": "We present a general technique toward 8-bit low precision inference of convolutional neural networks. ", "pdf": "/pdf/b9dd890039376e056a3498b29c2612b1fffa81e1.pdf", "paperhash": "shen|highly_efficient_8bit_low_precision_inference_of_convolutional_neural_networks", "_bibtex": "@misc{\nshen2019highly,\ntitle={{HIGHLY} {EFFICIENT} 8-{BIT} {LOW} {PRECISION} {INFERENCE} {OF} {CONVOLUTIONAL} {NEURAL} {NETWORKS}},\nauthor={Haihao Shen and Jiong Gong and Xiaoli Liu and Guoming Zhang and Ge Jin and and Eric Lin},\nyear={2019},\nurl={https://openreview.net/forum?id=SklzIjActX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "B1eZ4hRXeV", "original": null, "number": 1, "cdate": 1544969257201, "ddate": null, "tcdate": 1544969257201, "tmdate": 1545354489236, "tddate": null, "forum": "SklzIjActX", "replyto": "SklzIjActX", "invitation": "ICLR.cc/2019/Conference/-/Paper153/Meta_Review", "content": {"metareview": "The paper proposes to combine three methods of quantization and apply them to neural network compression.  The methods are known in the literature.  There is a lack of theoretical contribution, and experimental results show variable speedups that may not be competitive with the current state-of-the-art in neural network compression.\n\nThe majority of reviewers recommend that this paper be rejected.  The authors have not provided a response.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Area chair recommendation"}, "signatures": ["ICLR.cc/2019/Conference/Paper153/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper153/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HIGHLY EFFICIENT 8-BIT LOW PRECISION INFERENCE OF CONVOLUTIONAL NEURAL NETWORKS", "abstract": "High throughput and low latency inference of deep neural networks are critical for the deployment of deep learning applications. This paper presents a general technique toward 8-bit low precision inference of convolutional neural networks, including 1) channel-wise scale factors of weights, especially for depthwise convolution, 2) Winograd convolution, and 3) topology-wise 8-bit support. We experiment the techniques on top of a widely-used deep learning framework. The 8-bit optimized model is automatically generated with a calibration process from FP32 model without the need of fine-tuning or retraining. We perform a systematical and comprehensive study on 18 widely-used convolutional neural networks and demonstrate the effectiveness of 8-bit low precision inference across a wide range of applications and use cases, including image classification, object detection, image segmentation, and super resolution. We show that the inference throughput\nand latency are improved by 1.6X and 1.5X respectively with minimal within 0.6%1to no loss in accuracy from FP32 baseline. We believe the methodology can provide the guidance and reference design of 8-bit low precision inference for other frameworks. All the code and models will be publicly available soon.", "keywords": ["8-bit low precision inference", "convolutional neural networks", "statistical accuracy", "8-bit Winograd convolution"], "authorids": ["haihao.shen@intel.com", "jiong.gong@intel.com", "xiaoli.liu@intel.com", "guoming.zhang@intel.com", "ge.jin@intel.com", "eric.lin@intel.com"], "authors": ["Haihao Shen", "Jiong Gong", "Xiaoli Liu", "Guoming Zhang", "Ge Jin", "and Eric Lin"], "TL;DR": "We present a general technique toward 8-bit low precision inference of convolutional neural networks. ", "pdf": "/pdf/b9dd890039376e056a3498b29c2612b1fffa81e1.pdf", "paperhash": "shen|highly_efficient_8bit_low_precision_inference_of_convolutional_neural_networks", "_bibtex": "@misc{\nshen2019highly,\ntitle={{HIGHLY} {EFFICIENT} 8-{BIT} {LOW} {PRECISION} {INFERENCE} {OF} {CONVOLUTIONAL} {NEURAL} {NETWORKS}},\nauthor={Haihao Shen and Jiong Gong and Xiaoli Liu and Guoming Zhang and Ge Jin and and Eric Lin},\nyear={2019},\nurl={https://openreview.net/forum?id=SklzIjActX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper153/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353318594, "tddate": null, "super": null, "final": null, "reply": {"forum": "SklzIjActX", "replyto": "SklzIjActX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper153/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper153/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper153/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353318594}}}, {"id": "rJxpIeRR27", "original": null, "number": 3, "cdate": 1541492821177, "ddate": null, "tcdate": 1541492821177, "tmdate": 1541534238728, "tddate": null, "forum": "SklzIjActX", "replyto": "SklzIjActX", "invitation": "ICLR.cc/2019/Conference/-/Paper153/Official_Review", "content": {"title": "a solid paper, but no much novelty", "review": "This paper designs a system to automatically quantize the CNN pretrained models. This system contains three main components: 1) different scale factors for channel-wise network; 2) Winograd 8bit quantization; 3) topology wise 8bit operation support. All these three techniques are standard ways to perform model quantization. The work is solid in the sense that 1) as far as I know there is no work actually using all of these quantization schemes, and designs a system to automatically do quantization with additional algorithm support(retrain strategy). 2)  Significantly amount of experiment on quantizing 18 existing widely used CNN models for different applications, e.g., image classification, image segmentation, etc. 3) it reports the actually inference speed up comparing INT 32 to INT 8, although most of the speed up is less than 2.0.\n\nSeveral questions:\n\n1) What is the difference between retrain and calibration? As mentioned in the paper, the system does not require retrain, but it seems to me that calibration step, e.g., sampling to find the maximum values, etc, is a form of retrain. I think if that is the case, maybe some quantized models with retrain is worthy comparing with.\n\n2) Many quantization techniques are used in the system, are there any conclusions for what techniques are most important for a particular CNN network/application?\n\n3) Are there any new/novel quantization algorithms in the system? \n\n4) The inference speed up is mostly less than 2 times, with some are achieving 2.1 speed up while some are without any speedup. Any reason for that? Also what is the overhead of the inference time?\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper153/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HIGHLY EFFICIENT 8-BIT LOW PRECISION INFERENCE OF CONVOLUTIONAL NEURAL NETWORKS", "abstract": "High throughput and low latency inference of deep neural networks are critical for the deployment of deep learning applications. This paper presents a general technique toward 8-bit low precision inference of convolutional neural networks, including 1) channel-wise scale factors of weights, especially for depthwise convolution, 2) Winograd convolution, and 3) topology-wise 8-bit support. We experiment the techniques on top of a widely-used deep learning framework. The 8-bit optimized model is automatically generated with a calibration process from FP32 model without the need of fine-tuning or retraining. We perform a systematical and comprehensive study on 18 widely-used convolutional neural networks and demonstrate the effectiveness of 8-bit low precision inference across a wide range of applications and use cases, including image classification, object detection, image segmentation, and super resolution. We show that the inference throughput\nand latency are improved by 1.6X and 1.5X respectively with minimal within 0.6%1to no loss in accuracy from FP32 baseline. We believe the methodology can provide the guidance and reference design of 8-bit low precision inference for other frameworks. All the code and models will be publicly available soon.", "keywords": ["8-bit low precision inference", "convolutional neural networks", "statistical accuracy", "8-bit Winograd convolution"], "authorids": ["haihao.shen@intel.com", "jiong.gong@intel.com", "xiaoli.liu@intel.com", "guoming.zhang@intel.com", "ge.jin@intel.com", "eric.lin@intel.com"], "authors": ["Haihao Shen", "Jiong Gong", "Xiaoli Liu", "Guoming Zhang", "Ge Jin", "and Eric Lin"], "TL;DR": "We present a general technique toward 8-bit low precision inference of convolutional neural networks. ", "pdf": "/pdf/b9dd890039376e056a3498b29c2612b1fffa81e1.pdf", "paperhash": "shen|highly_efficient_8bit_low_precision_inference_of_convolutional_neural_networks", "_bibtex": "@misc{\nshen2019highly,\ntitle={{HIGHLY} {EFFICIENT} 8-{BIT} {LOW} {PRECISION} {INFERENCE} {OF} {CONVOLUTIONAL} {NEURAL} {NETWORKS}},\nauthor={Haihao Shen and Jiong Gong and Xiaoli Liu and Guoming Zhang and Ge Jin and and Eric Lin},\nyear={2019},\nurl={https://openreview.net/forum?id=SklzIjActX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper153/Official_Review", "cdate": 1542234526426, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SklzIjActX", "replyto": "SklzIjActX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper153/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335660654, "tmdate": 1552335660654, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper153/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJx1v9Vo2m", "original": null, "number": 2, "cdate": 1541257815454, "ddate": null, "tcdate": 1541257815454, "tmdate": 1541534238525, "tddate": null, "forum": "SklzIjActX", "replyto": "SklzIjActX", "invitation": "ICLR.cc/2019/Conference/-/Paper153/Official_Review", "content": {"title": "More experimental results should be provided", "review": "This paper try to speedup CNN inference with 8-bit quantization. It is practically useful and may be a nice reference for other developers. But the ideas in this paper are trivial and the motivation is not so convincing. \n\n1) For depthwise convolution in Figure 1(b), the dynamic range differences can be eliminated by batch normalization folding. I doubt the necessity of channel-wise scaling. Besides, the author didn't provide any comparison results of single scaling factor and channel-wise scaling factors for depthwise convolution and traditional convolution layers.\n2) For Winograd convolution,  the authors proposed to use scale factors after transformation. They should explain more about this method.\n3)Pooling and concatenation support is quite easy. Batch normalization folding is a common practice.\n4) The author should explain in more detail on fusing convolution and element-wise operations. Sorry I can't get their point. \n5) Calibration results should be provided. The author should also tell readers what hardware is used to evaluate the throughput and latency.\n6) Detail results of winograd should be given.\n\nOverall, this paper is in-complete and the authors should add more experimental results and improve their description.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper153/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HIGHLY EFFICIENT 8-BIT LOW PRECISION INFERENCE OF CONVOLUTIONAL NEURAL NETWORKS", "abstract": "High throughput and low latency inference of deep neural networks are critical for the deployment of deep learning applications. This paper presents a general technique toward 8-bit low precision inference of convolutional neural networks, including 1) channel-wise scale factors of weights, especially for depthwise convolution, 2) Winograd convolution, and 3) topology-wise 8-bit support. We experiment the techniques on top of a widely-used deep learning framework. The 8-bit optimized model is automatically generated with a calibration process from FP32 model without the need of fine-tuning or retraining. We perform a systematical and comprehensive study on 18 widely-used convolutional neural networks and demonstrate the effectiveness of 8-bit low precision inference across a wide range of applications and use cases, including image classification, object detection, image segmentation, and super resolution. We show that the inference throughput\nand latency are improved by 1.6X and 1.5X respectively with minimal within 0.6%1to no loss in accuracy from FP32 baseline. We believe the methodology can provide the guidance and reference design of 8-bit low precision inference for other frameworks. All the code and models will be publicly available soon.", "keywords": ["8-bit low precision inference", "convolutional neural networks", "statistical accuracy", "8-bit Winograd convolution"], "authorids": ["haihao.shen@intel.com", "jiong.gong@intel.com", "xiaoli.liu@intel.com", "guoming.zhang@intel.com", "ge.jin@intel.com", "eric.lin@intel.com"], "authors": ["Haihao Shen", "Jiong Gong", "Xiaoli Liu", "Guoming Zhang", "Ge Jin", "and Eric Lin"], "TL;DR": "We present a general technique toward 8-bit low precision inference of convolutional neural networks. ", "pdf": "/pdf/b9dd890039376e056a3498b29c2612b1fffa81e1.pdf", "paperhash": "shen|highly_efficient_8bit_low_precision_inference_of_convolutional_neural_networks", "_bibtex": "@misc{\nshen2019highly,\ntitle={{HIGHLY} {EFFICIENT} 8-{BIT} {LOW} {PRECISION} {INFERENCE} {OF} {CONVOLUTIONAL} {NEURAL} {NETWORKS}},\nauthor={Haihao Shen and Jiong Gong and Xiaoli Liu and Guoming Zhang and Ge Jin and and Eric Lin},\nyear={2019},\nurl={https://openreview.net/forum?id=SklzIjActX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper153/Official_Review", "cdate": 1542234526426, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SklzIjActX", "replyto": "SklzIjActX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper153/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335660654, "tmdate": 1552335660654, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper153/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJlm4Ejc2X", "original": null, "number": 1, "cdate": 1541219371114, "ddate": null, "tcdate": 1541219371114, "tmdate": 1541534238322, "tddate": null, "forum": "SklzIjActX", "replyto": "SklzIjActX", "invitation": "ICLR.cc/2019/Conference/-/Paper153/Official_Review", "content": {"title": "provides a white paper of engineering issues for INT8 inference on CPUs that appears to be companion to existing ones for INT8 for GPU", "review": "This paper reads more like a technical/hardware white paper than a research paper. No real theory is offered, and the results are not really experiments testing hypotheses, but simply reporting the results of their design choices on a various of models.  Thus, it is not clear that this paper is especially suitable for ICLR research track per se.  Furthermore, the calibration method (to find suitable INT8 weights from fp32 ones without further training) appears to be essentially identical to techniques already reported by Nvidia as used by their TensorRT.  The discussion of INT8 for Winograd is something that could have been new and interesting (i.e. this reader has not seen Nvidia discuss this issue previously), but in the end this paper did not offer anything surprising or especially insightful in the brief Section 3.2.2 explaining their approach.  Furthermore, the experiments such as Table 2 do not include Winograd results anyway because that does not give competitive results using INT8, as the authors admit. ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper153/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HIGHLY EFFICIENT 8-BIT LOW PRECISION INFERENCE OF CONVOLUTIONAL NEURAL NETWORKS", "abstract": "High throughput and low latency inference of deep neural networks are critical for the deployment of deep learning applications. This paper presents a general technique toward 8-bit low precision inference of convolutional neural networks, including 1) channel-wise scale factors of weights, especially for depthwise convolution, 2) Winograd convolution, and 3) topology-wise 8-bit support. We experiment the techniques on top of a widely-used deep learning framework. The 8-bit optimized model is automatically generated with a calibration process from FP32 model without the need of fine-tuning or retraining. We perform a systematical and comprehensive study on 18 widely-used convolutional neural networks and demonstrate the effectiveness of 8-bit low precision inference across a wide range of applications and use cases, including image classification, object detection, image segmentation, and super resolution. We show that the inference throughput\nand latency are improved by 1.6X and 1.5X respectively with minimal within 0.6%1to no loss in accuracy from FP32 baseline. We believe the methodology can provide the guidance and reference design of 8-bit low precision inference for other frameworks. All the code and models will be publicly available soon.", "keywords": ["8-bit low precision inference", "convolutional neural networks", "statistical accuracy", "8-bit Winograd convolution"], "authorids": ["haihao.shen@intel.com", "jiong.gong@intel.com", "xiaoli.liu@intel.com", "guoming.zhang@intel.com", "ge.jin@intel.com", "eric.lin@intel.com"], "authors": ["Haihao Shen", "Jiong Gong", "Xiaoli Liu", "Guoming Zhang", "Ge Jin", "and Eric Lin"], "TL;DR": "We present a general technique toward 8-bit low precision inference of convolutional neural networks. ", "pdf": "/pdf/b9dd890039376e056a3498b29c2612b1fffa81e1.pdf", "paperhash": "shen|highly_efficient_8bit_low_precision_inference_of_convolutional_neural_networks", "_bibtex": "@misc{\nshen2019highly,\ntitle={{HIGHLY} {EFFICIENT} 8-{BIT} {LOW} {PRECISION} {INFERENCE} {OF} {CONVOLUTIONAL} {NEURAL} {NETWORKS}},\nauthor={Haihao Shen and Jiong Gong and Xiaoli Liu and Guoming Zhang and Ge Jin and and Eric Lin},\nyear={2019},\nurl={https://openreview.net/forum?id=SklzIjActX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper153/Official_Review", "cdate": 1542234526426, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SklzIjActX", "replyto": "SklzIjActX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper153/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335660654, "tmdate": 1552335660654, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper153/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}