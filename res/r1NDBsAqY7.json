{"notes": [{"id": "r1NDBsAqY7", "original": "SJeL5ldttQ", "number": 97, "cdate": 1538087743350, "ddate": null, "tcdate": 1538087743350, "tmdate": 1545355392373, "tddate": null, "forum": "r1NDBsAqY7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Unsupervised Word Discovery with Segmental Neural Language Models", "abstract": "We propose a segmental neural language model that combines the representational power of neural networks and the structure learning mechanism of Bayesian nonparametrics, and show that it learns to discover semantically meaningful units (e.g., morphemes and words) from unsegmented character sequences. The model generates text as a sequence of segments, where each segment is generated either character-by-character from a sequence model or as a single draw from a lexical memory that stores multi-character units. Its parameters are fit to maximize the marginal likelihood of the training data, summing over all segmentations of the input, and its hyperparameters are likewise set to optimize held-out marginal likelihood.\nTo prevent the model from overusing the lexical memory, which leads to poor generalization and bad segmentation, we introduce a differentiable regularizer that penalizes based on the expected length of each segment. To our knowledge, this is the first demonstration of neural networks that have predictive distributions better than LSTM language models and also infer a segmentation into word-like units that are competitive with the best existing word discovery models.", "keywords": [], "authorids": ["kawakamik@google.com", "cdyer@google.com", "pblunsom@google.com"], "authors": ["Kazuya Kawakami", "Chris Dyer", "Phil Blunsom"], "TL;DR": "A LSTM language model that discovers words from unsegmented sequences of characters.", "pdf": "/pdf/e28755b019f2b6f54c02f81332378de81217cadc.pdf", "paperhash": "kawakami|unsupervised_word_discovery_with_segmental_neural_language_models", "_bibtex": "@misc{\nkawakami2019unsupervised,\ntitle={Unsupervised Word Discovery with Segmental Neural Language Models},\nauthor={Kazuya Kawakami and Chris Dyer and Phil Blunsom},\nyear={2019},\nurl={https://openreview.net/forum?id=r1NDBsAqY7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 17, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "SJxElUillN", "original": null, "number": 1, "cdate": 1544758763746, "ddate": null, "tcdate": 1544758763746, "tmdate": 1545354518729, "tddate": null, "forum": "r1NDBsAqY7", "replyto": "r1NDBsAqY7", "invitation": "ICLR.cc/2019/Conference/-/Paper97/Meta_Review", "content": {"metareview": "a major issue or complaint from the reviewers seems to come from perhaps a wrong framing of this submission. i believe the framing of this work should have been a better language model (or translation model) with word discovery as an awesome side effect, which i carefully guess would've been a perfectly good story assuming that the perplexity result in Table 4 translates to text with blank spaces left in (it is not possible tell whether this is the case from the text alone.) even discounting R1, who i disagree with on quite a few points, the other reviewers also did not see much of the merit of this work, again probably due to the framing issue above. \n\ni highly encourage the authors to change the framing, evaluate it as a usual sequence model on various benchmarks and resubmit it to another venue.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "reject"}, "signatures": ["ICLR.cc/2019/Conference/Paper97/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper97/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Word Discovery with Segmental Neural Language Models", "abstract": "We propose a segmental neural language model that combines the representational power of neural networks and the structure learning mechanism of Bayesian nonparametrics, and show that it learns to discover semantically meaningful units (e.g., morphemes and words) from unsegmented character sequences. The model generates text as a sequence of segments, where each segment is generated either character-by-character from a sequence model or as a single draw from a lexical memory that stores multi-character units. Its parameters are fit to maximize the marginal likelihood of the training data, summing over all segmentations of the input, and its hyperparameters are likewise set to optimize held-out marginal likelihood.\nTo prevent the model from overusing the lexical memory, which leads to poor generalization and bad segmentation, we introduce a differentiable regularizer that penalizes based on the expected length of each segment. To our knowledge, this is the first demonstration of neural networks that have predictive distributions better than LSTM language models and also infer a segmentation into word-like units that are competitive with the best existing word discovery models.", "keywords": [], "authorids": ["kawakamik@google.com", "cdyer@google.com", "pblunsom@google.com"], "authors": ["Kazuya Kawakami", "Chris Dyer", "Phil Blunsom"], "TL;DR": "A LSTM language model that discovers words from unsegmented sequences of characters.", "pdf": "/pdf/e28755b019f2b6f54c02f81332378de81217cadc.pdf", "paperhash": "kawakami|unsupervised_word_discovery_with_segmental_neural_language_models", "_bibtex": "@misc{\nkawakami2019unsupervised,\ntitle={Unsupervised Word Discovery with Segmental Neural Language Models},\nauthor={Kazuya Kawakami and Chris Dyer and Phil Blunsom},\nyear={2019},\nurl={https://openreview.net/forum?id=r1NDBsAqY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper97/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353337088, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1NDBsAqY7", "replyto": "r1NDBsAqY7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper97/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper97/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper97/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353337088}}}, {"id": "HkeptnOShQ", "original": null, "number": 1, "cdate": 1540881540730, "ddate": null, "tcdate": 1540881540730, "tmdate": 1543370418910, "tddate": null, "forum": "r1NDBsAqY7", "replyto": "r1NDBsAqY7", "invitation": "ICLR.cc/2019/Conference/-/Paper97/Official_Review", "content": {"title": "Interesting research direction, novel contributions, and well-written paper", "review": "This paper presented a novel approach for modeling a sequence of characters as a sequence of latent segmentations. The challenge here was how to efficiently compute the marginal likelihood of a character sequence (exponential number different of segmentations). The author(s) overcame this by having a segment generation process independent from the previous segment (only depends on a sequence of characters). The inference is then required a forward algorithm. To generate a segment, a model can either select a lexical unit (pre-processed from a training corpus) or generate character by character. \n\nOn the experiments, the author(s) showed that the model recovered semantical segmentation on many word segmentation dataset (including phonemes). The lexical memory and the length regularization both contribute significantly as shown in the analysis. The language modeling result (BPC) was also competitive with LSTM-based LMs. \n\nI think the overall model is interesting and well motivated, though it is a bit disappointing that the author(s) needed to use an extra regularizer to constraint the segment length (from the lexical memory?). Perhaps, the way they build a lexical memory should be investigated further. The experiment should also show an evidence that SNLM(+memory, -length) was overfitted as claimed.\n\nThe validation and test dataset have been modified to remove \"samples\" containing OOV characters. How many have been removed? The author(s) could opt for an unknown character similar to many word-level datasets.\n\nThe use of word segmentation data was quite clever, but this also downplayed other work that is not aimed to recover human-semantic segmentations. For example, a segment \"doyou\" on page 10 might be considered as a valid segmentation since it appears a whole lot. HM-LSTM though did poorly on the segmentation task but performed rather well on PTB LM task, but the author(s) decided to omit this comparison.\n\nSome minor comments:\n- A typo in the introduction \"... semi-Markov model. The the characters inside ...\".\n- Eq 3 is a bit hard to follow. Perhaps, a short derivation should be presented.\n- Is it possible to efficiently generate a sequence?\n\n[Updated after reconsidering other reviews]\nAlthough this paper misses some related work and comparison models, I think it still has a valid contribution to language modeling: a character-level language model that produces plausible word segmentation.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper97/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Unsupervised Word Discovery with Segmental Neural Language Models", "abstract": "We propose a segmental neural language model that combines the representational power of neural networks and the structure learning mechanism of Bayesian nonparametrics, and show that it learns to discover semantically meaningful units (e.g., morphemes and words) from unsegmented character sequences. The model generates text as a sequence of segments, where each segment is generated either character-by-character from a sequence model or as a single draw from a lexical memory that stores multi-character units. Its parameters are fit to maximize the marginal likelihood of the training data, summing over all segmentations of the input, and its hyperparameters are likewise set to optimize held-out marginal likelihood.\nTo prevent the model from overusing the lexical memory, which leads to poor generalization and bad segmentation, we introduce a differentiable regularizer that penalizes based on the expected length of each segment. To our knowledge, this is the first demonstration of neural networks that have predictive distributions better than LSTM language models and also infer a segmentation into word-like units that are competitive with the best existing word discovery models.", "keywords": [], "authorids": ["kawakamik@google.com", "cdyer@google.com", "pblunsom@google.com"], "authors": ["Kazuya Kawakami", "Chris Dyer", "Phil Blunsom"], "TL;DR": "A LSTM language model that discovers words from unsegmented sequences of characters.", "pdf": "/pdf/e28755b019f2b6f54c02f81332378de81217cadc.pdf", "paperhash": "kawakami|unsupervised_word_discovery_with_segmental_neural_language_models", "_bibtex": "@misc{\nkawakami2019unsupervised,\ntitle={Unsupervised Word Discovery with Segmental Neural Language Models},\nauthor={Kazuya Kawakami and Chris Dyer and Phil Blunsom},\nyear={2019},\nurl={https://openreview.net/forum?id=r1NDBsAqY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper97/Official_Review", "cdate": 1542234538586, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1NDBsAqY7", "replyto": "r1NDBsAqY7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper97/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335648593, "tmdate": 1552335648593, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper97/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJl5sY9ZAm", "original": null, "number": 8, "cdate": 1542724002019, "ddate": null, "tcdate": 1542724002019, "tmdate": 1542724002019, "tddate": null, "forum": "r1NDBsAqY7", "replyto": "HkeptnOShQ", "invitation": "ICLR.cc/2019/Conference/-/Paper97/Official_Comment", "content": {"title": "In response to AnonReviewer3", "comment": "Thank you for the comments.\n\nRegarding the question about the regularizer. Every model we know of in the literature that contains a nonparametric lexicon has had to have some explicit mechanism for preventing degenerate solutions (variously based on MDL criteria, nonparametric priors, traditional regularization criteria); while it\u2019s possible that the regularization due to drop-out or early stopping would be sufficient to prevent overfitting, those methods are tailored to the neural net aspect of the model (and used here), but we do not find it surprising that a model containing a new kind of memory would also need a new kind of capacity control. We see this not as a surprise or disappointment, but rather as another weak confirmation that the model is behaving as we expect.\n\nRegarding changes to the valid/test data, we removed a single utterance from the validation data (since it contained the rare phoneme \u2018zh\u2019 in the word pleasure, which was not present in the training data).\nRegarding the performance of models that discover \u201cbad\u201d segments but have good predictive distributions. There are indeed many such models, and traditional statistical criteria like mutual information discover many \u201cnon-words\u201d. We will clarify this in the paper, but we wish to refer the reviewers and area chairs to our introduction which articulates that there is intrinsic scientific value in developing models that can chunk unsegmented input into actual words (as humans do)."}, "signatures": ["ICLR.cc/2019/Conference/Paper97/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper97/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper97/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Word Discovery with Segmental Neural Language Models", "abstract": "We propose a segmental neural language model that combines the representational power of neural networks and the structure learning mechanism of Bayesian nonparametrics, and show that it learns to discover semantically meaningful units (e.g., morphemes and words) from unsegmented character sequences. The model generates text as a sequence of segments, where each segment is generated either character-by-character from a sequence model or as a single draw from a lexical memory that stores multi-character units. Its parameters are fit to maximize the marginal likelihood of the training data, summing over all segmentations of the input, and its hyperparameters are likewise set to optimize held-out marginal likelihood.\nTo prevent the model from overusing the lexical memory, which leads to poor generalization and bad segmentation, we introduce a differentiable regularizer that penalizes based on the expected length of each segment. To our knowledge, this is the first demonstration of neural networks that have predictive distributions better than LSTM language models and also infer a segmentation into word-like units that are competitive with the best existing word discovery models.", "keywords": [], "authorids": ["kawakamik@google.com", "cdyer@google.com", "pblunsom@google.com"], "authors": ["Kazuya Kawakami", "Chris Dyer", "Phil Blunsom"], "TL;DR": "A LSTM language model that discovers words from unsegmented sequences of characters.", "pdf": "/pdf/e28755b019f2b6f54c02f81332378de81217cadc.pdf", "paperhash": "kawakami|unsupervised_word_discovery_with_segmental_neural_language_models", "_bibtex": "@misc{\nkawakami2019unsupervised,\ntitle={Unsupervised Word Discovery with Segmental Neural Language Models},\nauthor={Kazuya Kawakami and Chris Dyer and Phil Blunsom},\nyear={2019},\nurl={https://openreview.net/forum?id=r1NDBsAqY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper97/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622139, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1NDBsAqY7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper97/Authors", "ICLR.cc/2019/Conference/Paper97/Reviewers", "ICLR.cc/2019/Conference/Paper97/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper97/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper97/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper97/Authors|ICLR.cc/2019/Conference/Paper97/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper97/Reviewers", "ICLR.cc/2019/Conference/Paper97/Authors", "ICLR.cc/2019/Conference/Paper97/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622139}}}, {"id": "rkgXtKcZ0X", "original": null, "number": 7, "cdate": 1542723962803, "ddate": null, "tcdate": 1542723962803, "tmdate": 1542723962803, "tddate": null, "forum": "r1NDBsAqY7", "replyto": "SJgVbBDkpm", "invitation": "ICLR.cc/2019/Conference/-/Paper97/Official_Comment", "content": {"title": "In response to AnonReviewer1", "comment": "We are puzzled by this unconstructive review. Having published numerous papers on non-parametric Bayesian models of language (including ones cited in the papers the reviewer suggests we do not understand) we hoped it was clear that this work is directly aiming to bring the advantages of Bayesian models of segmentation to state-of-the-art neural language models, while avoiding their disadvantages (e.g. there is nothing Bayesian or principled about Viterbi decoding from a point estimate derived from single Gibbs sample, we know as we have done it!).\nIf there is a specific reference that the reviewer can point to that demonstrates a Bayesian segmentation model achieving perplexities close to the current state of the art we would be delighted to see such a results. Conversely, if there is a reference that demonstrates a neural model achieving unsupervised segmentation performance anywhere near what are reported in our paper we would also be keen to know this.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper97/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper97/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper97/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Word Discovery with Segmental Neural Language Models", "abstract": "We propose a segmental neural language model that combines the representational power of neural networks and the structure learning mechanism of Bayesian nonparametrics, and show that it learns to discover semantically meaningful units (e.g., morphemes and words) from unsegmented character sequences. The model generates text as a sequence of segments, where each segment is generated either character-by-character from a sequence model or as a single draw from a lexical memory that stores multi-character units. Its parameters are fit to maximize the marginal likelihood of the training data, summing over all segmentations of the input, and its hyperparameters are likewise set to optimize held-out marginal likelihood.\nTo prevent the model from overusing the lexical memory, which leads to poor generalization and bad segmentation, we introduce a differentiable regularizer that penalizes based on the expected length of each segment. To our knowledge, this is the first demonstration of neural networks that have predictive distributions better than LSTM language models and also infer a segmentation into word-like units that are competitive with the best existing word discovery models.", "keywords": [], "authorids": ["kawakamik@google.com", "cdyer@google.com", "pblunsom@google.com"], "authors": ["Kazuya Kawakami", "Chris Dyer", "Phil Blunsom"], "TL;DR": "A LSTM language model that discovers words from unsegmented sequences of characters.", "pdf": "/pdf/e28755b019f2b6f54c02f81332378de81217cadc.pdf", "paperhash": "kawakami|unsupervised_word_discovery_with_segmental_neural_language_models", "_bibtex": "@misc{\nkawakami2019unsupervised,\ntitle={Unsupervised Word Discovery with Segmental Neural Language Models},\nauthor={Kazuya Kawakami and Chris Dyer and Phil Blunsom},\nyear={2019},\nurl={https://openreview.net/forum?id=r1NDBsAqY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper97/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622139, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1NDBsAqY7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper97/Authors", "ICLR.cc/2019/Conference/Paper97/Reviewers", "ICLR.cc/2019/Conference/Paper97/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper97/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper97/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper97/Authors|ICLR.cc/2019/Conference/Paper97/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper97/Reviewers", "ICLR.cc/2019/Conference/Paper97/Authors", "ICLR.cc/2019/Conference/Paper97/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622139}}}, {"id": "rJesgY9bAm", "original": null, "number": 6, "cdate": 1542723827151, "ddate": null, "tcdate": 1542723827151, "tmdate": 1542723827151, "tddate": null, "forum": "r1NDBsAqY7", "replyto": "S1lJFgDWTX", "invitation": "ICLR.cc/2019/Conference/-/Paper97/Official_Comment", "content": {"title": "In response to AnonReviewer4", "comment": "Thank you very much for the review.\nThe paper about Chinese segmentation ([3]) is a supervised segmentation model. Which is not comparable with our paper. As we discuss in the paper and in the reply to a comment posted below, the best performing segmentation models [2] rely on hyper-parameters tuned by looking at segmentation performance. However, we argue this is not purely unsupervised learning as it requires gold segmentations at training time. Those models will not work on zero-resource languages or multiple languages without manually tuning the parameters for each language. Nor does supervised heldout tuning provide a plausible model for human language acquisition. That\u2019s why we think it\u2019s important that our model, which is tuned to maximize unsupervised held-out likelihood, achieves competitive segmentation performance.\n\nThe keys are vectors and values are string as we use vectors to query what strings to be generated."}, "signatures": ["ICLR.cc/2019/Conference/Paper97/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper97/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper97/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Word Discovery with Segmental Neural Language Models", "abstract": "We propose a segmental neural language model that combines the representational power of neural networks and the structure learning mechanism of Bayesian nonparametrics, and show that it learns to discover semantically meaningful units (e.g., morphemes and words) from unsegmented character sequences. The model generates text as a sequence of segments, where each segment is generated either character-by-character from a sequence model or as a single draw from a lexical memory that stores multi-character units. Its parameters are fit to maximize the marginal likelihood of the training data, summing over all segmentations of the input, and its hyperparameters are likewise set to optimize held-out marginal likelihood.\nTo prevent the model from overusing the lexical memory, which leads to poor generalization and bad segmentation, we introduce a differentiable regularizer that penalizes based on the expected length of each segment. To our knowledge, this is the first demonstration of neural networks that have predictive distributions better than LSTM language models and also infer a segmentation into word-like units that are competitive with the best existing word discovery models.", "keywords": [], "authorids": ["kawakamik@google.com", "cdyer@google.com", "pblunsom@google.com"], "authors": ["Kazuya Kawakami", "Chris Dyer", "Phil Blunsom"], "TL;DR": "A LSTM language model that discovers words from unsegmented sequences of characters.", "pdf": "/pdf/e28755b019f2b6f54c02f81332378de81217cadc.pdf", "paperhash": "kawakami|unsupervised_word_discovery_with_segmental_neural_language_models", "_bibtex": "@misc{\nkawakami2019unsupervised,\ntitle={Unsupervised Word Discovery with Segmental Neural Language Models},\nauthor={Kazuya Kawakami and Chris Dyer and Phil Blunsom},\nyear={2019},\nurl={https://openreview.net/forum?id=r1NDBsAqY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper97/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622139, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1NDBsAqY7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper97/Authors", "ICLR.cc/2019/Conference/Paper97/Reviewers", "ICLR.cc/2019/Conference/Paper97/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper97/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper97/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper97/Authors|ICLR.cc/2019/Conference/Paper97/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper97/Reviewers", "ICLR.cc/2019/Conference/Paper97/Authors", "ICLR.cc/2019/Conference/Paper97/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622139}}}, {"id": "S1lJFgDWTX", "original": null, "number": 3, "cdate": 1541660790678, "ddate": null, "tcdate": 1541660790678, "tmdate": 1541660790678, "tddate": null, "forum": "r1NDBsAqY7", "replyto": "r1NDBsAqY7", "invitation": "ICLR.cc/2019/Conference/-/Paper97/Official_Review", "content": {"title": "Some interesting ideas, but comparison to previous work might be misleading", "review": "[Note to the authors: I was assigned this paper after the reviewing deadline.]\n\nThe authors train language models on unsegmented text, simultaneously discovering word boundaries\nwithout direct supervision. Given the past history, but ignoring past segmentation decisions\nto keep computations tractable, the model predicts the next character segment (word-like unit)\nby combining a character-level LSTM with a lexical memory. To prevent overusing the\nlexical memory, which would lead to poor generalization, the authors propose a segment\nlength penalty.\n\nStrengths:\n\nThe model architecture is interesting, combining the benefits of a character-level\nmodel (open vocabulary) with those of a lexical model (effective for frequent character\nsequences).\n\nDespite the exponential number of possible segmentations, inference remains tractable\nusing dynamic programming (with some simplifying assumptions).\n\nThe ablation study clearly shows that both the lexical memory and the length penalty\ncontribute significantly.\n\nWeaknesses:\n\nThe writing quality is somewhat weak. Many errors should have been caught when\nproofreading the paper (e.g. \"The segmentation decisions and decisions\" and \n\"The the characters\" on page 1).\n\nI am confused by the key-value pairs of the lexical memory. Shouldn't character\nsequences be keys, and their trainable vector representations be values?\n\nIt is hard to evaluate how good the language models are, as the strength of the\nbaselines is unclear. How well-tuned is the LSTM?\n\nComparison to some other segmentation approaches (not necessarily with language modeling)\nis limited. In particular, adaptor grammars perform very well on the Brent corpus [1].\nHowever, [2] is mentioned briefly. As these other approaches work better for segmentation,\nthe authors should carefully justify why having a single model that does both language\nmodeling and word segmentation well matters. Many neural approaches have also been\nsuggested for Chinese word segmentation (among others [3]). In these papers, results on the\nPKU dataset are much better.  Are these directly comparable with yours?\n\nI would have liked a finer analysis of the impact of the length penalty.\nA plot showing how validation likelihood and segmentation performance vary as\n\\lambda is increased could potentially be interesting.\n\n[1] Johnson and Goldwater. \"Improving nonparameteric Bayesian inference: experiments on\nunsupervised word segmentation with adaptor grammars\", HLT, 2009\n\n[2] Berg-Kirkpatrick et al. \"Painless Unsupervised Learning with Features\", NAACL, 2010\n\n[3] Yang et al. Yang, Jie, Yue Zhang, and Fei Dong. \"Neural Word Segmentation with Rich Pretraining\", ACL, 2017", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper97/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Word Discovery with Segmental Neural Language Models", "abstract": "We propose a segmental neural language model that combines the representational power of neural networks and the structure learning mechanism of Bayesian nonparametrics, and show that it learns to discover semantically meaningful units (e.g., morphemes and words) from unsegmented character sequences. The model generates text as a sequence of segments, where each segment is generated either character-by-character from a sequence model or as a single draw from a lexical memory that stores multi-character units. Its parameters are fit to maximize the marginal likelihood of the training data, summing over all segmentations of the input, and its hyperparameters are likewise set to optimize held-out marginal likelihood.\nTo prevent the model from overusing the lexical memory, which leads to poor generalization and bad segmentation, we introduce a differentiable regularizer that penalizes based on the expected length of each segment. To our knowledge, this is the first demonstration of neural networks that have predictive distributions better than LSTM language models and also infer a segmentation into word-like units that are competitive with the best existing word discovery models.", "keywords": [], "authorids": ["kawakamik@google.com", "cdyer@google.com", "pblunsom@google.com"], "authors": ["Kazuya Kawakami", "Chris Dyer", "Phil Blunsom"], "TL;DR": "A LSTM language model that discovers words from unsegmented sequences of characters.", "pdf": "/pdf/e28755b019f2b6f54c02f81332378de81217cadc.pdf", "paperhash": "kawakami|unsupervised_word_discovery_with_segmental_neural_language_models", "_bibtex": "@misc{\nkawakami2019unsupervised,\ntitle={Unsupervised Word Discovery with Segmental Neural Language Models},\nauthor={Kazuya Kawakami and Chris Dyer and Phil Blunsom},\nyear={2019},\nurl={https://openreview.net/forum?id=r1NDBsAqY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper97/Official_Review", "cdate": 1542234538586, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1NDBsAqY7", "replyto": "r1NDBsAqY7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper97/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335648593, "tmdate": 1552335648593, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper97/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SJgVbBDkpm", "original": null, "number": 5, "cdate": 1541530876105, "ddate": null, "tcdate": 1541530876105, "tmdate": 1541545745466, "tddate": null, "forum": "r1NDBsAqY7", "replyto": "SyxkjOIJTm", "invitation": "ICLR.cc/2019/Conference/-/Paper97/Official_Comment", "content": {"title": "Authors are still misunderstanding previous work", "comment": "Reading the rebuttal, I found that the authors do not understand well what were already done in previous work, which performs far better than this paper.\n\n- 1st, \"a lexicon that memorizes word chunk\" is exactly what is already done by nested PYP or two-stage language models. In such nonparametric models, the lexicon is regarded as a base measure of language models and its atoms, i.e. words, are created when necessary and reused afterwords by a dynamic interpolation. This is exactly the core idea that this paper claims to be novel.\n\n- 2nd, Bayesian segmenting model also produces good predictive accuracy as a language model. Therefore, the argument that this work only aims to produce good predictive accuracy is invalid.\n\n- 3rd, in Bayesian models, \"held-out likelihood\" is unnecessary in general. It is in fact\na general advantage of Bayesian approaches that we do not need such held out data, but just maximizing the likelihood of training data would yield good generalization performance. In fact, \"selecting hyperparameters empirically (ie, based on held-out likelihood) \" in the rebuttal is not true; hyperparameters are just sampled from its posterior using training data only, and it produces very good accuracy. I think this misunderstanding came from the fact that the authors only use a frequentist perspective.\n\n- 4th, evaluations in previous work are just done by splitting training and test data.\nTherefore, \"Bayesian unsupervised learning has traditionally been evaluated in a\nsetup which does not distinguish between a train and test set\" is simply false. \nOnce the model is learned from training data, the model is fixed and the\nsegmentation on test data is computed by a Viterbi decoding (but it should not \nalways be so because it is stochastic).\n\nI know very well that RNN language models have superior  performance than n-gram \nlanguage models, and it fits better to multimodal situations especially.\nTherefore, lately many segmenting neural models are proposed, but almost\nall of them fail to yield good segmentations that Bayesian approaches already\nachieved because these Bayesian work have a structured prior constructed to fit to\nnatural language well, and the hyperparameters are tuned automatically to maximize\nthe performance of language models, as opposed to frequentist methods that require\nheld-out data or heuristic regularization parameter.\nTherefore, in general I think that neural methods should incorporate these previous\nfindings in any way to combine its superior predictive performance to have a better\nsegmenting language models in the future.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper97/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper97/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper97/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Word Discovery with Segmental Neural Language Models", "abstract": "We propose a segmental neural language model that combines the representational power of neural networks and the structure learning mechanism of Bayesian nonparametrics, and show that it learns to discover semantically meaningful units (e.g., morphemes and words) from unsegmented character sequences. The model generates text as a sequence of segments, where each segment is generated either character-by-character from a sequence model or as a single draw from a lexical memory that stores multi-character units. Its parameters are fit to maximize the marginal likelihood of the training data, summing over all segmentations of the input, and its hyperparameters are likewise set to optimize held-out marginal likelihood.\nTo prevent the model from overusing the lexical memory, which leads to poor generalization and bad segmentation, we introduce a differentiable regularizer that penalizes based on the expected length of each segment. To our knowledge, this is the first demonstration of neural networks that have predictive distributions better than LSTM language models and also infer a segmentation into word-like units that are competitive with the best existing word discovery models.", "keywords": [], "authorids": ["kawakamik@google.com", "cdyer@google.com", "pblunsom@google.com"], "authors": ["Kazuya Kawakami", "Chris Dyer", "Phil Blunsom"], "TL;DR": "A LSTM language model that discovers words from unsegmented sequences of characters.", "pdf": "/pdf/e28755b019f2b6f54c02f81332378de81217cadc.pdf", "paperhash": "kawakami|unsupervised_word_discovery_with_segmental_neural_language_models", "_bibtex": "@misc{\nkawakami2019unsupervised,\ntitle={Unsupervised Word Discovery with Segmental Neural Language Models},\nauthor={Kazuya Kawakami and Chris Dyer and Phil Blunsom},\nyear={2019},\nurl={https://openreview.net/forum?id=r1NDBsAqY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper97/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622139, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1NDBsAqY7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper97/Authors", "ICLR.cc/2019/Conference/Paper97/Reviewers", "ICLR.cc/2019/Conference/Paper97/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper97/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper97/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper97/Authors|ICLR.cc/2019/Conference/Paper97/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper97/Reviewers", "ICLR.cc/2019/Conference/Paper97/Authors", "ICLR.cc/2019/Conference/Paper97/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622139}}}, {"id": "B1xAJ5Xo37", "original": null, "number": 2, "cdate": 1541253605686, "ddate": null, "tcdate": 1541253605686, "tmdate": 1541534286085, "tddate": null, "forum": "r1NDBsAqY7", "replyto": "r1NDBsAqY7", "invitation": "ICLR.cc/2019/Conference/-/Paper97/Official_Review", "content": {"title": "Very little awareness on latest research on unsupervised word segmentation", "review": "This paper proposes a neural architecture for segmental language modeling\nthat enables unsupervised word discoveries. The architecture employes a \ntwo-stage architecture that a word might be a type, or a sequence of characters\nof its spellings. \nThis idea is basically similar to Nested Pitman-Yor language models \n(Mochihashi et al. 2009) and two-stage language models (Goldwater et al. 2011),\nbut the authors seem not to notice these previous work.\nExperimental results show some improvements on naive baselines, but clearly \nbelow the state-of-the-art in unsupervised word segmentation.\n\nAs noted above, the crucial drawback of this paper is that the authors are\ncompletely unaware of latest achievements on unsupervised word segmentation\nand discovery, rather than old, simplistic baselines such as Goldwater+ (2009,\nidea is based on Goldwater+ ACL 2006) or Berg-Kirkpatrick (2010).\nThe idea of using characters and words is already exploited in Mochihashi+\n(ACL 2009) in a nonparametric Bayesian framework; it has a better F1 than this\nwork by a large margin. Moreover, it is recently extended (Uchiumi+ TACL \n2015) to also include latent word categories as well as segmentations to yield \nthe state-of-the-art accuracies on F1=81.6 on PKU corpus, as compared to 73.1 \nin this paper. \nNote that they employ a prior distribution on segment lengths as a (mixture of) \nPoisson distributions or negative binomials whose parameters are \nautomatically learned during inference, as compared to a post-hoc regularization\nused in this paper.\n\nIn a Bayesian framework, interpolations between words and characters are\ntheoretically derived and quite carefully learned, and regularizations are\nautomatically adjusted. While neural architectures have some potentials \nto improve over them, current heuristic architectures that have lower \nperformance does not have any advantage over these methods, \nboth theoretically and empicially.\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper97/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Word Discovery with Segmental Neural Language Models", "abstract": "We propose a segmental neural language model that combines the representational power of neural networks and the structure learning mechanism of Bayesian nonparametrics, and show that it learns to discover semantically meaningful units (e.g., morphemes and words) from unsegmented character sequences. The model generates text as a sequence of segments, where each segment is generated either character-by-character from a sequence model or as a single draw from a lexical memory that stores multi-character units. Its parameters are fit to maximize the marginal likelihood of the training data, summing over all segmentations of the input, and its hyperparameters are likewise set to optimize held-out marginal likelihood.\nTo prevent the model from overusing the lexical memory, which leads to poor generalization and bad segmentation, we introduce a differentiable regularizer that penalizes based on the expected length of each segment. To our knowledge, this is the first demonstration of neural networks that have predictive distributions better than LSTM language models and also infer a segmentation into word-like units that are competitive with the best existing word discovery models.", "keywords": [], "authorids": ["kawakamik@google.com", "cdyer@google.com", "pblunsom@google.com"], "authors": ["Kazuya Kawakami", "Chris Dyer", "Phil Blunsom"], "TL;DR": "A LSTM language model that discovers words from unsegmented sequences of characters.", "pdf": "/pdf/e28755b019f2b6f54c02f81332378de81217cadc.pdf", "paperhash": "kawakami|unsupervised_word_discovery_with_segmental_neural_language_models", "_bibtex": "@misc{\nkawakami2019unsupervised,\ntitle={Unsupervised Word Discovery with Segmental Neural Language Models},\nauthor={Kazuya Kawakami and Chris Dyer and Phil Blunsom},\nyear={2019},\nurl={https://openreview.net/forum?id=r1NDBsAqY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper97/Official_Review", "cdate": 1542234538586, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1NDBsAqY7", "replyto": "r1NDBsAqY7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper97/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335648593, "tmdate": 1552335648593, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper97/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SyxkjOIJTm", "original": null, "number": 4, "cdate": 1541527702692, "ddate": null, "tcdate": 1541527702692, "tmdate": 1541527702692, "tddate": null, "forum": "r1NDBsAqY7", "replyto": "B1xAJ5Xo37", "invitation": "ICLR.cc/2019/Conference/-/Paper97/Official_Comment", "content": {"title": " In response to AnonReviewer1", "comment": "This work presents a recurrent neural network language model that obtains better predictive distributions (perplexity) than an LSTM while also discovering the words that exist in language. The review above misconstrues the aim of this paper as simply producing the best segmentation accuracy, which the papers cited achieve by tuning for segmentation performance on held out data and sacrificing predictive accuracy. While we agree with the reviewer that there has been much excellent work done on nonparametric Bayesian segmentation (and two key ideas from that modeling tradition directly inspired this work!), no such model has been shown to achieve perplexities close to those of an RNN. However no previous RNN has been shown to discover plausible word segmentations. Our model achieves both. In doing so we argue that word segmentation is not a task that should be studied in isolation from the rest of the language learning but that the flexibility of neural models means they can approach other aspects (e.g., grounding in different modalities or tasks, learning large scale syntactic regularities) more naturally than would be practical with current Bayesian techniques.\n\nBelow we elaborate on several more technical objections to this review:\n\nFirst, our decision to focus on DP/HDP models rather than the extensions referred to in the review (specifically PYP/HPYPs, nested PYPs, integrating out hyperparameters, etc.) was not due to ignorance, but rather that we were incorporating a two core ideas from Bayesian nonparametric word segmentation/language modeling into neural networks and we chose the simplest possible Bayesian model that made our points. These are: (1) that a lexicon that memorizes word chunks is useful for inducing good segmentations; (2) that capacity control is important when you have a lexicon like this. We do agree that nested PYPs, which learn to model character sequences (although not across word boundaries), deserve discussion and we will update the paper accordingly (again we emphasize that this is an oversight, not something that changes the meaningfulness of our results). Thus, the DP/HDP models otherwise perfectly illustrates the points they needed to illustrate, and the newer variations do not offer any additional insight into how to fix the problems that RNNs have with discovering words.\n\nSecond, our results are not precisely comparable since Bayesian unsupervised learning has traditionally been evaluated in a setup which does not distinguish between a train and test set, or which uses observations from both when performing posterior inference. As we demonstrated, in Bayesian models, selecting hyperparameters empirically (ie, based on held-out likelihood) results in less effective structure discovery than setting the hyperparameters subjectively (however, since some standard datasets did not have a train/test split until this paper, we expect that in many cases, these models were chosen based on reported segmentation accuracies!). We certainly appreciate the insights that have been enabled by using both methodologies, but our perspective is that relying on held-out likelihood for model selection is eminently defensible methodology. However, held-out likelihood is indeed a radically different development/training/evaluation methodology for working on segmentation that is a better fit for neural models (and, we think, Bayesian models as well) than what came before, and it does make the results incomparable. Finally, another source of incompatibility is that the Uchiumi et al (2015) length distribution correction relies on hand-engineered features. We expected these were selected to improve reported segmentation accuracy (rather than validation likelihood), and as an ICLR paper, we are exploring how well we can do with learning representations, rather than engineering them.\n\nThird, the goal of this paper was to show that it is possible to align the goals of good segment discovery with good held-out models of language (after all, humans are good at both!). In their zeal to argue that our segmentation accuracy lags behind that of the best Bayesian models (which we questioned in the previous paragraph), the reviewers ignore the crucial fact that the most basic RNNs outperform the best hierarchical Bayesian language models by far in terms of predicting held-out data. Surprisingly, although posterior predictive checking is a standard tool for assessing Bayesian models, none of the existing Bayesian segmentation papers seem to have used this methodology, and so we had to include our own likelihood experiments (Table 4) to demonstrate this disparity. These results show clearly that while Bayesian models are perhaps slightly better than our models in terms of segmentation accuracy, they are far less good than RNNs in terms of predictive accuracy. On the other hand, our RNNs are good at both."}, "signatures": ["ICLR.cc/2019/Conference/Paper97/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper97/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper97/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Word Discovery with Segmental Neural Language Models", "abstract": "We propose a segmental neural language model that combines the representational power of neural networks and the structure learning mechanism of Bayesian nonparametrics, and show that it learns to discover semantically meaningful units (e.g., morphemes and words) from unsegmented character sequences. The model generates text as a sequence of segments, where each segment is generated either character-by-character from a sequence model or as a single draw from a lexical memory that stores multi-character units. Its parameters are fit to maximize the marginal likelihood of the training data, summing over all segmentations of the input, and its hyperparameters are likewise set to optimize held-out marginal likelihood.\nTo prevent the model from overusing the lexical memory, which leads to poor generalization and bad segmentation, we introduce a differentiable regularizer that penalizes based on the expected length of each segment. To our knowledge, this is the first demonstration of neural networks that have predictive distributions better than LSTM language models and also infer a segmentation into word-like units that are competitive with the best existing word discovery models.", "keywords": [], "authorids": ["kawakamik@google.com", "cdyer@google.com", "pblunsom@google.com"], "authors": ["Kazuya Kawakami", "Chris Dyer", "Phil Blunsom"], "TL;DR": "A LSTM language model that discovers words from unsegmented sequences of characters.", "pdf": "/pdf/e28755b019f2b6f54c02f81332378de81217cadc.pdf", "paperhash": "kawakami|unsupervised_word_discovery_with_segmental_neural_language_models", "_bibtex": "@misc{\nkawakami2019unsupervised,\ntitle={Unsupervised Word Discovery with Segmental Neural Language Models},\nauthor={Kazuya Kawakami and Chris Dyer and Phil Blunsom},\nyear={2019},\nurl={https://openreview.net/forum?id=r1NDBsAqY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper97/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622139, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1NDBsAqY7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper97/Authors", "ICLR.cc/2019/Conference/Paper97/Reviewers", "ICLR.cc/2019/Conference/Paper97/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper97/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper97/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper97/Authors|ICLR.cc/2019/Conference/Paper97/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper97/Reviewers", "ICLR.cc/2019/Conference/Paper97/Authors", "ICLR.cc/2019/Conference/Paper97/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622139}}}, {"id": "HyeWqgLaom", "original": null, "number": 5, "cdate": 1540345993105, "ddate": null, "tcdate": 1540345993105, "tmdate": 1540345993105, "tddate": null, "forum": "r1NDBsAqY7", "replyto": "HyehCdhmiQ", "invitation": "ICLR.cc/2019/Conference/-/Paper97/Public_Comment", "content": {"comment": "Thanks for your detailed responses!\nI think that many of these clarifications and additional details are very interesting and possibly of interest to other readers as well, so I hope you will be able to incorporate them into camera-ready or the appendix.\n\nSome small follow-ups:\nFor Q1, I now better understand what you meant, but it might be worth considering to drop or additionally qualify the word \"Bayesian\" -- it really did confuse me at first.\nFor Q2, I guess it is the frequency filtering that is key to getting, say, the PTB vocab from 6M (all length-10 strings) to 30k (only those that appear more than 100 times). I'd be curious how much the empirical success of the SNLM depends on the hyperparameter F (and, to a letter extent L and \\lambda), but I understand that there is little space (and maybe reason) to report much on the grid search you did over both.\n\nAgain, thanks for your detailed comments, I appreciate them.", "title": "Thanks!"}, "signatures": ["~Sebastian_J_Mielke1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper97/Reviewers/Unsubmitted"], "writers": ["~Sebastian_J_Mielke1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Word Discovery with Segmental Neural Language Models", "abstract": "We propose a segmental neural language model that combines the representational power of neural networks and the structure learning mechanism of Bayesian nonparametrics, and show that it learns to discover semantically meaningful units (e.g., morphemes and words) from unsegmented character sequences. The model generates text as a sequence of segments, where each segment is generated either character-by-character from a sequence model or as a single draw from a lexical memory that stores multi-character units. Its parameters are fit to maximize the marginal likelihood of the training data, summing over all segmentations of the input, and its hyperparameters are likewise set to optimize held-out marginal likelihood.\nTo prevent the model from overusing the lexical memory, which leads to poor generalization and bad segmentation, we introduce a differentiable regularizer that penalizes based on the expected length of each segment. To our knowledge, this is the first demonstration of neural networks that have predictive distributions better than LSTM language models and also infer a segmentation into word-like units that are competitive with the best existing word discovery models.", "keywords": [], "authorids": ["kawakamik@google.com", "cdyer@google.com", "pblunsom@google.com"], "authors": ["Kazuya Kawakami", "Chris Dyer", "Phil Blunsom"], "TL;DR": "A LSTM language model that discovers words from unsegmented sequences of characters.", "pdf": "/pdf/e28755b019f2b6f54c02f81332378de81217cadc.pdf", "paperhash": "kawakami|unsupervised_word_discovery_with_segmental_neural_language_models", "_bibtex": "@misc{\nkawakami2019unsupervised,\ntitle={Unsupervised Word Discovery with Segmental Neural Language Models},\nauthor={Kazuya Kawakami and Chris Dyer and Phil Blunsom},\nyear={2019},\nurl={https://openreview.net/forum?id=r1NDBsAqY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper97/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311919467, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "r1NDBsAqY7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper97/Authors", "ICLR.cc/2019/Conference/Paper97/Reviewers", "ICLR.cc/2019/Conference/Paper97/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper97/Authors", "ICLR.cc/2019/Conference/Paper97/Reviewers", "ICLR.cc/2019/Conference/Paper97/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311919467}}}, {"id": "HklWx4ABo7", "original": null, "number": 4, "cdate": 1539855337418, "ddate": null, "tcdate": 1539855337418, "tmdate": 1539855337418, "tddate": null, "forum": "r1NDBsAqY7", "replyto": "HylO-Yh7oX", "invitation": "ICLR.cc/2019/Conference/-/Paper97/Public_Comment", "content": {"comment": "Yes ofcourse, this is not the same as the above mentioned work! I just wanted to draw your attention to the paper in case you had missed it.", "title": "Yes! Ofcourse"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper97/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Word Discovery with Segmental Neural Language Models", "abstract": "We propose a segmental neural language model that combines the representational power of neural networks and the structure learning mechanism of Bayesian nonparametrics, and show that it learns to discover semantically meaningful units (e.g., morphemes and words) from unsegmented character sequences. The model generates text as a sequence of segments, where each segment is generated either character-by-character from a sequence model or as a single draw from a lexical memory that stores multi-character units. Its parameters are fit to maximize the marginal likelihood of the training data, summing over all segmentations of the input, and its hyperparameters are likewise set to optimize held-out marginal likelihood.\nTo prevent the model from overusing the lexical memory, which leads to poor generalization and bad segmentation, we introduce a differentiable regularizer that penalizes based on the expected length of each segment. To our knowledge, this is the first demonstration of neural networks that have predictive distributions better than LSTM language models and also infer a segmentation into word-like units that are competitive with the best existing word discovery models.", "keywords": [], "authorids": ["kawakamik@google.com", "cdyer@google.com", "pblunsom@google.com"], "authors": ["Kazuya Kawakami", "Chris Dyer", "Phil Blunsom"], "TL;DR": "A LSTM language model that discovers words from unsegmented sequences of characters.", "pdf": "/pdf/e28755b019f2b6f54c02f81332378de81217cadc.pdf", "paperhash": "kawakami|unsupervised_word_discovery_with_segmental_neural_language_models", "_bibtex": "@misc{\nkawakami2019unsupervised,\ntitle={Unsupervised Word Discovery with Segmental Neural Language Models},\nauthor={Kazuya Kawakami and Chris Dyer and Phil Blunsom},\nyear={2019},\nurl={https://openreview.net/forum?id=r1NDBsAqY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper97/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311919467, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "r1NDBsAqY7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper97/Authors", "ICLR.cc/2019/Conference/Paper97/Reviewers", "ICLR.cc/2019/Conference/Paper97/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper97/Authors", "ICLR.cc/2019/Conference/Paper97/Reviewers", "ICLR.cc/2019/Conference/Paper97/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311919467}}}, {"id": "HkgKMthXom", "original": null, "number": 3, "cdate": 1539717393038, "ddate": null, "tcdate": 1539717393038, "tmdate": 1539717393038, "tddate": null, "forum": "r1NDBsAqY7", "replyto": "SJxE5HCFcm", "invitation": "ICLR.cc/2019/Conference/-/Paper97/Official_Comment", "content": {"title": "Author response to questions. ", "comment": "We will cite the paper and carefully discuss their results. Briefly, there are several differences that explain the difference. One major difference is the training scheme. If you look at the code for the EMNLP paper (https: //github.com/Edward-Sun/SLM), the model is being tuned to maximize the segmentation performance by running segmentation evaluation during training. We argue that this is not a purely unsupervised model as it requires gold segmentation data at training time (e.g., setting maxlen=2 is an extremely strong form of supervision), and in our work, we are crucially interested in a model whose word predictive likelihood correlates with (unobserved) segmentation performance.\n\nThere are other differences in experimental setup.\n1) The EMNLP paper uses pre-trained word embeddings trained on a large corpus.\n2) The EMNLP paper preprocessed arabic numerals, punctuation symbols, and English words.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper97/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper97/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper97/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Word Discovery with Segmental Neural Language Models", "abstract": "We propose a segmental neural language model that combines the representational power of neural networks and the structure learning mechanism of Bayesian nonparametrics, and show that it learns to discover semantically meaningful units (e.g., morphemes and words) from unsegmented character sequences. The model generates text as a sequence of segments, where each segment is generated either character-by-character from a sequence model or as a single draw from a lexical memory that stores multi-character units. Its parameters are fit to maximize the marginal likelihood of the training data, summing over all segmentations of the input, and its hyperparameters are likewise set to optimize held-out marginal likelihood.\nTo prevent the model from overusing the lexical memory, which leads to poor generalization and bad segmentation, we introduce a differentiable regularizer that penalizes based on the expected length of each segment. To our knowledge, this is the first demonstration of neural networks that have predictive distributions better than LSTM language models and also infer a segmentation into word-like units that are competitive with the best existing word discovery models.", "keywords": [], "authorids": ["kawakamik@google.com", "cdyer@google.com", "pblunsom@google.com"], "authors": ["Kazuya Kawakami", "Chris Dyer", "Phil Blunsom"], "TL;DR": "A LSTM language model that discovers words from unsegmented sequences of characters.", "pdf": "/pdf/e28755b019f2b6f54c02f81332378de81217cadc.pdf", "paperhash": "kawakami|unsupervised_word_discovery_with_segmental_neural_language_models", "_bibtex": "@misc{\nkawakami2019unsupervised,\ntitle={Unsupervised Word Discovery with Segmental Neural Language Models},\nauthor={Kazuya Kawakami and Chris Dyer and Phil Blunsom},\nyear={2019},\nurl={https://openreview.net/forum?id=r1NDBsAqY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper97/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622139, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1NDBsAqY7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper97/Authors", "ICLR.cc/2019/Conference/Paper97/Reviewers", "ICLR.cc/2019/Conference/Paper97/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper97/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper97/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper97/Authors|ICLR.cc/2019/Conference/Paper97/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper97/Reviewers", "ICLR.cc/2019/Conference/Paper97/Authors", "ICLR.cc/2019/Conference/Paper97/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622139}}}, {"id": "HylO-Yh7oX", "original": null, "number": 2, "cdate": 1539717376410, "ddate": null, "tcdate": 1539717376410, "tmdate": 1539717376410, "tddate": null, "forum": "r1NDBsAqY7", "replyto": "S1xTLWQccQ", "invitation": "ICLR.cc/2019/Conference/-/Paper97/Official_Comment", "content": {"title": "Author response to questions. ", "comment": "Thank you for drawing our attention to this work, which we had managed to miss. It uses a similar strategy to populate its dictionary, but doesn\u2019t include a character-based method for generating \u201cinside words\u201d (see the response above to C1 for our empirical findings regarding the importance of this component to segmentation quality)."}, "signatures": ["ICLR.cc/2019/Conference/Paper97/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper97/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper97/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Word Discovery with Segmental Neural Language Models", "abstract": "We propose a segmental neural language model that combines the representational power of neural networks and the structure learning mechanism of Bayesian nonparametrics, and show that it learns to discover semantically meaningful units (e.g., morphemes and words) from unsegmented character sequences. The model generates text as a sequence of segments, where each segment is generated either character-by-character from a sequence model or as a single draw from a lexical memory that stores multi-character units. Its parameters are fit to maximize the marginal likelihood of the training data, summing over all segmentations of the input, and its hyperparameters are likewise set to optimize held-out marginal likelihood.\nTo prevent the model from overusing the lexical memory, which leads to poor generalization and bad segmentation, we introduce a differentiable regularizer that penalizes based on the expected length of each segment. To our knowledge, this is the first demonstration of neural networks that have predictive distributions better than LSTM language models and also infer a segmentation into word-like units that are competitive with the best existing word discovery models.", "keywords": [], "authorids": ["kawakamik@google.com", "cdyer@google.com", "pblunsom@google.com"], "authors": ["Kazuya Kawakami", "Chris Dyer", "Phil Blunsom"], "TL;DR": "A LSTM language model that discovers words from unsegmented sequences of characters.", "pdf": "/pdf/e28755b019f2b6f54c02f81332378de81217cadc.pdf", "paperhash": "kawakami|unsupervised_word_discovery_with_segmental_neural_language_models", "_bibtex": "@misc{\nkawakami2019unsupervised,\ntitle={Unsupervised Word Discovery with Segmental Neural Language Models},\nauthor={Kazuya Kawakami and Chris Dyer and Phil Blunsom},\nyear={2019},\nurl={https://openreview.net/forum?id=r1NDBsAqY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper97/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622139, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1NDBsAqY7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper97/Authors", "ICLR.cc/2019/Conference/Paper97/Reviewers", "ICLR.cc/2019/Conference/Paper97/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper97/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper97/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper97/Authors|ICLR.cc/2019/Conference/Paper97/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper97/Reviewers", "ICLR.cc/2019/Conference/Paper97/Authors", "ICLR.cc/2019/Conference/Paper97/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622139}}}, {"id": "HyehCdhmiQ", "original": null, "number": 1, "cdate": 1539717331788, "ddate": null, "tcdate": 1539717331788, "tmdate": 1539717356031, "tddate": null, "forum": "r1NDBsAqY7", "replyto": "BkeaRjdc5Q", "invitation": "ICLR.cc/2019/Conference/-/Paper97/Official_Comment", "content": {"title": "Author response to questions.", "comment": "Thank you very much for the interesting questions and giving us an opportunity to respond to questions that many readers might have had.\n\nQ1: The connection to Bayesian nonparametrics is just an analogy, referring to the fact that there is both a memorized component that grows unboundedly with the data and a penalty that prevents that excess capacity from being overused.\n\nQ2: We will add information about the softmax size to the appendix; it is big, around 20k, but it remains tractable.\n\nQ3: The character-level model enables the model to generate chunks that are not stored in memory. For example, numbers will not generally be in memory, but they should be a chunk. Empirically, we found removing p_char and putting single-length characters in the lexicon resulted in substantially worse results.\n\nQ4: BPE is a kind of unigram-model-based compression with capacity control provided by the limits on vocabulary size, and a particular greedy inference procedure -- and it can indeed be compared. The best segmentation result that would be possible with BPE (i.e., tuning the vocabulary size on segmentation accuracy) is P 24.97 R 36.59 F 29.68 which is better than the HMLSTM baseline but worse than LSTM surprisal baseline P 54.5 R 55.5 F 55.0. The optimal segmentation selected by likelihood would, presumably, be worse.\n\nAs for the Elman\u2019s surprisal baseline, we will clarify how we create these to make the paper more self-contained. For now, please refer to the original paper [https://crl.ucsd.edu/~elman/Papers/fsit.pdf]. Since they deterministically rely on changes in conditional entropy, rather than stochastic decisions, it\u2019s not obvious how to sample from them.\n\nQ5: The focus of this paper is on unsupervised word discovery, which we think is not a very coherent problem when spaces that mark word boundaries are present. Although some recent prior work has done segmentation on data that includes spaces, the very long history of this problem has almost entirely been explored without spaces.\n\nQ6: The comparison is primarily to draw attention to the fact that the memory is accessed sparsely, parameter counts of the +mem model cannot be compared straightforwardly to a model without memory, and therefore that the improvements in perplexity aren\u2019t just an artifact of \u201cmore parameters\u201d (we also find that doubling the size of the character-only LSTMs did not result in improved performance). We will clarify this.\n\nQ7: Our view is that when it comes to intrinsic evaluation of segmentation, there are several possible \u201cbest answers\u201d (e.g., any answer to the following questions can be justified: should bound morphemes be split? Should unbound morphemes be split? Should phonological words be the true units? Should idiosyncratic multiword expressions be the units? Should multiword expressions with transparent meanings be the units?); however, there are vastly many more truly bad segmentations (two bad segmentation of bad are \u201cb +ad\u201d and \u201cba +d\u201d). Therefore, a more ideal evaluation should give credit for getting any reasonable segmentation, but only penalizing you for proposing a completely wrong segment/word (this is similar to the approach taken by Alignment Error Rate (Och & Ney, 2000) in assessing the quality of bilingual word alignments which supports both \u201csure\u201d and \u201cpossible\u201d alignments; and it is reminiscent of the approach taken by Dyer (2009), who trained a supervised word segmentation model to update toward a lattice of possible segmentations). Since there are no annotations of \u201cpossible segmentations\u201d for the languages we were studying, we used conventional tasks and segmentation schemes for this problem to approximate this (although we do compare with two different Chinese segmentation standards). Regarding the question about Turkish, we expect that it would discover subword units since we already see evidence that morphemes like \u201ced\u201d \u201cing\u201d and \u201cs\u201d are segmented off in English in the likelihood-maximizing configuration.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper97/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper97/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper97/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Word Discovery with Segmental Neural Language Models", "abstract": "We propose a segmental neural language model that combines the representational power of neural networks and the structure learning mechanism of Bayesian nonparametrics, and show that it learns to discover semantically meaningful units (e.g., morphemes and words) from unsegmented character sequences. The model generates text as a sequence of segments, where each segment is generated either character-by-character from a sequence model or as a single draw from a lexical memory that stores multi-character units. Its parameters are fit to maximize the marginal likelihood of the training data, summing over all segmentations of the input, and its hyperparameters are likewise set to optimize held-out marginal likelihood.\nTo prevent the model from overusing the lexical memory, which leads to poor generalization and bad segmentation, we introduce a differentiable regularizer that penalizes based on the expected length of each segment. To our knowledge, this is the first demonstration of neural networks that have predictive distributions better than LSTM language models and also infer a segmentation into word-like units that are competitive with the best existing word discovery models.", "keywords": [], "authorids": ["kawakamik@google.com", "cdyer@google.com", "pblunsom@google.com"], "authors": ["Kazuya Kawakami", "Chris Dyer", "Phil Blunsom"], "TL;DR": "A LSTM language model that discovers words from unsegmented sequences of characters.", "pdf": "/pdf/e28755b019f2b6f54c02f81332378de81217cadc.pdf", "paperhash": "kawakami|unsupervised_word_discovery_with_segmental_neural_language_models", "_bibtex": "@misc{\nkawakami2019unsupervised,\ntitle={Unsupervised Word Discovery with Segmental Neural Language Models},\nauthor={Kazuya Kawakami and Chris Dyer and Phil Blunsom},\nyear={2019},\nurl={https://openreview.net/forum?id=r1NDBsAqY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper97/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622139, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1NDBsAqY7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper97/Authors", "ICLR.cc/2019/Conference/Paper97/Reviewers", "ICLR.cc/2019/Conference/Paper97/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper97/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper97/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper97/Authors|ICLR.cc/2019/Conference/Paper97/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper97/Reviewers", "ICLR.cc/2019/Conference/Paper97/Authors", "ICLR.cc/2019/Conference/Paper97/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622139}}}, {"id": "BkeaRjdc5Q", "original": null, "number": 3, "cdate": 1539111893011, "ddate": null, "tcdate": 1539111893011, "tmdate": 1539111893011, "tddate": null, "forum": "r1NDBsAqY7", "replyto": "r1NDBsAqY7", "invitation": "ICLR.cc/2019/Conference/-/Paper97/Public_Comment", "content": {"comment": "First of all, thank you for citing our paper on the usage of words and subwords in open-vocabulary language modeling ( https://arxiv.org/abs/1804.08205 )!\nWe are excited to see more work on the fascinating question of how to model linguistic sequences.\nAnd speaking of questions, I have a few that I would appreciate a response to:\n\n1) The abstract promises usage of the \"structure learning mechanism of Bayesian non-parametrics\", but that doesn't seem to be what is happening in the paper: the model is non-parametric in that the number of parameters is growing with the size of the training data (because \"all\" possible substrings from the training set are identified before training and used in a big lookup table), but the mechanisms of *Bayesian* non-parametrics, as they are used in previous work, leading up to the sequence memoizer (Wood et al., 2011 and earlier work) seem to be missing from this work. Is the abstract out of sync with the paper or am I missing where the promise of Bayesian non-parametrics is fulfilled?\n\n2) The lookup table is also interesting for practical reasons. Appendix B mentions that only sequences of up to some tuned maximum length and minimum frequency are kept in this table: how big then does that table become in practice and how is a softmax over all contained elements made tractable?\n\n3) Why include a model p_char in p(s)? Should the substrings not be enough once you relax the requirement of segments having at least length 2? Or does this lead to degenerate solutions in some sense?\n\n4) What makes the comparison to deterministic segmentations like BPE or sentencepiece hard or impossible? Can one tune their respective parameters to perform well as language models (as you correctly cite, we know the answer is yes) and could we maybe even achieve decent enough segmentations (maybe even outperforming the surprisal criterion or at least the surprisingly bad HMLSTM)?\nAlso, as a note: it might be worth explaining, perhaps in another appendix, just how you obtain segmentations from Elman's surprisal and the HMLSTM -- right now, section 5 is just stressing that they are prefix-only (good point!) and \"deterministic\" in some sense (though we should easily be able to \"sample\" these, too, right?).\n\n5) What necessitates the removal of spaces for the English data? In the interest of allowing comparison to previous work, why not keep them and observe whether the SNLM pastes spaces on the back or start of words? Does it do something uniform at all or does performance actually degrade?\n\n6) On your point of number of parameters, is it fair to equate the memory bank with the number of hidden units in your baseline LSTM or should it maybe rather be compared to a multi-layer LSTM (assuming that all your models are single-layer), which we know are significantly more powerful than single-layer LSTM?\n\n7) More open-ended: why is evaluating against the gold standard of your test sets meaningful? How are these gold standards defined and what makes them the \"best\" possible segmentation? Using, for example, spaces or the boundaries of existing tokenizers as gold bounds makes sense, but cross-linguistic analysis shows just how brittle and sometimes counterproductive the reliance on spaces is -- modeling morphemes may make more sense for morphologically rich languages. In fact, I am now wondering what would happen for less isolating languages, if you reproduced the samples from Table 3 (which I assume are already cherry-picked) for, say, Turkish?\n\nI understand that the ICLR layout leaves little space to answer all these questions, but I would appreciate hearing your opinion on them.", "title": "Some curious questions"}, "signatures": ["~Sebastian_J_Mielke1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper97/Reviewers/Unsubmitted"], "writers": ["~Sebastian_J_Mielke1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Word Discovery with Segmental Neural Language Models", "abstract": "We propose a segmental neural language model that combines the representational power of neural networks and the structure learning mechanism of Bayesian nonparametrics, and show that it learns to discover semantically meaningful units (e.g., morphemes and words) from unsegmented character sequences. The model generates text as a sequence of segments, where each segment is generated either character-by-character from a sequence model or as a single draw from a lexical memory that stores multi-character units. Its parameters are fit to maximize the marginal likelihood of the training data, summing over all segmentations of the input, and its hyperparameters are likewise set to optimize held-out marginal likelihood.\nTo prevent the model from overusing the lexical memory, which leads to poor generalization and bad segmentation, we introduce a differentiable regularizer that penalizes based on the expected length of each segment. To our knowledge, this is the first demonstration of neural networks that have predictive distributions better than LSTM language models and also infer a segmentation into word-like units that are competitive with the best existing word discovery models.", "keywords": [], "authorids": ["kawakamik@google.com", "cdyer@google.com", "pblunsom@google.com"], "authors": ["Kazuya Kawakami", "Chris Dyer", "Phil Blunsom"], "TL;DR": "A LSTM language model that discovers words from unsegmented sequences of characters.", "pdf": "/pdf/e28755b019f2b6f54c02f81332378de81217cadc.pdf", "paperhash": "kawakami|unsupervised_word_discovery_with_segmental_neural_language_models", "_bibtex": "@misc{\nkawakami2019unsupervised,\ntitle={Unsupervised Word Discovery with Segmental Neural Language Models},\nauthor={Kazuya Kawakami and Chris Dyer and Phil Blunsom},\nyear={2019},\nurl={https://openreview.net/forum?id=r1NDBsAqY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper97/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311919467, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "r1NDBsAqY7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper97/Authors", "ICLR.cc/2019/Conference/Paper97/Reviewers", "ICLR.cc/2019/Conference/Paper97/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper97/Authors", "ICLR.cc/2019/Conference/Paper97/Reviewers", "ICLR.cc/2019/Conference/Paper97/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311919467}}}, {"id": "S1xTLWQccQ", "original": null, "number": 2, "cdate": 1539088725060, "ddate": null, "tcdate": 1539088725060, "tmdate": 1539088725060, "tddate": null, "forum": "r1NDBsAqY7", "replyto": "r1NDBsAqY7", "invitation": "ICLR.cc/2019/Conference/-/Paper97/Public_Comment", "content": {"comment": "Another possibly relevant work that came out a couple of years ago is \"Multiscale sequence modeling with a learned dictionary\"[1]  might be of interest to you. \n\n[1] Multiscale sequence modeling with a learned dictionary, https://arxiv.org/abs/1707.00762, MLSLP 2017.", "title": "Another possibly relevant work"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper97/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Word Discovery with Segmental Neural Language Models", "abstract": "We propose a segmental neural language model that combines the representational power of neural networks and the structure learning mechanism of Bayesian nonparametrics, and show that it learns to discover semantically meaningful units (e.g., morphemes and words) from unsegmented character sequences. The model generates text as a sequence of segments, where each segment is generated either character-by-character from a sequence model or as a single draw from a lexical memory that stores multi-character units. Its parameters are fit to maximize the marginal likelihood of the training data, summing over all segmentations of the input, and its hyperparameters are likewise set to optimize held-out marginal likelihood.\nTo prevent the model from overusing the lexical memory, which leads to poor generalization and bad segmentation, we introduce a differentiable regularizer that penalizes based on the expected length of each segment. To our knowledge, this is the first demonstration of neural networks that have predictive distributions better than LSTM language models and also infer a segmentation into word-like units that are competitive with the best existing word discovery models.", "keywords": [], "authorids": ["kawakamik@google.com", "cdyer@google.com", "pblunsom@google.com"], "authors": ["Kazuya Kawakami", "Chris Dyer", "Phil Blunsom"], "TL;DR": "A LSTM language model that discovers words from unsegmented sequences of characters.", "pdf": "/pdf/e28755b019f2b6f54c02f81332378de81217cadc.pdf", "paperhash": "kawakami|unsupervised_word_discovery_with_segmental_neural_language_models", "_bibtex": "@misc{\nkawakami2019unsupervised,\ntitle={Unsupervised Word Discovery with Segmental Neural Language Models},\nauthor={Kazuya Kawakami and Chris Dyer and Phil Blunsom},\nyear={2019},\nurl={https://openreview.net/forum?id=r1NDBsAqY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper97/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311919467, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "r1NDBsAqY7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper97/Authors", "ICLR.cc/2019/Conference/Paper97/Reviewers", "ICLR.cc/2019/Conference/Paper97/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper97/Authors", "ICLR.cc/2019/Conference/Paper97/Reviewers", "ICLR.cc/2019/Conference/Paper97/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311919467}}}, {"id": "SJxE5HCFcm", "original": null, "number": 1, "cdate": 1539069324017, "ddate": null, "tcdate": 1539069324017, "tmdate": 1539069371340, "tddate": null, "forum": "r1NDBsAqY7", "replyto": "r1NDBsAqY7", "invitation": "ICLR.cc/2019/Conference/-/Paper97/Public_Comment", "content": {"comment": "You can mention [1] and its experimental results. SLM uses the same idea as SNLM and gives higher F1 (80.2) on PKU (while it has not been evaluated on Brent, PTB or CTB).\n\n[1] Unsupervised Neural Word Segmentation for Chinese via Segmental Language Modeling. EMNLP 2018. [ https://arxiv.org/abs/1810.03167 ]", "title": "You can compare Segmental Language Model (SLM) with your SNLM"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper97/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Word Discovery with Segmental Neural Language Models", "abstract": "We propose a segmental neural language model that combines the representational power of neural networks and the structure learning mechanism of Bayesian nonparametrics, and show that it learns to discover semantically meaningful units (e.g., morphemes and words) from unsegmented character sequences. The model generates text as a sequence of segments, where each segment is generated either character-by-character from a sequence model or as a single draw from a lexical memory that stores multi-character units. Its parameters are fit to maximize the marginal likelihood of the training data, summing over all segmentations of the input, and its hyperparameters are likewise set to optimize held-out marginal likelihood.\nTo prevent the model from overusing the lexical memory, which leads to poor generalization and bad segmentation, we introduce a differentiable regularizer that penalizes based on the expected length of each segment. To our knowledge, this is the first demonstration of neural networks that have predictive distributions better than LSTM language models and also infer a segmentation into word-like units that are competitive with the best existing word discovery models.", "keywords": [], "authorids": ["kawakamik@google.com", "cdyer@google.com", "pblunsom@google.com"], "authors": ["Kazuya Kawakami", "Chris Dyer", "Phil Blunsom"], "TL;DR": "A LSTM language model that discovers words from unsegmented sequences of characters.", "pdf": "/pdf/e28755b019f2b6f54c02f81332378de81217cadc.pdf", "paperhash": "kawakami|unsupervised_word_discovery_with_segmental_neural_language_models", "_bibtex": "@misc{\nkawakami2019unsupervised,\ntitle={Unsupervised Word Discovery with Segmental Neural Language Models},\nauthor={Kazuya Kawakami and Chris Dyer and Phil Blunsom},\nyear={2019},\nurl={https://openreview.net/forum?id=r1NDBsAqY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper97/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311919467, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "r1NDBsAqY7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper97/Authors", "ICLR.cc/2019/Conference/Paper97/Reviewers", "ICLR.cc/2019/Conference/Paper97/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper97/Authors", "ICLR.cc/2019/Conference/Paper97/Reviewers", "ICLR.cc/2019/Conference/Paper97/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311919467}}}], "count": 18}