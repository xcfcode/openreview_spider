{"notes": [{"id": "HkzyX3CcFQ", "original": "rygR1UpcY7", "number": 1320, "cdate": 1538087958989, "ddate": null, "tcdate": 1538087958989, "tmdate": 1545355417644, "tddate": null, "forum": "HkzyX3CcFQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Contextual Recurrent Convolutional Model for Robust Visual Learning", "abstract": "Feedforward convolutional neural network has achieved a great success in many computer vision tasks. While it validly imitates the hierarchical structure of biological visual system, it still lacks one essential architectural feature: contextual recurrent connections with feedback, which widely exists in biological visual system. In this work, we designed a Contextual Recurrent Convolutional Network with this feature embedded in a standard CNN structure. We found that such feedback connections could enable lower layers to ``rethink\" about their representations given the top-down contextual information. We carefully studied the components of this network, and showed its robustness and superiority over feedforward baselines in such tasks as noise image classification, partially occluded object recognition and fine-grained image classification. We believed this work could be an important step to help bridge the gap between computer vision models and real biological visual system.", "keywords": ["contextual modulation", "recurrent convolutional network", "robust visual learning"], "authorids": ["simingyan@pku.edu.cn", "mike.xiao@pku.edu.cn", "zym1010@gmail.com", "taislee@andrew.cmu.edu"], "authors": ["Siming Yan*", "Bowen Xiao*", "Yimeng Zhang", "Tai Sing Lee"], "TL;DR": "we proposed a novel contextual recurrent convolutional network with robust property of visual learning ", "pdf": "/pdf/9cb38f96a52d52f20bd1f4e26337716f2c1e7eb8.pdf", "paperhash": "yan|contextual_recurrent_convolutional_model_for_robust_visual_learning", "_bibtex": "@misc{\nyan*2019contextual,\ntitle={Contextual Recurrent Convolutional Model for Robust Visual Learning},\nauthor={Siming Yan* and Bowen Xiao* and Yimeng Zhang and Tai Sing Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkzyX3CcFQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "SkgzCLNiyV", "original": null, "number": 1, "cdate": 1544402634184, "ddate": null, "tcdate": 1544402634184, "tmdate": 1545354497293, "tddate": null, "forum": "HkzyX3CcFQ", "replyto": "HkzyX3CcFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1320/Meta_Review", "content": {"metareview": "This paper explores the addition of feedback connections to popular CNN architectures. All three reviewers suggest rejecting the paper, pointing to limited novelty with respect to other recent publications, and unconvincing experiments. The AC agrees with the reviewers.\n\n", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "metareview: limited novelty, unconvincing experiments"}, "signatures": ["ICLR.cc/2019/Conference/Paper1320/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1320/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Contextual Recurrent Convolutional Model for Robust Visual Learning", "abstract": "Feedforward convolutional neural network has achieved a great success in many computer vision tasks. While it validly imitates the hierarchical structure of biological visual system, it still lacks one essential architectural feature: contextual recurrent connections with feedback, which widely exists in biological visual system. In this work, we designed a Contextual Recurrent Convolutional Network with this feature embedded in a standard CNN structure. We found that such feedback connections could enable lower layers to ``rethink\" about their representations given the top-down contextual information. We carefully studied the components of this network, and showed its robustness and superiority over feedforward baselines in such tasks as noise image classification, partially occluded object recognition and fine-grained image classification. We believed this work could be an important step to help bridge the gap between computer vision models and real biological visual system.", "keywords": ["contextual modulation", "recurrent convolutional network", "robust visual learning"], "authorids": ["simingyan@pku.edu.cn", "mike.xiao@pku.edu.cn", "zym1010@gmail.com", "taislee@andrew.cmu.edu"], "authors": ["Siming Yan*", "Bowen Xiao*", "Yimeng Zhang", "Tai Sing Lee"], "TL;DR": "we proposed a novel contextual recurrent convolutional network with robust property of visual learning ", "pdf": "/pdf/9cb38f96a52d52f20bd1f4e26337716f2c1e7eb8.pdf", "paperhash": "yan|contextual_recurrent_convolutional_model_for_robust_visual_learning", "_bibtex": "@misc{\nyan*2019contextual,\ntitle={Contextual Recurrent Convolutional Model for Robust Visual Learning},\nauthor={Siming Yan* and Bowen Xiao* and Yimeng Zhang and Tai Sing Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkzyX3CcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1320/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352881593, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkzyX3CcFQ", "replyto": "HkzyX3CcFQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1320/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1320/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1320/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352881593}}}, {"id": "H1ewdKtsR7", "original": null, "number": 6, "cdate": 1543375215319, "ddate": null, "tcdate": 1543375215319, "tmdate": 1543375215319, "tddate": null, "forum": "HkzyX3CcFQ", "replyto": "HkgMSYFsCQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1320/Official_Comment", "content": {"title": "Point-by-point address to reviewer's concerns [part 2]", "comment": "6. Then robustness to noise and adversarial attacks tested on ImageNet and with a modification of the architecture. According to the caption of Fig. 4, this is done with 5 timesteps this time!\n\nWe apologize that we actually used 2 unroll times model for ImageNet classification instead of 5 unroll times. We have corrected this mistake. \n\n7. Accuracy on ImageNet needs to be reported ** especially ** if classification accuracy is not improved (as I expect). \n\nWe have reported the top-1 accuracy of ImageNet classification in Table 7 for your reference.\n\n8. Then experiments on fine-grained with ResNet-34! What architecture is this? Is this yet another number of loops and feedback iterations? When reporting that \"Our model can get a top-1 error of 25.1, while that of the ResNet-34 model is 26.5.\u201d Please provide published accuracy for the baseline algorithm. \n\nWe mentioned in the paper that \u201cNotice that our proposed models are based on VGG16 with 2 recurrent connection (loop1+loop2 in Figure 2) in all the tasks.\u201d\nAnd we have systematically compared our model against three baseline models (VGG, VGG-ATT, VGG-LR) on fine-grained image classification dataset on Table 2.\n\n9. For the experiment on occlusions, the authors report using \u201ca multi-recurrent model which is similar to the model mentioned in the Imagenet task\u201d. Sorry but this is not good enough. \n\nNow, we used the same version of our model in all tasks.\nWe have systematically compared our model against three baseline models (VGG, VGG-ATT, VGG-LR) on occlusion dataset on Table 3. \n\n10. Table 4 has literally no explanation. What is FF? What are unroll times? As a side note, VGG-GAP does not seem to be defined anywhere. \n\nWe apologize for not explaining everything clearly. FF here means feed-forward which is equal to VGG16 feed-forward model. \n\n11.When stating \"We investigated VGG16 (Simonyan & Zisserman, 2014), a standard CNN that closely approximate the ventral visual hierarchical stream, and its recurrent variants for comparison.\u201d, the authors probably meant \u201ccoarsely\u201d not \u201cclosely\".\n\nWe agree with you and have changed it to \u201ccoarsely\u201d. But VGG is probably the best among the various deep networks to the ventral stream, in terms of the gradual growth of the receptive field size, the number of stages. A number of studies showed that V1 neurons in monkeys are closest to conv2.3 in some datasets (Kohn and Schwartz dataset) and conv3.1 of VGG (Tolias dataset, and Tang dataset). \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1320/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1320/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1320/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Contextual Recurrent Convolutional Model for Robust Visual Learning", "abstract": "Feedforward convolutional neural network has achieved a great success in many computer vision tasks. While it validly imitates the hierarchical structure of biological visual system, it still lacks one essential architectural feature: contextual recurrent connections with feedback, which widely exists in biological visual system. In this work, we designed a Contextual Recurrent Convolutional Network with this feature embedded in a standard CNN structure. We found that such feedback connections could enable lower layers to ``rethink\" about their representations given the top-down contextual information. We carefully studied the components of this network, and showed its robustness and superiority over feedforward baselines in such tasks as noise image classification, partially occluded object recognition and fine-grained image classification. We believed this work could be an important step to help bridge the gap between computer vision models and real biological visual system.", "keywords": ["contextual modulation", "recurrent convolutional network", "robust visual learning"], "authorids": ["simingyan@pku.edu.cn", "mike.xiao@pku.edu.cn", "zym1010@gmail.com", "taislee@andrew.cmu.edu"], "authors": ["Siming Yan*", "Bowen Xiao*", "Yimeng Zhang", "Tai Sing Lee"], "TL;DR": "we proposed a novel contextual recurrent convolutional network with robust property of visual learning ", "pdf": "/pdf/9cb38f96a52d52f20bd1f4e26337716f2c1e7eb8.pdf", "paperhash": "yan|contextual_recurrent_convolutional_model_for_robust_visual_learning", "_bibtex": "@misc{\nyan*2019contextual,\ntitle={Contextual Recurrent Convolutional Model for Robust Visual Learning},\nauthor={Siming Yan* and Bowen Xiao* and Yimeng Zhang and Tai Sing Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkzyX3CcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1320/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606034, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkzyX3CcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1320/Authors", "ICLR.cc/2019/Conference/Paper1320/Reviewers", "ICLR.cc/2019/Conference/Paper1320/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1320/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1320/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1320/Authors|ICLR.cc/2019/Conference/Paper1320/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1320/Reviewers", "ICLR.cc/2019/Conference/Paper1320/Authors", "ICLR.cc/2019/Conference/Paper1320/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606034}}}, {"id": "HkgMSYFsCQ", "original": null, "number": 5, "cdate": 1543375162166, "ddate": null, "tcdate": 1543375162166, "tmdate": 1543375162166, "tddate": null, "forum": "HkzyX3CcFQ", "replyto": "rye4DnoY27", "invitation": "ICLR.cc/2019/Conference/-/Paper1320/Official_Comment", "content": {"title": "Point-by-point address to reviewer's concerns [part 1]", "comment": "Thank you for the valuable feedback and comments. Below we address your comments point by point.\n\n1. Architectures and hyper-parameters are casually changed from experiment to experiment (please refer to Do CIFAR-10 Classifiers Generalize to CIFAR-10? By Recht et al 2018 to understand why this is a serious problem.) \n\nTo address your concern, we now choose one model VGG models with loop1+loop2 (Figure 2) and test it against the other models in all the five tasks.\n  \n2. This study is not the first to look into recurrent / feedback processes. Indeed some (but not all) prior work is cited in the introduction. Some of these should be used as baselines as opposed to just feedforward networks. \n\nYes, now we implemented Li et al\u2019s VGG-LR (learning to rethink), and Jetley et al\u2019s VGG-ATT (attention), and test them in addition to our model and VGG in the different tasks. (see general response). We demonstrated in all these tasks, our model provides the best performance.\n\n3.Overall the improvements are relatively modest (e.g., see Fig. 4 right panel where the improvements are a fraction of a % or left panel with a couple % fooling rates improvements) \n\nWe have corrected this mistake.\nGenerally, in all tasks, our model outperformed the best of the other models by one or two percentage point. But in fine-grained recognition and noisy image recognition, our performance improvement reached 8% and 25% more than the best among the other models.\n\n4. The experiments are all over the place. What is the SOA on CIFAR-10 and CIFAR-100? If different from VGG please provide a strong rationale for testing the circuit on VGG and not SOA. In general, the experimental validation would be much stronger if consistent improvements were shown across architectures.\n\nWe now limit the comparison between one version of our model (the optimal one) against VGG and the other two VGG models with loops VGG-ATT and VGG-LR. VGG is used because it resembled the primate visual system the most in many aspects \u2013 gradual increase in receptive fields, convolution, and pooling. We consider each VGG stage, which includes multiple convolution layers followed by a pooling layer, to be equivalent to one visual area, hence, although VGG16 has 16 layers, it roughly has 6-7 stages, approximating the primate hierarchical visual system. \n\n5. Accuracy is reported for CIFAR-10 and CIFAR-100 for 1 and 2 feedback iterations and presumably with the architecture shown in Fig. 1. \n\nYes. The architecture used for CIFAR-10 and CIFAR-100 is VGG16 model with loop1+loop2, which is shown in Fig 1, Fig 2 and Fig 3. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1320/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1320/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1320/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Contextual Recurrent Convolutional Model for Robust Visual Learning", "abstract": "Feedforward convolutional neural network has achieved a great success in many computer vision tasks. While it validly imitates the hierarchical structure of biological visual system, it still lacks one essential architectural feature: contextual recurrent connections with feedback, which widely exists in biological visual system. In this work, we designed a Contextual Recurrent Convolutional Network with this feature embedded in a standard CNN structure. We found that such feedback connections could enable lower layers to ``rethink\" about their representations given the top-down contextual information. We carefully studied the components of this network, and showed its robustness and superiority over feedforward baselines in such tasks as noise image classification, partially occluded object recognition and fine-grained image classification. We believed this work could be an important step to help bridge the gap between computer vision models and real biological visual system.", "keywords": ["contextual modulation", "recurrent convolutional network", "robust visual learning"], "authorids": ["simingyan@pku.edu.cn", "mike.xiao@pku.edu.cn", "zym1010@gmail.com", "taislee@andrew.cmu.edu"], "authors": ["Siming Yan*", "Bowen Xiao*", "Yimeng Zhang", "Tai Sing Lee"], "TL;DR": "we proposed a novel contextual recurrent convolutional network with robust property of visual learning ", "pdf": "/pdf/9cb38f96a52d52f20bd1f4e26337716f2c1e7eb8.pdf", "paperhash": "yan|contextual_recurrent_convolutional_model_for_robust_visual_learning", "_bibtex": "@misc{\nyan*2019contextual,\ntitle={Contextual Recurrent Convolutional Model for Robust Visual Learning},\nauthor={Siming Yan* and Bowen Xiao* and Yimeng Zhang and Tai Sing Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkzyX3CcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1320/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606034, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkzyX3CcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1320/Authors", "ICLR.cc/2019/Conference/Paper1320/Reviewers", "ICLR.cc/2019/Conference/Paper1320/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1320/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1320/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1320/Authors|ICLR.cc/2019/Conference/Paper1320/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1320/Reviewers", "ICLR.cc/2019/Conference/Paper1320/Authors", "ICLR.cc/2019/Conference/Paper1320/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606034}}}, {"id": "BkgWrpdiR7", "original": null, "number": 4, "cdate": 1543372088921, "ddate": null, "tcdate": 1543372088921, "tmdate": 1543372088921, "tddate": null, "forum": "HkzyX3CcFQ", "replyto": "HygOWC_937", "invitation": "ICLR.cc/2019/Conference/-/Paper1320/Official_Comment", "content": {"title": "Point-by-point address to reviewer's concerns", "comment": "Thank you for the valuable feedback and comments. Below we address your comments point by point.\n\n1. -I think there are lots of related literature that shares a similar motivation to the current work. Just list a few that I know of: \nRonneberger, Olaf, Philipp Fischer, and Thomas Brox. \"U-net: Convolutional networks for biomedical image segmentation.\" International Conference on Medical image computing and computer-assisted intervention. Springer, Cham, 2015. \nLin, Tsung-Yi, et al. \"Feature Pyramid Networks for Object Detection.\" CVPR. Vol. 1. No. 2. 2017. \nNewell, Alejandro, Kaiyu Yang, and Jia Deng. \"Stacked hourglass networks for human pose estimation.\" European Conference on Computer Vision. Springer, Cham, 2016. \nYu, Fisher, et al. \"Deep layer aggregation.\" arXiv preprint arXiv:1707.06484 (2017). The current work is very similar to such works, in the sense that it tries to combine the higher-level features with the lower-level features. Compared to such works, it lacks both novelty and insights about what works and why it works.\n\n**Novelty**: Yes. Indeed, it has been long recognized that deep neural network is missing a key feature in cortical processing and there are a variety of feedback mechanisms used in the literature (1) concatenation of feedforward and feedback responses (e.g in U-net and others listed by reviewer 2); (2) unfolding feedback into a feedforward networks (U-net, autoencoder); (3) gating, or dot product (multiplication) as in Attentional network which use high-level semantic information to gate object segmentation and localization at lower layer; (4)  recurrent with gating as in LSTM and GRU, for keeping and using internal state and memory to gate sequence processing; (5) scaling, addition and subtraction typical in neuroscience. (6) Gated Boltzmann machine and transformer network. (7) capsule network. What we investigated, however, are neuroscience motivated questions: Why loops are predominantly between adjacent visual areas? What kind of contextual information would be useful and for what tasks? What is contextual modulation? These questions have not been answered by those technology-driven innovations. \nThe gating circuit we proposed are also neurally motivated, and designed to answer neuroscience question. For that reasons, we might also be the first to introduce loops between convolution layers and successfully train on real complicated image dataset(ImageNet) at the same time. \n\n**Insights**: Our work did provide a number of insights discussed in our general comments. We show loops are good, tight loops are better, more loops are better, top-down gating is important, context information should include both top-down and bottom information, and feedback improves feedforward connections. \n\n2. - The performance gain is pretty marginal, especially given that the proposed network has an iterative nature and can incur a lot of FLOPs. It would be great to show the FLOPs when comparing models to previous works. \n\nThe performance improvement is a means rather than an end to our research. We use it to gauge the importance of certain designs, to assess the importance of feedback in a certain task. So we are not particularly concerning with the number of FLOPs. Our primary motivation is for understanding the computational logic of brain structures, developing a new deep learning module for technology is secondary. We are afraid that we have not made these objectives clear and the paper was evaluated as a technology paper rather than a brain science paper. \n\nIn the earlier submission, we mainly provided results on the noisy image object recognition, though we mentioned results in adversary attacks and occlusion and fine-grained recognition, and we only compared our model against VGG. Now, we have implemented the other VGG-with-loops models such as VGG-ATT and VGG-LR and tested them as well. These models have similar FLOP to our model. We demonstrated our model is better.\n\n3.- It is interesting observation that the recurrent network has a better tolerance to noise and adversarial attacks, but I am not convinced giving the sparse set of experiments done in the paper. Overall I think the current work lacks novelty, significance and solid experiments to be accepted to ICLR.\n\nBased on your review that our performance gain is limited and the experiments are sparse, to address your concerns, we systematically compared our model against other baseline models in five different tasks (recognition in Cifar10 & 100 datasets, fine-grained image classification dataset CUB-200, occlusion dataset, and ImageNet with different level of noise, adversarial attack) and provided five additional tables (Table 1,2,3,7,8) and a figure(Figure 4) for systematic comparison. We systematically compared feedforward VGG, VGG-ATT and VGG-LR, and we demonstrated in all these tasks, our model provided the best performance, thus demonstrating the computational advantages of some of the neural constraints and design.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1320/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1320/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1320/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Contextual Recurrent Convolutional Model for Robust Visual Learning", "abstract": "Feedforward convolutional neural network has achieved a great success in many computer vision tasks. While it validly imitates the hierarchical structure of biological visual system, it still lacks one essential architectural feature: contextual recurrent connections with feedback, which widely exists in biological visual system. In this work, we designed a Contextual Recurrent Convolutional Network with this feature embedded in a standard CNN structure. We found that such feedback connections could enable lower layers to ``rethink\" about their representations given the top-down contextual information. We carefully studied the components of this network, and showed its robustness and superiority over feedforward baselines in such tasks as noise image classification, partially occluded object recognition and fine-grained image classification. We believed this work could be an important step to help bridge the gap between computer vision models and real biological visual system.", "keywords": ["contextual modulation", "recurrent convolutional network", "robust visual learning"], "authorids": ["simingyan@pku.edu.cn", "mike.xiao@pku.edu.cn", "zym1010@gmail.com", "taislee@andrew.cmu.edu"], "authors": ["Siming Yan*", "Bowen Xiao*", "Yimeng Zhang", "Tai Sing Lee"], "TL;DR": "we proposed a novel contextual recurrent convolutional network with robust property of visual learning ", "pdf": "/pdf/9cb38f96a52d52f20bd1f4e26337716f2c1e7eb8.pdf", "paperhash": "yan|contextual_recurrent_convolutional_model_for_robust_visual_learning", "_bibtex": "@misc{\nyan*2019contextual,\ntitle={Contextual Recurrent Convolutional Model for Robust Visual Learning},\nauthor={Siming Yan* and Bowen Xiao* and Yimeng Zhang and Tai Sing Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkzyX3CcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1320/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606034, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkzyX3CcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1320/Authors", "ICLR.cc/2019/Conference/Paper1320/Reviewers", "ICLR.cc/2019/Conference/Paper1320/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1320/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1320/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1320/Authors|ICLR.cc/2019/Conference/Paper1320/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1320/Reviewers", "ICLR.cc/2019/Conference/Paper1320/Authors", "ICLR.cc/2019/Conference/Paper1320/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606034}}}, {"id": "H1gYpXvjR7", "original": null, "number": 3, "cdate": 1543365568529, "ddate": null, "tcdate": 1543365568529, "tmdate": 1543365568529, "tddate": null, "forum": "HkzyX3CcFQ", "replyto": "B1lRdulhnQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1320/Official_Comment", "content": {"title": "Point-by-point address to reviewer's concerns", "comment": "Thank you for the valuable feedback and comments. Below we address your comments point by point.\n\n1.A similar idea has been explored by (Li, et al. 2018). Compared with that work, the novelty of this work is weaken and seems limited. The difference from Li is not very clear. The authors need to give more discussion. \n\nNovelty: Our proposed circuit has two novel neurally inspired novel recurrent circuit design: tight loops between adjacent visual areas, and contextual modulation. \nLi et al. proposed a recurrent module bringing semantic information from the FC layer to help a lower layer perform object detection and segmentation. However, in the visual cortex, most of the loops are between adjacent areas, between V1 and V2, between V2 and V4. Such **tight loop** has not been investigated earlier, hence the problem is **novel**.  Besides we also investigated different design of contextual modulation. (see general comments above).\n\n2.Furthermore, experimental comparison with Li et al. 2018 is also necessary. \n\nWe implemented Li\u2019s VGG-LR (Li, et al. 2018) for comparison in different tasks. Both models were unrolled 2 times during the training. In ImageNet classification task, our work shows a slightly improvement from 71.55% to 71.632% on top1 accuracy(Table 7).  Our model showed much stronger robustness against noise corruption in recognition tasks (Figure 4).  In addition, our model also outperformed VGG-LR (as well as VGG-ATT) in CIFAR 10 & 100 datasets recognition, fine-grained image classification(CUB-200) dataset as well as recognition under occlusion (Alan Yuille\u2019s dataset (Table 1, 2, 3). \n\n3. The performance gain is limited. The authors mainly evaluate their method for noisy image classification. Such application is very narrow and deviates a bit from realistic scenarios. \n\nWe now tested our model against the other models (VGG, VGG-ATT, VGG-LR) in 5 different tasks and in different datasets. We have added five additional tables and a figure (one for each task) to document the model\u2019s performance. We found our design outperformed in every task, with 25% improvement relative to the other baseline model for noisy image recognition, and 8% improvement relative to others in fine-grained recognition. Whether the gain is small or large is in the eye of the beholders, but our work demonstrated some computational benefits of recurrent connections, particularly for some relevant tasks. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1320/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1320/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1320/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Contextual Recurrent Convolutional Model for Robust Visual Learning", "abstract": "Feedforward convolutional neural network has achieved a great success in many computer vision tasks. While it validly imitates the hierarchical structure of biological visual system, it still lacks one essential architectural feature: contextual recurrent connections with feedback, which widely exists in biological visual system. In this work, we designed a Contextual Recurrent Convolutional Network with this feature embedded in a standard CNN structure. We found that such feedback connections could enable lower layers to ``rethink\" about their representations given the top-down contextual information. We carefully studied the components of this network, and showed its robustness and superiority over feedforward baselines in such tasks as noise image classification, partially occluded object recognition and fine-grained image classification. We believed this work could be an important step to help bridge the gap between computer vision models and real biological visual system.", "keywords": ["contextual modulation", "recurrent convolutional network", "robust visual learning"], "authorids": ["simingyan@pku.edu.cn", "mike.xiao@pku.edu.cn", "zym1010@gmail.com", "taislee@andrew.cmu.edu"], "authors": ["Siming Yan*", "Bowen Xiao*", "Yimeng Zhang", "Tai Sing Lee"], "TL;DR": "we proposed a novel contextual recurrent convolutional network with robust property of visual learning ", "pdf": "/pdf/9cb38f96a52d52f20bd1f4e26337716f2c1e7eb8.pdf", "paperhash": "yan|contextual_recurrent_convolutional_model_for_robust_visual_learning", "_bibtex": "@misc{\nyan*2019contextual,\ntitle={Contextual Recurrent Convolutional Model for Robust Visual Learning},\nauthor={Siming Yan* and Bowen Xiao* and Yimeng Zhang and Tai Sing Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkzyX3CcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1320/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606034, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkzyX3CcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1320/Authors", "ICLR.cc/2019/Conference/Paper1320/Reviewers", "ICLR.cc/2019/Conference/Paper1320/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1320/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1320/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1320/Authors|ICLR.cc/2019/Conference/Paper1320/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1320/Reviewers", "ICLR.cc/2019/Conference/Paper1320/Authors", "ICLR.cc/2019/Conference/Paper1320/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606034}}}, {"id": "BJeQtfvjAX", "original": null, "number": 2, "cdate": 1543365243292, "ddate": null, "tcdate": 1543365243292, "tmdate": 1543365243292, "tddate": null, "forum": "HkzyX3CcFQ", "replyto": "HkzyX3CcFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1320/Official_Comment", "content": {"title": "General Responses to the reviewers and the program committee", "comment": "We thank reviewers for their critique and valuable suggestions. The critiques of our work center on four issues: (1) *novelty* -- a number of deep networks inspired by recurrent feedback connections have already been developed such as U-net by unrolling feedback into feedforward networks with considerable performance increase, and there are also recent works of adding loops and local circuits (Li et al. 2018, Jetley et al. 2018, Nayebi et al. 2018) to improve performance; (2) *systematic comparison* is lacking across multiple tasks and against benchmark models, particularly VGG with loops; (3) *significance* -- improvement in performance seems marginal and unimpressive; (4) *insights* -- only show performance increase without explaining what work and why it works. \n\nWe agreed and disagreed but blamed ourselves for not communicating our contributions clearly. Here is our general rebuttal, followed by a point-by-point response to individual reviewers\u2019 comments.\n\n*Novelty*: \nWhile recent deep learning works have added loops to the feedforward networks (VGG-ATT (attention, Jetley), VGG-LR (learning to rethink \u2013 Li), and VGG-cortex (Nayebi)), their loops jump across many layers (many stages), typically bringing semantic information from the FC layer to help lower layers to do object detection and segmentation. In the visual cortex, most of the loops are between adjacent areas, between V1 and V2, between V2 and V4, between V4 and IT. The *computational advantages of these tight loops have never been demonstrated*. Hence, our investigation of *tight loops is novel*.  Note, we consider each visual area corresponds to a set of convolution layers ending with a pooling layer, thus loops between adjacent visual areas are modeled by loops between the last convolution layer before adjacent pooling layer. Another novel contribution is our systematic exploration of various *mechanisms of gated contextual modulation* in each loop to see what top-down and bottom-up (horizontal) contextual information are most useful, and whether gating is critical. Our module design involving top-down signals and *contextual modulation* signals (from the current layer and higher layer) with multiplicative interaction is novel. Even though concatenation, convolution and multiplication are standard operations, how to put them together is important. \n\n*Systematic evaluation*:\nIt is true that the evaluation in our original submission is a bit haphazard. Now, we reorganized our presentation and added additional comparison tests against VGG models with loops according to the reviewers\u2019 advice. We implemented VGG-ATT and VGG-LR because codes are not available but checked our implementation can reproduce their original reasons. Overall, we tested these two benchmark VGG-loop models and many versions of our VGG loop models in five tasks (ImageNet recognition (Table 7) , CIFAR10 and CIFAR100 recognition (Table 1), fine-grained recognition (Table 2), Noisy ImageNet recognition (Table 8 & Figure 4), adversary attack (Figure 4), recognition under occlusion (Table 3). In all tasks, our tight loop with top-down gating contextual modulation mechanisms have produced some improvement over VGG, VGG-ATT and VGG-LR models, but the most dramatic improvement was in fine-grained recognition (by 8%) and noisy ImageNet recognition (by 25% at high noise level). \n\n*Significance*:\nWe believe our work are significant in several aspects. The usefulness of feedback in deep learning, and particularly that of tight loops between cortical areas, has been elusive, and not demonstrated. Thus, *identifying and demonstrating what tasks* feedback and contextual modulation can be useful is important. We have demonstrated robustness against noise and fine-grained recognition benefit significantly from feedback and contextual modulation. Scientifically, we investigated the computational benefits of contextual modulation, which involve feedback and integration of horizontal information. Figuring what works best (top-down signal and top-down and horizontal context gating each other) provides potential insight not just for future module design but also to understanding neural mechanisms. Maybe the significance lies more on providing *insights in neuroscience* than some drastic improvement in technology. \n\n*Insights*:\nScientifically, our work provides insights to at least WHAT work, though we are still investigating WHY it works. Here is a list of interesting insights.\n1.\tLoops are helpful. The tighter loop is better than the wider loop in some tasks. \n2.\tMore loops work better. \n3.\tTop-down gating signal and contextual information (from top-down and bottom-up horizontal) are both important.\n4.\tUsing contextual modulation with gating work,  using contextual modulation to scale or add or subtract does not.\n5.\tHaving recurrent feedback with contextual modulation fundamentally change the feedforward representation. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1320/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1320/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1320/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Contextual Recurrent Convolutional Model for Robust Visual Learning", "abstract": "Feedforward convolutional neural network has achieved a great success in many computer vision tasks. While it validly imitates the hierarchical structure of biological visual system, it still lacks one essential architectural feature: contextual recurrent connections with feedback, which widely exists in biological visual system. In this work, we designed a Contextual Recurrent Convolutional Network with this feature embedded in a standard CNN structure. We found that such feedback connections could enable lower layers to ``rethink\" about their representations given the top-down contextual information. We carefully studied the components of this network, and showed its robustness and superiority over feedforward baselines in such tasks as noise image classification, partially occluded object recognition and fine-grained image classification. We believed this work could be an important step to help bridge the gap between computer vision models and real biological visual system.", "keywords": ["contextual modulation", "recurrent convolutional network", "robust visual learning"], "authorids": ["simingyan@pku.edu.cn", "mike.xiao@pku.edu.cn", "zym1010@gmail.com", "taislee@andrew.cmu.edu"], "authors": ["Siming Yan*", "Bowen Xiao*", "Yimeng Zhang", "Tai Sing Lee"], "TL;DR": "we proposed a novel contextual recurrent convolutional network with robust property of visual learning ", "pdf": "/pdf/9cb38f96a52d52f20bd1f4e26337716f2c1e7eb8.pdf", "paperhash": "yan|contextual_recurrent_convolutional_model_for_robust_visual_learning", "_bibtex": "@misc{\nyan*2019contextual,\ntitle={Contextual Recurrent Convolutional Model for Robust Visual Learning},\nauthor={Siming Yan* and Bowen Xiao* and Yimeng Zhang and Tai Sing Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkzyX3CcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1320/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606034, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkzyX3CcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1320/Authors", "ICLR.cc/2019/Conference/Paper1320/Reviewers", "ICLR.cc/2019/Conference/Paper1320/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1320/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1320/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1320/Authors|ICLR.cc/2019/Conference/Paper1320/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1320/Reviewers", "ICLR.cc/2019/Conference/Paper1320/Authors", "ICLR.cc/2019/Conference/Paper1320/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606034}}}, {"id": "B1lRdulhnQ", "original": null, "number": 3, "cdate": 1541306486211, "ddate": null, "tcdate": 1541306486211, "tmdate": 1541533236915, "tddate": null, "forum": "HkzyX3CcFQ", "replyto": "HkzyX3CcFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1320/Official_Review", "content": {"title": "an ok paper but not good enough", "review": "This paper introduces feedback connection to enhance feature learning through incorporating context information. \n\nA similar idea has been explored by (Li, et al. 2018). Compared with that work, the novelty of this work is weaken and seems limited. The difference from Li is not very clear. The authors need to give more discussion. Furthermore, experimental comparison with Li et al. 2018 is also necessary. \n\nThe performance gain is limited. The authors mainly evaluate their method for noisy image classification. Such application is very narrow and deviates a bit from realistic scenarios. ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1320/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Contextual Recurrent Convolutional Model for Robust Visual Learning", "abstract": "Feedforward convolutional neural network has achieved a great success in many computer vision tasks. While it validly imitates the hierarchical structure of biological visual system, it still lacks one essential architectural feature: contextual recurrent connections with feedback, which widely exists in biological visual system. In this work, we designed a Contextual Recurrent Convolutional Network with this feature embedded in a standard CNN structure. We found that such feedback connections could enable lower layers to ``rethink\" about their representations given the top-down contextual information. We carefully studied the components of this network, and showed its robustness and superiority over feedforward baselines in such tasks as noise image classification, partially occluded object recognition and fine-grained image classification. We believed this work could be an important step to help bridge the gap between computer vision models and real biological visual system.", "keywords": ["contextual modulation", "recurrent convolutional network", "robust visual learning"], "authorids": ["simingyan@pku.edu.cn", "mike.xiao@pku.edu.cn", "zym1010@gmail.com", "taislee@andrew.cmu.edu"], "authors": ["Siming Yan*", "Bowen Xiao*", "Yimeng Zhang", "Tai Sing Lee"], "TL;DR": "we proposed a novel contextual recurrent convolutional network with robust property of visual learning ", "pdf": "/pdf/9cb38f96a52d52f20bd1f4e26337716f2c1e7eb8.pdf", "paperhash": "yan|contextual_recurrent_convolutional_model_for_robust_visual_learning", "_bibtex": "@misc{\nyan*2019contextual,\ntitle={Contextual Recurrent Convolutional Model for Robust Visual Learning},\nauthor={Siming Yan* and Bowen Xiao* and Yimeng Zhang and Tai Sing Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkzyX3CcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1320/Official_Review", "cdate": 1542234255728, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkzyX3CcFQ", "replyto": "HkzyX3CcFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1320/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335921143, "tmdate": 1552335921143, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1320/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HygOWC_937", "original": null, "number": 2, "cdate": 1541209599757, "ddate": null, "tcdate": 1541209599757, "tmdate": 1541533236708, "tddate": null, "forum": "HkzyX3CcFQ", "replyto": "HkzyX3CcFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1320/Official_Review", "content": {"title": "The paper proposes to add \"recurrent\" connections inside a convolution network with gating mechanism. The idea is not novel and the performance improvement is marginal.", "review": "The paper proposes to add \"recurrent\" connections inside a convolution network with gating mechanism. The basic idea is to have higher layers to modulate the information in the lower layers in a convolution network. The way it is done is through upsampling the features from higher layers, concatenating them with lower layers and imposing a gate to control the information flow. Experiments show that the model is achieving better accuracy, especially in the case of noisy inputs or adversarial attacks. \n\n- I think there are lot of related literature that shares a similar motivation to the current work. Just list a few that I know of:\nRonneberger, Olaf, Philipp Fischer, and Thomas Brox. \"U-net: Convolutional networks for biomedical image segmentation.\" International Conference on Medical image computing and computer-assisted intervention. Springer, Cham, 2015.\nLin, Tsung-Yi, et al. \"Feature Pyramid Networks for Object Detection.\" CVPR. Vol. 1. No. 2. 2017.\nNewell, Alejandro, Kaiyu Yang, and Jia Deng. \"Stacked hourglass networks for human pose estimation.\" European Conference on Computer Vision. Springer, Cham, 2016.\nYu, Fisher, et al. \"Deep layer aggregation.\" arXiv preprint arXiv:1707.06484 (2017).\nThe current work is very similar to such works, in the sense that it tries to combine the higher-level features with the lower-level features. Compared to such works, it lacks both novelty and insights about what works and why it works.\n\n- The performance gain is pretty marginal, especially given that the proposed network has an iterative nature and can incur a lot of FLOPs. It would be great to show the FLOPs when comparing models to previous works.\n\n- It is interesting observation that the recurrent network has a better tolerance to noise and adversarial attacks, but I am not convinced giving the sparse set of experiments done in the paper.\n\nOverall I think the current work lacks novelty, significance and solid experiments to be accepted to ICLR.\n\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1320/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Contextual Recurrent Convolutional Model for Robust Visual Learning", "abstract": "Feedforward convolutional neural network has achieved a great success in many computer vision tasks. While it validly imitates the hierarchical structure of biological visual system, it still lacks one essential architectural feature: contextual recurrent connections with feedback, which widely exists in biological visual system. In this work, we designed a Contextual Recurrent Convolutional Network with this feature embedded in a standard CNN structure. We found that such feedback connections could enable lower layers to ``rethink\" about their representations given the top-down contextual information. We carefully studied the components of this network, and showed its robustness and superiority over feedforward baselines in such tasks as noise image classification, partially occluded object recognition and fine-grained image classification. We believed this work could be an important step to help bridge the gap between computer vision models and real biological visual system.", "keywords": ["contextual modulation", "recurrent convolutional network", "robust visual learning"], "authorids": ["simingyan@pku.edu.cn", "mike.xiao@pku.edu.cn", "zym1010@gmail.com", "taislee@andrew.cmu.edu"], "authors": ["Siming Yan*", "Bowen Xiao*", "Yimeng Zhang", "Tai Sing Lee"], "TL;DR": "we proposed a novel contextual recurrent convolutional network with robust property of visual learning ", "pdf": "/pdf/9cb38f96a52d52f20bd1f4e26337716f2c1e7eb8.pdf", "paperhash": "yan|contextual_recurrent_convolutional_model_for_robust_visual_learning", "_bibtex": "@misc{\nyan*2019contextual,\ntitle={Contextual Recurrent Convolutional Model for Robust Visual Learning},\nauthor={Siming Yan* and Bowen Xiao* and Yimeng Zhang and Tai Sing Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkzyX3CcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1320/Official_Review", "cdate": 1542234255728, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkzyX3CcFQ", "replyto": "HkzyX3CcFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1320/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335921143, "tmdate": 1552335921143, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1320/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rye4DnoY27", "original": null, "number": 1, "cdate": 1541155931831, "ddate": null, "tcdate": 1541155931831, "tmdate": 1541533236507, "tddate": null, "forum": "HkzyX3CcFQ", "replyto": "HkzyX3CcFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1320/Official_Review", "content": {"title": "An experimental mess", "review": "This paper presents a novel deep learning module for recurrent processes. The general idea and motivation are generally appealing but the experimental validation is a mess. Architectures and hyper-parameters are casually changed from experiment to experiment (please refer to Do CIFAR-10 Classifiers Generalize to CIFAR-10? By Recht et al 2018 to understand why this is a serious problem.) Some key evaluations are missing (see below). Key controls are also lacking. This study is not the first to look into recurrent / feedback processes. Indeed some (but not all) prior work is cited in the introduction. Some of these should be used as baselines as opposed to just feedforward networks. TBut with all that said, even addressing these concerns would not be sufficient for this paper to pass threshold since overall the improvements are relatively modest (e.g., see Fig. 4 right panel where the improvements are a fraction of a % or left panel with a couple % fooling rates improvements) for a module that adds significant computational cost to an architecture runtime. As a side note, I would advise to tone down some of the claims such as \"our network could outperform baseline feedforward networks by a large margin\u201d...\n\n****\nAdditional comments:\n\nThe experiments are all over the place. What is the SOA on CIFAR-10 and CIFAR-100? If different from VGG please provide a strong rationale for testing the circuit on VGG and not SOA. In general, the experimental validation would be much stronger if consistent improvements were shown across architectures.\n\nAccuracy is reported for CIFAR-10 and CIFAR-100 for 1 and 2 feedback iterations and presumably with the architecture shown in Fig. 1. Then robustness to noise and adversarial attacks tested on ImageNet and with a modification of the architecture. According to the caption of Fig. 4, this is done with 5 timesteps this time! Accuracy on ImageNet needs to be reported ** especially ** if classification accuracy is not improved (as I expect). \n\nThen experiments on fine-grained with ResNet-34! What architecture is this? Is this yet another number of loops and feedback iterations? When reporting that \"Our model can get a top-1 error of 25.1, while that of the ResNet-34 model is 26.5.\u201d Please provide published accuracy for the baseline algorithm.\n\nFor the experiment on occlusions, the authors report using \u201ca multi-recurrent model which is similar to the model mentioned in the Imagenet task\u201d. Sorry but this is not good enough.\n\nTable 4 has literally no explanation. What is FF? What are unroll times?\n\nAs a side note, VGG-GAP does not seem to be defined anywhere.\n\nWhen stating \"We investigated VGG16 (Simonyan & Zisserman, 2014), a standard CNN that closely approximate the ventral visual hierarchical stream, and its recurrent variants for comparison.\u201d, the authors probably meant \u201ccoarsely\u201d not \u201cclosely\".", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1320/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Contextual Recurrent Convolutional Model for Robust Visual Learning", "abstract": "Feedforward convolutional neural network has achieved a great success in many computer vision tasks. While it validly imitates the hierarchical structure of biological visual system, it still lacks one essential architectural feature: contextual recurrent connections with feedback, which widely exists in biological visual system. In this work, we designed a Contextual Recurrent Convolutional Network with this feature embedded in a standard CNN structure. We found that such feedback connections could enable lower layers to ``rethink\" about their representations given the top-down contextual information. We carefully studied the components of this network, and showed its robustness and superiority over feedforward baselines in such tasks as noise image classification, partially occluded object recognition and fine-grained image classification. We believed this work could be an important step to help bridge the gap between computer vision models and real biological visual system.", "keywords": ["contextual modulation", "recurrent convolutional network", "robust visual learning"], "authorids": ["simingyan@pku.edu.cn", "mike.xiao@pku.edu.cn", "zym1010@gmail.com", "taislee@andrew.cmu.edu"], "authors": ["Siming Yan*", "Bowen Xiao*", "Yimeng Zhang", "Tai Sing Lee"], "TL;DR": "we proposed a novel contextual recurrent convolutional network with robust property of visual learning ", "pdf": "/pdf/9cb38f96a52d52f20bd1f4e26337716f2c1e7eb8.pdf", "paperhash": "yan|contextual_recurrent_convolutional_model_for_robust_visual_learning", "_bibtex": "@misc{\nyan*2019contextual,\ntitle={Contextual Recurrent Convolutional Model for Robust Visual Learning},\nauthor={Siming Yan* and Bowen Xiao* and Yimeng Zhang and Tai Sing Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkzyX3CcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1320/Official_Review", "cdate": 1542234255728, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkzyX3CcFQ", "replyto": "HkzyX3CcFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1320/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335921143, "tmdate": 1552335921143, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1320/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 10}