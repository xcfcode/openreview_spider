{"notes": [{"id": "HkxQRTNYPH", "original": "S1xykub_Pr", "number": 850, "cdate": 1569439179135, "ddate": null, "tcdate": 1569439179135, "tmdate": 1589454741769, "tddate": null, "forum": "HkxQRTNYPH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["zhengzx.142857@gmail.com", "zhouhao.nlp@bytedance.com", "huangsj@nju.edu.cn", "lilei.02@bytedance.com", "daixinyu@nju.edu.cn", "chenjj@nju.edu.cn"], "title": "Mirror-Generative Neural Machine Translation", "authors": ["Zaixiang Zheng", "Hao Zhou", "Shujian Huang", "Lei Li", "Xin-Yu Dai", "Jiajun Chen"], "pdf": "/pdf/5c722bee553d0d9919584a7d1dab3d97d58b0b4e.pdf", "abstract": "\nTraining neural machine translation models (NMT) requires a large amount of parallel corpus, which is scarce for many language pairs. However, raw non-parallel corpora are often easy to obtain. Existing approaches have not exploited the full potential of non-parallel bilingual data either in training or decoding. In this paper, we propose the mirror-generative NMT (MGNMT), a single unified architecture that simultaneously integrates the source to target translation model, the target to source translation model, and two language models. Both translation models and language models share the same latent semantic space, therefore both translation directions can learn from non-parallel data more effectively. Besides, the translation models and language models can collaborate together during decoding. Our experiments show that the proposed MGNMT consistently outperforms existing approaches in a variety of scenarios and language pairs, including resource-rich and low-resource languages. ", "keywords": ["neural machine translation", "generative model", "mirror"], "paperhash": "zheng|mirrorgenerative_neural_machine_translation", "_bibtex": "@inproceedings{\nZheng2020Mirror-Generative,\ntitle={Mirror-Generative Neural Machine Translation},\nauthor={Zaixiang Zheng and Hao Zhou and Shujian Huang and Lei Li and Xin-Yu Dai and Jiajun Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxQRTNYPH}\n}", "original_pdf": "/attachment/05b875c12e10f36ee4ba33864926a28efe45ce3f.pdf"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "KlrEJKyoSu", "original": null, "number": 12, "cdate": 1589454609394, "ddate": null, "tcdate": 1589454609394, "tmdate": 1589454609394, "tddate": null, "forum": "HkxQRTNYPH", "replyto": "HlKMSryFEXT", "invitation": "ICLR.cc/2020/Conference/Paper850/-/Official_Comment", "content": {"title": "Response to related work", "comment": "Thanks for your kind reminder of the related work. We've updated our paper to include the discussion of the two papers.\n\nAs you mentioned, there are connections between [1] and our model. The most important one is that [1] and MGNMT jointly model both translation models and language models. However, there are still several differences:\n\n1. To model bilingual shared latent representations between source and target languages,  [1] chooses an implicit manner by sharing encoder features to act like an \"interlingual\", while MGNMT adopts an explicit manner by modeling the latent space under variational formulation.\n2. MGNMT can be trained in an end-to-end manner, while [1] follows the pipeline of three principles (initialization-language modeling-back translation) and requires incorporating an extra PBSMT.\n\nAs for distant language pairs, our main concern is that sharing vocabularies (BPEs) may make a little effect, which is one of the essential parts of [1]. We show experiments on Chinese-English in Table 4 of our paper, where GNMT-M-SSL (sharing BPE vocabulary) underperformed a simple back-translation method. This was not the case for related languages like English-German, where sharing vocabulary indeed contributed greatly to related languages. Hence, these results made us concerned about the potential issue of non-overlapping alphabets for distant languages.\n\n\nAs for [2], we compared with another similar paper [3] also using noisy channel model reranking from Facebook as well, which was shown to work very well. Please refer to the results in Table 6, where we find that MGNMT still works better. \n\n__\n\n[3]  Nathan Ng, Kyra Yee, Alexei Baevski, Myle Ott, Michael Auli, and Sergey Edunov. Facebook fairs wmt19 news translation task submission. In WMT, 2019.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper850/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper850/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhengzx.142857@gmail.com", "zhouhao.nlp@bytedance.com", "huangsj@nju.edu.cn", "lilei.02@bytedance.com", "daixinyu@nju.edu.cn", "chenjj@nju.edu.cn"], "title": "Mirror-Generative Neural Machine Translation", "authors": ["Zaixiang Zheng", "Hao Zhou", "Shujian Huang", "Lei Li", "Xin-Yu Dai", "Jiajun Chen"], "pdf": "/pdf/5c722bee553d0d9919584a7d1dab3d97d58b0b4e.pdf", "abstract": "\nTraining neural machine translation models (NMT) requires a large amount of parallel corpus, which is scarce for many language pairs. However, raw non-parallel corpora are often easy to obtain. Existing approaches have not exploited the full potential of non-parallel bilingual data either in training or decoding. In this paper, we propose the mirror-generative NMT (MGNMT), a single unified architecture that simultaneously integrates the source to target translation model, the target to source translation model, and two language models. Both translation models and language models share the same latent semantic space, therefore both translation directions can learn from non-parallel data more effectively. Besides, the translation models and language models can collaborate together during decoding. Our experiments show that the proposed MGNMT consistently outperforms existing approaches in a variety of scenarios and language pairs, including resource-rich and low-resource languages. ", "keywords": ["neural machine translation", "generative model", "mirror"], "paperhash": "zheng|mirrorgenerative_neural_machine_translation", "_bibtex": "@inproceedings{\nZheng2020Mirror-Generative,\ntitle={Mirror-Generative Neural Machine Translation},\nauthor={Zaixiang Zheng and Hao Zhou and Shujian Huang and Lei Li and Xin-Yu Dai and Jiajun Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxQRTNYPH}\n}", "original_pdf": "/attachment/05b875c12e10f36ee4ba33864926a28efe45ce3f.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxQRTNYPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper850/Authors", "ICLR.cc/2020/Conference/Paper850/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper850/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper850/Reviewers", "ICLR.cc/2020/Conference/Paper850/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper850/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper850/Authors|ICLR.cc/2020/Conference/Paper850/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504165250, "tmdate": 1576860530726, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper850/Authors", "ICLR.cc/2020/Conference/Paper850/Reviewers", "ICLR.cc/2020/Conference/Paper850/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper850/-/Official_Comment"}}}, {"id": "HlKMSryFEXT", "original": null, "number": 1, "cdate": 1588386336170, "ddate": null, "tcdate": 1588386336170, "tmdate": 1588386336170, "tddate": null, "forum": "HkxQRTNYPH", "replyto": "HkxQRTNYPH", "invitation": "ICLR.cc/2020/Conference/Paper850/-/Public_Comment", "content": {"title": "comparison to related work", "comment": "Dear Authors,\n could you please elaborate more on the relation of your work to these two prior works:\n\n[1] G. Lample, M. Ott, A. Conneau, L. Denoyer, M. Ranzato \" Phrase-Based & Neural Unsupervised Machine Translation\". EMNLP 2018\n\nand\n\n[2] P.J. Chen, J. Shen, M. Le, V. Chaudhary, A. El-Kishky, G. Wenzek, M. Ott, M. Ranzato \" Facebook AI's WAT19 Myanmar-English Translation Task Submission \".  Workshop on Asian Translation at EMNLP 2019\n\nIn particular, [1] trained jointly using p(y|x), p(x|y) and p(y) and p(x) (these last two terms in the form of a denoising auto-encoder). This seems similar to your method except for the particular choice of models and the variational formulation (which is anyway quite tricky to get to work in these applications as you stated).\n\n[2] uses both parallel and monolingual data and decodes off-line with noisy channel decoding (so in this case the language model is only used at decoding time).\n\nYour only comment to an earlier version of [1] was \" However, they may still fail to apply to distant language pairs\" but [1] was shown to work to some extent to distant languages like En-Ur.\n\nThanks!"}, "signatures": ["~MarcAurelio_Ranzato1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~MarcAurelio_Ranzato1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhengzx.142857@gmail.com", "zhouhao.nlp@bytedance.com", "huangsj@nju.edu.cn", "lilei.02@bytedance.com", "daixinyu@nju.edu.cn", "chenjj@nju.edu.cn"], "title": "Mirror-Generative Neural Machine Translation", "authors": ["Zaixiang Zheng", "Hao Zhou", "Shujian Huang", "Lei Li", "Xin-Yu Dai", "Jiajun Chen"], "pdf": "/pdf/5c722bee553d0d9919584a7d1dab3d97d58b0b4e.pdf", "abstract": "\nTraining neural machine translation models (NMT) requires a large amount of parallel corpus, which is scarce for many language pairs. However, raw non-parallel corpora are often easy to obtain. Existing approaches have not exploited the full potential of non-parallel bilingual data either in training or decoding. In this paper, we propose the mirror-generative NMT (MGNMT), a single unified architecture that simultaneously integrates the source to target translation model, the target to source translation model, and two language models. Both translation models and language models share the same latent semantic space, therefore both translation directions can learn from non-parallel data more effectively. Besides, the translation models and language models can collaborate together during decoding. Our experiments show that the proposed MGNMT consistently outperforms existing approaches in a variety of scenarios and language pairs, including resource-rich and low-resource languages. ", "keywords": ["neural machine translation", "generative model", "mirror"], "paperhash": "zheng|mirrorgenerative_neural_machine_translation", "_bibtex": "@inproceedings{\nZheng2020Mirror-Generative,\ntitle={Mirror-Generative Neural Machine Translation},\nauthor={Zaixiang Zheng and Hao Zhou and Shujian Huang and Lei Li and Xin-Yu Dai and Jiajun Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxQRTNYPH}\n}", "original_pdf": "/attachment/05b875c12e10f36ee4ba33864926a28efe45ce3f.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxQRTNYPH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504203278, "tmdate": 1576860564426, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper850/Authors", "ICLR.cc/2020/Conference/Paper850/Reviewers", "ICLR.cc/2020/Conference/Paper850/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper850/-/Public_Comment"}}}, {"id": "HmFOnYDNdx", "original": null, "number": 1, "cdate": 1576798707759, "ddate": null, "tcdate": 1576798707759, "tmdate": 1576800928581, "tddate": null, "forum": "HkxQRTNYPH", "replyto": "HkxQRTNYPH", "invitation": "ICLR.cc/2020/Conference/Paper850/-/Decision", "content": {"decision": "Accept (Talk)", "comment": "This paper proposes a novel method for considering translations in both directions within the framework of generative neural machine translation, significantly improving accuracy.\n\nAll three reviewers appreciated the paper, although they noted that the gains were somewhat small for the increased complexity of the model. Nonetheless, the baselines presented are already quite competitive, so improvements on these datasets are likely to never be extremely large.\n\nOverall, I found this to be a quite nice paper, and strongly recommend acceptance, perhaps as an oral presentation.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhengzx.142857@gmail.com", "zhouhao.nlp@bytedance.com", "huangsj@nju.edu.cn", "lilei.02@bytedance.com", "daixinyu@nju.edu.cn", "chenjj@nju.edu.cn"], "title": "Mirror-Generative Neural Machine Translation", "authors": ["Zaixiang Zheng", "Hao Zhou", "Shujian Huang", "Lei Li", "Xin-Yu Dai", "Jiajun Chen"], "pdf": "/pdf/5c722bee553d0d9919584a7d1dab3d97d58b0b4e.pdf", "abstract": "\nTraining neural machine translation models (NMT) requires a large amount of parallel corpus, which is scarce for many language pairs. However, raw non-parallel corpora are often easy to obtain. Existing approaches have not exploited the full potential of non-parallel bilingual data either in training or decoding. In this paper, we propose the mirror-generative NMT (MGNMT), a single unified architecture that simultaneously integrates the source to target translation model, the target to source translation model, and two language models. Both translation models and language models share the same latent semantic space, therefore both translation directions can learn from non-parallel data more effectively. Besides, the translation models and language models can collaborate together during decoding. Our experiments show that the proposed MGNMT consistently outperforms existing approaches in a variety of scenarios and language pairs, including resource-rich and low-resource languages. ", "keywords": ["neural machine translation", "generative model", "mirror"], "paperhash": "zheng|mirrorgenerative_neural_machine_translation", "_bibtex": "@inproceedings{\nZheng2020Mirror-Generative,\ntitle={Mirror-Generative Neural Machine Translation},\nauthor={Zaixiang Zheng and Hao Zhou and Shujian Huang and Lei Li and Xin-Yu Dai and Jiajun Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxQRTNYPH}\n}", "original_pdf": "/attachment/05b875c12e10f36ee4ba33864926a28efe45ce3f.pdf"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HkxQRTNYPH", "replyto": "HkxQRTNYPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795705978, "tmdate": 1576800253889, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper850/-/Decision"}}}, {"id": "rkeLk2RpFB", "original": null, "number": 1, "cdate": 1571838942417, "ddate": null, "tcdate": 1571838942417, "tmdate": 1574093725762, "tddate": null, "forum": "HkxQRTNYPH", "replyto": "HkxQRTNYPH", "invitation": "ICLR.cc/2020/Conference/Paper850/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "This paper proposes an approach to neural MT in which the joint (source, target) distribution is modeled as an average over two different factorizations: target given source and source given target. This gives rise to four distributions - two language models and two translation models - which are parameterized separately but conditioned on a common variational latent variable.  The model is trained on parallel data using a standard VAE approach. It can additionally be trained on non-parallel data in an approach similar to iterated back-translation at sentence-level granularity, but with language and translation model probabilities for observed sentences coupled by the latent variable. Inference iterates between sampling a latent variable given the current best hypothesis, and using beam search plus rescoring to find a new best hypothesis given the current latent variable. The approach is evaluated in several different scenarios (low- and high-resource, domain adaptation - trained on parallel data only or parallel plus monolingual data) and found to generally outperform previous work on generative NMT and iterated back-translation.\n\nStrengths: clearly written, well motivated, very comprehensive experiments comparing to relevant baselines.\n\nWeaknesses: somewhat incremental relative to Shah and Barber (Neurips 2018), results are only marginally positive, framework is probably too cumbersome to justify widespread adoption based on the results.\n\nI think the paper should be accepted. Although it\u2019s not highly original, it ties together three strands of work in a principled way: joint models, variational approaches, and back-translation / dual learning. The increment over Shah and Barber is bolstered by the addition of back-translation, which gives substantial improvements when using non-parallel data; and to a lesser extent by the argument about the advantage of separate models for distant language pairs. Using all possible LMs and TMs coupled with a latent variable feels like an area that was inevitably going to get explored, and this paper does a good job of it. Although the gains over the baselines are not overly compelling, they are quite systematic, indicating that the advantage is probably real, albeit slight. The authors are also to be commended on their use of not just the Shah and Barber baseline, but also the back-translation-based techniques, which are generally stronger competitors when monolingual data is incorporated.\n\nFurther comments/questions:\n\nWhy are there no results for Transformer+Dual in table 4? This omission looks odd, since Transformer+Dual was the strongest baseline in table 3.\n\nPlease add implementation details for the Transformer+{BT,JBT,Dual} baselines.\n\nIt was surprising not to see robustness experiments like Shah and Barber\u2019s dropped source words, since robustness to source noise could be one of the advantages of having an explicit model of the source sentence.\n\nA few additional suggestions for related work: noisy channel approaches (eg, The Neural Noisy Channel, Yu et al, ICLR 2017); decipherment (eg, Beyond parallel data: Joint word alignment and decipherment improves machine translation, EMNLP 2014 - yes, from SMT days, but still); other joint modeling work (KERMIT: Generative Insertion-Based Modeling for Sequences, Chan et al, 2019).\n\nConsider dropping the \u201c1+1 > 2\u201d metaphor. It\u2019s not clear to me exactly what it means, or what it adds to the paper.\n\n\u201cdeviation\u201d is used in a couple places where you probably meant \u201cderivation\u201d?\n\nLine 6 in algorithm 2 should use both forward and backward scores for rescoring.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper850/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper850/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhengzx.142857@gmail.com", "zhouhao.nlp@bytedance.com", "huangsj@nju.edu.cn", "lilei.02@bytedance.com", "daixinyu@nju.edu.cn", "chenjj@nju.edu.cn"], "title": "Mirror-Generative Neural Machine Translation", "authors": ["Zaixiang Zheng", "Hao Zhou", "Shujian Huang", "Lei Li", "Xin-Yu Dai", "Jiajun Chen"], "pdf": "/pdf/5c722bee553d0d9919584a7d1dab3d97d58b0b4e.pdf", "abstract": "\nTraining neural machine translation models (NMT) requires a large amount of parallel corpus, which is scarce for many language pairs. However, raw non-parallel corpora are often easy to obtain. Existing approaches have not exploited the full potential of non-parallel bilingual data either in training or decoding. In this paper, we propose the mirror-generative NMT (MGNMT), a single unified architecture that simultaneously integrates the source to target translation model, the target to source translation model, and two language models. Both translation models and language models share the same latent semantic space, therefore both translation directions can learn from non-parallel data more effectively. Besides, the translation models and language models can collaborate together during decoding. Our experiments show that the proposed MGNMT consistently outperforms existing approaches in a variety of scenarios and language pairs, including resource-rich and low-resource languages. ", "keywords": ["neural machine translation", "generative model", "mirror"], "paperhash": "zheng|mirrorgenerative_neural_machine_translation", "_bibtex": "@inproceedings{\nZheng2020Mirror-Generative,\ntitle={Mirror-Generative Neural Machine Translation},\nauthor={Zaixiang Zheng and Hao Zhou and Shujian Huang and Lei Li and Xin-Yu Dai and Jiajun Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxQRTNYPH}\n}", "original_pdf": "/attachment/05b875c12e10f36ee4ba33864926a28efe45ce3f.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkxQRTNYPH", "replyto": "HkxQRTNYPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper850/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper850/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574722376000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper850/Reviewers"], "noninvitees": [], "tcdate": 1570237746081, "tmdate": 1574723093959, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper850/-/Official_Review"}}}, {"id": "SyeFqSD2sH", "original": null, "number": 9, "cdate": 1573840273220, "ddate": null, "tcdate": 1573840273220, "tmdate": 1573840273220, "tddate": null, "forum": "HkxQRTNYPH", "replyto": "HJlMuQwnsH", "invitation": "ICLR.cc/2020/Conference/Paper850/-/Official_Comment", "content": {"title": "Thank you for your important suggestions!", "comment": "Your comments are really helpful. Thank you again."}, "signatures": ["ICLR.cc/2020/Conference/Paper850/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper850/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhengzx.142857@gmail.com", "zhouhao.nlp@bytedance.com", "huangsj@nju.edu.cn", "lilei.02@bytedance.com", "daixinyu@nju.edu.cn", "chenjj@nju.edu.cn"], "title": "Mirror-Generative Neural Machine Translation", "authors": ["Zaixiang Zheng", "Hao Zhou", "Shujian Huang", "Lei Li", "Xin-Yu Dai", "Jiajun Chen"], "pdf": "/pdf/5c722bee553d0d9919584a7d1dab3d97d58b0b4e.pdf", "abstract": "\nTraining neural machine translation models (NMT) requires a large amount of parallel corpus, which is scarce for many language pairs. However, raw non-parallel corpora are often easy to obtain. Existing approaches have not exploited the full potential of non-parallel bilingual data either in training or decoding. In this paper, we propose the mirror-generative NMT (MGNMT), a single unified architecture that simultaneously integrates the source to target translation model, the target to source translation model, and two language models. Both translation models and language models share the same latent semantic space, therefore both translation directions can learn from non-parallel data more effectively. Besides, the translation models and language models can collaborate together during decoding. Our experiments show that the proposed MGNMT consistently outperforms existing approaches in a variety of scenarios and language pairs, including resource-rich and low-resource languages. ", "keywords": ["neural machine translation", "generative model", "mirror"], "paperhash": "zheng|mirrorgenerative_neural_machine_translation", "_bibtex": "@inproceedings{\nZheng2020Mirror-Generative,\ntitle={Mirror-Generative Neural Machine Translation},\nauthor={Zaixiang Zheng and Hao Zhou and Shujian Huang and Lei Li and Xin-Yu Dai and Jiajun Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxQRTNYPH}\n}", "original_pdf": "/attachment/05b875c12e10f36ee4ba33864926a28efe45ce3f.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxQRTNYPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper850/Authors", "ICLR.cc/2020/Conference/Paper850/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper850/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper850/Reviewers", "ICLR.cc/2020/Conference/Paper850/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper850/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper850/Authors|ICLR.cc/2020/Conference/Paper850/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504165250, "tmdate": 1576860530726, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper850/Authors", "ICLR.cc/2020/Conference/Paper850/Reviewers", "ICLR.cc/2020/Conference/Paper850/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper850/-/Official_Comment"}}}, {"id": "HJlMuQwnsH", "original": null, "number": 8, "cdate": 1573839722144, "ddate": null, "tcdate": 1573839722144, "tmdate": 1573839722144, "tddate": null, "forum": "HkxQRTNYPH", "replyto": "r1eqJfw2sr", "invitation": "ICLR.cc/2020/Conference/Paper850/-/Official_Comment", "content": {"title": "Thank you for the clarification!", "comment": "These are very nice updates, thank you. I feel the paper will be significantly stronger and more complete after it is modified to reflect these additional theoretical and empirical comparisons."}, "signatures": ["ICLR.cc/2020/Conference/Paper850/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper850/Area_Chair1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhengzx.142857@gmail.com", "zhouhao.nlp@bytedance.com", "huangsj@nju.edu.cn", "lilei.02@bytedance.com", "daixinyu@nju.edu.cn", "chenjj@nju.edu.cn"], "title": "Mirror-Generative Neural Machine Translation", "authors": ["Zaixiang Zheng", "Hao Zhou", "Shujian Huang", "Lei Li", "Xin-Yu Dai", "Jiajun Chen"], "pdf": "/pdf/5c722bee553d0d9919584a7d1dab3d97d58b0b4e.pdf", "abstract": "\nTraining neural machine translation models (NMT) requires a large amount of parallel corpus, which is scarce for many language pairs. However, raw non-parallel corpora are often easy to obtain. Existing approaches have not exploited the full potential of non-parallel bilingual data either in training or decoding. In this paper, we propose the mirror-generative NMT (MGNMT), a single unified architecture that simultaneously integrates the source to target translation model, the target to source translation model, and two language models. Both translation models and language models share the same latent semantic space, therefore both translation directions can learn from non-parallel data more effectively. Besides, the translation models and language models can collaborate together during decoding. Our experiments show that the proposed MGNMT consistently outperforms existing approaches in a variety of scenarios and language pairs, including resource-rich and low-resource languages. ", "keywords": ["neural machine translation", "generative model", "mirror"], "paperhash": "zheng|mirrorgenerative_neural_machine_translation", "_bibtex": "@inproceedings{\nZheng2020Mirror-Generative,\ntitle={Mirror-Generative Neural Machine Translation},\nauthor={Zaixiang Zheng and Hao Zhou and Shujian Huang and Lei Li and Xin-Yu Dai and Jiajun Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxQRTNYPH}\n}", "original_pdf": "/attachment/05b875c12e10f36ee4ba33864926a28efe45ce3f.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxQRTNYPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper850/Authors", "ICLR.cc/2020/Conference/Paper850/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper850/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper850/Reviewers", "ICLR.cc/2020/Conference/Paper850/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper850/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper850/Authors|ICLR.cc/2020/Conference/Paper850/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504165250, "tmdate": 1576860530726, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper850/Authors", "ICLR.cc/2020/Conference/Paper850/Reviewers", "ICLR.cc/2020/Conference/Paper850/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper850/-/Official_Comment"}}}, {"id": "r1eqJfw2sr", "original": null, "number": 7, "cdate": 1573839329813, "ddate": null, "tcdate": 1573839329813, "tmdate": 1573839413811, "tddate": null, "forum": "HkxQRTNYPH", "replyto": "B1lUjQp_9r", "invitation": "ICLR.cc/2020/Conference/Paper850/-/Official_Comment", "content": {"title": "Response to Area Chair 1", "comment": "Thank you very much for your comments! \n\nQ1: Comparison with Cheng et al. (2016) and Tu et al. (2017) \n* Correction: Sorry we incorrectly cited the Cheng et al. (2017) in our paper, which is supposed to be Cheng et al (2016) instead. \n\nCheng et al (2016) exploit parallel corpus, and source and target monolingual corpora to jointly learn forward and backward TMs, which is equivalent to dual learning with a subtle difference (no LM reward). Given the empirical comparison that our method outperforms dual learning, our method should also be more effective than Cheng et al.\n\nDifferently, Tu et al. (2017) propose to use reconstruction regularization to improve forward TM, where they do not have a backward TM. They also show that reconstruction regularization is helpful to rerank the translation candidates. Our proposed MGNMT jointly models 4 different models, i.e., bidirectional TMs and LMs, which is very different from Tu et al. and can naturally take advantage of reranking by reconstructive score.\n\n\nQ2. Comparison with other methods incorporating backward TM at test time (Yu et al. 2016a, Hoang et al. 2017, and Ng et al. 2019)\nA2: Thanks. We will add comparisons with them in the revised draft. \n\nThe similarity with these works: leveraging bidirectional TMs to improve decoding at testing time. \n        - For Yu et al. (2016a), they propose a Neural Noisy Channel (NNC) model that leverages forward TM (direct model, $p(y|x)$), backward TM (channel model, $p(x|y)$, relying on a more sophisticated Segment-to-Segment Transduction model (Yu et al. 2016b)) and language model ($p(y)$) to improve the faithfulness of the output to the input, which also requires a more complicated Viterbi-based search algorithm for decoding. \n        - As for Hoang et al. (2017), they propose to treat decoding as a continuous optimization problem and use gradient-based methods (Exponentiated Gradient or SGD) to maximize arbitrary local/global objectives, e.g., a combination of forward and backward TMs scores. \n        - Simpler than the above both works, Ng et al. (2019) propose Noisy Channel Model Reranking, where an independently trained backward TM ($p(x|y)$) to rerank the N-best list of the translations of the forward TM, i.e., the reranking score function is: $p_(y|x) + \\lambda_1 * p(x|y) + \\lambda_2 * p(y)$.\n \nDifferences with these related work: the unified probabilistic modeling of MGNMT, which benefits both training and testing. \n        - That is to say, we have four unified and coupled models, which are jointly learned with the same latent space. As a result, this could make all these models cooperate well for decoding at testing time. Another advantage is that we can achieve iterative boosting, where the reranking in Ng et al. (2019) can be seen as a part of one iteration of our proposed decoding method. In our iterative decoding framework, the translations reranked by backward TM at the current iteration can help forward TM to generate better translations at the next iterations.\n\n\nQ3: Empirical comparisons with simple baselines  as suggested\nA3: As you suggested, we compare MGNMT with the Noisy Channel Model Reranking (NCMR) method in Ng et al. (2019) where, the reranking score function become: $p_(y|x) + \\lambda_1 * p(x|y) + \\lambda_2 * p(y)$, where $\\lambda_1=1$ and $\\lambda_2 = 0.3$ (similar to our decoding setting). We conduct comparisons on IWSLT task. The results are shown as follow, which have also been added to the revised draft:\n\n------------------------------------------------------------------------------------------\nModel                                                          EN-DE             DE-EN \n------------------------------------------------------------------------------------------\nTransformer+ BT w/ NCMR (w/o)          21.8 (20.9)    25.1 (24.3)\nGNMT-M-SSL w/ NCMR (w/o)                 22.4 (22.0)    25.6 (24.9)\nMGNMT (Transformer)                            22.8               26.1\n------------------------------------------------------------------------------------------\n\nWe find that the reranking method in Ng et al. (2019) is indeed effective and easy-to-use. But MGNMT still works better. This can be because that the advantage of the unified probabilistic modeling in MGNMT not only improves the effectiveness and efficiency of exploiting non-parallel data for training, but also enables the use of the highly-coupled language models and bidirectional translation models at testing time. The idea of MGNMT may also be useful to improve the multi-lingual translation.\n\n* Cheng, et al. \"Semi-Supervised Learning for Neural Machine Translation.\" ACL, 2016.\n* Cheng, et al. \"Joint Training for Pivot-based Neural Machine Translation.\" IJCAI, 2017.\n* Yu, et al. \"The neural noisy channel.\" arXiv preprint arXiv:1611.02554. 2016a.\n* Yu, et al. \"Online segment to segment neural transduction\". EMNLP. 2016b.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper850/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper850/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhengzx.142857@gmail.com", "zhouhao.nlp@bytedance.com", "huangsj@nju.edu.cn", "lilei.02@bytedance.com", "daixinyu@nju.edu.cn", "chenjj@nju.edu.cn"], "title": "Mirror-Generative Neural Machine Translation", "authors": ["Zaixiang Zheng", "Hao Zhou", "Shujian Huang", "Lei Li", "Xin-Yu Dai", "Jiajun Chen"], "pdf": "/pdf/5c722bee553d0d9919584a7d1dab3d97d58b0b4e.pdf", "abstract": "\nTraining neural machine translation models (NMT) requires a large amount of parallel corpus, which is scarce for many language pairs. However, raw non-parallel corpora are often easy to obtain. Existing approaches have not exploited the full potential of non-parallel bilingual data either in training or decoding. In this paper, we propose the mirror-generative NMT (MGNMT), a single unified architecture that simultaneously integrates the source to target translation model, the target to source translation model, and two language models. Both translation models and language models share the same latent semantic space, therefore both translation directions can learn from non-parallel data more effectively. Besides, the translation models and language models can collaborate together during decoding. Our experiments show that the proposed MGNMT consistently outperforms existing approaches in a variety of scenarios and language pairs, including resource-rich and low-resource languages. ", "keywords": ["neural machine translation", "generative model", "mirror"], "paperhash": "zheng|mirrorgenerative_neural_machine_translation", "_bibtex": "@inproceedings{\nZheng2020Mirror-Generative,\ntitle={Mirror-Generative Neural Machine Translation},\nauthor={Zaixiang Zheng and Hao Zhou and Shujian Huang and Lei Li and Xin-Yu Dai and Jiajun Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxQRTNYPH}\n}", "original_pdf": "/attachment/05b875c12e10f36ee4ba33864926a28efe45ce3f.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxQRTNYPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper850/Authors", "ICLR.cc/2020/Conference/Paper850/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper850/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper850/Reviewers", "ICLR.cc/2020/Conference/Paper850/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper850/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper850/Authors|ICLR.cc/2020/Conference/Paper850/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504165250, "tmdate": 1576860530726, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper850/Authors", "ICLR.cc/2020/Conference/Paper850/Reviewers", "ICLR.cc/2020/Conference/Paper850/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper850/-/Official_Comment"}}}, {"id": "BkgmuTUnjB", "original": null, "number": 6, "cdate": 1573838186599, "ddate": null, "tcdate": 1573838186599, "tmdate": 1573838186599, "tddate": null, "forum": "HkxQRTNYPH", "replyto": "rkeLk2RpFB", "invitation": "ICLR.cc/2020/Conference/Paper850/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "Thanks very much for your valuable comments!\n\nQ1: About missing Transformer+Dual in table 4\nA1: Thanks for your kind notes. We have RNMT+Dual in similar experiments in the Appendix showing that MGNMT (RNMT-based) outperforms RNMT+Dual in WMT En-De and NIST Zh-En. We provide results of Transformer+Dual here, which has also included in the updated paper draft:\n\n ----------------------------------------------------------------------------------\n Model                                            EN-DE  DE-EN  EN-ZH  ZH-EN \n ----------------------------------------------------------------------------------\n Transformer+Dual +NP             29.6      33.2      42.13    48.60 \n MGNMT (Transformer) + NP    30.3      33.8      42.56    49.05 \n ----------------------------------------------------------------------------------\n* NP: non-parallel\n      \nAs shown in the results, MGNMT is consistently better than dual learning on these high resource tasks.\n\nQ2: Experiments on noisy source sentence\nA2: Thank you so much for your nice suggestion! We conduct experiments on noisy source sentence to investigate the robustness of our models compared with GNMT. The experimental setting is similar to Shah & Barber's: each word of the source sentence has a 30% chance of being missing. \n\nGiven the noisy source sentence, MGNMT first samples a draft target translation as usual. And then,\n1) we compute the latent variable z given the current (noisy) source and target sentences; \n2) remember that MGNMT is symmetric, we can easily use TM(Y->X|Z) and LM(X|Z) to find a corrected source sentence (run line 5, Alg. 2). \n3) given the better-corrected source sentence, we can find a better target translation (run line 4-6, Alg.2). \n4) repeat 1) - 3) until converge.\n\nWe conduct experiments on WMT En-De. The results are as follow:\n\n------------------------------------------------------------------------------------------------\nModel                                EN-DE   DE-EN   EN-DE (noisy)   DE-EN (noisy) \n------------------------------------------------------------------------------------------------\nGNMT (Transformer)        27.5      31.1                19.4           23.0            \nMGNMT (Transformer)    27.7      31.4                20.3            24.1          \n------------------------------------------------------------------------------------------------      \n\nAs shown in the above table, our model is more robust than GNMT with noisy source input. This may be attributed to the unified probabilistic modeling of TMs and LMs in MGNMT, where the backward translation and language models are naturally and directly leveraged to better \"denoise\" the noisy source input. Nevertheless, the missing content in the noisy source input is still very hard to recover, leading to a large drop to all methods. Dealing with noisy input is interesting and we will leave it for future study.\n\nQ3: About missing references\nA3: Thanks. We will add content to discuss these related work in the next version.\n\nQ4: About line 6 in Alg 2\nA4: Yes, it should be both forward and backward scores and we actually implemented this exactly as you suggested. We will fix it in the next version.\n\nQ5: About implementation details for the Transformer+{BT,JBT,Dual} \nA5: We have added the implementation details for them in the revised draft."}, "signatures": ["ICLR.cc/2020/Conference/Paper850/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper850/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhengzx.142857@gmail.com", "zhouhao.nlp@bytedance.com", "huangsj@nju.edu.cn", "lilei.02@bytedance.com", "daixinyu@nju.edu.cn", "chenjj@nju.edu.cn"], "title": "Mirror-Generative Neural Machine Translation", "authors": ["Zaixiang Zheng", "Hao Zhou", "Shujian Huang", "Lei Li", "Xin-Yu Dai", "Jiajun Chen"], "pdf": "/pdf/5c722bee553d0d9919584a7d1dab3d97d58b0b4e.pdf", "abstract": "\nTraining neural machine translation models (NMT) requires a large amount of parallel corpus, which is scarce for many language pairs. However, raw non-parallel corpora are often easy to obtain. Existing approaches have not exploited the full potential of non-parallel bilingual data either in training or decoding. In this paper, we propose the mirror-generative NMT (MGNMT), a single unified architecture that simultaneously integrates the source to target translation model, the target to source translation model, and two language models. Both translation models and language models share the same latent semantic space, therefore both translation directions can learn from non-parallel data more effectively. Besides, the translation models and language models can collaborate together during decoding. Our experiments show that the proposed MGNMT consistently outperforms existing approaches in a variety of scenarios and language pairs, including resource-rich and low-resource languages. ", "keywords": ["neural machine translation", "generative model", "mirror"], "paperhash": "zheng|mirrorgenerative_neural_machine_translation", "_bibtex": "@inproceedings{\nZheng2020Mirror-Generative,\ntitle={Mirror-Generative Neural Machine Translation},\nauthor={Zaixiang Zheng and Hao Zhou and Shujian Huang and Lei Li and Xin-Yu Dai and Jiajun Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxQRTNYPH}\n}", "original_pdf": "/attachment/05b875c12e10f36ee4ba33864926a28efe45ce3f.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxQRTNYPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper850/Authors", "ICLR.cc/2020/Conference/Paper850/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper850/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper850/Reviewers", "ICLR.cc/2020/Conference/Paper850/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper850/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper850/Authors|ICLR.cc/2020/Conference/Paper850/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504165250, "tmdate": 1576860530726, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper850/Authors", "ICLR.cc/2020/Conference/Paper850/Reviewers", "ICLR.cc/2020/Conference/Paper850/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper850/-/Official_Comment"}}}, {"id": "HJgU75IhjB", "original": null, "number": 5, "cdate": 1573837341890, "ddate": null, "tcdate": 1573837341890, "tmdate": 1573837341890, "tddate": null, "forum": "HkxQRTNYPH", "replyto": "H1ei7P7g9H", "invitation": "ICLR.cc/2020/Conference/Paper850/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "Thanks very much for your insightful comments! \n\nQ1. About training and decoding costs of MGNMT\nA1: Yes, MGNMT introduces extra costs for training and decoding compared to Transformer baseline.  When being trained on parallel data, our model only slightly increases the training cost. However, the training cost regarding non-parallel training is larger than vanilla Transformer because of the on-fly sampling of pseudo-translation pairs, which is also the cost for JBT and dual learning. We list the training time of each model on IWSLT task as follow, which has also been added in the revised paper draft:\n\n--------------------------------------------------------------------------------------------------------------------      \n      Model              training time to converge (hrs, on a single 1080ti)       decoding\n--------------------------------------------------------------------------------------------------------------------\nTransformer                                  ~17h                                                                1x\nTransformer+BT +NP                   ~25h                                                                1x\nTransformer+JBT +NP                  ~34h                                                                1x\nTransformer+Dual +NP               ~52h                                                                1x\nGNMT-M-SSL +NP                         ~30h                                                                2.1x\nMGNMT (Transformer)                ~22h                                                                2.7x\nMGNMT (Transformer) + NP       ~45h                                                                2.7x\n--------------------------------------------------------------------------------------------------------------------      \n*NP: non-parallel\n\nWe can see that on-fly sampling leads to time-consumption, and MGNMT takes more training time than JBT but less than Dual. One possible way to improve the efficiency may be to sample and save these pseudo-translation pairs in advance to the next iteration of training. \n\nAs for inference time, Transformer+{BT/JBT/Dual} are roughly the same as vanilla Transformer because essentially they are different strategies for training Transformer which do not modify the decoding phase. Meanwhile, MGNMT requires ~2.7x time for decoding because MGNMT needs iterative decoding for several iterations.  \n\nQ2: Typos and related works\nA2: Thanks. We will fix typos and missing citations in the next version. "}, "signatures": ["ICLR.cc/2020/Conference/Paper850/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper850/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhengzx.142857@gmail.com", "zhouhao.nlp@bytedance.com", "huangsj@nju.edu.cn", "lilei.02@bytedance.com", "daixinyu@nju.edu.cn", "chenjj@nju.edu.cn"], "title": "Mirror-Generative Neural Machine Translation", "authors": ["Zaixiang Zheng", "Hao Zhou", "Shujian Huang", "Lei Li", "Xin-Yu Dai", "Jiajun Chen"], "pdf": "/pdf/5c722bee553d0d9919584a7d1dab3d97d58b0b4e.pdf", "abstract": "\nTraining neural machine translation models (NMT) requires a large amount of parallel corpus, which is scarce for many language pairs. However, raw non-parallel corpora are often easy to obtain. Existing approaches have not exploited the full potential of non-parallel bilingual data either in training or decoding. In this paper, we propose the mirror-generative NMT (MGNMT), a single unified architecture that simultaneously integrates the source to target translation model, the target to source translation model, and two language models. Both translation models and language models share the same latent semantic space, therefore both translation directions can learn from non-parallel data more effectively. Besides, the translation models and language models can collaborate together during decoding. Our experiments show that the proposed MGNMT consistently outperforms existing approaches in a variety of scenarios and language pairs, including resource-rich and low-resource languages. ", "keywords": ["neural machine translation", "generative model", "mirror"], "paperhash": "zheng|mirrorgenerative_neural_machine_translation", "_bibtex": "@inproceedings{\nZheng2020Mirror-Generative,\ntitle={Mirror-Generative Neural Machine Translation},\nauthor={Zaixiang Zheng and Hao Zhou and Shujian Huang and Lei Li and Xin-Yu Dai and Jiajun Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxQRTNYPH}\n}", "original_pdf": "/attachment/05b875c12e10f36ee4ba33864926a28efe45ce3f.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxQRTNYPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper850/Authors", "ICLR.cc/2020/Conference/Paper850/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper850/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper850/Reviewers", "ICLR.cc/2020/Conference/Paper850/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper850/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper850/Authors|ICLR.cc/2020/Conference/Paper850/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504165250, "tmdate": 1576860530726, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper850/Authors", "ICLR.cc/2020/Conference/Paper850/Reviewers", "ICLR.cc/2020/Conference/Paper850/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper850/-/Official_Comment"}}}, {"id": "rJe2KI8hiS", "original": null, "number": 4, "cdate": 1573836419998, "ddate": null, "tcdate": 1573836419998, "tmdate": 1573836419998, "tddate": null, "forum": "HkxQRTNYPH", "replyto": "r1x8BrKCFB", "invitation": "ICLR.cc/2020/Conference/Paper850/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "Thanks very much for your comments! \n\nQ1: About experimenting MGNMT with Transformer\nA1: All experimental results in the experiment section are implemented based on Transformer. We also give results implemented on RNMT, which are listed in the Appendix. We will make it clearer in the revision. Sorry for the confusion. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper850/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper850/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhengzx.142857@gmail.com", "zhouhao.nlp@bytedance.com", "huangsj@nju.edu.cn", "lilei.02@bytedance.com", "daixinyu@nju.edu.cn", "chenjj@nju.edu.cn"], "title": "Mirror-Generative Neural Machine Translation", "authors": ["Zaixiang Zheng", "Hao Zhou", "Shujian Huang", "Lei Li", "Xin-Yu Dai", "Jiajun Chen"], "pdf": "/pdf/5c722bee553d0d9919584a7d1dab3d97d58b0b4e.pdf", "abstract": "\nTraining neural machine translation models (NMT) requires a large amount of parallel corpus, which is scarce for many language pairs. However, raw non-parallel corpora are often easy to obtain. Existing approaches have not exploited the full potential of non-parallel bilingual data either in training or decoding. In this paper, we propose the mirror-generative NMT (MGNMT), a single unified architecture that simultaneously integrates the source to target translation model, the target to source translation model, and two language models. Both translation models and language models share the same latent semantic space, therefore both translation directions can learn from non-parallel data more effectively. Besides, the translation models and language models can collaborate together during decoding. Our experiments show that the proposed MGNMT consistently outperforms existing approaches in a variety of scenarios and language pairs, including resource-rich and low-resource languages. ", "keywords": ["neural machine translation", "generative model", "mirror"], "paperhash": "zheng|mirrorgenerative_neural_machine_translation", "_bibtex": "@inproceedings{\nZheng2020Mirror-Generative,\ntitle={Mirror-Generative Neural Machine Translation},\nauthor={Zaixiang Zheng and Hao Zhou and Shujian Huang and Lei Li and Xin-Yu Dai and Jiajun Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxQRTNYPH}\n}", "original_pdf": "/attachment/05b875c12e10f36ee4ba33864926a28efe45ce3f.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxQRTNYPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper850/Authors", "ICLR.cc/2020/Conference/Paper850/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper850/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper850/Reviewers", "ICLR.cc/2020/Conference/Paper850/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper850/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper850/Authors|ICLR.cc/2020/Conference/Paper850/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504165250, "tmdate": 1576860530726, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper850/Authors", "ICLR.cc/2020/Conference/Paper850/Reviewers", "ICLR.cc/2020/Conference/Paper850/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper850/-/Official_Comment"}}}, {"id": "r1x8BrKCFB", "original": null, "number": 2, "cdate": 1571882301988, "ddate": null, "tcdate": 1571882301988, "tmdate": 1572972544334, "tddate": null, "forum": "HkxQRTNYPH", "replyto": "HkxQRTNYPH", "invitation": "ICLR.cc/2020/Conference/Paper850/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work proposes a new translation model that combines translation models in two directions and language modelss in two languages by sharing a latent semantic representation. The basic idea to joint modeling of translations conditioning on the latent representations and the parameters are learned by generating pseudo translations in two directions. Decoding is also carefully designed by interchanging sampling in two directions in a greedy fashion. Empirical results show consistent gains when compared with heuristic methods to generate pseudo data, e.g., back translation. \n\nIt is an interesting work on proposing a unified framework to translation by conditioning on a shared latent space in four models. It is not only rivaling heuristic methods to generate pseudo data, but surpassing competitive Transformer baselines.\n\nOther comment:\n\n- It is a bit confusing that MGNMT was not experimented with Transformer, though the paper and appendix describe that it is easy to use the Transformer in the MGNMT setting. "}, "signatures": ["ICLR.cc/2020/Conference/Paper850/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper850/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhengzx.142857@gmail.com", "zhouhao.nlp@bytedance.com", "huangsj@nju.edu.cn", "lilei.02@bytedance.com", "daixinyu@nju.edu.cn", "chenjj@nju.edu.cn"], "title": "Mirror-Generative Neural Machine Translation", "authors": ["Zaixiang Zheng", "Hao Zhou", "Shujian Huang", "Lei Li", "Xin-Yu Dai", "Jiajun Chen"], "pdf": "/pdf/5c722bee553d0d9919584a7d1dab3d97d58b0b4e.pdf", "abstract": "\nTraining neural machine translation models (NMT) requires a large amount of parallel corpus, which is scarce for many language pairs. However, raw non-parallel corpora are often easy to obtain. Existing approaches have not exploited the full potential of non-parallel bilingual data either in training or decoding. In this paper, we propose the mirror-generative NMT (MGNMT), a single unified architecture that simultaneously integrates the source to target translation model, the target to source translation model, and two language models. Both translation models and language models share the same latent semantic space, therefore both translation directions can learn from non-parallel data more effectively. Besides, the translation models and language models can collaborate together during decoding. Our experiments show that the proposed MGNMT consistently outperforms existing approaches in a variety of scenarios and language pairs, including resource-rich and low-resource languages. ", "keywords": ["neural machine translation", "generative model", "mirror"], "paperhash": "zheng|mirrorgenerative_neural_machine_translation", "_bibtex": "@inproceedings{\nZheng2020Mirror-Generative,\ntitle={Mirror-Generative Neural Machine Translation},\nauthor={Zaixiang Zheng and Hao Zhou and Shujian Huang and Lei Li and Xin-Yu Dai and Jiajun Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxQRTNYPH}\n}", "original_pdf": "/attachment/05b875c12e10f36ee4ba33864926a28efe45ce3f.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkxQRTNYPH", "replyto": "HkxQRTNYPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper850/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper850/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574722376000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper850/Reviewers"], "noninvitees": [], "tcdate": 1570237746081, "tmdate": 1574723093959, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper850/-/Official_Review"}}}, {"id": "H1ei7P7g9H", "original": null, "number": 3, "cdate": 1571989282921, "ddate": null, "tcdate": 1571989282921, "tmdate": 1572972544290, "tddate": null, "forum": "HkxQRTNYPH", "replyto": "HkxQRTNYPH", "invitation": "ICLR.cc/2020/Conference/Paper850/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, the authors propose MGNMT (Mirror Generative NMT) which aims to integrate s2t, t2s, source and target language models in a single framework. They lay out the details of their framework and motivate the need for leveraging monolingual data in both source and target directions. They also talk about related work in this space. Finally, they perform experiments on low and high resource tasks. They also investigate certain specific phenomena like effect of non-parallel data, effect of target LM during decoding, and effect of adding one side monolingual data.\n\nPros:\n- Overall, the paper was clearly written and well motivated. The authors clearly lay out their new framework and establish it for the reader.\n- The set of experiments are very detailed and the authors make sure to compare against all semi-supervised works like BT, JBT and Dual learning.\n- The set of analyses at the end was also interesting and tried to dig deeper in certain phenomena.\n- All training details and hyperparameters have been laid it in the paper.\n\nCons:\n- For all the additional complexity, this newly proposed method only slightly outperforms other semi-supervised methods like BT, JBT & Dual learning as seen in Tables 3 and 4.\n- The authors could have been more upfront about training and inference costs of their proposed framework and compared it to the other setups. For example, decoding costs 2.7x more than a vanilla transformer. A comparison of decoding and training costs of all methods would have provided the right balance between complexity and quality. This additional complexity might outweigh the gains obtained in some cases.\n\nRating Justification:\nDespite the con of added complexity, I like the formulation of the new joint framework and I think this will serve as a good starting point for others to push in this direction further. Hence, I want to see this paper accepted.\n\nMinor comments:\nlast para of section 1: first line is too big. Please break into multiple lines.\n\"Exploiting non-parallel data for NMT\" - second para, please cite Dong et. al and Johnson et. al who also share al parameters and vocab in a single model.\nPage 5, section 3.2, second para - line 1 please rephrase."}, "signatures": ["ICLR.cc/2020/Conference/Paper850/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper850/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhengzx.142857@gmail.com", "zhouhao.nlp@bytedance.com", "huangsj@nju.edu.cn", "lilei.02@bytedance.com", "daixinyu@nju.edu.cn", "chenjj@nju.edu.cn"], "title": "Mirror-Generative Neural Machine Translation", "authors": ["Zaixiang Zheng", "Hao Zhou", "Shujian Huang", "Lei Li", "Xin-Yu Dai", "Jiajun Chen"], "pdf": "/pdf/5c722bee553d0d9919584a7d1dab3d97d58b0b4e.pdf", "abstract": "\nTraining neural machine translation models (NMT) requires a large amount of parallel corpus, which is scarce for many language pairs. However, raw non-parallel corpora are often easy to obtain. Existing approaches have not exploited the full potential of non-parallel bilingual data either in training or decoding. In this paper, we propose the mirror-generative NMT (MGNMT), a single unified architecture that simultaneously integrates the source to target translation model, the target to source translation model, and two language models. Both translation models and language models share the same latent semantic space, therefore both translation directions can learn from non-parallel data more effectively. Besides, the translation models and language models can collaborate together during decoding. Our experiments show that the proposed MGNMT consistently outperforms existing approaches in a variety of scenarios and language pairs, including resource-rich and low-resource languages. ", "keywords": ["neural machine translation", "generative model", "mirror"], "paperhash": "zheng|mirrorgenerative_neural_machine_translation", "_bibtex": "@inproceedings{\nZheng2020Mirror-Generative,\ntitle={Mirror-Generative Neural Machine Translation},\nauthor={Zaixiang Zheng and Hao Zhou and Shujian Huang and Lei Li and Xin-Yu Dai and Jiajun Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxQRTNYPH}\n}", "original_pdf": "/attachment/05b875c12e10f36ee4ba33864926a28efe45ce3f.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkxQRTNYPH", "replyto": "HkxQRTNYPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper850/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper850/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574722376000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper850/Reviewers"], "noninvitees": [], "tcdate": 1570237746081, "tmdate": 1574723093959, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper850/-/Official_Review"}}}, {"id": "B1lUjQp_9r", "original": null, "number": 3, "cdate": 1572553629572, "ddate": null, "tcdate": 1572553629572, "tmdate": 1572553629572, "tddate": null, "forum": "HkxQRTNYPH", "replyto": "HkxQRTNYPH", "invitation": "ICLR.cc/2020/Conference/Paper850/-/Official_Comment", "content": {"title": "Relationship to other methods considering bidirectional translation scores at test time?", "comment": "I've taken a look at this paper, and the method looks interesting! However, I had a question related to previous work. Because one of the fundamental ideas of the paper is that it should consider translation in both directions, including at test time, the most relevant baselines seem to be ones that do so.\n\nFor example, in the paper Tu et al. (2017) and Cheng et al. (2017) are cited as being essentially the same as the proposed methods. How would the proposed method compare to these theoretically and empirically?\n\nIn addition, there are much simpler methods of simply training bidirectional translation systems and combining them together at test time using reranking, or other more complicated search algorithms (Yu et al. 2016, Hoang et al. 2017). In particular, reranking with bidirectional scores is already being used, for example in FAIR's high-scoring submission for the WMT shared task this year, so it is obviously a practical and widely-understood method (Ng et al. 2019).\n\nCould the authors please clarify, at the very least, why the proposed method would theoretically be superior to these other methods? It would also be ideal if it was possible to empirically compare with another simple baseline, e.g. generating a large n-best list with a forward GNMT model, and re-ranking with the combination of forward and backward GNMT scores like was done in (Ng et al. 2019).\n\n* Yu, Lei, et al. \"The neural noisy channel.\" arXiv preprint arXiv:1611.02554 (2016).\n* Hoang, Cong Duy Vu, Gholamreza Haffari, and Trevor Cohn. \"Towards decoding as continuous optimisation in neural machine translation.\" Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. 2017.\n* Nathan Ng, Kyra Yee, Alexei Baevski, Myle Ott, Michael Auli and Sergey Edunov. \"Facebook FAIR\u2019s WMT19 News Translation Task Submission.\" Proceedings of WMT 2019. 2019."}, "signatures": ["ICLR.cc/2020/Conference/Paper850/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper850/Area_Chair1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhengzx.142857@gmail.com", "zhouhao.nlp@bytedance.com", "huangsj@nju.edu.cn", "lilei.02@bytedance.com", "daixinyu@nju.edu.cn", "chenjj@nju.edu.cn"], "title": "Mirror-Generative Neural Machine Translation", "authors": ["Zaixiang Zheng", "Hao Zhou", "Shujian Huang", "Lei Li", "Xin-Yu Dai", "Jiajun Chen"], "pdf": "/pdf/5c722bee553d0d9919584a7d1dab3d97d58b0b4e.pdf", "abstract": "\nTraining neural machine translation models (NMT) requires a large amount of parallel corpus, which is scarce for many language pairs. However, raw non-parallel corpora are often easy to obtain. Existing approaches have not exploited the full potential of non-parallel bilingual data either in training or decoding. In this paper, we propose the mirror-generative NMT (MGNMT), a single unified architecture that simultaneously integrates the source to target translation model, the target to source translation model, and two language models. Both translation models and language models share the same latent semantic space, therefore both translation directions can learn from non-parallel data more effectively. Besides, the translation models and language models can collaborate together during decoding. Our experiments show that the proposed MGNMT consistently outperforms existing approaches in a variety of scenarios and language pairs, including resource-rich and low-resource languages. ", "keywords": ["neural machine translation", "generative model", "mirror"], "paperhash": "zheng|mirrorgenerative_neural_machine_translation", "_bibtex": "@inproceedings{\nZheng2020Mirror-Generative,\ntitle={Mirror-Generative Neural Machine Translation},\nauthor={Zaixiang Zheng and Hao Zhou and Shujian Huang and Lei Li and Xin-Yu Dai and Jiajun Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxQRTNYPH}\n}", "original_pdf": "/attachment/05b875c12e10f36ee4ba33864926a28efe45ce3f.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxQRTNYPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper850/Authors", "ICLR.cc/2020/Conference/Paper850/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper850/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper850/Reviewers", "ICLR.cc/2020/Conference/Paper850/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper850/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper850/Authors|ICLR.cc/2020/Conference/Paper850/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504165250, "tmdate": 1576860530726, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper850/Authors", "ICLR.cc/2020/Conference/Paper850/Reviewers", "ICLR.cc/2020/Conference/Paper850/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper850/-/Official_Comment"}}}], "count": 14}