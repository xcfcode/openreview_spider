{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1393364940000, "tcdate": 1393364940000, "number": 7, "id": "OOhePdBmJiPK8", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "MMG-yUjRFZqpn", "replyto": "MMG-yUjRFZqpn", "signatures": ["Karl Moritz Hermann"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Dear Ryan,\r\n\r\nThank you for your comments on our submission. I think you are raising a valid point concerning the dimensionality of our embeddings - this is something we should have explained better in the paper. Effectively, we started using d=128 in the beginning (chosen somewhat arbitrarily based on prior work on distributed representations). Subsequently, we tried tuning this parameter a little bit, but it didn't make much of a difference and so we stuck with the original setting.\r\n\r\nFollowing your comments I have re-run all experiments with d=40. The results for this are as follows (matching Table 1):\r\n\r\nBiCVM        83.7    71.4\r\nBiCVM+      86.2    76.9\r\n\r\nIf you think this would be useful, we are happy to include these and other results with changing dimensionalities in the paper.\r\n\r\nBest\r\nKarl Moritz"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "A Simple Model for Learning Multilingual Compositional Semantics", "decision": "submitted, no decision", "abstract": "Distributed representations of meaning are a natural way to encode covariance relationships between words and phrases in NLP. By overcoming data sparsity problems, as well as providing information about semantic relatedness which is not available in discrete representations, distributed representations have proven useful in many NLP tasks. In particular, recent work has shown how compositional semantic representations can successfully be applied to a number of monolingual applications such as sentiment analysis. At the same time, there has been some initial success in work on learning shared word-level representations across languages. We combine these two approaches by proposing a method for learning compositional representations in a multilingual setup. Our model learns to assign similar embeddings to aligned sentences and dissimilar ones to sentence which are not aligned while not requiring word alignments. We show that our representations are semantically informative and apply them to a cross-lingual document classification task where we outperform the previous state of the art. Further, by employing parallel corpora of multiple language pairs we find that our model learns representations that capture semantic relationships across languages for which no parallel data was used.", "pdf": "https://arxiv.org/abs/1312.6173", "paperhash": "hermann|a_simple_model_for_learning_multilingual_compositional_semantics", "keywords": [], "conflicts": [], "authors": ["Karl Moritz Hermann", "Phil Blunsom"], "authorids": ["mail@karlmoritz.com", "phil.blunsom@cs.ox.ac.uk"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1393247760000, "tcdate": 1393247760000, "number": 6, "id": "KvQCK0VJuylci", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "MMG-yUjRFZqpn", "replyto": "MMG-yUjRFZqpn", "signatures": ["Ryan Kiros"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "The experiments of Klementiev et al. use d=40 dimensional word embeddings. How come you chose to use d=128? How do we know your improvements over Klementiev et al. are not just from using 3x the embedding dimensionality? I'm not sure that this is a fair comparison."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "A Simple Model for Learning Multilingual Compositional Semantics", "decision": "submitted, no decision", "abstract": "Distributed representations of meaning are a natural way to encode covariance relationships between words and phrases in NLP. By overcoming data sparsity problems, as well as providing information about semantic relatedness which is not available in discrete representations, distributed representations have proven useful in many NLP tasks. In particular, recent work has shown how compositional semantic representations can successfully be applied to a number of monolingual applications such as sentiment analysis. At the same time, there has been some initial success in work on learning shared word-level representations across languages. We combine these two approaches by proposing a method for learning compositional representations in a multilingual setup. Our model learns to assign similar embeddings to aligned sentences and dissimilar ones to sentence which are not aligned while not requiring word alignments. We show that our representations are semantically informative and apply them to a cross-lingual document classification task where we outperform the previous state of the art. Further, by employing parallel corpora of multiple language pairs we find that our model learns representations that capture semantic relationships across languages for which no parallel data was used.", "pdf": "https://arxiv.org/abs/1312.6173", "paperhash": "hermann|a_simple_model_for_learning_multilingual_compositional_semantics", "keywords": [], "conflicts": [], "authors": ["Karl Moritz Hermann", "Phil Blunsom"], "authorids": ["mail@karlmoritz.com", "phil.blunsom@cs.ox.ac.uk"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1393032360000, "tcdate": 1393032360000, "number": 3, "id": "ONvRZkPuanO6s", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "MMG-yUjRFZqpn", "replyto": "MMG-yUjRFZqpn", "signatures": ["Karl Moritz Hermann"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "An updated version has now been submitted to ArXiv. I'm not entirely sure how long it will require until it will be publicly viewable. We've changed the title to 'Multilingual Distributed Representations without Word Alignment' in response to the questions raised about the compositional nature of this model.\r\n\r\nThe other points have been addressed as stated in my previous post."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "A Simple Model for Learning Multilingual Compositional Semantics", "decision": "submitted, no decision", "abstract": "Distributed representations of meaning are a natural way to encode covariance relationships between words and phrases in NLP. By overcoming data sparsity problems, as well as providing information about semantic relatedness which is not available in discrete representations, distributed representations have proven useful in many NLP tasks. In particular, recent work has shown how compositional semantic representations can successfully be applied to a number of monolingual applications such as sentiment analysis. At the same time, there has been some initial success in work on learning shared word-level representations across languages. We combine these two approaches by proposing a method for learning compositional representations in a multilingual setup. Our model learns to assign similar embeddings to aligned sentences and dissimilar ones to sentence which are not aligned while not requiring word alignments. We show that our representations are semantically informative and apply them to a cross-lingual document classification task where we outperform the previous state of the art. Further, by employing parallel corpora of multiple language pairs we find that our model learns representations that capture semantic relationships across languages for which no parallel data was used.", "pdf": "https://arxiv.org/abs/1312.6173", "paperhash": "hermann|a_simple_model_for_learning_multilingual_compositional_semantics", "keywords": [], "conflicts": [], "authors": ["Karl Moritz Hermann", "Phil Blunsom"], "authorids": ["mail@karlmoritz.com", "phil.blunsom@cs.ox.ac.uk"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392662760000, "tcdate": 1392662760000, "number": 5, "id": "ddHnw4OEnrw3R", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "MMG-yUjRFZqpn", "replyto": "MMG-yUjRFZqpn", "signatures": ["Karl Moritz Hermann"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We would like to thank the anonymous reviewers for their thorough reviews and comments regarding our submission.\r\n\r\nSeveral reviewers took issue with our use of the term 'compositional semantics' in relation to an additive composition function (i.e. a bag-of-words model). I agree with reviewer 57c7, who pointed out that such models have been described as compositional in the past, but that the term might be misleading in light of more recent development in compositional semantics. I will update the paper to improve the wording regarding this term, and to clarify the limitations of our model versus fuller compositional approaches.\r\n\r\nAs pointed out by reviewers 9aaf and 048b, it would be interesting to see how our objective function performs in combination with a more complex model. I agree that the current BoW model is likely to be insufficient for tasks such as machine translation. The key contribution of the current submission are two: first, learning word embeddings without word alignment, and second the multilingual objective function that we use to achieve this and its parallel use for semantic transfer. I agree that it would be interesting to develop this further by using more complex composition models and also by attempting other tasks such as machine translation - we are currently studying these possibilities for further work.\r\n\r\nAlso, thank you for pointing out the Lauly et al. paper from the NIPS workshop; this seems indeed very relevant and I will incorporate and reference this paper accordingly.\r\n\r\nTo answer the question raised about sampling by reviewer 57c7: we sampled negative examples at every epoch. We will update the paper to address this and the other minor remarks made in the three reviews, and submit an updated paper to ArXiv in the next few days."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "A Simple Model for Learning Multilingual Compositional Semantics", "decision": "submitted, no decision", "abstract": "Distributed representations of meaning are a natural way to encode covariance relationships between words and phrases in NLP. By overcoming data sparsity problems, as well as providing information about semantic relatedness which is not available in discrete representations, distributed representations have proven useful in many NLP tasks. In particular, recent work has shown how compositional semantic representations can successfully be applied to a number of monolingual applications such as sentiment analysis. At the same time, there has been some initial success in work on learning shared word-level representations across languages. We combine these two approaches by proposing a method for learning compositional representations in a multilingual setup. Our model learns to assign similar embeddings to aligned sentences and dissimilar ones to sentence which are not aligned while not requiring word alignments. We show that our representations are semantically informative and apply them to a cross-lingual document classification task where we outperform the previous state of the art. Further, by employing parallel corpora of multiple language pairs we find that our model learns representations that capture semantic relationships across languages for which no parallel data was used.", "pdf": "https://arxiv.org/abs/1312.6173", "paperhash": "hermann|a_simple_model_for_learning_multilingual_compositional_semantics", "keywords": [], "conflicts": [], "authors": ["Karl Moritz Hermann", "Phil Blunsom"], "authorids": ["mail@karlmoritz.com", "phil.blunsom@cs.ox.ac.uk"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392555240000, "tcdate": 1392555240000, "number": 4, "id": "_6Xb6-WgbC_Yx", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "MMG-yUjRFZqpn", "replyto": "MMG-yUjRFZqpn", "signatures": ["anonymous reviewer 048b"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of A Simple Model for Learning Multilingual Compositional Semantics", "review": "This paper introduces an interesting model to learn single word vector embeddings for 2 languages simultaneously.\r\nIt is applied to a classification task.\r\n\r\nThe paper is very clear and well written.\r\n\r\nIt does not seem to actually learn compositional semantics in the usual sense:\r\n\r\nhttp://en.wikipedia.org/wiki/Principle_of_compositionality\r\nPrinciple of Compositionality is the principle that the meaning of a complex expression is determined by the meanings of its constituent expressions and the rules used to combine them.\r\n\r\nCertainly, averaging all words in a bag of words is not a compositional rule that would allow people to retrieve the meaning. From wikipedia: \r\n'The principle of compositionality states that in a meaningful sentence, if the lexical parts are taken out of the sentence, what remains will be the rules of composition. Take, for example, the sentence 'Socrates was a man'. Once the meaningful lexical items are taken away\u2014'Socrates' and 'man'\u2014what is left is the pseudo-sentence, 'S was a M'. The task becomes a matter of describing what the connection is between S and M.'\r\nThe connection between S and M would not be retrievable from a bag of words representation.\r\n\r\nOn a related note, the model could not be used for (presumable the final goal of) machine translation in its current form.\r\n\r\nIt would be great to see a comparison with the work from the same lab of Kalchbrenner and Blunsom.\r\n\r\nDespite its problems, it seems an interesting paper."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "A Simple Model for Learning Multilingual Compositional Semantics", "decision": "submitted, no decision", "abstract": "Distributed representations of meaning are a natural way to encode covariance relationships between words and phrases in NLP. By overcoming data sparsity problems, as well as providing information about semantic relatedness which is not available in discrete representations, distributed representations have proven useful in many NLP tasks. In particular, recent work has shown how compositional semantic representations can successfully be applied to a number of monolingual applications such as sentiment analysis. At the same time, there has been some initial success in work on learning shared word-level representations across languages. We combine these two approaches by proposing a method for learning compositional representations in a multilingual setup. Our model learns to assign similar embeddings to aligned sentences and dissimilar ones to sentence which are not aligned while not requiring word alignments. We show that our representations are semantically informative and apply them to a cross-lingual document classification task where we outperform the previous state of the art. Further, by employing parallel corpora of multiple language pairs we find that our model learns representations that capture semantic relationships across languages for which no parallel data was used.", "pdf": "https://arxiv.org/abs/1312.6173", "paperhash": "hermann|a_simple_model_for_learning_multilingual_compositional_semantics", "keywords": [], "conflicts": [], "authors": ["Karl Moritz Hermann", "Phil Blunsom"], "authorids": ["mail@karlmoritz.com", "phil.blunsom@cs.ox.ac.uk"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391787780000, "tcdate": 1391787780000, "number": 2, "id": "JFuEJvPoL0JVk", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "MMG-yUjRFZqpn", "replyto": "MMG-yUjRFZqpn", "signatures": ["anonymous reviewer 9aaf"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of A Simple Model for Learning Multilingual Compositional Semantics", "review": "This paper proposes a simple model to learn word embeddings in a\r\nbilingual setting. The model learns embeddings at the sentence-pair\r\nlevel, where aligned sentences are similarly represented. This simple\r\nmodel does not rely on word alignments or MT system.\r\n\r\nThe paper is well written and presents convincing results. My only\r\nconcern is why the authors use the term 'Models of Compositional\r\nDistributed Semantics' for a model that is more related to annotation\r\ntransfer. Moreover, the continuous representation of a sentence is the\r\nsum of word embeddings. This more related to a bag of word model than\r\na model that can handle compositionality. A minor remark about the\r\nresults: the authors observe that BICVM+ outperforms the BICVM model\r\nwhen training on English data, but performs worse in the opposite\r\ndirection. Could you comment on that.\r\n\r\nThe authors could read the paper of Lauly et al. at the last NIPS\r\nworkshop on Deep Learning. This is clearly related to this\r\nwork. Finally, I wonder what could be the performances with more\r\ncomplex model. For instance, the same authors proposed a translation\r\nmodel based on recurrent Net that could be used for this task."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "A Simple Model for Learning Multilingual Compositional Semantics", "decision": "submitted, no decision", "abstract": "Distributed representations of meaning are a natural way to encode covariance relationships between words and phrases in NLP. By overcoming data sparsity problems, as well as providing information about semantic relatedness which is not available in discrete representations, distributed representations have proven useful in many NLP tasks. In particular, recent work has shown how compositional semantic representations can successfully be applied to a number of monolingual applications such as sentiment analysis. At the same time, there has been some initial success in work on learning shared word-level representations across languages. We combine these two approaches by proposing a method for learning compositional representations in a multilingual setup. Our model learns to assign similar embeddings to aligned sentences and dissimilar ones to sentence which are not aligned while not requiring word alignments. We show that our representations are semantically informative and apply them to a cross-lingual document classification task where we outperform the previous state of the art. Further, by employing parallel corpora of multiple language pairs we find that our model learns representations that capture semantic relationships across languages for which no parallel data was used.", "pdf": "https://arxiv.org/abs/1312.6173", "paperhash": "hermann|a_simple_model_for_learning_multilingual_compositional_semantics", "keywords": [], "conflicts": [], "authors": ["Karl Moritz Hermann", "Phil Blunsom"], "authorids": ["mail@karlmoritz.com", "phil.blunsom@cs.ox.ac.uk"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391573040000, "tcdate": 1391573040000, "number": 1, "id": "kr2Dk3gUX7iFy", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "MMG-yUjRFZqpn", "replyto": "MMG-yUjRFZqpn", "signatures": ["anonymous reviewer 57c7"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of A Simple Model for Learning Multilingual Compositional Semantics", "review": "The paper considers learning cross-lingual representations of words using parallel data aligned at the level of sentences. A representation of a sentence is just a sum of embeddings of words in the sentence. These representations for pairs of sentences are learned to be similar. Specifically,  a ranking objective is used: for every sentence x in L1, the representation of the aligned sentence in L2 should be closer to the representation of x  than to representations of (a random sample of) other sentences in L2.  The resulting representations are used in transferring  a document classifier across languages (without retraining - i.e.  so called 'direct transfer').  Interestingly, the results are (mostly) better than these with cross-lingual word embeddings of Klementiev et al. (2012) learned using automatically word aligned sentences (as well as significantly better than a machine translation baseline -- which applies the classifier to automatically translated documents). \r\n\r\nI find the paper quite interesting and well written. The results are fairly impressive as well. \r\n\r\nHowever, I am not entirely convinced that calling this 'multilingual compositional semantics' is very appropriate. Though I agree that a more complex compositional model is probably not necessary for the document classification task (after all, a unigram model achieves competitive results on classifying RCV documents), it seems a bit misleading to call a bag-of-words approach compositional. After all, Klementiev et al. also sum word representation to yield a representation of a document. (However, such added compositional model, of course, have been considered in the past and called compositional -- e.g., Mitchell and Lapata (2008)).  From my perspective, the interesting aspect here is learning word representation without using word alignment information. In this way, the work is similar to the paper of Lauly et al. presented at the NIPS Deep Learning workshop (http://arxiv.org/abs/1401.1803). However, their results are not directly comparable as they used a different test set.  My concern is that a different learning objective would be needed if more expressive compositional models are used (perhaps combining both similarity across languages and the reconstruction error as in Socher (EMNLP 2011)). \r\n\r\nMinor:\r\n-- it would be interesting to see results on other language pairs (e.g., French was already used in training, but not in testing, even though RCV contains articles in French)\r\n-- also I am wondering how performance varies depending on the size of parallel data (as this might be a concern for low resource languages where direct transfer approaches are especially attractive)\r\n-- It was not entirely clear how negative examples are sampled (formula 6):  are they chosen at every epoch  (as, e.g., in Rendle et al. (UAI 2009)), or chosen once for every sentence pair and then kept fixed during training?\r\n-- section 2.1, par 1:  'multi-agent learning' -> 'multi-task learning' ?"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "A Simple Model for Learning Multilingual Compositional Semantics", "decision": "submitted, no decision", "abstract": "Distributed representations of meaning are a natural way to encode covariance relationships between words and phrases in NLP. By overcoming data sparsity problems, as well as providing information about semantic relatedness which is not available in discrete representations, distributed representations have proven useful in many NLP tasks. In particular, recent work has shown how compositional semantic representations can successfully be applied to a number of monolingual applications such as sentiment analysis. At the same time, there has been some initial success in work on learning shared word-level representations across languages. We combine these two approaches by proposing a method for learning compositional representations in a multilingual setup. Our model learns to assign similar embeddings to aligned sentences and dissimilar ones to sentence which are not aligned while not requiring word alignments. We show that our representations are semantically informative and apply them to a cross-lingual document classification task where we outperform the previous state of the art. Further, by employing parallel corpora of multiple language pairs we find that our model learns representations that capture semantic relationships across languages for which no parallel data was used.", "pdf": "https://arxiv.org/abs/1312.6173", "paperhash": "hermann|a_simple_model_for_learning_multilingual_compositional_semantics", "keywords": [], "conflicts": [], "authors": ["Karl Moritz Hermann", "Phil Blunsom"], "authorids": ["mail@karlmoritz.com", "phil.blunsom@cs.ox.ac.uk"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387952100000, "tcdate": 1387952100000, "number": 55, "id": "MMG-yUjRFZqpn", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "MMG-yUjRFZqpn", "signatures": ["mail@karlmoritz.com"], "readers": ["everyone"], "content": {"title": "A Simple Model for Learning Multilingual Compositional Semantics", "decision": "submitted, no decision", "abstract": "Distributed representations of meaning are a natural way to encode covariance relationships between words and phrases in NLP. By overcoming data sparsity problems, as well as providing information about semantic relatedness which is not available in discrete representations, distributed representations have proven useful in many NLP tasks. In particular, recent work has shown how compositional semantic representations can successfully be applied to a number of monolingual applications such as sentiment analysis. At the same time, there has been some initial success in work on learning shared word-level representations across languages. We combine these two approaches by proposing a method for learning compositional representations in a multilingual setup. Our model learns to assign similar embeddings to aligned sentences and dissimilar ones to sentence which are not aligned while not requiring word alignments. We show that our representations are semantically informative and apply them to a cross-lingual document classification task where we outperform the previous state of the art. Further, by employing parallel corpora of multiple language pairs we find that our model learns representations that capture semantic relationships across languages for which no parallel data was used.", "pdf": "https://arxiv.org/abs/1312.6173", "paperhash": "hermann|a_simple_model_for_learning_multilingual_compositional_semantics", "keywords": [], "conflicts": [], "authors": ["Karl Moritz Hermann", "Phil Blunsom"], "authorids": ["mail@karlmoritz.com", "phil.blunsom@cs.ox.ac.uk"]}, "writers": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 8}