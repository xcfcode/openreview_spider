{"notes": [{"id": "HXMvdd3nIOo", "original": null, "number": 3, "cdate": 1612866228186, "ddate": null, "tcdate": 1612866228186, "tmdate": 1616098688916, "tddate": null, "forum": "PxTIG12RRHS", "replyto": "PxTIG12RRHS", "invitation": "ICLR.cc/2021/Conference/Paper2561/-/Comment", "content": {"title": "Code release", "comment": "Our code and checkpoints are released at GitHub:\n1. JAX + FLAX codebase (recommended): https://github.com/yang-song/score_sde\n2. PyTorch codebase: https://github.com/yang-song/score_sde_pytorch\n\nWe additionally provide several colab notebooks to help people use our codebase and understand the basic ideas of this paper:\n* JAX + FLAX: Load our pretrained checkpoints and play with sampling, likelihood computation, and controllable synthesis. [link](https://colab.research.google.com/drive/1dRR_0gNRmfLtPavX2APzUggBuXyjWW55?usp=sharing)\n\n* PyTorch: Load our pretrained checkpoints and play with sampling, likelihood computation, and controllable synthesis. [link](https://colab.research.google.com/drive/17lTrPLTt_0EDXa4hkbHmbAFQEkpRDZnh?usp=sharing)\n\n* Tutorial of score-based generative models in JAX + FLAX. [link](https://colab.research.google.com/drive/1SeXMpILhkJPjXUaesvzEhc3Ke6Zl_zxJ?usp=sharing)\n\n* Tutorial of score-based generative models in PyTorch. [link](https://colab.research.google.com/drive/120kYYBOVa1i0TD85RjlEkFjaWDxSFUx3?usp=sharing)\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2561/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2561/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Score-Based Generative Modeling through Stochastic Differential Equations", "authorids": ["~Yang_Song1", "~Jascha_Sohl-Dickstein2", "~Diederik_P_Kingma1", "~Abhishek_Kumar1", "~Stefano_Ermon1", "~Ben_Poole1"], "authors": ["Yang Song", "Jascha Sohl-Dickstein", "Diederik P Kingma", "Abhishek Kumar", "Stefano Ermon", "Ben Poole"], "keywords": ["generative models", "score-based generative models", "stochastic differential equations", "score matching", "diffusion"], "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. \nCrucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of $1024\\times 1024$ images for the first time from a score-based generative model.", "one-sentence_summary": "A general framework for training and sampling from score-based models that unifies and generalizes previous methods, allows likelihood computation, and enables controllable generation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "song|scorebased_generative_modeling_through_stochastic_differential_equations", "pdf": "/pdf/ef0eadbe07115b0853e964f17aa09d811cd490f1.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsong2021scorebased,\ntitle={Score-Based Generative Modeling through Stochastic Differential Equations},\nauthor={Yang Song and Jascha Sohl-Dickstein and Diederik P Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=PxTIG12RRHS}\n}"}, "tags": [], "invitation": {"reply": {"forum": "PxTIG12RRHS", "readers": {"values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2021/Conference/Paper2561/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2561/Authors|ICLR.cc/2021/Conference/Paper2561/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs"}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}}, "multiReply": true, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["everyone"], "tcdate": 1610649449292, "tmdate": 1610649449292, "id": "ICLR.cc/2021/Conference/Paper2561/-/Comment"}}}, {"id": "PxTIG12RRHS", "original": "olrb2OosIhN", "number": 2561, "cdate": 1601308283310, "ddate": null, "tcdate": 1601308283310, "tmdate": 1612859321024, "tddate": null, "forum": "PxTIG12RRHS", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Score-Based Generative Modeling through Stochastic Differential Equations", "authorids": ["~Yang_Song1", "~Jascha_Sohl-Dickstein2", "~Diederik_P_Kingma1", "~Abhishek_Kumar1", "~Stefano_Ermon1", "~Ben_Poole1"], "authors": ["Yang Song", "Jascha Sohl-Dickstein", "Diederik P Kingma", "Abhishek Kumar", "Stefano Ermon", "Ben Poole"], "keywords": ["generative models", "score-based generative models", "stochastic differential equations", "score matching", "diffusion"], "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. \nCrucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of $1024\\times 1024$ images for the first time from a score-based generative model.", "one-sentence_summary": "A general framework for training and sampling from score-based models that unifies and generalizes previous methods, allows likelihood computation, and enables controllable generation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "song|scorebased_generative_modeling_through_stochastic_differential_equations", "pdf": "/pdf/ef0eadbe07115b0853e964f17aa09d811cd490f1.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsong2021scorebased,\ntitle={Score-Based Generative Modeling through Stochastic Differential Equations},\nauthor={Yang Song and Jascha Sohl-Dickstein and Diederik P Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=PxTIG12RRHS}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 15, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "H-iJ7fvOWq0", "original": null, "number": 2, "cdate": 1611173072516, "ddate": null, "tcdate": 1611173072516, "tmdate": 1611173072516, "tddate": null, "forum": "PxTIG12RRHS", "replyto": "XKMPwJph5dn", "invitation": "ICLR.cc/2021/Conference/Paper2561/-/Comment", "content": {"title": "Code is coming soon!", "comment": "Thanks for your interest! We are refactoring code and going through Google's code releasing policy. If everything goes well we will release code and checkpoints in the next few days. We will announce code link in a separate comment in openreview."}, "signatures": ["ICLR.cc/2021/Conference/Paper2561/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2561/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Score-Based Generative Modeling through Stochastic Differential Equations", "authorids": ["~Yang_Song1", "~Jascha_Sohl-Dickstein2", "~Diederik_P_Kingma1", "~Abhishek_Kumar1", "~Stefano_Ermon1", "~Ben_Poole1"], "authors": ["Yang Song", "Jascha Sohl-Dickstein", "Diederik P Kingma", "Abhishek Kumar", "Stefano Ermon", "Ben Poole"], "keywords": ["generative models", "score-based generative models", "stochastic differential equations", "score matching", "diffusion"], "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. \nCrucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of $1024\\times 1024$ images for the first time from a score-based generative model.", "one-sentence_summary": "A general framework for training and sampling from score-based models that unifies and generalizes previous methods, allows likelihood computation, and enables controllable generation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "song|scorebased_generative_modeling_through_stochastic_differential_equations", "pdf": "/pdf/ef0eadbe07115b0853e964f17aa09d811cd490f1.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsong2021scorebased,\ntitle={Score-Based Generative Modeling through Stochastic Differential Equations},\nauthor={Yang Song and Jascha Sohl-Dickstein and Diederik P Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=PxTIG12RRHS}\n}"}, "tags": [], "invitation": {"reply": {"forum": "PxTIG12RRHS", "readers": {"values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2021/Conference/Paper2561/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2561/Authors|ICLR.cc/2021/Conference/Paper2561/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs"}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}}, "multiReply": true, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["everyone"], "tcdate": 1610649449292, "tmdate": 1610649449292, "id": "ICLR.cc/2021/Conference/Paper2561/-/Comment"}}}, {"id": "XKMPwJph5dn", "original": null, "number": 1, "cdate": 1611003668537, "ddate": null, "tcdate": 1611003668537, "tmdate": 1611003668537, "tddate": null, "forum": "PxTIG12RRHS", "replyto": "PxTIG12RRHS", "invitation": "ICLR.cc/2021/Conference/Paper2561/-/Comment", "content": {"title": "Code", "comment": "Nice work! When can we expect the code release?"}, "signatures": ["~Yang_Zhao5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Yang_Zhao5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Score-Based Generative Modeling through Stochastic Differential Equations", "authorids": ["~Yang_Song1", "~Jascha_Sohl-Dickstein2", "~Diederik_P_Kingma1", "~Abhishek_Kumar1", "~Stefano_Ermon1", "~Ben_Poole1"], "authors": ["Yang Song", "Jascha Sohl-Dickstein", "Diederik P Kingma", "Abhishek Kumar", "Stefano Ermon", "Ben Poole"], "keywords": ["generative models", "score-based generative models", "stochastic differential equations", "score matching", "diffusion"], "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. \nCrucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of $1024\\times 1024$ images for the first time from a score-based generative model.", "one-sentence_summary": "A general framework for training and sampling from score-based models that unifies and generalizes previous methods, allows likelihood computation, and enables controllable generation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "song|scorebased_generative_modeling_through_stochastic_differential_equations", "pdf": "/pdf/ef0eadbe07115b0853e964f17aa09d811cd490f1.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsong2021scorebased,\ntitle={Score-Based Generative Modeling through Stochastic Differential Equations},\nauthor={Yang Song and Jascha Sohl-Dickstein and Diederik P Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=PxTIG12RRHS}\n}"}, "tags": [], "invitation": {"reply": {"forum": "PxTIG12RRHS", "readers": {"values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2021/Conference/Paper2561/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2561/Authors|ICLR.cc/2021/Conference/Paper2561/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs"}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}}, "multiReply": true, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["everyone"], "tcdate": 1610649449292, "tmdate": 1610649449292, "id": "ICLR.cc/2021/Conference/Paper2561/-/Comment"}}}, {"id": "Yo03K0Usn0Y", "original": null, "number": 1, "cdate": 1610040397367, "ddate": null, "tcdate": 1610040397367, "tmdate": 1610473992767, "tddate": null, "forum": "PxTIG12RRHS", "replyto": "PxTIG12RRHS", "invitation": "ICLR.cc/2021/Conference/Paper2561/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Oral)", "comment": "All reviewers agree that this is a well-written and interesting paper that will be of interest to the ICLR and broader ML community."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Score-Based Generative Modeling through Stochastic Differential Equations", "authorids": ["~Yang_Song1", "~Jascha_Sohl-Dickstein2", "~Diederik_P_Kingma1", "~Abhishek_Kumar1", "~Stefano_Ermon1", "~Ben_Poole1"], "authors": ["Yang Song", "Jascha Sohl-Dickstein", "Diederik P Kingma", "Abhishek Kumar", "Stefano Ermon", "Ben Poole"], "keywords": ["generative models", "score-based generative models", "stochastic differential equations", "score matching", "diffusion"], "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. \nCrucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of $1024\\times 1024$ images for the first time from a score-based generative model.", "one-sentence_summary": "A general framework for training and sampling from score-based models that unifies and generalizes previous methods, allows likelihood computation, and enables controllable generation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "song|scorebased_generative_modeling_through_stochastic_differential_equations", "pdf": "/pdf/ef0eadbe07115b0853e964f17aa09d811cd490f1.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsong2021scorebased,\ntitle={Score-Based Generative Modeling through Stochastic Differential Equations},\nauthor={Yang Song and Jascha Sohl-Dickstein and Diederik P Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=PxTIG12RRHS}\n}"}, "tags": [], "invitation": {"reply": {"forum": "PxTIG12RRHS", "replyto": "PxTIG12RRHS", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040397354, "tmdate": 1610473992750, "id": "ICLR.cc/2021/Conference/Paper2561/-/Decision"}}}, {"id": "7j3xdLuSZN", "original": null, "number": 1, "cdate": 1603565028836, "ddate": null, "tcdate": 1603565028836, "tmdate": 1606627914286, "tddate": null, "forum": "PxTIG12RRHS", "replyto": "PxTIG12RRHS", "invitation": "ICLR.cc/2021/Conference/Paper2561/-/Official_Review", "content": {"title": "Official Blind Review #3", "review": "#### Summary and contributions\nThis paper proposes a generalized framework for score-based generative modeling (SBGM). The proposed method subsumes previous SBGM techniques of score matching with Langevin dynamics (SMLD aka NCSN) and denoising diffusion probabilistic modeling (DDPM) and shows how they correspond to different discretizations of Stochastic Differential Equations (SDEs). The  continuous-time SDE generalizes the idea of a finite number of perturbation kernels used by previous methods to a continuum of them. The authors propose a forward SDE that transforms the data distribution into a known noise distribution and the corresponding reverse-time SDE that converts samples from this noise distribution to the data distribution. A predictor-corrector sampling framework is studied that leads to improved performance of both NCSN and DDPM frameworks. The paper also shows the equivalence of the proposed SDE to Neural ODEs which allows exact computation of the log-likelihood using the continuous change of variables formula. Quantitative experiments on the CIFAR10 dataset show that the proposed framework leads to significant improvements over previous SBGMs. Qualitative results on the CelebA-HQ dataset demonstrate the ability of the method to scale to high resolution images.\n\n#### Strengths\nThis paper makes significant technical and empirical contributions to the emerging area of score-based generative models. The generalized SDE framework subsumes recent works in this area and is also connected to Neural ODEs, enjoying exact likelihood calculation, which may be relevant to the normalizing flows and generative modeling community is general. The empirical evaluation is particularly well-done. It bridges the gap between the performance of NCSN and DDPM models leading to state-of-the-art performance. The authors also demonstrate the ability of the method to generate high quality images of human faces when trained on CelebA-HQ dataset. Preliminary experiments on class conditional generation, imputation, and image colorization demonstrate the wide applicability of the proposed method.\n\n#### Weaknesses\nThe paper does not suffer from any obvious weaknesses. The quantitive experiments could be strengthened by the addition of results on another dataset but the empirical evaluation is sufficient in its current state.\n\n#### Additional feedback\nQuestions:\n- In equation 11, how is the weighting function $\\lambda$ chosen?\n- In equation 11, apart from being able to sample from the transition kernel, it should also have a closed-form density for the evaluation of the score. Is my understanding correct?\n- In equation 21, should there be a discretization step-size corresponding to $\\Delta t$?\n- In table 1 (a), why does SMLD with corrector only perform so poorly? As far as I understand it is equivalent to NCSN. Can the authors clarify if I misunderstood something?\n\n-------\n\nPost Rebuttal: I thank the authors for clarifying on my questions and updating the manuscript. ", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2561/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2561/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Score-Based Generative Modeling through Stochastic Differential Equations", "authorids": ["~Yang_Song1", "~Jascha_Sohl-Dickstein2", "~Diederik_P_Kingma1", "~Abhishek_Kumar1", "~Stefano_Ermon1", "~Ben_Poole1"], "authors": ["Yang Song", "Jascha Sohl-Dickstein", "Diederik P Kingma", "Abhishek Kumar", "Stefano Ermon", "Ben Poole"], "keywords": ["generative models", "score-based generative models", "stochastic differential equations", "score matching", "diffusion"], "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. \nCrucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of $1024\\times 1024$ images for the first time from a score-based generative model.", "one-sentence_summary": "A general framework for training and sampling from score-based models that unifies and generalizes previous methods, allows likelihood computation, and enables controllable generation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "song|scorebased_generative_modeling_through_stochastic_differential_equations", "pdf": "/pdf/ef0eadbe07115b0853e964f17aa09d811cd490f1.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsong2021scorebased,\ntitle={Score-Based Generative Modeling through Stochastic Differential Equations},\nauthor={Yang Song and Jascha Sohl-Dickstein and Diederik P Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=PxTIG12RRHS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "PxTIG12RRHS", "replyto": "PxTIG12RRHS", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2561/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538093556, "tmdate": 1606915771842, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2561/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2561/-/Official_Review"}}}, {"id": "TcpwfhjAZ3u", "original": null, "number": 9, "cdate": 1606248247837, "ddate": null, "tcdate": 1606248247837, "tmdate": 1606248247837, "tddate": null, "forum": "PxTIG12RRHS", "replyto": "uiWg7hSErOY", "invitation": "ICLR.cc/2021/Conference/Paper2561/-/Official_Comment", "content": {"title": "Thanks for the response!", "comment": "The paper looks even better at the moment. I also liked Figure-2 very much in the sense that both stochastic/deterministic forward/backward flows are visualized. Great contribution! "}, "signatures": ["ICLR.cc/2021/Conference/Paper2561/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2561/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Score-Based Generative Modeling through Stochastic Differential Equations", "authorids": ["~Yang_Song1", "~Jascha_Sohl-Dickstein2", "~Diederik_P_Kingma1", "~Abhishek_Kumar1", "~Stefano_Ermon1", "~Ben_Poole1"], "authors": ["Yang Song", "Jascha Sohl-Dickstein", "Diederik P Kingma", "Abhishek Kumar", "Stefano Ermon", "Ben Poole"], "keywords": ["generative models", "score-based generative models", "stochastic differential equations", "score matching", "diffusion"], "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. \nCrucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of $1024\\times 1024$ images for the first time from a score-based generative model.", "one-sentence_summary": "A general framework for training and sampling from score-based models that unifies and generalizes previous methods, allows likelihood computation, and enables controllable generation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "song|scorebased_generative_modeling_through_stochastic_differential_equations", "pdf": "/pdf/ef0eadbe07115b0853e964f17aa09d811cd490f1.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsong2021scorebased,\ntitle={Score-Based Generative Modeling through Stochastic Differential Equations},\nauthor={Yang Song and Jascha Sohl-Dickstein and Diederik P Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=PxTIG12RRHS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PxTIG12RRHS", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2561/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2561/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2561/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2561/Authors|ICLR.cc/2021/Conference/Paper2561/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2561/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846927, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2561/-/Official_Comment"}}}, {"id": "BP5tFx3_p0c", "original": null, "number": 8, "cdate": 1606239460321, "ddate": null, "tcdate": 1606239460321, "tmdate": 1606239537850, "tddate": null, "forum": "PxTIG12RRHS", "replyto": "7j3xdLuSZN", "invitation": "ICLR.cc/2021/Conference/Paper2561/-/Official_Comment", "content": {"title": "Thank you for the questions and feedback", "comment": "Thank you for the detailed review and thoughtful feedback. Below we address specific questions.\n\n**Q: In equation 11 (now Eq. (7)), how is the weighting function $\\lambda(t)$ chosen?**\n\nA: As in [NCSN](https://arxiv.org/abs/1907.05600) and [DDPM](https://arxiv.org/abs/2006.11239), we recommend using $\\lambda \\propto 1/\\mathbb{E}[|| \\nabla_{\\mathbf{x}(t)} \\log p_{0t}(\\mathbf{x}(t) \\mid \\mathbf{x}(0)) ||_2^2]$. We have updated Section 3.3 to include this.\n\n**Q: In equation 11 (now Eq. (7)), apart from being able to sample from the transition kernel, it should also have a closed-form density for the evaluation of the score. Is my understanding correct?**\n\nA: You are right when using denoising score matching in Eq. (11) (now Eq. (7)). However, If we replace denoising score matching with other score matching objectives like sliced score matching, we only need to sample from the transition kernel and do not have to know its closed form. We have updated the text in Section 3.3 to make this more clear.\n\n**Q: In equation 21 (now Eq. (40)), should there be a discretization step-size corresponding to $\\Delta t$?**\n\nA: We absorbed $\\Delta t$ into the notations of $\\mathbf{f}_i$ and $\\mathbf{G}_i$. We have updated Appendix E to make this more clear.\n\n**Q: In table 1 (a), why does SMLD with corrector-only perform so poorly? As far as I understand it is equivalent to NCSN. Can the authors clarify if I misunderstood something?**\n\nA: Yes, SMLD with the corrector-only sampler is equivalent to NCSN. We find that running one Langevin step per noise level yields much worse performance than running many steps per noise level. For example, by using two Langevin steps per noise scale we can improve the FID from 39.2 to 20.4. Increasing the number of steps per noise level improves results and may match NCSN performance but would greatly increase the overall computation cost. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2561/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2561/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Score-Based Generative Modeling through Stochastic Differential Equations", "authorids": ["~Yang_Song1", "~Jascha_Sohl-Dickstein2", "~Diederik_P_Kingma1", "~Abhishek_Kumar1", "~Stefano_Ermon1", "~Ben_Poole1"], "authors": ["Yang Song", "Jascha Sohl-Dickstein", "Diederik P Kingma", "Abhishek Kumar", "Stefano Ermon", "Ben Poole"], "keywords": ["generative models", "score-based generative models", "stochastic differential equations", "score matching", "diffusion"], "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. \nCrucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of $1024\\times 1024$ images for the first time from a score-based generative model.", "one-sentence_summary": "A general framework for training and sampling from score-based models that unifies and generalizes previous methods, allows likelihood computation, and enables controllable generation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "song|scorebased_generative_modeling_through_stochastic_differential_equations", "pdf": "/pdf/ef0eadbe07115b0853e964f17aa09d811cd490f1.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsong2021scorebased,\ntitle={Score-Based Generative Modeling through Stochastic Differential Equations},\nauthor={Yang Song and Jascha Sohl-Dickstein and Diederik P Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=PxTIG12RRHS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PxTIG12RRHS", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2561/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2561/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2561/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2561/Authors|ICLR.cc/2021/Conference/Paper2561/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2561/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846927, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2561/-/Official_Comment"}}}, {"id": "bHEI3Z3LHSJ", "original": null, "number": 7, "cdate": 1606239260596, "ddate": null, "tcdate": 1606239260596, "tmdate": 1606239302752, "tddate": null, "forum": "PxTIG12RRHS", "replyto": "43rVB1LO1z", "invitation": "ICLR.cc/2021/Conference/Paper2561/-/Official_Comment", "content": {"title": "Continued", "comment": "**Q: Why do you think the FID values for the PC sampler with corrector are higher than without it for VP SDE? (Table 1) Why does a full PC sampler (with predictor and corrector) not incur extra computation in comparison to prediction-only approaches?**\n\nA: We have updated Table 1 with new results and better presentation (see C in [this response](https://openreview.net/forum?id=PxTIG12RRHS&noteId=SGRrXQmYuQs)). For both VP and VE SDEs, adding one corrector step always helps (see PC1000 vs. P1000) but also doubles computation. \n\nTo match the computation, predictor-only methods need to evaluate the score-based model at more noise scales than those seen at training, which can be accomplished by interpolating between noise scales in an ad hoc way. The performance of predictor-corrector vs. predictor-only with doubled noise scales (PC1000 vs. P2000) depends on many factors, such as the predictor, SDE, and interpolation methods. We experimented with two interpolation strategies, the first leads to results in Table 1 and the latter leads to the newly-added Table 4. For VE SDE, predictor-corrector uniformly outperforms predictor-only with doubled noise scales. For VP SDE results in Table 1, even though predictor-corrector has slightly worse performance than predictor-only for some predictors, it can achieve better performance than all other samplers when using the probability flow predictor; while for VP SDE results in Table 4, predictor-corrector samplers are uniformly better.\n\nThese results indicate that predictor-corrector methods (PC1000) can be beneficial for both SDEs to various extent, even compared to predictor-only methods with doubled noise scales and matched computation (P2000).\n\n\n**Q: Does it mean that ODE (12) and reverse SDE (10) (now Eq. (6)) map the same noise distribution $p_T(\\mathbf{x}(T))$ to the same distribution in the data space? What\u2019s the advantage of reverse SDE?**\n\nA: Yes. Theoretically they map the same noise distribution to the same data distribution. In practice solving the reverse SDE for sampling may lead to better performance. For example, in Table 2, the black-box ODE sampler obtained an FID of 3.77 (DDPM (probability flow)), whereas the ancestral sampling sampler (a predictor-only sampler that solves the reverse VP SDE) obtained an FID of 3.17 (DDPM ($L_\\text{simple}$)). In general, we empirically find that more accurately solving the reverse SDE or ODE does not always lead to improvements in sample quality and different approximate sampling schemes (such as including a corrector) could potentially improve performance.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2561/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2561/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Score-Based Generative Modeling through Stochastic Differential Equations", "authorids": ["~Yang_Song1", "~Jascha_Sohl-Dickstein2", "~Diederik_P_Kingma1", "~Abhishek_Kumar1", "~Stefano_Ermon1", "~Ben_Poole1"], "authors": ["Yang Song", "Jascha Sohl-Dickstein", "Diederik P Kingma", "Abhishek Kumar", "Stefano Ermon", "Ben Poole"], "keywords": ["generative models", "score-based generative models", "stochastic differential equations", "score matching", "diffusion"], "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. \nCrucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of $1024\\times 1024$ images for the first time from a score-based generative model.", "one-sentence_summary": "A general framework for training and sampling from score-based models that unifies and generalizes previous methods, allows likelihood computation, and enables controllable generation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "song|scorebased_generative_modeling_through_stochastic_differential_equations", "pdf": "/pdf/ef0eadbe07115b0853e964f17aa09d811cd490f1.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsong2021scorebased,\ntitle={Score-Based Generative Modeling through Stochastic Differential Equations},\nauthor={Yang Song and Jascha Sohl-Dickstein and Diederik P Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=PxTIG12RRHS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PxTIG12RRHS", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2561/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2561/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2561/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2561/Authors|ICLR.cc/2021/Conference/Paper2561/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2561/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846927, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2561/-/Official_Comment"}}}, {"id": "43rVB1LO1z", "original": null, "number": 6, "cdate": 1606239058100, "ddate": null, "tcdate": 1606239058100, "tmdate": 1606239058100, "tddate": null, "forum": "PxTIG12RRHS", "replyto": "zLwPct3Kc3k", "invitation": "ICLR.cc/2021/Conference/Paper2561/-/Official_Comment", "content": {"title": "Thank you for the questions and comments", "comment": "Thank you for the detailed review and thoughtful feedback. Below we address specific questions.\n\n**Q: The discussion of equivalent neural ODE and its differences to reverse SDE is a bit short.**\n\nA: Thanks for pointing this out. We have added a new section in the appendix (Appendix D) to expand on probability flow ODEs, where we included a detailed description of the derivation (Appendix D.1), likelihood computation (Appendix D.2), and probability flow sampling (Appendix D.3 and D.4). In addition, we provided additional experiments to verify the uniquely identifiable encoding property in Appendix D.5.\n\n**Q: The case of using general SDEs (not only those derived from SMLD and DDPM) is mentioned only briefly.**\n\nA: We have revised the paper to put more weight on the general SDE. In particular, we reordered Section 3 and moved the discussion of general SDEs before the introduction of VE/VP SDEs. Our general framework in Section 3.1-3.3 can be readily applied to SDEs $d\\mathbf{x} = \\mathbf{f}(\\mathbf{x}, t)dt + g(t) d\\mathbf{w}$, when $\\mathbf{f}(\\mathbf{x},t)$ is affine in $\\mathbf{x}$. With a slight modification, we can also use **SDEs of almost any form**, which we discuss in Appendix A. Although we didn\u2019t experiment on other SDEs, the fact that VP and VE SDEs behave differently indicates that exploring other SDEs is likely a valuable future direction.\n\n**Q: Is it correct that the function $\\sigma(t)$ in Eq. (6) is assumed to be monotonic and bounded by $\\sigma_\\text{max}$, while $\\beta(t)$ in Eq. (8) is bounded by 1, but doesn't have to be monotonic?**\n\nA: Yes, $\\sigma(t)$ in Eq. (6) (now Eq. (9)) is monotonic and bounded by $\\sigma_\\text{max}$. For $\\beta(t)$ in Eq. (8) (now Eq. (11)), it doesn\u2019t have to be monotonic. Although $0 < \\beta_i < 1$, the function $\\beta(t)$ is not bounded by 1. This is because $\\beta(\\frac{i-1}{N-1}) = (N-1) \\beta_i$ (see Appendix B), where $N$ denotes the number of noise scales.\n\n**Q:  Are there any assumptions on drift and diffusion function (such as monotonicity or boundedness)?**\n\nA: Yes. In order for the SDE to have a unique strong solution, we require both the drift and diffusion function to be globally Lipschitz in both state and time. We have added this assumption to Section 3.1 (right below Eq. (5)).\n\n**Q: Is it always necessary to have a closed form expression for  $\\nabla_\\mathbf{x} \\log p_{0t} (\\mathbf{x}(t) \\mid \\mathbf{x}(0))$ in order to compute the objective (11) (now (7)), or can it be estimated somehow without it?**\n\nA: Not really. When using sliced score matching instead of denoising score matching in Eq. (11) (now Eq. (7)), we only need to sample from $p_{0t}(\\mathbf{x}(t) \\mid \\mathbf{x}(0))$, which can be easily accomplished by simulating the SDE trajectories with numerical SDE solvers (see Appendix A).\n\nWhen restricted to denoising score matching, we need to know more about $p_{0t}(\\mathbf{x}(t) \\mid \\mathbf{x}(0))$. For SDEs with affine drift functions (like the VE and VP SDEs), $p_{0t}(\\mathbf{x}(t) \\mid \\mathbf{x}(0))$ is always a Gaussian. The mean and variance of this Gaussian can be obtained by solving a system of ODEs and are often in closed-form. For more general SDEs, $p_{0t}(\\mathbf{x}(t) \\mid \\mathbf{x}(0))$ can be obtained in theory by solving the Fokker-Planck equation (Kolmogorov\u2019s forward equation). We have added this discussion to Section 3.3. No matter whether we solve for $p_{0t}(\\mathbf{x}(t) \\mid \\mathbf{x}(0))$ in closed-form or with numerical methods, we only need to do it once. The solution can be reused when applying the same SDE to different datasets or training score-based models with different architectures. We consider developing faster and better estimators for $p_{0t}(\\mathbf{x}(t) \\mid \\mathbf{x}(0))$ an important direction for future research."}, "signatures": ["ICLR.cc/2021/Conference/Paper2561/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2561/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Score-Based Generative Modeling through Stochastic Differential Equations", "authorids": ["~Yang_Song1", "~Jascha_Sohl-Dickstein2", "~Diederik_P_Kingma1", "~Abhishek_Kumar1", "~Stefano_Ermon1", "~Ben_Poole1"], "authors": ["Yang Song", "Jascha Sohl-Dickstein", "Diederik P Kingma", "Abhishek Kumar", "Stefano Ermon", "Ben Poole"], "keywords": ["generative models", "score-based generative models", "stochastic differential equations", "score matching", "diffusion"], "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. \nCrucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of $1024\\times 1024$ images for the first time from a score-based generative model.", "one-sentence_summary": "A general framework for training and sampling from score-based models that unifies and generalizes previous methods, allows likelihood computation, and enables controllable generation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "song|scorebased_generative_modeling_through_stochastic_differential_equations", "pdf": "/pdf/ef0eadbe07115b0853e964f17aa09d811cd490f1.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsong2021scorebased,\ntitle={Score-Based Generative Modeling through Stochastic Differential Equations},\nauthor={Yang Song and Jascha Sohl-Dickstein and Diederik P Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=PxTIG12RRHS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PxTIG12RRHS", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2561/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2561/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2561/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2561/Authors|ICLR.cc/2021/Conference/Paper2561/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2561/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846927, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2561/-/Official_Comment"}}}, {"id": "uiWg7hSErOY", "original": null, "number": 5, "cdate": 1606238569855, "ddate": null, "tcdate": 1606238569855, "tmdate": 1606238624437, "tddate": null, "forum": "PxTIG12RRHS", "replyto": "7-AEi3izVVK", "invitation": "ICLR.cc/2021/Conference/Paper2561/-/Official_Comment", "content": {"title": "Thank you for the feedback", "comment": "Thank you for the detailed review and thoughtful feedback. Below we address specific questions.\n\n**Q: How to transform the reverse SDE into the probability flow ODE?**\n\nA: We have included a detailed derivation in Appendix D.1. The proof is based on the Fokker-Planck equation. The probability flow ODE defines a deterministic process that has the same marginal probability densities as the SDE, and thus can be used to generate samples from the same distribution as the SDE.\n\n**Q:  The discussion on the benefits/downsides of different SDE solvers, discretization time steps.**\n\nA: In this paper we mainly focus on demonstrating the flexibility of sampling algorithms allowed by our SDE framework. We compare different sampling methods (which correspond to different discretizations) in Table 1 and Table 2, and demonstrate qualitatively in Figure 4 that probability flow with black-box ODE solvers can save 90% computation without significantly hurting sample quality. We agree with the reviewer that a more detailed quantitative analysis on the merits and limitations of various samplers are important research questions, and consider it to be a major direction for future work.\n\n**Q: The paper lacks a \"toy example\" experiment.**\n\nA: Great suggestion! We added a toy example in Figure 2 to illustrate the main components of our framework, and demonstrated the trajectories of SDEs and probability flow ODEs.\n\n**Q: Typos and writing suggestions.**\n\nA: Thanks for pointing these out! We have fixed the typo and bolded best performing rows in Table 1.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2561/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2561/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Score-Based Generative Modeling through Stochastic Differential Equations", "authorids": ["~Yang_Song1", "~Jascha_Sohl-Dickstein2", "~Diederik_P_Kingma1", "~Abhishek_Kumar1", "~Stefano_Ermon1", "~Ben_Poole1"], "authors": ["Yang Song", "Jascha Sohl-Dickstein", "Diederik P Kingma", "Abhishek Kumar", "Stefano Ermon", "Ben Poole"], "keywords": ["generative models", "score-based generative models", "stochastic differential equations", "score matching", "diffusion"], "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. \nCrucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of $1024\\times 1024$ images for the first time from a score-based generative model.", "one-sentence_summary": "A general framework for training and sampling from score-based models that unifies and generalizes previous methods, allows likelihood computation, and enables controllable generation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "song|scorebased_generative_modeling_through_stochastic_differential_equations", "pdf": "/pdf/ef0eadbe07115b0853e964f17aa09d811cd490f1.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsong2021scorebased,\ntitle={Score-Based Generative Modeling through Stochastic Differential Equations},\nauthor={Yang Song and Jascha Sohl-Dickstein and Diederik P Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=PxTIG12RRHS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PxTIG12RRHS", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2561/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2561/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2561/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2561/Authors|ICLR.cc/2021/Conference/Paper2561/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2561/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846927, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2561/-/Official_Comment"}}}, {"id": "jpryUM7w7ml", "original": null, "number": 4, "cdate": 1606238409705, "ddate": null, "tcdate": 1606238409705, "tmdate": 1606238409705, "tddate": null, "forum": "PxTIG12RRHS", "replyto": "qL0RSCfEg8t", "invitation": "ICLR.cc/2021/Conference/Paper2561/-/Official_Comment", "content": {"title": "Thank you for the questions and feedback", "comment": "Thank you for the detailed review and thoughtful feedback. Below we address specific questions.\n\n**Q: To what degree the framework unifies previous approaches. VE and VP SDEs are handled separately throughout.**\n\nA: SMLD and DDPM, the two previous approaches, are both discretizations of SDEs (VE and VP SDE respectively) in our framework. Their original sampling algorithms are special cases of our predictor-corrector sampler. Specifically, SMLD uses an empty predictor and a Langevin dynamics corrector. DDPM uses an ancestral sampling predictor and an empty corrector.\n\nWe would like to clarify that VE and VP SDEs are just two instantiations of our SDE-based framework of generative modeling. Although VE and VP SDEs result in seemingly different sampling algorithms, they are powered by the same idea at the core. The predictor-corrector samplers for VE and VP SDEs in Algorithm 1 and 2 both consist of a reverse diffusion predictor and a Langevin dynamics corrector, where the former can be derived by following the same general procedure in Appendix E, and the latter is shared. \n\nTo improve clarity, we have reordered Section 3 and put the general framework before the discussion of VE/VP SDEs. The section for VE/VP SDEs is now titled \u201cSpecial cases: VE and VP SDEs\u201d to emphasize their relationship to the general framework.\n\n**Q: Results are best when computation is split between the predictor and corrector?**\n\nA: By \u201csplit\u201d, we mean that computation is allocated to both the predictor and corrector and not to one alone. We agree an even split may not be optimal, and $M$ should be tuned for the best performance. We have rephrased our text according to your suggestion.\n\n**Q: Numerical indication of how a practitioner could notice that running for more steps would or would not help?**\n\nA: In the current work we tune the number of steps by visually inspecting the generated samples. We agree that a numerical indication would be very useful for tuning the sampling algorithms and is a very interesting direction for future research.\n\n**Q: What is the last row in Table 1? I guess it's just employing the corrector.**\n\nA: You are correct, it is the corrector-only sampler. We have re-organized the table to make this more clear.\n\n**Q: In Table 1, it looks like the corrector helps for VE SDEs, but hurts for VP SDEs.**\n\nA: We have updated Table 1 with new results and better presentation (see C in [this response](https://openreview.net/forum?id=PxTIG12RRHS&noteId=SGRrXQmYuQs)). For both VP and VE SDEs, adding one corrector step always helps (see PC1000 vs. P1000) but also doubles computation. \n\nTo match the computation, predictor-only methods need to evaluate the score-based model at more noise scales than those seen at training, which can be accomplished by interpolating between noise scales in an ad hoc way. The performance of predictor-corrector vs. predictor-only with doubled noise scales (PC1000 vs. P2000) depends on many factors, such as the predictor, SDE, and interpolation methods. We experimented with two interpolation strategies, the first leads to results in Table 1 and the latter leads to the newly-added Table 4. For VE SDE, predictor-corrector uniformly outperforms predictor-only with doubled noise scales. For VP SDE results in Table 1, even though predictor-corrector has slightly worse performance than predictor-only for some predictors, it can achieve better performance than all other samplers when using the probability flow predictor; while for VP SDE results in Table 4, predictor-corrector samplers are uniformly better.\n\nThese results indicate that predictor-corrector methods (PC1000) can be beneficial for both SDEs to various extent, even compared to predictor-only methods with doubled noise scales and matched computation (P2000).\n\n**Q: Is it possible to compute the exact NLL for any real dataset, like a test dataset?**\n\nA: Yes. Our results in Table 2 are NLL numbers on the CIFAR-10 test dataset. We can compute the exact NLL for any input data, just like normalizing flow models and neural ODEs. We have made this point more clear in Section 4.3.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2561/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2561/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Score-Based Generative Modeling through Stochastic Differential Equations", "authorids": ["~Yang_Song1", "~Jascha_Sohl-Dickstein2", "~Diederik_P_Kingma1", "~Abhishek_Kumar1", "~Stefano_Ermon1", "~Ben_Poole1"], "authors": ["Yang Song", "Jascha Sohl-Dickstein", "Diederik P Kingma", "Abhishek Kumar", "Stefano Ermon", "Ben Poole"], "keywords": ["generative models", "score-based generative models", "stochastic differential equations", "score matching", "diffusion"], "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. \nCrucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of $1024\\times 1024$ images for the first time from a score-based generative model.", "one-sentence_summary": "A general framework for training and sampling from score-based models that unifies and generalizes previous methods, allows likelihood computation, and enables controllable generation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "song|scorebased_generative_modeling_through_stochastic_differential_equations", "pdf": "/pdf/ef0eadbe07115b0853e964f17aa09d811cd490f1.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsong2021scorebased,\ntitle={Score-Based Generative Modeling through Stochastic Differential Equations},\nauthor={Yang Song and Jascha Sohl-Dickstein and Diederik P Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=PxTIG12RRHS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PxTIG12RRHS", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2561/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2561/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2561/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2561/Authors|ICLR.cc/2021/Conference/Paper2561/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2561/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846927, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2561/-/Official_Comment"}}}, {"id": "SGRrXQmYuQs", "original": null, "number": 3, "cdate": 1606237575601, "ddate": null, "tcdate": 1606237575601, "tmdate": 1606237678746, "tddate": null, "forum": "PxTIG12RRHS", "replyto": "PxTIG12RRHS", "invitation": "ICLR.cc/2021/Conference/Paper2561/-/Official_Comment", "content": {"title": "A summary of updates", "comment": "We would like to thank all reviewers for providing high quality reviews and constructive feedback that have improved the paper. We are encouraged that reviewers think our paper \u201cmakes significant technical and empirical contributions to the emerging area of score-based generative models\u201d (R3); our proposed model and sampling algorithms \u201coffer substantial conceptual improvements to the existing models\u201d (R4); our empirical evaluation is \u201cextensive\u201d (R4),  \u201cparticularly well-done\u201d (R3), \u201ca nice evaluation of theoretical claims\u201d (R2); and our writeup \u201cwell-written\u201d (R1, R4), \u201cwell-structured\u201d (R4) and \u201cwas able to follow all without having neural SDEs or Langevin Samplers as a core competence\u201d (R1).\n\nWe have updated our draft to further improve the writing and incorporate suggestions from reviewers, extended the appendix with more details for reproducibility, and will be releasing code and model checkpoints. Below, we summarize changes made in the updated submission.\n\n### A. New toy example figure\n\nWe have added a new figure (Fig. 2 to Section 3) as suggested by R2. It depicts the SDE and ODE trajectories for transforming a one-dimensional toy data distribution to a standard Gaussian. We believe it provides a good overview of our score-based generative modeling framework with SDEs, and contrasts the trajectories of the SDE and ODE.\n\n### B. Clarifying how our general SDE framework encapsulates VE/VP SDE.\n\nIn response to R1 and R4 on how our framework unifies VE and VP SDEs, we have improved the text in Section 3. We now present our general SDE framework before the introduction of VE and VP SDEs, and make it more clear that they are two particular instantiations.\n\n### C. Improved comparison of different samplers\n\nWe have improved the clarity of Table 1, provided additional results, and highlighted the computational cost of each sampler. These results show that adding 1 corrector step for each predictor step (PC1000) doubles computation but always improves performance (against P1000). Moreover, it is typically better than doubling the number of predictor steps (P2000), where we have to interpolate between noise scales in an ad hoc manner (detailed in Appendix G) for SMLD/DDPM models. Depending on the interpolation method, predictor-corrector methods may uniformly outperform predictor-only samplers with the same computation (newly-added results in Table 4, PC1000 vs. P2000).\n\nWe also improved the results of corrector-only samplers (C2000) by using 2 corrector steps per noise scale, instead of 1 corrector step with interpolated noise scales. Predictor-corrector samplers (PC1000) still perform uniformly better than corrector-only ones.\n\n### D. Additional details on probability flow ODEs\n\nWe have polished the writing of Section 4.3. In addition, we added Appendix D, where we provide a detailed derivation of the probability flow ODE (Appendix D.1), full description of likelihood computation (Appendix D.2), detailed description of sampling with probability flows (Appendix D.3 and D.4), as well as additional experimental results for uniquely identifiable encoding (Appendix D.5, Figs. 8 and 9). \n\n### E. Updating results of previous work in Table 2 and 3.\n\nStyleGAN2-ADA authors updated their FID and inception scores after our paper submission, and we have incorporated these new results into Table 3. Our results are still state-of-the-art, though the gap is smaller. We also added results from NCSNv2 to Table 2, and results of RealNVP and iResNet to Table 3.\n\n### F. Inpainting and colorization on $256\\times 256$ images.\n\nWe have included inpainting and colorization results on $256\\times 256$ images from LSUN bedroom and church_outdoor. We updated Section 5, Fig. 5 and provided extended samples in Figs. 13-16 in Appendix I. These results present the same qualitative findings as the original submission with CIFAR-10 figures, but highlight that the method works well on higher resolution images too!\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2561/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2561/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Score-Based Generative Modeling through Stochastic Differential Equations", "authorids": ["~Yang_Song1", "~Jascha_Sohl-Dickstein2", "~Diederik_P_Kingma1", "~Abhishek_Kumar1", "~Stefano_Ermon1", "~Ben_Poole1"], "authors": ["Yang Song", "Jascha Sohl-Dickstein", "Diederik P Kingma", "Abhishek Kumar", "Stefano Ermon", "Ben Poole"], "keywords": ["generative models", "score-based generative models", "stochastic differential equations", "score matching", "diffusion"], "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. \nCrucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of $1024\\times 1024$ images for the first time from a score-based generative model.", "one-sentence_summary": "A general framework for training and sampling from score-based models that unifies and generalizes previous methods, allows likelihood computation, and enables controllable generation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "song|scorebased_generative_modeling_through_stochastic_differential_equations", "pdf": "/pdf/ef0eadbe07115b0853e964f17aa09d811cd490f1.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsong2021scorebased,\ntitle={Score-Based Generative Modeling through Stochastic Differential Equations},\nauthor={Yang Song and Jascha Sohl-Dickstein and Diederik P Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=PxTIG12RRHS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PxTIG12RRHS", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2561/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2561/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2561/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2561/Authors|ICLR.cc/2021/Conference/Paper2561/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2561/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846927, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2561/-/Official_Comment"}}}, {"id": "7-AEi3izVVK", "original": null, "number": 3, "cdate": 1603833324695, "ddate": null, "tcdate": 1603833324695, "tmdate": 1605024182060, "tddate": null, "forum": "PxTIG12RRHS", "replyto": "PxTIG12RRHS", "invitation": "ICLR.cc/2021/Conference/Paper2561/-/Official_Review", "content": {"title": "Fantastic Paper!", "review": "Summary: This paper presents a generative model based on stochastic differential equations (SDEs), which generalizes two other score-based generative models score matching with Langevin dynamics (SMLD) and denoising diffusion probabilistic modeling (DDPM). The recipe to sample from the data distribution is based on (i) the observation that both SMLD and DDPM can be formulated as the discretization of an SDE, (ii) the finding from Anderson (1982) about the reverse of an Ito process, (iii) a score model. A novel aspect of the presented technique is the use of the score model as \"predictor\", which gives the initial sample from the MCMC sampler that serves as \"corrector\". Finally, the Ito process induced by the reverse SDE is formulated in a deterministic manner, leading to a neural-ODE based generative model.\n\nPros:\n- The authors did a good job at showing connections between the previous score-based generative models and their model. I believe on its own this is a nice contribution.\n- The method is thoroughly analyzed. I went through the derivations and didn't find any errors.\n- Experiments show that combining the predictor and corrector routines leads to better performance, a nice validation of the theoretical claims.\n- As promised, the model achieves SOTA on several tasks.\n\nCons:\n- I'm having difficulty seeing the transformation of the reverse SDE into an ODE (from eq.10 to eq.12). Is it as simple as multiplying the second term with 1/2 and discarding the Brownian motion? Also, eq.12 is a simple ODE system, which has nothing to with a process as far as I understand. Maybe more explanation or pointers in Maoutsa et al., 2020 would be nice.\n- The paper lacks the discussion on the benefits/downsides of different SDE solvers, discretization time steps, etc.\n- As such, the paper lacks a \"toy example\" experiment, for example, on a simple 2D dataset like half-moons. A visual demonstration of the SDEs and probability flow (maybe corresponding vector fields and Brownian motion over time) would be interesting.\n\nAdditional comments:\n- A typo (intead) right below eq.9\n- Best performing rows can be bold in Table 1.", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2561/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2561/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Score-Based Generative Modeling through Stochastic Differential Equations", "authorids": ["~Yang_Song1", "~Jascha_Sohl-Dickstein2", "~Diederik_P_Kingma1", "~Abhishek_Kumar1", "~Stefano_Ermon1", "~Ben_Poole1"], "authors": ["Yang Song", "Jascha Sohl-Dickstein", "Diederik P Kingma", "Abhishek Kumar", "Stefano Ermon", "Ben Poole"], "keywords": ["generative models", "score-based generative models", "stochastic differential equations", "score matching", "diffusion"], "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. \nCrucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of $1024\\times 1024$ images for the first time from a score-based generative model.", "one-sentence_summary": "A general framework for training and sampling from score-based models that unifies and generalizes previous methods, allows likelihood computation, and enables controllable generation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "song|scorebased_generative_modeling_through_stochastic_differential_equations", "pdf": "/pdf/ef0eadbe07115b0853e964f17aa09d811cd490f1.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsong2021scorebased,\ntitle={Score-Based Generative Modeling through Stochastic Differential Equations},\nauthor={Yang Song and Jascha Sohl-Dickstein and Diederik P Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=PxTIG12RRHS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "PxTIG12RRHS", "replyto": "PxTIG12RRHS", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2561/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538093556, "tmdate": 1606915771842, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2561/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2561/-/Official_Review"}}}, {"id": "zLwPct3Kc3k", "original": null, "number": 2, "cdate": 1603714694540, "ddate": null, "tcdate": 1603714694540, "tmdate": 1605024181996, "tddate": null, "forum": "PxTIG12RRHS", "replyto": "PxTIG12RRHS", "invitation": "ICLR.cc/2021/Conference/Paper2561/-/Official_Review", "content": {"title": "Interesting generative model and good paper", "review": "SUMMARY\n\nThe submission proposes a score-based generative model, which uses an SDE to map the\ndata distribution to a simple noise distribution and the corresponding reverse-time SDE to\ngenerate samples by mapping the noise to the data space. The proposed model builds\nupon and generalises two existing models (SMLD and DDPM) by transforming the data\nusing a continuous SDE dynamics as opposed to perturbing the data with a finite number\nof noise distributions utilized by these models.\n\n##################################################################\n\nREASON FOR SCORE\n\nThe paper provides a clear motivation for the proposed modifications to SMLD and DDPM,\nas well as a clear technical description of these modifications, their analysis and discussion.\nI think the proposed model and sampling algorithms offer substantial conceptual\nimprovements to the existing models and should be of interest to the community. The\npaper is well written and structured.\n\n##################################################################\n\nPROS\n\n+ Clear motivation for the work.\n+ Detailed technical description of the proposed models.\n+ Interesting discussion of similarities and differences between SMLD, DDPM, and the SDE\nbased model, as well as corresponding sampling algorithms.\n+ Extensive experiments.\n\n##################################################################\n\nCONS\n\n- I found the discussion of equivalent neural ODE and its differences to reverse SDE\n a bit short, especially given that it is used in multiple experiments.\n- The case of using general SDEs (not only those derived from SMLD and DDPM) is\nmentioned only briefly, leaving it unclear if using a general SDE would require\nrelatively simple changes, or if the proposed model is limited to SDEs derived from\nSMLD and DDPM.\n\n##################################################################\n\nQUESTIONS and COMMENTS\n\n- Is it correct that the function \\sigma(t) in Eq. (6) is assumed to be monotonic\nand bounded by \\sigma_max, while \\beta(t) in Eq. (8) is bounded by 1, but doesn't have to\nbe monotonic?\n\n- In the case of general SDE for noise perturbations in Eq. (9), are there any assumptions on\ndrift and diffusion function (such as monotonicity or boundedness)?\n\n- In the case of SDEs (6) and (8), \\nabla_x p_{0t} (x(t)) is available in closed-form. Is it always\nnecessary to have such a closed form expression in order to compute the objective (11), or\ncan it be estimated somehow without it? (I guess for a general SDE, there is typically no\nclosed-form \\nabla_x p_{0t} (x(t)) available)\n\n- Why do you think the FID values for the PC sampler with corrector are higher than without it\nfor VP SDE? (Table 1b)\n\n- In section 4.2: \"[...] PC samplers significantly outperform the corrector-only method, and can\nimprove over predictor-only approaches for most cases without extra computation.\" Why does\nfull PC sampler (with predictor and corrector) not incur extra computation in comparison to\nprediction-only approaches. Don't we need to evaluate the approximate score function\ns_\\theta(x, i) for each of the M steps in the corrector sampler?\n\n- In section 4.3: \"[...] deterministic process whose trajectories induce the same evolution\nof densities\". Does it mean that ODE (12) and reverse SDE (10) map the same noise\ndistribution p(x(T)) to the same distribution in the data space? If so, are there any\nadvantages of using a reverse SDE instead of equivalent ODE if the latter is easier to solve\nnumerically and admits an exact computation of likelihoods?\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2561/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2561/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Score-Based Generative Modeling through Stochastic Differential Equations", "authorids": ["~Yang_Song1", "~Jascha_Sohl-Dickstein2", "~Diederik_P_Kingma1", "~Abhishek_Kumar1", "~Stefano_Ermon1", "~Ben_Poole1"], "authors": ["Yang Song", "Jascha Sohl-Dickstein", "Diederik P Kingma", "Abhishek Kumar", "Stefano Ermon", "Ben Poole"], "keywords": ["generative models", "score-based generative models", "stochastic differential equations", "score matching", "diffusion"], "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. \nCrucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of $1024\\times 1024$ images for the first time from a score-based generative model.", "one-sentence_summary": "A general framework for training and sampling from score-based models that unifies and generalizes previous methods, allows likelihood computation, and enables controllable generation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "song|scorebased_generative_modeling_through_stochastic_differential_equations", "pdf": "/pdf/ef0eadbe07115b0853e964f17aa09d811cd490f1.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsong2021scorebased,\ntitle={Score-Based Generative Modeling through Stochastic Differential Equations},\nauthor={Yang Song and Jascha Sohl-Dickstein and Diederik P Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=PxTIG12RRHS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "PxTIG12RRHS", "replyto": "PxTIG12RRHS", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2561/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538093556, "tmdate": 1606915771842, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2561/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2561/-/Official_Review"}}}, {"id": "qL0RSCfEg8t", "original": null, "number": 4, "cdate": 1603897537755, "ddate": null, "tcdate": 1603897537755, "tmdate": 1605024181854, "tddate": null, "forum": "PxTIG12RRHS", "replyto": "PxTIG12RRHS", "invitation": "ICLR.cc/2021/Conference/Paper2561/-/Official_Review", "content": {"title": "Review", "review": "This paper generalizes a family of score-based generative models that rely on sequences of noise scalings of the data and extends them to the continuous domain, which leads to an SDE-based framework. By using score-matching, the forward SDE, which transforms data into a tractable noise distribution, can be reversed and thus used as a generative model. This is then improved by employing a two-phase algorithm with a prediction step, followed by a tunable number of correction steps. Further, reformulating the problem as a neural ODE allows for exact likelihood computations and reduces the number of required function evaluations. The framework enables unconditional, as well as conditional samples.\n\nI find the paper to be very well written and straightforward. As someone who does not have neural SDEs or Langevin Samplers as a core competence, I was able to follow all of the writeup, which is remarkable. I think the framework is nice and there is substantial novel innovation to justify accepting this paper. The experiments are convincing.\n\nA few questions and remarks:\n- You claim that you unify current methods into a common framework. While I see that you attempt to do this (i.e. putting the algorithms side-by-side, etc.), but in essence, you still handle VE and VP SDEs separately throughout. My suggestion would be to either really try to unify them into a single formulation, or alternatively, tone down the claims of unification, maybe just say that you show commonalities.\n- In Figure 2, you claim that the results are best when computation is \"split\" between the predictor and corrector. However, this is a very imprecise statement. An equal split would mean just 1 step of corrector, but I don't see clear evidence that that's best. Do you have numerical evidence that a 1-to-1 split is best, or what do you mean by \"split\"? Otherwise, you could just say that M is a tunable hyperparameter.\n- Also in Figure 2, it seems that there is a clear shift at some point where the samples go from low to high quality. Do you have any numerical indication (without looking at a test set, FID, etc.) of how a practicioner could notice that running for more steps would or wouldn't help?\n- In Table 1, what is the last row? I guess it's just employing the corrector, but maybe a label for the row would be nice.\n- Also in Table 1, it looks like the corrector helps for VE SDEs, but hurts for VP SDEs. Do you have an explanation for this? Maybe it's somewhere in the text, but if it is, I've missed it, so maybe point me to it.\n- Given that you can compute exact likelihoods, is it possible to compute the exact NLL for any real dataset, like a test dataset?", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2561/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2561/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Score-Based Generative Modeling through Stochastic Differential Equations", "authorids": ["~Yang_Song1", "~Jascha_Sohl-Dickstein2", "~Diederik_P_Kingma1", "~Abhishek_Kumar1", "~Stefano_Ermon1", "~Ben_Poole1"], "authors": ["Yang Song", "Jascha Sohl-Dickstein", "Diederik P Kingma", "Abhishek Kumar", "Stefano Ermon", "Ben Poole"], "keywords": ["generative models", "score-based generative models", "stochastic differential equations", "score matching", "diffusion"], "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. \nCrucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of $1024\\times 1024$ images for the first time from a score-based generative model.", "one-sentence_summary": "A general framework for training and sampling from score-based models that unifies and generalizes previous methods, allows likelihood computation, and enables controllable generation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "song|scorebased_generative_modeling_through_stochastic_differential_equations", "pdf": "/pdf/ef0eadbe07115b0853e964f17aa09d811cd490f1.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsong2021scorebased,\ntitle={Score-Based Generative Modeling through Stochastic Differential Equations},\nauthor={Yang Song and Jascha Sohl-Dickstein and Diederik P Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=PxTIG12RRHS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "PxTIG12RRHS", "replyto": "PxTIG12RRHS", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2561/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538093556, "tmdate": 1606915771842, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2561/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2561/-/Official_Review"}}}], "count": 16}