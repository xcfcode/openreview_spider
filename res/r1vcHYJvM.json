{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124281654, "tcdate": 1518470542651, "number": 283, "cdate": 1518470542651, "id": "r1vcHYJvM", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "r1vcHYJvM", "signatures": ["~Joshua_Romoff1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Reward Estimation for Variance Reduction in Deep Reinforcement Learning", "abstract": "In reinforcement learning (RL), stochastic environments can make learning a policy difficult due to high degrees of variance. As such, variance reduction methods have been investigated in other works, such as advantage estimation and control-variates estimation. Here, we propose to learn a separate reward estimator to train the value function, to help reduce variance caused by a noisy reward signal. This results in theoretical reductions in variance in the tabular case, as well as empirical improvements in both the function approximation and tabular settings in environments where rewards are stochastic. To do so, we use a modified version of Advantage Actor Critic (A2C) on variations of Atari games.", "paperhash": "romoff|reward_estimation_for_variance_reduction_in_deep_reinforcement_learning", "keywords": ["Reinforcement Learning", "Deep Learning"], "_bibtex": "@misc{\n  romoff2018reward,\n  title={Reward Estimation for Variance Reduction in Deep Reinforcement Learning},\n  author={Joshua Romoff and Alexandre Piche and Peter Henderson and Vincent Francois-Lavet and Joelle Pineau},\n  year={2018},\n  url={https://openreview.net/forum?id=r1vcHYJvM}\n}", "authorids": ["joshua.romoff@mail.mcgill.ca", "alexandrelpiche@gmail.com", "peter.henderson@mail.mcgill.ca", "vincent.francois-lavet@mcgill.ca", "jpineau@cs.mcgill.ca"], "authors": ["Joshua Romoff", "Alexandre Piche", "Peter Henderson", "Vincent Francois-Lavet", "Joelle Pineau"], "TL;DR": "We propose to learn a separate reward estimator to train the value function, to help reduce variance caused by a noisy reward signal.", "pdf": "/pdf/7efc4a1b1807d036d56536aec6f1928a616752bb.pdf"}, "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582658704, "tcdate": 1520775620749, "number": 1, "cdate": 1520775620749, "id": "SkTpW2fFf", "invitation": "ICLR.cc/2018/Workshop/-/Paper283/Official_Review", "forum": "r1vcHYJvM", "replyto": "r1vcHYJvM", "signatures": ["ICLR.cc/2018/Workshop/Paper283/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper283/AnonReviewer3"], "content": {"title": "Interesting preliminary investigation", "rating": "7: Good paper, accept", "review": "The paper proposes and empirically studies a simple idea, learn a reward predictor and replace the true reward in the usual advantage or Bellman equation with the predicted reward. The purpose of this is to lower variance due to randomness in the rewards. There is also a potential for an auxiliary loss benefit due to improved representations from learning to predict reward.\n\nThe authors justify the variance reduction before giving experimental results on a 10 state MDP and 5 Atari games. The reward prediction is very effective at reducing RMSE of the value function approximation in the tabular example, really surprisingly so. In Atari the results are also generally positive but less consistent than for the toy problem. Here, the authors observe that, without additional reward noise, using the predicted reward generally hurts performance. But, when rewards are noisy using the predicted reward improves performance, sometimes very significantly, in some games. There is no improvement over the baseline in QBert and Seaquest, but even in these cases the predicted-reward case seems to suffer less from the noisy rewards than the true-reward case. Perhaps there is a cost in performance due to using predicted rewards, especially prominent in these games, cause by the delayed effect of learning a value function on a learned estimate of reward?\n\nIt is a simple, self-contained, experiment and interesting result. I'm tempted to say that the approach is novel, even though reward prediction as an auxilliary loss is not, and the properties of variance being exploited similarly. There is instead maybe a novel insight being communicated here that we can exploit these methods better in RL.  I appreciated the additional experiments on Atari in the appendix, the performance of Baseline+ was the first thing I wanted to ask about after reading the main text.\n\nIt is strange that the method seems to be so minimally impacted by the reward variance, and that the two games where it performs worst are both helped (at least in the low noise case) by reward prediction as an auxiliary loss.\n\nPros: Simple, clear method, clearly written, doesn't hide behind complex systems so the take-away is accessible. \n\nCons: Results are more mixed on Atari and a better understanding of what is going on with the last two games would be useful. Could do with more work on trade-offs we get when learning on a predicted signal as opposed to the raw signal. Exploring the drawbacks to this versus the variance reduction benefits.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reward Estimation for Variance Reduction in Deep Reinforcement Learning", "abstract": "In reinforcement learning (RL), stochastic environments can make learning a policy difficult due to high degrees of variance. As such, variance reduction methods have been investigated in other works, such as advantage estimation and control-variates estimation. Here, we propose to learn a separate reward estimator to train the value function, to help reduce variance caused by a noisy reward signal. This results in theoretical reductions in variance in the tabular case, as well as empirical improvements in both the function approximation and tabular settings in environments where rewards are stochastic. To do so, we use a modified version of Advantage Actor Critic (A2C) on variations of Atari games.", "paperhash": "romoff|reward_estimation_for_variance_reduction_in_deep_reinforcement_learning", "keywords": ["Reinforcement Learning", "Deep Learning"], "_bibtex": "@misc{\n  romoff2018reward,\n  title={Reward Estimation for Variance Reduction in Deep Reinforcement Learning},\n  author={Joshua Romoff and Alexandre Piche and Peter Henderson and Vincent Francois-Lavet and Joelle Pineau},\n  year={2018},\n  url={https://openreview.net/forum?id=r1vcHYJvM}\n}", "authorids": ["joshua.romoff@mail.mcgill.ca", "alexandrelpiche@gmail.com", "peter.henderson@mail.mcgill.ca", "vincent.francois-lavet@mcgill.ca", "jpineau@cs.mcgill.ca"], "authors": ["Joshua Romoff", "Alexandre Piche", "Peter Henderson", "Vincent Francois-Lavet", "Joelle Pineau"], "TL;DR": "We propose to learn a separate reward estimator to train the value function, to help reduce variance caused by a noisy reward signal.", "pdf": "/pdf/7efc4a1b1807d036d56536aec6f1928a616752bb.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582658474, "id": "ICLR.cc/2018/Workshop/-/Paper283/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper283/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper283/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper283/AnonReviewer2"], "reply": {"forum": "r1vcHYJvM", "replyto": "r1vcHYJvM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper283/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper283/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582658474}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582637409, "tcdate": 1520810756206, "number": 2, "cdate": 1520810756206, "id": "rk2WoE7KM", "invitation": "ICLR.cc/2018/Workshop/-/Paper283/Official_Review", "forum": "r1vcHYJvM", "replyto": "r1vcHYJvM", "signatures": ["ICLR.cc/2018/Workshop/Paper283/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper283/AnonReviewer2"], "content": {"title": "Obvious but makes reward non stationary", "rating": "5: Marginally below acceptance threshold", "review": "The authors propose to learn the average reward per state in case the reward is stochastic and use this estimate instead of the actual reward in an actor critic architecture. They show it theoretically reduces variance of the value estimate. \n\nFirst, I thing this idea is quite obvious. The average reward is actually what you need for the Bellman equation to hold so trying to predict that value is an obvious thing one would do if the state space is small enough.\n\nYet in the case of large state space, estimating the average can certainly take time and this method will make the estimated reward non stationary instead of just stochastic. The impact on stability might actually be worse than stochasticity. \n\nThe experiments are not very convincing as Gaussian noise is artificially added to Atari games natural rewards. It's a strong assumption that reward would have Gaussian distributions which is of course in favor of the method.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reward Estimation for Variance Reduction in Deep Reinforcement Learning", "abstract": "In reinforcement learning (RL), stochastic environments can make learning a policy difficult due to high degrees of variance. As such, variance reduction methods have been investigated in other works, such as advantage estimation and control-variates estimation. Here, we propose to learn a separate reward estimator to train the value function, to help reduce variance caused by a noisy reward signal. This results in theoretical reductions in variance in the tabular case, as well as empirical improvements in both the function approximation and tabular settings in environments where rewards are stochastic. To do so, we use a modified version of Advantage Actor Critic (A2C) on variations of Atari games.", "paperhash": "romoff|reward_estimation_for_variance_reduction_in_deep_reinforcement_learning", "keywords": ["Reinforcement Learning", "Deep Learning"], "_bibtex": "@misc{\n  romoff2018reward,\n  title={Reward Estimation for Variance Reduction in Deep Reinforcement Learning},\n  author={Joshua Romoff and Alexandre Piche and Peter Henderson and Vincent Francois-Lavet and Joelle Pineau},\n  year={2018},\n  url={https://openreview.net/forum?id=r1vcHYJvM}\n}", "authorids": ["joshua.romoff@mail.mcgill.ca", "alexandrelpiche@gmail.com", "peter.henderson@mail.mcgill.ca", "vincent.francois-lavet@mcgill.ca", "jpineau@cs.mcgill.ca"], "authors": ["Joshua Romoff", "Alexandre Piche", "Peter Henderson", "Vincent Francois-Lavet", "Joelle Pineau"], "TL;DR": "We propose to learn a separate reward estimator to train the value function, to help reduce variance caused by a noisy reward signal.", "pdf": "/pdf/7efc4a1b1807d036d56536aec6f1928a616752bb.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582658474, "id": "ICLR.cc/2018/Workshop/-/Paper283/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper283/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper283/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper283/AnonReviewer2"], "reply": {"forum": "r1vcHYJvM", "replyto": "r1vcHYJvM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper283/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper283/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582658474}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573572145, "tcdate": 1521573572145, "number": 128, "cdate": 1521573571808, "id": "H12TCC0FG", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "r1vcHYJvM", "replyto": "r1vcHYJvM", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reward Estimation for Variance Reduction in Deep Reinforcement Learning", "abstract": "In reinforcement learning (RL), stochastic environments can make learning a policy difficult due to high degrees of variance. As such, variance reduction methods have been investigated in other works, such as advantage estimation and control-variates estimation. Here, we propose to learn a separate reward estimator to train the value function, to help reduce variance caused by a noisy reward signal. This results in theoretical reductions in variance in the tabular case, as well as empirical improvements in both the function approximation and tabular settings in environments where rewards are stochastic. To do so, we use a modified version of Advantage Actor Critic (A2C) on variations of Atari games.", "paperhash": "romoff|reward_estimation_for_variance_reduction_in_deep_reinforcement_learning", "keywords": ["Reinforcement Learning", "Deep Learning"], "_bibtex": "@misc{\n  romoff2018reward,\n  title={Reward Estimation for Variance Reduction in Deep Reinforcement Learning},\n  author={Joshua Romoff and Alexandre Piche and Peter Henderson and Vincent Francois-Lavet and Joelle Pineau},\n  year={2018},\n  url={https://openreview.net/forum?id=r1vcHYJvM}\n}", "authorids": ["joshua.romoff@mail.mcgill.ca", "alexandrelpiche@gmail.com", "peter.henderson@mail.mcgill.ca", "vincent.francois-lavet@mcgill.ca", "jpineau@cs.mcgill.ca"], "authors": ["Joshua Romoff", "Alexandre Piche", "Peter Henderson", "Vincent Francois-Lavet", "Joelle Pineau"], "TL;DR": "We propose to learn a separate reward estimator to train the value function, to help reduce variance caused by a noisy reward signal.", "pdf": "/pdf/7efc4a1b1807d036d56536aec6f1928a616752bb.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 4}