{"notes": [{"id": "BkgMbCVFvr", "original": "rJg2dl4_wB", "number": 957, "cdate": 1569439226089, "ddate": null, "tcdate": 1569439226089, "tmdate": 1577168236412, "tddate": null, "forum": "BkgMbCVFvr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Pretraining boosts out-of-domain robustness for pose estimation", "authors": ["Alexander Mathis", "Mert Y\u00fcksekg\u00f6n\u00fcl", "Byron Rogers", "Matthias Bethge", "Mackenzie W. Mathis"], "authorids": ["amathis@fas.harvard.edu", "mertyuksekgonul@gmail.com", "byron@performancegenetics.com", "matthias.bethge@uni-tuebingen.de", "mathis@rowland.harvard.edu"], "keywords": ["pose estimation", "robustness", "out-of-domain", "transfer learning"], "TL;DR": "Transfer learning boosts out-of-domain robustness for pose estimation.", "abstract": "Deep neural networks are highly effective tools for human and animal pose estimation. However, robustness to out-of-domain data remains a challenge. Here, we probe the transfer and generalization ability for pose estimation with two architecture classes (MobileNetV2s and ResNets) pretrained on ImageNet. We generated a novel dataset of 30 horses that allowed for both within-domain and out-of-domain (unseen horse) testing. We find that pretraining on ImageNet strongly improves out-of-domain performance. Moreover, we show that for both pretrained and networks trained from scratch, better ImageNet-performing architectures perform better for pose estimation, with a substantial improvement on out-of-domain data when pretrained. Collectively, our results demonstrate that transfer learning is particularly beneficial for out-of-domain robustness.", "pdf": "/pdf/029ceb081cf7fefe7584a6e6028574f6bd38b76c.pdf", "paperhash": "mathis|pretraining_boosts_outofdomain_robustness_for_pose_estimation", "original_pdf": "/attachment/029ceb081cf7fefe7584a6e6028574f6bd38b76c.pdf", "_bibtex": "@misc{\nmathis2020pretraining,\ntitle={Pretraining boosts out-of-domain robustness for pose estimation},\nauthor={Alexander Mathis and Mert Y{\\\"u}ksekg{\\\"o}n{\\\"u}l and Byron Rogers and Matthias Bethge and Mackenzie W. Mathis},\nyear={2020},\nurl={https://openreview.net/forum?id=BkgMbCVFvr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "VRWzAXKCba", "original": null, "number": 1, "cdate": 1576798710693, "ddate": null, "tcdate": 1576798710693, "tmdate": 1576800925643, "tddate": null, "forum": "BkgMbCVFvr", "replyto": "BkgMbCVFvr", "invitation": "ICLR.cc/2020/Conference/Paper957/-/Decision", "content": {"decision": "Reject", "comment": "The paper presents a new dataset, containing around 8k pictures of 30 horses in different poses. This is used to study the benefits of pretraining for in- and out-of-domain images.\n\nThe paper is somewhat lacking in novelty. Others have studied the same type of pre-training in the past using other datasets, which makes the dataset the main novelty. But reviewers raised many questions about the dataset, in particular about how many of the frames of the same horse might be similar, and of how few horses there are; few enough to potentially not make the results statistically meaningful. The authors replied to these questions more by appealing to standards in other fields than by explaining why this is a good choice. Apart from these crucial weaknesses, however, the research appears good.\n\nThis is a pretty clear reject based on lack of novelty and oddities with the dataset.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pretraining boosts out-of-domain robustness for pose estimation", "authors": ["Alexander Mathis", "Mert Y\u00fcksekg\u00f6n\u00fcl", "Byron Rogers", "Matthias Bethge", "Mackenzie W. Mathis"], "authorids": ["amathis@fas.harvard.edu", "mertyuksekgonul@gmail.com", "byron@performancegenetics.com", "matthias.bethge@uni-tuebingen.de", "mathis@rowland.harvard.edu"], "keywords": ["pose estimation", "robustness", "out-of-domain", "transfer learning"], "TL;DR": "Transfer learning boosts out-of-domain robustness for pose estimation.", "abstract": "Deep neural networks are highly effective tools for human and animal pose estimation. However, robustness to out-of-domain data remains a challenge. Here, we probe the transfer and generalization ability for pose estimation with two architecture classes (MobileNetV2s and ResNets) pretrained on ImageNet. We generated a novel dataset of 30 horses that allowed for both within-domain and out-of-domain (unseen horse) testing. We find that pretraining on ImageNet strongly improves out-of-domain performance. Moreover, we show that for both pretrained and networks trained from scratch, better ImageNet-performing architectures perform better for pose estimation, with a substantial improvement on out-of-domain data when pretrained. Collectively, our results demonstrate that transfer learning is particularly beneficial for out-of-domain robustness.", "pdf": "/pdf/029ceb081cf7fefe7584a6e6028574f6bd38b76c.pdf", "paperhash": "mathis|pretraining_boosts_outofdomain_robustness_for_pose_estimation", "original_pdf": "/attachment/029ceb081cf7fefe7584a6e6028574f6bd38b76c.pdf", "_bibtex": "@misc{\nmathis2020pretraining,\ntitle={Pretraining boosts out-of-domain robustness for pose estimation},\nauthor={Alexander Mathis and Mert Y{\\\"u}ksekg{\\\"o}n{\\\"u}l and Byron Rogers and Matthias Bethge and Mackenzie W. Mathis},\nyear={2020},\nurl={https://openreview.net/forum?id=BkgMbCVFvr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BkgMbCVFvr", "replyto": "BkgMbCVFvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795728015, "tmdate": 1576800280342, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper957/-/Decision"}}}, {"id": "rkev8IoliS", "original": null, "number": 4, "cdate": 1573070415363, "ddate": null, "tcdate": 1573070415363, "tmdate": 1573070415363, "tddate": null, "forum": "BkgMbCVFvr", "replyto": "SJxaicuTYr", "invitation": "ICLR.cc/2020/Conference/Paper957/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "Review: The paper proposes a horse dataset to study transfer learning or so-called out-of-domain pose estimation. They also study which model is a better initialization model for pose estimation, and how to utilize transfer learning to get a better estimation model. \n\nThere are several questions from the paper:\n1. why are they not using human pose estimation datasets, as there are already lots of them and that will be easier to compare with other models: MPII, COCO, AI challenge, CrowdPose, etc.  I am not fully convinced the horse dataset is better than human pose. In terms of out domain, authors can use pose pre-trained models to analysis horse pose prediction.\n\n\nFirstly, typical human pose estimation benchmarks (MPII pose and COCO) contain many different individuals (>10k) in different contexts, but only few examples per individual (<10).\nThis setting does not allow to ask how well an algorithm generalizes to different individuals that have slightly/strongly different appearance (and we do not feel it is wise/ethical to cluster individuals based on their appearance, etc to test this). Unlike the human pose estimation benchmarks, typically in animal pose estimation datasets that have many (annotated) poses per individual (>200) and only a few individuals (~10-100). \n\nWe believe that the Horse Dataset we developed is a powerful dataset to study generalization across individuals and important for applications using small datasets. Furthermore using this dataset, we demonstrate that that \u201cbetter\u201d ImageNet networks transfer better as well as that task-driven training can match the performance of of transfer learning. However, we also found that for out-of-domain data, pretraining helps significantly, boosting performance up to 3 times (Fig 4). We think this is a novel result. It also shows that within domain/across domain horses are actually quite different.\n\n2. The analysis is good and with lots of experiments, however, the key part is that they do not provide a way to improve the overall performance for out-of-domain pose estimation.\n\nOne of our findings is that the out-of-domain test performance improves by 1.6% per percentage point of ImageNet performance (Fig 2B). Thus, our paper demonstrates that better (ImageNet pretrained) backbones will be more robust and contribute to close the gap between the within domain and out of domain performance.\n\nWe sincerely thank you for your time and efforts, and hope this clarifies our contribution to the field! - The Authors\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper957/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper957/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper957/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pretraining boosts out-of-domain robustness for pose estimation", "authors": ["Alexander Mathis", "Mert Y\u00fcksekg\u00f6n\u00fcl", "Byron Rogers", "Matthias Bethge", "Mackenzie W. Mathis"], "authorids": ["amathis@fas.harvard.edu", "mertyuksekgonul@gmail.com", "byron@performancegenetics.com", "matthias.bethge@uni-tuebingen.de", "mathis@rowland.harvard.edu"], "keywords": ["pose estimation", "robustness", "out-of-domain", "transfer learning"], "TL;DR": "Transfer learning boosts out-of-domain robustness for pose estimation.", "abstract": "Deep neural networks are highly effective tools for human and animal pose estimation. However, robustness to out-of-domain data remains a challenge. Here, we probe the transfer and generalization ability for pose estimation with two architecture classes (MobileNetV2s and ResNets) pretrained on ImageNet. We generated a novel dataset of 30 horses that allowed for both within-domain and out-of-domain (unseen horse) testing. We find that pretraining on ImageNet strongly improves out-of-domain performance. Moreover, we show that for both pretrained and networks trained from scratch, better ImageNet-performing architectures perform better for pose estimation, with a substantial improvement on out-of-domain data when pretrained. Collectively, our results demonstrate that transfer learning is particularly beneficial for out-of-domain robustness.", "pdf": "/pdf/029ceb081cf7fefe7584a6e6028574f6bd38b76c.pdf", "paperhash": "mathis|pretraining_boosts_outofdomain_robustness_for_pose_estimation", "original_pdf": "/attachment/029ceb081cf7fefe7584a6e6028574f6bd38b76c.pdf", "_bibtex": "@misc{\nmathis2020pretraining,\ntitle={Pretraining boosts out-of-domain robustness for pose estimation},\nauthor={Alexander Mathis and Mert Y{\\\"u}ksekg{\\\"o}n{\\\"u}l and Byron Rogers and Matthias Bethge and Mackenzie W. Mathis},\nyear={2020},\nurl={https://openreview.net/forum?id=BkgMbCVFvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkgMbCVFvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper957/Authors", "ICLR.cc/2020/Conference/Paper957/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper957/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper957/Reviewers", "ICLR.cc/2020/Conference/Paper957/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper957/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper957/Authors|ICLR.cc/2020/Conference/Paper957/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163490, "tmdate": 1576860552676, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper957/Authors", "ICLR.cc/2020/Conference/Paper957/Reviewers", "ICLR.cc/2020/Conference/Paper957/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper957/-/Official_Comment"}}}, {"id": "B1gXYBjlsr", "original": null, "number": 3, "cdate": 1573070202980, "ddate": null, "tcdate": 1573070202980, "tmdate": 1573070235426, "tddate": null, "forum": "BkgMbCVFvr", "replyto": "HkgSzHsgjB", "invitation": "ICLR.cc/2020/Conference/Paper957/-/Official_Comment", "content": {"title": "Response to Reviewer #1: Part 2", "comment": "Second, the dataset split strategy is weird. Images of 10 horse IDs are considered as the \"within domain\" dataset while the others as \"out-of-domain\" dataset, and a subset of the within domain dataset is used for training or finetuning the pose estimation networks. This means that the networks could seriously overfitted to the small image set of 10 horse IDs. Thus, it is obvious that the networks will work very well with and without ImageNet pretraining on the within domain test set in which exactly the same 10 horses appear, and that they will not work well on the \"out-of-domain\" dataset due to the overfitting issue. \n\n(1) We agree that models should work well on within domain data. That is what we find, but importantly, and this is one of our findings ImageNet accuracy predicts the within domain accuracy on pose estimation. This is consistent with what Kornblith et al. 2019 CVPR found this for object recognition. \n\n(2) Most importantly we find that better models generalize better to out-of domain horses. We do think that this result is surprising. Namely, better networks could also overfit more and thus generalize less. In fact, we find that the out-of-domain test performance improves by 1.6% per percentage point of ImageNet performance. \n\n(3) Now \u2014 going with your criticism \u2014 if we hypothesize that the \u201cout of domain\u201d and \u201cwithin domain\u201d horses are very similar. That would suggest: \n\n        a) the \u201cgeneralization\u201d to out of domain should be similar to within domain. But depending on the architecture the performance is up to 6 times worse (Fig 2B).\n\n        b) the slope ImageNet accuracy vs. Performance remains similar between within/out of domain. But again this is not the case. Instead, we find that better models are substantially better on out of domain data suggesting they the \u201cunderstand the task\u201d (Fig 2B).\n\n\nAlso, focusing only on the \"horse\" class looks not proper. Note that ImageNet already contains many horse images, and ImageNet pretrained networks would have a capability to extract horse-related features. Thus, the advantage of ImageNet pretraining on the horse pose estimation task is not surprising but a result that many in this field can easily expect. If this is not a big issue, I rather would like to recommend to exploit existing human pose datasets (e.g., MPII) since they are larger enough in size, and guarantee a larger variety of poses and person appearances than the proposed horse dataset.\n\nWe believe this is a misunderstanding. We find that given enough training data, training from scratch, with purely task-driven training can match the performance of of transfer learning (Fig 5). Again this is something that has recently been highlighted in He\u2019s et al. \u201cRethinking ImageNet pre-training\u201d also for COCO keypoint detection. The Horse dataset allows us to go beyond their finding. Thus, while it has been previously shown that training from scratch can match performance on in-domain data for sufficiently large amount of training data and training times, we show it clearly cannot match performance of pretrained networks on out-of-domain data (Figure 5), especially on small datasets. \n\n\n= Other comments\nThe manuscript is overall written clearly, but it is hard to understand the curves in the figures: colors are not clearly distinguishable (e.g., red vs. magenta), the roles of \"MobileNets\" and \"ResNets\" placed on top of upper horizontal bars are not unknown too (if they indicate that the top-2 accuracy scores are given by Resnets variants, why are the curves of MobileNets and ResNets connected?)\n\nThank you for these constructive comments. We will update the red vs. magenta color scheme to be more clear. The MobileNets and ResNets bar/text is to guide the reader about which points on the graph are coming from the 4 MobileNet and 2 ResNet variants (order ranked by top 1% ImageNet accuracy). All the points are linked so one can appreciate the major findings of the paper, i.e. (1) networks that perform better on ImageNet perform better on pose estimation, both within and out-of-domain, and (2) pretraining absolutely matters on small datasets, something not tested elsewhere.\n\nWe sincerely thank you for your time and efforts! - The Authors"}, "signatures": ["ICLR.cc/2020/Conference/Paper957/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper957/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper957/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pretraining boosts out-of-domain robustness for pose estimation", "authors": ["Alexander Mathis", "Mert Y\u00fcksekg\u00f6n\u00fcl", "Byron Rogers", "Matthias Bethge", "Mackenzie W. Mathis"], "authorids": ["amathis@fas.harvard.edu", "mertyuksekgonul@gmail.com", "byron@performancegenetics.com", "matthias.bethge@uni-tuebingen.de", "mathis@rowland.harvard.edu"], "keywords": ["pose estimation", "robustness", "out-of-domain", "transfer learning"], "TL;DR": "Transfer learning boosts out-of-domain robustness for pose estimation.", "abstract": "Deep neural networks are highly effective tools for human and animal pose estimation. However, robustness to out-of-domain data remains a challenge. Here, we probe the transfer and generalization ability for pose estimation with two architecture classes (MobileNetV2s and ResNets) pretrained on ImageNet. We generated a novel dataset of 30 horses that allowed for both within-domain and out-of-domain (unseen horse) testing. We find that pretraining on ImageNet strongly improves out-of-domain performance. Moreover, we show that for both pretrained and networks trained from scratch, better ImageNet-performing architectures perform better for pose estimation, with a substantial improvement on out-of-domain data when pretrained. Collectively, our results demonstrate that transfer learning is particularly beneficial for out-of-domain robustness.", "pdf": "/pdf/029ceb081cf7fefe7584a6e6028574f6bd38b76c.pdf", "paperhash": "mathis|pretraining_boosts_outofdomain_robustness_for_pose_estimation", "original_pdf": "/attachment/029ceb081cf7fefe7584a6e6028574f6bd38b76c.pdf", "_bibtex": "@misc{\nmathis2020pretraining,\ntitle={Pretraining boosts out-of-domain robustness for pose estimation},\nauthor={Alexander Mathis and Mert Y{\\\"u}ksekg{\\\"o}n{\\\"u}l and Byron Rogers and Matthias Bethge and Mackenzie W. Mathis},\nyear={2020},\nurl={https://openreview.net/forum?id=BkgMbCVFvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkgMbCVFvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper957/Authors", "ICLR.cc/2020/Conference/Paper957/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper957/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper957/Reviewers", "ICLR.cc/2020/Conference/Paper957/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper957/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper957/Authors|ICLR.cc/2020/Conference/Paper957/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163490, "tmdate": 1576860552676, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper957/Authors", "ICLR.cc/2020/Conference/Paper957/Reviewers", "ICLR.cc/2020/Conference/Paper957/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper957/-/Official_Comment"}}}, {"id": "HkgSzHsgjB", "original": null, "number": 2, "cdate": 1573070093386, "ddate": null, "tcdate": 1573070093386, "tmdate": 1573070093386, "tddate": null, "forum": "BkgMbCVFvr", "replyto": "rJxIkFwb5H", "invitation": "ICLR.cc/2020/Conference/Paper957/-/Official_Comment", "content": {"title": "Response to Review #1: Part 1.", "comment": "= Summary \nThis paper analyzes the effect of ImageNet pretraining on out-of-domain visual recognition. Specifically, in this paper, the recognition problem is narrowed to pose estimation on a horse profile image dataset where the out-of-domain indicates horse IDs unseen during training. The paper presents a new horse pose estimation dataset and extensive experimental analysis to demonstrate the benefit of ImageNet pretraining. \n\nThank you for your reviewing our work. Your comments essentially boil down to concerns with the novel Horse Dataset (\u201ctoo small & little diversity\u201d). This is disappointing, but here we hope to convince you that this dataset is important and can help us understand/improve pose estimation algorithms and representation learning more general. \n\nFirstly, typical human pose estimation benchmarks (MPII pose and COCO) contain many different individuals (>10k) in different contexts, but only few examples per individual (<10).\nIn real world applications of pose estimation (beyond human pose estimation benchmarks) one naturally asks the following question: Assume you have an algorithm that performs pose estimation with very high accuracy (for biomechanics/neuroscience applications) on a given (individual) animal with all its poses (this is important to relate e.g. pose to coding in the brain). How well will it generalize to different individuals that have slightly/strongly different appearance? Unlike the human pose estimation benchmarks one deals with datasets that have many (annotated) poses per individual (>200) and only a few individuals (~10-100). \n\nWe believe that the Horse Dataset we developed is a great dataset to study generalization across individuals and important for applications using small datasets. Furthermore using this dataset, we demonstrate that that \u201cbetter\u201d ImageNet networks transfer better as well as that task-driven training can match the performance of of transfer learning. However, we also found that for out-of-domain data, pretraining helps significantly, boosting performance up to 3 times (Fig 4). We think this is a novel result. It also shows that within domain/across domain horses are actually quite different.\n\nBelow we added additional information that we hope will improve the score and highlight why these types of datasets and the results are important. \n\n= Decision\nI would recommend to reject this submission mainly due to the shortcomings of the proposed dataset, which make the analysis and conclusion of the paper unconvincing.\n\nWe wish to take this opportunity to speak to why such new datasets are crucially important: \n(1) Animal pose estimation has important applications in neuroscience, ethology, technology, and medical studies (drug testing, etc. - see recent commentary in Nature: https://www.nature.com/articles/d41586-019-02942-5). The application of deep learning to animals is a budding field, empowered by the advances in human pose estimation, but has its own unique challenges - some of which we aim to address with this paper. Namely, small datasets, much much less than 8K are the norm - in fact, it is common for experimenters to only use 50-1000 images to create a training set, yet they demand good performance. Here are recent examples: Mathis et al 2018, Nath et al 2019, Graving et al 2019, Pereira et al 2019. Thus, we specifically wanted to create an animal benchmark dataset and test the relationship of how different architectures generalize across individuals. The data varies in horse color, the appearance of sunlight and shadow, and relative horse size as well as background. This makes the data set ideal for tests in robustness and generalization. If the term \u201cout-of-domain\u201d feels incorrect, we are very happy to revise this.\n\n(2) We tested the effect of pretraining on out-of-domain data on small datasets, which has not been done, to the best of our knowledge. \n\n(3) While the reviewer is absolutely correct that human pose benchmarks provide large datasets, they are not designed with an out-of-domain component (it has 40K individuals in 25K images). Nor do these benchmarks address (some of) the challenges that animal pose estimation researchers actually face (see above). \n\n[ continued below ]"}, "signatures": ["ICLR.cc/2020/Conference/Paper957/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper957/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper957/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pretraining boosts out-of-domain robustness for pose estimation", "authors": ["Alexander Mathis", "Mert Y\u00fcksekg\u00f6n\u00fcl", "Byron Rogers", "Matthias Bethge", "Mackenzie W. Mathis"], "authorids": ["amathis@fas.harvard.edu", "mertyuksekgonul@gmail.com", "byron@performancegenetics.com", "matthias.bethge@uni-tuebingen.de", "mathis@rowland.harvard.edu"], "keywords": ["pose estimation", "robustness", "out-of-domain", "transfer learning"], "TL;DR": "Transfer learning boosts out-of-domain robustness for pose estimation.", "abstract": "Deep neural networks are highly effective tools for human and animal pose estimation. However, robustness to out-of-domain data remains a challenge. Here, we probe the transfer and generalization ability for pose estimation with two architecture classes (MobileNetV2s and ResNets) pretrained on ImageNet. We generated a novel dataset of 30 horses that allowed for both within-domain and out-of-domain (unseen horse) testing. We find that pretraining on ImageNet strongly improves out-of-domain performance. Moreover, we show that for both pretrained and networks trained from scratch, better ImageNet-performing architectures perform better for pose estimation, with a substantial improvement on out-of-domain data when pretrained. Collectively, our results demonstrate that transfer learning is particularly beneficial for out-of-domain robustness.", "pdf": "/pdf/029ceb081cf7fefe7584a6e6028574f6bd38b76c.pdf", "paperhash": "mathis|pretraining_boosts_outofdomain_robustness_for_pose_estimation", "original_pdf": "/attachment/029ceb081cf7fefe7584a6e6028574f6bd38b76c.pdf", "_bibtex": "@misc{\nmathis2020pretraining,\ntitle={Pretraining boosts out-of-domain robustness for pose estimation},\nauthor={Alexander Mathis and Mert Y{\\\"u}ksekg{\\\"o}n{\\\"u}l and Byron Rogers and Matthias Bethge and Mackenzie W. Mathis},\nyear={2020},\nurl={https://openreview.net/forum?id=BkgMbCVFvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkgMbCVFvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper957/Authors", "ICLR.cc/2020/Conference/Paper957/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper957/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper957/Reviewers", "ICLR.cc/2020/Conference/Paper957/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper957/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper957/Authors|ICLR.cc/2020/Conference/Paper957/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163490, "tmdate": 1576860552676, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper957/Authors", "ICLR.cc/2020/Conference/Paper957/Reviewers", "ICLR.cc/2020/Conference/Paper957/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper957/-/Official_Comment"}}}, {"id": "SJxaicuTYr", "original": null, "number": 1, "cdate": 1571814052985, "ddate": null, "tcdate": 1571814052985, "tmdate": 1572972530686, "tddate": null, "forum": "BkgMbCVFvr", "replyto": "BkgMbCVFvr", "invitation": "ICLR.cc/2020/Conference/Paper957/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposes a horse dataset to study transfer learning or so-called out-of-domain pose estimation. They also study which model is a better initialization model for pose estimation, and how to utilize transfer learning to get a better estimation model. \n\nThere are several questions from the paper:\n1. why are they not using human pose estimation datasets, as there are already lots of them and that will be easier to compare with other models: MPII, COCO, AI challenge, CrowdPose, etc.  I am not fully convinced the horse dataset is better than human pose. In terms of out domain, authors can use pose pre-trained models to analysis horse pose prediction.\n2. The analysis is good and with lots of experiments, however, the key part is that they do not provide a way to improve the overall performance for out-of-domain pose estimation. "}, "signatures": ["ICLR.cc/2020/Conference/Paper957/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper957/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pretraining boosts out-of-domain robustness for pose estimation", "authors": ["Alexander Mathis", "Mert Y\u00fcksekg\u00f6n\u00fcl", "Byron Rogers", "Matthias Bethge", "Mackenzie W. Mathis"], "authorids": ["amathis@fas.harvard.edu", "mertyuksekgonul@gmail.com", "byron@performancegenetics.com", "matthias.bethge@uni-tuebingen.de", "mathis@rowland.harvard.edu"], "keywords": ["pose estimation", "robustness", "out-of-domain", "transfer learning"], "TL;DR": "Transfer learning boosts out-of-domain robustness for pose estimation.", "abstract": "Deep neural networks are highly effective tools for human and animal pose estimation. However, robustness to out-of-domain data remains a challenge. Here, we probe the transfer and generalization ability for pose estimation with two architecture classes (MobileNetV2s and ResNets) pretrained on ImageNet. We generated a novel dataset of 30 horses that allowed for both within-domain and out-of-domain (unseen horse) testing. We find that pretraining on ImageNet strongly improves out-of-domain performance. Moreover, we show that for both pretrained and networks trained from scratch, better ImageNet-performing architectures perform better for pose estimation, with a substantial improvement on out-of-domain data when pretrained. Collectively, our results demonstrate that transfer learning is particularly beneficial for out-of-domain robustness.", "pdf": "/pdf/029ceb081cf7fefe7584a6e6028574f6bd38b76c.pdf", "paperhash": "mathis|pretraining_boosts_outofdomain_robustness_for_pose_estimation", "original_pdf": "/attachment/029ceb081cf7fefe7584a6e6028574f6bd38b76c.pdf", "_bibtex": "@misc{\nmathis2020pretraining,\ntitle={Pretraining boosts out-of-domain robustness for pose estimation},\nauthor={Alexander Mathis and Mert Y{\\\"u}ksekg{\\\"o}n{\\\"u}l and Byron Rogers and Matthias Bethge and Mackenzie W. Mathis},\nyear={2020},\nurl={https://openreview.net/forum?id=BkgMbCVFvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BkgMbCVFvr", "replyto": "BkgMbCVFvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper957/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper957/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575756286683, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper957/Reviewers"], "noninvitees": [], "tcdate": 1570237744497, "tmdate": 1575756286701, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper957/-/Official_Review"}}}, {"id": "rJxIkFwb5H", "original": null, "number": 2, "cdate": 1572071645733, "ddate": null, "tcdate": 1572071645733, "tmdate": 1572972530640, "tddate": null, "forum": "BkgMbCVFvr", "replyto": "BkgMbCVFvr", "invitation": "ICLR.cc/2020/Conference/Paper957/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\n= Summary \nThis paper analyzes the effect of ImageNet pretraining on out-of-domain visual recognition. Specifically, in this paper, the recognition problem is narrowed to pose estimation on a horse profile image dataset where the out-of-domain indicates horse IDs unseen during training. The paper presents a new horse pose estimation dataset and extensive experimental analysis to demonstrate the benefit of ImageNet pretraining. \n\n\n= Decision\nI would recommend to reject this submission mainly due to the shortcomings of the proposed dataset, which make the analysis and conclusion of the paper unconvincing.\n\nFirst, the dataset is very limited in terms of diversity. It only contains 8K horse profiles images, each of which contains only a single horse, and only 30 horses of the same species appear in the images. Furthermore, since the dataset are sampled from video sequences, images of the same horse ID could be too similar in terms of appearance. Thus, all the images in the dataset seems within a single and very specific domain, \"profiles of a small number of Thoroughbred horses\", and not appropriate to evaluate \"out-of-domain\" robustness of pose estimation networks in consequence.\n\nSecond, the dataset split strategy is weird. Images of 10 horse IDs are considered as the \"within domain\" dataset while the others as \"out-of-domain\" dataset, and a subset of the within domain dataset is used for training or finetuning the pose estimation networks. This means that the networks could seriously overfitted to the small image set of 10 horse IDs. Thus, it is obvious that the networks will work very well with and without ImageNet pretraining on the within domain test set in which exactly the same 10 horses appear, and that they will not work well on the \"out-of-domain\" dataset due to the overfitting issue. \n\nAlso, focusing only on the \"horse\" class looks not proper. Note that ImageNet already contains many horse images, and ImageNet pretrained networks would have a capability to extract horse-related features. Thus, the advantage of ImageNet pretraining on the horse pose estimation task is not surprising but a result that many in this field can easily expect. If this is not a big issue, I rather would like to recommend to exploit existing human pose datasets (e.g., MPII) since they are larger enough in size, and guarantee a larger variety of poses and person appearances than the proposed horse dataset.\n\n\n= Other comments\nThe manuscript is overall written clearly, but it is hard to understand the curves in the figures: colors are not clearly distinguishable (e.g., red vs. magenta), the roles of \"MobileNets\" and \"ResNets\" placed on top of upper horizontal bars are not unknown too (if they indicate that the top-2 accuracy scores are given by Resnets variants, why are the curves of MobileNets and ResNets connected?)\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper957/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper957/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pretraining boosts out-of-domain robustness for pose estimation", "authors": ["Alexander Mathis", "Mert Y\u00fcksekg\u00f6n\u00fcl", "Byron Rogers", "Matthias Bethge", "Mackenzie W. Mathis"], "authorids": ["amathis@fas.harvard.edu", "mertyuksekgonul@gmail.com", "byron@performancegenetics.com", "matthias.bethge@uni-tuebingen.de", "mathis@rowland.harvard.edu"], "keywords": ["pose estimation", "robustness", "out-of-domain", "transfer learning"], "TL;DR": "Transfer learning boosts out-of-domain robustness for pose estimation.", "abstract": "Deep neural networks are highly effective tools for human and animal pose estimation. However, robustness to out-of-domain data remains a challenge. Here, we probe the transfer and generalization ability for pose estimation with two architecture classes (MobileNetV2s and ResNets) pretrained on ImageNet. We generated a novel dataset of 30 horses that allowed for both within-domain and out-of-domain (unseen horse) testing. We find that pretraining on ImageNet strongly improves out-of-domain performance. Moreover, we show that for both pretrained and networks trained from scratch, better ImageNet-performing architectures perform better for pose estimation, with a substantial improvement on out-of-domain data when pretrained. Collectively, our results demonstrate that transfer learning is particularly beneficial for out-of-domain robustness.", "pdf": "/pdf/029ceb081cf7fefe7584a6e6028574f6bd38b76c.pdf", "paperhash": "mathis|pretraining_boosts_outofdomain_robustness_for_pose_estimation", "original_pdf": "/attachment/029ceb081cf7fefe7584a6e6028574f6bd38b76c.pdf", "_bibtex": "@misc{\nmathis2020pretraining,\ntitle={Pretraining boosts out-of-domain robustness for pose estimation},\nauthor={Alexander Mathis and Mert Y{\\\"u}ksekg{\\\"o}n{\\\"u}l and Byron Rogers and Matthias Bethge and Mackenzie W. Mathis},\nyear={2020},\nurl={https://openreview.net/forum?id=BkgMbCVFvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BkgMbCVFvr", "replyto": "BkgMbCVFvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper957/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper957/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575756286683, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper957/Reviewers"], "noninvitees": [], "tcdate": 1570237744497, "tmdate": 1575756286701, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper957/-/Official_Review"}}}, {"id": "HygZk_om_r", "original": null, "number": 2, "cdate": 1570121689096, "ddate": null, "tcdate": 1570121689096, "tmdate": 1570121689096, "tddate": null, "forum": "BkgMbCVFvr", "replyto": "Hyxu9Rizdr", "invitation": "ICLR.cc/2020/Conference/Paper957/-/Public_Comment", "content": {"comment": "Dear authors,\n\nThank you for the very clear and kind explanation!", "title": "Re: Clarifying related work"}, "signatures": ["~Yosuke_Shinya1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Yosuke_Shinya1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pretraining boosts out-of-domain robustness for pose estimation", "authors": ["Alexander Mathis", "Mert Y\u00fcksekg\u00f6n\u00fcl", "Byron Rogers", "Matthias Bethge", "Mackenzie W. Mathis"], "authorids": ["amathis@fas.harvard.edu", "mertyuksekgonul@gmail.com", "byron@performancegenetics.com", "matthias.bethge@uni-tuebingen.de", "mathis@rowland.harvard.edu"], "keywords": ["pose estimation", "robustness", "out-of-domain", "transfer learning"], "TL;DR": "Transfer learning boosts out-of-domain robustness for pose estimation.", "abstract": "Deep neural networks are highly effective tools for human and animal pose estimation. However, robustness to out-of-domain data remains a challenge. Here, we probe the transfer and generalization ability for pose estimation with two architecture classes (MobileNetV2s and ResNets) pretrained on ImageNet. We generated a novel dataset of 30 horses that allowed for both within-domain and out-of-domain (unseen horse) testing. We find that pretraining on ImageNet strongly improves out-of-domain performance. Moreover, we show that for both pretrained and networks trained from scratch, better ImageNet-performing architectures perform better for pose estimation, with a substantial improvement on out-of-domain data when pretrained. Collectively, our results demonstrate that transfer learning is particularly beneficial for out-of-domain robustness.", "pdf": "/pdf/029ceb081cf7fefe7584a6e6028574f6bd38b76c.pdf", "paperhash": "mathis|pretraining_boosts_outofdomain_robustness_for_pose_estimation", "original_pdf": "/attachment/029ceb081cf7fefe7584a6e6028574f6bd38b76c.pdf", "_bibtex": "@misc{\nmathis2020pretraining,\ntitle={Pretraining boosts out-of-domain robustness for pose estimation},\nauthor={Alexander Mathis and Mert Y{\\\"u}ksekg{\\\"o}n{\\\"u}l and Byron Rogers and Matthias Bethge and Mackenzie W. Mathis},\nyear={2020},\nurl={https://openreview.net/forum?id=BkgMbCVFvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkgMbCVFvr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504201686, "tmdate": 1576860585874, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper957/Authors", "ICLR.cc/2020/Conference/Paper957/Reviewers", "ICLR.cc/2020/Conference/Paper957/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper957/-/Public_Comment"}}}, {"id": "Hyxu9Rizdr", "original": null, "number": 1, "cdate": 1570057871565, "ddate": null, "tcdate": 1570057871565, "tmdate": 1570106264830, "tddate": null, "forum": "BkgMbCVFvr", "replyto": "ryxCOO7Rwr", "invitation": "ICLR.cc/2020/Conference/Paper957/-/Official_Comment", "content": {"comment": "Dear Dr. Shinya, thanks for pointing us to the two papers [1,2].  Both of them\u2014like our work\u2014report on a beneficial effect of pre-training on robustness and we will definitely update our preprint to include a discussion of the relation to these two studies. \n\nWe believe that our work complements and builds on these in two important ways: The key difference in our work is that we report on a different type of robustness that is of direct importance to a practical application we are trying to solve, namely accurate animal pose estimation under general conditions. \n\nIn contrast, Jain et al [2] study robustness against adversarial attacks, and Hendrycks et al. [1] study CIFAR 10, CIFAR 100 and TinyImageNet instead of pose estimation. Hendrycks et al. [1] comes closer to our work than [2] as it shows a benefit for various types of robustness with out-of-domain detection being somewhat related to the problem that we are trying to solve. Note, however, that the analysis of out-of distribution-detection is quite different from the notion of out-of-domain robustness that we are testing. \n \nWe believe the most relevant result from the two papers for our study is Table 4 in [1]:\nhttp://proceedings.mlr.press/v97/hendrycks19a/hendrycks19a.pdf (page 7)\n \nThe performances in table 4 do not reflect task performance on out-of-domain test data but the ability to detect whether a sample is drawn from the original training distribution or from a corrupted version. In contrast, our work measures pose estimation performance on out-of-domain test data, for which we also have human annotated ground truth, and which is of high practical relevance.\n \nAgain, thank you for the comment, and the pdf will be updated accordingly.\n", "title": "Clarifying related work"}, "signatures": ["ICLR.cc/2020/Conference/Paper957/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper957/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pretraining boosts out-of-domain robustness for pose estimation", "authors": ["Alexander Mathis", "Mert Y\u00fcksekg\u00f6n\u00fcl", "Byron Rogers", "Matthias Bethge", "Mackenzie W. Mathis"], "authorids": ["amathis@fas.harvard.edu", "mertyuksekgonul@gmail.com", "byron@performancegenetics.com", "matthias.bethge@uni-tuebingen.de", "mathis@rowland.harvard.edu"], "keywords": ["pose estimation", "robustness", "out-of-domain", "transfer learning"], "TL;DR": "Transfer learning boosts out-of-domain robustness for pose estimation.", "abstract": "Deep neural networks are highly effective tools for human and animal pose estimation. However, robustness to out-of-domain data remains a challenge. Here, we probe the transfer and generalization ability for pose estimation with two architecture classes (MobileNetV2s and ResNets) pretrained on ImageNet. We generated a novel dataset of 30 horses that allowed for both within-domain and out-of-domain (unseen horse) testing. We find that pretraining on ImageNet strongly improves out-of-domain performance. Moreover, we show that for both pretrained and networks trained from scratch, better ImageNet-performing architectures perform better for pose estimation, with a substantial improvement on out-of-domain data when pretrained. Collectively, our results demonstrate that transfer learning is particularly beneficial for out-of-domain robustness.", "pdf": "/pdf/029ceb081cf7fefe7584a6e6028574f6bd38b76c.pdf", "paperhash": "mathis|pretraining_boosts_outofdomain_robustness_for_pose_estimation", "original_pdf": "/attachment/029ceb081cf7fefe7584a6e6028574f6bd38b76c.pdf", "_bibtex": "@misc{\nmathis2020pretraining,\ntitle={Pretraining boosts out-of-domain robustness for pose estimation},\nauthor={Alexander Mathis and Mert Y{\\\"u}ksekg{\\\"o}n{\\\"u}l and Byron Rogers and Matthias Bethge and Mackenzie W. Mathis},\nyear={2020},\nurl={https://openreview.net/forum?id=BkgMbCVFvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkgMbCVFvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper957/Authors", "ICLR.cc/2020/Conference/Paper957/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper957/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper957/Reviewers", "ICLR.cc/2020/Conference/Paper957/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper957/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper957/Authors|ICLR.cc/2020/Conference/Paper957/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163490, "tmdate": 1576860552676, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper957/Authors", "ICLR.cc/2020/Conference/Paper957/Reviewers", "ICLR.cc/2020/Conference/Paper957/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper957/-/Official_Comment"}}}, {"id": "ryxCOO7Rwr", "original": null, "number": 1, "cdate": 1569761397818, "ddate": null, "tcdate": 1569761397818, "tmdate": 1569761397818, "tddate": null, "forum": "BkgMbCVFvr", "replyto": "BkgMbCVFvr", "invitation": "ICLR.cc/2020/Conference/Paper957/-/Public_Comment", "content": {"comment": "Hi,\n\nWould you clarify the difference from [1, 2] in the paper?\nThanks.\n\n[1] Dan Hendrycks, Kimin Lee, Mantas Mazeika.\nUsing Pre-Training Can Improve Model Robustness and Uncertainty. ICML, 2019.\nhttp://proceedings.mlr.press/v97/hendrycks19a.html\nhttps://arxiv.org/abs/1901.09960\n[2] Naman Jain, Sahil Shah, Abhishek Kumar, Arjun Jain.\nOn the Robustness of Human Pose Estimation. CVPRW, 2019.\nhttp://openaccess.thecvf.com/content_CVPRW_2019/papers/Augmented%20Human%20Human-centric%20Understanding%20and%202D-3D%20Synthesis/Jain_On_the_Robustness_of_Human_Pose_Estimation_CVPRW_2019_paper.pdf\nhttps://arxiv.org/abs/1908.06401", "title": "Related work"}, "signatures": ["~Yosuke_Shinya1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Yosuke_Shinya1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pretraining boosts out-of-domain robustness for pose estimation", "authors": ["Alexander Mathis", "Mert Y\u00fcksekg\u00f6n\u00fcl", "Byron Rogers", "Matthias Bethge", "Mackenzie W. Mathis"], "authorids": ["amathis@fas.harvard.edu", "mertyuksekgonul@gmail.com", "byron@performancegenetics.com", "matthias.bethge@uni-tuebingen.de", "mathis@rowland.harvard.edu"], "keywords": ["pose estimation", "robustness", "out-of-domain", "transfer learning"], "TL;DR": "Transfer learning boosts out-of-domain robustness for pose estimation.", "abstract": "Deep neural networks are highly effective tools for human and animal pose estimation. However, robustness to out-of-domain data remains a challenge. Here, we probe the transfer and generalization ability for pose estimation with two architecture classes (MobileNetV2s and ResNets) pretrained on ImageNet. We generated a novel dataset of 30 horses that allowed for both within-domain and out-of-domain (unseen horse) testing. We find that pretraining on ImageNet strongly improves out-of-domain performance. Moreover, we show that for both pretrained and networks trained from scratch, better ImageNet-performing architectures perform better for pose estimation, with a substantial improvement on out-of-domain data when pretrained. Collectively, our results demonstrate that transfer learning is particularly beneficial for out-of-domain robustness.", "pdf": "/pdf/029ceb081cf7fefe7584a6e6028574f6bd38b76c.pdf", "paperhash": "mathis|pretraining_boosts_outofdomain_robustness_for_pose_estimation", "original_pdf": "/attachment/029ceb081cf7fefe7584a6e6028574f6bd38b76c.pdf", "_bibtex": "@misc{\nmathis2020pretraining,\ntitle={Pretraining boosts out-of-domain robustness for pose estimation},\nauthor={Alexander Mathis and Mert Y{\\\"u}ksekg{\\\"o}n{\\\"u}l and Byron Rogers and Matthias Bethge and Mackenzie W. Mathis},\nyear={2020},\nurl={https://openreview.net/forum?id=BkgMbCVFvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkgMbCVFvr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504201686, "tmdate": 1576860585874, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper957/Authors", "ICLR.cc/2020/Conference/Paper957/Reviewers", "ICLR.cc/2020/Conference/Paper957/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper957/-/Public_Comment"}}}], "count": 10}