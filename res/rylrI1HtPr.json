{"notes": [{"id": "rylrI1HtPr", "original": "B1euaI6dPH", "number": 1728, "cdate": 1569439565233, "ddate": null, "tcdate": 1569439565233, "tmdate": 1577168226498, "tddate": null, "forum": "rylrI1HtPr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["yingda.wang@unsw.edu.au", "p.swietojanski@unsw.edu.au", "ryan.armstrong@unsw.edu.au", "peyman@unsw.edu.au"], "title": "Pixel Co-Occurence Based Loss Metrics for Super Resolution Texture Recovery", "authors": ["Ying Da Wang", "Pawel Swietojanski", "Ryan T Armstrong", "Peyman Mostaghimi"], "pdf": "/pdf/d611e52ebd5e2adfb9a62ba2b859d54ea1a15313.pdf", "TL;DR": "We introduce an unbiased perceptual loss function and metric and show that it improves recovery of texture during super resolution", "abstract": "Single Image Super Resolution (SISR) has significantly improved with Convolutional Neural Networks (CNNs) and Generative Adversarial Networks (GANs), often achieving order of magnitude better pixelwise accuracies (distortions) and state-of-the-art perceptual accuracy. Due to the stochastic nature of GAN reconstruction and the ill-posed nature of the problem, perceptual accuracy tends to correlate inversely with pixelwise accuracy which is especially detrimental to SISR, where preservation of original content is an objective. GAN stochastics can be guided by intermediate loss functions such as the VGG featurewise loss, but these features are typically derived from biased pre-trained networks. Similarly, measurements of perceptual quality such as the human Mean Opinion Score (MOS) and no-reference measures have issues with pre-trained bias. The spatial relationships between pixel values can be measured without bias using the Grey Level Co-occurence Matrix (GLCM), which was found to match the cardinality and comparative value of the MOS while reducing subjectivity and automating the analytical process. In this work, the GLCM is also directly used as a loss function to guide the generation of perceptually accurate images based on spatial collocation of pixel values. We compare GLCM based loss against scenarios where (1) no intermediate guiding loss function, and (2) the VGG feature function are used. Experimental validation is carried on X-ray images of rock samples, characterised by significant number of high frequency texture features. We find GLCM-based loss to result in images with higher pixelwise accuracy and better perceptual scores.", "keywords": ["Super Resolution Generative Adversarial Networks", "Perceptual Loss Functions"], "paperhash": "wang|pixel_cooccurence_based_loss_metrics_for_super_resolution_texture_recovery", "original_pdf": "/attachment/d611e52ebd5e2adfb9a62ba2b859d54ea1a15313.pdf", "_bibtex": "@misc{\nwang2020pixel,\ntitle={Pixel Co-Occurence Based Loss Metrics for Super Resolution Texture Recovery},\nauthor={Ying Da Wang and Pawel Swietojanski and Ryan T Armstrong and Peyman Mostaghimi},\nyear={2020},\nurl={https://openreview.net/forum?id=rylrI1HtPr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "2aPZHzwv7c", "original": null, "number": 1, "cdate": 1576798730927, "ddate": null, "tcdate": 1576798730927, "tmdate": 1576800905549, "tddate": null, "forum": "rylrI1HtPr", "replyto": "rylrI1HtPr", "invitation": "ICLR.cc/2020/Conference/Paper1728/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes to use the grey level co-occurrence matrix method (GLCM) for both the performance evaluation metric and an auxiliary loss function for single image super resolution. Experiments are conducted on X-ray images of rock samples. Three reviewers provide comments. Two reviewers rated reject while one rated weak reject. The major concerns include the lack of clear and detailed description, low novelty, limited experiment on only one database, unconvincing improvement over the prior work, etc. The authors agree that the limited experiment on one database does not demonstrate the generalization capability of the proposed method. The AC agrees with the reviewers\u2019 comments, and recommend rejection.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yingda.wang@unsw.edu.au", "p.swietojanski@unsw.edu.au", "ryan.armstrong@unsw.edu.au", "peyman@unsw.edu.au"], "title": "Pixel Co-Occurence Based Loss Metrics for Super Resolution Texture Recovery", "authors": ["Ying Da Wang", "Pawel Swietojanski", "Ryan T Armstrong", "Peyman Mostaghimi"], "pdf": "/pdf/d611e52ebd5e2adfb9a62ba2b859d54ea1a15313.pdf", "TL;DR": "We introduce an unbiased perceptual loss function and metric and show that it improves recovery of texture during super resolution", "abstract": "Single Image Super Resolution (SISR) has significantly improved with Convolutional Neural Networks (CNNs) and Generative Adversarial Networks (GANs), often achieving order of magnitude better pixelwise accuracies (distortions) and state-of-the-art perceptual accuracy. Due to the stochastic nature of GAN reconstruction and the ill-posed nature of the problem, perceptual accuracy tends to correlate inversely with pixelwise accuracy which is especially detrimental to SISR, where preservation of original content is an objective. GAN stochastics can be guided by intermediate loss functions such as the VGG featurewise loss, but these features are typically derived from biased pre-trained networks. Similarly, measurements of perceptual quality such as the human Mean Opinion Score (MOS) and no-reference measures have issues with pre-trained bias. The spatial relationships between pixel values can be measured without bias using the Grey Level Co-occurence Matrix (GLCM), which was found to match the cardinality and comparative value of the MOS while reducing subjectivity and automating the analytical process. In this work, the GLCM is also directly used as a loss function to guide the generation of perceptually accurate images based on spatial collocation of pixel values. We compare GLCM based loss against scenarios where (1) no intermediate guiding loss function, and (2) the VGG feature function are used. Experimental validation is carried on X-ray images of rock samples, characterised by significant number of high frequency texture features. We find GLCM-based loss to result in images with higher pixelwise accuracy and better perceptual scores.", "keywords": ["Super Resolution Generative Adversarial Networks", "Perceptual Loss Functions"], "paperhash": "wang|pixel_cooccurence_based_loss_metrics_for_super_resolution_texture_recovery", "original_pdf": "/attachment/d611e52ebd5e2adfb9a62ba2b859d54ea1a15313.pdf", "_bibtex": "@misc{\nwang2020pixel,\ntitle={Pixel Co-Occurence Based Loss Metrics for Super Resolution Texture Recovery},\nauthor={Ying Da Wang and Pawel Swietojanski and Ryan T Armstrong and Peyman Mostaghimi},\nyear={2020},\nurl={https://openreview.net/forum?id=rylrI1HtPr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rylrI1HtPr", "replyto": "rylrI1HtPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795726323, "tmdate": 1576800278436, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1728/-/Decision"}}}, {"id": "HyguxlXIoS", "original": null, "number": 1, "cdate": 1573429232038, "ddate": null, "tcdate": 1573429232038, "tmdate": 1573429232038, "tddate": null, "forum": "rylrI1HtPr", "replyto": "Hyl0aS9aYr", "invitation": "ICLR.cc/2020/Conference/Paper1728/-/Official_Comment", "content": {"title": "Novelty and Experiments", "comment": "Regarding the comments:\n\n> We agree that the method we propose should be further tested on other image types, such as medical images and natural images. These experiments are in progress\n\n> The novelty of this work is the summation of its parts and the performance it can obtain. As we state in the previous point, we agree that results would be more convincing and general with reported results on natural images and other X-ray images."}, "signatures": ["ICLR.cc/2020/Conference/Paper1728/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1728/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yingda.wang@unsw.edu.au", "p.swietojanski@unsw.edu.au", "ryan.armstrong@unsw.edu.au", "peyman@unsw.edu.au"], "title": "Pixel Co-Occurence Based Loss Metrics for Super Resolution Texture Recovery", "authors": ["Ying Da Wang", "Pawel Swietojanski", "Ryan T Armstrong", "Peyman Mostaghimi"], "pdf": "/pdf/d611e52ebd5e2adfb9a62ba2b859d54ea1a15313.pdf", "TL;DR": "We introduce an unbiased perceptual loss function and metric and show that it improves recovery of texture during super resolution", "abstract": "Single Image Super Resolution (SISR) has significantly improved with Convolutional Neural Networks (CNNs) and Generative Adversarial Networks (GANs), often achieving order of magnitude better pixelwise accuracies (distortions) and state-of-the-art perceptual accuracy. Due to the stochastic nature of GAN reconstruction and the ill-posed nature of the problem, perceptual accuracy tends to correlate inversely with pixelwise accuracy which is especially detrimental to SISR, where preservation of original content is an objective. GAN stochastics can be guided by intermediate loss functions such as the VGG featurewise loss, but these features are typically derived from biased pre-trained networks. Similarly, measurements of perceptual quality such as the human Mean Opinion Score (MOS) and no-reference measures have issues with pre-trained bias. The spatial relationships between pixel values can be measured without bias using the Grey Level Co-occurence Matrix (GLCM), which was found to match the cardinality and comparative value of the MOS while reducing subjectivity and automating the analytical process. In this work, the GLCM is also directly used as a loss function to guide the generation of perceptually accurate images based on spatial collocation of pixel values. We compare GLCM based loss against scenarios where (1) no intermediate guiding loss function, and (2) the VGG feature function are used. Experimental validation is carried on X-ray images of rock samples, characterised by significant number of high frequency texture features. We find GLCM-based loss to result in images with higher pixelwise accuracy and better perceptual scores.", "keywords": ["Super Resolution Generative Adversarial Networks", "Perceptual Loss Functions"], "paperhash": "wang|pixel_cooccurence_based_loss_metrics_for_super_resolution_texture_recovery", "original_pdf": "/attachment/d611e52ebd5e2adfb9a62ba2b859d54ea1a15313.pdf", "_bibtex": "@misc{\nwang2020pixel,\ntitle={Pixel Co-Occurence Based Loss Metrics for Super Resolution Texture Recovery},\nauthor={Ying Da Wang and Pawel Swietojanski and Ryan T Armstrong and Peyman Mostaghimi},\nyear={2020},\nurl={https://openreview.net/forum?id=rylrI1HtPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rylrI1HtPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1728/Authors", "ICLR.cc/2020/Conference/Paper1728/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1728/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1728/Reviewers", "ICLR.cc/2020/Conference/Paper1728/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1728/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1728/Authors|ICLR.cc/2020/Conference/Paper1728/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504151748, "tmdate": 1576860556650, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1728/Authors", "ICLR.cc/2020/Conference/Paper1728/Reviewers", "ICLR.cc/2020/Conference/Paper1728/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1728/-/Official_Comment"}}}, {"id": "HygS97ittS", "original": null, "number": 1, "cdate": 1571562380688, "ddate": null, "tcdate": 1571562380688, "tmdate": 1572972431161, "tddate": null, "forum": "rylrI1HtPr", "replyto": "rylrI1HtPr", "invitation": "ICLR.cc/2020/Conference/Paper1728/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper addresses the super-resolution problem. The key is to use pixel co-occurrence-based loss metric. The idea is very straightforward. But the description could be clearer. For example, what is the spatial size of P (\\bar P)? How does it influence the optimization?\n\nEquation (2): There are four loss functions on the right hand. How are the loss defined?\n\nHow is the GAN used?\n\nIn experiments, there is no evidence showing the benefit from the pixel Co-occurrence\n\nThere is a lack of much details. Given the current presentation, I cannot judge if the quality reaches the ICLR bar. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1728/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1728/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yingda.wang@unsw.edu.au", "p.swietojanski@unsw.edu.au", "ryan.armstrong@unsw.edu.au", "peyman@unsw.edu.au"], "title": "Pixel Co-Occurence Based Loss Metrics for Super Resolution Texture Recovery", "authors": ["Ying Da Wang", "Pawel Swietojanski", "Ryan T Armstrong", "Peyman Mostaghimi"], "pdf": "/pdf/d611e52ebd5e2adfb9a62ba2b859d54ea1a15313.pdf", "TL;DR": "We introduce an unbiased perceptual loss function and metric and show that it improves recovery of texture during super resolution", "abstract": "Single Image Super Resolution (SISR) has significantly improved with Convolutional Neural Networks (CNNs) and Generative Adversarial Networks (GANs), often achieving order of magnitude better pixelwise accuracies (distortions) and state-of-the-art perceptual accuracy. Due to the stochastic nature of GAN reconstruction and the ill-posed nature of the problem, perceptual accuracy tends to correlate inversely with pixelwise accuracy which is especially detrimental to SISR, where preservation of original content is an objective. GAN stochastics can be guided by intermediate loss functions such as the VGG featurewise loss, but these features are typically derived from biased pre-trained networks. Similarly, measurements of perceptual quality such as the human Mean Opinion Score (MOS) and no-reference measures have issues with pre-trained bias. The spatial relationships between pixel values can be measured without bias using the Grey Level Co-occurence Matrix (GLCM), which was found to match the cardinality and comparative value of the MOS while reducing subjectivity and automating the analytical process. In this work, the GLCM is also directly used as a loss function to guide the generation of perceptually accurate images based on spatial collocation of pixel values. We compare GLCM based loss against scenarios where (1) no intermediate guiding loss function, and (2) the VGG feature function are used. Experimental validation is carried on X-ray images of rock samples, characterised by significant number of high frequency texture features. We find GLCM-based loss to result in images with higher pixelwise accuracy and better perceptual scores.", "keywords": ["Super Resolution Generative Adversarial Networks", "Perceptual Loss Functions"], "paperhash": "wang|pixel_cooccurence_based_loss_metrics_for_super_resolution_texture_recovery", "original_pdf": "/attachment/d611e52ebd5e2adfb9a62ba2b859d54ea1a15313.pdf", "_bibtex": "@misc{\nwang2020pixel,\ntitle={Pixel Co-Occurence Based Loss Metrics for Super Resolution Texture Recovery},\nauthor={Ying Da Wang and Pawel Swietojanski and Ryan T Armstrong and Peyman Mostaghimi},\nyear={2020},\nurl={https://openreview.net/forum?id=rylrI1HtPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rylrI1HtPr", "replyto": "rylrI1HtPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1728/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1728/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575518492057, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1728/Reviewers"], "noninvitees": [], "tcdate": 1570237733164, "tmdate": 1575518492072, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1728/-/Official_Review"}}}, {"id": "SJlcwtH2tB", "original": null, "number": 2, "cdate": 1571735906338, "ddate": null, "tcdate": 1571735906338, "tmdate": 1572972431118, "tddate": null, "forum": "rylrI1HtPr", "replyto": "rylrI1HtPr", "invitation": "ICLR.cc/2020/Conference/Paper1728/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper considers the problem of generating a high-resolution image from a low-resolution one. The paper introduces the Grey Level Co-occurrence Matrix Method (GLCM) for evaluating the performance of super-resolution techniques and as an auxiliary loss function for training neural networks to perform well for super-resolution. The GLCM was originally introduced in a 1973 paper and has been used in a few papers in the computer vision community. The paper trains and validates a super-resolution GAN (SRGAN)  and a super-resolution CNN (SRCNN) on the DeepRock-SR dataset. Specifically, for the SRGAN, the paper uses the EDSRGAN network trained on loss function particular to the paper: The loss function consists of the addition of the L1 pixel-wise loss plus the VGG19 perceptual objective plus the proposed GLCM loss.\nThe paper finds that SRCNN outperforms SRGAN in terms of the PSNR metric, but SRGAN performs better in terms of the spatial texture similarity.  \nNext, the paper shows that when trained with the mean L1-GMCM loss function, SRGAN performs best.\n\nIn summary, the paper proposes to use the GMCM loss for training and evaluation of the super-resolution methods. However, this metric is well known in the computer-vision community along with many others. Also, the idea to use this metric in training is only evaluated for one network (albeit a very sensible one) and only for one dataset (the DeepRock one). Since the novelty provided by the paper is small, I cannot recommend the acceptance of the paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper1728/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1728/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yingda.wang@unsw.edu.au", "p.swietojanski@unsw.edu.au", "ryan.armstrong@unsw.edu.au", "peyman@unsw.edu.au"], "title": "Pixel Co-Occurence Based Loss Metrics for Super Resolution Texture Recovery", "authors": ["Ying Da Wang", "Pawel Swietojanski", "Ryan T Armstrong", "Peyman Mostaghimi"], "pdf": "/pdf/d611e52ebd5e2adfb9a62ba2b859d54ea1a15313.pdf", "TL;DR": "We introduce an unbiased perceptual loss function and metric and show that it improves recovery of texture during super resolution", "abstract": "Single Image Super Resolution (SISR) has significantly improved with Convolutional Neural Networks (CNNs) and Generative Adversarial Networks (GANs), often achieving order of magnitude better pixelwise accuracies (distortions) and state-of-the-art perceptual accuracy. Due to the stochastic nature of GAN reconstruction and the ill-posed nature of the problem, perceptual accuracy tends to correlate inversely with pixelwise accuracy which is especially detrimental to SISR, where preservation of original content is an objective. GAN stochastics can be guided by intermediate loss functions such as the VGG featurewise loss, but these features are typically derived from biased pre-trained networks. Similarly, measurements of perceptual quality such as the human Mean Opinion Score (MOS) and no-reference measures have issues with pre-trained bias. The spatial relationships between pixel values can be measured without bias using the Grey Level Co-occurence Matrix (GLCM), which was found to match the cardinality and comparative value of the MOS while reducing subjectivity and automating the analytical process. In this work, the GLCM is also directly used as a loss function to guide the generation of perceptually accurate images based on spatial collocation of pixel values. We compare GLCM based loss against scenarios where (1) no intermediate guiding loss function, and (2) the VGG feature function are used. Experimental validation is carried on X-ray images of rock samples, characterised by significant number of high frequency texture features. We find GLCM-based loss to result in images with higher pixelwise accuracy and better perceptual scores.", "keywords": ["Super Resolution Generative Adversarial Networks", "Perceptual Loss Functions"], "paperhash": "wang|pixel_cooccurence_based_loss_metrics_for_super_resolution_texture_recovery", "original_pdf": "/attachment/d611e52ebd5e2adfb9a62ba2b859d54ea1a15313.pdf", "_bibtex": "@misc{\nwang2020pixel,\ntitle={Pixel Co-Occurence Based Loss Metrics for Super Resolution Texture Recovery},\nauthor={Ying Da Wang and Pawel Swietojanski and Ryan T Armstrong and Peyman Mostaghimi},\nyear={2020},\nurl={https://openreview.net/forum?id=rylrI1HtPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rylrI1HtPr", "replyto": "rylrI1HtPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1728/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1728/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575518492057, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1728/Reviewers"], "noninvitees": [], "tcdate": 1570237733164, "tmdate": 1575518492072, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1728/-/Official_Review"}}}, {"id": "Hyl0aS9aYr", "original": null, "number": 3, "cdate": 1571820997746, "ddate": null, "tcdate": 1571820997746, "tmdate": 1572972431085, "tddate": null, "forum": "rylrI1HtPr", "replyto": "rylrI1HtPr", "invitation": "ICLR.cc/2020/Conference/Paper1728/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper adopts a loss metric called Grey Level Co-occurence Matrix (GLCM) as a new measurement of perceptual quality for single image super-resolution. The GLCM is particularly well suited for automatic perceptual/textural in-domain comparisons, which does not require time-consuming expert MOS evaluations. Experimental validation is carried on X-ray images of rock samples and promising results are achieved.\n\nMy main concerns are as follows:\n- Novelty is quite limited. First, as the main contribution of this work, GLCM is proposed from Haralick et al.(1973), which is not novel. Second, the network structure used in this work is also based on SRGAN/EDSRGAN, which is also not novel.\n\n- Experiments are not convincing. In super-resolution task, the differences in Tab. 2 are quite minor, which can also be regarded as the same or may be caused by randomness. Moreover, the authors should conduct experiments on generic image SR to demonstrate the effectiveness of GLCM. Further, more qualitative visualizations are also needed to demonstrate the effectiveness of GLCM.\n\n- Page 5 only contains 1 figure, leaving a lot of  space that is not fully used. Besides, Fig. 3 cannot reflect the advantage of GLCM, since SRGAN is much larger and also better than SRCNN."}, "signatures": ["ICLR.cc/2020/Conference/Paper1728/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1728/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yingda.wang@unsw.edu.au", "p.swietojanski@unsw.edu.au", "ryan.armstrong@unsw.edu.au", "peyman@unsw.edu.au"], "title": "Pixel Co-Occurence Based Loss Metrics for Super Resolution Texture Recovery", "authors": ["Ying Da Wang", "Pawel Swietojanski", "Ryan T Armstrong", "Peyman Mostaghimi"], "pdf": "/pdf/d611e52ebd5e2adfb9a62ba2b859d54ea1a15313.pdf", "TL;DR": "We introduce an unbiased perceptual loss function and metric and show that it improves recovery of texture during super resolution", "abstract": "Single Image Super Resolution (SISR) has significantly improved with Convolutional Neural Networks (CNNs) and Generative Adversarial Networks (GANs), often achieving order of magnitude better pixelwise accuracies (distortions) and state-of-the-art perceptual accuracy. Due to the stochastic nature of GAN reconstruction and the ill-posed nature of the problem, perceptual accuracy tends to correlate inversely with pixelwise accuracy which is especially detrimental to SISR, where preservation of original content is an objective. GAN stochastics can be guided by intermediate loss functions such as the VGG featurewise loss, but these features are typically derived from biased pre-trained networks. Similarly, measurements of perceptual quality such as the human Mean Opinion Score (MOS) and no-reference measures have issues with pre-trained bias. The spatial relationships between pixel values can be measured without bias using the Grey Level Co-occurence Matrix (GLCM), which was found to match the cardinality and comparative value of the MOS while reducing subjectivity and automating the analytical process. In this work, the GLCM is also directly used as a loss function to guide the generation of perceptually accurate images based on spatial collocation of pixel values. We compare GLCM based loss against scenarios where (1) no intermediate guiding loss function, and (2) the VGG feature function are used. Experimental validation is carried on X-ray images of rock samples, characterised by significant number of high frequency texture features. We find GLCM-based loss to result in images with higher pixelwise accuracy and better perceptual scores.", "keywords": ["Super Resolution Generative Adversarial Networks", "Perceptual Loss Functions"], "paperhash": "wang|pixel_cooccurence_based_loss_metrics_for_super_resolution_texture_recovery", "original_pdf": "/attachment/d611e52ebd5e2adfb9a62ba2b859d54ea1a15313.pdf", "_bibtex": "@misc{\nwang2020pixel,\ntitle={Pixel Co-Occurence Based Loss Metrics for Super Resolution Texture Recovery},\nauthor={Ying Da Wang and Pawel Swietojanski and Ryan T Armstrong and Peyman Mostaghimi},\nyear={2020},\nurl={https://openreview.net/forum?id=rylrI1HtPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rylrI1HtPr", "replyto": "rylrI1HtPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1728/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1728/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575518492057, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1728/Reviewers"], "noninvitees": [], "tcdate": 1570237733164, "tmdate": 1575518492072, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1728/-/Official_Review"}}}], "count": 6}