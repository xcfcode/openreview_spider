{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1392750720000, "tcdate": 1392750720000, "number": 9, "id": "ttNb0MnzpZ0_v", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "bSaT4mmQt84Lx", "replyto": "bSaT4mmQt84Lx", "signatures": ["Razvan Pascanu"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We have posted a new revision (v5) of out manuscript. In this revision we address the reviewers' comments. \r\nThe most important changes are: \r\n\r\n* We reduced the length of the introduction and moved some of the detailed descriptions or intuitions near the relevant propositions or theorems. \r\n\r\n* We fixed some shortcomings of an example that was given in the Introduction of the previous version of the manuscript. \r\n\r\n* We added missing formal definitions and worked on making our notation more consistent and rigorous. \r\n\r\n* We overworked the proof of the former Theorem 8 (now Theorem 1). We included new diagrams illustrating the steps of the proof. We also included an example (Example 1) illustrating how the components of the proof are put together in the proof. \r\n\r\n* We added a new section (Section 5) describing a construction of weights for which deep models exhibit more linear regions than shallow ones, even for a small number of hidden layers. In this section we also illustrate specific functions that can be represented with this choice of weights. \r\n\r\n* We formulated bounds in terms of the number of regions per parameter computable by rectifier networks. These bounds behave similarly to the bounds expressed in terms of the number of units, showing that deep models are exponentially more efficient than shallow models."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "On the number of inference regions of deep feed forward networks with piece-wise linear activations", "decision": "submitted, no decision", "abstract": "This paper explores the complexity of deep feed forward networks with linear presynaptic couplings and rectified linear activations. This is a contribution to the growing body of work contrasting the representational power of deep and shallow network architectures. In particular, we offer a framework for comparing deep and shallow models that belong to the family of piece-wise linear functions based on computational geometry. We look at a deep (two hidden layers) rectifier multilayer perceptron (MLP) with linear outputs units and compare it with a single layer version of the model. In the asymptotic regime as the number of units goes to infinity, if the shallow model has $2n$ hidden units and $n_0$ inputs, then the number of linear regions is $O(n^{n_0})$. A two layer model with $n$ number of hidden units on each layer has $Omega(n^{n_0})$. We consider this as a first step towards understanding the complexity of these models and argue that better constructions in this framework might provide more accurate comparisons (especially for the interesting case of when the number of hidden layers goes to infinity).", "pdf": "https://arxiv.org/abs/1312.6098", "paperhash": "pascanu|on_the_number_of_inference_regions_of_deep_feed_forward_networks_with_piecewise_linear_activations", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Guido F. Montufar", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "guidomontufar@googlemail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392750720000, "tcdate": 1392750720000, "number": 1, "id": "26C7gYY01ohqX", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "bSaT4mmQt84Lx", "replyto": "s5xX4vEkuAnIR", "signatures": ["Razvan Pascanu"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "We appreciate the detailed comments of Reviewer 2699. They were very helpful for preparing the present revision of the manuscript. In the following we address all comments of the reviewer and give description of the changes made to the manuscript. \r\n\r\nIn response to the general comments we \r\n\r\n* Changed the title of the manuscript to ``On the number of response regions of deep feedforward networks with piecewise linear activations''\r\n\r\n* Shortened the Introduction (and removed an erroneous example that was given there)\r\n\r\n* Completely overworked the proof of the former Theorem 8 (now Theorem 1). \r\n\r\n* Moved the asymptotic analysis to the appendix\r\n\r\n* Included a new section (Section 5) discussing tighter bounds for deep models. \r\n\r\nIn the following we address the detailed comments. \r\n\r\n* ``Computational geometry'' refers to the study of algorithms using geometry. Here, using the word ``computational'' is just a matter of taste. Our motivation is that a neural network is a computational system and an algorithm (compute output of unit $i$ for $iin [n]$, sum the outputs of units $iin[n]$, etc.).  \r\n\r\n* We included a reference to Hajnal's work in the Introduction. \r\n\r\n* We included pointers to the work of Montufar et al. and Martens et al. in the Introduction. \r\n\r\n* We removed the long discussion from the Introduction and decided, instead, to include an example (Example 1) in the vicinity of the main theorem (Theorem 1). \r\n\r\n* We included several definitions and worked on making our formulations more precise. \r\n\r\n* We corrected a missing reference to Zaslavsky's work. \r\n\r\n* We included the formal definition of ``general position'' and comments on infinitesimal perturbations.\r\n\r\n* We no longer use the expression ``relative position'' For clarity, in the previous manuscript, the definition was as follows: Two arrangements have the same relative position if they are combinatorially equivalent, or more formally, if there is a bijection of their intersection posets, where the intersection poset of an arrangement is the set of all nonempty intersection of its hyperplanes partially ordered by reverse inclusion.\r\n\r\n* We reformulated the former Proposition 6 in terms of scaling and shifting, moving technical details to the proof, and avoiding the use of the expression ``relative position''. \r\n\r\n* We corrected the former Figure 3, which now shows just 1 dotted circle instead of 2. \r\n\r\n* We explained the notion of ``essentialization'' in more detail. Proposition 4 describes the combinatorics of $n$-dimensional arrangements with $2$-dimensional essentialization; that is, arrangements of hyperplanes whose intersections with the span of their normal vectors build a $2$-dimensional arrangement (on the span of the normal vectors). \r\n\r\n* We corrected '${0,...,n}, a < b' -> '{0,...,n} s.t. a < b$'. \r\n\r\n* We improved our formulations, especially about ``independent groups of units'' and ``enumeration'' of the hyperplanes in an arrangement. \r\n\r\n* We made significant efforts in clarifying how the construction works, bottom to top. \r\n\r\n* We improved the formulations ``we find'', ``groups'', trying to make the arguments more formal and clearer. \r\n\r\n* The reviewer asked how Proposition 7 was used in the proof of the theorem. Using the same activation weights for a collection of units would cause them to behave identically. The entire collection of units would have an output of dimension at most one, which would not be useful for our proof. Perturbing the weights to produce a full dimensional matrix would work. A high level proof could be formulated in this way. We found it important to give an explicit choice of weights for which certain well defined properties hold, instead of relying only on high level arguments, in particular, because this allows us to verify the accuracy of our intuitions. \r\n\r\nIn fact, our construction is stable, in the sense that small perturbations of the specified weights cause only small perturbations of the computed function. The resulting perturbed function has at least as many linear regions as the original one. \r\n \r\n* The word `decompose' was meant in the opposite way that `compose' is used for compositions of functions, $fcirc g$. Thanks for the comment, we tried to use more precise expressions. \r\n\r\n* The reviewer asked \r\n``Page 9: What does it mean for a linear map to be 'with Ii-coordinates equal\r\nto...'?  The 'Ii-coordinates' are the inputs to this map? The outputs? ''\r\n\r\nWe tried to make this more precise in the revision. For clarity, the terminology is the standard one: \r\nA ``coordinate'' of a map $f : R^n\to R^m; (x_1,ldots, x_n) mapsto (f_1(x_1,ldots, x_n), ldots, f_m(x_1,dots, x_n))$ is any of the functions $f_i: R^n \to R; (x_1,ldots, x_n)mapsto f_i(x_1,ldots, x_n)$ for $i=1,ldots, m$.  Given a subset $I$ of ${1,ldots, m}$, the $I$-coordinates of the map $f$ are the functions $f_i$ with $iin I$. \r\nFor example, if $I={i_1,ldots, i_{|I|} }subseteq {1,ldots, m}$, we can consider the map defined by the $I$-coordinates of $f$, which is the map $f_I: R^n\to R^{|I|}; (x_1,ldots, x_n) mapsto (f_{i_1}(x_1,ldots, x_n) , ldots, f_{i_{|I|}}(x_1,ldots, x_n) )$. \r\n\r\n* The reviewer asked ``What is $R_i^{(1)}$? It is never defined...''. \r\n\r\nWe worked on better explaining the notation and using it uniformly. \r\n\r\n* The reviewer wrote ``Page 9: You say that something 'passed through' $\rho^2$ is the 'input' to the''... \r\n\r\nWe improved the terminology. \r\n\r\n* Also ``Page 9: How can $\rho_i^(2)(R_i^{(1)})$ be a 'subset' of an subspace that lives...''\r\n\r\nWe overworked these parts as well. \r\n\r\n* The reviewer suggested a new construction for proving statements about deep models. We do not argue that our construction or our analysis yields the maximal number of regions of linearity. It merely demonstrates that deep models are exponentially more efficient than shallow models. In the revision we included a new construction of weights of deep rectifier networks (in Section 5), which shows tighter bounds for certain choices of layer widths. Other constructions exploiting higher dimensional versions of the former Proposition 7 are worth studying in the future, in order to arrive at yet tighter bounds. \r\n\r\n* We moved the asymptotic analysis to the appendix. In the Discussion we included comments about the number of linear regions computable per parameter."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "On the number of inference regions of deep feed forward networks with piece-wise linear activations", "decision": "submitted, no decision", "abstract": "This paper explores the complexity of deep feed forward networks with linear presynaptic couplings and rectified linear activations. This is a contribution to the growing body of work contrasting the representational power of deep and shallow network architectures. In particular, we offer a framework for comparing deep and shallow models that belong to the family of piece-wise linear functions based on computational geometry. We look at a deep (two hidden layers) rectifier multilayer perceptron (MLP) with linear outputs units and compare it with a single layer version of the model. In the asymptotic regime as the number of units goes to infinity, if the shallow model has $2n$ hidden units and $n_0$ inputs, then the number of linear regions is $O(n^{n_0})$. A two layer model with $n$ number of hidden units on each layer has $Omega(n^{n_0})$. We consider this as a first step towards understanding the complexity of these models and argue that better constructions in this framework might provide more accurate comparisons (especially for the interesting case of when the number of hidden layers goes to infinity).", "pdf": "https://arxiv.org/abs/1312.6098", "paperhash": "pascanu|on_the_number_of_inference_regions_of_deep_feed_forward_networks_with_piecewise_linear_activations", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Guido F. Montufar", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "guidomontufar@googlemail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392750600000, "tcdate": 1392750600000, "number": 1, "id": "_xtmg4TcjB_5l", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "bSaT4mmQt84Lx", "replyto": "fjB3fGG430jr7", "signatures": ["Razvan Pascanu"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thank you for your comments Reviewer 67e9. We have carefully considered them and integrated them in the new version of the paper, which is now available on arxiv (v5). In what follows let us answer to some of your concerns. \r\n\r\n'But there are several problems with the paper. The writing is unclear, and overall the paper feels like a preliminary draft, not ready for prime time.'\r\n\r\nWe think the paper has improved steadily since the initial submission and we hope that, in the new version, it is clear and up to ICLR quality standards. \r\nThe paper offers a new perspective on a hard question and we think that the presented ideas can be useful for addressing a variety of related problems. \r\n\r\n'The introduction can be tightened up.'\r\n\r\nWe shortened the Introduction. \r\n\r\n'A more significant comment concerns the important attempt to give some intuition, at the top of page 3...'\r\n\r\nIn our attempt to provide a simplest possible example of the mechanism behind our proof, we unfortunately made a mistake. You are right, with a single input unit it is not possible to networks for which distinct units are active at different input intervals in the way that it was claimed in that example. Thank you for pointing this out. We fixed the mistake. Proposition 7 (now Proposition 4) indicates the relevant conditions. \r\n\r\n'I think the basic intuition is that a higher-level unit can act as an OR of several lower level regions...'\r\n\r\nExactly, this is the intuition that we were trying to convey in that paragraph. This intuition builds the main mechanism behind our proof of the main theorem. We hope that with the changes made to the manuscript the construction is now clearer. \r\n\r\n'Also, one would expect that the ability to represent AND operations would also lead to significant expressivity gains. '\r\n\r\nThe offered proof relies only on the OR operation, but one should be careful about what this means exactly. \r\nSpecifically,  we do not compute the OR between two values and provide the result as output of the layer. Instead, what this OR operations describes is that some particular output value can be obtained from various inputs input1 OR input2 OR input3, etc. In this context an AND operation does not make sense. \r\n\r\nWhat we are describing here is a function that is not injective, i.e., which has distinct domain values that are mapped to the same output value. Of course, the injectivity is lifted to the level of domain or input regions rather than individual input values. However you can see how AND becomes impossible to express in these terms. \r\n\r\n'In addition, it would be very helpful to have some concrete example of a function that can be computed by a deep net of the sort analyzed here...'\r\n\r\nThank you for the suggestion. We included a description of more intuitive classes of functions computable by rectifier models in Section 5, together with toy examples with 2-dimensional inputs. \r\nWhile Theorem focuses more on the asymptotic regime, the new construction given in Section 5 shows that there are classes of functions that can be computed far more efficiently by deep networks than by shallow ones, even if the number of layers of the deep networks is relatively small, say equal to 3 or 4. \r\n\r\n'I also found the proof of Theorem 8 very hard to understand..'\r\n\r\nWe completely overworked that proof, putting attention to the consistency of the notation and maintaining the mathematics precise. We extracted parts of the proof into propositions, in order to make the steps clearer.\r\n\r\n'Finally, I would recommend exploring the relationship between the ideas in this paper and the extensive work in circuit complexity... '\r\n\r\nThank you. We will look carefully at that literature. There might be some interesting connections."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "On the number of inference regions of deep feed forward networks with piece-wise linear activations", "decision": "submitted, no decision", "abstract": "This paper explores the complexity of deep feed forward networks with linear presynaptic couplings and rectified linear activations. This is a contribution to the growing body of work contrasting the representational power of deep and shallow network architectures. In particular, we offer a framework for comparing deep and shallow models that belong to the family of piece-wise linear functions based on computational geometry. We look at a deep (two hidden layers) rectifier multilayer perceptron (MLP) with linear outputs units and compare it with a single layer version of the model. In the asymptotic regime as the number of units goes to infinity, if the shallow model has $2n$ hidden units and $n_0$ inputs, then the number of linear regions is $O(n^{n_0})$. A two layer model with $n$ number of hidden units on each layer has $Omega(n^{n_0})$. We consider this as a first step towards understanding the complexity of these models and argue that better constructions in this framework might provide more accurate comparisons (especially for the interesting case of when the number of hidden layers goes to infinity).", "pdf": "https://arxiv.org/abs/1312.6098", "paperhash": "pascanu|on_the_number_of_inference_regions_of_deep_feed_forward_networks_with_piecewise_linear_activations", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Guido F. Montufar", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "guidomontufar@googlemail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392137460000, "tcdate": 1392137460000, "number": 8, "id": "s5xX4vEkuAnIR", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "bSaT4mmQt84Lx", "replyto": "bSaT4mmQt84Lx", "signatures": ["anonymous reviewer 2699"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of On the number of inference regions of deep feed forward networks with piece-wise linear activations", "review": "The authors of this paper analyse feed-forward networks of linear rectifier units (RELUs) in terms of the number of regions in which they act linearly.  They give an upper bound on the number of regions for networks with a single hidden layer based on known results in geometry, and then show how deeper networks can have a much larger number of regions by constructing examples.   The constructions form the main novel technical contribution, and they seem non-trivial and interesting.\r\n\r\nOverall I think this is a good and interesting paper.  It is well written with the notable exception of the proof of theorem 8 and the latter half of the introduction.  In most spots, the math is precise and accessible (to me anyway), the results nicely broken into lemmas, and the diagrams are very useful for providing intuition.\r\n\r\nThese results can be interpreted as separating networks with a single hidden layer from deep networks in terms of the types of functions they can efficiently compute.  However, number of linear regions is a pretty abstract notion, and it isn't obvious what these results can say about the expressibility by neural nets of functions that we can actually write down.  Do you know of any natural examples of functions that require a finite but super-exponential number of regions?  \r\n\r\nUnfortunately, region counting can't say anything about the representability of functions defined on such input spaces of the form S^n0 where S is a finite set, since there are only |S|^n0 input values, and |S|^n0 << n^n0 = region upper bound.\r\n\r\n\r\nAbout Theorem 8: \r\n\r\nAfter hours trying to understand the proof of Theorem 8 I gave up.  However, I was able to use Prop 7, and intuition provided from the diagrams, to prove a slightly different version of Thm 8 myself, and so I think the result is correct, and the proof is probably trying to describe basically the same thing I came up with (except my proof went from the top layer down, instead of the bottom layer up).  So while I don't doubt the correctness of the statement of Thm 8, but the write-up of the proof of Thm 8 needs to be completely redone to be understandable and intuitive.  I don't think you need to make it 100% formal (Prop 7 isn't completely formal either, but it's fine as is), but you need to make it possible to understand with a reasonable amount of effort.\r\n\r\n\r\n\r\n\r\nDetailed comments:\r\n---\r\n\r\nTitle: Please pick a different title.  These are feed-forward networks so calling the regions 'inference regions' doesn't make sense.\r\n\r\nAbs: Why is it 'computational geometry' and not just 'geometry'?  What is specifically computational about arrangements of hyperplanes?\r\n\r\nPage 2: Missing from the review of previous results about the power networks is all of the work done on threshold units (see the papers of Wolfgang Maass for example, or the seminal of Hajnal et al. proving lower bounds for shallow threshold networks).  Unlike the single paper by Hastad et al. cited, none of these require the weights to be non-negative.  Moreover, these results are hardly non-realistic, as neural networks with sigmoids can easily simulate thresholds, and under certain assumptions the reverse simulation can be done approximately and reasonably efficiently too.\r\n\r\nAlso missing from this review is recent work of Montufar et al. and Martens et al. analysing the expressive power of generative models (RBMs).\r\n\r\nBeginning of page 3: I have a hard time following this high-level discussion.  I think this doesn't belong in the introduction, as it is too long and convoluted.  Instead, I think you should include such discussion as intuition about your formal constructions *as you give them*.  The way it is written right now, the discussion tries to be intuitive, precise, and comprehensive, and it doesn't really succeed at being any of these.\r\n\r\n\r\nPage 3: You should formally define what you mean by 'hyperplane' and 'arrangement'.   In particular, a hyperplane is the set of points defined by the equation, not the equation itself.  And if an arrangement is taken to be a set of hyperplanes (as per the usual definition), then the statement in Prop 6 isn't formal (although its meaning is still obvious).  In particular, how does a ball S 'intersect' with a set of hyperplanes?  Do you mean that it intersects with the union of the hyperplanes in the arrangement?  I know these are nit-picky points, but if you should try to be technically precise.\r\n\r\nPage 5: There is a missing reference here: 'Zaslavsky's theorem (?, Theorem A)'\r\n\r\nPage 5: You should explain the concept of general position precisely.  I don't know what 'generic weights' is supposed to mean, the actual definition has to do with lack of colinearity.  You might want to point out that any choice of hyperplanes can be infinitesimally perturbed so that they end up in general position.\r\n\r\nPage 6: 'Relative position' is never formally defined, and it's not immediately obvious what it means.\r\n\r\nPage 6: The explanation after the statement of Prop 6 is much clearer.  Perhaps you should just prove this (stronger) statement directly, and not the fairly opaque and abstract statement made in Prop 6.\r\n\r\nFigure 3: Why are there 2 dotted circles instead of just 1?\r\n\r\nPage 7: 'arrangements 2-dimensional essentialization'?\r\n\r\nPage 7: '{0,...,n}, a < b'  ->  '{0,...,n} s.t. a < b'\r\n\r\nPage 7: What do you mean by 'independent groups of n0 units within the n units of a layer'?  How can a unit be 'within' a unit?\r\n\r\nPage 7: What do you mean by an 'enumeration' of a 2-dimensional arrangement?  \r\n\r\n\r\nPage 9:  Below the statement of prop 7, it was suggested that the construction would be top down, where each group in a lower layer 'duplicates' the number of regions constructed by the layer above.  But the construction given here seems to proceed bottom up... at least in the structure of the proof.  This makes it less intuitive.\r\n\r\nPage 9:  The proof starts out very confusingly.  I wasn't able to follow the sentence 'Then we find...'.  These 'groups' haven't been formally defined at this stage, only informally alluded to.  \r\n\r\nAnd I have another question:  how is Prop 7 actually used here?  Merely to establish the existence of network where there is n/n0 regions that each only turn on for different groups Ii?  Isn't it trivial to construct such a thing?  i.e. take the input weights to the units in a given group to be all the same, and make sure the square n0xn0 matrix formed by taking the weight vector for each group is full-rank (e.g. the identity matrix)? \r\n\r\nI suspect the reason this doesn't work is that the dimensions would collapse to 1 since each unit in a group would behave identically.  However, one could then perturb the final expanded weight matrix to get a general position matrix, so that the subspace associated with each group wouldn't collapse to dimension 1.  Is there anything wrong with this?  \r\n\r\n\r\nPage 9: Don't say 'decompose' here.  'Decompose' implies you already have weights and are decomposing them into factors.  Instead, what you are doing is defining some weights to be a product of particular matrices.\r\n\r\nYou should emphasize that it is a common V shared by all the i.  This is easy to miss.\r\n\r\n\r\nPage 9: What does it mean for a linear map to be 'with Ii-coordinates equal to...'?  The 'Ii-coordinates' are the inputs to this map?  The outputs?\r\n\r\nPage 9: What is R_i^(1)?  It is never defined.  Is it different from the version without the superscript?  Is it defined implicitly the first time it is used?  If so, what is a 'region of activation values'?\r\n\r\nWhat is very confusing is that you use x to define this function, when I had the impression that x was for the original inputs only.  This isn't consistent with the h notation you use in figure 1 for example.\r\n\r\n\r\nPage 9: You say that something 'passed through' rho^2 is the 'input' to the second layer.  This is the input to the rectifier units without their weights (which I'm guessing are contained in rho^2)?  Or is it these units with the 'V' factor part of their weights only, but not the 'U' part?  This is all extremely confusing.  Without careful definitions of your notation it requires a lot more work on the part of the reader to understand what is going on. \r\n\r\nPage 9: How can rho_i^(2)(R_i^(1)) be a 'subset' of an subspace that lives in R^(n1)?  rho_i^(2)(R_i^(1)) is going to be a set of vectors living in R^(n0)!\r\n\r\n\r\nPage 10:  What do you mean when you say that 'this arrangement is repeated once in each region'?  What does it mean for an arrangement to be 'repeated in a region'? \r\n\r\nI feel like the proof becomes mostly a proof-by-diagram as this point.  Maybe you should have started off with this kind of diagram and the intuition of 'duplicating regions', explaining how composing piece-wise linear functions can achieve this kind of thing (which is really the critical point that gets glossed over), and then proceeding to show that you could formally construct each 'piece' required to do this.   And you should have done the construction starting at the top layer going down.\r\n\r\nHaving reconstructed the proof in a way that I actually understood it, it seemed that one could also proof that one can (prod_{i=1}^{k-1} n_i)/2^{k-1} * sum_i=0^2 (n_k choose i) regions, which in some cases might be a lot larger than the expression you arrived at.   Unlike your Thm 8 does, this version would actually need to use the fact the constructions are 2-dimensional in Prop 7.\r\n\r\nPage 11:  The asymptotic analysis is just very routine and uninteresting computations and should be in the appendix.  It breaks the flow of your paper.  I would much prefer to see more detailed commentary about the implications of Thm 8."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "On the number of inference regions of deep feed forward networks with piece-wise linear activations", "decision": "submitted, no decision", "abstract": "This paper explores the complexity of deep feed forward networks with linear presynaptic couplings and rectified linear activations. This is a contribution to the growing body of work contrasting the representational power of deep and shallow network architectures. In particular, we offer a framework for comparing deep and shallow models that belong to the family of piece-wise linear functions based on computational geometry. We look at a deep (two hidden layers) rectifier multilayer perceptron (MLP) with linear outputs units and compare it with a single layer version of the model. In the asymptotic regime as the number of units goes to infinity, if the shallow model has $2n$ hidden units and $n_0$ inputs, then the number of linear regions is $O(n^{n_0})$. A two layer model with $n$ number of hidden units on each layer has $Omega(n^{n_0})$. We consider this as a first step towards understanding the complexity of these models and argue that better constructions in this framework might provide more accurate comparisons (especially for the interesting case of when the number of hidden layers goes to infinity).", "pdf": "https://arxiv.org/abs/1312.6098", "paperhash": "pascanu|on_the_number_of_inference_regions_of_deep_feed_forward_networks_with_piecewise_linear_activations", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Guido F. Montufar", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "guidomontufar@googlemail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392105960000, "tcdate": 1392105960000, "number": 7, "id": "fjB3fGG430jr7", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "bSaT4mmQt84Lx", "replyto": "bSaT4mmQt84Lx", "signatures": ["anonymous reviewer 67e9"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of On the number of inference regions of deep feed forward networks with piece-wise linear activations", "review": "This is a very interesting and relevant paper, attempting to prove that\r\na deep neural net composed of rectified linear units and a linear output\r\nlayer is potentially significantly more powerful than a single layer\r\nnet with the same number of units.\r\n\r\nA strength of the paper is its constructive approach, building up\r\nan understanding of the expressiveness of a model, in terms of the\r\nnumber of regions it represents. It is also notable that the authors pull \r\nin techniques from computational geometry for this construction.\r\n\r\nBut there are several problems with the paper.  The writing is unclear,\r\nand overall the paper feels like a preliminary draft, not ready for prime \r\ntime.\r\n\r\nThe introduction can be tightened up.  A more significant comment\r\nconcerns the important attempt to give some intuition, at the top of\r\npage 3.  The paragraph starting with 'Specifically' doesn't make sense\r\nto me. How can all but one hidden unit be 0 for different intervals\r\nalong the real line?  Each hidden unit will be active above (or below)\r\nits threshold value, so many positive ones will be active at the same\r\ntime.  We could compose two hidden units to construct one that is only\r\nactive within an interval in R, but I don't see how to do this in one\r\nlayer.  I must be missing something simple here. \r\n\r\nI think the basic intuition is that a higher-level unit can act as an\r\nOR of several lower level regions, and gain expressivity by repeating\r\nits operation on these regions, is the idea.  But the construction is\r\nnot clear.  Also, one would expect that the ability to represent AND\r\noperations would also lead to significant expressivity gains.  Is this\r\nalso part of the construction?  These basic issues should be clarified.\r\n\r\nIn addition, it would be very helpful to have some concrete example\r\nof a function that can be computed by a deep net of the sort analyzed\r\nhere, which cannot be computed by a shallow net of the same size.  \r\nAs it stands the characterization of the difference between the deep \r\nand equivalent one-layer network is too abstract to be very compelling.\r\n\r\nI also found the proof of Theorem 8 very hard to understand.  This\r\nis not a key problem, as the authors do a good job building up to\r\nthis main theorem, in sections 3 and 4.  But it does mean that I am\r\nnot confident that the proof is correct.\r\n\r\nFinally, I would recommend exploring the relationship between the ideas\r\nin this paper and the extensive work in circuit complexity that deals \r\nwith multi-level circuits, for example the paper by Hajnal et al on 'Threshold circuits of bounded depth'."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "On the number of inference regions of deep feed forward networks with piece-wise linear activations", "decision": "submitted, no decision", "abstract": "This paper explores the complexity of deep feed forward networks with linear presynaptic couplings and rectified linear activations. This is a contribution to the growing body of work contrasting the representational power of deep and shallow network architectures. In particular, we offer a framework for comparing deep and shallow models that belong to the family of piece-wise linear functions based on computational geometry. We look at a deep (two hidden layers) rectifier multilayer perceptron (MLP) with linear outputs units and compare it with a single layer version of the model. In the asymptotic regime as the number of units goes to infinity, if the shallow model has $2n$ hidden units and $n_0$ inputs, then the number of linear regions is $O(n^{n_0})$. A two layer model with $n$ number of hidden units on each layer has $Omega(n^{n_0})$. We consider this as a first step towards understanding the complexity of these models and argue that better constructions in this framework might provide more accurate comparisons (especially for the interesting case of when the number of hidden layers goes to infinity).", "pdf": "https://arxiv.org/abs/1312.6098", "paperhash": "pascanu|on_the_number_of_inference_regions_of_deep_feed_forward_networks_with_piecewise_linear_activations", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Guido F. Montufar", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "guidomontufar@googlemail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392071280000, "tcdate": 1392071280000, "number": 1, "id": "c_ej7ww5zf_BP", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "bSaT4mmQt84Lx", "replyto": "n6U96C27ST6iZ", "signatures": ["Razvan Pascanu"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thank you for your comments Reviewer 355b. \r\n\r\n Regarding the number of parameters, indeed it is not clear what is a fair measure of capacity for these models. \r\n\r\n We can easily show that our results hold even when we enforce the number of parameters to be the same. We've added a note about this in the discussion. Specifically, to do so, we can look at how fast the ratio between the number of regions and the number of parameters grows for deep versus shallow models. We can see that in this situation we have something of the form $Omega( {n/n_0}^{k-1} n^{n_0-2}$ for deep models and $O(k^{n_0} n^{n_0-1})$ for a shallow model. The ratio still grows exponentially faster with $k$ for deep models and polynomially with $n$ when $n_0$ is fixed. \r\n\r\n Further more, as a comment to your question, we argue that the number of linear region is a property that correlates with the representational power of the model, while the number of units (or parameters) is a measure of capacity. Our paper attempts to show that for the same capacity deep models can be more expressive. \r\n\r\n Regarding the generalization power of a deep model (versus a shallow one), this can be a tricky question to answer.  Issues with the learning process only makes the answer even more complicated.  Addressing all of these things together, while it is very important for deep learning, is much more than what our paper is trying to do. \r\n \r\n For now our analysis is limited to the representational power of a deep model, which is different from its generalization power. The question we are trying to answer is the following: 'Consider the family of all possible shallow model of a certain size (capacity) and then pick from this family of functions the one that is closes to some  function $f$.  Do the same for the family of deep models of same capacity. Which of these two picked models are doing a better job at approximating $f$ ?' \r\n \r\n Of course if we do not bound the capacity within each family the answer would that both approximate $f$ arbitrarily well and then we can not distinguished between them. This is true because we know both are universal approximators.  \r\n\r\n Generalization, for piece-wise linear deep models, comes from the fact that while we can have more linear regions, these linear regions are restricted in some sense. Deep models rely on symmetry, repeating the partition done on some higher layer in different regions of the input.  That means that the functions they can represent efficiently are highly structured, even though they look arbitrarily complicated. \r\n\r\n In other words, deep models can approximate much better then shallow models only certain families of functions (those that have a particular structure). If we try to approximate white noise, because it has no structure, deep models will not fare better than shallow ones. \r\n \r\n Analyzing formally this idea is a very interesting direction that we are\r\n considering for future work. \r\n\r\n And as a final note, we have submitted to arxiv a new version of the paper. It will be available starting with Tue, 11 Feb 2014 01:00:00 GMT.\r\n Let us quickly summarize the main changes: \r\n\r\n We have added section 4.1 which is a construction that only works for\r\n $n=2n_0$.  In this construction we pair the hidden units at each layer, except the last one, and make each pair to behave as the absolute value of some coordinate. This will make each quadrant of the input space of the layer to the first quadrant, where the input space of the layer is the image of the input layer through all the layers below. However we partition the first quadrant in the higher layer, this partition will be repeated in all the other quadrants resulting in $Omega(2^{n_0(k-1)} n^{n_0})$ linear regions. This bound shows that deep models can be more efficient than shallow ones even for a small number of layers (like 2).\r\n\r\n We have added  paragraph (paragraph 2 in the conclusion) that talks about forcing the models to have the same number of parameters rather than the same number of units. \r\n\r\n Additional changes from the last submission (Mon 27 Jan):\r\n\r\n We have added a paragraph (paragraph 3) talking about how $n_0$ is affected by the observation that, in real tasks, data might live near a manifold of much lower dimension. \r\n\r\n We have added to paragraphs in the introduction (paragraph 3 and 4 on page 2) motivating our picked measure of flexibility, namely the number of linear regions."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "On the number of inference regions of deep feed forward networks with piece-wise linear activations", "decision": "submitted, no decision", "abstract": "This paper explores the complexity of deep feed forward networks with linear presynaptic couplings and rectified linear activations. This is a contribution to the growing body of work contrasting the representational power of deep and shallow network architectures. In particular, we offer a framework for comparing deep and shallow models that belong to the family of piece-wise linear functions based on computational geometry. We look at a deep (two hidden layers) rectifier multilayer perceptron (MLP) with linear outputs units and compare it with a single layer version of the model. In the asymptotic regime as the number of units goes to infinity, if the shallow model has $2n$ hidden units and $n_0$ inputs, then the number of linear regions is $O(n^{n_0})$. A two layer model with $n$ number of hidden units on each layer has $Omega(n^{n_0})$. We consider this as a first step towards understanding the complexity of these models and argue that better constructions in this framework might provide more accurate comparisons (especially for the interesting case of when the number of hidden layers goes to infinity).", "pdf": "https://arxiv.org/abs/1312.6098", "paperhash": "pascanu|on_the_number_of_inference_regions_of_deep_feed_forward_networks_with_piecewise_linear_activations", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Guido F. Montufar", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "guidomontufar@googlemail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391861220000, "tcdate": 1391861220000, "number": 6, "id": "n6U96C27ST6iZ", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "bSaT4mmQt84Lx", "replyto": "bSaT4mmQt84Lx", "signatures": ["anonymous reviewer 355b"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of On the number of inference regions of deep feed forward networks with piece-wise linear activations", "review": "This paper studies the representation power of deep nets with piecewise linear activation functions. The main idea is to show that deep net with the same number units can represent (in terms of generating linear regions) more complex mappings than the shallow model.\r\n\r\nThis is an interesting paper: it leverages known results in arranging hyperplanes in the space and then cleverly show how those can be used to show how linear regions can be learnt by multiple layers. \r\n\r\nWhile the theoretical results seem right, I could not help but wondering whether the comparison is 'fair':  using the same number of units does not necessarily imply the deep net is 'restricted' in any way -- in fact, the deep net has more parameters than the shallow model. Is not that enough to argue (at least qualitatively) that deep net must have more representation power than the shallow model? (Of course, the value of the analysis is to show more precisely how many regions there are.)\r\n\r\nAdditionally, the learning process does not necessarily mean that the deep net indeed constructs that many regions --- thus, purely comparing the number of regions is unlikely to explain how well the model is able to generalize well than shallow model or how the model is prevented from overfitting.\r\n\r\nNonetheless, the paper presents a novel direction to pursuit to instigate further research."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "On the number of inference regions of deep feed forward networks with piece-wise linear activations", "decision": "submitted, no decision", "abstract": "This paper explores the complexity of deep feed forward networks with linear presynaptic couplings and rectified linear activations. This is a contribution to the growing body of work contrasting the representational power of deep and shallow network architectures. In particular, we offer a framework for comparing deep and shallow models that belong to the family of piece-wise linear functions based on computational geometry. We look at a deep (two hidden layers) rectifier multilayer perceptron (MLP) with linear outputs units and compare it with a single layer version of the model. In the asymptotic regime as the number of units goes to infinity, if the shallow model has $2n$ hidden units and $n_0$ inputs, then the number of linear regions is $O(n^{n_0})$. A two layer model with $n$ number of hidden units on each layer has $Omega(n^{n_0})$. We consider this as a first step towards understanding the complexity of these models and argue that better constructions in this framework might provide more accurate comparisons (especially for the interesting case of when the number of hidden layers goes to infinity).", "pdf": "https://arxiv.org/abs/1312.6098", "paperhash": "pascanu|on_the_number_of_inference_regions_of_deep_feed_forward_networks_with_piecewise_linear_activations", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Guido F. Montufar", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "guidomontufar@googlemail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390529820000, "tcdate": 1390529820000, "number": 5, "id": "GqWRGvurSDqeX", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "bSaT4mmQt84Lx", "replyto": "bSaT4mmQt84Lx", "signatures": ["Razvan Pascanu"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Your point is well taken and we are well aware of it, motivating current work that would provide a stronger bound. Within that new construction, even for reasonably models like 3 layers with only 2 n_0 units on each layer, we still get more regions in the deep model than in the shallow one. In that new theorem we count every region of the deep model. In the proof presented in the paper, we only look at a lower bound on the maximal number of regions for the deep model, a bound that is not necessarily tight. The reason is that we only count certain regions and ignore the rest. Unfortunately counting all regions for the deep model construction given in the paper is difficult. \r\n\r\nThe second comment we would like to make is that we recover a high ratio of n to n_0 if the data live near a low-dimensional manifold (effectively like reducing the input size n_0). One-layer models can reach the upper bound of regions only by spanning all the dimensions of the input. That is, for any subspace of the input we can not concentrate most of the regions in that subspace. If, as commonly assumed, data live near a lower dimensional manifold, then we care only about the number of regions we can get in the directions of that manifold. One way of thinking about it is if you do a PCA on your data. You will have a lot of directions (say on MNIST) where few variations are seen in the data (and if you see some, it is mostly due to noise that you want to ignore). In such a situation you care about how many regions you have within the directions in which the data do change. In such situations n >> n_0, making the proof of the paper more relevant.\r\n\r\nLastly, we'll argue that the paper is making an important contribution to the asymptotic case but covers neural network architectures (with rectifiers) that were not previously covered. The paper is showing an important representational advantage from using deep models versus shallow ones, at least asymptotically. This kind of result is important to motivate the importance of deep models. Although there are previous results with the same objective, they have not been with the kind of commonly used non-linearity (rectified linear units) that we are able to cover here."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "On the number of inference regions of deep feed forward networks with piece-wise linear activations", "decision": "submitted, no decision", "abstract": "This paper explores the complexity of deep feed forward networks with linear presynaptic couplings and rectified linear activations. This is a contribution to the growing body of work contrasting the representational power of deep and shallow network architectures. In particular, we offer a framework for comparing deep and shallow models that belong to the family of piece-wise linear functions based on computational geometry. We look at a deep (two hidden layers) rectifier multilayer perceptron (MLP) with linear outputs units and compare it with a single layer version of the model. In the asymptotic regime as the number of units goes to infinity, if the shallow model has $2n$ hidden units and $n_0$ inputs, then the number of linear regions is $O(n^{n_0})$. A two layer model with $n$ number of hidden units on each layer has $Omega(n^{n_0})$. We consider this as a first step towards understanding the complexity of these models and argue that better constructions in this framework might provide more accurate comparisons (especially for the interesting case of when the number of hidden layers goes to infinity).", "pdf": "https://arxiv.org/abs/1312.6098", "paperhash": "pascanu|on_the_number_of_inference_regions_of_deep_feed_forward_networks_with_piecewise_linear_activations", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Guido F. Montufar", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "guidomontufar@googlemail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390448460000, "tcdate": 1390448460000, "number": 4, "id": "qVWJL1t42vKik", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "bSaT4mmQt84Lx", "replyto": "bSaT4mmQt84Lx", "signatures": ["Alexandre Dalyac"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "sorry if I'm wrong, but it seems to me that typically, k is never large, and if n[0] is only ever large when n is also large, such that n/n[0] = O(1). in those circumstances, is there a significant difference in the number of linear regions between the 2 architectures?\ufeff"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "On the number of inference regions of deep feed forward networks with piece-wise linear activations", "decision": "submitted, no decision", "abstract": "This paper explores the complexity of deep feed forward networks with linear presynaptic couplings and rectified linear activations. This is a contribution to the growing body of work contrasting the representational power of deep and shallow network architectures. In particular, we offer a framework for comparing deep and shallow models that belong to the family of piece-wise linear functions based on computational geometry. We look at a deep (two hidden layers) rectifier multilayer perceptron (MLP) with linear outputs units and compare it with a single layer version of the model. In the asymptotic regime as the number of units goes to infinity, if the shallow model has $2n$ hidden units and $n_0$ inputs, then the number of linear regions is $O(n^{n_0})$. A two layer model with $n$ number of hidden units on each layer has $Omega(n^{n_0})$. We consider this as a first step towards understanding the complexity of these models and argue that better constructions in this framework might provide more accurate comparisons (especially for the interesting case of when the number of hidden layers goes to infinity).", "pdf": "https://arxiv.org/abs/1312.6098", "paperhash": "pascanu|on_the_number_of_inference_regions_of_deep_feed_forward_networks_with_piecewise_linear_activations", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Guido F. Montufar", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "guidomontufar@googlemail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390216500000, "tcdate": 1390216500000, "number": 3, "id": "zhkyU33YqKzMj", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "bSaT4mmQt84Lx", "replyto": "bSaT4mmQt84Lx", "signatures": ["Razvan Pascanu"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "David, thanks a lot for your comments. We will definitely consider them in the next version of the paper. I have to wonder however, which version of the paper did you look at? Version 2 (which is online since 6th Jan) does not have an equation 13, while the first draft did. My hope is that the version 2, that is online, already\r\nanswers most of your questions. Note that we have a different construction that changes much of the flow of the paper as well as \r\nthe final result. \r\n\r\nI would quickly try to summarize some specific answers to your questions:\r\n\r\n (1) Regarding the scope of the paper. We do not generalize Zaslavsky's Theorem to scenarios that arise in deep models, as you suggest.  For a single layer rectifier MLP it turns out that each hidden unit partitions the space using a hyperplane. This means that the number of regions we have is the number of regions you can get from an arrangement of $n$ hyperplanes. The answer to this question is given by Zaslavsky's Theorem. This offers as an upper bound to the maximal number of regions a single hidden layer model has. To make our point (that deep models are more efficient) we only need to show now that there exist deep models that result in more regions. We do so by constructing a specific deep model for which we can compute lower bound to the number of linear regions it generates. Asymptotically this lower bound is much larger than the maximal number of regions a single layer model has. \r\n\r\n (2) 'region'/'region of linearity'/'input space partition space' means the same thing. It is a connected region of the input space U, such that within this region every hidden unit either stays positive or is equal to 0. Or in other words, within this region the MLP is a linear function with respect to its input.\r\n\r\n (3) Regarding figure 2, we do use a new construction now that is depicted in more detail. \r\n\r\n (4) Proposition 8 and 9 have been replaced completely. Proposition 8 describes a lower bound on how many new regions you can get within a layer, given the number of regions you have generated up to that layer. Proposition 9 used proposition 8 to give a lower bound on the total number of regions of a deep model.\r\n\r\n (5) Regarding the statement about the output activation function. We simply meant to say that it is sufficient to study models with linear output activation function to get a sense for MLPs with sigmoids or softmax output activations. To see this do the following. Consider you have some rectifier MLP with sigmoids at the output layer, and some target function $f$. Let $s$ stand for the sigmoid activation function. To get a sense of how well we can approximate $f$, we can look at how well we can approximate $s^{-1}(f)$ with a rectifer MLP that has a linear output activation function."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "On the number of inference regions of deep feed forward networks with piece-wise linear activations", "decision": "submitted, no decision", "abstract": "This paper explores the complexity of deep feed forward networks with linear presynaptic couplings and rectified linear activations. This is a contribution to the growing body of work contrasting the representational power of deep and shallow network architectures. In particular, we offer a framework for comparing deep and shallow models that belong to the family of piece-wise linear functions based on computational geometry. We look at a deep (two hidden layers) rectifier multilayer perceptron (MLP) with linear outputs units and compare it with a single layer version of the model. In the asymptotic regime as the number of units goes to infinity, if the shallow model has $2n$ hidden units and $n_0$ inputs, then the number of linear regions is $O(n^{n_0})$. A two layer model with $n$ number of hidden units on each layer has $Omega(n^{n_0})$. We consider this as a first step towards understanding the complexity of these models and argue that better constructions in this framework might provide more accurate comparisons (especially for the interesting case of when the number of hidden layers goes to infinity).", "pdf": "https://arxiv.org/abs/1312.6098", "paperhash": "pascanu|on_the_number_of_inference_regions_of_deep_feed_forward_networks_with_piecewise_linear_activations", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Guido F. Montufar", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "guidomontufar@googlemail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390216500000, "tcdate": 1390216500000, "number": 2, "id": "8nQMn2_oFgvXP", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "bSaT4mmQt84Lx", "replyto": "bSaT4mmQt84Lx", "signatures": ["Razvan Pascanu"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "David, thanks a lot for your comments. We will definitely consider them in the next version of the paper. I have to wonder however, which version of the paper did you look at? Version 2 (which is online since 6th Jan) does not have an equation 13, while the first draft did. My hope is that the version 2, that is online, already\r\nanswers most of your questions. Note that we have a different construction that changes much of the flow of the paper as well as \r\nthe final result. \r\n\r\nI would quickly try to summarize some specific answers to your questions:\r\n\r\n (1) Regarding the scope of the paper. We do not generalize Zaslavsky's Theorem to scenarios that arise in deep models, as you suggest.  For a single layer rectifier MLP it turns out that each hidden unit partitions the space using a hyperplane. This means that the number of regions we have is the number of regions you can get from an arrangement of $n$ hyperplanes. The answer to this question is given by Zaslavsky's Theorem. This offers as an upper bound to the maximal number of regions a single hidden layer model has. To make our point (that deep models are more efficient) we only need to show now that there exist deep models that result in more regions. We do so by constructing a specific deep model for which we can compute lower bound to the number of linear regions it generates. Asymptotically this lower bound is much larger than the maximal number of regions a single layer model has. \r\n\r\n (2) 'region'/'region of linearity'/'input space partition space' means the same thing. It is a connected region of the input space U, such that within this region every hidden unit either stays positive or is equal to 0. Or in other words, within this region the MLP is a linear function with respect to its input.\r\n\r\n (3) Regarding figure 2, we do use a new construction now that is depicted in more detail. \r\n\r\n (4) Proposition 8 and 9 have been replaced completely. Proposition 8 describes a lower bound on how many new regions you can get within a layer, given the number of regions you have generated up to that layer. Proposition 9 used proposition 8 to give a lower bound on the total number of regions of a deep model.\r\n\r\n (5) Regarding the statement about the output activation function. We simply meant to say that it is sufficient to study models with linear output activation function to get a sense for MLPs with sigmoids or softmax output activations. To see this do the following. Consider you have some rectifier MLP with sigmoids at the output layer, and some target function $f$. Let $s$ stand for the sigmoid activation function. To get a sense of how well we can approximate $f$, we can look at how well we can approximate $s^{-1}(f)$ with a rectifer MLP that has a linear output activation function."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "On the number of inference regions of deep feed forward networks with piece-wise linear activations", "decision": "submitted, no decision", "abstract": "This paper explores the complexity of deep feed forward networks with linear presynaptic couplings and rectified linear activations. This is a contribution to the growing body of work contrasting the representational power of deep and shallow network architectures. In particular, we offer a framework for comparing deep and shallow models that belong to the family of piece-wise linear functions based on computational geometry. We look at a deep (two hidden layers) rectifier multilayer perceptron (MLP) with linear outputs units and compare it with a single layer version of the model. In the asymptotic regime as the number of units goes to infinity, if the shallow model has $2n$ hidden units and $n_0$ inputs, then the number of linear regions is $O(n^{n_0})$. A two layer model with $n$ number of hidden units on each layer has $Omega(n^{n_0})$. We consider this as a first step towards understanding the complexity of these models and argue that better constructions in this framework might provide more accurate comparisons (especially for the interesting case of when the number of hidden layers goes to infinity).", "pdf": "https://arxiv.org/abs/1312.6098", "paperhash": "pascanu|on_the_number_of_inference_regions_of_deep_feed_forward_networks_with_piecewise_linear_activations", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Guido F. Montufar", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "guidomontufar@googlemail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1389921060000, "tcdate": 1389921060000, "number": 1, "id": "dl7nPGGwxql6h", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "bSaT4mmQt84Lx", "replyto": "bSaT4mmQt84Lx", "signatures": ["David Krueger"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "I like the topic of this paper, but I found it very difficult to read.  \r\n\r\nAlthough I don't have many specific suggestions along these lines, I feel like it could probably be made much easier to understand if the proofs were introduced with more simple English explanations of how the proofs work.  It seems like the main issue is generally showing how Zaslavsky's Theorem can be applied to specific scenarios that arise using deep networks.  \r\n\r\nI think you should define what you mean by a 'region' or 'region of linearity' or 'input space partition region' and use one term consistently (I think these are all the same thing?)\r\n\r\nIn figure 2, I would add plots of what lower levels look like, so we can see how the final plot emerges.  \r\n\r\nI don't follow the proofs of Proposition 8 or 9.\r\n\r\nIn proposition 8, I think the first summation should be up to n_1 and summing n_2 choose k?\r\n\r\nI would move the definitions of big-O, Omega and Theta notation to the preliminaries.\r\n\r\nYou need to add Zaslavsky to the citations.\r\n\r\nI think equation (13) has a typo.  Should 2n_0 be 2^{n_0}?   \r\n\r\nI don't follow the part about how linearity is not a restriction in the Discussion; in fact, I'm not sure exactly what is meant by that statement.  \r\n\r\n\r\n\r\nLittle edits:\r\nproof of lemma 4: 'the regions of linear behavior [...] ARE'\r\n\r\nbelow equation (3) 'the first layer has TWO operational modes'\r\n\r\nlater that paragraph: 'gradient equal to vr_i' should just be r_i, I think.  \r\n\r\nProposition 6: your proof sketch only demonstrates the formula for r(A), not b(A); you should say that.  \r\n\r\nSection 4:\r\nyou define but never use b(n_0,n_1).  You don't specify, but I assume that r and u mentioned below are the r(n_0,n_1), u(n_0,n_1) you define.  I would use n = n_1 and d = n_0 in this definition, for clarity.  \r\n\r\nDiscussion: say 'e.g.' or 'for example', but not 'for e.g.'"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "On the number of inference regions of deep feed forward networks with piece-wise linear activations", "decision": "submitted, no decision", "abstract": "This paper explores the complexity of deep feed forward networks with linear presynaptic couplings and rectified linear activations. This is a contribution to the growing body of work contrasting the representational power of deep and shallow network architectures. In particular, we offer a framework for comparing deep and shallow models that belong to the family of piece-wise linear functions based on computational geometry. We look at a deep (two hidden layers) rectifier multilayer perceptron (MLP) with linear outputs units and compare it with a single layer version of the model. In the asymptotic regime as the number of units goes to infinity, if the shallow model has $2n$ hidden units and $n_0$ inputs, then the number of linear regions is $O(n^{n_0})$. A two layer model with $n$ number of hidden units on each layer has $Omega(n^{n_0})$. We consider this as a first step towards understanding the complexity of these models and argue that better constructions in this framework might provide more accurate comparisons (especially for the interesting case of when the number of hidden layers goes to infinity).", "pdf": "https://arxiv.org/abs/1312.6098", "paperhash": "pascanu|on_the_number_of_inference_regions_of_deep_feed_forward_networks_with_piecewise_linear_activations", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Guido F. Montufar", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "guidomontufar@googlemail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387780620000, "tcdate": 1387780620000, "number": 26, "id": "bSaT4mmQt84Lx", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "bSaT4mmQt84Lx", "signatures": ["r.pascanu@gmail.com"], "readers": ["everyone"], "content": {"title": "On the number of inference regions of deep feed forward networks with piece-wise linear activations", "decision": "submitted, no decision", "abstract": "This paper explores the complexity of deep feed forward networks with linear presynaptic couplings and rectified linear activations. This is a contribution to the growing body of work contrasting the representational power of deep and shallow network architectures. In particular, we offer a framework for comparing deep and shallow models that belong to the family of piece-wise linear functions based on computational geometry. We look at a deep (two hidden layers) rectifier multilayer perceptron (MLP) with linear outputs units and compare it with a single layer version of the model. In the asymptotic regime as the number of units goes to infinity, if the shallow model has $2n$ hidden units and $n_0$ inputs, then the number of linear regions is $O(n^{n_0})$. A two layer model with $n$ number of hidden units on each layer has $Omega(n^{n_0})$. We consider this as a first step towards understanding the complexity of these models and argue that better constructions in this framework might provide more accurate comparisons (especially for the interesting case of when the number of hidden layers goes to infinity).", "pdf": "https://arxiv.org/abs/1312.6098", "paperhash": "pascanu|on_the_number_of_inference_regions_of_deep_feed_forward_networks_with_piecewise_linear_activations", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Guido F. Montufar", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "guidomontufar@googlemail.com", "yoshua.bengio@gmail.com"]}, "writers": [], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 13}