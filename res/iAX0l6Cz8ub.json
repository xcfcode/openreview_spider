{"notes": [{"id": "iAX0l6Cz8ub", "original": "B5LtmBbRE9z", "number": 332, "cdate": 1601308044728, "ddate": null, "tcdate": 1601308044728, "tmdate": 1615370971897, "tddate": null, "forum": "iAX0l6Cz8ub", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Geometry-aware Instance-reweighted Adversarial Training", "authorids": ["~Jingfeng_Zhang1", "~Jianing_Zhu2", "~Gang_Niu1", "~Bo_Han1", "~Masashi_Sugiyama1", "~Mohan_Kankanhalli1"], "authors": ["Jingfeng Zhang", "Jianing Zhu", "Gang Niu", "Bo Han", "Masashi Sugiyama", "Mohan Kankanhalli"], "keywords": ["Adversarial robustness"], "abstract": "In adversarial machine learning, there was a common belief that robustness and accuracy hurt each other. The belief was challenged by recent studies where we can maintain the robustness and improve the accuracy. However, the other direction, whether we can keep the accuracy and improve the robustness, is conceptually and practically more interesting, since robust accuracy should be lower than standard accuracy for any model. In this paper, we show this direction is also promising. Firstly, we find even over-parameterized deep networks may still have insufficient model capacity, because adversarial training has an overwhelming smoothing effect. Secondly, given limited model capacity, we argue adversarial data should have unequal importance: geometrically speaking, a natural data point closer to/farther from the class boundary is less/more robust, and the corresponding adversarial data point should be assigned with larger/smaller weight. Finally, to implement the idea, we propose geometry-aware instance-reweighted adversarial training, where the weights are based on how difficult it is to attack a natural data point. Experiments show that our proposal boosts the robustness of standard adversarial training; combining two directions, we improve both robustness and accuracy of standard adversarial training.", "one-sentence_summary": "This paper has proposed a novel adversarial training method, i.e., geometry-aware instance-reweighted adversarial training (GAIRAT), which sheds new lights on improving the adversarial training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|geometryaware_instancereweighted_adversarial_training", "supplementary_material": "/attachment/befac1f842cb530d192c70f91e2aae6bc3f03e20.zip", "pdf": "/pdf/746bc7119712b56c0426659dcb575f5864fa29ec.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021geometryaware,\ntitle={Geometry-aware Instance-reweighted Adversarial Training},\nauthor={Jingfeng Zhang and Jianing Zhu and Gang Niu and Bo Han and Masashi Sugiyama and Mohan Kankanhalli},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iAX0l6Cz8ub}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 17, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "l__E2Obr3C-", "original": null, "number": 5, "cdate": 1614991430555, "ddate": null, "tcdate": 1614991430555, "tmdate": 1614997155181, "tddate": null, "forum": "iAX0l6Cz8ub", "replyto": "iAX0l6Cz8ub", "invitation": "ICLR.cc/2021/Conference/Paper332/-/Comment", "content": {"title": "Reply to \"A few more comments on the Logit Scaling Attack\" by Iacopo Masi", "comment": "Thanks for your attention to our work! I hereby discuss some issues on your tech report and the logit attack itself one by one.\n\nA1. First of all, the importance weighting (IW) implementation we have released on github is just one of possible implementations of GAIRAT. There are many other ways to map the number of PGD steps to instance weights, and the number of PGD steps is only a rough approximation of the geometry property we would like to model, i.e., the non-linear distance from an instance to the class boundary it belongs to. Unlike IW for distribution shifts where we know the optimal weights, we have no idea that which IW map is the best, at least theoretically.\n\nA2. Hence, the IW map itself may be a hyperparameter depending on the task --- the data, the loss, the model, the optimizer, and the set of future attacks. Technically, if we consider flatter IW maps, GAIRAT becomes closer to the standard AT; indeed, GAIRAT with possible IW maps can be regarded as a strictly general case of the standard AT since GAIRAT is just AT equipped with IW.\n\nA3. Moreover, even if the IW map is fixed rather than tuned, other hyperparameters such as weight decay, learning rate, and the choice of the \"best epoch\" should be tuned on validation data, where the validation data are attacked by some stronger test-time attack instead of the weaker training-time attack. Note that AT is much more sensitive to changes of hyperparameters than natural training without simulating the attack; see \"Bag of Tricks for Adversarial Training\" (ICLR 2021: https://openreview.net/forum?id=Xb8xvrtB8Ce).\n\nThen, let me talk about the logit attack itself.\n\nB1. The community knew the attack long time ago and knew how to cancel its effect by actively scaling the weights and biases in the layer where soft-max serves as the activation. If the logit values will be multiplied by 10, the weights and biases will be divided by 10. Then the attacker can argue that the logit values will be multiplied by 100, and so on and so forth... This is an endless game. Even though the attacker can multiply the logit values by a very large number like 1 million, as long as the defender first divide the weights and biases by a much larger number like 1 billion, the robust accuracy can be as close to the natural accuracy as possible (not close to 100%). Therefore, in recent years, almost no paper has played this endless game, since the protocol of experimenting AT algorithms is that neither attacker nor defender is allowed to actively scale the logit values.\n\nB2. Furthermore, the central philosophy of AT is to simulate the future attacks. Certainly, we would like to consider the diversity of attacks, but if we are only allowed to simulate a single attack, we will simulate the strongest attack that is still computationally affordable. By \"computationally affordable\", I meant we cannot simulate PGD-100 even when we know that the attacker will use PGD-100, but we can simulate PGD-10 with eps=16/255 when we know that the attacker will use either PGD-10 or PGD-100 with eps=16/255 rather than eps=8/255 on CIFAR-10. Similarly, multiplying the logit values by 10 is computationally affordable, and if doing so makes the attack stronger than before, we can simulate and should simulate the stronger attack during training.\n\nB3. In my humble opinion, science and engineering are quite different. Kaggle newcomers know that models should be ensembled in practice, but almost no NeurIPS, ICML, or ICLR paper did that unless the paper works on ensemble learning itself. This is science vs engineering. In science, we don't consider all factors for the ease of ablation study. We cannot conclude that it is hard to believe top ML researchers still don't know ensemble learning after it has been proposed for 40+ years. There is significant difference between knowing a thing, knowing how to properly use a thing, and knowing when/why should/shouldn't use a thing.\n\nThanks again for your attention to our work, and hope my explanations above clarified some of your concerns."}, "signatures": ["~Gang_Niu1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Gang_Niu1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Geometry-aware Instance-reweighted Adversarial Training", "authorids": ["~Jingfeng_Zhang1", "~Jianing_Zhu2", "~Gang_Niu1", "~Bo_Han1", "~Masashi_Sugiyama1", "~Mohan_Kankanhalli1"], "authors": ["Jingfeng Zhang", "Jianing Zhu", "Gang Niu", "Bo Han", "Masashi Sugiyama", "Mohan Kankanhalli"], "keywords": ["Adversarial robustness"], "abstract": "In adversarial machine learning, there was a common belief that robustness and accuracy hurt each other. The belief was challenged by recent studies where we can maintain the robustness and improve the accuracy. However, the other direction, whether we can keep the accuracy and improve the robustness, is conceptually and practically more interesting, since robust accuracy should be lower than standard accuracy for any model. In this paper, we show this direction is also promising. Firstly, we find even over-parameterized deep networks may still have insufficient model capacity, because adversarial training has an overwhelming smoothing effect. Secondly, given limited model capacity, we argue adversarial data should have unequal importance: geometrically speaking, a natural data point closer to/farther from the class boundary is less/more robust, and the corresponding adversarial data point should be assigned with larger/smaller weight. Finally, to implement the idea, we propose geometry-aware instance-reweighted adversarial training, where the weights are based on how difficult it is to attack a natural data point. Experiments show that our proposal boosts the robustness of standard adversarial training; combining two directions, we improve both robustness and accuracy of standard adversarial training.", "one-sentence_summary": "This paper has proposed a novel adversarial training method, i.e., geometry-aware instance-reweighted adversarial training (GAIRAT), which sheds new lights on improving the adversarial training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|geometryaware_instancereweighted_adversarial_training", "supplementary_material": "/attachment/befac1f842cb530d192c70f91e2aae6bc3f03e20.zip", "pdf": "/pdf/746bc7119712b56c0426659dcb575f5864fa29ec.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021geometryaware,\ntitle={Geometry-aware Instance-reweighted Adversarial Training},\nauthor={Jingfeng Zhang and Jianing Zhu and Gang Niu and Bo Han and Masashi Sugiyama and Mohan Kankanhalli},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iAX0l6Cz8ub}\n}"}, "tags": [], "invitation": {"reply": {"forum": "iAX0l6Cz8ub", "readers": {"values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2021/Conference/Paper332/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper332/Authors|ICLR.cc/2021/Conference/Paper332/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs"}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}}, "multiReply": true, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["everyone"], "tcdate": 1610649462800, "tmdate": 1610649462800, "id": "ICLR.cc/2021/Conference/Paper332/-/Comment"}}}, {"id": "GmeuuMVwNW1", "original": null, "number": 7, "cdate": 1614996967473, "ddate": null, "tcdate": 1614996967473, "tmdate": 1614996967473, "tddate": null, "forum": "iAX0l6Cz8ub", "replyto": "l__E2Obr3C-", "invitation": "ICLR.cc/2021/Conference/Paper332/-/Comment", "content": {"title": "My final note on the logit scaling trick", "comment": "I would like to deliver my final note concerning Figure 1 in the tech report entitled \"Evaluating the Robustness of Geometry-Aware Instance-Reweighted Adversarial Training\".\n\nWhile dividing the logit values by a large number can make PGD attacks weaker, multiplying by a large number does not necessarily make PGD attacks stronger. Of course, if the soft-max outputs are flatter, the signal-to-noise ratio of the following gradient-based attack algorithms goes lower, and if the soft-max outputs are sharper, the signal-to-noise ratio goes higher and it seems more friendly to the following gradient-based attack algorithms. However, both of these operations introduce an additional bias so that the observed gradients are no longer the loss-maximizing directions. Therefore, there exists a bias-variance tradeoff when we want to make PGD attacks stronger by scaling the logit values, but there is no tradeoff when we want to make PGD attacks weaker by scaling the weights and biases before the soft-max activation.\n\nThis explanation is consistent with Figure 1 in the tech report."}, "signatures": ["~Gang_Niu1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Gang_Niu1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Geometry-aware Instance-reweighted Adversarial Training", "authorids": ["~Jingfeng_Zhang1", "~Jianing_Zhu2", "~Gang_Niu1", "~Bo_Han1", "~Masashi_Sugiyama1", "~Mohan_Kankanhalli1"], "authors": ["Jingfeng Zhang", "Jianing Zhu", "Gang Niu", "Bo Han", "Masashi Sugiyama", "Mohan Kankanhalli"], "keywords": ["Adversarial robustness"], "abstract": "In adversarial machine learning, there was a common belief that robustness and accuracy hurt each other. The belief was challenged by recent studies where we can maintain the robustness and improve the accuracy. However, the other direction, whether we can keep the accuracy and improve the robustness, is conceptually and practically more interesting, since robust accuracy should be lower than standard accuracy for any model. In this paper, we show this direction is also promising. Firstly, we find even over-parameterized deep networks may still have insufficient model capacity, because adversarial training has an overwhelming smoothing effect. Secondly, given limited model capacity, we argue adversarial data should have unequal importance: geometrically speaking, a natural data point closer to/farther from the class boundary is less/more robust, and the corresponding adversarial data point should be assigned with larger/smaller weight. Finally, to implement the idea, we propose geometry-aware instance-reweighted adversarial training, where the weights are based on how difficult it is to attack a natural data point. Experiments show that our proposal boosts the robustness of standard adversarial training; combining two directions, we improve both robustness and accuracy of standard adversarial training.", "one-sentence_summary": "This paper has proposed a novel adversarial training method, i.e., geometry-aware instance-reweighted adversarial training (GAIRAT), which sheds new lights on improving the adversarial training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|geometryaware_instancereweighted_adversarial_training", "supplementary_material": "/attachment/befac1f842cb530d192c70f91e2aae6bc3f03e20.zip", "pdf": "/pdf/746bc7119712b56c0426659dcb575f5864fa29ec.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021geometryaware,\ntitle={Geometry-aware Instance-reweighted Adversarial Training},\nauthor={Jingfeng Zhang and Jianing Zhu and Gang Niu and Bo Han and Masashi Sugiyama and Mohan Kankanhalli},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iAX0l6Cz8ub}\n}"}, "tags": [], "invitation": {"reply": {"forum": "iAX0l6Cz8ub", "readers": {"values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2021/Conference/Paper332/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper332/Authors|ICLR.cc/2021/Conference/Paper332/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs"}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}}, "multiReply": true, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["everyone"], "tcdate": 1610649462800, "tmdate": 1610649462800, "id": "ICLR.cc/2021/Conference/Paper332/-/Comment"}}}, {"id": "qqIW8uauBOR", "original": null, "number": 6, "cdate": 1614992930320, "ddate": null, "tcdate": 1614992930320, "tmdate": 1614992930320, "tddate": null, "forum": "iAX0l6Cz8ub", "replyto": "l__E2Obr3C-", "invitation": "ICLR.cc/2021/Conference/Paper332/-/Comment", "content": {"title": "The formulation of AT is more general than the formulation of the logit attack", "comment": "There is another reason why the logit attack is not popularly considered. It requires that the surrogate loss function is the soft-max cross entropy. It works at most for proper composite losses [1,2], where the scores (cf. logit values) are transformed by a link function (strictly speaking, an inverse link function in probability theory; cf. soft-max) and then the estimated class-posterior probabilities are sent to a base loss (cf. cross entropy). It cannot work if the surrogate loss is directly computed based on the scores, including but not limited to the multi-class margin loss, the one-vs-rest loss, and the pairwise comparison loss [3--8]. Without the transformation by a link function, there is no effect of gradient masking.\n\nNevertheless, in the formulation of AT, the surrogate loss can be any loss, not necessarily probabilistic, let alone proper composite.\n\n[1] Mark D. Reid and Robert C. Williamson. Composite binary losses. JMLR, 11:2387--2422, 2010.\n\n[2] Robert C. Williamson, Elodie Vernet, and Mark D. Reid. Composite multiclass losses. JMLR, 17:1-52, 2016.\n\n[3] Koby Crammer and Yoram Singer. On the algorithmic implementation of kernel-based vector machines. JMLR, 2:265--292, 2001.\n\n[4] Jason Weston and Chris Watkins. Multi-class support vector machines. Technical Report CSDTR-98-04, Department of Computer Science, Royal Holloway College, University of London, 1998.\n\n[5] Yoonkyung Lee, Yi Lin, and Grace Wahba. Multicategory support vector machines: Theory and application to the classification of microarray data and satellite radiance data. Journal of the American Statistical Association, 99(465):67--81, 2004.\n\n[6] Tong Zhang. An infinity-sample theory for multi-category large margin classification. NeurIPS, 2004a.\n\n[7] Tong Zhang. Statistical analysis of some multi-category large margin classification methods. JMLR, 5:1225--1251, 2004b.\n\n[8] Tong Zhang. Statistical behavior and consistency of classification methods based on convex risk minimization. Annals of Statistics, 32(1):56--85, 2004c."}, "signatures": ["~Gang_Niu1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Gang_Niu1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Geometry-aware Instance-reweighted Adversarial Training", "authorids": ["~Jingfeng_Zhang1", "~Jianing_Zhu2", "~Gang_Niu1", "~Bo_Han1", "~Masashi_Sugiyama1", "~Mohan_Kankanhalli1"], "authors": ["Jingfeng Zhang", "Jianing Zhu", "Gang Niu", "Bo Han", "Masashi Sugiyama", "Mohan Kankanhalli"], "keywords": ["Adversarial robustness"], "abstract": "In adversarial machine learning, there was a common belief that robustness and accuracy hurt each other. The belief was challenged by recent studies where we can maintain the robustness and improve the accuracy. However, the other direction, whether we can keep the accuracy and improve the robustness, is conceptually and practically more interesting, since robust accuracy should be lower than standard accuracy for any model. In this paper, we show this direction is also promising. Firstly, we find even over-parameterized deep networks may still have insufficient model capacity, because adversarial training has an overwhelming smoothing effect. Secondly, given limited model capacity, we argue adversarial data should have unequal importance: geometrically speaking, a natural data point closer to/farther from the class boundary is less/more robust, and the corresponding adversarial data point should be assigned with larger/smaller weight. Finally, to implement the idea, we propose geometry-aware instance-reweighted adversarial training, where the weights are based on how difficult it is to attack a natural data point. Experiments show that our proposal boosts the robustness of standard adversarial training; combining two directions, we improve both robustness and accuracy of standard adversarial training.", "one-sentence_summary": "This paper has proposed a novel adversarial training method, i.e., geometry-aware instance-reweighted adversarial training (GAIRAT), which sheds new lights on improving the adversarial training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|geometryaware_instancereweighted_adversarial_training", "supplementary_material": "/attachment/befac1f842cb530d192c70f91e2aae6bc3f03e20.zip", "pdf": "/pdf/746bc7119712b56c0426659dcb575f5864fa29ec.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021geometryaware,\ntitle={Geometry-aware Instance-reweighted Adversarial Training},\nauthor={Jingfeng Zhang and Jianing Zhu and Gang Niu and Bo Han and Masashi Sugiyama and Mohan Kankanhalli},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iAX0l6Cz8ub}\n}"}, "tags": [], "invitation": {"reply": {"forum": "iAX0l6Cz8ub", "readers": {"values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2021/Conference/Paper332/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper332/Authors|ICLR.cc/2021/Conference/Paper332/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs"}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}}, "multiReply": true, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["everyone"], "tcdate": 1610649462800, "tmdate": 1610649462800, "id": "ICLR.cc/2021/Conference/Paper332/-/Comment"}}}, {"id": "t4FXWx3B_-_", "original": null, "number": 4, "cdate": 1614956157595, "ddate": null, "tcdate": 1614956157595, "tmdate": 1614956157595, "tddate": null, "forum": "iAX0l6Cz8ub", "replyto": "g2Uy1hg-k8u", "invitation": "ICLR.cc/2021/Conference/Paper332/-/Comment", "content": {"title": "A few more comments on the Logit Scaling Attack", "comment": "Dear All,\n\nwe are the authors of the report here. Thanks to Ming to bring our report up and thanks to Jingfeng for his answer. First of all, we believe GAIRAT is a valuable method and the rationale behind makes sense. I came up with something similar but then after some time, I discovered at my own expense that was subject to gradient masking technique related to scaling the logits. When I read GAIRAT, it was interesting and I had the hunch that it could be prone to this attack and we tested it out and wrote the short report. \n\nWe were not aware the logit scaling attack was shown firstly by Carlini [1], our attempt was inspired by Section 4 of the AA paper [2]. Thank you, Ming, for bringing this to our attention. \n\nIn light of this, we are going to update our report by citing [1] as well. We also are aware only now by looking at the GitHub issues that the follow-up paper [3] shows the same result; despite that, we believe is important to remind **why** the accuracy goes down (and not just show that it does) related to the gradient masking produced by defenses that scale the baseline CE loss. We hope that this can raise awareness in our community.\n\nBest Regards,\n\n[1] https://arxiv.org/pdf/1607.04311.pdf\n\n[2] https://arxiv.org/pdf/2003.01690.pdf\n\n[3] https://arxiv.org/pdf/2102.07327.pdf"}, "signatures": ["~Iacopo_Masi2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Iacopo_Masi2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Geometry-aware Instance-reweighted Adversarial Training", "authorids": ["~Jingfeng_Zhang1", "~Jianing_Zhu2", "~Gang_Niu1", "~Bo_Han1", "~Masashi_Sugiyama1", "~Mohan_Kankanhalli1"], "authors": ["Jingfeng Zhang", "Jianing Zhu", "Gang Niu", "Bo Han", "Masashi Sugiyama", "Mohan Kankanhalli"], "keywords": ["Adversarial robustness"], "abstract": "In adversarial machine learning, there was a common belief that robustness and accuracy hurt each other. The belief was challenged by recent studies where we can maintain the robustness and improve the accuracy. However, the other direction, whether we can keep the accuracy and improve the robustness, is conceptually and practically more interesting, since robust accuracy should be lower than standard accuracy for any model. In this paper, we show this direction is also promising. Firstly, we find even over-parameterized deep networks may still have insufficient model capacity, because adversarial training has an overwhelming smoothing effect. Secondly, given limited model capacity, we argue adversarial data should have unequal importance: geometrically speaking, a natural data point closer to/farther from the class boundary is less/more robust, and the corresponding adversarial data point should be assigned with larger/smaller weight. Finally, to implement the idea, we propose geometry-aware instance-reweighted adversarial training, where the weights are based on how difficult it is to attack a natural data point. Experiments show that our proposal boosts the robustness of standard adversarial training; combining two directions, we improve both robustness and accuracy of standard adversarial training.", "one-sentence_summary": "This paper has proposed a novel adversarial training method, i.e., geometry-aware instance-reweighted adversarial training (GAIRAT), which sheds new lights on improving the adversarial training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|geometryaware_instancereweighted_adversarial_training", "supplementary_material": "/attachment/befac1f842cb530d192c70f91e2aae6bc3f03e20.zip", "pdf": "/pdf/746bc7119712b56c0426659dcb575f5864fa29ec.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021geometryaware,\ntitle={Geometry-aware Instance-reweighted Adversarial Training},\nauthor={Jingfeng Zhang and Jianing Zhu and Gang Niu and Bo Han and Masashi Sugiyama and Mohan Kankanhalli},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iAX0l6Cz8ub}\n}"}, "tags": [], "invitation": {"reply": {"forum": "iAX0l6Cz8ub", "readers": {"values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2021/Conference/Paper332/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper332/Authors|ICLR.cc/2021/Conference/Paper332/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs"}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}}, "multiReply": true, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["everyone"], "tcdate": 1610649462800, "tmdate": 1610649462800, "id": "ICLR.cc/2021/Conference/Paper332/-/Comment"}}}, {"id": "g2Uy1hg-k8u", "original": null, "number": 2, "cdate": 1614750139523, "ddate": null, "tcdate": 1614750139523, "tmdate": 1614776293753, "tddate": null, "forum": "iAX0l6Cz8ub", "replyto": "t68kcmLpMs", "invitation": "ICLR.cc/2021/Conference/Paper332/-/Comment", "content": {"title": "Many thanks for bring us this very interesting paper. ", "comment": "Dear Ming Xiao,\n\nMany thanks for bring us this interesting paper. \n\nFor the similar matter, you can also refer to this GitHub link <https://github.com/zjfheart/Geometry-aware-Instance-reweighted-Adversarial-Training/issues/1>. \n\nGAIRAT training under the PGD attacks can defend PGD attacks very well, but indeed, it cannot perform equally well on all existing attacks. \nFrom the philosophical perspective, we cannot expect defense under one specific attack can defend all existing attacks. This philosophy echoes the previous finding that \"in adversarial training, it is essential to include adversarial examples produced by all known attacks, as the defensive training is non-adaptive. [1]\"\n\nOne potential solution is to incorporate all existing attacks into the training procedure.\nGAIRAT actually is not quite a specific method, it is an idea, i.e., data are inherently different, and adversarial training should treat data differently.\nIncorporating all attacks in GAIRAT yet preserving the efficiency is an interesting future direction. \n\nBesides, you can refer to Appendix C.10 to check the AA attack results. \n\nAgain, many thanks for bring us attention the paper. \n\n[1] SoK: Towards the Science of Security and Privacy in Machine Learning.\n\nBest wishes,\\\nJingfeng"}, "signatures": ["~Jingfeng_Zhang1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Jingfeng_Zhang1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Geometry-aware Instance-reweighted Adversarial Training", "authorids": ["~Jingfeng_Zhang1", "~Jianing_Zhu2", "~Gang_Niu1", "~Bo_Han1", "~Masashi_Sugiyama1", "~Mohan_Kankanhalli1"], "authors": ["Jingfeng Zhang", "Jianing Zhu", "Gang Niu", "Bo Han", "Masashi Sugiyama", "Mohan Kankanhalli"], "keywords": ["Adversarial robustness"], "abstract": "In adversarial machine learning, there was a common belief that robustness and accuracy hurt each other. The belief was challenged by recent studies where we can maintain the robustness and improve the accuracy. However, the other direction, whether we can keep the accuracy and improve the robustness, is conceptually and practically more interesting, since robust accuracy should be lower than standard accuracy for any model. In this paper, we show this direction is also promising. Firstly, we find even over-parameterized deep networks may still have insufficient model capacity, because adversarial training has an overwhelming smoothing effect. Secondly, given limited model capacity, we argue adversarial data should have unequal importance: geometrically speaking, a natural data point closer to/farther from the class boundary is less/more robust, and the corresponding adversarial data point should be assigned with larger/smaller weight. Finally, to implement the idea, we propose geometry-aware instance-reweighted adversarial training, where the weights are based on how difficult it is to attack a natural data point. Experiments show that our proposal boosts the robustness of standard adversarial training; combining two directions, we improve both robustness and accuracy of standard adversarial training.", "one-sentence_summary": "This paper has proposed a novel adversarial training method, i.e., geometry-aware instance-reweighted adversarial training (GAIRAT), which sheds new lights on improving the adversarial training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|geometryaware_instancereweighted_adversarial_training", "supplementary_material": "/attachment/befac1f842cb530d192c70f91e2aae6bc3f03e20.zip", "pdf": "/pdf/746bc7119712b56c0426659dcb575f5864fa29ec.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021geometryaware,\ntitle={Geometry-aware Instance-reweighted Adversarial Training},\nauthor={Jingfeng Zhang and Jianing Zhu and Gang Niu and Bo Han and Masashi Sugiyama and Mohan Kankanhalli},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iAX0l6Cz8ub}\n}"}, "tags": [], "invitation": {"reply": {"forum": "iAX0l6Cz8ub", "readers": {"values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2021/Conference/Paper332/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper332/Authors|ICLR.cc/2021/Conference/Paper332/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs"}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}}, "multiReply": true, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["everyone"], "tcdate": 1610649462800, "tmdate": 1610649462800, "id": "ICLR.cc/2021/Conference/Paper332/-/Comment"}}}, {"id": "P3dEJ84fLcO", "original": null, "number": 3, "cdate": 1614752357152, "ddate": null, "tcdate": 1614752357152, "tmdate": 1614754296112, "tddate": null, "forum": "iAX0l6Cz8ub", "replyto": "g2Uy1hg-k8u", "invitation": "ICLR.cc/2021/Conference/Paper332/-/Comment", "content": {"title": "Logic Scaling Attack is simple, basic and as important as PGD", "comment": "Thanks for your response.\n\nI would say logic scaling attack is as simple, basic, and important as PGD. Anyone can make any NN near 100% robust under vanilla PGD by only changing the temperature (e.g. 0.001) of cross-entropy loss. And it is a long-lasting pitfall when evaluating the robustness. It's hard to believe that the community still knows little about the logic scaling attack in 2021 (five years after it was originally proposed). \n\nSee the following paper\nhttps://arxiv.org/pdf/1607.04311.pdf\n\n@Nicholas Carlini"}, "signatures": ["~Ming_Xiao1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Ming_Xiao1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Geometry-aware Instance-reweighted Adversarial Training", "authorids": ["~Jingfeng_Zhang1", "~Jianing_Zhu2", "~Gang_Niu1", "~Bo_Han1", "~Masashi_Sugiyama1", "~Mohan_Kankanhalli1"], "authors": ["Jingfeng Zhang", "Jianing Zhu", "Gang Niu", "Bo Han", "Masashi Sugiyama", "Mohan Kankanhalli"], "keywords": ["Adversarial robustness"], "abstract": "In adversarial machine learning, there was a common belief that robustness and accuracy hurt each other. The belief was challenged by recent studies where we can maintain the robustness and improve the accuracy. However, the other direction, whether we can keep the accuracy and improve the robustness, is conceptually and practically more interesting, since robust accuracy should be lower than standard accuracy for any model. In this paper, we show this direction is also promising. Firstly, we find even over-parameterized deep networks may still have insufficient model capacity, because adversarial training has an overwhelming smoothing effect. Secondly, given limited model capacity, we argue adversarial data should have unequal importance: geometrically speaking, a natural data point closer to/farther from the class boundary is less/more robust, and the corresponding adversarial data point should be assigned with larger/smaller weight. Finally, to implement the idea, we propose geometry-aware instance-reweighted adversarial training, where the weights are based on how difficult it is to attack a natural data point. Experiments show that our proposal boosts the robustness of standard adversarial training; combining two directions, we improve both robustness and accuracy of standard adversarial training.", "one-sentence_summary": "This paper has proposed a novel adversarial training method, i.e., geometry-aware instance-reweighted adversarial training (GAIRAT), which sheds new lights on improving the adversarial training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|geometryaware_instancereweighted_adversarial_training", "supplementary_material": "/attachment/befac1f842cb530d192c70f91e2aae6bc3f03e20.zip", "pdf": "/pdf/746bc7119712b56c0426659dcb575f5864fa29ec.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021geometryaware,\ntitle={Geometry-aware Instance-reweighted Adversarial Training},\nauthor={Jingfeng Zhang and Jianing Zhu and Gang Niu and Bo Han and Masashi Sugiyama and Mohan Kankanhalli},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iAX0l6Cz8ub}\n}"}, "tags": [], "invitation": {"reply": {"forum": "iAX0l6Cz8ub", "readers": {"values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2021/Conference/Paper332/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper332/Authors|ICLR.cc/2021/Conference/Paper332/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs"}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}}, "multiReply": true, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["everyone"], "tcdate": 1610649462800, "tmdate": 1610649462800, "id": "ICLR.cc/2021/Conference/Paper332/-/Comment"}}}, {"id": "t68kcmLpMs", "original": null, "number": 1, "cdate": 1614748376728, "ddate": null, "tcdate": 1614748376728, "tmdate": 1614748376728, "tddate": null, "forum": "iAX0l6Cz8ub", "replyto": "iAX0l6Cz8ub", "invitation": "ICLR.cc/2021/Conference/Paper332/-/Comment", "content": {"title": "An interesting paper on today's arXiv", "comment": "This paper easily breaks GAIRAT by a simple logic scaling attack\n\nhttps://arxiv.org/abs/2103.01914"}, "signatures": ["~Ming_Xiao1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Ming_Xiao1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Geometry-aware Instance-reweighted Adversarial Training", "authorids": ["~Jingfeng_Zhang1", "~Jianing_Zhu2", "~Gang_Niu1", "~Bo_Han1", "~Masashi_Sugiyama1", "~Mohan_Kankanhalli1"], "authors": ["Jingfeng Zhang", "Jianing Zhu", "Gang Niu", "Bo Han", "Masashi Sugiyama", "Mohan Kankanhalli"], "keywords": ["Adversarial robustness"], "abstract": "In adversarial machine learning, there was a common belief that robustness and accuracy hurt each other. The belief was challenged by recent studies where we can maintain the robustness and improve the accuracy. However, the other direction, whether we can keep the accuracy and improve the robustness, is conceptually and practically more interesting, since robust accuracy should be lower than standard accuracy for any model. In this paper, we show this direction is also promising. Firstly, we find even over-parameterized deep networks may still have insufficient model capacity, because adversarial training has an overwhelming smoothing effect. Secondly, given limited model capacity, we argue adversarial data should have unequal importance: geometrically speaking, a natural data point closer to/farther from the class boundary is less/more robust, and the corresponding adversarial data point should be assigned with larger/smaller weight. Finally, to implement the idea, we propose geometry-aware instance-reweighted adversarial training, where the weights are based on how difficult it is to attack a natural data point. Experiments show that our proposal boosts the robustness of standard adversarial training; combining two directions, we improve both robustness and accuracy of standard adversarial training.", "one-sentence_summary": "This paper has proposed a novel adversarial training method, i.e., geometry-aware instance-reweighted adversarial training (GAIRAT), which sheds new lights on improving the adversarial training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|geometryaware_instancereweighted_adversarial_training", "supplementary_material": "/attachment/befac1f842cb530d192c70f91e2aae6bc3f03e20.zip", "pdf": "/pdf/746bc7119712b56c0426659dcb575f5864fa29ec.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021geometryaware,\ntitle={Geometry-aware Instance-reweighted Adversarial Training},\nauthor={Jingfeng Zhang and Jianing Zhu and Gang Niu and Bo Han and Masashi Sugiyama and Mohan Kankanhalli},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iAX0l6Cz8ub}\n}"}, "tags": [], "invitation": {"reply": {"forum": "iAX0l6Cz8ub", "readers": {"values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2021/Conference/Paper332/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper332/Authors|ICLR.cc/2021/Conference/Paper332/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs"}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}}, "multiReply": true, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["everyone"], "tcdate": 1610649462800, "tmdate": 1610649462800, "id": "ICLR.cc/2021/Conference/Paper332/-/Comment"}}}, {"id": "RBkRBjAlPNd", "original": null, "number": 1, "cdate": 1610040477987, "ddate": null, "tcdate": 1610040477987, "tmdate": 1610474082674, "tddate": null, "forum": "iAX0l6Cz8ub", "replyto": "iAX0l6Cz8ub", "invitation": "ICLR.cc/2021/Conference/Paper332/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Oral)", "comment": "The paper proposes an insightful study on the robustness and accuracy of the model. It was hard to simultaneously keep the robustness and accuracy. A few works tried to improve accuracy while maintaining the robustness by investigating more data, early stopping or dropout. From a different perspective, this paper aims to improve robustness while maintaining accuracy. \n\nThere are some interesting findings in this paper, which could deepen our understanding of adversarial training. For example, the authors conducted experiments with different sizes of the network in standard training and adversarial training. The capacity of an overparameterized network can be sufficient for standard training, but it may be far from enough to fit adversarial data, because of the smoothing effect. Hence given the limited model capacity, adversarial data all have unequal importance. Though this technique is simple and widely studied in traditional ML, it is an interesting attempt in adversarial ML and the authors provide extensive experimental results to justify its effectiveness. \n\nIn the authors' responses, the concerns raised by the reviewers have been well addressed. The new version becomes more complete by including more results on different PGD steps and the insights on designing weight assignment function. Also, the authors gave an interesting discussion on enough model size for the adversarial training, though it is still kind of an open question. I would thus like to recommend the acceptance of this paper.  "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Geometry-aware Instance-reweighted Adversarial Training", "authorids": ["~Jingfeng_Zhang1", "~Jianing_Zhu2", "~Gang_Niu1", "~Bo_Han1", "~Masashi_Sugiyama1", "~Mohan_Kankanhalli1"], "authors": ["Jingfeng Zhang", "Jianing Zhu", "Gang Niu", "Bo Han", "Masashi Sugiyama", "Mohan Kankanhalli"], "keywords": ["Adversarial robustness"], "abstract": "In adversarial machine learning, there was a common belief that robustness and accuracy hurt each other. The belief was challenged by recent studies where we can maintain the robustness and improve the accuracy. However, the other direction, whether we can keep the accuracy and improve the robustness, is conceptually and practically more interesting, since robust accuracy should be lower than standard accuracy for any model. In this paper, we show this direction is also promising. Firstly, we find even over-parameterized deep networks may still have insufficient model capacity, because adversarial training has an overwhelming smoothing effect. Secondly, given limited model capacity, we argue adversarial data should have unequal importance: geometrically speaking, a natural data point closer to/farther from the class boundary is less/more robust, and the corresponding adversarial data point should be assigned with larger/smaller weight. Finally, to implement the idea, we propose geometry-aware instance-reweighted adversarial training, where the weights are based on how difficult it is to attack a natural data point. Experiments show that our proposal boosts the robustness of standard adversarial training; combining two directions, we improve both robustness and accuracy of standard adversarial training.", "one-sentence_summary": "This paper has proposed a novel adversarial training method, i.e., geometry-aware instance-reweighted adversarial training (GAIRAT), which sheds new lights on improving the adversarial training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|geometryaware_instancereweighted_adversarial_training", "supplementary_material": "/attachment/befac1f842cb530d192c70f91e2aae6bc3f03e20.zip", "pdf": "/pdf/746bc7119712b56c0426659dcb575f5864fa29ec.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021geometryaware,\ntitle={Geometry-aware Instance-reweighted Adversarial Training},\nauthor={Jingfeng Zhang and Jianing Zhu and Gang Niu and Bo Han and Masashi Sugiyama and Mohan Kankanhalli},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iAX0l6Cz8ub}\n}"}, "tags": [], "invitation": {"reply": {"forum": "iAX0l6Cz8ub", "replyto": "iAX0l6Cz8ub", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040477974, "tmdate": 1610474082659, "id": "ICLR.cc/2021/Conference/Paper332/-/Decision"}}}, {"id": "d4JvKmvAuO6", "original": null, "number": 3, "cdate": 1603822314635, "ddate": null, "tcdate": 1603822314635, "tmdate": 1606409532351, "tddate": null, "forum": "iAX0l6Cz8ub", "replyto": "iAX0l6Cz8ub", "invitation": "ICLR.cc/2021/Conference/Paper332/-/Official_Review", "content": {"title": "Interesting paper", "review": "Summary:\nThe paper focused on the sample importance in the adversarial training. The authors firstly revealed that over-parameterized deep models on natural data may have insufficient model capacity for adversarial data, because the training loss is hard to zero for adversarial training. Then, the authors argued that limited capacity should be used for these important samples, that is, we should not treat samples equally important. They used the distance to the decision boundary to distinguish important samples and proposed geometry-aware instance-reweighted adversarial training. Experiments show the superiority over baselines. \n\nPros:\n- The finding on insufficient model capacity is very interesting. The following motivation for GAIRAT is intuitive and well explained. \n- The authors proposed a realized measurement to compute the distance to the decision boundary. This is inspiring for a series of decision-based work. \n- The experiments demonstrate the effectiveness of the proposed method. \n\nCons:\n- Treating data differently has been investigated in related work like MART and MMA. The authors should discuss the difference from these methods. \n- The capacity analysis provides a very good perspective to analyze adversarial training, however, the explanations in Figure 2 are a little bit weak. \n- The weight function of Eq. (6) lack some intuitive explanations. Why such a formula? Why choose these constants?\n- PGD steps are also investigated in CAT and DAT papers. The authors should also discuss the difference to them. \n- The experiments should compare with some baselines considering the example difference, such as MART, MMA. \n- The evaluations should test some modern white-box attacks, like auto-attack, only PGD is not convincing. Besides, Black-box attacks should be tested for a complete evaluation and checking the obfuscated gradients. \n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper332/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper332/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Geometry-aware Instance-reweighted Adversarial Training", "authorids": ["~Jingfeng_Zhang1", "~Jianing_Zhu2", "~Gang_Niu1", "~Bo_Han1", "~Masashi_Sugiyama1", "~Mohan_Kankanhalli1"], "authors": ["Jingfeng Zhang", "Jianing Zhu", "Gang Niu", "Bo Han", "Masashi Sugiyama", "Mohan Kankanhalli"], "keywords": ["Adversarial robustness"], "abstract": "In adversarial machine learning, there was a common belief that robustness and accuracy hurt each other. The belief was challenged by recent studies where we can maintain the robustness and improve the accuracy. However, the other direction, whether we can keep the accuracy and improve the robustness, is conceptually and practically more interesting, since robust accuracy should be lower than standard accuracy for any model. In this paper, we show this direction is also promising. Firstly, we find even over-parameterized deep networks may still have insufficient model capacity, because adversarial training has an overwhelming smoothing effect. Secondly, given limited model capacity, we argue adversarial data should have unequal importance: geometrically speaking, a natural data point closer to/farther from the class boundary is less/more robust, and the corresponding adversarial data point should be assigned with larger/smaller weight. Finally, to implement the idea, we propose geometry-aware instance-reweighted adversarial training, where the weights are based on how difficult it is to attack a natural data point. Experiments show that our proposal boosts the robustness of standard adversarial training; combining two directions, we improve both robustness and accuracy of standard adversarial training.", "one-sentence_summary": "This paper has proposed a novel adversarial training method, i.e., geometry-aware instance-reweighted adversarial training (GAIRAT), which sheds new lights on improving the adversarial training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|geometryaware_instancereweighted_adversarial_training", "supplementary_material": "/attachment/befac1f842cb530d192c70f91e2aae6bc3f03e20.zip", "pdf": "/pdf/746bc7119712b56c0426659dcb575f5864fa29ec.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021geometryaware,\ntitle={Geometry-aware Instance-reweighted Adversarial Training},\nauthor={Jingfeng Zhang and Jianing Zhu and Gang Niu and Bo Han and Masashi Sugiyama and Mohan Kankanhalli},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iAX0l6Cz8ub}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "iAX0l6Cz8ub", "replyto": "iAX0l6Cz8ub", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper332/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538145439, "tmdate": 1606915794186, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper332/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper332/-/Official_Review"}}}, {"id": "ezpQKKCDTnu", "original": null, "number": 7, "cdate": 1606100064565, "ddate": null, "tcdate": 1606100064565, "tmdate": 1606100064565, "tddate": null, "forum": "iAX0l6Cz8ub", "replyto": "4dOEDE3sFyG", "invitation": "ICLR.cc/2021/Conference/Paper332/-/Official_Comment", "content": {"title": "Thanks for the rebuttal", "comment": "I read through the responses and other reviewers' comments. The authors have addressed my questions and I support acceptance."}, "signatures": ["ICLR.cc/2021/Conference/Paper332/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper332/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Geometry-aware Instance-reweighted Adversarial Training", "authorids": ["~Jingfeng_Zhang1", "~Jianing_Zhu2", "~Gang_Niu1", "~Bo_Han1", "~Masashi_Sugiyama1", "~Mohan_Kankanhalli1"], "authors": ["Jingfeng Zhang", "Jianing Zhu", "Gang Niu", "Bo Han", "Masashi Sugiyama", "Mohan Kankanhalli"], "keywords": ["Adversarial robustness"], "abstract": "In adversarial machine learning, there was a common belief that robustness and accuracy hurt each other. The belief was challenged by recent studies where we can maintain the robustness and improve the accuracy. However, the other direction, whether we can keep the accuracy and improve the robustness, is conceptually and practically more interesting, since robust accuracy should be lower than standard accuracy for any model. In this paper, we show this direction is also promising. Firstly, we find even over-parameterized deep networks may still have insufficient model capacity, because adversarial training has an overwhelming smoothing effect. Secondly, given limited model capacity, we argue adversarial data should have unequal importance: geometrically speaking, a natural data point closer to/farther from the class boundary is less/more robust, and the corresponding adversarial data point should be assigned with larger/smaller weight. Finally, to implement the idea, we propose geometry-aware instance-reweighted adversarial training, where the weights are based on how difficult it is to attack a natural data point. Experiments show that our proposal boosts the robustness of standard adversarial training; combining two directions, we improve both robustness and accuracy of standard adversarial training.", "one-sentence_summary": "This paper has proposed a novel adversarial training method, i.e., geometry-aware instance-reweighted adversarial training (GAIRAT), which sheds new lights on improving the adversarial training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|geometryaware_instancereweighted_adversarial_training", "supplementary_material": "/attachment/befac1f842cb530d192c70f91e2aae6bc3f03e20.zip", "pdf": "/pdf/746bc7119712b56c0426659dcb575f5864fa29ec.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021geometryaware,\ntitle={Geometry-aware Instance-reweighted Adversarial Training},\nauthor={Jingfeng Zhang and Jianing Zhu and Gang Niu and Bo Han and Masashi Sugiyama and Mohan Kankanhalli},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iAX0l6Cz8ub}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "iAX0l6Cz8ub", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper332/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper332/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper332/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper332/Authors|ICLR.cc/2021/Conference/Paper332/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper332/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872086, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper332/-/Official_Comment"}}}, {"id": "Pu4Ywikmhmi", "original": null, "number": 2, "cdate": 1605522316168, "ddate": null, "tcdate": 1605522316168, "tmdate": 1605772527463, "tddate": null, "forum": "iAX0l6Cz8ub", "replyto": "nb6k5Ow6Ug", "invitation": "ICLR.cc/2021/Conference/Paper332/-/Official_Comment", "content": {"title": "PGD+ is strong enough and converged. ", "comment": "Many thanks for your question! \n\nFrom your question, I guess you are looking at AT and GAIRAT\u2019s comparisons at the last-checkpoint in Table 1. \nNevertheless, We have conduct PGD-200 for evaluations on models in Table 1. The results---median (std)---are\n\nDefense (best checkpoint)| PGD-200 Acc. | PGD+ Acc. | \\\n|AT(Madry) |51.76 (0.23) | 51.28 (0.23) |\\\n| FAT |46.63 (0.18) | 46.14 (0.19) |\\\n| GAIRAT |57.81 (0.49) | 55.61 (0.61)|\\\n| GAIR-FAT |56.27 (0.53) | 53.50 (0.60) |\n\n Defense(last checkpoint)| PGD-200 Acc. | PGD+ Acc. | \\\n|AT(Madry) | 46.46 (0.05) | 46.08 (0.07) |\\\n| FAT | 46.36 (0.24) | 45.80 (0.16) |\\\n| GAIRAT |53.61 (0.49) | 50.32 (0.48) |\\\n| GAIR-FAT | 50.36 (0.55) | 47.51(0.51) |\n\nPGD+ is PGD with five random starts, and each start has 40 steps with step size 0.01 (PGD+ has 40 \u00d7 5 = 200 iterations for each test data).  PGD-200 is one random start with 200 steps with step size 0.001.\n*$\\epsilon_{test}$ = 0.031, which is very small, where adversarial data cannot exceed the small norm ball; compared with 0.01, step size of 0.001 is more fine-tuned for PGD-200. Therefore, PGD-200 have converged. \n\nFrom the above table, PGD + is stronger than PGD-200 because PGD is easily trapped in local minima. \n\nBesides, we updated Appendix C.8 adding a plot that shows accuracy as a function of PGD steps. We have found that the attacks are converged on GAIRAT models. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper332/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper332/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Geometry-aware Instance-reweighted Adversarial Training", "authorids": ["~Jingfeng_Zhang1", "~Jianing_Zhu2", "~Gang_Niu1", "~Bo_Han1", "~Masashi_Sugiyama1", "~Mohan_Kankanhalli1"], "authors": ["Jingfeng Zhang", "Jianing Zhu", "Gang Niu", "Bo Han", "Masashi Sugiyama", "Mohan Kankanhalli"], "keywords": ["Adversarial robustness"], "abstract": "In adversarial machine learning, there was a common belief that robustness and accuracy hurt each other. The belief was challenged by recent studies where we can maintain the robustness and improve the accuracy. However, the other direction, whether we can keep the accuracy and improve the robustness, is conceptually and practically more interesting, since robust accuracy should be lower than standard accuracy for any model. In this paper, we show this direction is also promising. Firstly, we find even over-parameterized deep networks may still have insufficient model capacity, because adversarial training has an overwhelming smoothing effect. Secondly, given limited model capacity, we argue adversarial data should have unequal importance: geometrically speaking, a natural data point closer to/farther from the class boundary is less/more robust, and the corresponding adversarial data point should be assigned with larger/smaller weight. Finally, to implement the idea, we propose geometry-aware instance-reweighted adversarial training, where the weights are based on how difficult it is to attack a natural data point. Experiments show that our proposal boosts the robustness of standard adversarial training; combining two directions, we improve both robustness and accuracy of standard adversarial training.", "one-sentence_summary": "This paper has proposed a novel adversarial training method, i.e., geometry-aware instance-reweighted adversarial training (GAIRAT), which sheds new lights on improving the adversarial training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|geometryaware_instancereweighted_adversarial_training", "supplementary_material": "/attachment/befac1f842cb530d192c70f91e2aae6bc3f03e20.zip", "pdf": "/pdf/746bc7119712b56c0426659dcb575f5864fa29ec.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021geometryaware,\ntitle={Geometry-aware Instance-reweighted Adversarial Training},\nauthor={Jingfeng Zhang and Jianing Zhu and Gang Niu and Bo Han and Masashi Sugiyama and Mohan Kankanhalli},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iAX0l6Cz8ub}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "iAX0l6Cz8ub", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper332/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper332/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper332/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper332/Authors|ICLR.cc/2021/Conference/Paper332/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper332/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872086, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper332/-/Official_Comment"}}}, {"id": "I1wOPq5cb9", "original": null, "number": 5, "cdate": 1605771741845, "ddate": null, "tcdate": 1605771741845, "tmdate": 1605772159268, "tddate": null, "forum": "iAX0l6Cz8ub", "replyto": "Pji8Q4aybzs", "invitation": "ICLR.cc/2021/Conference/Paper332/-/Official_Comment", "content": {"title": "Replies to Reviewer 3", "comment": "Many thanks for the great comments. Please find our replies below.\n\n1 Explain some principles on designing weight assignment functions. \n\nThe optimal weight assignment is still an open question. Therefore, we have conduct experiments in Section C.3 and C.4, empirically evaluating different weight assignment functions $\\omega$ and different starting epochs to apply $\\omega$. \\\nIn terms of some principles on designing the weight assignment function $\\omega$,\\\n(a) In GAIRAT, the weight assignment is non-increasing w.r.t. the $\\kappa$ value of the natural data. It reflects different degrees of focus on different data. \\\nGAIRAT puts more focus on attackable data whose adversarial variants are easily misclassified and less focus on guarded data whose adversarial variants are hardly misclassified.\\\n(b) The design of $\\omega$ should be dataset-aware. For example, a suitable $\\omega$ applies to the CIFAR-10 dataset may not perfectly fit the SVHN dataset. \\\nCompared with the CIFAR-10 dataset (in Figure 4), the portion of guarded SVHN data (in Figure 10) becomes very large ($\\kappa = 10$) at very initial training epochs. A better $\\omega$ that is aware of this phenomenon may further increase the performance. \\\n(c) The design of $\\omega$ should be aware of the training stage. At different epochs, learning may need different $\\omega$ for the reweighting instance-dependent adversarial losses. \\\nWe leave these explorations for future work.\n\n2 More explanations on robust overfitting.\n\nIn Section C.1, we have extensive discussions and more experiments on **GAIRAT relieve robust overfitting**.\\\n-why the robust overfitting exists in standard adversarial training? \\\nAs the training progresses, the adversarially robust model engenders the increasing number of guarded training data (larger $\\kappa$ value in bottom left panel in Figure 1) and the decreasing number of attackable training data (smaller $\\kappa$ value). \nEqually focusing on training on the adversarial data may cause a vast number of adversarial variants of guarded data to *overwhelm* the model during the training, leading to undesirable robust overfitting.\\\n-how/why your GAIRAT methods relieve it?\\\nGAIRAT explicitly assigns less weight on the large portion of guarded data and assigns more weight on the small portion of attackable data, therefore ameliorating this *overwhelm* issue. \\\nAs a result, our GAIRAT can facilitate a flatter loss landscape. This fact is manifested in the bottom-middle and -right panels in Figure 4 and Figure 10; the illustrations are in Figure 9.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper332/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper332/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Geometry-aware Instance-reweighted Adversarial Training", "authorids": ["~Jingfeng_Zhang1", "~Jianing_Zhu2", "~Gang_Niu1", "~Bo_Han1", "~Masashi_Sugiyama1", "~Mohan_Kankanhalli1"], "authors": ["Jingfeng Zhang", "Jianing Zhu", "Gang Niu", "Bo Han", "Masashi Sugiyama", "Mohan Kankanhalli"], "keywords": ["Adversarial robustness"], "abstract": "In adversarial machine learning, there was a common belief that robustness and accuracy hurt each other. The belief was challenged by recent studies where we can maintain the robustness and improve the accuracy. However, the other direction, whether we can keep the accuracy and improve the robustness, is conceptually and practically more interesting, since robust accuracy should be lower than standard accuracy for any model. In this paper, we show this direction is also promising. Firstly, we find even over-parameterized deep networks may still have insufficient model capacity, because adversarial training has an overwhelming smoothing effect. Secondly, given limited model capacity, we argue adversarial data should have unequal importance: geometrically speaking, a natural data point closer to/farther from the class boundary is less/more robust, and the corresponding adversarial data point should be assigned with larger/smaller weight. Finally, to implement the idea, we propose geometry-aware instance-reweighted adversarial training, where the weights are based on how difficult it is to attack a natural data point. Experiments show that our proposal boosts the robustness of standard adversarial training; combining two directions, we improve both robustness and accuracy of standard adversarial training.", "one-sentence_summary": "This paper has proposed a novel adversarial training method, i.e., geometry-aware instance-reweighted adversarial training (GAIRAT), which sheds new lights on improving the adversarial training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|geometryaware_instancereweighted_adversarial_training", "supplementary_material": "/attachment/befac1f842cb530d192c70f91e2aae6bc3f03e20.zip", "pdf": "/pdf/746bc7119712b56c0426659dcb575f5864fa29ec.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021geometryaware,\ntitle={Geometry-aware Instance-reweighted Adversarial Training},\nauthor={Jingfeng Zhang and Jianing Zhu and Gang Niu and Bo Han and Masashi Sugiyama and Mohan Kankanhalli},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iAX0l6Cz8ub}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "iAX0l6Cz8ub", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper332/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper332/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper332/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper332/Authors|ICLR.cc/2021/Conference/Paper332/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper332/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872086, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper332/-/Official_Comment"}}}, {"id": "4dOEDE3sFyG", "original": null, "number": 4, "cdate": 1605770355170, "ddate": null, "tcdate": 1605770355170, "tmdate": 1605771418405, "tddate": null, "forum": "iAX0l6Cz8ub", "replyto": "eFk4zWtOKes", "invitation": "ICLR.cc/2021/Conference/Paper332/-/Official_Comment", "content": {"title": "Replies to Reviewer 4", "comment": "Many thanks for the great comments. Please find our replies below. \n\n1 SVHN experiments have a period of increasing robustness training error in Figure 10. \n\nIn the SVHN dataset, we believe this is due to the shortage of adversarial training data at the later training stage. \\\nAs the training progresses, most natural data quickly reach the $\\kappa$ (the number of PGD steps needed to fool the current model) value up to 10 (red lines in the bottom left panel). \\\nOur weight assignment function (in the top left panel) assigns zero weights to the losses of adversarial data whose natural data have $\\kappa = 10$; thus, very few adversarial data are utilized to update the model at the later training stage. \\\nWhen trained with very few data points, the robust training error gets increased.\n\n2 How large the DNN should be enough for adversarial training? \n\nThis is still an open question, which is very interesting. Although there exist no exact answers, I can still provide some insights. \\\n(a) Slightly larger defense parameter $\\epsilon_{train}$ usually requires significantly larger models. \nAdversarial training forces DNN to memorize the natural data's local neighborhood; this local neigborhood is exponentially large w.r.t. input dimensions, i.e. $||1+\\epsilon_{train}||^{input \\ dim}$. \\\nTherefore, even a slightly larger $\\epsilon_{train}$ can significantly enlarge the local neighborhood. Smoothing the large neighborhood requires larger models. \\\n(b) From this neighborhood smoothing perspective, I guess the current network structure, e.g., (Wide) ResNets, may not cater to the input smoothing. \\\nFor example, when networks become very deep or wide, the amount of tunable parameters is tremendous, which not only makes the decision boundary very complicated but also hurdles the optimization.\\\nTherefore, a new type of network structure catering to local smoothing (adversarial training) is encouraged, rather than purely focusing on increasing the network size. \\\n(c) Optimization is difficult in adversarial training. \\\nFor example, Zhang et al. (2020) showed adversarial training has cross-over mixture issues, which can potentially *kill* the learning [1].\\\nTherefore, a new optimization caters to adversarial training is encouraged. \\\n[1] Attacks Which Do Not Kill Training Make Adversarial Learning Stronger, ICML 2020    \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper332/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper332/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Geometry-aware Instance-reweighted Adversarial Training", "authorids": ["~Jingfeng_Zhang1", "~Jianing_Zhu2", "~Gang_Niu1", "~Bo_Han1", "~Masashi_Sugiyama1", "~Mohan_Kankanhalli1"], "authors": ["Jingfeng Zhang", "Jianing Zhu", "Gang Niu", "Bo Han", "Masashi Sugiyama", "Mohan Kankanhalli"], "keywords": ["Adversarial robustness"], "abstract": "In adversarial machine learning, there was a common belief that robustness and accuracy hurt each other. The belief was challenged by recent studies where we can maintain the robustness and improve the accuracy. However, the other direction, whether we can keep the accuracy and improve the robustness, is conceptually and practically more interesting, since robust accuracy should be lower than standard accuracy for any model. In this paper, we show this direction is also promising. Firstly, we find even over-parameterized deep networks may still have insufficient model capacity, because adversarial training has an overwhelming smoothing effect. Secondly, given limited model capacity, we argue adversarial data should have unequal importance: geometrically speaking, a natural data point closer to/farther from the class boundary is less/more robust, and the corresponding adversarial data point should be assigned with larger/smaller weight. Finally, to implement the idea, we propose geometry-aware instance-reweighted adversarial training, where the weights are based on how difficult it is to attack a natural data point. Experiments show that our proposal boosts the robustness of standard adversarial training; combining two directions, we improve both robustness and accuracy of standard adversarial training.", "one-sentence_summary": "This paper has proposed a novel adversarial training method, i.e., geometry-aware instance-reweighted adversarial training (GAIRAT), which sheds new lights on improving the adversarial training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|geometryaware_instancereweighted_adversarial_training", "supplementary_material": "/attachment/befac1f842cb530d192c70f91e2aae6bc3f03e20.zip", "pdf": "/pdf/746bc7119712b56c0426659dcb575f5864fa29ec.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021geometryaware,\ntitle={Geometry-aware Instance-reweighted Adversarial Training},\nauthor={Jingfeng Zhang and Jianing Zhu and Gang Niu and Bo Han and Masashi Sugiyama and Mohan Kankanhalli},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iAX0l6Cz8ub}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "iAX0l6Cz8ub", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper332/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper332/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper332/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper332/Authors|ICLR.cc/2021/Conference/Paper332/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper332/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872086, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper332/-/Official_Comment"}}}, {"id": "SQCuoJoRmyZ", "original": null, "number": 3, "cdate": 1605769933783, "ddate": null, "tcdate": 1605769933783, "tmdate": 1605771022888, "tddate": null, "forum": "iAX0l6Cz8ub", "replyto": "d4JvKmvAuO6", "invitation": "ICLR.cc/2021/Conference/Paper332/-/Official_Comment", "content": {"title": "Replies to Reviewer 1", "comment": "Many thanks for the great comments! Please find our replies below. \n\n1 Discuss the difference with related work. (MART, MMA, CAT, and DAT)\n\nMMA, CAT, and DAT generated differently adversarial data for updating the model. Specifically, the adversary strength is measured by PGD steps (CAT), convergence quality (DAT), and perturbations bound epsilon (MMA). \\\nDifferent from those existing methods, our GAIRAT treats adversarial data differently by explicitly assigning different weights on the adversarial loss of adversarial data. Explicitly assigning weights has the benefit of breaking the ``blocking effect\u2019\u2019 (The blocking effect is stated in Section 3.2). \\\nNote that MART's learning objective also explicitly assigned weights, not directly on the adversarial loss but KL divergence loss. The KL divergence loss helps to strengthen the smoothness within the norm ball of natural data, which is also used in VAT and TRADES. \\\nDifferently from MART, our GAIRAT explicitly assigns weights on the adversarial loss. Therefore, we can easily modify MART to GAIR-MART. \\\nBesides, MART assigns weights based on the model\u2019s prediction confidence on the natural data. GAIRAT assigns weights based on how easy the natural data can be attacked. \\\nWe have updated the main paper in Section 3.3 adding those discussions. \n\n2 Compare experiments with MART and MMA.\n\nWe have updated Appendix C.7 comparing MMA, MART, and our GAIR-MART. \\\nThe experiments show our GAIRAT outperforms the baselines.\n\n3 Weak explanations in Figure 2. \n\nWe have updated Figure 2 by adding the learning curve of standard training (red line). \\\nThere is a big gap between the red line and blue lines. \\\nThe over-parameterized networks that can easily memorize all data in standard training, find it difficult to fit data (both natural data and adversarial data) in adversarial training.\\\nCould I know in which part I can strengthen the explanations? \n\n4 The weight function of Eq (6) lacks some intuitive explanations. \n\nIn GAIRAT, weight assignment function $\\omega$ is non-increasing w.r.t. the geometry value $\\kappa$. \\\nEq. (6) is just one example, which is fungible. In Section C.3, we have discussed different types of $\\omega$. Experiments show all non-increasing $\\omega$ can enhance robustness significantly. \\\nFor more intuitive explanations,  $\\omega$ serves for the purpose of enforcing the different focus by the optimizer. The optimizer will focus less on already-guarded data and focus more on those attackable data. \\\nThe choices of formula and the constants are hyperparameters dependent on various datasets & learning tasks. \\\nIt is still an open question on choosing the optimal weight assignment functions; we will leave this as future work. \n\n5 Evaluations using AA attacks.\n\nWe have leveraged 500K unlabeled data (preprocessed by Carmon et al. 2019). Our geometry-aware instance-reweighed method can still facilitate a good WRN-28-10 model in terms of both robustness and accuracy. \\\nWe evaluate the model using auto attack (AA). AA attack is a combination of two white-box attacks and two black-box attacks. \nThe standard test accuracy is 89.36%, and AA attack accuracy (on the full test set) is 59.64%. \\\nWe have added Appendix C10 illustrating the details and the results. \\\nFor the code, you can check the updated attachment for the training details and verifying our methods.  "}, "signatures": ["ICLR.cc/2021/Conference/Paper332/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper332/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Geometry-aware Instance-reweighted Adversarial Training", "authorids": ["~Jingfeng_Zhang1", "~Jianing_Zhu2", "~Gang_Niu1", "~Bo_Han1", "~Masashi_Sugiyama1", "~Mohan_Kankanhalli1"], "authors": ["Jingfeng Zhang", "Jianing Zhu", "Gang Niu", "Bo Han", "Masashi Sugiyama", "Mohan Kankanhalli"], "keywords": ["Adversarial robustness"], "abstract": "In adversarial machine learning, there was a common belief that robustness and accuracy hurt each other. The belief was challenged by recent studies where we can maintain the robustness and improve the accuracy. However, the other direction, whether we can keep the accuracy and improve the robustness, is conceptually and practically more interesting, since robust accuracy should be lower than standard accuracy for any model. In this paper, we show this direction is also promising. Firstly, we find even over-parameterized deep networks may still have insufficient model capacity, because adversarial training has an overwhelming smoothing effect. Secondly, given limited model capacity, we argue adversarial data should have unequal importance: geometrically speaking, a natural data point closer to/farther from the class boundary is less/more robust, and the corresponding adversarial data point should be assigned with larger/smaller weight. Finally, to implement the idea, we propose geometry-aware instance-reweighted adversarial training, where the weights are based on how difficult it is to attack a natural data point. Experiments show that our proposal boosts the robustness of standard adversarial training; combining two directions, we improve both robustness and accuracy of standard adversarial training.", "one-sentence_summary": "This paper has proposed a novel adversarial training method, i.e., geometry-aware instance-reweighted adversarial training (GAIRAT), which sheds new lights on improving the adversarial training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|geometryaware_instancereweighted_adversarial_training", "supplementary_material": "/attachment/befac1f842cb530d192c70f91e2aae6bc3f03e20.zip", "pdf": "/pdf/746bc7119712b56c0426659dcb575f5864fa29ec.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021geometryaware,\ntitle={Geometry-aware Instance-reweighted Adversarial Training},\nauthor={Jingfeng Zhang and Jianing Zhu and Gang Niu and Bo Han and Masashi Sugiyama and Mohan Kankanhalli},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iAX0l6Cz8ub}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "iAX0l6Cz8ub", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper332/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper332/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper332/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper332/Authors|ICLR.cc/2021/Conference/Paper332/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper332/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872086, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper332/-/Official_Comment"}}}, {"id": "nb6k5Ow6Ug", "original": null, "number": 1, "cdate": 1605252605606, "ddate": null, "tcdate": 1605252605606, "tmdate": 1605252605606, "tddate": null, "forum": "iAX0l6Cz8ub", "replyto": "iAX0l6Cz8ub", "invitation": "ICLR.cc/2021/Conference/Paper332/-/Public_Comment", "content": {"title": "How does the defense perform with more iterations of PGD?", "comment": "This paper evaluates against 40 iterations of PGD, where the strongest result is then +7% accuracy. By repeating this to 5 random restarts, the increase goes down to +4.2% accuracy. What happens if you run for more (several hundred) iterations of gradient descent? Does the effect size continue to decrease?\n\nNote there is a difference between N iterations of gradient descent repeated M times, and N*M iterations of gradient descent. It is not always clear how to perform this tradeoff, but in many cases in the past 40 iterations of PGD has not been sufficient to converge.\n\nFor example, it may help to introduce a plot that shows accuracy as a function of PGD steps, as done for example in Figure 1 of Madry et al. 2017. Note that here the stop at 100 because the attack has (mostly) converged---you may have to try more if things have not converged by 100 iterations."}, "signatures": ["~Nicholas_Carlini1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Nicholas_Carlini1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Geometry-aware Instance-reweighted Adversarial Training", "authorids": ["~Jingfeng_Zhang1", "~Jianing_Zhu2", "~Gang_Niu1", "~Bo_Han1", "~Masashi_Sugiyama1", "~Mohan_Kankanhalli1"], "authors": ["Jingfeng Zhang", "Jianing Zhu", "Gang Niu", "Bo Han", "Masashi Sugiyama", "Mohan Kankanhalli"], "keywords": ["Adversarial robustness"], "abstract": "In adversarial machine learning, there was a common belief that robustness and accuracy hurt each other. The belief was challenged by recent studies where we can maintain the robustness and improve the accuracy. However, the other direction, whether we can keep the accuracy and improve the robustness, is conceptually and practically more interesting, since robust accuracy should be lower than standard accuracy for any model. In this paper, we show this direction is also promising. Firstly, we find even over-parameterized deep networks may still have insufficient model capacity, because adversarial training has an overwhelming smoothing effect. Secondly, given limited model capacity, we argue adversarial data should have unequal importance: geometrically speaking, a natural data point closer to/farther from the class boundary is less/more robust, and the corresponding adversarial data point should be assigned with larger/smaller weight. Finally, to implement the idea, we propose geometry-aware instance-reweighted adversarial training, where the weights are based on how difficult it is to attack a natural data point. Experiments show that our proposal boosts the robustness of standard adversarial training; combining two directions, we improve both robustness and accuracy of standard adversarial training.", "one-sentence_summary": "This paper has proposed a novel adversarial training method, i.e., geometry-aware instance-reweighted adversarial training (GAIRAT), which sheds new lights on improving the adversarial training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|geometryaware_instancereweighted_adversarial_training", "supplementary_material": "/attachment/befac1f842cb530d192c70f91e2aae6bc3f03e20.zip", "pdf": "/pdf/746bc7119712b56c0426659dcb575f5864fa29ec.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021geometryaware,\ntitle={Geometry-aware Instance-reweighted Adversarial Training},\nauthor={Jingfeng Zhang and Jianing Zhu and Gang Niu and Bo Han and Masashi Sugiyama and Mohan Kankanhalli},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iAX0l6Cz8ub}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "iAX0l6Cz8ub", "readers": {"description": "User groups that will be able to read this comment.", "values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed."}}, "expdate": 1605630600000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper332/Authors", "ICLR.cc/2021/Conference/Paper332/Reviewers", "ICLR.cc/2021/Conference/Paper332/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1605024983209, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper332/-/Public_Comment"}}}, {"id": "eFk4zWtOKes", "original": null, "number": 2, "cdate": 1603780294279, "ddate": null, "tcdate": 1603780294279, "tmdate": 1605024712463, "tddate": null, "forum": "iAX0l6Cz8ub", "replyto": "iAX0l6Cz8ub", "invitation": "ICLR.cc/2021/Conference/Paper332/-/Official_Review", "content": {"title": "Reviews for GAIRAT", "review": "This paper focuses on adversarial learning. It improves the robustness while keeping the accuracy. To achieve this point, the authors find that adversarial data should have unequal importance, which naturally brings geometry-award instance-reweighted adversarial training (GAIRAT).\n\nPros:\n1. The paper has strong novelty in philosophy level. The common belief is that robustness and accuracy hurt each other. However, this paper shows that the robustness can be improved while keeping accuracy. As far as I know, this point has never been explored before.\n\n2. The paper is well motivated and easy to follow. First, the authors use Figure 1 to illustrate the GAIRAT, which explicitly gives larger weights on the losses of adversarial data. The authors use two toy examples in Figure 3 to explain GAIRAT more. Second, the whole logic of this paper is easy to follow. For example, after explaining motivations of GAIRAT, we can clearly see the objective function of GAIRAT and its realization.\n\n3. The paper is sufficiently justified in experiments. For example, PGD-200 has been used to verify the robustness of GAIRAT. From my personal opinion, this result is quite strong. Moreover, the authors upgrade their method by incorporing FAT and verify the robustness of GAIR-FAT.\n\nCons:\n1.In the top right panel of Figure 10, the SVHN experiments have a period of increasing robustness training error for GAIRAT. Could you explain this? \n\n2.Although authors show that model capacity is not enough in adversarial training, how large the DNN should be enough? What do you think?", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper332/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper332/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Geometry-aware Instance-reweighted Adversarial Training", "authorids": ["~Jingfeng_Zhang1", "~Jianing_Zhu2", "~Gang_Niu1", "~Bo_Han1", "~Masashi_Sugiyama1", "~Mohan_Kankanhalli1"], "authors": ["Jingfeng Zhang", "Jianing Zhu", "Gang Niu", "Bo Han", "Masashi Sugiyama", "Mohan Kankanhalli"], "keywords": ["Adversarial robustness"], "abstract": "In adversarial machine learning, there was a common belief that robustness and accuracy hurt each other. The belief was challenged by recent studies where we can maintain the robustness and improve the accuracy. However, the other direction, whether we can keep the accuracy and improve the robustness, is conceptually and practically more interesting, since robust accuracy should be lower than standard accuracy for any model. In this paper, we show this direction is also promising. Firstly, we find even over-parameterized deep networks may still have insufficient model capacity, because adversarial training has an overwhelming smoothing effect. Secondly, given limited model capacity, we argue adversarial data should have unequal importance: geometrically speaking, a natural data point closer to/farther from the class boundary is less/more robust, and the corresponding adversarial data point should be assigned with larger/smaller weight. Finally, to implement the idea, we propose geometry-aware instance-reweighted adversarial training, where the weights are based on how difficult it is to attack a natural data point. Experiments show that our proposal boosts the robustness of standard adversarial training; combining two directions, we improve both robustness and accuracy of standard adversarial training.", "one-sentence_summary": "This paper has proposed a novel adversarial training method, i.e., geometry-aware instance-reweighted adversarial training (GAIRAT), which sheds new lights on improving the adversarial training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|geometryaware_instancereweighted_adversarial_training", "supplementary_material": "/attachment/befac1f842cb530d192c70f91e2aae6bc3f03e20.zip", "pdf": "/pdf/746bc7119712b56c0426659dcb575f5864fa29ec.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021geometryaware,\ntitle={Geometry-aware Instance-reweighted Adversarial Training},\nauthor={Jingfeng Zhang and Jianing Zhu and Gang Niu and Bo Han and Masashi Sugiyama and Mohan Kankanhalli},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iAX0l6Cz8ub}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "iAX0l6Cz8ub", "replyto": "iAX0l6Cz8ub", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper332/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538145439, "tmdate": 1606915794186, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper332/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper332/-/Official_Review"}}}, {"id": "Pji8Q4aybzs", "original": null, "number": 1, "cdate": 1603780262283, "ddate": null, "tcdate": 1603780262283, "tmdate": 1605024712341, "tddate": null, "forum": "iAX0l6Cz8ub", "replyto": "iAX0l6Cz8ub", "invitation": "ICLR.cc/2021/Conference/Paper332/-/Official_Review", "content": {"title": "Paper332 AnonReviewer3", "review": "This paper challenges the common belief of the inherent tradeoff between robustness and accuracy.\nInstead of recent methods improving accuracy while maintaining robustness, this paper proposes a geometry aware instance reweighed adversarial training (GAIRAT) method to improve robustness while maintaining accuracy. \n\nPros:\n1 The direction---improving robustness while maintaining accuracy---is novel and interesting. \n\nSpecifically, several papers are challenging the inherent tradeoff, e.g., using more data [1], utilizing early stopped PGD [2], and incorporating dropout [3]. This paper still challenges the inherent tradeoff. \nHowever, different from [2,3] improving accuracy while maintaining robustness, this paper goes the other direction. \nTo my knowledge, this is the first paper to explore this direction. \n\n[1] Understanding and Mitigating the Tradeoff Between Robustness and Accuracy, ICML 2020\n[2] Attacks Which Do Not Kill Training Make Adversarial Learning Stronger, ICML 2020\n[3] A closer Look at Accuracy vs. Robustness, NeurIPS 2020\n\n2 This paper has made two conceptual improvements. a) This paper explicitly argues that the overparameterized networks that have enough model capacity in standard training suffer from the insufficiency in adversarial training (though many studies have already shown AT needs the large model). b) This paper argues that under limited model capacity, adversarial data should have unequal importance. Unequal data's treatment was explored in the traditional ML methods several years ago, but it is rare in deep learning at this moment. \n\n3 The proposed GAIRAT method is effective, indeed increasing robustness while retaining accuracy. The experiments are comprehensive over different network structures, datasets and attack methods. The experiments in the appendix provide much useful information. \n\n\nCons:\n1.The design of weight assignment function in Section 3.3 seems heuristic. Would you explain some principles on assigning instance dependent weights? \n\n2.In Figure 4, the GAIRAT method can relieve undesirable robust overfitting. Would you explain more about this? For example, why the robust overfitting exists in standard adversarial training? how/why your GAIRAT methods relieve it?", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper332/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper332/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Geometry-aware Instance-reweighted Adversarial Training", "authorids": ["~Jingfeng_Zhang1", "~Jianing_Zhu2", "~Gang_Niu1", "~Bo_Han1", "~Masashi_Sugiyama1", "~Mohan_Kankanhalli1"], "authors": ["Jingfeng Zhang", "Jianing Zhu", "Gang Niu", "Bo Han", "Masashi Sugiyama", "Mohan Kankanhalli"], "keywords": ["Adversarial robustness"], "abstract": "In adversarial machine learning, there was a common belief that robustness and accuracy hurt each other. The belief was challenged by recent studies where we can maintain the robustness and improve the accuracy. However, the other direction, whether we can keep the accuracy and improve the robustness, is conceptually and practically more interesting, since robust accuracy should be lower than standard accuracy for any model. In this paper, we show this direction is also promising. Firstly, we find even over-parameterized deep networks may still have insufficient model capacity, because adversarial training has an overwhelming smoothing effect. Secondly, given limited model capacity, we argue adversarial data should have unequal importance: geometrically speaking, a natural data point closer to/farther from the class boundary is less/more robust, and the corresponding adversarial data point should be assigned with larger/smaller weight. Finally, to implement the idea, we propose geometry-aware instance-reweighted adversarial training, where the weights are based on how difficult it is to attack a natural data point. Experiments show that our proposal boosts the robustness of standard adversarial training; combining two directions, we improve both robustness and accuracy of standard adversarial training.", "one-sentence_summary": "This paper has proposed a novel adversarial training method, i.e., geometry-aware instance-reweighted adversarial training (GAIRAT), which sheds new lights on improving the adversarial training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|geometryaware_instancereweighted_adversarial_training", "supplementary_material": "/attachment/befac1f842cb530d192c70f91e2aae6bc3f03e20.zip", "pdf": "/pdf/746bc7119712b56c0426659dcb575f5864fa29ec.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021geometryaware,\ntitle={Geometry-aware Instance-reweighted Adversarial Training},\nauthor={Jingfeng Zhang and Jianing Zhu and Gang Niu and Bo Han and Masashi Sugiyama and Mohan Kankanhalli},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iAX0l6Cz8ub}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "iAX0l6Cz8ub", "replyto": "iAX0l6Cz8ub", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper332/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538145439, "tmdate": 1606915794186, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper332/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper332/-/Official_Review"}}}], "count": 18}