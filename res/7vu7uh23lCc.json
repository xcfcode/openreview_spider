{"notes": [{"id": "7vu7uh23lCc", "original": "o_YjfOhwMOc", "number": 19, "cdate": 1615310252042, "ddate": null, "tcdate": 1615310252042, "tmdate": 1615313021573, "tddate": null, "forum": "7vu7uh23lCc", "replyto": null, "invitation": "ICLR.cc/2021/Workshop/SSL-RL/-/Blind_Submission", "content": {"title": "Learning Generalizable Robotic Reward Functions from \"In-The-Wild\" Human Videos", "authorids": ["ICLR.cc/2021/Workshop/SSL-RL/Paper19/Authors"], "authors": ["Anonymous"], "keywords": ["Deep Reinforcement Learning", "Robot Learning", "Multi-Task Learning"], "TL;DR": "Using \"In-The-Wild\" videos of humans to learn generalizable reward functions for robot learning.", "abstract": "We are motivated by the goal of generalist robotic agents that can complete a wide range of tasks across many environments. Critical to this is the robot\u2019s ability to acquire some metric of task success or reward, which is necessary for reinforcement learning, planning, or knowing when to ask for help. For a general-purpose robot operating in the real world, this reward function must also be able to generalize broadly across environments, tasks, and objects, while depending only on-board sensor observations (e.g. RGB images). While deep learning on large and diverse datasets has shown promise as a path towards such generalization in computer vision and natural language, collecting high quality data of robotic interaction at scale remains an open challenge. In contrast, \u201cin-the-wild\u201d videos of humans (e.g. YouTube) contain an extensive collection of people doing interesting tasks across a diverse range of settings. Unlocking this data for learning robotic reward functions could lead to more scalable training of robots for numerous tasks and would be an important step towards generalist robots. In this work, we propose a simple approach, Domain-agnostic Video Discriminator (DVD), for learning multitask reward functions which can generalize by virtue of learning from a small amount of robot data with a broad dataset of human videos. We find that by leveraging diverse human datasets, this reward function (a) can generalize zero shot to unseen environments, (b) generalize zero shot to unseen tasks, and (c) can be combined with visual model predictive control to solve robotic manipulation tasks on a real WidowX200 robot in an unseen environment from a single human demo.", "pdf": "/pdf/ae3c6837c639761ccda76389957bc2531d510a0f.pdf", "paperhash": "anonymous|learning_generalizable_robotic_reward_functions_from_inthewild_human_videos", "_bibtex": "@inproceedings{\nanonymous2021learning,\ntitle={Learning Generalizable Robotic Reward Functions from ''In-The-Wild'' Human Videos},\nauthor={Anonymous},\nbooktitle={Submitted to Self-Supervision for Reinforcement Learning Workshop - ICLR 2021},\nyear={2021},\nurl={https://openreview.net/forum?id=7vu7uh23lCc},\nnote={under review}\n}"}, "signatures": ["ICLR.cc/2021/Workshop/SSL-RL"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Workshop/SSL-RL"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Workshop/SSL-RL"]}, "signatures": {"values": ["ICLR.cc/2021/Workshop/SSL-RL"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Workshop/SSL-RL"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Workshop/SSL-RL"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1615310247528, "tmdate": 1615313016556, "id": "ICLR.cc/2021/Workshop/SSL-RL/-/Blind_Submission"}}, "tauthor": "~Super_User1"}], "count": 1}