{"notes": [{"id": "Hyg_X2C5FX", "original": "rkgTbxR9Ym", "number": 1371, "cdate": 1538087967675, "ddate": null, "tcdate": 1538087967675, "tmdate": 1550876244176, "tddate": null, "forum": "Hyg_X2C5FX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "GAN Dissection: Visualizing and Understanding Generative Adversarial Networks", "abstract": "Generative Adversarial Networks (GANs) have recently achieved impressive results for many real-world applications, and many GAN variants have emerged with improvements in sample quality and training stability. However, visualization and understanding of GANs is largely missing. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models.\n\nIn this work, we present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to object concepts with a segmentation-based network dissection method. Then, we quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. Finally, we examine the contextual relationship between these units and their surrounding by inserting the discovered object concepts into new images. We show several practical applications enabled by our framework, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing artifact-causing units, to interactively manipulating objects in the scene. We provide open source interpretation tools to help peer researchers and practitioners better understand their GAN models.", "paperhash": "bau|gan_dissection_visualizing_and_understanding_generative_adversarial_networks", "TL;DR": "GAN representations are examined in detail, and sets of representation units are found that control the generation of semantic concepts in the output.", "authorids": ["davidbau@csail.mit.edu", "junyanz@csail.mit.edu", "hendrik.strobelt@ibm.com", "bzhou@csail.mit.edu", "jbt@csail.mit.edu", "billf@csail.mit.edu", "torralba@csail.mit.edu"], "authors": ["David Bau", "Jun-Yan Zhu", "Hendrik Strobelt", "Bolei Zhou", "Joshua B. Tenenbaum", "William T. Freeman", "Antonio Torralba"], "keywords": ["GANs", "representation", "interpretability", "causality"], "pdf": "/pdf/850f5beb9fbbedefecd6a9a8753e693cc9a0aa37.pdf", "_bibtex": "@inproceedings{\nbau2018visualizing,\ntitle={Visualizing and Understanding Generative Adversarial Networks},\nauthor={David Bau and Jun-Yan Zhu and Hendrik Strobelt and Bolei Zhou and Joshua B. Tenenbaum and William T. Freeman and Antonio Torralba},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hyg_X2C5FX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "B1la7rH-g4", "original": null, "number": 1, "cdate": 1544799524642, "ddate": null, "tcdate": 1544799524642, "tmdate": 1545354526021, "tddate": null, "forum": "Hyg_X2C5FX", "replyto": "Hyg_X2C5FX", "invitation": "ICLR.cc/2019/Conference/-/Paper1371/Meta_Review", "content": {"metareview": "The paper proposes an interesting framework for visualizing and understanding GANs, that will be of clear help for understanding existing models and might provide insights for developing new ones. ", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "Intersting framework for the analysis of GANs"}, "signatures": ["ICLR.cc/2019/Conference/Paper1371/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1371/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GAN Dissection: Visualizing and Understanding Generative Adversarial Networks", "abstract": "Generative Adversarial Networks (GANs) have recently achieved impressive results for many real-world applications, and many GAN variants have emerged with improvements in sample quality and training stability. However, visualization and understanding of GANs is largely missing. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models.\n\nIn this work, we present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to object concepts with a segmentation-based network dissection method. Then, we quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. Finally, we examine the contextual relationship between these units and their surrounding by inserting the discovered object concepts into new images. We show several practical applications enabled by our framework, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing artifact-causing units, to interactively manipulating objects in the scene. We provide open source interpretation tools to help peer researchers and practitioners better understand their GAN models.", "paperhash": "bau|gan_dissection_visualizing_and_understanding_generative_adversarial_networks", "TL;DR": "GAN representations are examined in detail, and sets of representation units are found that control the generation of semantic concepts in the output.", "authorids": ["davidbau@csail.mit.edu", "junyanz@csail.mit.edu", "hendrik.strobelt@ibm.com", "bzhou@csail.mit.edu", "jbt@csail.mit.edu", "billf@csail.mit.edu", "torralba@csail.mit.edu"], "authors": ["David Bau", "Jun-Yan Zhu", "Hendrik Strobelt", "Bolei Zhou", "Joshua B. Tenenbaum", "William T. Freeman", "Antonio Torralba"], "keywords": ["GANs", "representation", "interpretability", "causality"], "pdf": "/pdf/850f5beb9fbbedefecd6a9a8753e693cc9a0aa37.pdf", "_bibtex": "@inproceedings{\nbau2018visualizing,\ntitle={Visualizing and Understanding Generative Adversarial Networks},\nauthor={David Bau and Jun-Yan Zhu and Hendrik Strobelt and Bolei Zhou and Joshua B. Tenenbaum and William T. Freeman and Antonio Torralba},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hyg_X2C5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1371/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352779578, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hyg_X2C5FX", "replyto": "Hyg_X2C5FX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1371/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1371/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1371/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352779578}}}, {"id": "SJlKyI2FA7", "original": null, "number": 4, "cdate": 1543255520729, "ddate": null, "tcdate": 1543255520729, "tmdate": 1543255520729, "tddate": null, "forum": "Hyg_X2C5FX", "replyto": "rylRgFDnnQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1371/Official_Comment", "content": {"title": "Answers to questions for AnonReviewer3", "comment": "Thank you for your comments and questions; we have incorporated your suggestions in the revision, and we also answer your questions below.\n\nQ7:  apply the author's methods to other architecture, and to other application domains? \n \nA7: We have applied our method to WGAN-GP model with a different generator architecture, as shown in Figure 16 in Section S-6.3. Our method can find interpretable units for different GANs objectives and architectures.\n\nThe general framework can be extended beyond generative models for vision, although that topic is beyond the scope of the current paper.  Concurrent work submitted to ICLR 2019 is an example of similar ideas being applied to natural language translation. (https://openreview.net/forum?id=H1z-PsR5KX)\n\nQ8: how to choose the 'units' for which they seek interpretation when reporting their results?\n\nA8: We do two analyses.  For the dissection analysis examining correlation, u are analyzed as individual units (i.e., |U| = 1). We analyze every individual unit in a layer, and we plot all units that match a segmented concept with IoU exceeding 5%.\n\nFor the causal analysis, we choose the elements of U by doing the optimization described in equation (6), which finds an alpha that specifies a contribution for every unit to maximize causal effects, ranking units according to highest alpha, and choosing the number needed to achieve a desired causal effect.\n\nQ9:  How large does u tend to be? How would one choose it?  Is it one filter out of all filters in a certain layer?\n\nA9: To choose U to have strong causal effects, we measure and plot the causal effect of different numbers of units for U as in Figure 4. The increase in causal effect diminishes after about 20 units. To be able to compare different causal sets on an equal basis, we set |U| = 20 for most of our experiments.\n\nQ10: When optimizing for sets of units together (using the alpha probabilities and the optimization in eq. 6) what is d? Is it performed for all units in a single layer? More details would be useful here.\n\nA10: Yes, we perform an optimization for all units in a single layer.  d is the number of all units in a single layer (512, for the case of layer 4 of our Progressive GAN).\n\nFor the dissection analysis, we analyze every individual unit in a layer, and we plot all units that match a segmented concept with IoU exceeding 5%.  The causal analysis requires identifying sets of units, which is done through the optimization in equation (6).\n\nBeyond this objective, learning U involves several additional details including how to specify the big constant for positive intervention, how to sample class-relevant positions, and how to initialize the coefficient alpha. We have added a section S-6.4 to supplementary materials with these implementation details.\n\nQ11: Regarding SWD and FID\n\nA11: SWD and FID are measures which estimate realism of the GAN output by measuring the distance between the generated distribution of images and the true distribution of images; Borji (arXiv 2018) surveys and compares these methods at https://arxiv.org/abs/1802.03446.  We have clarified these terms and added citations in the paper.\n\nQ12: No reference to supp. info and minor typos: \n\nA12: Thank you for your detailed comments; we have updated the text and expanded the supplementary materials. We also added a brief summary of the supplementary material in each section of the main paper. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1371/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1371/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1371/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GAN Dissection: Visualizing and Understanding Generative Adversarial Networks", "abstract": "Generative Adversarial Networks (GANs) have recently achieved impressive results for many real-world applications, and many GAN variants have emerged with improvements in sample quality and training stability. However, visualization and understanding of GANs is largely missing. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models.\n\nIn this work, we present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to object concepts with a segmentation-based network dissection method. Then, we quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. Finally, we examine the contextual relationship between these units and their surrounding by inserting the discovered object concepts into new images. We show several practical applications enabled by our framework, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing artifact-causing units, to interactively manipulating objects in the scene. We provide open source interpretation tools to help peer researchers and practitioners better understand their GAN models.", "paperhash": "bau|gan_dissection_visualizing_and_understanding_generative_adversarial_networks", "TL;DR": "GAN representations are examined in detail, and sets of representation units are found that control the generation of semantic concepts in the output.", "authorids": ["davidbau@csail.mit.edu", "junyanz@csail.mit.edu", "hendrik.strobelt@ibm.com", "bzhou@csail.mit.edu", "jbt@csail.mit.edu", "billf@csail.mit.edu", "torralba@csail.mit.edu"], "authors": ["David Bau", "Jun-Yan Zhu", "Hendrik Strobelt", "Bolei Zhou", "Joshua B. Tenenbaum", "William T. Freeman", "Antonio Torralba"], "keywords": ["GANs", "representation", "interpretability", "causality"], "pdf": "/pdf/850f5beb9fbbedefecd6a9a8753e693cc9a0aa37.pdf", "_bibtex": "@inproceedings{\nbau2018visualizing,\ntitle={Visualizing and Understanding Generative Adversarial Networks},\nauthor={David Bau and Jun-Yan Zhu and Hendrik Strobelt and Bolei Zhou and Joshua B. Tenenbaum and William T. Freeman and Antonio Torralba},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hyg_X2C5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1371/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610073, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hyg_X2C5FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1371/Authors", "ICLR.cc/2019/Conference/Paper1371/Reviewers", "ICLR.cc/2019/Conference/Paper1371/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1371/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1371/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1371/Authors|ICLR.cc/2019/Conference/Paper1371/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1371/Reviewers", "ICLR.cc/2019/Conference/Paper1371/Authors", "ICLR.cc/2019/Conference/Paper1371/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610073}}}, {"id": "Syga2ShtR7", "original": null, "number": 3, "cdate": 1543255476977, "ddate": null, "tcdate": 1543255476977, "tmdate": 1543255476977, "tddate": null, "forum": "Hyg_X2C5FX", "replyto": "Bklj6-einQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1371/Official_Comment", "content": {"title": "Answers to questions for AnonReviewer2", "comment": "Thank you for your comments and questions; we have incorporated your suggestions in the revision, and we answer your questions below.\n\nQ3: About diagnosing and improving GANs, please give more details of the human annotation for the artifacts.\n\nA3: We visualize the top 10 highest activating images for each unit, and we manually identify units with noticeable artifacts in this set.  (This human annotation was done by an author.)  \nDetails have been added to section 4.2.  This method for diagnosing and improving GANs is further analyzed and expanded in the supplementary materials, in section S-6.1.\n\nQ4: Minor - I think there is a typo in the first and second paragraphs in section 4.2, Figure 14 -> Figure 8.\n\nA4: Thanks for your detailed comments. We have fixed it. \n\nQ5: Have you ever considered to handle these imperfect semantic segmentation models?\n\nA5: We totally agree with the reviewer: the success of our method is linked to the accuracy and comprehensiveness of the segmentation model used.  We have performed a human evaluation regarding the accuracy of our method on a Progressive GAN model (on LSUN living rooms), and have found that, our method provides correct labels for 96% of interpretable units.  Further details of the evaluation can be found in section S-6.2.\n\nIn addition, a semantic segmentation model can perform poorly if the analyzed images are very different from the images on which the semantic segmentation was trained.  For example in the \u201cbedroom\u201d scene category, if a unit is labeled as correlating with \u2018swimming pool\u2019 this may be due to a poorly performing GAN model. We have partly addressed this issue by measuring the average realism of each unit using the FID metric. In practice, in Figure 16, we show the effect of such a filter in which we only report \u201crealistic\u201d and interpretable units.  Details of such an approach have been added to section S-6.3.\n\nAs more accurate and robust segmentation models are developed, we expect our method to be able to identify more semantic concepts inside a representation.\n\n\nQ6: Is there a way to apply the framework to the training process of GANs?\n\nA6: By using a per-unit realism score based on the FID metric on generator units learned by the GAN, we can identify units that should be zeroed to improve the realism of the GAN output.  (We assign a realism score to each unit by measuring FID for a subset of images that highly activate the unit.) Zeroing the units with the highest FID score as measured this way will improve the quality of the output nearly as well as ablating units identified manually. This modification could be incorporated into an automatic training process. S-6.1 has further details and a preliminary evaluation of this idea for introducing per-unit analysis in an automatic process.  A full development of this idea is left to future work.\n\nDissection can also be used to monitor the progress of training by quantifying the emergence, diversity, and quality of semantic units.  For example, in Figure 18 we show dissections of layer4 representations of a Progressive GAN model trained on bedrooms, captured at a sequence of checkpoints during training.  As training proceeds, the number of units matching objects (and the number of object classes with matching units) increases, and the quality of object detectors as measured by average IoU over units increases.  During this successful training, dissection suggests that the model is learning the structure of a bedroom, because increasingly units converge to meaningful bedroom concepts.  We add this analysis to section S-6.6."}, "signatures": ["ICLR.cc/2019/Conference/Paper1371/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1371/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1371/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GAN Dissection: Visualizing and Understanding Generative Adversarial Networks", "abstract": "Generative Adversarial Networks (GANs) have recently achieved impressive results for many real-world applications, and many GAN variants have emerged with improvements in sample quality and training stability. However, visualization and understanding of GANs is largely missing. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models.\n\nIn this work, we present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to object concepts with a segmentation-based network dissection method. Then, we quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. Finally, we examine the contextual relationship between these units and their surrounding by inserting the discovered object concepts into new images. We show several practical applications enabled by our framework, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing artifact-causing units, to interactively manipulating objects in the scene. We provide open source interpretation tools to help peer researchers and practitioners better understand their GAN models.", "paperhash": "bau|gan_dissection_visualizing_and_understanding_generative_adversarial_networks", "TL;DR": "GAN representations are examined in detail, and sets of representation units are found that control the generation of semantic concepts in the output.", "authorids": ["davidbau@csail.mit.edu", "junyanz@csail.mit.edu", "hendrik.strobelt@ibm.com", "bzhou@csail.mit.edu", "jbt@csail.mit.edu", "billf@csail.mit.edu", "torralba@csail.mit.edu"], "authors": ["David Bau", "Jun-Yan Zhu", "Hendrik Strobelt", "Bolei Zhou", "Joshua B. Tenenbaum", "William T. Freeman", "Antonio Torralba"], "keywords": ["GANs", "representation", "interpretability", "causality"], "pdf": "/pdf/850f5beb9fbbedefecd6a9a8753e693cc9a0aa37.pdf", "_bibtex": "@inproceedings{\nbau2018visualizing,\ntitle={Visualizing and Understanding Generative Adversarial Networks},\nauthor={David Bau and Jun-Yan Zhu and Hendrik Strobelt and Bolei Zhou and Joshua B. Tenenbaum and William T. Freeman and Antonio Torralba},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hyg_X2C5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1371/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610073, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hyg_X2C5FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1371/Authors", "ICLR.cc/2019/Conference/Paper1371/Reviewers", "ICLR.cc/2019/Conference/Paper1371/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1371/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1371/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1371/Authors|ICLR.cc/2019/Conference/Paper1371/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1371/Reviewers", "ICLR.cc/2019/Conference/Paper1371/Authors", "ICLR.cc/2019/Conference/Paper1371/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610073}}}, {"id": "HJlb9ShFRm", "original": null, "number": 2, "cdate": 1543255433350, "ddate": null, "tcdate": 1543255433350, "tmdate": 1543255433350, "tddate": null, "forum": "Hyg_X2C5FX", "replyto": "H1lQioJchm", "invitation": "ICLR.cc/2019/Conference/-/Paper1371/Official_Comment", "content": {"title": "Answers to questions for AnonReviewer1", "comment": "Thank you for your comments and questions; we have incorporated your suggestions in the revision, and we also answer your questions below.\n\nQ1: Theoretical interpretation of the visualization, and comparisons to the Class Activation Maps (CAM)?\n \nA1: Our visualization is very simple and corresponds to equation (2): we upsample a single channel of the activation featuremap and show the region exceeding a threshold: unlike CAM, no gradients are considered. The threshold used is chosen to maximize relative mutual information with the best-matching object class based on semantic segmentation, however, a fixed threshold such as a top 1% quantile level would look very similar.\n\nIt is also informative to consider a CAM-like visualization of the causal impact of interventions in the model on later layers: we can create a heatmap where each pixel shows the magnitude of the last featuremap layer change that results when making an intervention at each pixel in an early layer.  The result is shown in Figure 17 of supplementary materials S-6.4: this visualization shows that the effects of an intervention at different locations are not uniform. The heatmap pattern reveals the structure of the model\u2019s sensitivity to a specific concept at various locations.\n\nQ2: How is the rate of finding the correct sets of units for a particular visual class?\n\nA2:   Our method provides a correct label for 96% of interpretable units, as measured by the following human evaluation, which we have added to supplementary materials, section S-6.2.\n\nFor each of 512 units of layer 4 of a \"living room\" progressive GAN,  5-9 human labels are collected (3728 labels total), where the AMT worker is asked to provide one or two words describing the highlighted patches in a set of top-activating images for a unit.  Of the 512 units, 201 units were described by a consistent word (such as \"sofa\", \"fireplace\" or \"wicker\") that was supplied by 50% or more of the human labels.\n\nApplying our segmentation-based dissection method, 154/201 of these units are also labeled with a confident label with IoU > 0.05 by dissection.  In most of the cases (104/154), the segmentation-based method gave the same label word as the human labelers, and most others are slight shifts in specificity (e.g. segmentation says \"ottoman\" or \"curtain\" or \"painting\" when a person says \"sofa\" or \"window\" or \"picture\").  A second AMT evaluation was done to rate the accuracy of both segmentation-derived and human-derived labels.  Human-derived labels scored 100% (i.e., of the 201 human-labeled units, all of the labels were rated to be accurate by most raters).  Of the 154 of our segmentation-generated labels, 149 (96%) were rated as accurate by most AMT raters as well.\n\nThe five failure cases (where the segmentation is confident but rated as inaccurate by humans) arise from situations in which human evaluators saw one pattern from seeing only 20 top-activating images, while the algorithm, in evaluating 1000 images, counted a different concept as dominant.  (E.g., in one example shown in Figure 14a, there are only a few ceilings highlighted and mostly sofas, whereas in the larger 1000-image set, mostly ceilings are triggered.)\n\nThere were also 47/201 cases where the segmenter was not confident while humans had consensus.  Some of these are due to missing concepts in the segmenter.  For example, several units are devoted to letterboxing (white stripes at the top and bottom of images), and the segmentation had no confident label to assign to these (Figure 14b).\n\nWe expect that as semantic segmentations improve to be able to identify more concepts such as abstract shapes, more of these units can be automatically identified.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1371/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1371/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1371/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GAN Dissection: Visualizing and Understanding Generative Adversarial Networks", "abstract": "Generative Adversarial Networks (GANs) have recently achieved impressive results for many real-world applications, and many GAN variants have emerged with improvements in sample quality and training stability. However, visualization and understanding of GANs is largely missing. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models.\n\nIn this work, we present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to object concepts with a segmentation-based network dissection method. Then, we quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. Finally, we examine the contextual relationship between these units and their surrounding by inserting the discovered object concepts into new images. We show several practical applications enabled by our framework, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing artifact-causing units, to interactively manipulating objects in the scene. We provide open source interpretation tools to help peer researchers and practitioners better understand their GAN models.", "paperhash": "bau|gan_dissection_visualizing_and_understanding_generative_adversarial_networks", "TL;DR": "GAN representations are examined in detail, and sets of representation units are found that control the generation of semantic concepts in the output.", "authorids": ["davidbau@csail.mit.edu", "junyanz@csail.mit.edu", "hendrik.strobelt@ibm.com", "bzhou@csail.mit.edu", "jbt@csail.mit.edu", "billf@csail.mit.edu", "torralba@csail.mit.edu"], "authors": ["David Bau", "Jun-Yan Zhu", "Hendrik Strobelt", "Bolei Zhou", "Joshua B. Tenenbaum", "William T. Freeman", "Antonio Torralba"], "keywords": ["GANs", "representation", "interpretability", "causality"], "pdf": "/pdf/850f5beb9fbbedefecd6a9a8753e693cc9a0aa37.pdf", "_bibtex": "@inproceedings{\nbau2018visualizing,\ntitle={Visualizing and Understanding Generative Adversarial Networks},\nauthor={David Bau and Jun-Yan Zhu and Hendrik Strobelt and Bolei Zhou and Joshua B. Tenenbaum and William T. Freeman and Antonio Torralba},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hyg_X2C5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1371/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610073, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hyg_X2C5FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1371/Authors", "ICLR.cc/2019/Conference/Paper1371/Reviewers", "ICLR.cc/2019/Conference/Paper1371/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1371/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1371/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1371/Authors|ICLR.cc/2019/Conference/Paper1371/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1371/Reviewers", "ICLR.cc/2019/Conference/Paper1371/Authors", "ICLR.cc/2019/Conference/Paper1371/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610073}}}, {"id": "BJgk8S3K0m", "original": null, "number": 1, "cdate": 1543255366753, "ddate": null, "tcdate": 1543255366753, "tmdate": 1543255366753, "tddate": null, "forum": "Hyg_X2C5FX", "replyto": "Hyg_X2C5FX", "invitation": "ICLR.cc/2019/Conference/-/Paper1371/Official_Comment", "content": {"title": "Summary of changes to the manuscript", "comment": "We thank all the reviewers for their helpful comments. We are glad that they found the topic important, the idea new, and the visualization results convincing. We have addressed individual questions raised by the reviewers in separate posts. Below we summarize the major changes in this revision. \n\n- In supplementary material S-6.1, we show an automatic evaluation of per-unit realism that can be done using FID measurements, and we show that zeroing these units improves the quality of the output. We have also corrected our FID computation by eliminating JPEG artifacts in our evaluation pipeline and recomputed FID comparisons in Table 1. (R2Q3, R2Q6)\n- In S-6.2, we conduct a human evaluation of dissection label accuracy for interpretable units. (R1Q2, R2Q5) \n- In S-6.3, we show how unit realism can be used to filter the results to protect the segmenter against unrealistic images that can be produced by some GAN models. (R2Q5, R3Q7)\n- In S-6.4, we provide details of our method for optimizing causal units. To eliminate a hyperparameter, we have defined the large constant \u201cc\u201d used for positive interventions to be a mean conditioned on the target class, rather than an unconditional 99 percentile value.  Figures 4, 9, 10, and 11 have been updated with results based on this adjustment. (R3Q10) \n- In S-6.5, we have traced the effects of interventions through downstream layers and show how a CAM-like heatmap can be used to visualize these effects. (R1Q1)\n- In S-6.6, we show how dissection can be used to monitor the emergence of unit semantics during the training epochs of a GAN. (R2Q6)\n- We have fixed minor typos and grammar errors (R2Q4, R3Q12)\n- We have clarified the method for manually identifying artifact units (R2Q3)\n- We have clarified the method for identifying causal sets of units described in equations 5 and 6 (R3Q8,9,10)\n- We have clarified the definition of SWD and FID and added citations (R3Q11)"}, "signatures": ["ICLR.cc/2019/Conference/Paper1371/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1371/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1371/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GAN Dissection: Visualizing and Understanding Generative Adversarial Networks", "abstract": "Generative Adversarial Networks (GANs) have recently achieved impressive results for many real-world applications, and many GAN variants have emerged with improvements in sample quality and training stability. However, visualization and understanding of GANs is largely missing. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models.\n\nIn this work, we present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to object concepts with a segmentation-based network dissection method. Then, we quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. Finally, we examine the contextual relationship between these units and their surrounding by inserting the discovered object concepts into new images. We show several practical applications enabled by our framework, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing artifact-causing units, to interactively manipulating objects in the scene. We provide open source interpretation tools to help peer researchers and practitioners better understand their GAN models.", "paperhash": "bau|gan_dissection_visualizing_and_understanding_generative_adversarial_networks", "TL;DR": "GAN representations are examined in detail, and sets of representation units are found that control the generation of semantic concepts in the output.", "authorids": ["davidbau@csail.mit.edu", "junyanz@csail.mit.edu", "hendrik.strobelt@ibm.com", "bzhou@csail.mit.edu", "jbt@csail.mit.edu", "billf@csail.mit.edu", "torralba@csail.mit.edu"], "authors": ["David Bau", "Jun-Yan Zhu", "Hendrik Strobelt", "Bolei Zhou", "Joshua B. Tenenbaum", "William T. Freeman", "Antonio Torralba"], "keywords": ["GANs", "representation", "interpretability", "causality"], "pdf": "/pdf/850f5beb9fbbedefecd6a9a8753e693cc9a0aa37.pdf", "_bibtex": "@inproceedings{\nbau2018visualizing,\ntitle={Visualizing and Understanding Generative Adversarial Networks},\nauthor={David Bau and Jun-Yan Zhu and Hendrik Strobelt and Bolei Zhou and Joshua B. Tenenbaum and William T. Freeman and Antonio Torralba},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hyg_X2C5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1371/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610073, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hyg_X2C5FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1371/Authors", "ICLR.cc/2019/Conference/Paper1371/Reviewers", "ICLR.cc/2019/Conference/Paper1371/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1371/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1371/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1371/Authors|ICLR.cc/2019/Conference/Paper1371/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1371/Reviewers", "ICLR.cc/2019/Conference/Paper1371/Authors", "ICLR.cc/2019/Conference/Paper1371/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610073}}}, {"id": "rylRgFDnnQ", "original": null, "number": 3, "cdate": 1541335285662, "ddate": null, "tcdate": 1541335285662, "tmdate": 1541533005090, "tddate": null, "forum": "Hyg_X2C5FX", "replyto": "Hyg_X2C5FX", "invitation": "ICLR.cc/2019/Conference/-/Paper1371/Official_Review", "content": {"title": "New methods for interpreting GANs, with nice practical contribution for improving GANs outputs.", "review": "The paper proposes a method for visualizing and understanding GANs representation. This seems an important topic as several such methods were performed for networks trained in supervised learning, which relate\nto the predicted outcome, but there is lack of methods for interpreting GANs which are learned in an unsupervised manner and it is generally unclear what is the representation learned by GANs. \nThe method is finding correlations between the appearance of objects and the activation of units in each layer of the learned network. \nIn addition, the paper presents a 'causal' measure, where a causal effect of a unit is measured by removing and adding this unit from/to the network and computing the average effect on object appearance.\nThe authors demonstrate how the methods are applied by improving the appearance of images, by modifying units which were detected as important for specific objects. \nThe authors also provide an interactive interface where users can manually examine and modify their trained GANs in order to add/remove objects and to remove artifacts. \n\nThe method proposed by the authors seem to be appropriate for convolutional neural networks, where 'units' in each layer may correspond to objects and can be searched for in particular locations of image. \nIt is not clear to me if and how one can apply the author's methods to other architecture, and to other application domains (besides images), or whether the method is limited to vision applications. \nThe authors do not explain specifically how do they choose the 'units' for which they seek interpretation when reporting their results. It is written that each layer is divided into two sets: \nu  and u-bar, where we seek interpretation of u. But how large does u tend to be? how would one choose it? is it one filter out of all filters in a certain layer? when optimizing for sets of units together\n(using the alpha probabilities and the optimization in eq. 6) what is d? is it performed for all units in a single layer? more details would be useful here. \n\nThe paper is overall clearly written, with lots of visual examples demonstrating the methods presented in it. \nThe paper presents a new methodological idea, which allows for nice practical contribution. There is no theoretical contribution or any deep analysis. \nThere is no reference in the paper to the supp. info. figures and therefore it is not clear if and how the supp. info. adds valuable information to the reader. \nThe authors use scores like SWD and FIT for performance, but give no explanations for what do these scores measure. \n\n\nMinor: \n\nAbstract: immprovements -> improvements \n\nPage 6, middle: 'train on four LSUN' -> 'trained on four LSUN'\n\nPage 7, bottom: Fig. 14a and 14b should be Fig. 8a and 8b\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1371/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GAN Dissection: Visualizing and Understanding Generative Adversarial Networks", "abstract": "Generative Adversarial Networks (GANs) have recently achieved impressive results for many real-world applications, and many GAN variants have emerged with improvements in sample quality and training stability. However, visualization and understanding of GANs is largely missing. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models.\n\nIn this work, we present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to object concepts with a segmentation-based network dissection method. Then, we quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. Finally, we examine the contextual relationship between these units and their surrounding by inserting the discovered object concepts into new images. We show several practical applications enabled by our framework, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing artifact-causing units, to interactively manipulating objects in the scene. We provide open source interpretation tools to help peer researchers and practitioners better understand their GAN models.", "paperhash": "bau|gan_dissection_visualizing_and_understanding_generative_adversarial_networks", "TL;DR": "GAN representations are examined in detail, and sets of representation units are found that control the generation of semantic concepts in the output.", "authorids": ["davidbau@csail.mit.edu", "junyanz@csail.mit.edu", "hendrik.strobelt@ibm.com", "bzhou@csail.mit.edu", "jbt@csail.mit.edu", "billf@csail.mit.edu", "torralba@csail.mit.edu"], "authors": ["David Bau", "Jun-Yan Zhu", "Hendrik Strobelt", "Bolei Zhou", "Joshua B. Tenenbaum", "William T. Freeman", "Antonio Torralba"], "keywords": ["GANs", "representation", "interpretability", "causality"], "pdf": "/pdf/850f5beb9fbbedefecd6a9a8753e693cc9a0aa37.pdf", "_bibtex": "@inproceedings{\nbau2018visualizing,\ntitle={Visualizing and Understanding Generative Adversarial Networks},\nauthor={David Bau and Jun-Yan Zhu and Hendrik Strobelt and Bolei Zhou and Joshua B. Tenenbaum and William T. Freeman and Antonio Torralba},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hyg_X2C5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1371/Official_Review", "cdate": 1542234196015, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hyg_X2C5FX", "replyto": "Hyg_X2C5FX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1371/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335980337, "tmdate": 1552335980337, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1371/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Bklj6-einQ", "original": null, "number": 2, "cdate": 1541239234955, "ddate": null, "tcdate": 1541239234955, "tmdate": 1541533004876, "tddate": null, "forum": "Hyg_X2C5FX", "replyto": "Hyg_X2C5FX", "invitation": "ICLR.cc/2019/Conference/-/Paper1371/Official_Review", "content": {"title": "An interesting idea to visualize and explain the representation of GANs and to provide a new potential way to further improve the quality of the generated images by GANs", "review": "## Summary\nThis work proposes a novel analytic framework exploited on a semantic segmentation model to visualize GANs at unit (feature map) level. The authors show that some GAN representations can be interpreted, correlate with the parsing result from the semantic segmentation model but as variables that have a causal effect on the synthesis of semantic objects in the output. This framework could allow to detect and remove the artifacts to improve the quality of the generated images.\n\nThe paper is well-written and organized. The dissection and intervention for finding relationships between representation units and objects are simple, straightforward and meaningful. The visualizations are convincing and insightful. I recommend to accept the paper.\n\n## Detail comments\nAbout diagnosing and improving GANs, please give more details of the human annotation for the artifacts.\n\nI think there is a typo in the first and second paragraphs in section 4.3, Figure 14 -> Figure 8.  \n\nThe whole framework is based on a semantic segmentation model. The model is highly possibly imperfect and could have very different performances on different objects. Have you ever considerate to handle these imperfect models?\n\nIs there a way to apply the framework to the training process of GANs?\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1371/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GAN Dissection: Visualizing and Understanding Generative Adversarial Networks", "abstract": "Generative Adversarial Networks (GANs) have recently achieved impressive results for many real-world applications, and many GAN variants have emerged with improvements in sample quality and training stability. However, visualization and understanding of GANs is largely missing. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models.\n\nIn this work, we present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to object concepts with a segmentation-based network dissection method. Then, we quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. Finally, we examine the contextual relationship between these units and their surrounding by inserting the discovered object concepts into new images. We show several practical applications enabled by our framework, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing artifact-causing units, to interactively manipulating objects in the scene. We provide open source interpretation tools to help peer researchers and practitioners better understand their GAN models.", "paperhash": "bau|gan_dissection_visualizing_and_understanding_generative_adversarial_networks", "TL;DR": "GAN representations are examined in detail, and sets of representation units are found that control the generation of semantic concepts in the output.", "authorids": ["davidbau@csail.mit.edu", "junyanz@csail.mit.edu", "hendrik.strobelt@ibm.com", "bzhou@csail.mit.edu", "jbt@csail.mit.edu", "billf@csail.mit.edu", "torralba@csail.mit.edu"], "authors": ["David Bau", "Jun-Yan Zhu", "Hendrik Strobelt", "Bolei Zhou", "Joshua B. Tenenbaum", "William T. Freeman", "Antonio Torralba"], "keywords": ["GANs", "representation", "interpretability", "causality"], "pdf": "/pdf/850f5beb9fbbedefecd6a9a8753e693cc9a0aa37.pdf", "_bibtex": "@inproceedings{\nbau2018visualizing,\ntitle={Visualizing and Understanding Generative Adversarial Networks},\nauthor={David Bau and Jun-Yan Zhu and Hendrik Strobelt and Bolei Zhou and Joshua B. Tenenbaum and William T. Freeman and Antonio Torralba},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hyg_X2C5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1371/Official_Review", "cdate": 1542234196015, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hyg_X2C5FX", "replyto": "Hyg_X2C5FX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1371/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335980337, "tmdate": 1552335980337, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1371/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "H1lQioJchm", "original": null, "number": 1, "cdate": 1541172123475, "ddate": null, "tcdate": 1541172123475, "tmdate": 1541533004666, "tddate": null, "forum": "Hyg_X2C5FX", "replyto": "Hyg_X2C5FX", "invitation": "ICLR.cc/2019/Conference/-/Paper1371/Official_Review", "content": {"title": "This paper reveals the essence of GAN through experiments.", "review": "This paper provides a visualization framework to understand the generative neural network in GAN models. To achieve this, they first find a group of interpretable units and then quantify the causal effect of interpretable units. Finally, the contextual relationship between these units and their surrounding is examined by inserting the discovered object concepts into new images. Extensive experiments are presented and a video is provided.\n\nOverall, I think this paper is very valuable and well-written. The experiments clearly show the questions proposed in the introduction are answered. Two concerns are as follows.\n\nCons:\n1) The visualization seems to be very heuristic. What I want to know is the theoretical interpretation of the visualization. For example, the Class Activation Maps (CAM) can be directly calculated by the output values of softmax function. How about the visual class for the generative neural networks?\n2) I am also very curious, how is the rate of finding the correct sets of units for a particular visual class?\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1371/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GAN Dissection: Visualizing and Understanding Generative Adversarial Networks", "abstract": "Generative Adversarial Networks (GANs) have recently achieved impressive results for many real-world applications, and many GAN variants have emerged with improvements in sample quality and training stability. However, visualization and understanding of GANs is largely missing. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models.\n\nIn this work, we present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to object concepts with a segmentation-based network dissection method. Then, we quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. Finally, we examine the contextual relationship between these units and their surrounding by inserting the discovered object concepts into new images. We show several practical applications enabled by our framework, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing artifact-causing units, to interactively manipulating objects in the scene. We provide open source interpretation tools to help peer researchers and practitioners better understand their GAN models.", "paperhash": "bau|gan_dissection_visualizing_and_understanding_generative_adversarial_networks", "TL;DR": "GAN representations are examined in detail, and sets of representation units are found that control the generation of semantic concepts in the output.", "authorids": ["davidbau@csail.mit.edu", "junyanz@csail.mit.edu", "hendrik.strobelt@ibm.com", "bzhou@csail.mit.edu", "jbt@csail.mit.edu", "billf@csail.mit.edu", "torralba@csail.mit.edu"], "authors": ["David Bau", "Jun-Yan Zhu", "Hendrik Strobelt", "Bolei Zhou", "Joshua B. Tenenbaum", "William T. Freeman", "Antonio Torralba"], "keywords": ["GANs", "representation", "interpretability", "causality"], "pdf": "/pdf/850f5beb9fbbedefecd6a9a8753e693cc9a0aa37.pdf", "_bibtex": "@inproceedings{\nbau2018visualizing,\ntitle={Visualizing and Understanding Generative Adversarial Networks},\nauthor={David Bau and Jun-Yan Zhu and Hendrik Strobelt and Bolei Zhou and Joshua B. Tenenbaum and William T. Freeman and Antonio Torralba},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hyg_X2C5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1371/Official_Review", "cdate": 1542234196015, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hyg_X2C5FX", "replyto": "Hyg_X2C5FX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1371/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335980337, "tmdate": 1552335980337, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1371/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 9}