{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1490164752918, "tcdate": 1478287687953, "number": 360, "id": "Hkg8bDqee", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "Hkg8bDqee", "signatures": ["~Aahitagni_Mukherjee1"], "readers": ["everyone"], "content": {"title": "Introspection:Accelerating Neural Network Training By Learning Weight Evolution", "abstract": "Neural Networks are function approximators that have achieved state-of-the-art accuracy in numerous machine learning tasks. In spite of their great success in terms of accuracy, their large training time makes it difficult to use them for various tasks. In this paper, we explore the idea of learning weight evolution pattern from a simple network for accelerating training of novel neural networks.\n\nWe use a neural network to learn the training pattern from MNIST classification and utilize it to accelerate training of neural networks used for CIFAR-10 and ImageNet classification. Our method has a low memory footprint and is computationally efficient. This method can also be used with other optimizers to give faster convergence. The results indicate a general trend in the weight evolution during training of neural networks.", "pdf": "/pdf/f5316305b0560db063525a71f36ca95d1932981e.pdf", "TL;DR": "Acceleration of training by performing weight updates, using knowledge obtained from training other neural networks.", "paperhash": "sinha|introspectionaccelerating_neural_network_training_by_learning_weight_evolution", "keywords": ["Computer vision", "Deep learning", "Optimization"], "conflicts": ["iitk.ac.in", "iitkgp.ac.in", "adobe.com"], "authors": ["Abhishek Sinha", "Aahitagni Mukherjee", "Mausoom Sarkar", "Balaji Krishnamurthy"], "authorids": ["abhishek.sinha94@gmail.com", "ahitagnimukherjeeam@gmail.com", "msarkar@adobe.com", "kbalaji@adobe.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 23, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396538671, "tcdate": 1486396538671, "number": 1, "id": "HJQt3MI_e", "invitation": "ICLR.cc/2017/conference/-/paper360/acceptance", "forum": "Hkg8bDqee", "replyto": "Hkg8bDqee", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "Interesting paper and clear accept. Not recommended for an oral presentation because of weaknesses in the empirical contribution that make the significance of the results unclear.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introspection:Accelerating Neural Network Training By Learning Weight Evolution", "abstract": "Neural Networks are function approximators that have achieved state-of-the-art accuracy in numerous machine learning tasks. In spite of their great success in terms of accuracy, their large training time makes it difficult to use them for various tasks. In this paper, we explore the idea of learning weight evolution pattern from a simple network for accelerating training of novel neural networks.\n\nWe use a neural network to learn the training pattern from MNIST classification and utilize it to accelerate training of neural networks used for CIFAR-10 and ImageNet classification. Our method has a low memory footprint and is computationally efficient. This method can also be used with other optimizers to give faster convergence. The results indicate a general trend in the weight evolution during training of neural networks.", "pdf": "/pdf/f5316305b0560db063525a71f36ca95d1932981e.pdf", "TL;DR": "Acceleration of training by performing weight updates, using knowledge obtained from training other neural networks.", "paperhash": "sinha|introspectionaccelerating_neural_network_training_by_learning_weight_evolution", "keywords": ["Computer vision", "Deep learning", "Optimization"], "conflicts": ["iitk.ac.in", "iitkgp.ac.in", "adobe.com"], "authors": ["Abhishek Sinha", "Aahitagni Mukherjee", "Mausoom Sarkar", "Balaji Krishnamurthy"], "authorids": ["abhishek.sinha94@gmail.com", "ahitagnimukherjeeam@gmail.com", "msarkar@adobe.com", "kbalaji@adobe.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396539180, "id": "ICLR.cc/2017/conference/-/paper360/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "Hkg8bDqee", "replyto": "Hkg8bDqee", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396539180}}}, {"tddate": null, "tmdate": 1486346214697, "tcdate": 1486346214697, "number": 14, "id": "rkJedIrOl", "invitation": "ICLR.cc/2017/conference/-/paper360/public/comment", "forum": "Hkg8bDqee", "replyto": "Hkg8bDqee", "signatures": ["~Balaji_Krishnamurthy1"], "readers": ["everyone"], "writers": ["~Balaji_Krishnamurthy1"], "content": {"title": "Updates on Inception V1 Results", "comment": "We have revised the paper with updated results on experiments with Inception V1 network, which continues to show promise."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introspection:Accelerating Neural Network Training By Learning Weight Evolution", "abstract": "Neural Networks are function approximators that have achieved state-of-the-art accuracy in numerous machine learning tasks. In spite of their great success in terms of accuracy, their large training time makes it difficult to use them for various tasks. In this paper, we explore the idea of learning weight evolution pattern from a simple network for accelerating training of novel neural networks.\n\nWe use a neural network to learn the training pattern from MNIST classification and utilize it to accelerate training of neural networks used for CIFAR-10 and ImageNet classification. Our method has a low memory footprint and is computationally efficient. This method can also be used with other optimizers to give faster convergence. The results indicate a general trend in the weight evolution during training of neural networks.", "pdf": "/pdf/f5316305b0560db063525a71f36ca95d1932981e.pdf", "TL;DR": "Acceleration of training by performing weight updates, using knowledge obtained from training other neural networks.", "paperhash": "sinha|introspectionaccelerating_neural_network_training_by_learning_weight_evolution", "keywords": ["Computer vision", "Deep learning", "Optimization"], "conflicts": ["iitk.ac.in", "iitkgp.ac.in", "adobe.com"], "authors": ["Abhishek Sinha", "Aahitagni Mukherjee", "Mausoom Sarkar", "Balaji Krishnamurthy"], "authorids": ["abhishek.sinha94@gmail.com", "ahitagnimukherjeeam@gmail.com", "msarkar@adobe.com", "kbalaji@adobe.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287608850, "id": "ICLR.cc/2017/conference/-/paper360/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hkg8bDqee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper360/reviewers", "ICLR.cc/2017/conference/paper360/areachairs"], "cdate": 1485287608850}}}, {"tddate": null, "tmdate": 1485278934769, "tcdate": 1485278934769, "number": 13, "id": "BJkJyfBPx", "invitation": "ICLR.cc/2017/conference/-/paper360/public/comment", "forum": "Hkg8bDqee", "replyto": "SktFDJDNl", "signatures": ["~Mausoom_Sarkar1"], "readers": ["everyone"], "writers": ["~Mausoom_Sarkar1"], "content": {"title": "Thanks", "comment": "Thanks for updating the score."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introspection:Accelerating Neural Network Training By Learning Weight Evolution", "abstract": "Neural Networks are function approximators that have achieved state-of-the-art accuracy in numerous machine learning tasks. In spite of their great success in terms of accuracy, their large training time makes it difficult to use them for various tasks. In this paper, we explore the idea of learning weight evolution pattern from a simple network for accelerating training of novel neural networks.\n\nWe use a neural network to learn the training pattern from MNIST classification and utilize it to accelerate training of neural networks used for CIFAR-10 and ImageNet classification. Our method has a low memory footprint and is computationally efficient. This method can also be used with other optimizers to give faster convergence. The results indicate a general trend in the weight evolution during training of neural networks.", "pdf": "/pdf/f5316305b0560db063525a71f36ca95d1932981e.pdf", "TL;DR": "Acceleration of training by performing weight updates, using knowledge obtained from training other neural networks.", "paperhash": "sinha|introspectionaccelerating_neural_network_training_by_learning_weight_evolution", "keywords": ["Computer vision", "Deep learning", "Optimization"], "conflicts": ["iitk.ac.in", "iitkgp.ac.in", "adobe.com"], "authors": ["Abhishek Sinha", "Aahitagni Mukherjee", "Mausoom Sarkar", "Balaji Krishnamurthy"], "authorids": ["abhishek.sinha94@gmail.com", "ahitagnimukherjeeam@gmail.com", "msarkar@adobe.com", "kbalaji@adobe.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287608850, "id": "ICLR.cc/2017/conference/-/paper360/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hkg8bDqee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper360/reviewers", "ICLR.cc/2017/conference/paper360/areachairs"], "cdate": 1485287608850}}}, {"tddate": null, "tmdate": 1485278916480, "tcdate": 1485278916480, "number": 12, "id": "S12TAbSDe", "invitation": "ICLR.cc/2017/conference/-/paper360/public/comment", "forum": "Hkg8bDqee", "replyto": "B1F8hRVEg", "signatures": ["~Mausoom_Sarkar1"], "readers": ["everyone"], "writers": ["~Mausoom_Sarkar1"], "content": {"title": "Thanks", "comment": "Thanks for updating the score."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introspection:Accelerating Neural Network Training By Learning Weight Evolution", "abstract": "Neural Networks are function approximators that have achieved state-of-the-art accuracy in numerous machine learning tasks. In spite of their great success in terms of accuracy, their large training time makes it difficult to use them for various tasks. In this paper, we explore the idea of learning weight evolution pattern from a simple network for accelerating training of novel neural networks.\n\nWe use a neural network to learn the training pattern from MNIST classification and utilize it to accelerate training of neural networks used for CIFAR-10 and ImageNet classification. Our method has a low memory footprint and is computationally efficient. This method can also be used with other optimizers to give faster convergence. The results indicate a general trend in the weight evolution during training of neural networks.", "pdf": "/pdf/f5316305b0560db063525a71f36ca95d1932981e.pdf", "TL;DR": "Acceleration of training by performing weight updates, using knowledge obtained from training other neural networks.", "paperhash": "sinha|introspectionaccelerating_neural_network_training_by_learning_weight_evolution", "keywords": ["Computer vision", "Deep learning", "Optimization"], "conflicts": ["iitk.ac.in", "iitkgp.ac.in", "adobe.com"], "authors": ["Abhishek Sinha", "Aahitagni Mukherjee", "Mausoom Sarkar", "Balaji Krishnamurthy"], "authorids": ["abhishek.sinha94@gmail.com", "ahitagnimukherjeeam@gmail.com", "msarkar@adobe.com", "kbalaji@adobe.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287608850, "id": "ICLR.cc/2017/conference/-/paper360/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hkg8bDqee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper360/reviewers", "ICLR.cc/2017/conference/paper360/areachairs"], "cdate": 1485287608850}}}, {"tddate": null, "tmdate": 1485260112498, "tcdate": 1485260112498, "number": 11, "id": "SkuLHTNvg", "invitation": "ICLR.cc/2017/conference/-/paper360/public/comment", "forum": "Hkg8bDqee", "replyto": "SyKNZokDe", "signatures": ["~Mausoom_Sarkar1"], "readers": ["everyone"], "writers": ["~Mausoom_Sarkar1"], "content": {"title": "Thanks", "comment": "Your constructive comments helped a lot and thanks for updating the score. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introspection:Accelerating Neural Network Training By Learning Weight Evolution", "abstract": "Neural Networks are function approximators that have achieved state-of-the-art accuracy in numerous machine learning tasks. In spite of their great success in terms of accuracy, their large training time makes it difficult to use them for various tasks. In this paper, we explore the idea of learning weight evolution pattern from a simple network for accelerating training of novel neural networks.\n\nWe use a neural network to learn the training pattern from MNIST classification and utilize it to accelerate training of neural networks used for CIFAR-10 and ImageNet classification. Our method has a low memory footprint and is computationally efficient. This method can also be used with other optimizers to give faster convergence. The results indicate a general trend in the weight evolution during training of neural networks.", "pdf": "/pdf/f5316305b0560db063525a71f36ca95d1932981e.pdf", "TL;DR": "Acceleration of training by performing weight updates, using knowledge obtained from training other neural networks.", "paperhash": "sinha|introspectionaccelerating_neural_network_training_by_learning_weight_evolution", "keywords": ["Computer vision", "Deep learning", "Optimization"], "conflicts": ["iitk.ac.in", "iitkgp.ac.in", "adobe.com"], "authors": ["Abhishek Sinha", "Aahitagni Mukherjee", "Mausoom Sarkar", "Balaji Krishnamurthy"], "authorids": ["abhishek.sinha94@gmail.com", "ahitagnimukherjeeam@gmail.com", "msarkar@adobe.com", "kbalaji@adobe.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287608850, "id": "ICLR.cc/2017/conference/-/paper360/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hkg8bDqee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper360/reviewers", "ICLR.cc/2017/conference/paper360/areachairs"], "cdate": 1485287608850}}}, {"tddate": null, "tmdate": 1485248158761, "tcdate": 1482120272640, "number": 2, "id": "B1F8hRVEg", "invitation": "ICLR.cc/2017/conference/-/paper360/official/review", "forum": "Hkg8bDqee", "replyto": "Hkg8bDqee", "signatures": ["ICLR.cc/2017/conference/paper360/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper360/AnonReviewer3"], "content": {"title": "novel idea but requires more details / experimentation", "rating": "8: Top 50% of accepted papers, clear accept", "review": "The paper reads well and the idea is new.\nSadly, many details needed for replicating the results (such as layer sizes of the CNNs, learning rates) are missing. \nThe training of the introspection network could have been described in more detail. \nAlso, I think that a model, which is closer to the current state-of-the-art should have been used in the ImageNet experiments. That would have made the results more convincing.\nDue to the novelty of the idea, I recommend the paper. I would increase the rating if an updated draft addresses the mentioned issues.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introspection:Accelerating Neural Network Training By Learning Weight Evolution", "abstract": "Neural Networks are function approximators that have achieved state-of-the-art accuracy in numerous machine learning tasks. In spite of their great success in terms of accuracy, their large training time makes it difficult to use them for various tasks. In this paper, we explore the idea of learning weight evolution pattern from a simple network for accelerating training of novel neural networks.\n\nWe use a neural network to learn the training pattern from MNIST classification and utilize it to accelerate training of neural networks used for CIFAR-10 and ImageNet classification. Our method has a low memory footprint and is computationally efficient. This method can also be used with other optimizers to give faster convergence. The results indicate a general trend in the weight evolution during training of neural networks.", "pdf": "/pdf/f5316305b0560db063525a71f36ca95d1932981e.pdf", "TL;DR": "Acceleration of training by performing weight updates, using knowledge obtained from training other neural networks.", "paperhash": "sinha|introspectionaccelerating_neural_network_training_by_learning_weight_evolution", "keywords": ["Computer vision", "Deep learning", "Optimization"], "conflicts": ["iitk.ac.in", "iitkgp.ac.in", "adobe.com"], "authors": ["Abhishek Sinha", "Aahitagni Mukherjee", "Mausoom Sarkar", "Balaji Krishnamurthy"], "authorids": ["abhishek.sinha94@gmail.com", "ahitagnimukherjeeam@gmail.com", "msarkar@adobe.com", "kbalaji@adobe.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512612581, "id": "ICLR.cc/2017/conference/-/paper360/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper360/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper360/AnonReviewer2", "ICLR.cc/2017/conference/paper360/AnonReviewer3", "ICLR.cc/2017/conference/paper360/AnonReviewer1"], "reply": {"forum": "Hkg8bDqee", "replyto": "Hkg8bDqee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper360/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper360/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512612581}}}, {"tddate": null, "tmdate": 1485214896998, "tcdate": 1482254208934, "number": 3, "id": "SktFDJDNl", "invitation": "ICLR.cc/2017/conference/-/paper360/official/review", "forum": "Hkg8bDqee", "replyto": "Hkg8bDqee", "signatures": ["ICLR.cc/2017/conference/paper360/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper360/AnonReviewer1"], "content": {"title": "Review", "rating": "7: Good paper, accept", "review": "In this paper, the authors use a separate introspection neural network to predict the future value of the weights directly from their past history. The introspection network is trained on the parameter progressions collected from training separate set of meta learning models using a typical optimizer, e.g. SGD.  \n\nPros:\n+ The organization is generally very clear\n+ Novel meta-learning approach that is different than the previous learning to learn approach\n\nCons: \n- The paper will benefit from more thorough experiments on other neural network architectures where the geometry of the parameter space are sufficiently different than CNNs such as fully connected and recurrent neural networks.  \n- Neither MNIST nor CIFAR experimental section explained the architectural details\n- Mini-batch size for the experiments were not included in the paper\n- Comparison with different baseline optimizer such as Adam would be a strong addition or at least explain how the hyper-parameters, such as learning rate and momentum, are chosen for the baseline SGD method. \n\nOverall, due to the omission of the experimental details in the current revision, it is hard to draw any conclusive insight about the proposed method. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introspection:Accelerating Neural Network Training By Learning Weight Evolution", "abstract": "Neural Networks are function approximators that have achieved state-of-the-art accuracy in numerous machine learning tasks. In spite of their great success in terms of accuracy, their large training time makes it difficult to use them for various tasks. In this paper, we explore the idea of learning weight evolution pattern from a simple network for accelerating training of novel neural networks.\n\nWe use a neural network to learn the training pattern from MNIST classification and utilize it to accelerate training of neural networks used for CIFAR-10 and ImageNet classification. Our method has a low memory footprint and is computationally efficient. This method can also be used with other optimizers to give faster convergence. The results indicate a general trend in the weight evolution during training of neural networks.", "pdf": "/pdf/f5316305b0560db063525a71f36ca95d1932981e.pdf", "TL;DR": "Acceleration of training by performing weight updates, using knowledge obtained from training other neural networks.", "paperhash": "sinha|introspectionaccelerating_neural_network_training_by_learning_weight_evolution", "keywords": ["Computer vision", "Deep learning", "Optimization"], "conflicts": ["iitk.ac.in", "iitkgp.ac.in", "adobe.com"], "authors": ["Abhishek Sinha", "Aahitagni Mukherjee", "Mausoom Sarkar", "Balaji Krishnamurthy"], "authorids": ["abhishek.sinha94@gmail.com", "ahitagnimukherjeeam@gmail.com", "msarkar@adobe.com", "kbalaji@adobe.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512612581, "id": "ICLR.cc/2017/conference/-/paper360/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper360/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper360/AnonReviewer2", "ICLR.cc/2017/conference/paper360/AnonReviewer3", "ICLR.cc/2017/conference/paper360/AnonReviewer1"], "reply": {"forum": "Hkg8bDqee", "replyto": "Hkg8bDqee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper360/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper360/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512612581}}}, {"tddate": null, "tmdate": 1485214880561, "tcdate": 1485214880561, "number": 6, "id": "H1FoEMNwe", "invitation": "ICLR.cc/2017/conference/-/paper360/official/comment", "forum": "Hkg8bDqee", "replyto": "SJ6y_jUIx", "signatures": ["ICLR.cc/2017/conference/paper360/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper360/AnonReviewer1"], "content": {"title": "improved paper", "comment": "The revised paper demonstrated the effectiveness of the introspection network under different optimizers and addressed most of my concerns. I will increase my score accordingly. It is interesting to see that Adam + introspection provides a substantial gain for the fully connected architecture but not as much for the convolutional architecture. I am wondering if the weight initialization and the performance of the introspection network are correlated. Namely, if the weights of a network are initialized to a wrong scale, the introspection network can easily fix that in its first few jumps. Experiments along these lines could further improve the strength of this paper. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introspection:Accelerating Neural Network Training By Learning Weight Evolution", "abstract": "Neural Networks are function approximators that have achieved state-of-the-art accuracy in numerous machine learning tasks. In spite of their great success in terms of accuracy, their large training time makes it difficult to use them for various tasks. In this paper, we explore the idea of learning weight evolution pattern from a simple network for accelerating training of novel neural networks.\n\nWe use a neural network to learn the training pattern from MNIST classification and utilize it to accelerate training of neural networks used for CIFAR-10 and ImageNet classification. Our method has a low memory footprint and is computationally efficient. This method can also be used with other optimizers to give faster convergence. The results indicate a general trend in the weight evolution during training of neural networks.", "pdf": "/pdf/f5316305b0560db063525a71f36ca95d1932981e.pdf", "TL;DR": "Acceleration of training by performing weight updates, using knowledge obtained from training other neural networks.", "paperhash": "sinha|introspectionaccelerating_neural_network_training_by_learning_weight_evolution", "keywords": ["Computer vision", "Deep learning", "Optimization"], "conflicts": ["iitk.ac.in", "iitkgp.ac.in", "adobe.com"], "authors": ["Abhishek Sinha", "Aahitagni Mukherjee", "Mausoom Sarkar", "Balaji Krishnamurthy"], "authorids": ["abhishek.sinha94@gmail.com", "ahitagnimukherjeeam@gmail.com", "msarkar@adobe.com", "kbalaji@adobe.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287608723, "id": "ICLR.cc/2017/conference/-/paper360/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "Hkg8bDqee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper360/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper360/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper360/reviewers", "ICLR.cc/2017/conference/paper360/areachairs"], "cdate": 1485287608723}}}, {"tddate": null, "tmdate": 1484923185098, "tcdate": 1484923185098, "number": 5, "id": "SyKNZokDe", "invitation": "ICLR.cc/2017/conference/-/paper360/official/comment", "forum": "Hkg8bDqee", "replyto": "ryAUwsILl", "signatures": ["ICLR.cc/2017/conference/paper360/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper360/AnonReviewer2"], "content": {"title": "Updated score", "comment": "The authors have addressed all my comments, and worked hard to improve the paper. In my opinion, the new revision is very interesting and has high potential significance. The authors do not attempt to oversell the idea and present extensive experimental evaluation. I have updated my score to reflect my satisfaction with the updated version."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introspection:Accelerating Neural Network Training By Learning Weight Evolution", "abstract": "Neural Networks are function approximators that have achieved state-of-the-art accuracy in numerous machine learning tasks. In spite of their great success in terms of accuracy, their large training time makes it difficult to use them for various tasks. In this paper, we explore the idea of learning weight evolution pattern from a simple network for accelerating training of novel neural networks.\n\nWe use a neural network to learn the training pattern from MNIST classification and utilize it to accelerate training of neural networks used for CIFAR-10 and ImageNet classification. Our method has a low memory footprint and is computationally efficient. This method can also be used with other optimizers to give faster convergence. The results indicate a general trend in the weight evolution during training of neural networks.", "pdf": "/pdf/f5316305b0560db063525a71f36ca95d1932981e.pdf", "TL;DR": "Acceleration of training by performing weight updates, using knowledge obtained from training other neural networks.", "paperhash": "sinha|introspectionaccelerating_neural_network_training_by_learning_weight_evolution", "keywords": ["Computer vision", "Deep learning", "Optimization"], "conflicts": ["iitk.ac.in", "iitkgp.ac.in", "adobe.com"], "authors": ["Abhishek Sinha", "Aahitagni Mukherjee", "Mausoom Sarkar", "Balaji Krishnamurthy"], "authorids": ["abhishek.sinha94@gmail.com", "ahitagnimukherjeeam@gmail.com", "msarkar@adobe.com", "kbalaji@adobe.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287608723, "id": "ICLR.cc/2017/conference/-/paper360/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "Hkg8bDqee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper360/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper360/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper360/reviewers", "ICLR.cc/2017/conference/paper360/areachairs"], "cdate": 1485287608723}}}, {"tddate": null, "tmdate": 1484922727184, "tcdate": 1481984107391, "number": 1, "id": "Bkm_OazEx", "invitation": "ICLR.cc/2017/conference/-/paper360/official/review", "forum": "Hkg8bDqee", "replyto": "Hkg8bDqee", "signatures": ["ICLR.cc/2017/conference/paper360/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper360/AnonReviewer2"], "content": {"title": "Valuable insight but needs careful analysis", "rating": "9: Top 15% of accepted papers, strong accept", "review": "EDIT: Updated score. See additional comment.\n\nI quite like the main idea of the paper, which is based on the observation in Sec. 3.0 - that the authors find many predictable patterns in the independent evolution of weights during neural network training. It is very encouraging that a simple neural network can be used to speed up training by directly predicting weights.\n\nHowever the technical quality of the current paper leaves much to be desired, and I encourage the authors to do more rigorous analysis of the approach. Here are some concrete suggestions:\n\n- The findings in Section 3.0 which motivate the approach, should be clearly presented in the paper. Presently they are stated as anecdotes.\n\n- A central issue with the paper is that the training of the Introspection network I is completely glossed over. How well did the training work, in terms of training, validation/test losses? How well does it need to work in order to be useful for speeding up training? These are important questions for anyone interested in this approach.\n\n- An additional important issue is that of baselines. Would a simple linear/quadratic model also work instead of a neural network? What about a simple heuristic rule to increase/decrease weights? I think it's important to compare to such baselines to understand the complexity of the weight evolution learned by the neural network.\n\n- I do not think that default tensorflow example hyperparameters should be used, as mentioned by authors on OpenReview. There is no scientific basis for using them. Instead, first hyperparameters which produce good results in a reasonable time should be selected as the baseline, and then added the benefit of the introspection network to speed up training (and reaching a similar result) should be shown.\n\n- The authors state in the discussion on OpenReview that they also tried RNNs as the introspection network but it didn't work with small state size. What does \"didn't work\" mean in this context? Did it underfit? I find it hard to imagine that a large state size would be required for this task. Even if it is, that doesn't rule out evaluation due to memory issues because the RNN can be run on the weights in 'mini-batch' mode. In general, I think other baselines are more important than RNN.\n\n- A question about jump points: \nThe I is trained on SGD trajectories. While using I to speed up training at several jump points, if the input weights cross previous jump points, then I gets input data from a weight evolution which is not from SGD (it has been altered by I). This seems problematic but doesn't seem to affect your experiments. I feel that this again highlights the importance of the baselines. Perhaps I is doing something extremely simple that is not affected by this issue.\n\nSince the main idea is very interesting, I will be happy to update my score if the above concerns are addressed. ", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introspection:Accelerating Neural Network Training By Learning Weight Evolution", "abstract": "Neural Networks are function approximators that have achieved state-of-the-art accuracy in numerous machine learning tasks. In spite of their great success in terms of accuracy, their large training time makes it difficult to use them for various tasks. In this paper, we explore the idea of learning weight evolution pattern from a simple network for accelerating training of novel neural networks.\n\nWe use a neural network to learn the training pattern from MNIST classification and utilize it to accelerate training of neural networks used for CIFAR-10 and ImageNet classification. Our method has a low memory footprint and is computationally efficient. This method can also be used with other optimizers to give faster convergence. The results indicate a general trend in the weight evolution during training of neural networks.", "pdf": "/pdf/f5316305b0560db063525a71f36ca95d1932981e.pdf", "TL;DR": "Acceleration of training by performing weight updates, using knowledge obtained from training other neural networks.", "paperhash": "sinha|introspectionaccelerating_neural_network_training_by_learning_weight_evolution", "keywords": ["Computer vision", "Deep learning", "Optimization"], "conflicts": ["iitk.ac.in", "iitkgp.ac.in", "adobe.com"], "authors": ["Abhishek Sinha", "Aahitagni Mukherjee", "Mausoom Sarkar", "Balaji Krishnamurthy"], "authorids": ["abhishek.sinha94@gmail.com", "ahitagnimukherjeeam@gmail.com", "msarkar@adobe.com", "kbalaji@adobe.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512612581, "id": "ICLR.cc/2017/conference/-/paper360/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper360/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper360/AnonReviewer2", "ICLR.cc/2017/conference/paper360/AnonReviewer3", "ICLR.cc/2017/conference/paper360/AnonReviewer1"], "reply": {"forum": "Hkg8bDqee", "replyto": "Hkg8bDqee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper360/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper360/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512612581}}}, {"tddate": null, "tmdate": 1484851850797, "tcdate": 1484851850797, "number": 10, "id": "HJmq9tCIx", "invitation": "ICLR.cc/2017/conference/-/paper360/public/comment", "forum": "Hkg8bDqee", "replyto": "SktFDJDNl", "signatures": ["~Mausoom_Sarkar1"], "readers": ["everyone"], "writers": ["~Mausoom_Sarkar1"], "content": {"title": "Inception Result", "comment": "We have now updated our paper with some early results on Inception V1 running on Imagenet classification task. We have put these results in the Appendix section and are going to update the results when we have more data from training. We are expecting the training to complete in a week or two. Looking at the initial trends we think that this trend will continue as per our earlier observations with other networks. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introspection:Accelerating Neural Network Training By Learning Weight Evolution", "abstract": "Neural Networks are function approximators that have achieved state-of-the-art accuracy in numerous machine learning tasks. In spite of their great success in terms of accuracy, their large training time makes it difficult to use them for various tasks. In this paper, we explore the idea of learning weight evolution pattern from a simple network for accelerating training of novel neural networks.\n\nWe use a neural network to learn the training pattern from MNIST classification and utilize it to accelerate training of neural networks used for CIFAR-10 and ImageNet classification. Our method has a low memory footprint and is computationally efficient. This method can also be used with other optimizers to give faster convergence. The results indicate a general trend in the weight evolution during training of neural networks.", "pdf": "/pdf/f5316305b0560db063525a71f36ca95d1932981e.pdf", "TL;DR": "Acceleration of training by performing weight updates, using knowledge obtained from training other neural networks.", "paperhash": "sinha|introspectionaccelerating_neural_network_training_by_learning_weight_evolution", "keywords": ["Computer vision", "Deep learning", "Optimization"], "conflicts": ["iitk.ac.in", "iitkgp.ac.in", "adobe.com"], "authors": ["Abhishek Sinha", "Aahitagni Mukherjee", "Mausoom Sarkar", "Balaji Krishnamurthy"], "authorids": ["abhishek.sinha94@gmail.com", "ahitagnimukherjeeam@gmail.com", "msarkar@adobe.com", "kbalaji@adobe.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287608850, "id": "ICLR.cc/2017/conference/-/paper360/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hkg8bDqee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper360/reviewers", "ICLR.cc/2017/conference/paper360/areachairs"], "cdate": 1485287608850}}}, {"tddate": null, "tmdate": 1484851816651, "tcdate": 1484851816651, "number": 9, "id": "rJbdcYAIe", "invitation": "ICLR.cc/2017/conference/-/paper360/public/comment", "forum": "Hkg8bDqee", "replyto": "Bkm_OazEx", "signatures": ["~Mausoom_Sarkar1"], "readers": ["everyone"], "writers": ["~Mausoom_Sarkar1"], "content": {"title": "Inception result", "comment": "We have now updated our paper with some early results on Inception V1 running on Imagenet classification task. We have put these results in the Appendix section and we are going to update the results when we have more data from training. We are expecting the training to complete in a week or two. Looking at the initial trends we think that this trend will continue as per our earlier observations with other networks. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introspection:Accelerating Neural Network Training By Learning Weight Evolution", "abstract": "Neural Networks are function approximators that have achieved state-of-the-art accuracy in numerous machine learning tasks. In spite of their great success in terms of accuracy, their large training time makes it difficult to use them for various tasks. In this paper, we explore the idea of learning weight evolution pattern from a simple network for accelerating training of novel neural networks.\n\nWe use a neural network to learn the training pattern from MNIST classification and utilize it to accelerate training of neural networks used for CIFAR-10 and ImageNet classification. Our method has a low memory footprint and is computationally efficient. This method can also be used with other optimizers to give faster convergence. The results indicate a general trend in the weight evolution during training of neural networks.", "pdf": "/pdf/f5316305b0560db063525a71f36ca95d1932981e.pdf", "TL;DR": "Acceleration of training by performing weight updates, using knowledge obtained from training other neural networks.", "paperhash": "sinha|introspectionaccelerating_neural_network_training_by_learning_weight_evolution", "keywords": ["Computer vision", "Deep learning", "Optimization"], "conflicts": ["iitk.ac.in", "iitkgp.ac.in", "adobe.com"], "authors": ["Abhishek Sinha", "Aahitagni Mukherjee", "Mausoom Sarkar", "Balaji Krishnamurthy"], "authorids": ["abhishek.sinha94@gmail.com", "ahitagnimukherjeeam@gmail.com", "msarkar@adobe.com", "kbalaji@adobe.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287608850, "id": "ICLR.cc/2017/conference/-/paper360/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hkg8bDqee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper360/reviewers", "ICLR.cc/2017/conference/paper360/areachairs"], "cdate": 1485287608850}}}, {"tddate": null, "tmdate": 1484851658202, "tcdate": 1484851658202, "number": 8, "id": "B1MAKKALl", "invitation": "ICLR.cc/2017/conference/-/paper360/public/comment", "forum": "Hkg8bDqee", "replyto": "HJPiliOIg", "signatures": ["~Mausoom_Sarkar1"], "readers": ["everyone"], "writers": ["~Mausoom_Sarkar1"], "content": {"title": "Inception Results", "comment": "As per your recommendation of trying a network closer to state of art, we have now updated our paper with some early results on Inception V1 running on Imagenet classification task. We have put these results in the Appendix section and are going to update the results when we have more data from training. We are expecting the training to complete in a week or two. Looking at the initial trends we think that this trend will continue as per our earlier observations with other networks. \n\nWe have also reduced the number of pages and will be working to reduce them further.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introspection:Accelerating Neural Network Training By Learning Weight Evolution", "abstract": "Neural Networks are function approximators that have achieved state-of-the-art accuracy in numerous machine learning tasks. In spite of their great success in terms of accuracy, their large training time makes it difficult to use them for various tasks. In this paper, we explore the idea of learning weight evolution pattern from a simple network for accelerating training of novel neural networks.\n\nWe use a neural network to learn the training pattern from MNIST classification and utilize it to accelerate training of neural networks used for CIFAR-10 and ImageNet classification. Our method has a low memory footprint and is computationally efficient. This method can also be used with other optimizers to give faster convergence. The results indicate a general trend in the weight evolution during training of neural networks.", "pdf": "/pdf/f5316305b0560db063525a71f36ca95d1932981e.pdf", "TL;DR": "Acceleration of training by performing weight updates, using knowledge obtained from training other neural networks.", "paperhash": "sinha|introspectionaccelerating_neural_network_training_by_learning_weight_evolution", "keywords": ["Computer vision", "Deep learning", "Optimization"], "conflicts": ["iitk.ac.in", "iitkgp.ac.in", "adobe.com"], "authors": ["Abhishek Sinha", "Aahitagni Mukherjee", "Mausoom Sarkar", "Balaji Krishnamurthy"], "authorids": ["abhishek.sinha94@gmail.com", "ahitagnimukherjeeam@gmail.com", "msarkar@adobe.com", "kbalaji@adobe.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287608850, "id": "ICLR.cc/2017/conference/-/paper360/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hkg8bDqee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper360/reviewers", "ICLR.cc/2017/conference/paper360/areachairs"], "cdate": 1485287608850}}}, {"tddate": null, "tmdate": 1484464286843, "tcdate": 1484464286843, "number": 4, "id": "HJPiliOIg", "invitation": "ICLR.cc/2017/conference/-/paper360/official/comment", "forum": "Hkg8bDqee", "replyto": "HkbhvsLLg", "signatures": ["ICLR.cc/2017/conference/paper360/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper360/AnonReviewer3"], "content": {"title": "Paper is too long", "comment": "Even though there is no page limit, I find the paper too long and would recommend to move major part of the figures to appendix/supplementary material. Otherwise, updates address most of the concerns."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introspection:Accelerating Neural Network Training By Learning Weight Evolution", "abstract": "Neural Networks are function approximators that have achieved state-of-the-art accuracy in numerous machine learning tasks. In spite of their great success in terms of accuracy, their large training time makes it difficult to use them for various tasks. In this paper, we explore the idea of learning weight evolution pattern from a simple network for accelerating training of novel neural networks.\n\nWe use a neural network to learn the training pattern from MNIST classification and utilize it to accelerate training of neural networks used for CIFAR-10 and ImageNet classification. Our method has a low memory footprint and is computationally efficient. This method can also be used with other optimizers to give faster convergence. The results indicate a general trend in the weight evolution during training of neural networks.", "pdf": "/pdf/f5316305b0560db063525a71f36ca95d1932981e.pdf", "TL;DR": "Acceleration of training by performing weight updates, using knowledge obtained from training other neural networks.", "paperhash": "sinha|introspectionaccelerating_neural_network_training_by_learning_weight_evolution", "keywords": ["Computer vision", "Deep learning", "Optimization"], "conflicts": ["iitk.ac.in", "iitkgp.ac.in", "adobe.com"], "authors": ["Abhishek Sinha", "Aahitagni Mukherjee", "Mausoom Sarkar", "Balaji Krishnamurthy"], "authorids": ["abhishek.sinha94@gmail.com", "ahitagnimukherjeeam@gmail.com", "msarkar@adobe.com", "kbalaji@adobe.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287608723, "id": "ICLR.cc/2017/conference/-/paper360/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "Hkg8bDqee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper360/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper360/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper360/reviewers", "ICLR.cc/2017/conference/paper360/areachairs"], "cdate": 1485287608723}}}, {"tddate": null, "tmdate": 1484335077419, "tcdate": 1484335077419, "number": 7, "id": "SJ6y_jUIx", "invitation": "ICLR.cc/2017/conference/-/paper360/public/comment", "forum": "Hkg8bDqee", "replyto": "SktFDJDNl", "signatures": ["~Mausoom_Sarkar1"], "readers": ["everyone"], "writers": ["~Mausoom_Sarkar1"], "content": {"title": "Reply to AnonReviewer1", "comment": "Thanks for your comments and suggestions. They were very helpful and we have tried to address the issues you have raised.\n\nWe have now reported several experiments, which gives more insight into our approach. We have reported experiments on fully connected networks and RNNs as well. We observed that the same Introspection network was able to accelerate the training of fully connected and RNN as well. \n\nWe have now mentioned the mini-batch sizes, layer sizes, learning rates and other hyperparameters for every network that we have used.\n\nWe have added several baseline in section 4.3 and have also compared SGD vs SGD + introspection vs Adam vs Adam + Introspection with plots in section on \u201cComparison with Adam optimizer\u201d\nThe Introspection Network was able to accelerate when applied over SGD and Adam.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introspection:Accelerating Neural Network Training By Learning Weight Evolution", "abstract": "Neural Networks are function approximators that have achieved state-of-the-art accuracy in numerous machine learning tasks. In spite of their great success in terms of accuracy, their large training time makes it difficult to use them for various tasks. In this paper, we explore the idea of learning weight evolution pattern from a simple network for accelerating training of novel neural networks.\n\nWe use a neural network to learn the training pattern from MNIST classification and utilize it to accelerate training of neural networks used for CIFAR-10 and ImageNet classification. Our method has a low memory footprint and is computationally efficient. This method can also be used with other optimizers to give faster convergence. The results indicate a general trend in the weight evolution during training of neural networks.", "pdf": "/pdf/f5316305b0560db063525a71f36ca95d1932981e.pdf", "TL;DR": "Acceleration of training by performing weight updates, using knowledge obtained from training other neural networks.", "paperhash": "sinha|introspectionaccelerating_neural_network_training_by_learning_weight_evolution", "keywords": ["Computer vision", "Deep learning", "Optimization"], "conflicts": ["iitk.ac.in", "iitkgp.ac.in", "adobe.com"], "authors": ["Abhishek Sinha", "Aahitagni Mukherjee", "Mausoom Sarkar", "Balaji Krishnamurthy"], "authorids": ["abhishek.sinha94@gmail.com", "ahitagnimukherjeeam@gmail.com", "msarkar@adobe.com", "kbalaji@adobe.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287608850, "id": "ICLR.cc/2017/conference/-/paper360/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hkg8bDqee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper360/reviewers", "ICLR.cc/2017/conference/paper360/areachairs"], "cdate": 1485287608850}}}, {"tddate": null, "tmdate": 1484335017098, "tcdate": 1484335017098, "number": 6, "id": "HkbhvsLLg", "invitation": "ICLR.cc/2017/conference/-/paper360/public/comment", "forum": "Hkg8bDqee", "replyto": "B1F8hRVEg", "signatures": ["~Mausoom_Sarkar1"], "readers": ["everyone"], "writers": ["~Mausoom_Sarkar1"], "content": {"title": "Reply for AnonReviewer3", "comment": "Thanks for the insightful comments they helped in framing the paper better.\nWe have now mentioned the layer sizes, learning rates and other hyperparameters for every network we have used starting from section 4.2.\n\nIntrospection training has been detailed out in section4.1\n\nWe used our method on Imagenet classification to show that this can be used on huge datasets and is also efficient enough in terms of compute and memory, to be used with large networks. The current Introspection network has been trained using data just from MNIST N0 weight evolution and yet it was able to accelerate training of novel networks with different architectures, activations and datasets.\nWe are confident that by adding more training data and using a more complex Introspection network would be able to accelerate state of art networks. This can be part of future investigations along with the other directions mentioned in the paper.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introspection:Accelerating Neural Network Training By Learning Weight Evolution", "abstract": "Neural Networks are function approximators that have achieved state-of-the-art accuracy in numerous machine learning tasks. In spite of their great success in terms of accuracy, their large training time makes it difficult to use them for various tasks. In this paper, we explore the idea of learning weight evolution pattern from a simple network for accelerating training of novel neural networks.\n\nWe use a neural network to learn the training pattern from MNIST classification and utilize it to accelerate training of neural networks used for CIFAR-10 and ImageNet classification. Our method has a low memory footprint and is computationally efficient. This method can also be used with other optimizers to give faster convergence. The results indicate a general trend in the weight evolution during training of neural networks.", "pdf": "/pdf/f5316305b0560db063525a71f36ca95d1932981e.pdf", "TL;DR": "Acceleration of training by performing weight updates, using knowledge obtained from training other neural networks.", "paperhash": "sinha|introspectionaccelerating_neural_network_training_by_learning_weight_evolution", "keywords": ["Computer vision", "Deep learning", "Optimization"], "conflicts": ["iitk.ac.in", "iitkgp.ac.in", "adobe.com"], "authors": ["Abhishek Sinha", "Aahitagni Mukherjee", "Mausoom Sarkar", "Balaji Krishnamurthy"], "authorids": ["abhishek.sinha94@gmail.com", "ahitagnimukherjeeam@gmail.com", "msarkar@adobe.com", "kbalaji@adobe.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287608850, "id": "ICLR.cc/2017/conference/-/paper360/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hkg8bDqee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper360/reviewers", "ICLR.cc/2017/conference/paper360/areachairs"], "cdate": 1485287608850}}}, {"tddate": null, "tmdate": 1484334933885, "tcdate": 1484334933885, "number": 5, "id": "ryAUwsILl", "invitation": "ICLR.cc/2017/conference/-/paper360/public/comment", "forum": "Hkg8bDqee", "replyto": "Bkm_OazEx", "signatures": ["~Mausoom_Sarkar1"], "readers": ["everyone"], "writers": ["~Mausoom_Sarkar1"], "content": {"title": "Reply for AnonReviewer2", "comment": "Your comments have been very insightful and helped us in bringing out several interesting properties and also helped frame the paper better.\n \nThe Section 3.0 \u201cPatterns in Weight evolution\u201d now has more details to quantify the weight evolution patterns.\n\nDetails on training of introspection network have been added to the section 4.1. We trained the Introspection model until the validation loss stopped decreasing. We till now didn\u2019t stop the training of the introspection earlier to check if it gives better speedups during inference. This adds an additional dimension in our experiments, which we are yet to explore. Also thanks to your suggestion of comparisons to base line methods, we now know that Introspection should work better that the baselines.\n\nWe have created several baselines in Section 4.3 by comparing the introspection against linear fit, quadratic fit, random noise with various jump ratios (K of Kx) . We also used linear Introspection network i.e. with no nonlinearity to create additional baselines. In all our experiments only quadratic fit managed to outperform Introspection for once but it was very unstable and finicky on jump points and jump ratios. It has a tendency to take the network towards extremely high loss from where the training never recovers.\n\nWe have now mentioned our hyperparameters for each experiment from section 4.2\n\nOur thought behind the use of RNN was to remove the burden of sampling from our shoulders. The idea was to keep a state for each weight and update it repeatedly using the RNN after a fixed number of steps and predict the future weight value from this state. Now the dimensions in the state increases the amount of memory we would need to hold on the GPU i.e. a state size of N means we need memory for N*#weights. Yes we could transfer it to the disk and then do prediction but that will eat up time hence we were trying to keep the N small so that it can be kept on the GPU. We choose N in the range of 4 to 32 but none of them converged to anywhere close to the loss we were getting with plain Introspection. We will explore this direction further later on.\n\nYes you are right about jump points. The introspection is fed data that has been altered by I itself, but it still seems to work fine even though it has not seen such self altered data during training. \nA reason for such behavior could be that because we are using just 4 values for prediction, it doesn\u2019t matter if there are some sudden changes in weight evolution as the Introspection Network might have seen such values occurring naturally during its training. May be this could also suggest that our training of Introspection didn\u2019t overfit.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introspection:Accelerating Neural Network Training By Learning Weight Evolution", "abstract": "Neural Networks are function approximators that have achieved state-of-the-art accuracy in numerous machine learning tasks. In spite of their great success in terms of accuracy, their large training time makes it difficult to use them for various tasks. In this paper, we explore the idea of learning weight evolution pattern from a simple network for accelerating training of novel neural networks.\n\nWe use a neural network to learn the training pattern from MNIST classification and utilize it to accelerate training of neural networks used for CIFAR-10 and ImageNet classification. Our method has a low memory footprint and is computationally efficient. This method can also be used with other optimizers to give faster convergence. The results indicate a general trend in the weight evolution during training of neural networks.", "pdf": "/pdf/f5316305b0560db063525a71f36ca95d1932981e.pdf", "TL;DR": "Acceleration of training by performing weight updates, using knowledge obtained from training other neural networks.", "paperhash": "sinha|introspectionaccelerating_neural_network_training_by_learning_weight_evolution", "keywords": ["Computer vision", "Deep learning", "Optimization"], "conflicts": ["iitk.ac.in", "iitkgp.ac.in", "adobe.com"], "authors": ["Abhishek Sinha", "Aahitagni Mukherjee", "Mausoom Sarkar", "Balaji Krishnamurthy"], "authorids": ["abhishek.sinha94@gmail.com", "ahitagnimukherjeeam@gmail.com", "msarkar@adobe.com", "kbalaji@adobe.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287608850, "id": "ICLR.cc/2017/conference/-/paper360/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hkg8bDqee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper360/reviewers", "ICLR.cc/2017/conference/paper360/areachairs"], "cdate": 1485287608850}}}, {"tddate": null, "tmdate": 1481715395582, "tcdate": 1481709599281, "number": 4, "id": "HJPQ_qAXl", "invitation": "ICLR.cc/2017/conference/-/paper360/public/comment", "forum": "Hkg8bDqee", "replyto": "H1myVFJ7g", "signatures": ["~Mausoom_Sarkar1"], "readers": ["everyone"], "writers": ["~Mausoom_Sarkar1"], "content": {"title": "Answers for AnonReviewer1's questions. ", "comment": "Thanks for the insightful comments this helped us in carrying out additional experiments\n \n1.We chose the default SGD parameters as present in the tensorflow MNIST,CIFAR,Imagenet examples.And we were always training for best validation error.\n2. The architecture of MNIST1 and MNIST2 has now been described in the paper.\n3. The training loss was also going down faster when Introspection was applied. We are going to include the training plot in the next revision.\n4. We tried Introspection network on multi layerd fc network with sigmoid activation and it converged faster. It also works with Batch Norm and layer Norm. The cifar network was using batch normalization. We will mention these in our next revision of the paper.\n5. Also we need to do the experiments for long duration to check if there are any improvements in accuracy or overfitting"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introspection:Accelerating Neural Network Training By Learning Weight Evolution", "abstract": "Neural Networks are function approximators that have achieved state-of-the-art accuracy in numerous machine learning tasks. In spite of their great success in terms of accuracy, their large training time makes it difficult to use them for various tasks. In this paper, we explore the idea of learning weight evolution pattern from a simple network for accelerating training of novel neural networks.\n\nWe use a neural network to learn the training pattern from MNIST classification and utilize it to accelerate training of neural networks used for CIFAR-10 and ImageNet classification. Our method has a low memory footprint and is computationally efficient. This method can also be used with other optimizers to give faster convergence. The results indicate a general trend in the weight evolution during training of neural networks.", "pdf": "/pdf/f5316305b0560db063525a71f36ca95d1932981e.pdf", "TL;DR": "Acceleration of training by performing weight updates, using knowledge obtained from training other neural networks.", "paperhash": "sinha|introspectionaccelerating_neural_network_training_by_learning_weight_evolution", "keywords": ["Computer vision", "Deep learning", "Optimization"], "conflicts": ["iitk.ac.in", "iitkgp.ac.in", "adobe.com"], "authors": ["Abhishek Sinha", "Aahitagni Mukherjee", "Mausoom Sarkar", "Balaji Krishnamurthy"], "authorids": ["abhishek.sinha94@gmail.com", "ahitagnimukherjeeam@gmail.com", "msarkar@adobe.com", "kbalaji@adobe.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287608850, "id": "ICLR.cc/2017/conference/-/paper360/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hkg8bDqee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper360/reviewers", "ICLR.cc/2017/conference/paper360/areachairs"], "cdate": 1485287608850}}}, {"tddate": null, "tmdate": 1481302452935, "tcdate": 1481302452926, "number": 3, "id": "ryT2-wu7l", "invitation": "ICLR.cc/2017/conference/-/paper360/public/comment", "forum": "Hkg8bDqee", "replyto": "H1K8I9kXg", "signatures": ["~Mausoom_Sarkar1"], "readers": ["everyone"], "writers": ["~Mausoom_Sarkar1"], "content": {"title": "Simiar questions", "comment": "We chose the default SGD parameters as present in the tensorflow MNIST,CIFAR,Imagenet examples. And we were always training for best validation error.\nThe training loss was also going down faster when Introspection was applied. We are going to put that plot in the paper going forward.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introspection:Accelerating Neural Network Training By Learning Weight Evolution", "abstract": "Neural Networks are function approximators that have achieved state-of-the-art accuracy in numerous machine learning tasks. In spite of their great success in terms of accuracy, their large training time makes it difficult to use them for various tasks. In this paper, we explore the idea of learning weight evolution pattern from a simple network for accelerating training of novel neural networks.\n\nWe use a neural network to learn the training pattern from MNIST classification and utilize it to accelerate training of neural networks used for CIFAR-10 and ImageNet classification. Our method has a low memory footprint and is computationally efficient. This method can also be used with other optimizers to give faster convergence. The results indicate a general trend in the weight evolution during training of neural networks.", "pdf": "/pdf/f5316305b0560db063525a71f36ca95d1932981e.pdf", "TL;DR": "Acceleration of training by performing weight updates, using knowledge obtained from training other neural networks.", "paperhash": "sinha|introspectionaccelerating_neural_network_training_by_learning_weight_evolution", "keywords": ["Computer vision", "Deep learning", "Optimization"], "conflicts": ["iitk.ac.in", "iitkgp.ac.in", "adobe.com"], "authors": ["Abhishek Sinha", "Aahitagni Mukherjee", "Mausoom Sarkar", "Balaji Krishnamurthy"], "authorids": ["abhishek.sinha94@gmail.com", "ahitagnimukherjeeam@gmail.com", "msarkar@adobe.com", "kbalaji@adobe.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287608850, "id": "ICLR.cc/2017/conference/-/paper360/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hkg8bDqee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper360/reviewers", "ICLR.cc/2017/conference/paper360/areachairs"], "cdate": 1485287608850}}}, {"tddate": null, "tmdate": 1481277990442, "tcdate": 1481277350346, "number": 2, "id": "rJCik-dXe", "invitation": "ICLR.cc/2017/conference/-/paper360/public/comment", "forum": "Hkg8bDqee", "replyto": "r1cvzzPQx", "signatures": ["~Mausoom_Sarkar1"], "readers": ["everyone"], "writers": ["~Mausoom_Sarkar1"], "content": {"title": "Experimenting with hyperparameters, batch norm", "comment": "We have not varied the value of k as of yet. \nOur criteria for the selection of number of samples was the prediction accuracy on MNIST training weight trajectory. Another of our objective was to cause minimum increase to memory footprint hence our goal was to reduce the number of samples to as little as possible.\nWe did vary the number of samples and 4 was the least number of sample which gave good results. Increasing the number of sample improved the Introspection's MNIST accuracy but not by much.\nAnother thing to mention here would be that \nwe also tried RNN for doing the prediction but that didn't work with small state size. We couldn't have used bigger state because that inflates the memory footprint because you need #weight times #statesize to run the RNN.\n\nAs for batch normalization, yes it does better with batch normalization(BN).\nIn fact our results for cifar were always good with BN whereas without BN we had sporadic success with the same network. The figure for cifar accuracy is with BN\nThe Imagenet results are without BN but has lrn after the 1st  and 2nd convolution and we are in the process of repeating it with BN. \n\nIn the experiments we stopped the training when our validation accuracy hit the non Introspection aided training benchmark. We are in the process of repeating these experiments to see if there are improvements in accuracy."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introspection:Accelerating Neural Network Training By Learning Weight Evolution", "abstract": "Neural Networks are function approximators that have achieved state-of-the-art accuracy in numerous machine learning tasks. In spite of their great success in terms of accuracy, their large training time makes it difficult to use them for various tasks. In this paper, we explore the idea of learning weight evolution pattern from a simple network for accelerating training of novel neural networks.\n\nWe use a neural network to learn the training pattern from MNIST classification and utilize it to accelerate training of neural networks used for CIFAR-10 and ImageNet classification. Our method has a low memory footprint and is computationally efficient. This method can also be used with other optimizers to give faster convergence. The results indicate a general trend in the weight evolution during training of neural networks.", "pdf": "/pdf/f5316305b0560db063525a71f36ca95d1932981e.pdf", "TL;DR": "Acceleration of training by performing weight updates, using knowledge obtained from training other neural networks.", "paperhash": "sinha|introspectionaccelerating_neural_network_training_by_learning_weight_evolution", "keywords": ["Computer vision", "Deep learning", "Optimization"], "conflicts": ["iitk.ac.in", "iitkgp.ac.in", "adobe.com"], "authors": ["Abhishek Sinha", "Aahitagni Mukherjee", "Mausoom Sarkar", "Balaji Krishnamurthy"], "authorids": ["abhishek.sinha94@gmail.com", "ahitagnimukherjeeam@gmail.com", "msarkar@adobe.com", "kbalaji@adobe.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287608850, "id": "ICLR.cc/2017/conference/-/paper360/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hkg8bDqee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper360/reviewers", "ICLR.cc/2017/conference/paper360/areachairs"], "cdate": 1485287608850}}}, {"tddate": null, "tmdate": 1481213950004, "tcdate": 1481213949997, "number": 3, "id": "ryU-uZwmg", "invitation": "ICLR.cc/2017/conference/-/paper360/pre-review/question", "forum": "Hkg8bDqee", "replyto": "Hkg8bDqee", "signatures": ["ICLR.cc/2017/conference/paper360/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper360/AnonReviewer3"], "content": {"title": "Experimenting with hyperparameters, batch norm", "question": "Did the authors perform experiments varying hyperparameter k and the number of samples from the training history of weights?\nI'd be interested to know if the introspection method would provide improvement in accuracy and training speed if batch normalization is used."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introspection:Accelerating Neural Network Training By Learning Weight Evolution", "abstract": "Neural Networks are function approximators that have achieved state-of-the-art accuracy in numerous machine learning tasks. In spite of their great success in terms of accuracy, their large training time makes it difficult to use them for various tasks. In this paper, we explore the idea of learning weight evolution pattern from a simple network for accelerating training of novel neural networks.\n\nWe use a neural network to learn the training pattern from MNIST classification and utilize it to accelerate training of neural networks used for CIFAR-10 and ImageNet classification. Our method has a low memory footprint and is computationally efficient. This method can also be used with other optimizers to give faster convergence. The results indicate a general trend in the weight evolution during training of neural networks.", "pdf": "/pdf/f5316305b0560db063525a71f36ca95d1932981e.pdf", "TL;DR": "Acceleration of training by performing weight updates, using knowledge obtained from training other neural networks.", "paperhash": "sinha|introspectionaccelerating_neural_network_training_by_learning_weight_evolution", "keywords": ["Computer vision", "Deep learning", "Optimization"], "conflicts": ["iitk.ac.in", "iitkgp.ac.in", "adobe.com"], "authors": ["Abhishek Sinha", "Aahitagni Mukherjee", "Mausoom Sarkar", "Balaji Krishnamurthy"], "authorids": ["abhishek.sinha94@gmail.com", "ahitagnimukherjeeam@gmail.com", "msarkar@adobe.com", "kbalaji@adobe.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481213950557, "id": "ICLR.cc/2017/conference/-/paper360/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper360/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper360/AnonReviewer1", "ICLR.cc/2017/conference/paper360/AnonReviewer2", "ICLR.cc/2017/conference/paper360/AnonReviewer3"], "reply": {"forum": "Hkg8bDqee", "replyto": "Hkg8bDqee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper360/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper360/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481213950557}}}, {"tddate": null, "tmdate": 1480726097123, "tcdate": 1480726097118, "number": 2, "id": "H1K8I9kXg", "invitation": "ICLR.cc/2017/conference/-/paper360/pre-review/question", "forum": "Hkg8bDqee", "replyto": "Hkg8bDqee", "signatures": ["ICLR.cc/2017/conference/paper360/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper360/AnonReviewer2"], "content": {"title": "Simiar questions", "question": "My questions are same as AnonReviewer1.\n\nHow were the SGD parameters chosen? For best training loss, or best validation error?\nHow does the training loss behave?\n\nHaving read Sec. 4.3, I still have the question: how did you select the jump points for the reported results? You mention that using I later in training helped more, which implies that other jump points were also tried. What were the results then?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introspection:Accelerating Neural Network Training By Learning Weight Evolution", "abstract": "Neural Networks are function approximators that have achieved state-of-the-art accuracy in numerous machine learning tasks. In spite of their great success in terms of accuracy, their large training time makes it difficult to use them for various tasks. In this paper, we explore the idea of learning weight evolution pattern from a simple network for accelerating training of novel neural networks.\n\nWe use a neural network to learn the training pattern from MNIST classification and utilize it to accelerate training of neural networks used for CIFAR-10 and ImageNet classification. Our method has a low memory footprint and is computationally efficient. This method can also be used with other optimizers to give faster convergence. The results indicate a general trend in the weight evolution during training of neural networks.", "pdf": "/pdf/f5316305b0560db063525a71f36ca95d1932981e.pdf", "TL;DR": "Acceleration of training by performing weight updates, using knowledge obtained from training other neural networks.", "paperhash": "sinha|introspectionaccelerating_neural_network_training_by_learning_weight_evolution", "keywords": ["Computer vision", "Deep learning", "Optimization"], "conflicts": ["iitk.ac.in", "iitkgp.ac.in", "adobe.com"], "authors": ["Abhishek Sinha", "Aahitagni Mukherjee", "Mausoom Sarkar", "Balaji Krishnamurthy"], "authorids": ["abhishek.sinha94@gmail.com", "ahitagnimukherjeeam@gmail.com", "msarkar@adobe.com", "kbalaji@adobe.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481213950557, "id": "ICLR.cc/2017/conference/-/paper360/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper360/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper360/AnonReviewer1", "ICLR.cc/2017/conference/paper360/AnonReviewer2", "ICLR.cc/2017/conference/paper360/AnonReviewer3"], "reply": {"forum": "Hkg8bDqee", "replyto": "Hkg8bDqee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper360/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper360/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481213950557}}}, {"tddate": null, "tmdate": 1480721370699, "tcdate": 1480721370694, "number": 1, "id": "H1myVFJ7g", "invitation": "ICLR.cc/2017/conference/-/paper360/pre-review/question", "forum": "Hkg8bDqee", "replyto": "Hkg8bDqee", "signatures": ["ICLR.cc/2017/conference/paper360/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper360/AnonReviewer1"], "content": {"title": "Questions about the experiments", "question": "I have questions regarding the experiment setups:\n\nHow are the hyper-parameter of the SGD are chosen? Are the learning rate and momentum tuned for the best training time performance?\n\nIn sec. 4.2.1, it seems MNIST1 model is more powerful than MNIST2. Why is the MNIST1 model perform worse than MNIST2? \n\nIt is a paper on improving training neural networks, it will be nice to include the plots that shows the actual training objective function.\n\nDoes the Introspection network cause any overfitting problem? From the plots shown by the author, it seems non of those plots are extensive enough for model to overfit on the validation set.\n\nIt seems all the models that authors have experimented with are vanilla convolutional neural networks. Does the I network work well for fully connected architecture? Is the I network trained on normal nets able to predict weight evolution for the models using batch norm or layer norm? "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introspection:Accelerating Neural Network Training By Learning Weight Evolution", "abstract": "Neural Networks are function approximators that have achieved state-of-the-art accuracy in numerous machine learning tasks. In spite of their great success in terms of accuracy, their large training time makes it difficult to use them for various tasks. In this paper, we explore the idea of learning weight evolution pattern from a simple network for accelerating training of novel neural networks.\n\nWe use a neural network to learn the training pattern from MNIST classification and utilize it to accelerate training of neural networks used for CIFAR-10 and ImageNet classification. Our method has a low memory footprint and is computationally efficient. This method can also be used with other optimizers to give faster convergence. The results indicate a general trend in the weight evolution during training of neural networks.", "pdf": "/pdf/f5316305b0560db063525a71f36ca95d1932981e.pdf", "TL;DR": "Acceleration of training by performing weight updates, using knowledge obtained from training other neural networks.", "paperhash": "sinha|introspectionaccelerating_neural_network_training_by_learning_weight_evolution", "keywords": ["Computer vision", "Deep learning", "Optimization"], "conflicts": ["iitk.ac.in", "iitkgp.ac.in", "adobe.com"], "authors": ["Abhishek Sinha", "Aahitagni Mukherjee", "Mausoom Sarkar", "Balaji Krishnamurthy"], "authorids": ["abhishek.sinha94@gmail.com", "ahitagnimukherjeeam@gmail.com", "msarkar@adobe.com", "kbalaji@adobe.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481213950557, "id": "ICLR.cc/2017/conference/-/paper360/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper360/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper360/AnonReviewer1", "ICLR.cc/2017/conference/paper360/AnonReviewer2", "ICLR.cc/2017/conference/paper360/AnonReviewer3"], "reply": {"forum": "Hkg8bDqee", "replyto": "Hkg8bDqee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper360/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper360/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481213950557}}}], "count": 24}