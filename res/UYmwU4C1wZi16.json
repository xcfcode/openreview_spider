{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1392121260000, "tcdate": 1392121260000, "number": 6, "id": "v23y6jYd8a2B5", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "UYmwU4C1wZi16", "replyto": "UYmwU4C1wZi16", "signatures": ["Richard Davis"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "In response to the above comments:\r\n1. The main idea is a mechanism which forms a hierarchy of new features which are decorrelated successively from one layer to the next. This mechanism is the reason why the deep architecture is able to decorrelate features better than using PCA in combination with a simple SVM.\r\n\r\nWe thought the novel aspects were the following:\r\na) the use of a deep objective function, in which each node is separately optimized \r\nb) the additional objective that the global training error must improve to retain node modifications, \r\nc) the initialization of the deep SVM to the coefficients of a shallow SVM which guarantees the starting point is a good one, and \r\nd) for deep learning in regression, feature learning at a node level can be effectively done by training the node to the target.\r\n \r\nWe are not claiming that the novelty lies in stacking machine learning modules, which is a commonplace technique. Rather, the multi-layer objective function can produce significantly better results even using layers of linear SVMs (which overall is just a linear function), which we thought was an interesting result.\r\n\r\n2. We tested regression datasets from a well-known repository, http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/. \r\n\r\n3. Learning a non-linear function such as (sum x_i)^2 using linear building blocks is non-trivial, and the method significantly outperformed the other competing methods, some of which were non-linear. We tested other functions with more terms and found similar results.  \r\n\r\n4. We used datasets of dimension ~25 to show that the technique can improve over a standard SVM even for datasets of relatively small dimension.   \r\n\r\n5. The method did show some benefits on a few classification tests but we decided to focus on regression first.   \r\n\r\n6. Finding correlated groups of features can be done quite easily using pairwise correlation tests. Its not necessary to find the optimal permutation, significant gains over the shallow SVM can be achieved even with a fairly good permutation.   \r\n\r\n7. overline denotes a mean over the node outputs. Each SVM is trained using the output of the previous layer, scaling the target so that it has the  correct range using the output mean from the current node.   \r\n\r\n8. Adding normalized errors would help with the presentation and can be done easily.  \r\n\r\n9. We achieved significant gains in out-sample error performance using the technique.\r\n\r\n10. The gains from the method over a standard shallow SVM are dataset-dependent. If the data have completely uncorrelated features, the method will not improve over the shallow SVM. However if some features are correlated, as is usually the case, the method is able to improve over the shallow SVM using the architecture.\r\n\r\n11. The method gives a way to move from a shallow to a deep model incrementally so that at each stage the model improves. It has multiple outputs per group of correlated features, since there are multiple layers in the model. This multilayer structure allows for more complex models to be built."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Feature Graph Architectures", "decision": "submitted, no decision", "abstract": "In this article we propose feature graph architectures (FGA), which are deep learning systems employing a structured initialisation and training method based on a feature graph which facilitates improved generalisation performance compared with a standard shallow architecture. The goal is to explore alternative perspectives on the problem of deep network training. We evaluate FGA performance for deep SVMs on some experimental datasets, and show how generalisation and stability results may be derived for these models. We describe the effect of permutations on the model accuracy, and give a criterion for the optimal permutation in terms of feature correlations. The experimental results show that the algorithm produces robust and significant test set improvements over a standard shallow SVM training method for a range of datasets. These gains are achieved with a moderate increase in time complexity.", "pdf": "https://arxiv.org/abs/1312.4209", "paperhash": "davis|feature_graph_architectures", "keywords": [], "conflicts": [], "authors": ["Richard Davis", "Sanjay Chawla", "Philip Leong"], "authorids": ["davisconsulting@gmail.com", "sanjay.chawla@sydney.edu.au", "philip.leong@sydney.edu.au"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391999400000, "tcdate": 1391999400000, "number": 5, "id": "gWPx76RhvA7ue", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "UYmwU4C1wZi16", "replyto": "UYmwU4C1wZi16", "signatures": ["anonymous reviewer 1e36"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Feature Graph Architectures", "review": "The authors propose a hierarchical SVM approach for learning. The method uses the prediction of lower layer SVMs as input to the higher-layer SVMs. Adaptive methods are proposed where lower nodes are updated incrementally in order to improve the overall accuracy.\r\n\r\nI could not find what the proposed algorithm exactly does and what is the exact function implemented by the feature graph architecture. I assume that the authors use linear SVMs for regression. Then, should not a combination of these linear SVMs also be linear? Or is a nonlinear kernel being used? The overall lack of specification of the proposed approach, makes it difficult to compare with existing alternatives (hierarchical kernels, boosting, neural networks).\r\n\r\nThe paper is rich content-wise, including generalization bounds and stability analysis. Authors derive a generalization bound for the feature graph architecture, showing that surplus error grows linearly with the number of modified nodes in the graph. The generalization bound does not seem to be very tight, as authors show empirically that generalization is on par with basic SVMs.\r\n\r\nThe authors propose maximizing feature correlation in the first layer as a heuristic to construct the feature hierarchy. This is generally a good idea, but I am wondering whether having only one output per group of correlated features is sufficient in order to learn complex models."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Feature Graph Architectures", "decision": "submitted, no decision", "abstract": "In this article we propose feature graph architectures (FGA), which are deep learning systems employing a structured initialisation and training method based on a feature graph which facilitates improved generalisation performance compared with a standard shallow architecture. The goal is to explore alternative perspectives on the problem of deep network training. We evaluate FGA performance for deep SVMs on some experimental datasets, and show how generalisation and stability results may be derived for these models. We describe the effect of permutations on the model accuracy, and give a criterion for the optimal permutation in terms of feature correlations. The experimental results show that the algorithm produces robust and significant test set improvements over a standard shallow SVM training method for a range of datasets. These gains are achieved with a moderate increase in time complexity.", "pdf": "https://arxiv.org/abs/1312.4209", "paperhash": "davis|feature_graph_architectures", "keywords": [], "conflicts": [], "authors": ["Richard Davis", "Sanjay Chawla", "Philip Leong"], "authorids": ["davisconsulting@gmail.com", "sanjay.chawla@sydney.edu.au", "philip.leong@sydney.edu.au"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391835720000, "tcdate": 1391835720000, "number": 4, "id": "stQhRcrjlysjD", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "UYmwU4C1wZi16", "replyto": "UYmwU4C1wZi16", "signatures": ["Richard Davis"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "A few comments in response to the above points:  \r\n\r\n1. We think the novel aspects are a) the use of a deep objective function, in which each node is separately optimized b) the additional objective that the global training error must improve to retain node modifications, c) the initialization of the deep SVM to the coefficients of a shallow SVM which guarantees the starting point is a good one, and d) for deep learning in regression, feature learning at a node level can be effectively done by training the node to the target.  We are not claiming that the novelty lies in stacking machine learning modules, which is a commonplace technique. Rather, the main idea is that the deep objective function can produce significantly better results even with a deep SVM using linear kernels (which as you correctly point out is just a linear function), which we feel is an interesting result.   \r\n\r\n2. Learning a non-linear function such as (sum x_i)^2 using linear building blocks is non-trivial, and the method significantly outperformed the other competing methods, some of which were non-linear. We tested other functions with more terms and found similar results.  \r\n\r\n3. We plan to test the method on a much wider range of datasets but the ones we tested are regression datasets from a well-known repository,  http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/. \r\n\r\n4. We used datasets of dimension ~25 to show that the technique can improve over a standard SVM even for datasets of relatively small dimension.   \r\n\r\n5. The method did show some benefits on a few classification tests but we decided to focus on regression first.   \r\n\r\n6. Finding correlated groups of features can be done quite easily using pairwise correlation tests. Its not necessary to find the optimal permutation, significant gains over the shallow SVM can be achieved even with a fairly good permutation.  \r\n\r\n7. overline denotes a mean over the node outputs. Each SVM is trained using the output of the previous layer, scaling the target so that it has the \r\ncorrect range using the output mean from the current node.  \r\n\r\n8. Adding normalized errors would help with the presentation and can be done easily.  \r\n\r\n9. We achieved significant gains in out-sample error performance using the technique.  \r\n\r\nIf you have time we would be very grateful to hear your thoughts on these areas."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Feature Graph Architectures", "decision": "submitted, no decision", "abstract": "In this article we propose feature graph architectures (FGA), which are deep learning systems employing a structured initialisation and training method based on a feature graph which facilitates improved generalisation performance compared with a standard shallow architecture. The goal is to explore alternative perspectives on the problem of deep network training. We evaluate FGA performance for deep SVMs on some experimental datasets, and show how generalisation and stability results may be derived for these models. We describe the effect of permutations on the model accuracy, and give a criterion for the optimal permutation in terms of feature correlations. The experimental results show that the algorithm produces robust and significant test set improvements over a standard shallow SVM training method for a range of datasets. These gains are achieved with a moderate increase in time complexity.", "pdf": "https://arxiv.org/abs/1312.4209", "paperhash": "davis|feature_graph_architectures", "keywords": [], "conflicts": [], "authors": ["Richard Davis", "Sanjay Chawla", "Philip Leong"], "authorids": ["davisconsulting@gmail.com", "sanjay.chawla@sydney.edu.au", "philip.leong@sydney.edu.au"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391832240000, "tcdate": 1391832240000, "number": 3, "id": "2bIiNpLOMP2fT", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "UYmwU4C1wZi16", "replyto": "UYmwU4C1wZi16", "signatures": ["Richard Davis"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "A few comments in response to the above points:\r\n\r\n1. We think the novel aspects are a) the use of a deep objective function, in which each node is separately optimized b) the additional objective that the global training error must improve to retain node modifications, c) the initialization of the deep SVM to the coefficients of a shallow SVM which guarantees the starting point is a good one, and d) for deep learning in regression, feature learning at a node level can be effectively done by training the node to the target.\r\n\r\nWe are not claiming that the novelty lies in stacking machine learning modules, which is a commonplace technique. Rather, the main idea is that the deep objective function can produce significantly better results even with a deep SVM using linear kernels (which as you correctly point out is just a linear function), which we feel is an interesting result. \r\n\r\n2. Learning a non-linear function such as (sum x_i)^2 using linear building blocks is non-trivial, and the method significantly outperformed the other competing methods, some of which were non-linear. We tested other functions with more terms and found similar results.\r\n\r\n3. We plan to test the method on a much wider range of datasets but the ones we tested are regression datasets from a well-known repository, \r\nhttp://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/. We used datasets of dimension ~25 to show that the technique can improve over a standard SVM even for datasets of relatively small dimension. \r\n\r\n5. The method did show some benefits on a few classification tests but we decided to focus on regression first. \r\n\r\n6. Finding correlated groups of features can be done quite easily using pairwise correlation tests. Its not necessary to find the optimal permutation, significant gains over the shallow SVM can be achieved even with a fairly good permutation.\r\n\r\n7. overline denotes a mean over the node outputs. Each SVM is trained using the output of the previous layer, scaling the target so that it has the correct range using the output mean from the current node.\r\n\r\n8. Adding normalized errors would help with the presentation and can be done easily.\r\n\r\n9. We achieved significant gains in out-sample error performance using the technique.\r\n\r\nIf you have time we would be very grateful to hear your thoughts on these areas."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Feature Graph Architectures", "decision": "submitted, no decision", "abstract": "In this article we propose feature graph architectures (FGA), which are deep learning systems employing a structured initialisation and training method based on a feature graph which facilitates improved generalisation performance compared with a standard shallow architecture. The goal is to explore alternative perspectives on the problem of deep network training. We evaluate FGA performance for deep SVMs on some experimental datasets, and show how generalisation and stability results may be derived for these models. We describe the effect of permutations on the model accuracy, and give a criterion for the optimal permutation in terms of feature correlations. The experimental results show that the algorithm produces robust and significant test set improvements over a standard shallow SVM training method for a range of datasets. These gains are achieved with a moderate increase in time complexity.", "pdf": "https://arxiv.org/abs/1312.4209", "paperhash": "davis|feature_graph_architectures", "keywords": [], "conflicts": [], "authors": ["Richard Davis", "Sanjay Chawla", "Philip Leong"], "authorids": ["davisconsulting@gmail.com", "sanjay.chawla@sydney.edu.au", "philip.leong@sydney.edu.au"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391703720000, "tcdate": 1391703720000, "number": 2, "id": "DeInCVqfH-CS9", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "UYmwU4C1wZi16", "replyto": "UYmwU4C1wZi16", "signatures": ["anonymous reviewer 07bc"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Feature Graph Architectures", "review": "This paper presents a tree-structured architecture whose leafs are\r\nSVMs on subsets of attributes and whose internal nodes are SVM taking\r\nas input predictions computed by the children.\r\n\r\nThe generality of the method is unclear: you seem to implicitly assume\r\nregression problems but this is never explicitly mentioned. Could the\r\nmethod work for classification?\r\n\r\nThe significance seems modest to me. First, it is unclear in what\r\nsense the algorithm performs feature learning as the intermediate\r\nlayers contain in facts predictions. Additionally, all the experiments\r\nuse a linear kernel and thus from what I understand from Section 3 the\r\ntree in Figure 1 computes a linear function of its input. Clearly I\r\nmust be missing something otherwise this would not be a deep learning\r\nsystem at all. But the presentation should be improved to clarify\r\nthis. The notation is also confusing, for example does overline{y}\r\ndenote a mean? Over what quantities exactly (targets or predictions)?\r\nPseudo-code 2 seem to contradict Figure 1 as each SVM is trained on\r\ninputs x (again probably a notational issue).\r\n\r\nThe experiments are only preliminary and based on small data sets. The\r\nreported R and hat{R} seem to be unnormalized, why?"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Feature Graph Architectures", "decision": "submitted, no decision", "abstract": "In this article we propose feature graph architectures (FGA), which are deep learning systems employing a structured initialisation and training method based on a feature graph which facilitates improved generalisation performance compared with a standard shallow architecture. The goal is to explore alternative perspectives on the problem of deep network training. We evaluate FGA performance for deep SVMs on some experimental datasets, and show how generalisation and stability results may be derived for these models. We describe the effect of permutations on the model accuracy, and give a criterion for the optimal permutation in terms of feature correlations. The experimental results show that the algorithm produces robust and significant test set improvements over a standard shallow SVM training method for a range of datasets. These gains are achieved with a moderate increase in time complexity.", "pdf": "https://arxiv.org/abs/1312.4209", "paperhash": "davis|feature_graph_architectures", "keywords": [], "conflicts": [], "authors": ["Richard Davis", "Sanjay Chawla", "Philip Leong"], "authorids": ["davisconsulting@gmail.com", "sanjay.chawla@sydney.edu.au", "philip.leong@sydney.edu.au"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391403660000, "tcdate": 1391403660000, "number": 1, "id": "aK7XSqgON9aOi", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "UYmwU4C1wZi16", "replyto": "UYmwU4C1wZi16", "signatures": ["anonymous reviewer bbb2"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Feature Graph Architectures", "review": "* A brief summary of the paper's contributions, in the context of prior work.\r\nPapers suggests to stack multiple machine learning modules on top of each other. Applies it to SVMs.\r\n\r\n* An assessment of novelty and quality. \r\n        Not novel, quality is low.\r\n\r\n* A list of pros and cons (reasons to accept/reject).\r\npros :\r\nExploration of non-standard deep architectures. Good direction of research.\r\n\r\ncons :\r\nPaper suggests randomly establish group of features to find correlated groups. This task might be extremely expensive and infeasible. \r\n\r\nVery poor experiments. One experiment on synthetic data, which is trivial to learn (sum xi) ^ 2, and others on unknown datasets.\r\n\r\nIt is unclear where is non-linearity coming from. If this are just SVMs stacked on top of each other, and there is no non-linearity in between ? Then entire procedure is just a linear classifier with regularization. \r\n\r\nPaper doesn\u2019t state what is optimization objective of the entire system. It just brings an algorithm.\r\n\r\nDimensionality of data is extremely small ~25 dims."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Feature Graph Architectures", "decision": "submitted, no decision", "abstract": "In this article we propose feature graph architectures (FGA), which are deep learning systems employing a structured initialisation and training method based on a feature graph which facilitates improved generalisation performance compared with a standard shallow architecture. The goal is to explore alternative perspectives on the problem of deep network training. We evaluate FGA performance for deep SVMs on some experimental datasets, and show how generalisation and stability results may be derived for these models. We describe the effect of permutations on the model accuracy, and give a criterion for the optimal permutation in terms of feature correlations. The experimental results show that the algorithm produces robust and significant test set improvements over a standard shallow SVM training method for a range of datasets. These gains are achieved with a moderate increase in time complexity.", "pdf": "https://arxiv.org/abs/1312.4209", "paperhash": "davis|feature_graph_architectures", "keywords": [], "conflicts": [], "authors": ["Richard Davis", "Sanjay Chawla", "Philip Leong"], "authorids": ["davisconsulting@gmail.com", "sanjay.chawla@sydney.edu.au", "philip.leong@sydney.edu.au"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387332060000, "tcdate": 1387332060000, "number": 2, "id": "UYmwU4C1wZi16", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "UYmwU4C1wZi16", "signatures": ["davisconsulting@gmail.com"], "readers": ["everyone"], "content": {"title": "Feature Graph Architectures", "decision": "submitted, no decision", "abstract": "In this article we propose feature graph architectures (FGA), which are deep learning systems employing a structured initialisation and training method based on a feature graph which facilitates improved generalisation performance compared with a standard shallow architecture. The goal is to explore alternative perspectives on the problem of deep network training. We evaluate FGA performance for deep SVMs on some experimental datasets, and show how generalisation and stability results may be derived for these models. We describe the effect of permutations on the model accuracy, and give a criterion for the optimal permutation in terms of feature correlations. The experimental results show that the algorithm produces robust and significant test set improvements over a standard shallow SVM training method for a range of datasets. These gains are achieved with a moderate increase in time complexity.", "pdf": "https://arxiv.org/abs/1312.4209", "paperhash": "davis|feature_graph_architectures", "keywords": [], "conflicts": [], "authors": ["Richard Davis", "Sanjay Chawla", "Philip Leong"], "authorids": ["davisconsulting@gmail.com", "sanjay.chawla@sydney.edu.au", "philip.leong@sydney.edu.au"]}, "writers": [], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 7}