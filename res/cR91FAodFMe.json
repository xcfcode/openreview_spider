{"notes": [{"id": "cR91FAodFMe", "original": "K76NZMMPaOs", "number": 3405, "cdate": 1601308378000, "ddate": null, "tcdate": 1601308378000, "tmdate": 1613075142829, "tddate": null, "forum": "cR91FAodFMe", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Learning to Set Waypoints for Audio-Visual Navigation", "authorids": ["~Changan_Chen2", "~Sagnik_Majumder1", "~Ziad_Al-Halah2", "~Ruohan_Gao2", "~Santhosh_Kumar_Ramakrishnan1", "~Kristen_Grauman1"], "authors": ["Changan Chen", "Sagnik Majumder", "Ziad Al-Halah", "Ruohan Gao", "Santhosh Kumar Ramakrishnan", "Kristen Grauman"], "keywords": ["visual navigation", "audio visual learning", "embodied vision"], "abstract": "In audio-visual navigation, an agent intelligently travels through a complex, unmapped 3D environment using both sights and sounds to find a sound source (e.g., a phone ringing in another room). Existing models learn to act at a fixed granularity of agent motion and rely on simple recurrent aggregations of the audio observations. We introduce a reinforcement learning approach to audio-visual navigation with two key novel elements: 1) waypoints that are dynamically set and learned end-to-end within the navigation policy, and 2) an acoustic memory that provides a structured, spatially grounded record of what the agent has heard as it moves. Both new ideas capitalize on the synergy of audio and visual data for revealing the geometry of an unmapped space. We demonstrate our approach on two challenging datasets of real-world 3D scenes, Replica and Matterport3D. Our model improves the state of the art by a substantial margin, and our experiments reveal that learning the links between sights, sounds, and space is essential for audio-visual navigation.", "one-sentence_summary": "We introduce a hierarchical reinforcement learning approach to audio-visual navigation that learns to dynamically set waypoints in an end-to-end fashion ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|learning_to_set_waypoints_for_audiovisual_navigation", "supplementary_material": "/attachment/6f3bef1c2ed666064e1776c2511e6a60c174723f.zip", "pdf": "/pdf/fa0a991905ae30b2fa74ca7b101b3acabd532c13.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021learning,\ntitle={Learning to Set Waypoints for Audio-Visual Navigation},\nauthor={Changan Chen and Sagnik Majumder and Ziad Al-Halah and Ruohan Gao and Santhosh Kumar Ramakrishnan and Kristen Grauman},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=cR91FAodFMe}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "DlA8oKiPbfG", "original": null, "number": 1, "cdate": 1610040468986, "ddate": null, "tcdate": 1610040468986, "tmdate": 1610474072840, "tddate": null, "forum": "cR91FAodFMe", "replyto": "cR91FAodFMe", "invitation": "ICLR.cc/2021/Conference/Paper3405/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "The paper considers a variant of the point-goal navigation problem in which the agent additionally receives an audio signal emitted from the goal. The proposed framework incorporates a form of acoustic memory to build a map of acoustic signals over time. This memory is used in combination with an egocentric depth map to choose waypoints that serve as intermediate subgoals for planning. The method is shown to outperform state-of-the-art baselines in two navigation domains.\n\nThe reviewers all agree that the paper is very well written and that the evaluations are thorough, showing that the proposed framework offers clear performance gains. The idea of combining acoustic memory as a form of map with an occupancy grid representation as a means of choosing intermediate goals is interesting. However, the significance of the contributions and their relevance are limited by the narrow scope of the audio-video navigation task, which seems a bit contrived. The paper also overstates the novelty of the work at times (e.g., being the first use of end-to-end learned subgoals for navigation). The author response resolves some of these concerns, but others remain."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Set Waypoints for Audio-Visual Navigation", "authorids": ["~Changan_Chen2", "~Sagnik_Majumder1", "~Ziad_Al-Halah2", "~Ruohan_Gao2", "~Santhosh_Kumar_Ramakrishnan1", "~Kristen_Grauman1"], "authors": ["Changan Chen", "Sagnik Majumder", "Ziad Al-Halah", "Ruohan Gao", "Santhosh Kumar Ramakrishnan", "Kristen Grauman"], "keywords": ["visual navigation", "audio visual learning", "embodied vision"], "abstract": "In audio-visual navigation, an agent intelligently travels through a complex, unmapped 3D environment using both sights and sounds to find a sound source (e.g., a phone ringing in another room). Existing models learn to act at a fixed granularity of agent motion and rely on simple recurrent aggregations of the audio observations. We introduce a reinforcement learning approach to audio-visual navigation with two key novel elements: 1) waypoints that are dynamically set and learned end-to-end within the navigation policy, and 2) an acoustic memory that provides a structured, spatially grounded record of what the agent has heard as it moves. Both new ideas capitalize on the synergy of audio and visual data for revealing the geometry of an unmapped space. We demonstrate our approach on two challenging datasets of real-world 3D scenes, Replica and Matterport3D. Our model improves the state of the art by a substantial margin, and our experiments reveal that learning the links between sights, sounds, and space is essential for audio-visual navigation.", "one-sentence_summary": "We introduce a hierarchical reinforcement learning approach to audio-visual navigation that learns to dynamically set waypoints in an end-to-end fashion ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|learning_to_set_waypoints_for_audiovisual_navigation", "supplementary_material": "/attachment/6f3bef1c2ed666064e1776c2511e6a60c174723f.zip", "pdf": "/pdf/fa0a991905ae30b2fa74ca7b101b3acabd532c13.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021learning,\ntitle={Learning to Set Waypoints for Audio-Visual Navigation},\nauthor={Changan Chen and Sagnik Majumder and Ziad Al-Halah and Ruohan Gao and Santhosh Kumar Ramakrishnan and Kristen Grauman},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=cR91FAodFMe}\n}"}, "tags": [], "invitation": {"reply": {"forum": "cR91FAodFMe", "replyto": "cR91FAodFMe", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040468974, "tmdate": 1610474072825, "id": "ICLR.cc/2021/Conference/Paper3405/-/Decision"}}}, {"id": "hc8Bt5Qch2", "original": null, "number": 3, "cdate": 1603928835767, "ddate": null, "tcdate": 1603928835767, "tmdate": 1607150929249, "tddate": null, "forum": "cR91FAodFMe", "replyto": "cR91FAodFMe", "invitation": "ICLR.cc/2021/Conference/Paper3405/-/Official_Review", "content": {"title": "Recommendation to Accept", "review": "This paper tackles the AudioGoal task of navigating to a acoustic source in a 3D environment. It introduces the idea of an acoustic memory, which maps and aggregates acoustic intensity over time. An agent\u2019s acoustic memory, in tandem with its egocentric depth view, is then used to select navigation waypoints in an end-to-end manner. Their method beats SoTA in AudioGoal for two environments: Replica and Matterport3D.\n\nThe paper presents a simple end-to-end solution to waypoint selection that sits on top of an environment\u2019s low-level controls. There\u2019s a very nice symmetry between the occupancy map (used for waypoint selection) and the acoustic memory map \u2014 backed, of course, by experimental results and convincing ablations.\n\nOverall, the paper is extremely well written. Namely, in the exposition of the AudioGoal task. This is coming from someone who (works on embodied language and) is aware of, but not deeply familiar with, the tasks.\n\nThe paper provides a comprehensive set of experiments, baselines, and ablations. I particularly like Figure 4, which demonstrates the efficacy of acoustic memory in the presence of microphone noise.\n\nFinally, most clarifying questions that I had are addressed in the Supplemental section \u2014 not distracting from the main points of the paper.\n\n[Clarifying Questions]\n\nIn Section 3.5, the authors should define what a successful episode means in each respective environment (e.g., within 3 meters of the goal). This affects how SR and SPL are interpreted.\n\n[Post Rebuttal]\n\nThank you to the authors for addressing my question. The paper presents a simple and elegant approach to the AudioGoal task, backed by extensive experiments and good writing. I'd like to maintain my positive rating.\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3405/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3405/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Set Waypoints for Audio-Visual Navigation", "authorids": ["~Changan_Chen2", "~Sagnik_Majumder1", "~Ziad_Al-Halah2", "~Ruohan_Gao2", "~Santhosh_Kumar_Ramakrishnan1", "~Kristen_Grauman1"], "authors": ["Changan Chen", "Sagnik Majumder", "Ziad Al-Halah", "Ruohan Gao", "Santhosh Kumar Ramakrishnan", "Kristen Grauman"], "keywords": ["visual navigation", "audio visual learning", "embodied vision"], "abstract": "In audio-visual navigation, an agent intelligently travels through a complex, unmapped 3D environment using both sights and sounds to find a sound source (e.g., a phone ringing in another room). Existing models learn to act at a fixed granularity of agent motion and rely on simple recurrent aggregations of the audio observations. We introduce a reinforcement learning approach to audio-visual navigation with two key novel elements: 1) waypoints that are dynamically set and learned end-to-end within the navigation policy, and 2) an acoustic memory that provides a structured, spatially grounded record of what the agent has heard as it moves. Both new ideas capitalize on the synergy of audio and visual data for revealing the geometry of an unmapped space. We demonstrate our approach on two challenging datasets of real-world 3D scenes, Replica and Matterport3D. Our model improves the state of the art by a substantial margin, and our experiments reveal that learning the links between sights, sounds, and space is essential for audio-visual navigation.", "one-sentence_summary": "We introduce a hierarchical reinforcement learning approach to audio-visual navigation that learns to dynamically set waypoints in an end-to-end fashion ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|learning_to_set_waypoints_for_audiovisual_navigation", "supplementary_material": "/attachment/6f3bef1c2ed666064e1776c2511e6a60c174723f.zip", "pdf": "/pdf/fa0a991905ae30b2fa74ca7b101b3acabd532c13.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021learning,\ntitle={Learning to Set Waypoints for Audio-Visual Navigation},\nauthor={Changan Chen and Sagnik Majumder and Ziad Al-Halah and Ruohan Gao and Santhosh Kumar Ramakrishnan and Kristen Grauman},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=cR91FAodFMe}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "cR91FAodFMe", "replyto": "cR91FAodFMe", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3405/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538076417, "tmdate": 1606915791782, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3405/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3405/-/Official_Review"}}}, {"id": "u625kkWy1zz", "original": null, "number": 4, "cdate": 1604426972059, "ddate": null, "tcdate": 1604426972059, "tmdate": 1606780192361, "tddate": null, "forum": "cR91FAodFMe", "replyto": "cR91FAodFMe", "invitation": "ICLR.cc/2021/Conference/Paper3405/-/Official_Review", "content": {"title": "Thorough and useful paper, with some clarifications and restated contributions.", "review": "This work presents an approach for audio-visual navigation, in which an agent receives both an RGBD observation of the world and an audio signal emitted from the goal. The proposed approach leverages a structured memory via an occupancy grid and an acoustic map. A learned hierarchical policy is used to set waypoints within the occupancy grid at a high level, with a low level search over the free occupancy grid. The approach is demonstrated over baselines to reach the goal at a high rate and to do so efficiently. \n\nThe paper is well written and clear. The figures and videos are useful. The baselines and results are thorough and show clear benefit of the method and design choices. I appreciate both the comparison to state of the art methods for audio-visual and the baseline comparisons. A few clarifications that should be made:\n- The right side of Fig. 2 is slightly unclear due to the graph, which on a quick look brings notions of techniques like Savinov 2018. As the graph is just used by the simulator, I\u2019m not sure it makes sense to visualize in this way.\n- The figures alternate between showing the observation as RGB and as depth. My understanding from text is that this uses RGB-D, but from figures like Fig. 2 it is not clear where the RGB is used. For Fig. 1 the depth is not shown (though from reading, I understand it to be projected into the occupancy map).\n- Is directionality from the audio signal used at all within the acoustic memory? \n- What happens if the waypoint is not possible with the graph search?\n- How are unexplored regions treated for the graph search?\n\nThe paper is somewhat limited by the impactfulness of the setting, audio-visual navigation. The authors make a clear case for uses of such a problem, but in general the setting appears somewhat manufactured. It boils down to a setting like point navigation but with a noisily observed goal with an uncertainty distribution based on audio. Another setting with this noisy goal is something like semantic or object navigation, e.g., https://arxiv.org/pdf/2007.14545.pdf, https://arxiv.org/pdf/2007.00643.pdf. Overall I believe approaches from this work may be applied in these settings and the paper could have significantly greater impact if these settings were considered. At a minimum, I believe the paper would benefit from a discussion of applications of ideas from this work beyond audio-visual navigation.\n\nMy other concern is that at times the paper is unclear or overstates contributions. Such as stating:\n- \u201cThis is a novel technical contribution independent of the audio-visual setting, as it frees the agent to dynamically identify subgoals driven by the ultimate navigation goal.\u201d and \u201cThis is a new idea for 3D navigation subgoals in general, not specific to audio-visual\u201d. Many of the cited navigation papers use a hierarchical approach as a baseline, with the \u201cheuristics\u201d they describe presented as benefits over this unstructured hierarchy. Furthermore, many pure HRL papers present results in a navigation setting. \n- \u201cWe show that the multi-modal memory is essential for the agent to produce good action sequences.\u201d Based on the ablations, the multi-modal memory is \u201cbeneficial\u201d but not \u201cessential\u201d as the performance differences are somewhat small. \n\nOther notes:\n- The ablations should be moved into the main body of the paper though as they are quite important and they should include variance for each approach to really understand the significance of the choices. It would also be interesting to include a human baseline for navigation to put performance into context. \n- The authors should ablate for unheard sounds. I expect the audio memory, which is purely based on intensity may perform well here.\n\n\n_____\n\nPost author rebuttal:\n\nI appreciate the author\u2019s response and overall the authors have addressed my concerns. I am thus raising my score. \n\nThe only point that I believe still stands is #7, though I should have updated earlier. My issue with claiming this as the first use of end-to-end learned subgoals in navigation is that there have been many recent works from goal-conditioned hierarchical RL that use end-to-end learned subgoals, e.g., \nhttps://arxiv.org/pdf/1712.00948.pdf, https://arxiv.org/pdf/1805.08296.pdf, https://arxiv.org/pdf/1909.10618.pdf. Navigation to a known goal is a version of this problem and indeed in these works, the approaches are shown navigating between states. Others have applied end-to-end to navigation and manipulation, http://proceedings.mlr.press/v100/li20a/li20a.pdf. Overall, application of end-to-end HRL to the navigation problem is an interesting area to study, but to claim it as a major contribution I believe the paper should thoroughly examine the tradeoffs as applied to that problem, which I believe requires a detailed and standalone work.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3405/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3405/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Set Waypoints for Audio-Visual Navigation", "authorids": ["~Changan_Chen2", "~Sagnik_Majumder1", "~Ziad_Al-Halah2", "~Ruohan_Gao2", "~Santhosh_Kumar_Ramakrishnan1", "~Kristen_Grauman1"], "authors": ["Changan Chen", "Sagnik Majumder", "Ziad Al-Halah", "Ruohan Gao", "Santhosh Kumar Ramakrishnan", "Kristen Grauman"], "keywords": ["visual navigation", "audio visual learning", "embodied vision"], "abstract": "In audio-visual navigation, an agent intelligently travels through a complex, unmapped 3D environment using both sights and sounds to find a sound source (e.g., a phone ringing in another room). Existing models learn to act at a fixed granularity of agent motion and rely on simple recurrent aggregations of the audio observations. We introduce a reinforcement learning approach to audio-visual navigation with two key novel elements: 1) waypoints that are dynamically set and learned end-to-end within the navigation policy, and 2) an acoustic memory that provides a structured, spatially grounded record of what the agent has heard as it moves. Both new ideas capitalize on the synergy of audio and visual data for revealing the geometry of an unmapped space. We demonstrate our approach on two challenging datasets of real-world 3D scenes, Replica and Matterport3D. Our model improves the state of the art by a substantial margin, and our experiments reveal that learning the links between sights, sounds, and space is essential for audio-visual navigation.", "one-sentence_summary": "We introduce a hierarchical reinforcement learning approach to audio-visual navigation that learns to dynamically set waypoints in an end-to-end fashion ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|learning_to_set_waypoints_for_audiovisual_navigation", "supplementary_material": "/attachment/6f3bef1c2ed666064e1776c2511e6a60c174723f.zip", "pdf": "/pdf/fa0a991905ae30b2fa74ca7b101b3acabd532c13.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021learning,\ntitle={Learning to Set Waypoints for Audio-Visual Navigation},\nauthor={Changan Chen and Sagnik Majumder and Ziad Al-Halah and Ruohan Gao and Santhosh Kumar Ramakrishnan and Kristen Grauman},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=cR91FAodFMe}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "cR91FAodFMe", "replyto": "cR91FAodFMe", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3405/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538076417, "tmdate": 1606915791782, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3405/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3405/-/Official_Review"}}}, {"id": "aN5dAGl6P15", "original": null, "number": 2, "cdate": 1603877015413, "ddate": null, "tcdate": 1603877015413, "tmdate": 1606414944448, "tddate": null, "forum": "cR91FAodFMe", "replyto": "cR91FAodFMe", "invitation": "ICLR.cc/2021/Conference/Paper3405/-/Official_Review", "content": {"title": "Official Blind Review #3", "review": "Summary:\n\nThe authors address the audio-visual navigation problem, which aims to find a sound source in a 3D environment using both audio and visual information. The key innovation of the paper is to learn to set audio-visual waypoints, which decomposes a final goal to useful subgoals. Acoustic memory is introduced to strengthen the auditory perception. Experiments are performed on 3D environments of Replica and Matterport3D using SoundSpaces audio.\n\nPros:\n\n(1)  A deep reinforcement learning approach for AudioGoal navigation with audio-visual waypoints is proposed. It learns to set useful subgoals and address the navigation in a hierarchical manner.\n\n(2) The experiments are thorough and can well validate the effectiveness of the proposed audio-visual waypoint-based approach. \n\n(3) The paper is easy to follow and the provided demo can nicely illustrate the problem and demonstrate the superiority of the proposed method.\n\nCons:\n\n(1) Rather than only current audio, the authors propose to use the acoustic memory, which aggregates the audio intensity over time in a structured manner. Although the authors claim that acoustic memory can strengthen auditory perception, we only observe relatively small improvements (AV-WaN vs. AV-WaN w/o At) in Table 2.\n\n(2) Any failure examples? Please provide some failure results of the proposed audio-visual waypoint-based method and give an analysis in the main paper. Failure cases can help us to understand the drawbacks of the current subgoal based model. \n\n*** Post-Rebuttal ***\n\nThe authors addressed my concerns in the rebuttal. Overall, this is an interesting paper and extensive experiments are conducted. Thus, I would like to keep my positive rating. \n\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3405/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3405/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Set Waypoints for Audio-Visual Navigation", "authorids": ["~Changan_Chen2", "~Sagnik_Majumder1", "~Ziad_Al-Halah2", "~Ruohan_Gao2", "~Santhosh_Kumar_Ramakrishnan1", "~Kristen_Grauman1"], "authors": ["Changan Chen", "Sagnik Majumder", "Ziad Al-Halah", "Ruohan Gao", "Santhosh Kumar Ramakrishnan", "Kristen Grauman"], "keywords": ["visual navigation", "audio visual learning", "embodied vision"], "abstract": "In audio-visual navigation, an agent intelligently travels through a complex, unmapped 3D environment using both sights and sounds to find a sound source (e.g., a phone ringing in another room). Existing models learn to act at a fixed granularity of agent motion and rely on simple recurrent aggregations of the audio observations. We introduce a reinforcement learning approach to audio-visual navigation with two key novel elements: 1) waypoints that are dynamically set and learned end-to-end within the navigation policy, and 2) an acoustic memory that provides a structured, spatially grounded record of what the agent has heard as it moves. Both new ideas capitalize on the synergy of audio and visual data for revealing the geometry of an unmapped space. We demonstrate our approach on two challenging datasets of real-world 3D scenes, Replica and Matterport3D. Our model improves the state of the art by a substantial margin, and our experiments reveal that learning the links between sights, sounds, and space is essential for audio-visual navigation.", "one-sentence_summary": "We introduce a hierarchical reinforcement learning approach to audio-visual navigation that learns to dynamically set waypoints in an end-to-end fashion ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|learning_to_set_waypoints_for_audiovisual_navigation", "supplementary_material": "/attachment/6f3bef1c2ed666064e1776c2511e6a60c174723f.zip", "pdf": "/pdf/fa0a991905ae30b2fa74ca7b101b3acabd532c13.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021learning,\ntitle={Learning to Set Waypoints for Audio-Visual Navigation},\nauthor={Changan Chen and Sagnik Majumder and Ziad Al-Halah and Ruohan Gao and Santhosh Kumar Ramakrishnan and Kristen Grauman},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=cR91FAodFMe}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "cR91FAodFMe", "replyto": "cR91FAodFMe", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3405/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538076417, "tmdate": 1606915791782, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3405/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3405/-/Official_Review"}}}, {"id": "kjCa6a73irM", "original": null, "number": 6, "cdate": 1605842244195, "ddate": null, "tcdate": 1605842244195, "tmdate": 1605843457614, "tddate": null, "forum": "cR91FAodFMe", "replyto": "jRezrVYFhWW", "invitation": "ICLR.cc/2021/Conference/Paper3405/-/Official_Comment", "content": {"title": "Response to reviewer 2", "comment": "We appreciate your helpful feedback. \n\n1: Missing one baseline that predicts next step directly instead of waypoint.   \nAs suggested by the reviewer, we have experimentally validated that the waypoints are better than next-step actions by training our model to predict low-level actions directly instead of waypoints, and updated the 'Ablations' paragraph in Section 4 of the main paper with the results. We note that our full AV-WaN model outperforms this ablation by a large margin. For example, on Matterport3D our model achieves 28\\% and 15\\% higher SPL compared to the ablated version without waypoints for the heard and unheard sound settings, respectively.\n\n2: Search for audio goal using a room-centric representation.   \nRecall that the AudioGoal task requires the agent to navigate in unmapped environments (e.g., rescue search) as is the case in other navigation tasks like PointGoal and ObjectGoal. Lacking a floor plan at the start of the episode makes it hard for the agent to identify the rooms' layout. Furthermore, our model is flexible in terms of where to set the next waypoint. If the model finds strong cues to enter a room based on its observations then it will set waypoints that will take it towards the door of the room and then inside (as the agent gradually builds the map of the environment). \n\nFinally, a brute-force room search could be applicable in one- or two-room apartments (although the agent still needs to find the exact location of the goal, see Fig. 3), but it is highly inefficient in large environments like Matterport3D (Chang et al., 3DV 2017) where one scene has more than 22 rooms on average. "}, "signatures": ["ICLR.cc/2021/Conference/Paper3405/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3405/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Set Waypoints for Audio-Visual Navigation", "authorids": ["~Changan_Chen2", "~Sagnik_Majumder1", "~Ziad_Al-Halah2", "~Ruohan_Gao2", "~Santhosh_Kumar_Ramakrishnan1", "~Kristen_Grauman1"], "authors": ["Changan Chen", "Sagnik Majumder", "Ziad Al-Halah", "Ruohan Gao", "Santhosh Kumar Ramakrishnan", "Kristen Grauman"], "keywords": ["visual navigation", "audio visual learning", "embodied vision"], "abstract": "In audio-visual navigation, an agent intelligently travels through a complex, unmapped 3D environment using both sights and sounds to find a sound source (e.g., a phone ringing in another room). Existing models learn to act at a fixed granularity of agent motion and rely on simple recurrent aggregations of the audio observations. We introduce a reinforcement learning approach to audio-visual navigation with two key novel elements: 1) waypoints that are dynamically set and learned end-to-end within the navigation policy, and 2) an acoustic memory that provides a structured, spatially grounded record of what the agent has heard as it moves. Both new ideas capitalize on the synergy of audio and visual data for revealing the geometry of an unmapped space. We demonstrate our approach on two challenging datasets of real-world 3D scenes, Replica and Matterport3D. Our model improves the state of the art by a substantial margin, and our experiments reveal that learning the links between sights, sounds, and space is essential for audio-visual navigation.", "one-sentence_summary": "We introduce a hierarchical reinforcement learning approach to audio-visual navigation that learns to dynamically set waypoints in an end-to-end fashion ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|learning_to_set_waypoints_for_audiovisual_navigation", "supplementary_material": "/attachment/6f3bef1c2ed666064e1776c2511e6a60c174723f.zip", "pdf": "/pdf/fa0a991905ae30b2fa74ca7b101b3acabd532c13.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021learning,\ntitle={Learning to Set Waypoints for Audio-Visual Navigation},\nauthor={Changan Chen and Sagnik Majumder and Ziad Al-Halah and Ruohan Gao and Santhosh Kumar Ramakrishnan and Kristen Grauman},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=cR91FAodFMe}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "cR91FAodFMe", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3405/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3405/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3405/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3405/Authors|ICLR.cc/2021/Conference/Paper3405/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3405/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837886, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3405/-/Official_Comment"}}}, {"id": "xbUf-bYKu8S", "original": null, "number": 5, "cdate": 1605842157746, "ddate": null, "tcdate": 1605842157746, "tmdate": 1605843410591, "tddate": null, "forum": "cR91FAodFMe", "replyto": "aN5dAGl6P15", "invitation": "ICLR.cc/2021/Conference/Paper3405/-/Official_Comment", "content": {"title": "Response to reviewer 3", "comment": "We appreciate your positive feedback. \n\n1: Acoustic memory gives relatively small improvements.   \nThe acoustic map gives relatively small improvements when used with clean audio. However, when used with noisy audio, our model with an acoustic map is much more robust than the other models, as can be seen in Fig. 4b. Specifically, when the noise level exceeds 30 dB, our model with the acoustic memory suffers a very minor decline in performance; however, without the acoustic memory we see the noise has a significant impact on the model.\n\n2: Any failure examples?   \nYes, we provided some failure examples in the supplementary video (see starting from 5:25). Sometimes when the audio goal is just next to a wall or cornered between obstacles, the audio reflections could be strong, and the agent after reaching the goal quickly would oscillate around the goal trying to locate the exact location. We also saw some cases where the agent would issue a stop action prematurely just next to the goal. We expect the changes in audio intensity are less detectable in the immediate area around the goal where the audio is the loudest which may lead to this behavior. We have updated the paper to include an analysis of failure cases in `Navigation results' under Section 4."}, "signatures": ["ICLR.cc/2021/Conference/Paper3405/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3405/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Set Waypoints for Audio-Visual Navigation", "authorids": ["~Changan_Chen2", "~Sagnik_Majumder1", "~Ziad_Al-Halah2", "~Ruohan_Gao2", "~Santhosh_Kumar_Ramakrishnan1", "~Kristen_Grauman1"], "authors": ["Changan Chen", "Sagnik Majumder", "Ziad Al-Halah", "Ruohan Gao", "Santhosh Kumar Ramakrishnan", "Kristen Grauman"], "keywords": ["visual navigation", "audio visual learning", "embodied vision"], "abstract": "In audio-visual navigation, an agent intelligently travels through a complex, unmapped 3D environment using both sights and sounds to find a sound source (e.g., a phone ringing in another room). Existing models learn to act at a fixed granularity of agent motion and rely on simple recurrent aggregations of the audio observations. We introduce a reinforcement learning approach to audio-visual navigation with two key novel elements: 1) waypoints that are dynamically set and learned end-to-end within the navigation policy, and 2) an acoustic memory that provides a structured, spatially grounded record of what the agent has heard as it moves. Both new ideas capitalize on the synergy of audio and visual data for revealing the geometry of an unmapped space. We demonstrate our approach on two challenging datasets of real-world 3D scenes, Replica and Matterport3D. Our model improves the state of the art by a substantial margin, and our experiments reveal that learning the links between sights, sounds, and space is essential for audio-visual navigation.", "one-sentence_summary": "We introduce a hierarchical reinforcement learning approach to audio-visual navigation that learns to dynamically set waypoints in an end-to-end fashion ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|learning_to_set_waypoints_for_audiovisual_navigation", "supplementary_material": "/attachment/6f3bef1c2ed666064e1776c2511e6a60c174723f.zip", "pdf": "/pdf/fa0a991905ae30b2fa74ca7b101b3acabd532c13.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021learning,\ntitle={Learning to Set Waypoints for Audio-Visual Navigation},\nauthor={Changan Chen and Sagnik Majumder and Ziad Al-Halah and Ruohan Gao and Santhosh Kumar Ramakrishnan and Kristen Grauman},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=cR91FAodFMe}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "cR91FAodFMe", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3405/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3405/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3405/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3405/Authors|ICLR.cc/2021/Conference/Paper3405/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3405/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837886, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3405/-/Official_Comment"}}}, {"id": "eOlCEE5YVVd", "original": null, "number": 4, "cdate": 1605842088715, "ddate": null, "tcdate": 1605842088715, "tmdate": 1605843395669, "tddate": null, "forum": "cR91FAodFMe", "replyto": "hc8Bt5Qch2", "invitation": "ICLR.cc/2021/Conference/Paper3405/-/Official_Comment", "content": {"title": "Response to reviewer 4", "comment": "We appreciate your positive feedback. \n\nDefinition of successful episode.  \nAn episode is successful if and only if the agent stops at the exact audio goal location on the grid, as mentioned in the definition of Success Rate in Section 6.7 in the supplementary. We have also included the criterion for success in 'Metrics' in Section 4 of the revised main paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper3405/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3405/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Set Waypoints for Audio-Visual Navigation", "authorids": ["~Changan_Chen2", "~Sagnik_Majumder1", "~Ziad_Al-Halah2", "~Ruohan_Gao2", "~Santhosh_Kumar_Ramakrishnan1", "~Kristen_Grauman1"], "authors": ["Changan Chen", "Sagnik Majumder", "Ziad Al-Halah", "Ruohan Gao", "Santhosh Kumar Ramakrishnan", "Kristen Grauman"], "keywords": ["visual navigation", "audio visual learning", "embodied vision"], "abstract": "In audio-visual navigation, an agent intelligently travels through a complex, unmapped 3D environment using both sights and sounds to find a sound source (e.g., a phone ringing in another room). Existing models learn to act at a fixed granularity of agent motion and rely on simple recurrent aggregations of the audio observations. We introduce a reinforcement learning approach to audio-visual navigation with two key novel elements: 1) waypoints that are dynamically set and learned end-to-end within the navigation policy, and 2) an acoustic memory that provides a structured, spatially grounded record of what the agent has heard as it moves. Both new ideas capitalize on the synergy of audio and visual data for revealing the geometry of an unmapped space. We demonstrate our approach on two challenging datasets of real-world 3D scenes, Replica and Matterport3D. Our model improves the state of the art by a substantial margin, and our experiments reveal that learning the links between sights, sounds, and space is essential for audio-visual navigation.", "one-sentence_summary": "We introduce a hierarchical reinforcement learning approach to audio-visual navigation that learns to dynamically set waypoints in an end-to-end fashion ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|learning_to_set_waypoints_for_audiovisual_navigation", "supplementary_material": "/attachment/6f3bef1c2ed666064e1776c2511e6a60c174723f.zip", "pdf": "/pdf/fa0a991905ae30b2fa74ca7b101b3acabd532c13.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021learning,\ntitle={Learning to Set Waypoints for Audio-Visual Navigation},\nauthor={Changan Chen and Sagnik Majumder and Ziad Al-Halah and Ruohan Gao and Santhosh Kumar Ramakrishnan and Kristen Grauman},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=cR91FAodFMe}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "cR91FAodFMe", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3405/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3405/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3405/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3405/Authors|ICLR.cc/2021/Conference/Paper3405/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3405/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837886, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3405/-/Official_Comment"}}}, {"id": "zNerllBqvcN", "original": null, "number": 3, "cdate": 1605841967065, "ddate": null, "tcdate": 1605841967065, "tmdate": 1605843372868, "tddate": null, "forum": "cR91FAodFMe", "replyto": "u625kkWy1zz", "invitation": "ICLR.cc/2021/Conference/Paper3405/-/Official_Comment", "content": {"title": "Response to reviewer 1", "comment": "We appreciate your helpful feedback. \n\n1: Fig. 2 slightly unclear.  \nThank you.  We intended to show the graph maintained by our planner module that is constructed based on the map as the agent moves (Section 3.4). However, to avoid possible confusion we updated Fig. 2.\n\n2: Not clear if model uses RGB or depth image.  \nAs shown in Fig. 2, we use depth images for building the geometric map (via the projection operation). We do not use RGB as input.  As noted in the first paragraph in Section 3.2, we use depth because it is more effective than RGB for building geometric maps (Chaplot et al., ICLR 2020). Fig. 1 is meant to illustrate the high-level idea in the paper.  We will make sure it is clear.\n\n3: Is audio directionality used in the acoustic memory?  \nThe model does not explicitly encode audio directionality in the acoustic memory. However, the gradient in audio intensity stored in the memory can indicate to the agent the direction during navigation (i.e., the goal is usually in the direction of increasing audio intensity).\n\n4: What happens if the waypoint is not possible with the graph search?  \nIf the waypoint is not reachable using graph search, the agent takes a single random action and breaks the planning loop. We have added this point to Section 3.4 in the updated version of the paper.\n\n5: How are unexplored regions treated for the graph search?  \nThe unexplored regions are considered as free space during planning, following Chaplot et al., ICLR 2020. We have updated Section 3.4 of the paper with this point.\n\n6: Impactfulness of this setting beyond audio-visual navigation.   \nFirst, we believe that the AV-navigation task is impactful, in terms of both real-world applications and learning challenges for the  agent to translate raw audio-visual sensing into intelligent navigation actions. Second, please note that, unlike our approach, one of the compared methods (Gan et al., ICRA 2020) does indeed boil down the problem to point-goal navigation after predicting the goal location from audio, and it substantially underperforms the proposed approach (see Section 4).  This shows that isolating the audio as a final goal predictor is insufficient, and our model's joint learning from audio and visual throughout the entire navigation process to predict waypoints is important. Third, we agree that extensions to an audio-based semantic object navigation task could be interesting future work. We added a note about such applications to the Conclusion section.\n\n7: Unclear or overstated contributions with respect to hierarchical policy learning.  \nAs we noted in Section 1, hierachical policies for navigation are not new (e.g., Chaplot et al., ICLR 2020; Stein et al., PMLR 2018; Bansal et al., CoRL 2019; Caley et al., IROS 2016). However, to our knowledge, learning to set useful subgoals in an end-to-end fashion for the navigation task is new. The novelty is learning audio-visual waypoints of auto-adaptive granularity to maximize performance. To our knowledge, this is a contribution that is orthogonal to the fact that we tackle audio-visual navigation, and can also be applied to other application settings (see the Conclusion section for examples). Our idea improves over manually-designed heuristic definitions of waypoints, as it allows the agent to be more or less conservative in waypoint selection as per the demands of the situation, as shown through our model's improved performance over the Frontier Waypoints and Supervised Waypoints baselines. Further, we couldn't find anywhere in the cited literature any claims about \"heuristics\" being better than \"unstructured hierarchy\". If we have still missed something, we would like to request the reviewer to point us to the specific paper.\n\n8: Wording: multi-modal memory is  ''benefical\" but not ''essential\".  \nFair enough, the acoustic map is beneficial for the case of clean audio. However, in the presence of audio noise, it does become essential (without it, our SPL drops 0.7 points for 20 dB noise level, see Fig. 4b).  We think the results strongly justify its inclusion in the model. \n\n9: i) Move ablation to the main body and report variance; ii) ablate for unheard sounds.  \nWe have updated the 'Ablations' paragraph in Section 4 of the paper with these changes and additional numbers. We report the standard deviation of each model with 5 test runs, each having a different random seed. The standard deviation is $\\leq 0.5$, which is smaller than most of the improvement gains."}, "signatures": ["ICLR.cc/2021/Conference/Paper3405/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3405/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Set Waypoints for Audio-Visual Navigation", "authorids": ["~Changan_Chen2", "~Sagnik_Majumder1", "~Ziad_Al-Halah2", "~Ruohan_Gao2", "~Santhosh_Kumar_Ramakrishnan1", "~Kristen_Grauman1"], "authors": ["Changan Chen", "Sagnik Majumder", "Ziad Al-Halah", "Ruohan Gao", "Santhosh Kumar Ramakrishnan", "Kristen Grauman"], "keywords": ["visual navigation", "audio visual learning", "embodied vision"], "abstract": "In audio-visual navigation, an agent intelligently travels through a complex, unmapped 3D environment using both sights and sounds to find a sound source (e.g., a phone ringing in another room). Existing models learn to act at a fixed granularity of agent motion and rely on simple recurrent aggregations of the audio observations. We introduce a reinforcement learning approach to audio-visual navigation with two key novel elements: 1) waypoints that are dynamically set and learned end-to-end within the navigation policy, and 2) an acoustic memory that provides a structured, spatially grounded record of what the agent has heard as it moves. Both new ideas capitalize on the synergy of audio and visual data for revealing the geometry of an unmapped space. We demonstrate our approach on two challenging datasets of real-world 3D scenes, Replica and Matterport3D. Our model improves the state of the art by a substantial margin, and our experiments reveal that learning the links between sights, sounds, and space is essential for audio-visual navigation.", "one-sentence_summary": "We introduce a hierarchical reinforcement learning approach to audio-visual navigation that learns to dynamically set waypoints in an end-to-end fashion ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|learning_to_set_waypoints_for_audiovisual_navigation", "supplementary_material": "/attachment/6f3bef1c2ed666064e1776c2511e6a60c174723f.zip", "pdf": "/pdf/fa0a991905ae30b2fa74ca7b101b3acabd532c13.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021learning,\ntitle={Learning to Set Waypoints for Audio-Visual Navigation},\nauthor={Changan Chen and Sagnik Majumder and Ziad Al-Halah and Ruohan Gao and Santhosh Kumar Ramakrishnan and Kristen Grauman},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=cR91FAodFMe}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "cR91FAodFMe", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3405/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3405/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3405/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3405/Authors|ICLR.cc/2021/Conference/Paper3405/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3405/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837886, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3405/-/Official_Comment"}}}, {"id": "e-W8aB43f8", "original": null, "number": 2, "cdate": 1605841690705, "ddate": null, "tcdate": 1605841690705, "tmdate": 1605841690705, "tddate": null, "forum": "cR91FAodFMe", "replyto": "cR91FAodFMe", "invitation": "ICLR.cc/2021/Conference/Paper3405/-/Official_Comment", "content": {"title": "Meta response for all reviewers", "comment": "We thank all the reviewers for their valuable feedback. Overall, the reviewers have appreciated our model design, comprehensive experimentation and strong results, detailed ablation studies and analyses of the model's behavior. They have also suggested some changes and asked for some clarifications. We address them in this rebuttal and by making minor revisions to the paper (highlighted in blue)."}, "signatures": ["ICLR.cc/2021/Conference/Paper3405/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3405/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Set Waypoints for Audio-Visual Navigation", "authorids": ["~Changan_Chen2", "~Sagnik_Majumder1", "~Ziad_Al-Halah2", "~Ruohan_Gao2", "~Santhosh_Kumar_Ramakrishnan1", "~Kristen_Grauman1"], "authors": ["Changan Chen", "Sagnik Majumder", "Ziad Al-Halah", "Ruohan Gao", "Santhosh Kumar Ramakrishnan", "Kristen Grauman"], "keywords": ["visual navigation", "audio visual learning", "embodied vision"], "abstract": "In audio-visual navigation, an agent intelligently travels through a complex, unmapped 3D environment using both sights and sounds to find a sound source (e.g., a phone ringing in another room). Existing models learn to act at a fixed granularity of agent motion and rely on simple recurrent aggregations of the audio observations. We introduce a reinforcement learning approach to audio-visual navigation with two key novel elements: 1) waypoints that are dynamically set and learned end-to-end within the navigation policy, and 2) an acoustic memory that provides a structured, spatially grounded record of what the agent has heard as it moves. Both new ideas capitalize on the synergy of audio and visual data for revealing the geometry of an unmapped space. We demonstrate our approach on two challenging datasets of real-world 3D scenes, Replica and Matterport3D. Our model improves the state of the art by a substantial margin, and our experiments reveal that learning the links between sights, sounds, and space is essential for audio-visual navigation.", "one-sentence_summary": "We introduce a hierarchical reinforcement learning approach to audio-visual navigation that learns to dynamically set waypoints in an end-to-end fashion ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|learning_to_set_waypoints_for_audiovisual_navigation", "supplementary_material": "/attachment/6f3bef1c2ed666064e1776c2511e6a60c174723f.zip", "pdf": "/pdf/fa0a991905ae30b2fa74ca7b101b3acabd532c13.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021learning,\ntitle={Learning to Set Waypoints for Audio-Visual Navigation},\nauthor={Changan Chen and Sagnik Majumder and Ziad Al-Halah and Ruohan Gao and Santhosh Kumar Ramakrishnan and Kristen Grauman},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=cR91FAodFMe}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "cR91FAodFMe", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3405/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3405/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3405/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3405/Authors|ICLR.cc/2021/Conference/Paper3405/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3405/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837886, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3405/-/Official_Comment"}}}, {"id": "jRezrVYFhWW", "original": null, "number": 1, "cdate": 1602973286750, "ddate": null, "tcdate": 1602973286750, "tmdate": 1605024006505, "tddate": null, "forum": "cR91FAodFMe", "replyto": "cR91FAodFMe", "invitation": "ICLR.cc/2021/Conference/Paper3405/-/Official_Review", "content": {"title": "Interesting submission on audio-visual navigation. The role of waypoints can be better justified.", "review": "This paper studies the problem of navigating to the sound source in a virtual environment such as Replica.  The main contribution is a new formulation that learns a policy on the next \"waypoint\" and uses the predicted waypoints as intermediate goals for path planning. The results are promising, much better than recently published baselines especially Gan et al and Chen et al.\n\nThe paper is complete, well-written, and the reference is thorough. The formulation is well-justified. The results look promising. The authors have also included some interesting analyses to better understand how the model works.\n\nOn the negative side, I'm still not fully convinced that this is a practically useful problem, or how challenging it could be if formulated in the right way: assuming a household robot has a room-centric representation once deployed, even simply walking through all rooms will let it quickly identify the audio source. In this case, it's unclear why we need such complicated policy learning algorithms. But I don't mean to reject this paper based on this philosophical argument.  The authors don't have to respond to this point.\n\nWhat I do want to hear from the authors is why waypoints are useful. Though the authors have included some ablated models, there misses one baseline that employs exactly the current formulation but lets the policy learn to predict the next step (action) directly instead of to predict waypoints. Path planning is therefore no longer required. If the authors restrict the action space to be the space of rooms (i.e. actions are \"go to the room on the right\", instead of \"moving right by 1 foot\"), unless the agent believes it's already in the same room as the audio source, then such a policy learning method may work quite well, maybe even comparable with the waypoint-based method? \n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3405/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3405/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Set Waypoints for Audio-Visual Navigation", "authorids": ["~Changan_Chen2", "~Sagnik_Majumder1", "~Ziad_Al-Halah2", "~Ruohan_Gao2", "~Santhosh_Kumar_Ramakrishnan1", "~Kristen_Grauman1"], "authors": ["Changan Chen", "Sagnik Majumder", "Ziad Al-Halah", "Ruohan Gao", "Santhosh Kumar Ramakrishnan", "Kristen Grauman"], "keywords": ["visual navigation", "audio visual learning", "embodied vision"], "abstract": "In audio-visual navigation, an agent intelligently travels through a complex, unmapped 3D environment using both sights and sounds to find a sound source (e.g., a phone ringing in another room). Existing models learn to act at a fixed granularity of agent motion and rely on simple recurrent aggregations of the audio observations. We introduce a reinforcement learning approach to audio-visual navigation with two key novel elements: 1) waypoints that are dynamically set and learned end-to-end within the navigation policy, and 2) an acoustic memory that provides a structured, spatially grounded record of what the agent has heard as it moves. Both new ideas capitalize on the synergy of audio and visual data for revealing the geometry of an unmapped space. We demonstrate our approach on two challenging datasets of real-world 3D scenes, Replica and Matterport3D. Our model improves the state of the art by a substantial margin, and our experiments reveal that learning the links between sights, sounds, and space is essential for audio-visual navigation.", "one-sentence_summary": "We introduce a hierarchical reinforcement learning approach to audio-visual navigation that learns to dynamically set waypoints in an end-to-end fashion ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|learning_to_set_waypoints_for_audiovisual_navigation", "supplementary_material": "/attachment/6f3bef1c2ed666064e1776c2511e6a60c174723f.zip", "pdf": "/pdf/fa0a991905ae30b2fa74ca7b101b3acabd532c13.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021learning,\ntitle={Learning to Set Waypoints for Audio-Visual Navigation},\nauthor={Changan Chen and Sagnik Majumder and Ziad Al-Halah and Ruohan Gao and Santhosh Kumar Ramakrishnan and Kristen Grauman},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=cR91FAodFMe}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "cR91FAodFMe", "replyto": "cR91FAodFMe", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3405/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538076417, "tmdate": 1606915791782, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3405/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3405/-/Official_Review"}}}], "count": 11}