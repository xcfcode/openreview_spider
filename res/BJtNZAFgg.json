{"notes": [{"tddate": null, "nonreaders": null, "tmdate": 1491024257933, "tcdate": 1491007872738, "number": 4, "id": "SJYYt_22x", "invitation": "ICLR.cc/2017/conference/-/paper153/public/comment", "forum": "BJtNZAFgg", "replyto": "BJtNZAFgg", "signatures": ["~Jeff_Donahue1"], "readers": ["everyone"], "writers": ["~Jeff_Donahue1"], "content": {"title": "updated results", "comment": "Note: the latest version includes significantly improved ImageNet classification results in Table 2 (up ~2% across the board compared to the previous version).  This is due to the fact that in previous versions, we evaluated using the predictions for a single (center) crop at test time, rather than averaging over 10 crops as we learned by correspondence was how the previous results from Noroozi & Favaro (2016) were obtained.  (Thanks to Mehdi for bringing this up!) There are also very slightly improved VOC classification results (Table 3) due to an unrelated minor bug."}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Adversarial Feature Learning", "abstract": "The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing generators learn to \"linearize semantics\" in the latent space of such models. Intuitively, such latent spaces may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.", "pdf": "/pdf/bdadeedba940885e490acb24066c5ebab17354f5.pdf", "paperhash": "donahue|adversarial_feature_learning", "keywords": [], "conflicts": ["berkeley.edu", "utexas.edu"], "authors": ["Jeff Donahue", "Philipp Kr\u00e4henb\u00fchl", "Trevor Darrell"], "authorids": ["jdonahue@cs.berkeley.edu", "philkr@utexas.edu", "trevor@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287709661, "id": "ICLR.cc/2017/conference/-/paper153/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJtNZAFgg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper153/reviewers", "ICLR.cc/2017/conference/paper153/areachairs"], "cdate": 1485287709661}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1491007005223, "tcdate": 1478250800766, "number": 153, "id": "BJtNZAFgg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "BJtNZAFgg", "signatures": ["~Jeff_Donahue1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Adversarial Feature Learning", "abstract": "The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing generators learn to \"linearize semantics\" in the latent space of such models. Intuitively, such latent spaces may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.", "pdf": "/pdf/bdadeedba940885e490acb24066c5ebab17354f5.pdf", "paperhash": "donahue|adversarial_feature_learning", "keywords": [], "conflicts": ["berkeley.edu", "utexas.edu"], "authors": ["Jeff Donahue", "Philipp Kr\u00e4henb\u00fchl", "Trevor Darrell"], "authorids": ["jdonahue@cs.berkeley.edu", "philkr@utexas.edu", "trevor@eecs.berkeley.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396382773, "tcdate": 1486396382773, "number": 1, "id": "HJPy3M8Ol", "invitation": "ICLR.cc/2017/conference/-/paper153/acceptance", "forum": "BJtNZAFgg", "replyto": "BJtNZAFgg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "All reviewers unanimously praised the novelty and quality of the paper. Minor revisions, following the reviewers' suggestions, will make the paper even better. ", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Adversarial Feature Learning", "abstract": "The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing generators learn to \"linearize semantics\" in the latent space of such models. Intuitively, such latent spaces may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.", "pdf": "/pdf/bdadeedba940885e490acb24066c5ebab17354f5.pdf", "paperhash": "donahue|adversarial_feature_learning", "keywords": [], "conflicts": ["berkeley.edu", "utexas.edu"], "authors": ["Jeff Donahue", "Philipp Kr\u00e4henb\u00fchl", "Trevor Darrell"], "authorids": ["jdonahue@cs.berkeley.edu", "philkr@utexas.edu", "trevor@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396383650, "id": "ICLR.cc/2017/conference/-/paper153/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "BJtNZAFgg", "replyto": "BJtNZAFgg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396383650}}}, {"tddate": null, "tmdate": 1483670298879, "tcdate": 1483670298879, "number": 3, "id": "rkmX7FnHx", "invitation": "ICLR.cc/2017/conference/-/paper153/public/comment", "forum": "BJtNZAFgg", "replyto": "BJtNZAFgg", "signatures": ["~Jeff_Donahue1"], "readers": ["everyone"], "writers": ["~Jeff_Donahue1"], "content": {"title": "Update in response to reviewer feedback", "comment": "We thank all reviewers for their thoughtful comments and suggestions. We\u2019ve uploaded a revision with an expanded introduction which clarifies the motivation for BiGAN and the theory in Section 3 in the context of feature learning.\n\nNote that our theoretical arguments go beyond simply showing that BiGAN maintains the properties of the original GAN framework.  In particular, we show that the BiGAN objective is equivalent to that of an \u201cL0 autoencoder\u201d, encouraging the encoder and generator to invert one another (Theorem 3). If fully optimized, the encoder and generator are inverses of one another almost everywhere (Theorem 2).  We believe that these insights are quite important to understanding BiGAN\u2019s behavior and its use as a model for feature learning.  We hope that the revised introduction explains this better.\n\nWith respect to the ImageNet visual feature learning evaluation, our results show BiGAN is state-of-the-art among purely unsupervised learning methods.  The methods that outperform BiGAN (e.g., Doersch et al.) are all based on \u201cself-supervision,\u201d requiring the design of a domain-specific supervised prediction task, and some (Agrawal et al. and Wang & Gupta) are \u201cweakly supervised,\u201d requiring auxiliary information (video, egomotion, or tracking) unavailable in images alone.  We\u2019ve reorganized Table 3 to emphasize this point, and added discussion to Section 4.4 on the benefits of purely unsupervised approaches vs. self-supervised approaches.  (Following your suggestion, we\u2019ve also bolded the best results of each group in Tables 2 and 3.)  We do agree, however, that our results are somewhat preliminary and likely to improve significantly with further model architecture search and improvements to the optimization.\n\nMost optimization details (optimization algorithm, learning rate / step size, etc.) were included in Appendix C.  In the updated version, network initialization details are now included in Appendix C as well.  We\u2019ll also be releasing the training code."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Adversarial Feature Learning", "abstract": "The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing generators learn to \"linearize semantics\" in the latent space of such models. Intuitively, such latent spaces may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.", "pdf": "/pdf/bdadeedba940885e490acb24066c5ebab17354f5.pdf", "paperhash": "donahue|adversarial_feature_learning", "keywords": [], "conflicts": ["berkeley.edu", "utexas.edu"], "authors": ["Jeff Donahue", "Philipp Kr\u00e4henb\u00fchl", "Trevor Darrell"], "authorids": ["jdonahue@cs.berkeley.edu", "philkr@utexas.edu", "trevor@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287709661, "id": "ICLR.cc/2017/conference/-/paper153/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJtNZAFgg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper153/reviewers", "ICLR.cc/2017/conference/paper153/areachairs"], "cdate": 1485287709661}}}, {"tddate": null, "tmdate": 1483024210879, "tcdate": 1483024210879, "number": 3, "id": "HJoLwsMBl", "invitation": "ICLR.cc/2017/conference/-/paper153/official/review", "forum": "BJtNZAFgg", "replyto": "BJtNZAFgg", "signatures": ["ICLR.cc/2017/conference/paper153/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper153/AnonReviewer2"], "content": {"title": "An interesting idea", "rating": "7: Good paper, accept", "review": "This paper provides an interesting idea, which extends GAN by taking into account bidirectional network. Totally, the paper is well-written, and easy to follow what is contribution of this paper. From the theoretical parts, the proposed method, BiGAN, inherits similar properties in GAN. The experimental results show that BiGAN is competitive with other methods. A drawback would a non-convex optimization problem in BiGAN, this paper is still suitable to be accepted in my opinion. ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Adversarial Feature Learning", "abstract": "The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing generators learn to \"linearize semantics\" in the latent space of such models. Intuitively, such latent spaces may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.", "pdf": "/pdf/bdadeedba940885e490acb24066c5ebab17354f5.pdf", "paperhash": "donahue|adversarial_feature_learning", "keywords": [], "conflicts": ["berkeley.edu", "utexas.edu"], "authors": ["Jeff Donahue", "Philipp Kr\u00e4henb\u00fchl", "Trevor Darrell"], "authorids": ["jdonahue@cs.berkeley.edu", "philkr@utexas.edu", "trevor@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483024211552, "id": "ICLR.cc/2017/conference/-/paper153/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper153/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper153/AnonReviewer3", "ICLR.cc/2017/conference/paper153/AnonReviewer1", "ICLR.cc/2017/conference/paper153/AnonReviewer2"], "reply": {"forum": "BJtNZAFgg", "replyto": "BJtNZAFgg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper153/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper153/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483024211552}}}, {"tddate": null, "tmdate": 1482189036127, "tcdate": 1482189036127, "number": 2, "id": "BJNlKkLNl", "invitation": "ICLR.cc/2017/conference/-/paper153/official/review", "forum": "BJtNZAFgg", "replyto": "BJtNZAFgg", "signatures": ["ICLR.cc/2017/conference/paper153/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper153/AnonReviewer1"], "content": {"title": "", "rating": "7: Good paper, accept", "review": "This is a parallel work with ALI.  The idea is using auto encoder to provide extra information for discriminator. This approach seems is promising from reported result. For feature learning part of BiGAN, there still is a lot of space to improve, compare to standard supervised convnet. ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Adversarial Feature Learning", "abstract": "The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing generators learn to \"linearize semantics\" in the latent space of such models. Intuitively, such latent spaces may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.", "pdf": "/pdf/bdadeedba940885e490acb24066c5ebab17354f5.pdf", "paperhash": "donahue|adversarial_feature_learning", "keywords": [], "conflicts": ["berkeley.edu", "utexas.edu"], "authors": ["Jeff Donahue", "Philipp Kr\u00e4henb\u00fchl", "Trevor Darrell"], "authorids": ["jdonahue@cs.berkeley.edu", "philkr@utexas.edu", "trevor@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483024211552, "id": "ICLR.cc/2017/conference/-/paper153/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper153/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper153/AnonReviewer3", "ICLR.cc/2017/conference/paper153/AnonReviewer1", "ICLR.cc/2017/conference/paper153/AnonReviewer2"], "reply": {"forum": "BJtNZAFgg", "replyto": "BJtNZAFgg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper153/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper153/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483024211552}}}, {"tddate": null, "tmdate": 1481908032730, "tcdate": 1481908032730, "number": 1, "id": "B1FSJoZVx", "invitation": "ICLR.cc/2017/conference/-/paper153/official/review", "forum": "BJtNZAFgg", "replyto": "BJtNZAFgg", "signatures": ["ICLR.cc/2017/conference/paper153/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper153/AnonReviewer3"], "content": {"title": "Nice work, some structural issues", "rating": "7: Good paper, accept", "review": "The authors extend GANs by an inference path from the data space to the latent space and a discriminator that operates on the joint latend/data space. They show that the theoretical properties of GANs still hold for BiGAN and evaluate the features learned unsupervised in the inference path with respect to performance on supervised tasks after retraining deeper layers.\n\nI see one structural issue with this paper: Given that, as stated in the abstract, the main purpose of the paper is to learn unsupervised features (and not to improve GANs), the paper might spent too much space on detailing the relationship to GANs and all the theoretical properties. It is not clear whether they actually would help with the goal of learning good features. While reading the paper, I actually totally forgot about the unsupervised features until they reappeared on page 6. I think it would be helpful if the text of the paper would be more aligned with this main story.\n\nStill, the BiGAN framework is an elegant and compelling extension to GANs. However, it is not obvious how much the theoretical properties help us as the model is clearly not fully converged. To me, especially Figure 4 seems to suggest that G(E(x)) might be doing not much more than some kind of nearest neighbour retrival (and indeed one criticism for GANs has always been that they might just memorize some samples). By the way, it would be very interesting to know how well the discriminator actually performs after training.\n\nComing back to the goal of learning powerful features: The method does not reach state-of-the-art performance on most evaluated tasks (Table 2 and 3) but performs competitive and it would be interesting to see how much this improves if the BiGAN training (and the convolutional architecture used) would be improved.\n\nThe paper is very well written and provides most necessary details, although some more details on the training (learning rates, initialization) would be helpful for reproducing the results.\n\nOverall I think the paper provides a very interesting framework for further research, even though the results presented here are not too impressive both with respect to the feature evaluation (and the GAN learning).\n\nMinor: It might be helpful to highlight the best performance numbers in Tables 2 and 3.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Adversarial Feature Learning", "abstract": "The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing generators learn to \"linearize semantics\" in the latent space of such models. Intuitively, such latent spaces may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.", "pdf": "/pdf/bdadeedba940885e490acb24066c5ebab17354f5.pdf", "paperhash": "donahue|adversarial_feature_learning", "keywords": [], "conflicts": ["berkeley.edu", "utexas.edu"], "authors": ["Jeff Donahue", "Philipp Kr\u00e4henb\u00fchl", "Trevor Darrell"], "authorids": ["jdonahue@cs.berkeley.edu", "philkr@utexas.edu", "trevor@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483024211552, "id": "ICLR.cc/2017/conference/-/paper153/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper153/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper153/AnonReviewer3", "ICLR.cc/2017/conference/paper153/AnonReviewer1", "ICLR.cc/2017/conference/paper153/AnonReviewer2"], "reply": {"forum": "BJtNZAFgg", "replyto": "BJtNZAFgg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper153/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper153/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483024211552}}}, {"tddate": null, "tmdate": 1481029562705, "tcdate": 1481029562698, "number": 2, "id": "rk7TPN4me", "invitation": "ICLR.cc/2017/conference/-/paper153/public/comment", "forum": "BJtNZAFgg", "replyto": "ryRug71Xx", "signatures": ["~Jeff_Donahue1"], "readers": ["everyone"], "writers": ["~Jeff_Donahue1"], "content": {"title": "Responses to AnonReviewer3", "comment": "> Given that you have the encoder from the pixel space to the latent space, I wonder whether it is possible to calculate log-likelihoods of the data under the model.\n\nThe idea of using the encoder to compute a log-likelihood is intriguing. One difficulty could be that Pixel-RNN uses a discrete distribution, while GAN based models rely on a continuous parametrization. It might be possible to compare the two by integrating over the probability density P_{EX}.  We will consider this further.\n\n> An additional question regarding Table 2: For the two rows about your model, I understand that you retrain the deeper layers supervised on ImageNet. However I am not sure how to interpret the rows above. Which networks exactly did you freeze and retrain here? If it is other networks than your own, do they have the same architecture (e.g. conv1-conv5)?\n\nAll Table 2 results other than our own are from Noroozi & Favaro (2016).  All networks included in the table are minor variants on AlexNet, pre-trained for the self-supervised/unsupervised task described in the cited paper.  The Noroozi & Favaro architecture might have a slight advantage here as their model produces larger intermediate feature maps -- with stride 2 (rather than 4) in the conv1 layer. (We will make a note of this difference in the results table.) All others use the same convolutional and pooling layer settings (kernel size, stride, padding) used in AlexNet."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Adversarial Feature Learning", "abstract": "The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing generators learn to \"linearize semantics\" in the latent space of such models. Intuitively, such latent spaces may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.", "pdf": "/pdf/bdadeedba940885e490acb24066c5ebab17354f5.pdf", "paperhash": "donahue|adversarial_feature_learning", "keywords": [], "conflicts": ["berkeley.edu", "utexas.edu"], "authors": ["Jeff Donahue", "Philipp Kr\u00e4henb\u00fchl", "Trevor Darrell"], "authorids": ["jdonahue@cs.berkeley.edu", "philkr@utexas.edu", "trevor@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287709661, "id": "ICLR.cc/2017/conference/-/paper153/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJtNZAFgg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper153/reviewers", "ICLR.cc/2017/conference/paper153/areachairs"], "cdate": 1485287709661}}}, {"tddate": null, "tmdate": 1481029524955, "tcdate": 1481029524949, "number": 1, "id": "B1aqPNVQg", "invitation": "ICLR.cc/2017/conference/-/paper153/public/comment", "forum": "BJtNZAFgg", "replyto": "HJ9vCck7x", "signatures": ["~Jeff_Donahue1"], "readers": ["everyone"], "writers": ["~Jeff_Donahue1"], "content": {"title": "Responses to AnonReviewer1", "comment": "> Do you observe improvement of diversity of G by using Z?\n\nWe haven\u2019t figured out a principled way to measure the diversity of generator outputs.  Anecdotally, the BiGAN generator outputs appear to be similar to standard GAN generator outputs.\n\n> Also, Z is usually a uniform distribution, but seems E(x) should be very different to Z.\n\nWe show in Theorem 1 that P_{EX} = P_{GZ} at the global optimum.  As a necessary condition for this, the distribution over E(x) (with x ~ p_x) must be uniform as well. We understand that this is counter-intuitive as this is not the typical latent feature distribution produced by convnets. We tried other distributions, such as Gaussians, with similar results.\n\n> What is relation of E(G(Z)) to Z?\n\nTheorem 2 shows that for all z in the support of P_Z, we have E(G(z)) = z (symmetric to the G(E(x)) = x result).  In practice, we don\u2019t train D or G/E to convergence (doing so may be difficult or impossible), so these properties will not hold exactly, just as G(E(x)) = x doesn\u2019t hold exactly as shown in Figs. 2 and 4."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Adversarial Feature Learning", "abstract": "The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing generators learn to \"linearize semantics\" in the latent space of such models. Intuitively, such latent spaces may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.", "pdf": "/pdf/bdadeedba940885e490acb24066c5ebab17354f5.pdf", "paperhash": "donahue|adversarial_feature_learning", "keywords": [], "conflicts": ["berkeley.edu", "utexas.edu"], "authors": ["Jeff Donahue", "Philipp Kr\u00e4henb\u00fchl", "Trevor Darrell"], "authorids": ["jdonahue@cs.berkeley.edu", "philkr@utexas.edu", "trevor@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287709661, "id": "ICLR.cc/2017/conference/-/paper153/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJtNZAFgg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper153/reviewers", "ICLR.cc/2017/conference/paper153/areachairs"], "cdate": 1485287709661}}}, {"tddate": null, "tmdate": 1480728162086, "tcdate": 1480728162081, "number": 2, "id": "HJ9vCck7x", "invitation": "ICLR.cc/2017/conference/-/paper153/pre-review/question", "forum": "BJtNZAFgg", "replyto": "BJtNZAFgg", "signatures": ["ICLR.cc/2017/conference/paper153/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper153/AnonReviewer1"], "content": {"title": "Prereview Question", "question": "For BiGAN,  G(E(x)) seems produces reasonable output similar to x. Do you observe improvement of diversity of G by using Z? Also, Z is usually a uniform distribution, but seems E(x) should be very different to Z. What is relation of E(G(Z)) to Z?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Adversarial Feature Learning", "abstract": "The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing generators learn to \"linearize semantics\" in the latent space of such models. Intuitively, such latent spaces may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.", "pdf": "/pdf/bdadeedba940885e490acb24066c5ebab17354f5.pdf", "paperhash": "donahue|adversarial_feature_learning", "keywords": [], "conflicts": ["berkeley.edu", "utexas.edu"], "authors": ["Jeff Donahue", "Philipp Kr\u00e4henb\u00fchl", "Trevor Darrell"], "authorids": ["jdonahue@cs.berkeley.edu", "philkr@utexas.edu", "trevor@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959436548, "id": "ICLR.cc/2017/conference/-/paper153/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper153/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper153/AnonReviewer3", "ICLR.cc/2017/conference/paper153/AnonReviewer1"], "reply": {"forum": "BJtNZAFgg", "replyto": "BJtNZAFgg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper153/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper153/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959436548}}}, {"tddate": null, "tmdate": 1480695925665, "tcdate": 1480695925660, "number": 1, "id": "ryRug71Xx", "invitation": "ICLR.cc/2017/conference/-/paper153/pre-review/question", "forum": "BJtNZAFgg", "replyto": "BJtNZAFgg", "signatures": ["ICLR.cc/2017/conference/paper153/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper153/AnonReviewer3"], "content": {"title": "Questions", "question": "Given that you have the encoder from the pixel space to the latent space, I wonder whether it is possible to calculate log-likelihoods of the data under the model. As shown by Theis et al (https://arxiv.org/abs/1511.01844),  evaluating only on samples as indication of a good model fit is very problematic. I would be very interested to see how the model compares to other models like Pixel-RNN in terms of likelihood.\n\nAn additional question regarding Table 2: For the two rows about your model, I understand that you retraing the deeper layers supervised on ImageNet. However I am not sure how to interpret the rows above. Which networks exactly did you freeze and retrain here? If it is other networks than your own, do they have the same architecture (e.g. conv1-conv5)?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Adversarial Feature Learning", "abstract": "The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing generators learn to \"linearize semantics\" in the latent space of such models. Intuitively, such latent spaces may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.", "pdf": "/pdf/bdadeedba940885e490acb24066c5ebab17354f5.pdf", "paperhash": "donahue|adversarial_feature_learning", "keywords": [], "conflicts": ["berkeley.edu", "utexas.edu"], "authors": ["Jeff Donahue", "Philipp Kr\u00e4henb\u00fchl", "Trevor Darrell"], "authorids": ["jdonahue@cs.berkeley.edu", "philkr@utexas.edu", "trevor@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959436548, "id": "ICLR.cc/2017/conference/-/paper153/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper153/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper153/AnonReviewer3", "ICLR.cc/2017/conference/paper153/AnonReviewer1"], "reply": {"forum": "BJtNZAFgg", "replyto": "BJtNZAFgg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper153/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper153/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959436548}}}], "count": 11}