{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396458311, "tcdate": 1486396458311, "number": 1, "id": "rJM4hM8Ox", "invitation": "ICLR.cc/2017/conference/-/paper247/acceptance", "forum": "Hyanrrqlg", "replyto": "Hyanrrqlg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "This paper presents some interesting and potentially useful ideas, but multiple reviewers point out that the main appeal of the paper's contributions would be in potential follow-up work and that the paper as-is does not present a compelling use case for the novel ideas. For that reason, the recommendation is to reject the paper. I would encourage the authors to reframe and improve the paper and extend it to cover the more interesting possible empirical investigations brought up in the discussion. Unfortunately, the paper is not sufficiently ground breaking to be a good fit for the workshop track."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "HFH: Homologically Functional Hashing for Compressing Deep Neural Networks", "abstract": "As the complexity of deep neural networks (DNNs) trends to grow to absorb the increasing sizes of data, memory and energy consumption has been receiving more and more attentions for industrial applications, especially on mobile devices. This paper presents a novel structure based on homologically functional hashing to compress DNNs, shortly named as HFH. For each  weight entry in a deep net, HFH uses multiple low-cost hash functions to fetch values in a compression space, and then employs a small reconstruction network to recover that entry. The compression space is homological because all layers fetch hashed values from it. The reconstruction network is plugged into the whole network and trained jointly. On several benchmark datasets, HFH demonstrates high compression ratios with little loss on prediction accuracy. Particularly, HFH includes the recently proposed HashedNets as a degenerated scenario and shows significantly improved performance. Moreover, the homological hashing essence facilitates us to efficiently figure out one single desired compression ratio, instead of exhaustive searching throughout a combinatory space configured by all layers.", "pdf": "/pdf/4e797ada2bdadb112346e457d431180bc0290b93.pdf", "paperhash": "shi|hfh_homologically_functional_hashing_for_compressing_deep_neural_networks", "conflicts": ["baidu.com"], "keywords": [], "authors": ["Lei Shi", "Shikun Feng", "Zhifan Zhu"], "authorids": ["shilei06@baidu.com", "fengshikun01@baidu.com", "zhuzhifan@baidu.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396458839, "id": "ICLR.cc/2017/conference/-/paper247/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "Hyanrrqlg", "replyto": "Hyanrrqlg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396458839}}}, {"tddate": null, "tmdate": 1484146851864, "tcdate": 1484146851864, "number": 3, "id": "r1ns_6X8g", "invitation": "ICLR.cc/2017/conference/-/paper247/public/comment", "forum": "Hyanrrqlg", "replyto": "BJN9G3BBe", "signatures": ["~shikun_feng1"], "readers": ["everyone"], "writers": ["~shikun_feng1"], "content": {"title": "Answers to AnonReviewer3", "comment": "Concern 1. The computation cost seems worse than HashedNets and is not discussed.\nReply: Thank you for the remind. We will add detailed computational time cost in the revised version. Particularly for example on CIFR-10 by a CNN, the inference time for a single image is about 37 ms, while it is about 45 ms for HFH, both by Torch on a single core Tesla K40m GPU. It should be noted that, there is still certain room for improvement on HFH implementation efficiency, for example by employing more light-weighted hashing functions.\n \n\nConcern 2. Immediate practical implication of the paper is not clear given that alternative pruning strategies perform better and should be faster at inference.\nReply: To achieve higher compression and faster inference, we can combine HFH and the pruning strategy seamlessly. We have found this is a good choice in practice.\n \n\nConcern 3. Are the experiments only run once for each configuration? Please run multiple times and report average / standard error.\nReply: Having running multiple independent times in experiments, we found that the performance standard error is quite low especially on large datasets like ImageNet. We will report such results during revision.\n \n\nConcern 4. For completeness, please add U1 results to Table 1. And U4-G3 is listed twice with two different numbers.\nReply: The corresponding results will be provided. We are sorry that this is a typo, and the actual test error of U4-G3 is 1.32%. Thanks a lot for correction.\n \n\nConcern 5. Some sentences are not grammatically correct. Please improve the writing\nReply: The language expression will be improved under serious gramma checking and polishing. Thank you for the encouraging comments."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "HFH: Homologically Functional Hashing for Compressing Deep Neural Networks", "abstract": "As the complexity of deep neural networks (DNNs) trends to grow to absorb the increasing sizes of data, memory and energy consumption has been receiving more and more attentions for industrial applications, especially on mobile devices. This paper presents a novel structure based on homologically functional hashing to compress DNNs, shortly named as HFH. For each  weight entry in a deep net, HFH uses multiple low-cost hash functions to fetch values in a compression space, and then employs a small reconstruction network to recover that entry. The compression space is homological because all layers fetch hashed values from it. The reconstruction network is plugged into the whole network and trained jointly. On several benchmark datasets, HFH demonstrates high compression ratios with little loss on prediction accuracy. Particularly, HFH includes the recently proposed HashedNets as a degenerated scenario and shows significantly improved performance. Moreover, the homological hashing essence facilitates us to efficiently figure out one single desired compression ratio, instead of exhaustive searching throughout a combinatory space configured by all layers.", "pdf": "/pdf/4e797ada2bdadb112346e457d431180bc0290b93.pdf", "paperhash": "shi|hfh_homologically_functional_hashing_for_compressing_deep_neural_networks", "conflicts": ["baidu.com"], "keywords": [], "authors": ["Lei Shi", "Shikun Feng", "Zhifan Zhu"], "authorids": ["shilei06@baidu.com", "fengshikun01@baidu.com", "zhuzhifan@baidu.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287667085, "id": "ICLR.cc/2017/conference/-/paper247/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hyanrrqlg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper247/reviewers", "ICLR.cc/2017/conference/paper247/areachairs"], "cdate": 1485287667085}}}, {"tddate": null, "tmdate": 1484146784253, "tcdate": 1484146784253, "number": 2, "id": "HkuvO6mIx", "invitation": "ICLR.cc/2017/conference/-/paper247/public/comment", "forum": "Hyanrrqlg", "replyto": "rknpJiIBg", "signatures": ["~shikun_feng1"], "readers": ["everyone"], "writers": ["~shikun_feng1"], "content": {"title": "Answers to AnonReviewer1", "comment": "Concern 1: Are you also counting the extra parameters for reconstruction network for the memory comparison?\nReply: Compared to the original network, there are significantly less parameters in the reconstruction network. For instance, the U4-G3 reconstruction network has merely 19 parameters, which are neglectable on memory consumption.\n \nConcern 2\uff1aPlease also show the impact on running time\u3001I am not convinced that this method will be lightweight.\nReply: We will specify this clearly in the revised version. Specifically for instance on CIFR-10 by a CNN, the inference time for a single image is about 37 ms, while it is about 45 ms for HFH, both by Torch on a single core Tesla K40m GPU. It should be clarified that, there is still certain room for improvement on HFH implementation efficiency, for example by employing more light-weighted hashing functions. Thanks for your remind."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "HFH: Homologically Functional Hashing for Compressing Deep Neural Networks", "abstract": "As the complexity of deep neural networks (DNNs) trends to grow to absorb the increasing sizes of data, memory and energy consumption has been receiving more and more attentions for industrial applications, especially on mobile devices. This paper presents a novel structure based on homologically functional hashing to compress DNNs, shortly named as HFH. For each  weight entry in a deep net, HFH uses multiple low-cost hash functions to fetch values in a compression space, and then employs a small reconstruction network to recover that entry. The compression space is homological because all layers fetch hashed values from it. The reconstruction network is plugged into the whole network and trained jointly. On several benchmark datasets, HFH demonstrates high compression ratios with little loss on prediction accuracy. Particularly, HFH includes the recently proposed HashedNets as a degenerated scenario and shows significantly improved performance. Moreover, the homological hashing essence facilitates us to efficiently figure out one single desired compression ratio, instead of exhaustive searching throughout a combinatory space configured by all layers.", "pdf": "/pdf/4e797ada2bdadb112346e457d431180bc0290b93.pdf", "paperhash": "shi|hfh_homologically_functional_hashing_for_compressing_deep_neural_networks", "conflicts": ["baidu.com"], "keywords": [], "authors": ["Lei Shi", "Shikun Feng", "Zhifan Zhu"], "authorids": ["shilei06@baidu.com", "fengshikun01@baidu.com", "zhuzhifan@baidu.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287667085, "id": "ICLR.cc/2017/conference/-/paper247/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hyanrrqlg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper247/reviewers", "ICLR.cc/2017/conference/paper247/areachairs"], "cdate": 1485287667085}}}, {"tddate": null, "tmdate": 1483284419784, "tcdate": 1483284419784, "number": 3, "id": "rknpJiIBg", "invitation": "ICLR.cc/2017/conference/-/paper247/official/review", "forum": "Hyanrrqlg", "replyto": "Hyanrrqlg", "signatures": ["ICLR.cc/2017/conference/paper247/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper247/AnonReviewer1"], "content": {"title": "Hashing and Reconstruction Cost will make networks slow", "rating": "5: Marginally below acceptance threshold", "review": "The paper proposed a very complex compression and reconstruction method (with additional parameters) for reducing the memory footprint of deep networks.\n\nThe authors show that this complex proposal is better than simple hashed net proposal. One question: Are you also counting the extra parameters for reconstruction network for the memory comparison? Otherwise, the experiments are unfair.  \n\nSince hashing and reconstruction cost will dominate the feed-forward and back-propagation updates, it is imperative to compare the two methods on running time. For hashed net, this is quite simple, yet it created an additional bottleneck.  Please also show the impact on running time. Small improvements for a big loss in computational cost may not be acceptable. I am not convinced that this method will be lightweight. If we are allowed complicated compression and reconstruction then we can use any off-shelf methods, but the cost will be huge\n\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "HFH: Homologically Functional Hashing for Compressing Deep Neural Networks", "abstract": "As the complexity of deep neural networks (DNNs) trends to grow to absorb the increasing sizes of data, memory and energy consumption has been receiving more and more attentions for industrial applications, especially on mobile devices. This paper presents a novel structure based on homologically functional hashing to compress DNNs, shortly named as HFH. For each  weight entry in a deep net, HFH uses multiple low-cost hash functions to fetch values in a compression space, and then employs a small reconstruction network to recover that entry. The compression space is homological because all layers fetch hashed values from it. The reconstruction network is plugged into the whole network and trained jointly. On several benchmark datasets, HFH demonstrates high compression ratios with little loss on prediction accuracy. Particularly, HFH includes the recently proposed HashedNets as a degenerated scenario and shows significantly improved performance. Moreover, the homological hashing essence facilitates us to efficiently figure out one single desired compression ratio, instead of exhaustive searching throughout a combinatory space configured by all layers.", "pdf": "/pdf/4e797ada2bdadb112346e457d431180bc0290b93.pdf", "paperhash": "shi|hfh_homologically_functional_hashing_for_compressing_deep_neural_networks", "conflicts": ["baidu.com"], "keywords": [], "authors": ["Lei Shi", "Shikun Feng", "Zhifan Zhu"], "authorids": ["shilei06@baidu.com", "fengshikun01@baidu.com", "zhuzhifan@baidu.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483284420441, "id": "ICLR.cc/2017/conference/-/paper247/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper247/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper247/AnonReviewer2", "ICLR.cc/2017/conference/paper247/AnonReviewer3", "ICLR.cc/2017/conference/paper247/AnonReviewer1"], "reply": {"forum": "Hyanrrqlg", "replyto": "Hyanrrqlg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper247/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper247/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483284420441}}}, {"tddate": null, "tmdate": 1483223768141, "tcdate": 1483223691991, "number": 2, "id": "BJN9G3BBe", "invitation": "ICLR.cc/2017/conference/-/paper247/official/review", "forum": "Hyanrrqlg", "replyto": "Hyanrrqlg", "signatures": ["ICLR.cc/2017/conference/paper247/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper247/AnonReviewer3"], "content": {"title": "Review", "rating": "6: Marginally above acceptance threshold", "review": "The paper presents a method to reduce the memory footprint of a neural network at some increase in the computation cost. This paper is a generalization of HashedNets by Chen et al. (ICML'15) where parameters of a neural network are mapped into smaller memory arrays using some hash functions with possible collisions. Instead of training the original parameters, given a hash function, the elements of the compressed memory arrays are trained using back-propagation. In this paper, some new tricks are proposed including: (1) the compression space is shared among the layers of the neural network (2) multiple hash functions are used to reduce the effects of collisions (3) a small network is used to combine the elements retrieved from multiple hash tables into a single parameter. Fig 1 of the paper describes the gist of the approach vs. HashedNets.\n\nOn the positive side,\n+ The proposed ideas are novel and seem useful.\n+ Some theoretical justification is presented to describe why using multiple hash functions is a good idea.\n+ All of the experiments suggest that the proposed MFH approach outperforms HashedNets.\nOn the negative side,\n- The computation cost seems worse than HashedNets and is not discussed.\n- Immediate practical implication of the paper is not clear given that alternative pruning strategies perform better and should be faster at inference.\n\nThat said, I believe this paper benefits the deep learning community as it sheds light into ways to share parameters across layers of a neural network potentially leading to more interesting follow-ups. I recommend accept, while asking the authors to address the comments below.\n\nMore comments:\n- Please discuss the computation cost for both HashedNets and MFH for both fully connected and convolutional layers.\n- Are the experiments only run once for each configuration? Please run multiple times and report average / standard error.\n- For completeness, please add U1 results to Table 1.\n- In Table 1, U4-G3 is listed twice with two different numbers.\n- Some sentences are not grammatically correct. Please improve the writing.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "HFH: Homologically Functional Hashing for Compressing Deep Neural Networks", "abstract": "As the complexity of deep neural networks (DNNs) trends to grow to absorb the increasing sizes of data, memory and energy consumption has been receiving more and more attentions for industrial applications, especially on mobile devices. This paper presents a novel structure based on homologically functional hashing to compress DNNs, shortly named as HFH. For each  weight entry in a deep net, HFH uses multiple low-cost hash functions to fetch values in a compression space, and then employs a small reconstruction network to recover that entry. The compression space is homological because all layers fetch hashed values from it. The reconstruction network is plugged into the whole network and trained jointly. On several benchmark datasets, HFH demonstrates high compression ratios with little loss on prediction accuracy. Particularly, HFH includes the recently proposed HashedNets as a degenerated scenario and shows significantly improved performance. Moreover, the homological hashing essence facilitates us to efficiently figure out one single desired compression ratio, instead of exhaustive searching throughout a combinatory space configured by all layers.", "pdf": "/pdf/4e797ada2bdadb112346e457d431180bc0290b93.pdf", "paperhash": "shi|hfh_homologically_functional_hashing_for_compressing_deep_neural_networks", "conflicts": ["baidu.com"], "keywords": [], "authors": ["Lei Shi", "Shikun Feng", "Zhifan Zhu"], "authorids": ["shilei06@baidu.com", "fengshikun01@baidu.com", "zhuzhifan@baidu.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483284420441, "id": "ICLR.cc/2017/conference/-/paper247/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper247/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper247/AnonReviewer2", "ICLR.cc/2017/conference/paper247/AnonReviewer3", "ICLR.cc/2017/conference/paper247/AnonReviewer1"], "reply": {"forum": "Hyanrrqlg", "replyto": "Hyanrrqlg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper247/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper247/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483284420441}}}, {"tddate": null, "tmdate": 1482215036534, "tcdate": 1482214894804, "number": 1, "id": "SkvgCSUNg", "invitation": "ICLR.cc/2017/conference/-/paper247/official/review", "forum": "Hyanrrqlg", "replyto": "Hyanrrqlg", "signatures": ["ICLR.cc/2017/conference/paper247/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper247/AnonReviewer2"], "content": {"title": "Neat idea applied to the wrong settings.", "rating": "4: Ok but not good enough - rejection", "review": "The paper describes an extension of the HasheNets work, with several novel twists. Instead of using a single hash function, the proposed HFH approach uses multiple hash function to associate each \"virtual\" (to-be-synthesized) weight location to several components of an underlying parameter vector (shared across all layers). These components are then passed through a small MLP to synthesize the final weight.\n\nThis is an interesting and novel idea, and the experiments demonstrate that it improves substantially over HashedNets. However, HashedNets is not a particularly compelling technique for neural network model compression, especially when compared with more recent work on pruning- and quantization-based approaches. The experiments in this paper demonstrate that the proposed approach yields worse accuracy at the same compression ratios as pruning-based approaches, while providing no runtime speedup benefits. While the authors mention the technique is only 20% slower (which I am pleasantly surprised by), I don't understand why this technique should ever be used over competing approaches for the kinds of networks the authors present experimental results on. The authors suggest that the technique could be combined with pruning based approaches... this may be true, but no experiments to this effect are provided. The paper also suggests that ease of setting the compression ratio is a benefit of HFH, but I don't think that's a sufficient win to justify the numerous other downsides (in accuracy and speed). \n\nIn response to a question, the authors point out that the technique works very well for compressing embeddings, and for this setting the technique does appear like a genuinely useful contribution, given the marginal overhead and substantial train-time benefits. If the paper focused on this setting and showed experimental results on e.g. language modeling tasks or other scenarios with high-dimensional sparse/one-hot inputs require large embedding layers, I could enthusiastically recommend acceptance. However for the CNN and MLP networks which are the main focus of the experiments, I don't think the technique is suitable, as much as I like the basic idea."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "HFH: Homologically Functional Hashing for Compressing Deep Neural Networks", "abstract": "As the complexity of deep neural networks (DNNs) trends to grow to absorb the increasing sizes of data, memory and energy consumption has been receiving more and more attentions for industrial applications, especially on mobile devices. This paper presents a novel structure based on homologically functional hashing to compress DNNs, shortly named as HFH. For each  weight entry in a deep net, HFH uses multiple low-cost hash functions to fetch values in a compression space, and then employs a small reconstruction network to recover that entry. The compression space is homological because all layers fetch hashed values from it. The reconstruction network is plugged into the whole network and trained jointly. On several benchmark datasets, HFH demonstrates high compression ratios with little loss on prediction accuracy. Particularly, HFH includes the recently proposed HashedNets as a degenerated scenario and shows significantly improved performance. Moreover, the homological hashing essence facilitates us to efficiently figure out one single desired compression ratio, instead of exhaustive searching throughout a combinatory space configured by all layers.", "pdf": "/pdf/4e797ada2bdadb112346e457d431180bc0290b93.pdf", "paperhash": "shi|hfh_homologically_functional_hashing_for_compressing_deep_neural_networks", "conflicts": ["baidu.com"], "keywords": [], "authors": ["Lei Shi", "Shikun Feng", "Zhifan Zhu"], "authorids": ["shilei06@baidu.com", "fengshikun01@baidu.com", "zhuzhifan@baidu.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483284420441, "id": "ICLR.cc/2017/conference/-/paper247/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper247/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper247/AnonReviewer2", "ICLR.cc/2017/conference/paper247/AnonReviewer3", "ICLR.cc/2017/conference/paper247/AnonReviewer1"], "reply": {"forum": "Hyanrrqlg", "replyto": "Hyanrrqlg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper247/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper247/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483284420441}}}, {"tddate": null, "tmdate": 1481698178795, "tcdate": 1481698178787, "number": 1, "id": "BkiKsDRXl", "invitation": "ICLR.cc/2017/conference/-/paper247/public/comment", "forum": "Hyanrrqlg", "replyto": "rkDrf01mx", "signatures": ["~shikun_feng1"], "readers": ["everyone"], "writers": ["~shikun_feng1"], "content": {"title": "Answers to AnonReviewer2", "comment": "Your helpful comments are greatly appreciated.\n\nReply for Q1: On CIFR-10 by a CNN, the inference time for a single image is about 37 ms, while it is about 45 ms for HFH,  both by Torch on a single core Tesla K40m GPU. It should be noted that, there is still certain room for improvement on HFH implementation efficiency, for example by employing more light-weighted hashing functions. Even though, the additional time cost brought by HFH is acceptable compared to convolution operations. Last but not the least, this additional time cost is constant w.r.t. the compression ratio. We will make this clear in the revised version. Thank you.\n\nReply for Q2: The goal of HFH is to save memory under limited hardware budget, and the running time of current HFH implementation is just 20% lower than the original method. Furthermore, to achieve higher compression, we can combine HFH and the pruning work of Han et al seamlessly. HFH is also a good choice in practice. Taking DNNs with word embedding for example, 19G memory is required to save 5 million words with 1024 dimensions, which will further stand out in the personalization scenario, i.e., 190G memory is required to maintain user embeddings for 50 million users. In this case, a pure pruning method is hard to execute because it has to load the original memory to GPU, while HFH succeeds easily by just loading the compression space."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "HFH: Homologically Functional Hashing for Compressing Deep Neural Networks", "abstract": "As the complexity of deep neural networks (DNNs) trends to grow to absorb the increasing sizes of data, memory and energy consumption has been receiving more and more attentions for industrial applications, especially on mobile devices. This paper presents a novel structure based on homologically functional hashing to compress DNNs, shortly named as HFH. For each  weight entry in a deep net, HFH uses multiple low-cost hash functions to fetch values in a compression space, and then employs a small reconstruction network to recover that entry. The compression space is homological because all layers fetch hashed values from it. The reconstruction network is plugged into the whole network and trained jointly. On several benchmark datasets, HFH demonstrates high compression ratios with little loss on prediction accuracy. Particularly, HFH includes the recently proposed HashedNets as a degenerated scenario and shows significantly improved performance. Moreover, the homological hashing essence facilitates us to efficiently figure out one single desired compression ratio, instead of exhaustive searching throughout a combinatory space configured by all layers.", "pdf": "/pdf/4e797ada2bdadb112346e457d431180bc0290b93.pdf", "paperhash": "shi|hfh_homologically_functional_hashing_for_compressing_deep_neural_networks", "conflicts": ["baidu.com"], "keywords": [], "authors": ["Lei Shi", "Shikun Feng", "Zhifan Zhu"], "authorids": ["shilei06@baidu.com", "fengshikun01@baidu.com", "zhuzhifan@baidu.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287667085, "id": "ICLR.cc/2017/conference/-/paper247/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hyanrrqlg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper247/reviewers", "ICLR.cc/2017/conference/paper247/areachairs"], "cdate": 1485287667085}}}, {"tddate": null, "tmdate": 1480741438720, "tcdate": 1480741438716, "number": 1, "id": "rkDrf01mx", "invitation": "ICLR.cc/2017/conference/-/paper247/pre-review/question", "forum": "Hyanrrqlg", "replyto": "Hyanrrqlg", "signatures": ["ICLR.cc/2017/conference/paper247/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper247/AnonReviewer2"], "content": {"title": "Questions", "question": "1. Can you provide any concrete numbers as to how much slower this approach is during inference? It seems like it would be quite substantial, since the weight values must be computed on the fly on each pass for memory savings to be realized.\n2. There are many other techniques in the literature without such speed tradeoffs, including several mentioned in the paper such as the pruning work of Han et al. HFH seems to provide worse accuracy at the same compression ratio than these methods, while also having significant performance implications. I think the fact that HFH improves on HashedNets is an interesting finding, but are there circumstances where using it would be a good idea in practice?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "HFH: Homologically Functional Hashing for Compressing Deep Neural Networks", "abstract": "As the complexity of deep neural networks (DNNs) trends to grow to absorb the increasing sizes of data, memory and energy consumption has been receiving more and more attentions for industrial applications, especially on mobile devices. This paper presents a novel structure based on homologically functional hashing to compress DNNs, shortly named as HFH. For each  weight entry in a deep net, HFH uses multiple low-cost hash functions to fetch values in a compression space, and then employs a small reconstruction network to recover that entry. The compression space is homological because all layers fetch hashed values from it. The reconstruction network is plugged into the whole network and trained jointly. On several benchmark datasets, HFH demonstrates high compression ratios with little loss on prediction accuracy. Particularly, HFH includes the recently proposed HashedNets as a degenerated scenario and shows significantly improved performance. Moreover, the homological hashing essence facilitates us to efficiently figure out one single desired compression ratio, instead of exhaustive searching throughout a combinatory space configured by all layers.", "pdf": "/pdf/4e797ada2bdadb112346e457d431180bc0290b93.pdf", "paperhash": "shi|hfh_homologically_functional_hashing_for_compressing_deep_neural_networks", "conflicts": ["baidu.com"], "keywords": [], "authors": ["Lei Shi", "Shikun Feng", "Zhifan Zhu"], "authorids": ["shilei06@baidu.com", "fengshikun01@baidu.com", "zhuzhifan@baidu.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959382314, "id": "ICLR.cc/2017/conference/-/paper247/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper247/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper247/AnonReviewer2"], "reply": {"forum": "Hyanrrqlg", "replyto": "Hyanrrqlg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper247/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper247/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959382314}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1478280629217, "tcdate": 1478280629210, "number": 247, "id": "Hyanrrqlg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "Hyanrrqlg", "signatures": ["~Lei_Shi1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "HFH: Homologically Functional Hashing for Compressing Deep Neural Networks", "abstract": "As the complexity of deep neural networks (DNNs) trends to grow to absorb the increasing sizes of data, memory and energy consumption has been receiving more and more attentions for industrial applications, especially on mobile devices. This paper presents a novel structure based on homologically functional hashing to compress DNNs, shortly named as HFH. For each  weight entry in a deep net, HFH uses multiple low-cost hash functions to fetch values in a compression space, and then employs a small reconstruction network to recover that entry. The compression space is homological because all layers fetch hashed values from it. The reconstruction network is plugged into the whole network and trained jointly. On several benchmark datasets, HFH demonstrates high compression ratios with little loss on prediction accuracy. Particularly, HFH includes the recently proposed HashedNets as a degenerated scenario and shows significantly improved performance. Moreover, the homological hashing essence facilitates us to efficiently figure out one single desired compression ratio, instead of exhaustive searching throughout a combinatory space configured by all layers.", "pdf": "/pdf/4e797ada2bdadb112346e457d431180bc0290b93.pdf", "paperhash": "shi|hfh_homologically_functional_hashing_for_compressing_deep_neural_networks", "conflicts": ["baidu.com"], "keywords": [], "authors": ["Lei Shi", "Shikun Feng", "Zhifan Zhu"], "authorids": ["shilei06@baidu.com", "fengshikun01@baidu.com", "zhuzhifan@baidu.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}], "count": 9}