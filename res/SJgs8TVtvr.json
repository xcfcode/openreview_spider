{"notes": [{"id": "SJgs8TVtvr", "original": "SylPGTKwwH", "number": 573, "cdate": 1569439059381, "ddate": null, "tcdate": 1569439059381, "tmdate": 1577168294958, "tddate": null, "forum": "SJgs8TVtvr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Mixture-of-Experts Variational Autoencoder for clustering and generating from similarity-based representations", "authors": ["Andreas Kopf", "Vincent Fortuin", "Vignesh Ram Somnath", "Manfred Claassen"], "authorids": ["akopf@ethz.ch", "fortuin@inf.ethz.ch", "vsomnath@student.ethz.ch", "mclaassen@ethz.ch"], "keywords": ["Variational Autoencoder", "Clustering", "Generative model"], "abstract": "Clustering high-dimensional data, such as images or biological measurements, is a long-standing problem and has been studied extensively. Recently, Deep Clustering gained popularity due to the non-linearity of neural networks, which allows for flexibility in fitting the specific peculiarities of complex data. Here we introduce the Mixture-of-Experts Similarity Variational Autoencoder (MoE-Sim-VAE), a novel generative clustering model. The model can learn multi-modal distributions of high-dimensional data and use these to generate realistic data with high efficacy and efficiency. MoE-Sim-VAE is based on a Variational Autoencoder (VAE), where the decoder consists of a Mixture-of-Experts (MoE) architecture. This specific architecture allows for various modes of the data to be automatically learned by means of the experts. Additionally, we encourage the latent representation of our model to follow a Gaussian mixture distribution and to accurately represent the similarities between the data points. We assess the performance of our model on synthetic data, the MNIST benchmark data set, and a challenging real-world task of defining cell subpopulations from mass cytometry (CyTOF) measurements on hundreds of different datasets. MoE-Sim-VAE exhibits superior clustering performance on all these tasks in comparison to the baselines and we show that the MoE architecture in the decoder reduces the computational cost of sampling specific data modes with high fidelity.", "pdf": "/pdf/e9f65742aa83266dd94be945c0538a040ac39237.pdf", "paperhash": "kopf|mixtureofexperts_variational_autoencoder_for_clustering_and_generating_from_similaritybased_representations", "original_pdf": "/attachment/18e826c2873754d2b9fc94bf40ee467b75795685.pdf", "_bibtex": "@misc{\nkopf2020mixtureofexperts,\ntitle={Mixture-of-Experts Variational Autoencoder for clustering and generating from similarity-based representations},\nauthor={Andreas Kopf and Vincent Fortuin and Vignesh Ram Somnath and Manfred Claassen},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgs8TVtvr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "JWRbBs2-kH", "original": null, "number": 1, "cdate": 1576798700125, "ddate": null, "tcdate": 1576798700125, "tmdate": 1576800935783, "tddate": null, "forum": "SJgs8TVtvr", "replyto": "SJgs8TVtvr", "invitation": "ICLR.cc/2020/Conference/Paper573/-/Decision", "content": {"decision": "Reject", "comment": "The paper proposes a VAE with a mixture-of-experts decoder for clustering and generation of high-dimensional data. Overall, the reviewers found the paper well-written and structured , but in post rebuttal discussion questioned the overall importance and interest of the work to the community.  This is genuinely a borderline submission. However, the calibrated average score currently falls below the acceptance threshold, so I\u2019m recommending rejection, but strongly encouraging the authors to continue the work, better motivating the importance of the work, and resubmitting.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixture-of-Experts Variational Autoencoder for clustering and generating from similarity-based representations", "authors": ["Andreas Kopf", "Vincent Fortuin", "Vignesh Ram Somnath", "Manfred Claassen"], "authorids": ["akopf@ethz.ch", "fortuin@inf.ethz.ch", "vsomnath@student.ethz.ch", "mclaassen@ethz.ch"], "keywords": ["Variational Autoencoder", "Clustering", "Generative model"], "abstract": "Clustering high-dimensional data, such as images or biological measurements, is a long-standing problem and has been studied extensively. Recently, Deep Clustering gained popularity due to the non-linearity of neural networks, which allows for flexibility in fitting the specific peculiarities of complex data. Here we introduce the Mixture-of-Experts Similarity Variational Autoencoder (MoE-Sim-VAE), a novel generative clustering model. The model can learn multi-modal distributions of high-dimensional data and use these to generate realistic data with high efficacy and efficiency. MoE-Sim-VAE is based on a Variational Autoencoder (VAE), where the decoder consists of a Mixture-of-Experts (MoE) architecture. This specific architecture allows for various modes of the data to be automatically learned by means of the experts. Additionally, we encourage the latent representation of our model to follow a Gaussian mixture distribution and to accurately represent the similarities between the data points. We assess the performance of our model on synthetic data, the MNIST benchmark data set, and a challenging real-world task of defining cell subpopulations from mass cytometry (CyTOF) measurements on hundreds of different datasets. MoE-Sim-VAE exhibits superior clustering performance on all these tasks in comparison to the baselines and we show that the MoE architecture in the decoder reduces the computational cost of sampling specific data modes with high fidelity.", "pdf": "/pdf/e9f65742aa83266dd94be945c0538a040ac39237.pdf", "paperhash": "kopf|mixtureofexperts_variational_autoencoder_for_clustering_and_generating_from_similaritybased_representations", "original_pdf": "/attachment/18e826c2873754d2b9fc94bf40ee467b75795685.pdf", "_bibtex": "@misc{\nkopf2020mixtureofexperts,\ntitle={Mixture-of-Experts Variational Autoencoder for clustering and generating from similarity-based representations},\nauthor={Andreas Kopf and Vincent Fortuin and Vignesh Ram Somnath and Manfred Claassen},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgs8TVtvr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SJgs8TVtvr", "replyto": "SJgs8TVtvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795723141, "tmdate": 1576800274573, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper573/-/Decision"}}}, {"id": "Skgy1BS4iB", "original": null, "number": 4, "cdate": 1573307606917, "ddate": null, "tcdate": 1573307606917, "tmdate": 1573307606917, "tddate": null, "forum": "SJgs8TVtvr", "replyto": "H1eo64BVor", "invitation": "ICLR.cc/2020/Conference/Paper573/-/Official_Comment", "content": {"title": "Response to Review #1 ", "comment": "\u2014\u2014\u2014\u2014--\n\n***- in section 4.1 the MNIST data are taken with k=10. Though it is nicely explained and illustrated on this data set, it is possibly somewhat misleading as an example. The reason is that this is a classification problem with 10 classes, therefore the choice k=10 is obvious. It would be more important to consider benchmark problems for clustering, instead of classification, for which the choice of k is also an important model selection issue and for which k is unknown (how should k be selected then?).***\n\nChoosing the parameter k=10 for the experiments on MNIST was mainly for purposes of fair comparison to the competitor methods, which used k=10 in their respective publications which are also explicitly clustering approaches. The main point of this experiment was to compare our method against the baselines on a popular benchmark problem, but we agree that the sensitivity of the method\u2019s performance to the number of clusters is also a relevant question. The current manuscript already contains a few experiments for suitable applications on varying the parameter k. Specifically, we refer to section 4.1 where for synthetic data, generated from GMMs, we show the ability of our model to learn the correct number of clusters. In section 4.3 we report experiments on real biological data. For the biological data, we chose for all experiments k=25, an overestimation of the true number of cell populations in the data sets, and could still achieve good performance when comparing f-measures and therefore clustering results.\n\n\u2014\u2014\u2014\u2014--\n\n***- is each cluster always be assumed to be a Gaussian (which seems to be a strong assumption in general, and possibly not always realistic)? Could other components be used in the mixture?***\n\nWe fully agree that this is a limitation in the presented implementation of the method. A Gaussian mixture will not always be the best option for many datasets, which is why we also discuss in our conclusion that replacing the Gaussian mixture with different mixture distributions might be an interesting avenue for future work. Such extensions to the MoE-Sim-VAE are expected to be straightforward since our model does not make strict assumptions about the parametric form of the mixture distributions.\n\nWe hope that this addresses your questions and concerns. If you have any other suggestions on how we could improve our paper, please do let us know.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper573/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper573/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixture-of-Experts Variational Autoencoder for clustering and generating from similarity-based representations", "authors": ["Andreas Kopf", "Vincent Fortuin", "Vignesh Ram Somnath", "Manfred Claassen"], "authorids": ["akopf@ethz.ch", "fortuin@inf.ethz.ch", "vsomnath@student.ethz.ch", "mclaassen@ethz.ch"], "keywords": ["Variational Autoencoder", "Clustering", "Generative model"], "abstract": "Clustering high-dimensional data, such as images or biological measurements, is a long-standing problem and has been studied extensively. Recently, Deep Clustering gained popularity due to the non-linearity of neural networks, which allows for flexibility in fitting the specific peculiarities of complex data. Here we introduce the Mixture-of-Experts Similarity Variational Autoencoder (MoE-Sim-VAE), a novel generative clustering model. The model can learn multi-modal distributions of high-dimensional data and use these to generate realistic data with high efficacy and efficiency. MoE-Sim-VAE is based on a Variational Autoencoder (VAE), where the decoder consists of a Mixture-of-Experts (MoE) architecture. This specific architecture allows for various modes of the data to be automatically learned by means of the experts. Additionally, we encourage the latent representation of our model to follow a Gaussian mixture distribution and to accurately represent the similarities between the data points. We assess the performance of our model on synthetic data, the MNIST benchmark data set, and a challenging real-world task of defining cell subpopulations from mass cytometry (CyTOF) measurements on hundreds of different datasets. MoE-Sim-VAE exhibits superior clustering performance on all these tasks in comparison to the baselines and we show that the MoE architecture in the decoder reduces the computational cost of sampling specific data modes with high fidelity.", "pdf": "/pdf/e9f65742aa83266dd94be945c0538a040ac39237.pdf", "paperhash": "kopf|mixtureofexperts_variational_autoencoder_for_clustering_and_generating_from_similaritybased_representations", "original_pdf": "/attachment/18e826c2873754d2b9fc94bf40ee467b75795685.pdf", "_bibtex": "@misc{\nkopf2020mixtureofexperts,\ntitle={Mixture-of-Experts Variational Autoencoder for clustering and generating from similarity-based representations},\nauthor={Andreas Kopf and Vincent Fortuin and Vignesh Ram Somnath and Manfred Claassen},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgs8TVtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJgs8TVtvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper573/Authors", "ICLR.cc/2020/Conference/Paper573/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper573/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper573/Reviewers", "ICLR.cc/2020/Conference/Paper573/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper573/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper573/Authors|ICLR.cc/2020/Conference/Paper573/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169425, "tmdate": 1576860529138, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper573/Authors", "ICLR.cc/2020/Conference/Paper573/Reviewers", "ICLR.cc/2020/Conference/Paper573/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper573/-/Official_Comment"}}}, {"id": "H1eo64BVor", "original": null, "number": 3, "cdate": 1573307586860, "ddate": null, "tcdate": 1573307586860, "tmdate": 1573307586860, "tddate": null, "forum": "SJgs8TVtvr", "replyto": "H1gaakiptr", "invitation": "ICLR.cc/2020/Conference/Paper573/-/Official_Comment", "content": {"title": "Response to Review #1", "comment": "Thank you very much for reviewing our paper. We addressed your concerns and suggestions as follows.  Reviewer comments are pasted and marked with ***, author responses follow below:\n\n***- in the abstract it is claimed that the method works for high-dimensional data.However, it should be better explained why this is the case. The method is largely based on density estimation with a mixture of Gaussians which is known to have limitations in higher dimensions (see e.g. classical textbooks like Bishop 1995)***\n\nThe reviewer is right about the issues of GMMs with high dimensional input. The input data for our model can readily be high-dimensional since the Variational Autoencoders have been demonstrated to be able to handle very well, i.e. to generate informative representations in a lower-dimensional latent space [Aljalbout et al. \u201cClustering with deep learning: Taxonomy and new methods\u201d]. While being low-dimensional these representations capture the main differences in variance to be able to reconstruct the data. We fit our GMMs in this lower-dimensional latent space. We agree that one has to find a trade-off when choosing the dimensionality of the latent representation: it should be large enough to find separation and capture the variability of the input data, but also small enough such that the Gaussian mixtures can fit the clusters. We updated the manuscript to alert potential users about this issue and make this tradeoff clearer. We specifically address it in section 2 where we introduce the model and specify how we train the GMM on the latent representation. Specifically, for the MNIST application (d=68) and the real biological data (d=9) and we found good clustering performance where GMMs have been demonstrated to operate well [Jiang et al. \u201cVariational deep embedding: An unsupervised and generative approach to clustering\u201d]. \n\n\u2014\u2014\u2014\u2014--\n\n***- the similarity matrix and the similarity values should be carefully defined. Is there also an underlying similarity function assumed?***\n\nThe similarity matrix indeed has to be defined carefully, since otherwise, the clustering network attempts to cluster data objects which might not be similar and therefore also the latent representation might not separate the clusters. The similarity matrices for each training batch in our experiments are defined either via k-nearest neighbors or via distance thresholds where the similarity function is the Euclidean distance. Both are applied to the transformed data using UMAP. To define the similarity matrix like that was the most straightforward way we could think of but could be easily replaced by a different approach in the MoE-Sim-VAE framework. The details are stated in the sections of the respective experiments. Further, we added an ablation study on the similarity matrix to our paper to section 4.2 and additionally with a figure (A4) in the appendix. Herewith, we show the importance and also the positive influence of the similarity matrix on the separation of the clusters in the latent representation. It shows a lower separation of the different classes in the latent representation when ignoring the similarity matrix when training the model.\n\n\u2014\u2014\u2014\u2014--\n\n***- a main shortcoming is that there is no discussion or experimental comparison with methods like spectral clustering and kernel spectral clustering. Given that the paper and the proposed method relates to similarity-based representations it would be important to know how it compares to such methods. Though e.g. in Table 1 the authors compare with about 10 other methods it would be more relevant that among some of these would have been spectral clustering and kernel spectral clustering, because of the similarity-based representations.***\n\nThanks for the pointer towards spectral clustering methods. We added results from a recent ICLR publication which performs spectral clustering in various forms on MNIST and compare accuracies and NMIs to the results of our model in Table 1 and show better performance with the MoE-Sim-VAE. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper573/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper573/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixture-of-Experts Variational Autoencoder for clustering and generating from similarity-based representations", "authors": ["Andreas Kopf", "Vincent Fortuin", "Vignesh Ram Somnath", "Manfred Claassen"], "authorids": ["akopf@ethz.ch", "fortuin@inf.ethz.ch", "vsomnath@student.ethz.ch", "mclaassen@ethz.ch"], "keywords": ["Variational Autoencoder", "Clustering", "Generative model"], "abstract": "Clustering high-dimensional data, such as images or biological measurements, is a long-standing problem and has been studied extensively. Recently, Deep Clustering gained popularity due to the non-linearity of neural networks, which allows for flexibility in fitting the specific peculiarities of complex data. Here we introduce the Mixture-of-Experts Similarity Variational Autoencoder (MoE-Sim-VAE), a novel generative clustering model. The model can learn multi-modal distributions of high-dimensional data and use these to generate realistic data with high efficacy and efficiency. MoE-Sim-VAE is based on a Variational Autoencoder (VAE), where the decoder consists of a Mixture-of-Experts (MoE) architecture. This specific architecture allows for various modes of the data to be automatically learned by means of the experts. Additionally, we encourage the latent representation of our model to follow a Gaussian mixture distribution and to accurately represent the similarities between the data points. We assess the performance of our model on synthetic data, the MNIST benchmark data set, and a challenging real-world task of defining cell subpopulations from mass cytometry (CyTOF) measurements on hundreds of different datasets. MoE-Sim-VAE exhibits superior clustering performance on all these tasks in comparison to the baselines and we show that the MoE architecture in the decoder reduces the computational cost of sampling specific data modes with high fidelity.", "pdf": "/pdf/e9f65742aa83266dd94be945c0538a040ac39237.pdf", "paperhash": "kopf|mixtureofexperts_variational_autoencoder_for_clustering_and_generating_from_similaritybased_representations", "original_pdf": "/attachment/18e826c2873754d2b9fc94bf40ee467b75795685.pdf", "_bibtex": "@misc{\nkopf2020mixtureofexperts,\ntitle={Mixture-of-Experts Variational Autoencoder for clustering and generating from similarity-based representations},\nauthor={Andreas Kopf and Vincent Fortuin and Vignesh Ram Somnath and Manfred Claassen},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgs8TVtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJgs8TVtvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper573/Authors", "ICLR.cc/2020/Conference/Paper573/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper573/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper573/Reviewers", "ICLR.cc/2020/Conference/Paper573/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper573/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper573/Authors|ICLR.cc/2020/Conference/Paper573/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169425, "tmdate": 1576860529138, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper573/Authors", "ICLR.cc/2020/Conference/Paper573/Reviewers", "ICLR.cc/2020/Conference/Paper573/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper573/-/Official_Comment"}}}, {"id": "ByxBU4SEir", "original": null, "number": 2, "cdate": 1573307468853, "ddate": null, "tcdate": 1573307468853, "tmdate": 1573307468853, "tddate": null, "forum": "SJgs8TVtvr", "replyto": "H1gqaqHe5H", "invitation": "ICLR.cc/2020/Conference/Paper573/-/Official_Comment", "content": {"title": "Response to Review #3", "comment": "Thank you very much for reviewing our paper and your overall positive feedback. "}, "signatures": ["ICLR.cc/2020/Conference/Paper573/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper573/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixture-of-Experts Variational Autoencoder for clustering and generating from similarity-based representations", "authors": ["Andreas Kopf", "Vincent Fortuin", "Vignesh Ram Somnath", "Manfred Claassen"], "authorids": ["akopf@ethz.ch", "fortuin@inf.ethz.ch", "vsomnath@student.ethz.ch", "mclaassen@ethz.ch"], "keywords": ["Variational Autoencoder", "Clustering", "Generative model"], "abstract": "Clustering high-dimensional data, such as images or biological measurements, is a long-standing problem and has been studied extensively. Recently, Deep Clustering gained popularity due to the non-linearity of neural networks, which allows for flexibility in fitting the specific peculiarities of complex data. Here we introduce the Mixture-of-Experts Similarity Variational Autoencoder (MoE-Sim-VAE), a novel generative clustering model. The model can learn multi-modal distributions of high-dimensional data and use these to generate realistic data with high efficacy and efficiency. MoE-Sim-VAE is based on a Variational Autoencoder (VAE), where the decoder consists of a Mixture-of-Experts (MoE) architecture. This specific architecture allows for various modes of the data to be automatically learned by means of the experts. Additionally, we encourage the latent representation of our model to follow a Gaussian mixture distribution and to accurately represent the similarities between the data points. We assess the performance of our model on synthetic data, the MNIST benchmark data set, and a challenging real-world task of defining cell subpopulations from mass cytometry (CyTOF) measurements on hundreds of different datasets. MoE-Sim-VAE exhibits superior clustering performance on all these tasks in comparison to the baselines and we show that the MoE architecture in the decoder reduces the computational cost of sampling specific data modes with high fidelity.", "pdf": "/pdf/e9f65742aa83266dd94be945c0538a040ac39237.pdf", "paperhash": "kopf|mixtureofexperts_variational_autoencoder_for_clustering_and_generating_from_similaritybased_representations", "original_pdf": "/attachment/18e826c2873754d2b9fc94bf40ee467b75795685.pdf", "_bibtex": "@misc{\nkopf2020mixtureofexperts,\ntitle={Mixture-of-Experts Variational Autoencoder for clustering and generating from similarity-based representations},\nauthor={Andreas Kopf and Vincent Fortuin and Vignesh Ram Somnath and Manfred Claassen},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgs8TVtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJgs8TVtvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper573/Authors", "ICLR.cc/2020/Conference/Paper573/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper573/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper573/Reviewers", "ICLR.cc/2020/Conference/Paper573/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper573/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper573/Authors|ICLR.cc/2020/Conference/Paper573/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169425, "tmdate": 1576860529138, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper573/Authors", "ICLR.cc/2020/Conference/Paper573/Reviewers", "ICLR.cc/2020/Conference/Paper573/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper573/-/Official_Comment"}}}, {"id": "rygr7VrEoS", "original": null, "number": 1, "cdate": 1573307420930, "ddate": null, "tcdate": 1573307420930, "tmdate": 1573307420930, "tddate": null, "forum": "SJgs8TVtvr", "replyto": "S1gYbn7fqS", "invitation": "ICLR.cc/2020/Conference/Paper573/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "Thank you very much for reviewing our paper and your overall positive feedback. Reviewer comments are pasted and marked with ***, author responses follow below:\n\n**** It's not clear if the similarity matrix S is already solving the\n  clustering problem - in which case, why do we need the rest of the\n  model?  For example, in your experiments you often used UMAP to\n  cluster data.  How does using UMAP by itself work?  (Along these\n  lines, it was not clear if your GMM experiments clustered data in\n  the original space, or in the UMAP'd space - please clarify this).\n  A good ablation would be to somehow remove the S matrix, to see if\n  the model can accurately cluster samples.\n\nExperiments to run:\n\nAn ablation regarding the similarity matrix S.***\n\nThe similarity matrix in combination with the clustering network is a key component of the model. We followed your advice to assess in more detail whether the similarity matrix already solves the clustering problem and performed an ablation study on the similarity matrix. We did so by rerunning the experiment on MNIST and setting the loss coefficient for L_Similarity to zero and thereby effectively removing the similarity matrix from our model. As a result, we observed that the clustering network is not able to perform the clustering anymore, mainly because the separation of the different classes in the latent representation is heavily impaired. We added the results to the revised version of our manuscript to section 4.2 and additionally with a figure (A4) in the appendix.\n\n\u2014\u2014\u2014\u2014--\n\n***Clarification of whether GMM experiments are run in data-space, or\nUMAP'd space.***\n\nSimilarly, as in our model, we run the GMM experiments also on the data space. The UMAP projection is only used as a transformation and similarity measure to include domain knowledge encoded in the similarity matrix and therefore also to show the advantage of our model. \n\n\u2014\u2014\u2014\u2014--\n\n**** There is not a one-to-one mapping of clusters to labels, so it is\n  hard to use this method to generate a specific type of data (for\n  example, it is hard to generate a specific digit).  This is a big\n  difference from, say, a conditional sampler as learned by a GAN.\n  This also arises in Fig. 3, where it is clear that latent cluster\n  assignments do not match human-interpretable cluster assignments.  I\n  suppose this is to be expected, but taken with the previous point\n  (little variance in generated samples) I think it seriously weakens\n  the paper's claim that this is an \"accurate an efficient data\n  generation method.\"***\n\nWe fully agree with your feedback on the one-to-one mapping between cluster-ID and labels. This is indeed a disadvantage in comparison to the conditional GANs you mentioned. Future work could incorporate a feature in the framework to be able to condition on the label for generation purposes. Nevertheless, we believe that currently with little effort one can interpret the latent representation and therefore also find a mapping between the experts and label IDs.\n\n\u2014\u2014\u2014\u2014--\n\n**** There is little variance in the generated samples. ***\n\nWe discuss this issue in the conclusion. It is a known \u201cproblem\u201d with VAE that the sample variances and also the sharpness of generated images is lower in comparison to for example images generated with GANs [Dumoulin et al. \u201cAdversarially Learned Inference\u201d;  Theis et al. \u201cA note on the evaluation of generative models\u201d]. We expect that adding adversarial training could be a remedy to generate even more realistic samples as discussed in the conclusion. \n\n\u2014\u2014\u2014\u2014--\n\n***MIXAE features prominently in your related works, but is not compared\nto in your experiments.  It sounds like a natural comparison.  Please\nrun this experiment, or explain why it is not a comparable method.***\n\nDue to an oversight on our end, we missed to include the results from MIXAE, which we discuss in our related work section. We added them to the manuscript now (Table 1).\n\nWe hope that this addresses your questions and concerns. If you have any other suggestions on how we could improve our paper, please do let us know.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper573/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper573/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixture-of-Experts Variational Autoencoder for clustering and generating from similarity-based representations", "authors": ["Andreas Kopf", "Vincent Fortuin", "Vignesh Ram Somnath", "Manfred Claassen"], "authorids": ["akopf@ethz.ch", "fortuin@inf.ethz.ch", "vsomnath@student.ethz.ch", "mclaassen@ethz.ch"], "keywords": ["Variational Autoencoder", "Clustering", "Generative model"], "abstract": "Clustering high-dimensional data, such as images or biological measurements, is a long-standing problem and has been studied extensively. Recently, Deep Clustering gained popularity due to the non-linearity of neural networks, which allows for flexibility in fitting the specific peculiarities of complex data. Here we introduce the Mixture-of-Experts Similarity Variational Autoencoder (MoE-Sim-VAE), a novel generative clustering model. The model can learn multi-modal distributions of high-dimensional data and use these to generate realistic data with high efficacy and efficiency. MoE-Sim-VAE is based on a Variational Autoencoder (VAE), where the decoder consists of a Mixture-of-Experts (MoE) architecture. This specific architecture allows for various modes of the data to be automatically learned by means of the experts. Additionally, we encourage the latent representation of our model to follow a Gaussian mixture distribution and to accurately represent the similarities between the data points. We assess the performance of our model on synthetic data, the MNIST benchmark data set, and a challenging real-world task of defining cell subpopulations from mass cytometry (CyTOF) measurements on hundreds of different datasets. MoE-Sim-VAE exhibits superior clustering performance on all these tasks in comparison to the baselines and we show that the MoE architecture in the decoder reduces the computational cost of sampling specific data modes with high fidelity.", "pdf": "/pdf/e9f65742aa83266dd94be945c0538a040ac39237.pdf", "paperhash": "kopf|mixtureofexperts_variational_autoencoder_for_clustering_and_generating_from_similaritybased_representations", "original_pdf": "/attachment/18e826c2873754d2b9fc94bf40ee467b75795685.pdf", "_bibtex": "@misc{\nkopf2020mixtureofexperts,\ntitle={Mixture-of-Experts Variational Autoencoder for clustering and generating from similarity-based representations},\nauthor={Andreas Kopf and Vincent Fortuin and Vignesh Ram Somnath and Manfred Claassen},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgs8TVtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJgs8TVtvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper573/Authors", "ICLR.cc/2020/Conference/Paper573/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper573/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper573/Reviewers", "ICLR.cc/2020/Conference/Paper573/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper573/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper573/Authors|ICLR.cc/2020/Conference/Paper573/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169425, "tmdate": 1576860529138, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper573/Authors", "ICLR.cc/2020/Conference/Paper573/Reviewers", "ICLR.cc/2020/Conference/Paper573/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper573/-/Official_Comment"}}}, {"id": "H1gaakiptr", "original": null, "number": 1, "cdate": 1571823556530, "ddate": null, "tcdate": 1571823556530, "tmdate": 1572972578534, "tddate": null, "forum": "SJgs8TVtvr", "replyto": "SJgs8TVtvr", "invitation": "ICLR.cc/2020/Conference/Paper573/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The proposed method of mixture-of-experts variational autoencoders\nis valuable and insightful.\nOn the other hand the work could be improved and clarified at some points:\n\n- in the abstract it is claimed that the method works for high-dimensional data.However, it should be better explained why this is the case. The method is largely based on density estimation with a mixture of Gaussians which is known to have limitations in higher dimensions (see e.g. classical textbooks like Bishop 1995)\n\n- the similarity matrix and the similarity values should be carefully defined. Is there also an underlying similarity function assumed?\n\n- a main shortcoming is that there is no discussion or experimental comparison with methods like spectral clustering and kernel spectral clustering. Given that the paper and the proposed method relates to similarity-based representations it would be important to know how it compares to such methods. Though e.g. in Table 1 the authors compare with about 10 other methods it would be more relevant that among some of these would have been spectral clustering and kernel spectral clustering, because of the similarity-based representations.\n\n- in section 4.1 the MNIST data are taken with k=10. Though it is nicely explained and illustrated on this data set, it is possibly somewhat misleading as an example. The reason is that this is a classification problem with 10 classes, therefore the choice k=10 is obvious. It would be more important to consider benchmark problems for clustering, instead of classification, for which the choice of k is also an important model selection issue and for which k is unknown (how should k be selected then?).\n\n- is each cluster always be assumed to be a Gaussian (which seems to be a strong assumption in general, and possibly not always realistic)? Could other components be used in the mixture?"}, "signatures": ["ICLR.cc/2020/Conference/Paper573/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper573/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixture-of-Experts Variational Autoencoder for clustering and generating from similarity-based representations", "authors": ["Andreas Kopf", "Vincent Fortuin", "Vignesh Ram Somnath", "Manfred Claassen"], "authorids": ["akopf@ethz.ch", "fortuin@inf.ethz.ch", "vsomnath@student.ethz.ch", "mclaassen@ethz.ch"], "keywords": ["Variational Autoencoder", "Clustering", "Generative model"], "abstract": "Clustering high-dimensional data, such as images or biological measurements, is a long-standing problem and has been studied extensively. Recently, Deep Clustering gained popularity due to the non-linearity of neural networks, which allows for flexibility in fitting the specific peculiarities of complex data. Here we introduce the Mixture-of-Experts Similarity Variational Autoencoder (MoE-Sim-VAE), a novel generative clustering model. The model can learn multi-modal distributions of high-dimensional data and use these to generate realistic data with high efficacy and efficiency. MoE-Sim-VAE is based on a Variational Autoencoder (VAE), where the decoder consists of a Mixture-of-Experts (MoE) architecture. This specific architecture allows for various modes of the data to be automatically learned by means of the experts. Additionally, we encourage the latent representation of our model to follow a Gaussian mixture distribution and to accurately represent the similarities between the data points. We assess the performance of our model on synthetic data, the MNIST benchmark data set, and a challenging real-world task of defining cell subpopulations from mass cytometry (CyTOF) measurements on hundreds of different datasets. MoE-Sim-VAE exhibits superior clustering performance on all these tasks in comparison to the baselines and we show that the MoE architecture in the decoder reduces the computational cost of sampling specific data modes with high fidelity.", "pdf": "/pdf/e9f65742aa83266dd94be945c0538a040ac39237.pdf", "paperhash": "kopf|mixtureofexperts_variational_autoencoder_for_clustering_and_generating_from_similaritybased_representations", "original_pdf": "/attachment/18e826c2873754d2b9fc94bf40ee467b75795685.pdf", "_bibtex": "@misc{\nkopf2020mixtureofexperts,\ntitle={Mixture-of-Experts Variational Autoencoder for clustering and generating from similarity-based representations},\nauthor={Andreas Kopf and Vincent Fortuin and Vignesh Ram Somnath and Manfred Claassen},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgs8TVtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJgs8TVtvr", "replyto": "SJgs8TVtvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper573/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper573/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575629190727, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper573/Reviewers"], "noninvitees": [], "tcdate": 1570237750175, "tmdate": 1575629190742, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper573/-/Official_Review"}}}, {"id": "H1gqaqHe5H", "original": null, "number": 2, "cdate": 1571998401570, "ddate": null, "tcdate": 1571998401570, "tmdate": 1572972578485, "tddate": null, "forum": "SJgs8TVtvr", "replyto": "SJgs8TVtvr", "invitation": "ICLR.cc/2020/Conference/Paper573/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The authors present an extension of variational autoencoders (VAEs), where Gaussian distribution of the latent variable is replaced by a mixture of Gaussians. The approach can be used for clustering and generation. The authors carry out experiments to evaluate the performance of the method in these tasks and compare it to competing methods.\n\nThe paper is well written and easy to read and understand. Specialized related work is discussed. I find the extension of VAEs to GMMs interesting for the ICLR community, although it is somewhat straight forward in terms of its technical difficulty. However, the technical novelty together with the fine empirical evaluation are just good enough for ICLR, in my opinion.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper573/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper573/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixture-of-Experts Variational Autoencoder for clustering and generating from similarity-based representations", "authors": ["Andreas Kopf", "Vincent Fortuin", "Vignesh Ram Somnath", "Manfred Claassen"], "authorids": ["akopf@ethz.ch", "fortuin@inf.ethz.ch", "vsomnath@student.ethz.ch", "mclaassen@ethz.ch"], "keywords": ["Variational Autoencoder", "Clustering", "Generative model"], "abstract": "Clustering high-dimensional data, such as images or biological measurements, is a long-standing problem and has been studied extensively. Recently, Deep Clustering gained popularity due to the non-linearity of neural networks, which allows for flexibility in fitting the specific peculiarities of complex data. Here we introduce the Mixture-of-Experts Similarity Variational Autoencoder (MoE-Sim-VAE), a novel generative clustering model. The model can learn multi-modal distributions of high-dimensional data and use these to generate realistic data with high efficacy and efficiency. MoE-Sim-VAE is based on a Variational Autoencoder (VAE), where the decoder consists of a Mixture-of-Experts (MoE) architecture. This specific architecture allows for various modes of the data to be automatically learned by means of the experts. Additionally, we encourage the latent representation of our model to follow a Gaussian mixture distribution and to accurately represent the similarities between the data points. We assess the performance of our model on synthetic data, the MNIST benchmark data set, and a challenging real-world task of defining cell subpopulations from mass cytometry (CyTOF) measurements on hundreds of different datasets. MoE-Sim-VAE exhibits superior clustering performance on all these tasks in comparison to the baselines and we show that the MoE architecture in the decoder reduces the computational cost of sampling specific data modes with high fidelity.", "pdf": "/pdf/e9f65742aa83266dd94be945c0538a040ac39237.pdf", "paperhash": "kopf|mixtureofexperts_variational_autoencoder_for_clustering_and_generating_from_similaritybased_representations", "original_pdf": "/attachment/18e826c2873754d2b9fc94bf40ee467b75795685.pdf", "_bibtex": "@misc{\nkopf2020mixtureofexperts,\ntitle={Mixture-of-Experts Variational Autoencoder for clustering and generating from similarity-based representations},\nauthor={Andreas Kopf and Vincent Fortuin and Vignesh Ram Somnath and Manfred Claassen},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgs8TVtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJgs8TVtvr", "replyto": "SJgs8TVtvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper573/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper573/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575629190727, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper573/Reviewers"], "noninvitees": [], "tcdate": 1570237750175, "tmdate": 1575629190742, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper573/-/Official_Review"}}}, {"id": "S1gYbn7fqS", "original": null, "number": 3, "cdate": 1572121601129, "ddate": null, "tcdate": 1572121601129, "tmdate": 1572972578434, "tddate": null, "forum": "SJgs8TVtvr", "replyto": "SJgs8TVtvr", "invitation": "ICLR.cc/2020/Conference/Paper573/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "\nSummary:\n\nThe paper proposes to expand the VAE architecture with a\nmixture-of-experts latent representation, with a\nmixture-component-specific decoder that can specialize in a specific\ncluster.  Importantly, the method can take advantage of a similarity\nmatrix to help with the clustering.\n\nOverall, I recommend a weak accept.  The method seems reasonable, and\nthe paper is well-written, but the results are only marginally better\nthan other methods, and there are several weaknesses with the proposed\narchitecture and experimental setup.\n\nPositives:\n\n* The idea of a more expressive variational distribution seems good,\n  although it is not novel.\n\n* The ability to have multiple decoder networks seems reasonable.\n\n* The ability to incorporate domain knowledge (in the form of a\n  similarity matrix S) is a plus.\n\n* The experiments are thorough, although the method is generally only\n  slightly better than competing methods.\n\nNegatives:\n\n* It's not clear if the similarity matrix S is already solving the\n  clustering problem - in which case, why do we need the rest of the\n  model?  For example, in your experiments you often used UMAP to\n  cluster data.  How does using UMAP by itself work?  (Along these\n  lines, it was not clear if your GMM experiments clustered data in\n  the original space, or in the UMAP'd space - please clarify this).\n  A good ablation would be to somehow remove the S matrix, to see if\n  the model can accurately cluster samples.\n\n* There is little variance in the generated samples.  \n\n* There is not a one-to-one mapping of clusters to labels, so it is\n  hard to use this method to generate a specific type of data (for\n  example, it is hard to generate a specific digit).  This is a big\n  difference from, say, a conditional sampler as learned by a GAN.\n  This also arises in Fig. 3, where it is clear that latent cluster\n  assignments do not match human-interpretable cluster assignments.  I\n  suppose this is to be expected, but taken with the previous point\n  (little variance in generated samples) I think it seriously weakens\n  the paper's claim that this is an \"accurate an efficient data\n  generation method.\"\n\n* The method does not do well when the number of clusters is large.\n  Regular GMMs seem to outperform it.\n\n* I felt that this paper made excessive use of the appendix.  The\n  paper is not self-contained enough, effectively violating the length\n  restrictions.  Please make an effort to move key results back in to\n  the main body of the paper.\n\n\nExperiments to run:\n\nAn ablation regarding the similarity matrix S.\n\nClarification of whether GMM experiments are run in data-space, or\nUMAP'd space.\n\nMIXAE features prominently in your related works, but is not compared\nto in your experiments.  It sounds like a natural comparison.  Please\nrun this experiment, or explain why it is not a comparable method.\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper573/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper573/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixture-of-Experts Variational Autoencoder for clustering and generating from similarity-based representations", "authors": ["Andreas Kopf", "Vincent Fortuin", "Vignesh Ram Somnath", "Manfred Claassen"], "authorids": ["akopf@ethz.ch", "fortuin@inf.ethz.ch", "vsomnath@student.ethz.ch", "mclaassen@ethz.ch"], "keywords": ["Variational Autoencoder", "Clustering", "Generative model"], "abstract": "Clustering high-dimensional data, such as images or biological measurements, is a long-standing problem and has been studied extensively. Recently, Deep Clustering gained popularity due to the non-linearity of neural networks, which allows for flexibility in fitting the specific peculiarities of complex data. Here we introduce the Mixture-of-Experts Similarity Variational Autoencoder (MoE-Sim-VAE), a novel generative clustering model. The model can learn multi-modal distributions of high-dimensional data and use these to generate realistic data with high efficacy and efficiency. MoE-Sim-VAE is based on a Variational Autoencoder (VAE), where the decoder consists of a Mixture-of-Experts (MoE) architecture. This specific architecture allows for various modes of the data to be automatically learned by means of the experts. Additionally, we encourage the latent representation of our model to follow a Gaussian mixture distribution and to accurately represent the similarities between the data points. We assess the performance of our model on synthetic data, the MNIST benchmark data set, and a challenging real-world task of defining cell subpopulations from mass cytometry (CyTOF) measurements on hundreds of different datasets. MoE-Sim-VAE exhibits superior clustering performance on all these tasks in comparison to the baselines and we show that the MoE architecture in the decoder reduces the computational cost of sampling specific data modes with high fidelity.", "pdf": "/pdf/e9f65742aa83266dd94be945c0538a040ac39237.pdf", "paperhash": "kopf|mixtureofexperts_variational_autoencoder_for_clustering_and_generating_from_similaritybased_representations", "original_pdf": "/attachment/18e826c2873754d2b9fc94bf40ee467b75795685.pdf", "_bibtex": "@misc{\nkopf2020mixtureofexperts,\ntitle={Mixture-of-Experts Variational Autoencoder for clustering and generating from similarity-based representations},\nauthor={Andreas Kopf and Vincent Fortuin and Vignesh Ram Somnath and Manfred Claassen},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgs8TVtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJgs8TVtvr", "replyto": "SJgs8TVtvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper573/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper573/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575629190727, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper573/Reviewers"], "noninvitees": [], "tcdate": 1570237750175, "tmdate": 1575629190742, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper573/-/Official_Review"}}}], "count": 9}