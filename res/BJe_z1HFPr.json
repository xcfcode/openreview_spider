{"notes": [{"id": "BJe_z1HFPr", "original": "B1xHFG2_wr", "number": 1584, "cdate": 1569439503552, "ddate": null, "tcdate": 1569439503552, "tmdate": 1577168240925, "tddate": null, "forum": "BJe_z1HFPr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Resizable Neural Networks", "authors": ["Yichen Zhu", "Xiangyu Zhang", "Tong Yang", "Jian Sun"], "authorids": ["k.zhu@mail.utoronto.ca", "zhangxiangyu@megvii.com", "yangtong@megvii.com", "sunjian@megvii.com"], "keywords": [], "abstract": "\nIn this paper, we present a deep convolutional neural network (CNN) which performs arbitrary resize operation on intermediate feature map resolution at stage-level. Motivated by weight sharing mechanism in neural architecture search, where a super-network is trained and sub-networks inherit the weights from the super-network, we present a novel CNN approach. We construct a spatial super-network which consists of multiple sub-networks, where each sub-network is a single scale network that obtain a unique spatial configuration, the convolutional layers are shared across all sub-networks. Such network, named as Resizable Neural Networks, are equivalent to training infinite single scale networks, but has no extra computational cost. Moreover, we present a training algorithm such that all sub-networks achieve better performance than individually trained counterparts. On large-scale ImageNet classification, we demonstrate its effectiveness on various modern network architectures such as MobileNet, ShuffleNet, and ResNet.\n\n\nTo go even further, we present three variants of resizable networks: 1) Resizable as Architecture Search (Resizable-NAS). On ImageNet, Resizable-NAS ResNet-50 attain 0.4% higher on accuracy and 44% smaller than the baseline model. 2) Resizable as Data Augmentation (Resizable-Aug). While we use resizable networks as a data augmentation technique, it obtains superior performance on ImageNet classification, outperform AutoAugment by 1.2% with ResNet-50. 3) Adaptive Resizable Network (Resizable-Adapt). We introduce the adaptive resizable networks as dynamic networks, which further improve the performance with less computational cost via data-dependent inference.", "pdf": "/pdf/02c1a8f8b0e38f18d9a6d5dcb84ae6c95095007e.pdf", "paperhash": "zhu|resizable_neural_networks", "original_pdf": "/attachment/287397a239b7d18c355cbf123933f94d305c7da3.pdf", "_bibtex": "@misc{\nzhu2020resizable,\ntitle={Resizable Neural Networks},\nauthor={Yichen Zhu and Xiangyu Zhang and Tong Yang and Jian Sun},\nyear={2020},\nurl={https://openreview.net/forum?id=BJe_z1HFPr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "KzPCNG-WpX", "original": null, "number": 1, "cdate": 1576944644234, "ddate": null, "tcdate": 1576944644234, "tmdate": 1576945140418, "tddate": null, "forum": "BJe_z1HFPr", "replyto": "BJe_z1HFPr", "invitation": "ICLR.cc/2020/Conference/Paper1584/-/Public_Comment", "content": {"title": "Concerns about the novelty of this work", "comment": "I have great concerns about the novelty of this work and do not quite agree with the opinions of PCs and reviewers that this work is novel.\n\nThe essential idea of this paper is very similar to that of our \"Stochastic Downsampling for Cost-Adjustable Inference and Improved Regularization in Convolutional Networks\" paper published at CVPR 2018. The methods proposed by these two works essentially focus on training multiple sub-networks sharing the weights of a single full network based on different feature map resizing/downsampling configurations. The main objective of these two works is to enable test-time evaluation of different sub-networks (each has a different accuracy-efficiency operating point) obtained from a single model without re-training or fine-tuning. Both also share the identical idea of having separate Batch Normalization statistics for the different sub-networks -- known as Scale-Aware Batch Normalization in this work and Instance-Specific Batch Normalization in Stochastic Downsampling. Despite the strong similarities, this work does not mention or cite our Stochastic Downsampling paper at all.\n\nThe major difference between the two works is the \"search\" space for the feature map resizing/downsampling. Resizable Neural Networks resizes the feature map for each stage by randomly sampling a size (scaling ratio) from a predefined range of sizes. Whereas, Stochastic Downsampling samples a random layer index and a downsampling ratio for resizing the selected layer's feature map. I would say the fair sampling method introduced by this work is quite novel in the context of neural networks with multiple accuracy-efficiency operating points. But the accuracy gains shown in Table 10 (page 18) are apparently neither consistent nor significant.\n\nI hope the authors will take the Stochastic Downsampling paper into good consideration and include it as a related work in the future version of this work."}, "signatures": ["~Jason_Kuen1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Jason_Kuen1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Resizable Neural Networks", "authors": ["Yichen Zhu", "Xiangyu Zhang", "Tong Yang", "Jian Sun"], "authorids": ["k.zhu@mail.utoronto.ca", "zhangxiangyu@megvii.com", "yangtong@megvii.com", "sunjian@megvii.com"], "keywords": [], "abstract": "\nIn this paper, we present a deep convolutional neural network (CNN) which performs arbitrary resize operation on intermediate feature map resolution at stage-level. Motivated by weight sharing mechanism in neural architecture search, where a super-network is trained and sub-networks inherit the weights from the super-network, we present a novel CNN approach. We construct a spatial super-network which consists of multiple sub-networks, where each sub-network is a single scale network that obtain a unique spatial configuration, the convolutional layers are shared across all sub-networks. Such network, named as Resizable Neural Networks, are equivalent to training infinite single scale networks, but has no extra computational cost. Moreover, we present a training algorithm such that all sub-networks achieve better performance than individually trained counterparts. On large-scale ImageNet classification, we demonstrate its effectiveness on various modern network architectures such as MobileNet, ShuffleNet, and ResNet.\n\n\nTo go even further, we present three variants of resizable networks: 1) Resizable as Architecture Search (Resizable-NAS). On ImageNet, Resizable-NAS ResNet-50 attain 0.4% higher on accuracy and 44% smaller than the baseline model. 2) Resizable as Data Augmentation (Resizable-Aug). While we use resizable networks as a data augmentation technique, it obtains superior performance on ImageNet classification, outperform AutoAugment by 1.2% with ResNet-50. 3) Adaptive Resizable Network (Resizable-Adapt). We introduce the adaptive resizable networks as dynamic networks, which further improve the performance with less computational cost via data-dependent inference.", "pdf": "/pdf/02c1a8f8b0e38f18d9a6d5dcb84ae6c95095007e.pdf", "paperhash": "zhu|resizable_neural_networks", "original_pdf": "/attachment/287397a239b7d18c355cbf123933f94d305c7da3.pdf", "_bibtex": "@misc{\nzhu2020resizable,\ntitle={Resizable Neural Networks},\nauthor={Yichen Zhu and Xiangyu Zhang and Tong Yang and Jian Sun},\nyear={2020},\nurl={https://openreview.net/forum?id=BJe_z1HFPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJe_z1HFPr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504192675, "tmdate": 1576860584070, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1584/Authors", "ICLR.cc/2020/Conference/Paper1584/Reviewers", "ICLR.cc/2020/Conference/Paper1584/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1584/-/Public_Comment"}}}, {"id": "NMEreOFQEY", "original": null, "number": 1, "cdate": 1576798727210, "ddate": null, "tcdate": 1576798727210, "tmdate": 1576800909293, "tddate": null, "forum": "BJe_z1HFPr", "replyto": "BJe_z1HFPr", "invitation": "ICLR.cc/2020/Conference/Paper1584/-/Decision", "content": {"decision": "Reject", "comment": "This paper offers likely novel schemes for image resizing.  The performance improvement is clear.  Unfortunately two reviewers find substantial clarity issues in the manuscript after revision, and the AC concurs that this is still an issue.  The paper is borderline but given the number of higher ranked papers in the pool is unable to be accepted unfortunately. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Resizable Neural Networks", "authors": ["Yichen Zhu", "Xiangyu Zhang", "Tong Yang", "Jian Sun"], "authorids": ["k.zhu@mail.utoronto.ca", "zhangxiangyu@megvii.com", "yangtong@megvii.com", "sunjian@megvii.com"], "keywords": [], "abstract": "\nIn this paper, we present a deep convolutional neural network (CNN) which performs arbitrary resize operation on intermediate feature map resolution at stage-level. Motivated by weight sharing mechanism in neural architecture search, where a super-network is trained and sub-networks inherit the weights from the super-network, we present a novel CNN approach. We construct a spatial super-network which consists of multiple sub-networks, where each sub-network is a single scale network that obtain a unique spatial configuration, the convolutional layers are shared across all sub-networks. Such network, named as Resizable Neural Networks, are equivalent to training infinite single scale networks, but has no extra computational cost. Moreover, we present a training algorithm such that all sub-networks achieve better performance than individually trained counterparts. On large-scale ImageNet classification, we demonstrate its effectiveness on various modern network architectures such as MobileNet, ShuffleNet, and ResNet.\n\n\nTo go even further, we present three variants of resizable networks: 1) Resizable as Architecture Search (Resizable-NAS). On ImageNet, Resizable-NAS ResNet-50 attain 0.4% higher on accuracy and 44% smaller than the baseline model. 2) Resizable as Data Augmentation (Resizable-Aug). While we use resizable networks as a data augmentation technique, it obtains superior performance on ImageNet classification, outperform AutoAugment by 1.2% with ResNet-50. 3) Adaptive Resizable Network (Resizable-Adapt). We introduce the adaptive resizable networks as dynamic networks, which further improve the performance with less computational cost via data-dependent inference.", "pdf": "/pdf/02c1a8f8b0e38f18d9a6d5dcb84ae6c95095007e.pdf", "paperhash": "zhu|resizable_neural_networks", "original_pdf": "/attachment/287397a239b7d18c355cbf123933f94d305c7da3.pdf", "_bibtex": "@misc{\nzhu2020resizable,\ntitle={Resizable Neural Networks},\nauthor={Yichen Zhu and Xiangyu Zhang and Tong Yang and Jian Sun},\nyear={2020},\nurl={https://openreview.net/forum?id=BJe_z1HFPr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BJe_z1HFPr", "replyto": "BJe_z1HFPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795705120, "tmdate": 1576800252827, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1584/-/Decision"}}}, {"id": "BygNawPTYS", "original": null, "number": 2, "cdate": 1571809212127, "ddate": null, "tcdate": 1571809212127, "tmdate": 1574407238590, "tddate": null, "forum": "BJe_z1HFPr", "replyto": "BJe_z1HFPr", "invitation": "ICLR.cc/2020/Conference/Paper1584/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "This paper proposes Resizable Neural Networks, which trains networks with different resolution scalings at the same time with shared weights. It serves as data-augmentation and improves accuracy over base networks. Additionally, the same technique can perform an architecture search. Experimental results show significant accuracy gains.\n\nThe reported accuracy gains are substantial. The proposed method is potentially useful in many applications. However, several details are missing or hard to understand. Without additional descriptions, it is not straightforward to implement the method. Thus, I suggest for rejection. The score might be raised depending on updates and code release.\n\nMajor comments:\n1) Algorithm 1 has \"predefined spatial list L.\" How to choose it in practice?\n2) Algorithm 1 indicates that the training time is len(L) times longer. Additionally, according to the implementation details, Resizable Networks are trained two times many epochs. It seems hard to justify such a longer training time.\n3) This is similar to (2), but why Resizable-NAS is better than the all len(L) models separately to find better architectures?\n4) In Sec. 3.4, how the \"target model\" is selected after the training of Resizable Networks?\n5) Why is Resizable-Adapt omitted from Table 2?\n6) References are out-dated. For example, there are no \"Figure 5.\"\n\nMinor comments:\n1) Are any ablation studies available for Fair Sampling?\n\n===== Update\n\nThank you for the response and update. The revision made the paper easier to understand. However, I still have concerns about the presentation of the paper, and I keep my score. I think the novelty and experimental results are significant and match the bar of ICLR. If the other two reviewers think that the revised paper is fairly well-written and recommend acceptance, I will not challenge the decision.\n\nMajor comments:\n1) I do not understand how the training times of random-sampling in Appendix C were estimated.\n2) Concerning Table 11, how was the result when we randomly select each scale and train network with the same number of epochs with fair-sampling? If there were no much difference with the result in Table 11, it seems fair sampling does not have clear advantages over random sampling. If so, I suggest to alter fiar-sampling by random-sampling and reduce the complexity of the proposed method.\n\nMinor comments:\n1) Table 10 has two captions.\n2) I did not understand that L is a list of possible scaling factors (scalar) in the initial review. I guess it is partially because I did not understand why the number of sampling should be len(L) for each mini-batch (it is actually due to fair-sampling). I think adding some remarks on it will help to make the pseudo-code easier to understand. (nit: for j= 0, ..., len(L) should be for j=0, ..., len(L) -1 or for j=1,  ..., len(L))", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper1584/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1584/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Resizable Neural Networks", "authors": ["Yichen Zhu", "Xiangyu Zhang", "Tong Yang", "Jian Sun"], "authorids": ["k.zhu@mail.utoronto.ca", "zhangxiangyu@megvii.com", "yangtong@megvii.com", "sunjian@megvii.com"], "keywords": [], "abstract": "\nIn this paper, we present a deep convolutional neural network (CNN) which performs arbitrary resize operation on intermediate feature map resolution at stage-level. Motivated by weight sharing mechanism in neural architecture search, where a super-network is trained and sub-networks inherit the weights from the super-network, we present a novel CNN approach. We construct a spatial super-network which consists of multiple sub-networks, where each sub-network is a single scale network that obtain a unique spatial configuration, the convolutional layers are shared across all sub-networks. Such network, named as Resizable Neural Networks, are equivalent to training infinite single scale networks, but has no extra computational cost. Moreover, we present a training algorithm such that all sub-networks achieve better performance than individually trained counterparts. On large-scale ImageNet classification, we demonstrate its effectiveness on various modern network architectures such as MobileNet, ShuffleNet, and ResNet.\n\n\nTo go even further, we present three variants of resizable networks: 1) Resizable as Architecture Search (Resizable-NAS). On ImageNet, Resizable-NAS ResNet-50 attain 0.4% higher on accuracy and 44% smaller than the baseline model. 2) Resizable as Data Augmentation (Resizable-Aug). While we use resizable networks as a data augmentation technique, it obtains superior performance on ImageNet classification, outperform AutoAugment by 1.2% with ResNet-50. 3) Adaptive Resizable Network (Resizable-Adapt). We introduce the adaptive resizable networks as dynamic networks, which further improve the performance with less computational cost via data-dependent inference.", "pdf": "/pdf/02c1a8f8b0e38f18d9a6d5dcb84ae6c95095007e.pdf", "paperhash": "zhu|resizable_neural_networks", "original_pdf": "/attachment/287397a239b7d18c355cbf123933f94d305c7da3.pdf", "_bibtex": "@misc{\nzhu2020resizable,\ntitle={Resizable Neural Networks},\nauthor={Yichen Zhu and Xiangyu Zhang and Tong Yang and Jian Sun},\nyear={2020},\nurl={https://openreview.net/forum?id=BJe_z1HFPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJe_z1HFPr", "replyto": "BJe_z1HFPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1584/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1584/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575917871629, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1584/Reviewers"], "noninvitees": [], "tcdate": 1570237735252, "tmdate": 1575917871640, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1584/-/Official_Review"}}}, {"id": "HylWQ0t2ir", "original": null, "number": 3, "cdate": 1573850649012, "ddate": null, "tcdate": 1573850649012, "tmdate": 1573850649012, "tddate": null, "forum": "BJe_z1HFPr", "replyto": "Syl6fYR_OH", "invitation": "ICLR.cc/2020/Conference/Paper1584/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "Reviewer 3: \nThanks for your detailed and thoughtful reviews as well as your helpful suggestions.\n\nQ1: Regarding writing, grammatical error, typos.\nA1: We improved the writing and fixed the typos. We enlarged Figure 2 for readability. As you suggested, we weaken the idea of sub-networks. The link of distillation paper you mentioned is out-dated, we will include this in our reference if you can kindly provide the name of the paper.\n\nQ2: Scale up resizable network?\nA2: The resizable network can indeed scale up. We did preliminary experiments on scale up ResNet-50 by 4x times larger FLOPs and achieve better performance we reported in EfficientNet paper (Table 3). We will consider your advice and do more experiments to see the possibility of scaling up the resizable network.\n\nQ3: More details and ablation study on fair sampling.\nA3: We improved writing and included more descriptions of fair sampling. We added an ablation study on fair sampling. \n\nQ4: Regarding the scaling factor combination.\nA4: We included the choice of scaling factor combination we used in the experiments section. We added a new section in Appendix A to discuss the impact of the choice of scaling factor on our method.\n\nQ5: In Section 3.5 I'm not sure what is meant by \"It is inefficient to process all images to one-subnetwork, as the algorithm spends equal energy at each sample\".  I assume energy use is proportional to the number of FLOPS, which in turn depends on spatial resolution.\nA5: Yes. We understand the description of energy can be ambiguous; thus, we make a revision on Section 3.5 (Section 4.3 in new revision).\n\nQ6: As ImageNet is the only dataset considered in the paper, could you provide error bars?\nA6: At this time, we can not provide error bars due to limited time constraints. We will consider your advice to provide error bars and do more experiments on other datasets (other tasks) in the future revision."}, "signatures": ["ICLR.cc/2020/Conference/Paper1584/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1584/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Resizable Neural Networks", "authors": ["Yichen Zhu", "Xiangyu Zhang", "Tong Yang", "Jian Sun"], "authorids": ["k.zhu@mail.utoronto.ca", "zhangxiangyu@megvii.com", "yangtong@megvii.com", "sunjian@megvii.com"], "keywords": [], "abstract": "\nIn this paper, we present a deep convolutional neural network (CNN) which performs arbitrary resize operation on intermediate feature map resolution at stage-level. Motivated by weight sharing mechanism in neural architecture search, where a super-network is trained and sub-networks inherit the weights from the super-network, we present a novel CNN approach. We construct a spatial super-network which consists of multiple sub-networks, where each sub-network is a single scale network that obtain a unique spatial configuration, the convolutional layers are shared across all sub-networks. Such network, named as Resizable Neural Networks, are equivalent to training infinite single scale networks, but has no extra computational cost. Moreover, we present a training algorithm such that all sub-networks achieve better performance than individually trained counterparts. On large-scale ImageNet classification, we demonstrate its effectiveness on various modern network architectures such as MobileNet, ShuffleNet, and ResNet.\n\n\nTo go even further, we present three variants of resizable networks: 1) Resizable as Architecture Search (Resizable-NAS). On ImageNet, Resizable-NAS ResNet-50 attain 0.4% higher on accuracy and 44% smaller than the baseline model. 2) Resizable as Data Augmentation (Resizable-Aug). While we use resizable networks as a data augmentation technique, it obtains superior performance on ImageNet classification, outperform AutoAugment by 1.2% with ResNet-50. 3) Adaptive Resizable Network (Resizable-Adapt). We introduce the adaptive resizable networks as dynamic networks, which further improve the performance with less computational cost via data-dependent inference.", "pdf": "/pdf/02c1a8f8b0e38f18d9a6d5dcb84ae6c95095007e.pdf", "paperhash": "zhu|resizable_neural_networks", "original_pdf": "/attachment/287397a239b7d18c355cbf123933f94d305c7da3.pdf", "_bibtex": "@misc{\nzhu2020resizable,\ntitle={Resizable Neural Networks},\nauthor={Yichen Zhu and Xiangyu Zhang and Tong Yang and Jian Sun},\nyear={2020},\nurl={https://openreview.net/forum?id=BJe_z1HFPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJe_z1HFPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1584/Authors", "ICLR.cc/2020/Conference/Paper1584/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1584/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1584/Reviewers", "ICLR.cc/2020/Conference/Paper1584/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1584/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1584/Authors|ICLR.cc/2020/Conference/Paper1584/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153857, "tmdate": 1576860550814, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1584/Authors", "ICLR.cc/2020/Conference/Paper1584/Reviewers", "ICLR.cc/2020/Conference/Paper1584/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1584/-/Official_Comment"}}}, {"id": "ByxPE3YniH", "original": null, "number": 2, "cdate": 1573850159230, "ddate": null, "tcdate": 1573850159230, "tmdate": 1573850159230, "tddate": null, "forum": "BJe_z1HFPr", "replyto": "BygNawPTYS", "invitation": "ICLR.cc/2020/Conference/Paper1584/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "Thanks for your detailed and thoughtful reviews. We have addressed all the questions below:\n\nQ1: Implementations details and code release.\nA1: We understand that in our previous version, several details are missing, and some concepts are hard to understand. Besides improving writing and fixing the typos, we included more details (both method descriptions and experiment settings) in the new revision. We are working on cleaning the code, and we will soon release the code.\n\nQ2: Algorithm 1 has \"predefined spatial list L.\" How to choose it in practice?\nA2: We added a subsection in method (Section 3) to define L. We included the choice of L we used in the experiments section. We added a new section in Appendix A to discuss the choice of pre-defined spatial list L.\n\nQ3: How to justify a longer training time for resizable network.\nA3: Regarding your question on training time, we claim that the resizable network is a single trained network that can scale depending on a FLOPs budget. The ResizableNets indeed need more training time, because it is trained with multiple scale configurations in one network.  On the one hand, it is more efficient than training hundreds of individual networks with different feature map scale configuration. On the other hand, if we have a target network and use resizable augmentation to improve generalization in Section 3.4 (Section 4.1 in the new revision), the training time is less or equal to other data augmentation techniques. Moreover, the performance gain of ResizableNet-NAS is not coming from longer training time. We did experiments to increase the training time of ShuffletNetV2, and the result is below:\nShuffletNetV2 1.5x:\nBaseline (250 epochs): 72.6% \n2x longer training time (500 epochs): 72.6%\n8x longer training time (2000 epochs: 72.6%\nOur preliminary experiment indicates that simply extend the training time can not improve the performance of the network.\n\n\nQ4: Why Resizable-NAS is better than the all len(L) models separately to find better architectures?\nA4: We do not claim that it is better to use Resizable-NAS for finding better architectures compared to training all len(L) models separately. Admittedly, training all models separately and then find the network in these already trained models would be the best option. However, the training cost is overwhelming. Resizable-NAS is more efficient because we train different scale configuration in a single network with shared weights, then search for the target sub-network that satisfy the efficiency constraints.\nMore importantly, our contribution of Resizable-NAS is not the searching method since we propose to use a lookup table, which is not the most efficient way for searching (other searching methods such as evolution algorithm can be more efficient). Instead, our contribution is that we propose an application scenario for ResizableNet. As far as we are aware, our paper is the first do neural architecture search on **resolution** and showed surprisingly good results.\n\nQ5: How the \"target model\" is selected after the training of Resizable Networks?\nA5: The target model is predefined. It depends on which scale configuration you wish to apply the resizable augmentation technique. The experiments on resizable augmentation in our paper use the same setting as the baseline methods (all scale factors in the scale configuration is 0.5; s = 0.5).\n\nQ6: Why is Resizable-Adapt omitted from Table 2?\nA6: The Resizable-Adapt is a dynamic inference method. Since Table 2 compared the performance of different data augmentation techniques, we don't think it is fair to include Resizable-Aug in Table 2.\n\nQ7:References are out-dated. For example, there are no \"Figure 5.\"\nA7: We fixed it in the new revision.\n\nQ8: Ablation studies for Fair Sampling?\nA8: We included ablation studies on Fair Sampling in the Appendix."}, "signatures": ["ICLR.cc/2020/Conference/Paper1584/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1584/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Resizable Neural Networks", "authors": ["Yichen Zhu", "Xiangyu Zhang", "Tong Yang", "Jian Sun"], "authorids": ["k.zhu@mail.utoronto.ca", "zhangxiangyu@megvii.com", "yangtong@megvii.com", "sunjian@megvii.com"], "keywords": [], "abstract": "\nIn this paper, we present a deep convolutional neural network (CNN) which performs arbitrary resize operation on intermediate feature map resolution at stage-level. Motivated by weight sharing mechanism in neural architecture search, where a super-network is trained and sub-networks inherit the weights from the super-network, we present a novel CNN approach. We construct a spatial super-network which consists of multiple sub-networks, where each sub-network is a single scale network that obtain a unique spatial configuration, the convolutional layers are shared across all sub-networks. Such network, named as Resizable Neural Networks, are equivalent to training infinite single scale networks, but has no extra computational cost. Moreover, we present a training algorithm such that all sub-networks achieve better performance than individually trained counterparts. On large-scale ImageNet classification, we demonstrate its effectiveness on various modern network architectures such as MobileNet, ShuffleNet, and ResNet.\n\n\nTo go even further, we present three variants of resizable networks: 1) Resizable as Architecture Search (Resizable-NAS). On ImageNet, Resizable-NAS ResNet-50 attain 0.4% higher on accuracy and 44% smaller than the baseline model. 2) Resizable as Data Augmentation (Resizable-Aug). While we use resizable networks as a data augmentation technique, it obtains superior performance on ImageNet classification, outperform AutoAugment by 1.2% with ResNet-50. 3) Adaptive Resizable Network (Resizable-Adapt). We introduce the adaptive resizable networks as dynamic networks, which further improve the performance with less computational cost via data-dependent inference.", "pdf": "/pdf/02c1a8f8b0e38f18d9a6d5dcb84ae6c95095007e.pdf", "paperhash": "zhu|resizable_neural_networks", "original_pdf": "/attachment/287397a239b7d18c355cbf123933f94d305c7da3.pdf", "_bibtex": "@misc{\nzhu2020resizable,\ntitle={Resizable Neural Networks},\nauthor={Yichen Zhu and Xiangyu Zhang and Tong Yang and Jian Sun},\nyear={2020},\nurl={https://openreview.net/forum?id=BJe_z1HFPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJe_z1HFPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1584/Authors", "ICLR.cc/2020/Conference/Paper1584/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1584/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1584/Reviewers", "ICLR.cc/2020/Conference/Paper1584/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1584/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1584/Authors|ICLR.cc/2020/Conference/Paper1584/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153857, "tmdate": 1576860550814, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1584/Authors", "ICLR.cc/2020/Conference/Paper1584/Reviewers", "ICLR.cc/2020/Conference/Paper1584/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1584/-/Official_Comment"}}}, {"id": "SJeSE9YnjB", "original": null, "number": 1, "cdate": 1573849645299, "ddate": null, "tcdate": 1573849645299, "tmdate": 1573849645299, "tddate": null, "forum": "BJe_z1HFPr", "replyto": "HJeQ86Ug9r", "invitation": "ICLR.cc/2020/Conference/Paper1584/-/Official_Comment", "content": {"title": "Response to Reviewer #1 ", "comment": "Thanks for your detailed and thoughtful reviews! We have addressed all the questions below:\n\nWe improved the writing and fixed the typos you have mentioned. We reorganized the position of tables and figures in our paper. We added subsection \"notation\" at the beginning of the experiment section to clarify the different words we used in our experiments. We enlarged the Figure 2 for readability. \n\nQ1: why Bilinear sampling was chosen? Could the authors provide a comparison with other sampling methods?\nA1: Bilinear sampling is chosen because it can downsample the feature map to any resolution.\nAt this time, we are unable to finish the comparison of bilinear sampling with other sampling methods due to limited time constraints, but we will finish the experiments and add the ablation study in the final revision.\n\nQ2: It is overclaimed that \"Most dynamic networks methods sacrifice accuracy in exchange of adaption in inference\" in the related work section for dynamic neural networks.\nA2: You\u2019re correct. We revised this section and included the paper [1] in the reference you have mentioned.\n\nQ3: How did you find the architecture shown in Figure 3 in the Appendix? What is Xception? Please specify the details.\nA3: The Xception is proposed by the paper [2], and the search space we use is described in [3]. We used the search method in paper [4]. We included this reference in our new revision. We did not discuss the details about how we find architecture because it is not the focus of our paper.\n\nQ4: Regarding the choice of pre-defined spatial list of L.\nA4: We added a subsection in method (Section 3) to define L. We included the choice of L we used in the experiments section. We added a new section in Appendix A to discuss the choice of pre-defined spatial list L. \n\nQ5: Unfair comparison with other data augmentation methods in terms of training budgets.\nA5: We first note that, for all resizable augmentation methods, we use the pre-defined spatial list of [0.45, 0.64, 0.85, 1.0], thus the len(L) = 4. Moreover, our training epochs is the same as the baseline, e.g., 100 epochs for ResNet-50 with batch size 512.\n\nRegarding the training budget, our resizable augmentation spends less or the same training budget compare to other data augmentation methods. Notably, data augmentation methods normally spend longer training time to improve generalization. For comparison, the baseline of ResNet-50 is trained by 100 epochs with batch size 512, AutoAugment [5] spend 21.6x longer training time (270 epochs with batch size 4096) and MixUp [6] spend 4x longer training time (200 epochs with batch size 1024) to train ResNet-50 compared to baseline, according to their paper. Resizable augmentation spend approximately 4x longer training time (100 epochs with batch size 512, but len(L) = 4). Thus the comparison with the other data augmentation methods in terms of training budget is fair. \n\nQ6: what ResizeLearner learns and how ResizeLearner selects the optimal sub-network?\nA6: We understand that the description in Section 3.5 is ambiguous. Thus, we made a revision on Section 3.5 (Section 4.3 in the new revision). To answer your question, in short, ResizeLearner learns the optimal scale configuration for each input image. The input of ResizeLearner is the feature map of an input image (process by the already trained ResizableNet), and the output is a scale configuration for this input. The ResizableNet can make predictions on this input use the scale configuration provided by ResizaLearner, thus make the inference data-dependent.\n\n\n[1] Yu, Jiahui, et al. Universally Slimmable Networks and Improved Training Techniques, In ICCV 2019.\n[2] Chollet, Fran\u00e7ois Xception: Deep Learning with Depthwise Separable Convolutions, In CVPR 2017.\n[3] Guo, Zichao, et al. Single Path One-Shot Neural Architecture Search with Uniform Sampling, https://arxiv.org/abs/1904.00420.pdf\n[4] VAENAS: Sampling Matters in Neural Architecture Search, https://openreview.net/forum?id=S1xKYJSYwS\n[5] Cubuk, Ekin D., et al. AutoAugment: Learning Augmentation Policies from Data. In CVPR 2019\n[6] Zhang, Hongyi, et al. mixup: BEYOND EMPIRICAL RISK MINIMIZATION. In ICLR 2018"}, "signatures": ["ICLR.cc/2020/Conference/Paper1584/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1584/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Resizable Neural Networks", "authors": ["Yichen Zhu", "Xiangyu Zhang", "Tong Yang", "Jian Sun"], "authorids": ["k.zhu@mail.utoronto.ca", "zhangxiangyu@megvii.com", "yangtong@megvii.com", "sunjian@megvii.com"], "keywords": [], "abstract": "\nIn this paper, we present a deep convolutional neural network (CNN) which performs arbitrary resize operation on intermediate feature map resolution at stage-level. Motivated by weight sharing mechanism in neural architecture search, where a super-network is trained and sub-networks inherit the weights from the super-network, we present a novel CNN approach. We construct a spatial super-network which consists of multiple sub-networks, where each sub-network is a single scale network that obtain a unique spatial configuration, the convolutional layers are shared across all sub-networks. Such network, named as Resizable Neural Networks, are equivalent to training infinite single scale networks, but has no extra computational cost. Moreover, we present a training algorithm such that all sub-networks achieve better performance than individually trained counterparts. On large-scale ImageNet classification, we demonstrate its effectiveness on various modern network architectures such as MobileNet, ShuffleNet, and ResNet.\n\n\nTo go even further, we present three variants of resizable networks: 1) Resizable as Architecture Search (Resizable-NAS). On ImageNet, Resizable-NAS ResNet-50 attain 0.4% higher on accuracy and 44% smaller than the baseline model. 2) Resizable as Data Augmentation (Resizable-Aug). While we use resizable networks as a data augmentation technique, it obtains superior performance on ImageNet classification, outperform AutoAugment by 1.2% with ResNet-50. 3) Adaptive Resizable Network (Resizable-Adapt). We introduce the adaptive resizable networks as dynamic networks, which further improve the performance with less computational cost via data-dependent inference.", "pdf": "/pdf/02c1a8f8b0e38f18d9a6d5dcb84ae6c95095007e.pdf", "paperhash": "zhu|resizable_neural_networks", "original_pdf": "/attachment/287397a239b7d18c355cbf123933f94d305c7da3.pdf", "_bibtex": "@misc{\nzhu2020resizable,\ntitle={Resizable Neural Networks},\nauthor={Yichen Zhu and Xiangyu Zhang and Tong Yang and Jian Sun},\nyear={2020},\nurl={https://openreview.net/forum?id=BJe_z1HFPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJe_z1HFPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1584/Authors", "ICLR.cc/2020/Conference/Paper1584/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1584/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1584/Reviewers", "ICLR.cc/2020/Conference/Paper1584/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1584/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1584/Authors|ICLR.cc/2020/Conference/Paper1584/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153857, "tmdate": 1576860550814, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1584/Authors", "ICLR.cc/2020/Conference/Paper1584/Reviewers", "ICLR.cc/2020/Conference/Paper1584/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1584/-/Official_Comment"}}}, {"id": "Syl6fYR_OH", "original": null, "number": 1, "cdate": 1570461973254, "ddate": null, "tcdate": 1570461973254, "tmdate": 1572972449535, "tddate": null, "forum": "BJe_z1HFPr", "replyto": "BJe_z1HFPr", "invitation": "ICLR.cc/2020/Conference/Paper1584/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "In a standard multi-stage neural network such as a ResNet, the resize operation between stages typically reduces the spatial resolution by 0.5.  (input ---> stage 1---> 0.5 reduction ---> stage 2 --> 0.5 reduction ---> ....). In this paper the authors apply a variety of reduction factors in between these stages for different training examples (input ---> stage 1---> variable reduction ---> stage 2 --> variable reduction ---> ....). They demonstrate that simply training in this way is a powerful form of augmentation. It also produces a network which may be scaled depending on a FLOP budget.\n\nI think this is a really neat idea, and as far as I'm aware it is novel.  It is similar in spirit to EfficientNet, although more flexible. The experimental results are good. However, the paper is let down by poor writing and a lack of detail.\n\nThe paper needs a rewrite as there are many grammatical errors, which cause a bad impression:\n\n- \"performs arbitrary resize operation\" ->  \"performs arbitrary resize operations\"\n- \"Scale is the fundamental component of the physical world\" ---> *a* fundamental component!\n- \"i.e the acuracy of NISP\" --> e.g.\n- \"compare with baseline\" -> \"compared to the baseline\"\n- There are several instances of a space missed out between a letter and an open bracket \n- \"The Figure 2(a)\" --> \"Figure 2(a)\"\n\nThe comparison to weight-sharing NAS methods is unnecessary. Those entail searching for architectures and sharing weights through the search process, whereas in this work it is having a network that can take different sized inputs at different stages. On that note, it doesn't feel right to me to refer to there being many \"subnetworks\". What there really is, is just a single network that is robust to multi-scaled inputs (which is a good thing!)\n\nFigure 1 is nice!\n\nEfficientNet-B0 is a base-network that can be scaled up with a compound scaling approach found through grid search. What happens if you scale up your Resizeable net to the same FLOPs as e.g. EfficientNet-B7?\n\nI like the old-school vision citations, although referring to object detection makes me wonder why there are no experiments on it. For distillation, I recommend you cite https://arxiv.org/abs/1312.6184, as the Hinton paper is really just an extension of this.\n\nThe fair sampling seems important to the method. Could a detailed explanation be included in the appendix? Do you given the performance as a result of naive sampling? What do you mean by \"Some convolutional layers can go through more training issues than others\"? \n\nA big problem in this paper is that (as far as I can tell) the scaling factors considered aren't given (but we are told that they lie between 0 and 1). It isn't possible to sample these arbitrarily as you indicate that a batch-norm layer is needed for each one, so these must be discrete (because of this, it isn't correct to say that you have infinite networks contained within).  It therefore isn't clear e.g. in Table 1 which permutation of scaling factors were used in upsampling the networks. Are the results given representative of the best-case selection of these scaling? I hope this isn't the case. I am assuming it is uniform scaling in the case of the \"individually trained counterparts\". It would be interesting to know more generally which combinations worked well.\n\nIn Section 3.5 I'm not sure what is meant by \"It is inefficient to process all images to one-subnetwork, as the algorithm spends equal energy at each sample\".  I assume energy use is proportional to the number of FLOPS, which in turn depends on spatial resolution.\n\nThe legend in Figure 2 is close-to unreadable and needs changing.\n\nThe results are impressive, but error bars would be appreciated if possible. As ImageNet is the only dataset considered, this would give some needed clout. \n\nPros\n-------\n\n- Nice, novel method\n- Good experimental results\n\nCons\n--------\n\n- Paper is poorly written\n- Very few details of the scaling factor variations\n- Only one dataset considered\n\nAlthough the paper is written badly and the narrative is muddled, the underlying idea is a nice one, which is executed well experimentally. Because of this I would like to recommend a Weak Accept, subject to the authors (i) doing a rewrite and (ii) including more information regarding the scaling permutations. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1584/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1584/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Resizable Neural Networks", "authors": ["Yichen Zhu", "Xiangyu Zhang", "Tong Yang", "Jian Sun"], "authorids": ["k.zhu@mail.utoronto.ca", "zhangxiangyu@megvii.com", "yangtong@megvii.com", "sunjian@megvii.com"], "keywords": [], "abstract": "\nIn this paper, we present a deep convolutional neural network (CNN) which performs arbitrary resize operation on intermediate feature map resolution at stage-level. Motivated by weight sharing mechanism in neural architecture search, where a super-network is trained and sub-networks inherit the weights from the super-network, we present a novel CNN approach. We construct a spatial super-network which consists of multiple sub-networks, where each sub-network is a single scale network that obtain a unique spatial configuration, the convolutional layers are shared across all sub-networks. Such network, named as Resizable Neural Networks, are equivalent to training infinite single scale networks, but has no extra computational cost. Moreover, we present a training algorithm such that all sub-networks achieve better performance than individually trained counterparts. On large-scale ImageNet classification, we demonstrate its effectiveness on various modern network architectures such as MobileNet, ShuffleNet, and ResNet.\n\n\nTo go even further, we present three variants of resizable networks: 1) Resizable as Architecture Search (Resizable-NAS). On ImageNet, Resizable-NAS ResNet-50 attain 0.4% higher on accuracy and 44% smaller than the baseline model. 2) Resizable as Data Augmentation (Resizable-Aug). While we use resizable networks as a data augmentation technique, it obtains superior performance on ImageNet classification, outperform AutoAugment by 1.2% with ResNet-50. 3) Adaptive Resizable Network (Resizable-Adapt). We introduce the adaptive resizable networks as dynamic networks, which further improve the performance with less computational cost via data-dependent inference.", "pdf": "/pdf/02c1a8f8b0e38f18d9a6d5dcb84ae6c95095007e.pdf", "paperhash": "zhu|resizable_neural_networks", "original_pdf": "/attachment/287397a239b7d18c355cbf123933f94d305c7da3.pdf", "_bibtex": "@misc{\nzhu2020resizable,\ntitle={Resizable Neural Networks},\nauthor={Yichen Zhu and Xiangyu Zhang and Tong Yang and Jian Sun},\nyear={2020},\nurl={https://openreview.net/forum?id=BJe_z1HFPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJe_z1HFPr", "replyto": "BJe_z1HFPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1584/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1584/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575917871629, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1584/Reviewers"], "noninvitees": [], "tcdate": 1570237735252, "tmdate": 1575917871640, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1584/-/Official_Review"}}}, {"id": "HJeQ86Ug9r", "original": null, "number": 3, "cdate": 1572003146558, "ddate": null, "tcdate": 1572003146558, "tmdate": 1572972449460, "tddate": null, "forum": "BJe_z1HFPr", "replyto": "BJe_z1HFPr", "invitation": "ICLR.cc/2020/Conference/Paper1584/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a new method that involves multi-scale inputs for each layer that could be used as network architecture search or data agumentation or \n\nPros)\n(+) The idea looks interesting.\n(+) The experimental results look promising.\n\nCons)\n(-) Many typos when denoting figures and tables. See the minor comments below.\n(-) I believe the authors could organize the paper better. Tables and figures that are referred in a page are hard to find quickly. I recommend the authors refine the paper again for better readability.\n(-) Some notations (such as RS-, RS-NAS, and so on) are so vague that hard to follow.  \n(-) I recommend the authors redraw all the figures for clarity. For example, each legend in Figure 2 is hard to take a look at.\n(-) + the comments below.\n\nComments)\n- When doing feature map resize in terms of the resolution, why Bilinear sampling was chosen? Could the authors provide a comparison with other sampling methods?\n- In the related work section for dynamic neural networks, the authors claimed that \"Most dynamic networks methods sacrifice accuracy in exchange of adaption in inference\", but it seems to be quite overclaimed. As shown in the paper [1],  one can find that the author presented they could improve both accuracy and efficiency.\n- How did you find the architecture shown In Figure 3 in the Appendix? What is Xception? Please specify the details.\n-  Designing the pre-defined spatial list of L looks critical, so the authors should describe L in the implementation details. \n- One of the main problem I think is the training budget issue. According to algorithm 1, the inner loop of \"for j=0,..,len(L)\", the overall training time will clearly take L times longer than that of the training setting w/o resizable training. Thus, it does not seem to be fair comparison in terms of the training budget. Namely, it seems that the authors compared with the other data augmentation methods which spend much less training budgets.\n- Hard to grasp Section 3.5 of Adaptive resizable neural network. ResizeLearner looks being attached at the last stage of the original network after the original network is trained, but there is no further information about what ResizeLearner learns and how ResizeLearner selects the optimal sub-network. \n\n[1] Universally Slimmable Networks and Improved Training Techniques, https://arxiv.org/pdf/1903.05134.pdf.\n\nMinor comments)\n- Wrong section and figure references:\n  - 'It also mitigates the co-adaptation issue which we will discuss in Section 3.3'(indeed it is Section 3.4), 'The network architecture along with feature map resolution and channels number are shown in Figure 4' (it should be Figure 3).\n  - - Figure 3(d) referred to in section 4.4  would be Figure 2(d) indeed.\n\nAbout rating)\nThe authors provided a novel technique about the resizable approach and the experimental results look promising. However, the paper needs to be revised and looks like it does not ready to be published now. If the authors could revise the paper and concern my comments well, I would increase my rating. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1584/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1584/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Resizable Neural Networks", "authors": ["Yichen Zhu", "Xiangyu Zhang", "Tong Yang", "Jian Sun"], "authorids": ["k.zhu@mail.utoronto.ca", "zhangxiangyu@megvii.com", "yangtong@megvii.com", "sunjian@megvii.com"], "keywords": [], "abstract": "\nIn this paper, we present a deep convolutional neural network (CNN) which performs arbitrary resize operation on intermediate feature map resolution at stage-level. Motivated by weight sharing mechanism in neural architecture search, where a super-network is trained and sub-networks inherit the weights from the super-network, we present a novel CNN approach. We construct a spatial super-network which consists of multiple sub-networks, where each sub-network is a single scale network that obtain a unique spatial configuration, the convolutional layers are shared across all sub-networks. Such network, named as Resizable Neural Networks, are equivalent to training infinite single scale networks, but has no extra computational cost. Moreover, we present a training algorithm such that all sub-networks achieve better performance than individually trained counterparts. On large-scale ImageNet classification, we demonstrate its effectiveness on various modern network architectures such as MobileNet, ShuffleNet, and ResNet.\n\n\nTo go even further, we present three variants of resizable networks: 1) Resizable as Architecture Search (Resizable-NAS). On ImageNet, Resizable-NAS ResNet-50 attain 0.4% higher on accuracy and 44% smaller than the baseline model. 2) Resizable as Data Augmentation (Resizable-Aug). While we use resizable networks as a data augmentation technique, it obtains superior performance on ImageNet classification, outperform AutoAugment by 1.2% with ResNet-50. 3) Adaptive Resizable Network (Resizable-Adapt). We introduce the adaptive resizable networks as dynamic networks, which further improve the performance with less computational cost via data-dependent inference.", "pdf": "/pdf/02c1a8f8b0e38f18d9a6d5dcb84ae6c95095007e.pdf", "paperhash": "zhu|resizable_neural_networks", "original_pdf": "/attachment/287397a239b7d18c355cbf123933f94d305c7da3.pdf", "_bibtex": "@misc{\nzhu2020resizable,\ntitle={Resizable Neural Networks},\nauthor={Yichen Zhu and Xiangyu Zhang and Tong Yang and Jian Sun},\nyear={2020},\nurl={https://openreview.net/forum?id=BJe_z1HFPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJe_z1HFPr", "replyto": "BJe_z1HFPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1584/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1584/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575917871629, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1584/Reviewers"], "noninvitees": [], "tcdate": 1570237735252, "tmdate": 1575917871640, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1584/-/Official_Review"}}}], "count": 9}