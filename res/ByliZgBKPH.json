{"notes": [{"id": "ByliZgBKPH", "original": "Skl31ZxYvH", "number": 2148, "cdate": 1569439746800, "ddate": null, "tcdate": 1569439746800, "tmdate": 1577168241799, "tddate": null, "forum": "ByliZgBKPH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["daniel.c.mcnamee@gmail.com"], "title": "Policy path programming", "authors": ["Daniel McNamee"], "pdf": "/pdf/9a5bf9a6fdba519699650ede06e085ed122acf34.pdf", "TL;DR": "A normative theory of hierarchical model-based policy optimization", "abstract": "We develop a normative theory of hierarchical model-based policy optimization for Markov decision processes resulting in a full-depth, full-width policy iteration algorithm. This method performs policy updates which integrate reward information over all states at all horizons simultaneously thus sequentially maximizing the expected reward obtained per algorithmic iteration. Effectively, policy path programming ascends the expected cumulative reward gradient in the space of policies defined over all state-space paths. An exact formula is derived which finitely parametrizes these path gradients in terms of action preferences. Policy path gradients can be directly computed using an internal model thus obviating the need to sample paths in order to optimize in depth. They are quadratic in successor representation entries and afford natural generalizations to higher-order gradient techniques. In simulations, it is shown that intuitive hierarchical reasoning is emergent within the associated policy optimization dynamics.", "keywords": ["markov decision process", "planning", "hierarchical", "reinforcement learning"], "paperhash": "mcnamee|policy_path_programming", "original_pdf": "/attachment/2bfb0137b82e2798a3ef6cf5693e7ecfafb12c4d.pdf", "_bibtex": "@misc{\nmcnamee2020policy,\ntitle={Policy path programming},\nauthor={Daniel McNamee},\nyear={2020},\nurl={https://openreview.net/forum?id=ByliZgBKPH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "3Orygi4zw", "original": null, "number": 1, "cdate": 1576798741772, "ddate": null, "tcdate": 1576798741772, "tmdate": 1576800894482, "tddate": null, "forum": "ByliZgBKPH", "replyto": "ByliZgBKPH", "invitation": "ICLR.cc/2020/Conference/Paper2148/-/Decision", "content": {"decision": "Reject", "comment": "The reviewers were not convinced about the significance of this work. There is no empirical or theoretical result justifying why this method has advantages over the existing methods. The reviewers also raised concerns related to the scalability of the proposal. Since none of the reviewers were enthusiastic about the paper, including the expert ones, I cannot recommend acceptance of this work.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["daniel.c.mcnamee@gmail.com"], "title": "Policy path programming", "authors": ["Daniel McNamee"], "pdf": "/pdf/9a5bf9a6fdba519699650ede06e085ed122acf34.pdf", "TL;DR": "A normative theory of hierarchical model-based policy optimization", "abstract": "We develop a normative theory of hierarchical model-based policy optimization for Markov decision processes resulting in a full-depth, full-width policy iteration algorithm. This method performs policy updates which integrate reward information over all states at all horizons simultaneously thus sequentially maximizing the expected reward obtained per algorithmic iteration. Effectively, policy path programming ascends the expected cumulative reward gradient in the space of policies defined over all state-space paths. An exact formula is derived which finitely parametrizes these path gradients in terms of action preferences. Policy path gradients can be directly computed using an internal model thus obviating the need to sample paths in order to optimize in depth. They are quadratic in successor representation entries and afford natural generalizations to higher-order gradient techniques. In simulations, it is shown that intuitive hierarchical reasoning is emergent within the associated policy optimization dynamics.", "keywords": ["markov decision process", "planning", "hierarchical", "reinforcement learning"], "paperhash": "mcnamee|policy_path_programming", "original_pdf": "/attachment/2bfb0137b82e2798a3ef6cf5693e7ecfafb12c4d.pdf", "_bibtex": "@misc{\nmcnamee2020policy,\ntitle={Policy path programming},\nauthor={Daniel McNamee},\nyear={2020},\nurl={https://openreview.net/forum?id=ByliZgBKPH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "ByliZgBKPH", "replyto": "ByliZgBKPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795710059, "tmdate": 1576800258971, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2148/-/Decision"}}}, {"id": "ryxWltR5sS", "original": null, "number": 1, "cdate": 1573738728738, "ddate": null, "tcdate": 1573738728738, "tmdate": 1573738728738, "tddate": null, "forum": "ByliZgBKPH", "replyto": "ByliZgBKPH", "invitation": "ICLR.cc/2020/Conference/Paper2148/-/Official_Comment", "content": {"title": "Author response", "comment": "Thanks to all reviewers for your feedback. The manuscript has been edited to address some of the issues raised and to improve its clarity and precision. The overall impression is that comparative demonstrations of this theory embedded in a scalable RL algorithm is required which is not possible at this stage. To address Reviewer #1's questions directly:\n\n- No. In fact, the natural path gradient could be combined with the natural policy gradient through the reparameterization rule for Fisher informations. Concretely, this would result in policy parameter updates sensitive to state-action correlations under the policy-induced path distribution (which is not the case with the natural policy gradient).\n- Untested as yet.\n- To do an exact full-width, full-depth backup via roll-outs is impossible since this would require an infinite number of infinitely deep samples in general. Our model accomplishes this using a known environmental model (and exhibits hierarchical processing of the environment and policy dynamics). With respect to an implementation in a scalable RL agent, though untested, our method suggests an alternative approach by which a full-depth (or n-step) backup may be approximated based on estimating the components of the path gradient calculation during exploration. This approach has the additional benefit that estimated path gradient components transfer across reward functions."}, "signatures": ["ICLR.cc/2020/Conference/Paper2148/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2148/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["daniel.c.mcnamee@gmail.com"], "title": "Policy path programming", "authors": ["Daniel McNamee"], "pdf": "/pdf/9a5bf9a6fdba519699650ede06e085ed122acf34.pdf", "TL;DR": "A normative theory of hierarchical model-based policy optimization", "abstract": "We develop a normative theory of hierarchical model-based policy optimization for Markov decision processes resulting in a full-depth, full-width policy iteration algorithm. This method performs policy updates which integrate reward information over all states at all horizons simultaneously thus sequentially maximizing the expected reward obtained per algorithmic iteration. Effectively, policy path programming ascends the expected cumulative reward gradient in the space of policies defined over all state-space paths. An exact formula is derived which finitely parametrizes these path gradients in terms of action preferences. Policy path gradients can be directly computed using an internal model thus obviating the need to sample paths in order to optimize in depth. They are quadratic in successor representation entries and afford natural generalizations to higher-order gradient techniques. In simulations, it is shown that intuitive hierarchical reasoning is emergent within the associated policy optimization dynamics.", "keywords": ["markov decision process", "planning", "hierarchical", "reinforcement learning"], "paperhash": "mcnamee|policy_path_programming", "original_pdf": "/attachment/2bfb0137b82e2798a3ef6cf5693e7ecfafb12c4d.pdf", "_bibtex": "@misc{\nmcnamee2020policy,\ntitle={Policy path programming},\nauthor={Daniel McNamee},\nyear={2020},\nurl={https://openreview.net/forum?id=ByliZgBKPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByliZgBKPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2148/Authors", "ICLR.cc/2020/Conference/Paper2148/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2148/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2148/Reviewers", "ICLR.cc/2020/Conference/Paper2148/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2148/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2148/Authors|ICLR.cc/2020/Conference/Paper2148/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145634, "tmdate": 1576860550477, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2148/Authors", "ICLR.cc/2020/Conference/Paper2148/Reviewers", "ICLR.cc/2020/Conference/Paper2148/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2148/-/Official_Comment"}}}, {"id": "rJg_CgCUYH", "original": null, "number": 1, "cdate": 1571377359817, "ddate": null, "tcdate": 1571377359817, "tmdate": 1572972376917, "tddate": null, "forum": "ByliZgBKPH", "replyto": "ByliZgBKPH", "invitation": "ICLR.cc/2020/Conference/Paper2148/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper presents a reinforcement learning method that exploits the full-depth backup. The policy update based on the full-depth backup is derived for entropy-regularized MDP. The state-action correlation function is introduced and the Fisher information matrix is computed with it. The proposed method is evaluated on tasks with discrete states and actions.\n\nI understand the concept of using the full path for updating the policy, but I do not see significant novelty of the proposed method from the current manuscript. The proposed method looks equivalent to the natural policy gradient with full-depth backup for entropy-regularized MDP, which is a special case of existing methods. \n\nMy concern is that the scalability of the proposed method. The use of the full-depth backup should suffer from the large variance, and I think the proposed method will not work on tasks with the high-dimensional state space. \nThe evaluation is limited to tasks of which the state space is small, and the proposed method is not compared with existing methods.\n\nDue to the unclear novelty and limited empirical results, I give weak reject to the paper in the current form.\n\nI request authors to answer the following questions to improve the clarity.\n\n- Is the proposed method equivalent to use the natural policy gradient with the full-depth backup for a softmax energy-based policy? If they are different, what it the crucial difference?\n\n- I think that the variance of the estimation of the gradient is large when using the full-depth back up. I'm curious about the performance of the proposed method in high-dimensional tasks. However, the evaluation is very limited to simple tasks in which the state space is relatively small compared with tasks commonly used in deep RL papers.\nDoes the proposed method scale to more complex tasks, such as Atari games?\n\n- When using the n-step TD learning, increasing n does not always improve the performance, and n should be set to an intermediate value\nWhat is the motivation of using the full paths for updating the policy? Does the proposed method outperform existing methods? Especially, the comparison with natural policy gradient methods is necessary to show the benefit of the proposed algorithm.\n\nMinor comments:\n\n- In page 2, \"A state-space \\mathcal{X} is composed of states x \\in \\mathcal{X}\" <- Authors may want to replace x with s in this sentence.\n - In page 6, \"0\\lambda < 1\" I think that \"<\" is missing between \"0\" and \"\\lambda\"\n- In page 6, some variables are explained after Equation (10). However, it took me a while to find \"lambda\" in Equation (10), since Equation (10) has 9 lines and many terms. I think the description in page 6 can be improved. For example, I recommend to use \"\\exp\" instead of \"e^\" for readability."}, "signatures": ["ICLR.cc/2020/Conference/Paper2148/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2148/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["daniel.c.mcnamee@gmail.com"], "title": "Policy path programming", "authors": ["Daniel McNamee"], "pdf": "/pdf/9a5bf9a6fdba519699650ede06e085ed122acf34.pdf", "TL;DR": "A normative theory of hierarchical model-based policy optimization", "abstract": "We develop a normative theory of hierarchical model-based policy optimization for Markov decision processes resulting in a full-depth, full-width policy iteration algorithm. This method performs policy updates which integrate reward information over all states at all horizons simultaneously thus sequentially maximizing the expected reward obtained per algorithmic iteration. Effectively, policy path programming ascends the expected cumulative reward gradient in the space of policies defined over all state-space paths. An exact formula is derived which finitely parametrizes these path gradients in terms of action preferences. Policy path gradients can be directly computed using an internal model thus obviating the need to sample paths in order to optimize in depth. They are quadratic in successor representation entries and afford natural generalizations to higher-order gradient techniques. In simulations, it is shown that intuitive hierarchical reasoning is emergent within the associated policy optimization dynamics.", "keywords": ["markov decision process", "planning", "hierarchical", "reinforcement learning"], "paperhash": "mcnamee|policy_path_programming", "original_pdf": "/attachment/2bfb0137b82e2798a3ef6cf5693e7ecfafb12c4d.pdf", "_bibtex": "@misc{\nmcnamee2020policy,\ntitle={Policy path programming},\nauthor={Daniel McNamee},\nyear={2020},\nurl={https://openreview.net/forum?id=ByliZgBKPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ByliZgBKPH", "replyto": "ByliZgBKPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2148/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2148/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575877499366, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2148/Reviewers"], "noninvitees": [], "tcdate": 1570237727018, "tmdate": 1575877499377, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2148/-/Official_Review"}}}, {"id": "SkxEoyMAYr", "original": null, "number": 2, "cdate": 1571852187584, "ddate": null, "tcdate": 1571852187584, "tmdate": 1572972376882, "tddate": null, "forum": "ByliZgBKPH", "replyto": "ByliZgBKPH", "invitation": "ICLR.cc/2020/Conference/Paper2148/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work proposes a policy iteration algorithm that implements full-depth, full-width backups in contrast to one-step, full-width methods. The authors go over existing algorithms and talks a bit how their proposal conceptually differs in how it performs said backups. They provide a bit of intuition to help explain their algorithm's derivation. Finally, they provide a few experiments showing that their algorithm works.\n\nMy personal issues are with these experiments. First, I would like to see better comparisons between this method and existing policy iteration methods. I don't have a good sense in which one would choose to use this algorithm over any baseline methods. Is it faster in any sense? Does it produce better policies during certain games? For the experiments themselves, I don't see much clarification of what the various graphs even show. More effort should have been spent analyzing these. \n\nI come away from this work not fully appreciating the impact it is trying to sell me on. I also think the discussion section should have been more fleshed out."}, "signatures": ["ICLR.cc/2020/Conference/Paper2148/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2148/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["daniel.c.mcnamee@gmail.com"], "title": "Policy path programming", "authors": ["Daniel McNamee"], "pdf": "/pdf/9a5bf9a6fdba519699650ede06e085ed122acf34.pdf", "TL;DR": "A normative theory of hierarchical model-based policy optimization", "abstract": "We develop a normative theory of hierarchical model-based policy optimization for Markov decision processes resulting in a full-depth, full-width policy iteration algorithm. This method performs policy updates which integrate reward information over all states at all horizons simultaneously thus sequentially maximizing the expected reward obtained per algorithmic iteration. Effectively, policy path programming ascends the expected cumulative reward gradient in the space of policies defined over all state-space paths. An exact formula is derived which finitely parametrizes these path gradients in terms of action preferences. Policy path gradients can be directly computed using an internal model thus obviating the need to sample paths in order to optimize in depth. They are quadratic in successor representation entries and afford natural generalizations to higher-order gradient techniques. In simulations, it is shown that intuitive hierarchical reasoning is emergent within the associated policy optimization dynamics.", "keywords": ["markov decision process", "planning", "hierarchical", "reinforcement learning"], "paperhash": "mcnamee|policy_path_programming", "original_pdf": "/attachment/2bfb0137b82e2798a3ef6cf5693e7ecfafb12c4d.pdf", "_bibtex": "@misc{\nmcnamee2020policy,\ntitle={Policy path programming},\nauthor={Daniel McNamee},\nyear={2020},\nurl={https://openreview.net/forum?id=ByliZgBKPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ByliZgBKPH", "replyto": "ByliZgBKPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2148/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2148/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575877499366, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2148/Reviewers"], "noninvitees": [], "tcdate": 1570237727018, "tmdate": 1575877499377, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2148/-/Official_Review"}}}, {"id": "ryer2GGXqB", "original": null, "number": 3, "cdate": 1572180653396, "ddate": null, "tcdate": 1572180653396, "tmdate": 1572972376837, "tddate": null, "forum": "ByliZgBKPH", "replyto": "ByliZgBKPH", "invitation": "ICLR.cc/2020/Conference/Paper2148/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper considers the problem of entropy-regularized discounted Markov decision processes with discrete state space. Instead of working on the parameter space of policy (\\pi_ij), the paper has proposed to reparametrize with natural parameters (A_ij). The reparameterization trick helps to learn the natural parameters using the natural gradient method.\nThe writing is easy to follow. However, it is not clear what is the benefit of learning policy using path representation compared with other methods in the literature. The paper does not clearly state the motivation of the proposed method.\nThe experimental section presents the convergence of the proposed methods in 3 small problems including decision trees of four levels, the tower of Hanoi problem, and four-room grid worlds. The experiment setting is very simple with a small number of states in the policy. It is not clear how the proposed method is able to scale up the size of state space. Besides, there is no baseline method in the literature to be presented to compare with the proposed method. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2148/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2148/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["daniel.c.mcnamee@gmail.com"], "title": "Policy path programming", "authors": ["Daniel McNamee"], "pdf": "/pdf/9a5bf9a6fdba519699650ede06e085ed122acf34.pdf", "TL;DR": "A normative theory of hierarchical model-based policy optimization", "abstract": "We develop a normative theory of hierarchical model-based policy optimization for Markov decision processes resulting in a full-depth, full-width policy iteration algorithm. This method performs policy updates which integrate reward information over all states at all horizons simultaneously thus sequentially maximizing the expected reward obtained per algorithmic iteration. Effectively, policy path programming ascends the expected cumulative reward gradient in the space of policies defined over all state-space paths. An exact formula is derived which finitely parametrizes these path gradients in terms of action preferences. Policy path gradients can be directly computed using an internal model thus obviating the need to sample paths in order to optimize in depth. They are quadratic in successor representation entries and afford natural generalizations to higher-order gradient techniques. In simulations, it is shown that intuitive hierarchical reasoning is emergent within the associated policy optimization dynamics.", "keywords": ["markov decision process", "planning", "hierarchical", "reinforcement learning"], "paperhash": "mcnamee|policy_path_programming", "original_pdf": "/attachment/2bfb0137b82e2798a3ef6cf5693e7ecfafb12c4d.pdf", "_bibtex": "@misc{\nmcnamee2020policy,\ntitle={Policy path programming},\nauthor={Daniel McNamee},\nyear={2020},\nurl={https://openreview.net/forum?id=ByliZgBKPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ByliZgBKPH", "replyto": "ByliZgBKPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2148/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2148/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575877499366, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2148/Reviewers"], "noninvitees": [], "tcdate": 1570237727018, "tmdate": 1575877499377, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2148/-/Official_Review"}}}, {"id": "rygNnxhncS", "original": null, "number": 4, "cdate": 1572810924245, "ddate": null, "tcdate": 1572810924245, "tmdate": 1572972376793, "tddate": null, "forum": "ByliZgBKPH", "replyto": "ByliZgBKPH", "invitation": "ICLR.cc/2020/Conference/Paper2148/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper considers the problem of finding the optimal policy in the Markovian decision Processes, where a KL policy regularizer is added to the objective function. Instead of the closed form solution which leads  to the KL-regularized Bellman equation the paper proposes to use an incremental gradient ascent algorithm. The paper recommends an iterative policy gradient scheme to  optimize this  objective function. There exists a substantial literature on the subject of KL-regularized RL as well as using the  policy gradient algorithms  to optimize this objective function using policy gradient schemes (See all variants of KL(entropy)-constraint actor-critic or reinforce algorithms, e.g. A2C, IMPALA,...). Unfortunately the paper doesn\u2019t provide any comparison with those methods. In the absence of those comparisons the significance of this work to the literature of RL is not clear, as it is not solving an open problem which hasn't addressed before, neither it  provides theoretical/empirical evidence that it has advanced the start-of-the-art in terms of providing a more efficient solution.\n\n The paper considers a setting which is quite well-studied as there exist efficient solvers for optimizing the KL regularized RL (including policy-gradient variants). Why the proposed approach is better than those already existed in the literature? What is the outstanding problem in the literature of KL-regularized RL that this work tries to address? I couldn\u2019t find a satisfying argument with respect to these questions in the current submission.    In the absence of any theoretical or empirical result to justify the merits of the proposed algorithm the contribution of this paper to the literature is not clear. Also it is not clear how this approach can scale up to anything beyond the finite state-action  problems as it relies on knowing quantities like state-action transitions and the inverse of state transition matrix which in practice is quite difficult to estimate. I recommend the authors to rethink their approach from the point of view of whether  It provides solution to some open problems in RL/control or it advances the-state-of-the-art. If this is the case, the paper needs to provide theoretical/empirical evidence to back up its claim. Unfortunately the current submission does not satisfy these requirements.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2148/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2148/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["daniel.c.mcnamee@gmail.com"], "title": "Policy path programming", "authors": ["Daniel McNamee"], "pdf": "/pdf/9a5bf9a6fdba519699650ede06e085ed122acf34.pdf", "TL;DR": "A normative theory of hierarchical model-based policy optimization", "abstract": "We develop a normative theory of hierarchical model-based policy optimization for Markov decision processes resulting in a full-depth, full-width policy iteration algorithm. This method performs policy updates which integrate reward information over all states at all horizons simultaneously thus sequentially maximizing the expected reward obtained per algorithmic iteration. Effectively, policy path programming ascends the expected cumulative reward gradient in the space of policies defined over all state-space paths. An exact formula is derived which finitely parametrizes these path gradients in terms of action preferences. Policy path gradients can be directly computed using an internal model thus obviating the need to sample paths in order to optimize in depth. They are quadratic in successor representation entries and afford natural generalizations to higher-order gradient techniques. In simulations, it is shown that intuitive hierarchical reasoning is emergent within the associated policy optimization dynamics.", "keywords": ["markov decision process", "planning", "hierarchical", "reinforcement learning"], "paperhash": "mcnamee|policy_path_programming", "original_pdf": "/attachment/2bfb0137b82e2798a3ef6cf5693e7ecfafb12c4d.pdf", "_bibtex": "@misc{\nmcnamee2020policy,\ntitle={Policy path programming},\nauthor={Daniel McNamee},\nyear={2020},\nurl={https://openreview.net/forum?id=ByliZgBKPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ByliZgBKPH", "replyto": "ByliZgBKPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2148/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2148/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575877499366, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2148/Reviewers"], "noninvitees": [], "tcdate": 1570237727018, "tmdate": 1575877499377, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2148/-/Official_Review"}}}], "count": 7}