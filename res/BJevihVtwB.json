{"notes": [{"id": "BJevihVtwB", "original": "SkxagHgWwr", "number": 154, "cdate": 1569438878545, "ddate": null, "tcdate": 1569438878545, "tmdate": 1577168246398, "tddate": null, "forum": "BJevihVtwB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "BOOSTING ENCODER-DECODER CNN FOR INVERSE PROBLEMS", "authors": ["Eunju Cha", "Jaeduck Jang", "Junho Lee", "Eunha Lee", "Jong Chul Ye"], "authorids": ["eunju.cha@kaist.ac.kr", "jduck.jang@samsung.com", "jh0325.lee@samsung.com", "eunhayo.lee@samsung.com", "jong.ye@kaist.ac.kr"], "keywords": ["Prediction error", "Boosting", "Encoder-decoder convolutional neural network", "Inverse problem"], "abstract": "Encoder-decoder convolutional neural networks  (CNN)  have been extensively used for various  inverse problems. However,  their prediction error  for unseen test data is difficult to estimate  a priori, since the neural networks are trained using only selected data and their architectures are largely considered blackboxes. This poses a fundamental  challenge in improving the performance of  neural networks. Recently, it was shown that Stein\u2019s unbiased risk estimator (SURE) can be used as an unbiased estimator of the prediction error for denoising problems. However, the computation of the divergence term in SURE is difficult to implement in a neural network framework, and the condition to avoid trivial identity mapping is not well defined. In this paper, inspired by the finding that  an encoder-decoder CNN can be expressed as a piecewise linear representation, we provide a  close form expression of  the unbiased estimator for the prediction error. The close form representation leads to a novel boosting scheme to prevent a neural network from converging to an identity mapping so that it can enhance the performance. Experimental results show that the proposed algorithm provides consistent improvement in various inverse problems.", "pdf": "/pdf/8bc038924aef9dc0a4d004adf50f08a472fe6d60.pdf", "paperhash": "cha|boosting_encoderdecoder_cnn_for_inverse_problems", "original_pdf": "/attachment/ac0989bae63a902dfc9d9f2629f316e0ba7307e8.pdf", "_bibtex": "@misc{\ncha2020boosting,\ntitle={{\\{}BOOSTING{\\}} {\\{}ENCODER{\\}}-{\\{}DECODER{\\}} {\\{}CNN{\\}} {\\{}FOR{\\}} {\\{}INVERSE{\\}} {\\{}PROBLEMS{\\}}},\nauthor={Eunju Cha and Jaeduck Jang and Junho Lee and Eunha Lee and Jong Chul Ye},\nyear={2020},\nurl={https://openreview.net/forum?id=BJevihVtwB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "20LZpQD4al", "original": null, "number": 1, "cdate": 1576798688797, "ddate": null, "tcdate": 1576798688797, "tmdate": 1576800946305, "tddate": null, "forum": "BJevihVtwB", "replyto": "BJevihVtwB", "invitation": "ICLR.cc/2020/Conference/Paper154/-/Decision", "content": {"decision": "Reject", "comment": "This paper introduces a closed-form expression for the Stein\u2019s unbiased estimator for the prediction error, and a boosting approach based on this, with empirical evaluation. While this paper is interesting, all reviewers seem to agree that more work is required before this paper can be published at ICLR.  ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "BOOSTING ENCODER-DECODER CNN FOR INVERSE PROBLEMS", "authors": ["Eunju Cha", "Jaeduck Jang", "Junho Lee", "Eunha Lee", "Jong Chul Ye"], "authorids": ["eunju.cha@kaist.ac.kr", "jduck.jang@samsung.com", "jh0325.lee@samsung.com", "eunhayo.lee@samsung.com", "jong.ye@kaist.ac.kr"], "keywords": ["Prediction error", "Boosting", "Encoder-decoder convolutional neural network", "Inverse problem"], "abstract": "Encoder-decoder convolutional neural networks  (CNN)  have been extensively used for various  inverse problems. However,  their prediction error  for unseen test data is difficult to estimate  a priori, since the neural networks are trained using only selected data and their architectures are largely considered blackboxes. This poses a fundamental  challenge in improving the performance of  neural networks. Recently, it was shown that Stein\u2019s unbiased risk estimator (SURE) can be used as an unbiased estimator of the prediction error for denoising problems. However, the computation of the divergence term in SURE is difficult to implement in a neural network framework, and the condition to avoid trivial identity mapping is not well defined. In this paper, inspired by the finding that  an encoder-decoder CNN can be expressed as a piecewise linear representation, we provide a  close form expression of  the unbiased estimator for the prediction error. The close form representation leads to a novel boosting scheme to prevent a neural network from converging to an identity mapping so that it can enhance the performance. Experimental results show that the proposed algorithm provides consistent improvement in various inverse problems.", "pdf": "/pdf/8bc038924aef9dc0a4d004adf50f08a472fe6d60.pdf", "paperhash": "cha|boosting_encoderdecoder_cnn_for_inverse_problems", "original_pdf": "/attachment/ac0989bae63a902dfc9d9f2629f316e0ba7307e8.pdf", "_bibtex": "@misc{\ncha2020boosting,\ntitle={{\\{}BOOSTING{\\}} {\\{}ENCODER{\\}}-{\\{}DECODER{\\}} {\\{}CNN{\\}} {\\{}FOR{\\}} {\\{}INVERSE{\\}} {\\{}PROBLEMS{\\}}},\nauthor={Eunju Cha and Jaeduck Jang and Junho Lee and Eunha Lee and Jong Chul Ye},\nyear={2020},\nurl={https://openreview.net/forum?id=BJevihVtwB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BJevihVtwB", "replyto": "BJevihVtwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795723823, "tmdate": 1576800275367, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper154/-/Decision"}}}, {"id": "SkgVua6MoB", "original": null, "number": 2, "cdate": 1573211500331, "ddate": null, "tcdate": 1573211500331, "tmdate": 1573692102117, "tddate": null, "forum": "BJevihVtwB", "replyto": "HJejsuDCtH", "invitation": "ICLR.cc/2020/Conference/Paper154/-/Official_Comment", "content": {"title": "Reply to Reviewer1: We have clarified the contents to avoid confusion by readers", "comment": "General Comments: \n==> Thanks for the constructive comments. In the revised article and in this letter we have done our best to clarify the contents of the article and to avoid confusion by the reviewers.\n\n1.(1) I think Proposition 1 has minor errors, there is no need to apply Jensen's inequality since there's nothing random, but I think the claim is correct -- it is trivial. \n\n==> We would like to assure the reviewer that Jensen's inequality does NOT necessarily require a random variable since it relates the value of a convex function of an integral (or sum) to the integral (or sum) of the convex function (Please see https://en.wikipedia.org/wiki/Jensen%27s_inequality ).  In fact, Jensen's inequality in the probability theory is a special case of the original Jensen's inequality when a convex function of a random variable is used.\n\n1.(2)- In experiments, they use attention network which is not a CNN, so I'm not sure how any of the theory applies to this case, can you please clarify?\n\n==> Thanks for the comment. We would also like to assure the reviewer that the attention module provides only weighting factors $ \\ {w_k \\} $ for the weighted average calculation in (13), which has nothing to do with the kernel-type results for the encoder-decoder CNN that can simplify the divergence term in the SURE Estimator in (8). Therefore, it does not matter whether it is either in the form of CNN or a fully connected network.  Since we only calculate the K-scalar values in the attention module, the operation is similar to the last layer in a classifier. This led us to use a simple fully connected layer.\n\n2. Experimental focus of the paper is to analyze biomedical datasets -- HCP, EDX and the authors compared their method to *only* one baseline. I suggest that they perform some more comparisons on natural images like http://vllab.ucmerced.edu/wlai24/cvpr16_deblur_study/\n\n==> Thanks for the constructive comment.  In our revised manuscript (which has been uploaded),  more comparison results on natural images for the super-resolution task are also provided in the Appendix. Moreover, the comparison results with the standard bagging baseline are also provided. We believe that the additional experimental results clearly showed that the proposed method provides better performance."}, "signatures": ["ICLR.cc/2020/Conference/Paper154/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper154/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "BOOSTING ENCODER-DECODER CNN FOR INVERSE PROBLEMS", "authors": ["Eunju Cha", "Jaeduck Jang", "Junho Lee", "Eunha Lee", "Jong Chul Ye"], "authorids": ["eunju.cha@kaist.ac.kr", "jduck.jang@samsung.com", "jh0325.lee@samsung.com", "eunhayo.lee@samsung.com", "jong.ye@kaist.ac.kr"], "keywords": ["Prediction error", "Boosting", "Encoder-decoder convolutional neural network", "Inverse problem"], "abstract": "Encoder-decoder convolutional neural networks  (CNN)  have been extensively used for various  inverse problems. However,  their prediction error  for unseen test data is difficult to estimate  a priori, since the neural networks are trained using only selected data and their architectures are largely considered blackboxes. This poses a fundamental  challenge in improving the performance of  neural networks. Recently, it was shown that Stein\u2019s unbiased risk estimator (SURE) can be used as an unbiased estimator of the prediction error for denoising problems. However, the computation of the divergence term in SURE is difficult to implement in a neural network framework, and the condition to avoid trivial identity mapping is not well defined. In this paper, inspired by the finding that  an encoder-decoder CNN can be expressed as a piecewise linear representation, we provide a  close form expression of  the unbiased estimator for the prediction error. The close form representation leads to a novel boosting scheme to prevent a neural network from converging to an identity mapping so that it can enhance the performance. Experimental results show that the proposed algorithm provides consistent improvement in various inverse problems.", "pdf": "/pdf/8bc038924aef9dc0a4d004adf50f08a472fe6d60.pdf", "paperhash": "cha|boosting_encoderdecoder_cnn_for_inverse_problems", "original_pdf": "/attachment/ac0989bae63a902dfc9d9f2629f316e0ba7307e8.pdf", "_bibtex": "@misc{\ncha2020boosting,\ntitle={{\\{}BOOSTING{\\}} {\\{}ENCODER{\\}}-{\\{}DECODER{\\}} {\\{}CNN{\\}} {\\{}FOR{\\}} {\\{}INVERSE{\\}} {\\{}PROBLEMS{\\}}},\nauthor={Eunju Cha and Jaeduck Jang and Junho Lee and Eunha Lee and Jong Chul Ye},\nyear={2020},\nurl={https://openreview.net/forum?id=BJevihVtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJevihVtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper154/Authors", "ICLR.cc/2020/Conference/Paper154/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper154/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper154/Reviewers", "ICLR.cc/2020/Conference/Paper154/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper154/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper154/Authors|ICLR.cc/2020/Conference/Paper154/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175551, "tmdate": 1576860548605, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper154/Authors", "ICLR.cc/2020/Conference/Paper154/Reviewers", "ICLR.cc/2020/Conference/Paper154/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper154/-/Official_Comment"}}}, {"id": "rklQHI6zsB", "original": null, "number": 1, "cdate": 1573209659078, "ddate": null, "tcdate": 1573209659078, "tmdate": 1573692009841, "tddate": null, "forum": "BJevihVtwB", "replyto": "SyegTqoCFr", "invitation": "ICLR.cc/2020/Conference/Paper154/-/Official_Comment", "content": {"title": "Reply to Reviewer2: We have addressed your concern", "comment": "General Comments:\n==> We appreciate the reviewer for careful reading and constructive comments.  We have revised the paper accordingly. In particular, we have clearly explained the motivation and provided experimental results that clearly show the advantages of the weighted average from the attention module over the standard bagging baseline.\n\n1. In the model description part, the intuition behind the attention modules is never mentioned. It will be nice to explain the intuition and possibly attached the derivation of the loss function to the attention modules. \n\n==>  Thank you for the constructive comments. Although the standard way of aggregation in the bagging estimator is a simple average of the overall results from the regression network, this may not be the best method when the number of bootstrap subsampling is limited. Therefore,   we propose a weighted averaging scheme whose weight is calculated by the data attention module so that it efficiently combines all data by adaptively incorporating output from various bootstrap sub-sampling patterns. As shown in Fig. 4 and Fig. 5, the weighted averaging from the weights calculated by the attention module significantly improved the performance.\n\n2. the author seems misunderstand the difference between boosting and bagging. The way described in the paper is bagging and in order to do boosting, a sequential type of network structure probably need to be proposed.\n\n==> Thanks for your careful reading. Although the term \"boosting\" could be used for general performance improvements, we agree with the reviewer and this revision uses the more accurate term \"bootstrap and subsampling (bagging)\".\n\n3. How will be model performance compared with simple bagging for the baseline compared in the experiment part?\n\n==> Thanks for the suggestion.  Fig. 4 and Fig. 5 now clearly show that the proposed weighted average significantly outperformed the simple bagging baseline."}, "signatures": ["ICLR.cc/2020/Conference/Paper154/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper154/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "BOOSTING ENCODER-DECODER CNN FOR INVERSE PROBLEMS", "authors": ["Eunju Cha", "Jaeduck Jang", "Junho Lee", "Eunha Lee", "Jong Chul Ye"], "authorids": ["eunju.cha@kaist.ac.kr", "jduck.jang@samsung.com", "jh0325.lee@samsung.com", "eunhayo.lee@samsung.com", "jong.ye@kaist.ac.kr"], "keywords": ["Prediction error", "Boosting", "Encoder-decoder convolutional neural network", "Inverse problem"], "abstract": "Encoder-decoder convolutional neural networks  (CNN)  have been extensively used for various  inverse problems. However,  their prediction error  for unseen test data is difficult to estimate  a priori, since the neural networks are trained using only selected data and their architectures are largely considered blackboxes. This poses a fundamental  challenge in improving the performance of  neural networks. Recently, it was shown that Stein\u2019s unbiased risk estimator (SURE) can be used as an unbiased estimator of the prediction error for denoising problems. However, the computation of the divergence term in SURE is difficult to implement in a neural network framework, and the condition to avoid trivial identity mapping is not well defined. In this paper, inspired by the finding that  an encoder-decoder CNN can be expressed as a piecewise linear representation, we provide a  close form expression of  the unbiased estimator for the prediction error. The close form representation leads to a novel boosting scheme to prevent a neural network from converging to an identity mapping so that it can enhance the performance. Experimental results show that the proposed algorithm provides consistent improvement in various inverse problems.", "pdf": "/pdf/8bc038924aef9dc0a4d004adf50f08a472fe6d60.pdf", "paperhash": "cha|boosting_encoderdecoder_cnn_for_inverse_problems", "original_pdf": "/attachment/ac0989bae63a902dfc9d9f2629f316e0ba7307e8.pdf", "_bibtex": "@misc{\ncha2020boosting,\ntitle={{\\{}BOOSTING{\\}} {\\{}ENCODER{\\}}-{\\{}DECODER{\\}} {\\{}CNN{\\}} {\\{}FOR{\\}} {\\{}INVERSE{\\}} {\\{}PROBLEMS{\\}}},\nauthor={Eunju Cha and Jaeduck Jang and Junho Lee and Eunha Lee and Jong Chul Ye},\nyear={2020},\nurl={https://openreview.net/forum?id=BJevihVtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJevihVtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper154/Authors", "ICLR.cc/2020/Conference/Paper154/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper154/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper154/Reviewers", "ICLR.cc/2020/Conference/Paper154/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper154/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper154/Authors|ICLR.cc/2020/Conference/Paper154/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175551, "tmdate": 1576860548605, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper154/Authors", "ICLR.cc/2020/Conference/Paper154/Reviewers", "ICLR.cc/2020/Conference/Paper154/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper154/-/Official_Comment"}}}, {"id": "B1lIQuQIir", "original": null, "number": 4, "cdate": 1573431326166, "ddate": null, "tcdate": 1573431326166, "tmdate": 1573691794899, "tddate": null, "forum": "BJevihVtwB", "replyto": "BJevihVtwB", "invitation": "ICLR.cc/2020/Conference/Paper154/-/Official_Comment", "content": {"title": "General Comments to Reviewers", "comment": "We thank all reviewers for their careful reviews and constructive comments. The reviewers agreed that the experiment results and the theoretical derivation are encouraging. To address the comment that the paper needs clarification on a few points, we have significantly revised the paper to make the content clears.  The revised paper has been uploaded. The major changes are as follows.\n\n1.  Clarification of attention module\n\nAlthough the standard way of aggregation in the bagging estimator is a simple average of the overall results from the regression network, this may not be the best method when the number of bootstrap subsampling is limited. Therefore,   we propose a weighted averaging scheme whose weight is calculated by the data attention module so that it efficiently combines all data by adaptively incorporating output from various bootstrap sub-sampling patterns. As shown in Fig. 4 and Fig. 5, the weighted averaging from the weights calculated by the attention module significantly improved the performance.\n\nWe also clarified that the attention module is not necessary a CNN to make the theory valid. Note that the attention module provides only weighting factors $\\{w_k \\} $ for the weighted average calculation in (13), which has nothing to do with the kernel-type results for the encoder-decoder CNN that can simplify the divergence term in the SURE Estimator in (8). Since we only calculate the K-scalar values in the attention module, the operation is similar to the last layer in a classifier. This led us to use a simple fully connected layer.\n\n2. Additional Experiments\n\nIn our revised manuscript,  more comparison results on natural images for the super-resolution task are also provided in the Appendix. Moreover, the comparison results with the \"standard bagging baseline\" are also provided. The additional experimental results clearly showed that the proposed method provides better performance.\n\n\n3. Other clarification\n\nWe have also clarified that PSNR and SSIM values in the table are for the all test data set, whereas those in the figures are for each reconstruction result. In the reply letter, we also pointed out that Jensen's inequality does NOT necessarily require a random variable since it relates the value of a convex function of an integral (or sum) to the integral (or sum) of the convex function.  To avoid potential confusion,  we use the term \"bootstrap and subsampling (bagging)\" consistently throughout the paper. We have also changed the title accordingly. Other minor typos and grammatical errors have been corrected."}, "signatures": ["ICLR.cc/2020/Conference/Paper154/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper154/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "BOOSTING ENCODER-DECODER CNN FOR INVERSE PROBLEMS", "authors": ["Eunju Cha", "Jaeduck Jang", "Junho Lee", "Eunha Lee", "Jong Chul Ye"], "authorids": ["eunju.cha@kaist.ac.kr", "jduck.jang@samsung.com", "jh0325.lee@samsung.com", "eunhayo.lee@samsung.com", "jong.ye@kaist.ac.kr"], "keywords": ["Prediction error", "Boosting", "Encoder-decoder convolutional neural network", "Inverse problem"], "abstract": "Encoder-decoder convolutional neural networks  (CNN)  have been extensively used for various  inverse problems. However,  their prediction error  for unseen test data is difficult to estimate  a priori, since the neural networks are trained using only selected data and their architectures are largely considered blackboxes. This poses a fundamental  challenge in improving the performance of  neural networks. Recently, it was shown that Stein\u2019s unbiased risk estimator (SURE) can be used as an unbiased estimator of the prediction error for denoising problems. However, the computation of the divergence term in SURE is difficult to implement in a neural network framework, and the condition to avoid trivial identity mapping is not well defined. In this paper, inspired by the finding that  an encoder-decoder CNN can be expressed as a piecewise linear representation, we provide a  close form expression of  the unbiased estimator for the prediction error. The close form representation leads to a novel boosting scheme to prevent a neural network from converging to an identity mapping so that it can enhance the performance. Experimental results show that the proposed algorithm provides consistent improvement in various inverse problems.", "pdf": "/pdf/8bc038924aef9dc0a4d004adf50f08a472fe6d60.pdf", "paperhash": "cha|boosting_encoderdecoder_cnn_for_inverse_problems", "original_pdf": "/attachment/ac0989bae63a902dfc9d9f2629f316e0ba7307e8.pdf", "_bibtex": "@misc{\ncha2020boosting,\ntitle={{\\{}BOOSTING{\\}} {\\{}ENCODER{\\}}-{\\{}DECODER{\\}} {\\{}CNN{\\}} {\\{}FOR{\\}} {\\{}INVERSE{\\}} {\\{}PROBLEMS{\\}}},\nauthor={Eunju Cha and Jaeduck Jang and Junho Lee and Eunha Lee and Jong Chul Ye},\nyear={2020},\nurl={https://openreview.net/forum?id=BJevihVtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJevihVtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper154/Authors", "ICLR.cc/2020/Conference/Paper154/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper154/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper154/Reviewers", "ICLR.cc/2020/Conference/Paper154/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper154/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper154/Authors|ICLR.cc/2020/Conference/Paper154/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175551, "tmdate": 1576860548605, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper154/Authors", "ICLR.cc/2020/Conference/Paper154/Reviewers", "ICLR.cc/2020/Conference/Paper154/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper154/-/Official_Comment"}}}, {"id": "ByxXMf0fsB", "original": null, "number": 3, "cdate": 1573212682939, "ddate": null, "tcdate": 1573212682939, "tmdate": 1573217951172, "tddate": null, "forum": "BJevihVtwB", "replyto": "Bkg9o73nKH", "invitation": "ICLR.cc/2020/Conference/Paper154/-/Official_Comment", "content": {"title": "Reply to Reviewer3: Thanks for the positive comments", "comment": "However taking that as a given, they well-motive the proposed architecture and achieve impressive experimental results. The experiments are well described.\n\n==> Thanks for your understanding. We have also added additional experiments for the super-resolution tasks with natural images in the revised paper. Additionally, the revised paper also provides a comparison with the standard bagging baseline to show that adaptive averaging with the help of the attention module improves the performance. The results consistently show that the proposed method is better than the existing approaches.\n\n3. Questions \na) Why do Table 1 and Figure 3 provide different PSNR and SSIM values?\n\n==> Fig. 3 was the PSNR/SSIM value for the individual figures, whereas Table 1 provides the average values for the *entire* test data set. We have clarified this in the main context in the revised paper.\n\nb) Is there any way to measure accuracy to ground-truth with the EDX data? Or are the results just qualitative? \n\n==> For the EDX case, no truth-relevant data is available for supervised training. This was the main reason why we developed our method. However, we would like to assure the reviewer that our material scientist has confirmed that the denoised images are clearly consistent with the expected layered structures of the quantum dots that they had expected from their manufacturing protocols.   To provide more experimental results with the groud-truth data, the revised manuscript also provides the additional experimental results for super-resolution tasks. The results clearly show that the proposed method improves the performance.\n\nc) With respect to Figure 2, and in general for autoencoders, the input and output have the same dimension. So how do you reconcile this with undersampled MRI and EDX data? I understand you train on fully sampled data\u2014then how do you input undersampled data? Are the unknown samples set to zero?\n\n==>  Yes, you are right.  Thanks for your careful observation.  For the MRI case, we use the zero-filled k-space data as our network input, and for the EDX case, we used the noisy image as input since they are the same size as the network output. "}, "signatures": ["ICLR.cc/2020/Conference/Paper154/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper154/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "BOOSTING ENCODER-DECODER CNN FOR INVERSE PROBLEMS", "authors": ["Eunju Cha", "Jaeduck Jang", "Junho Lee", "Eunha Lee", "Jong Chul Ye"], "authorids": ["eunju.cha@kaist.ac.kr", "jduck.jang@samsung.com", "jh0325.lee@samsung.com", "eunhayo.lee@samsung.com", "jong.ye@kaist.ac.kr"], "keywords": ["Prediction error", "Boosting", "Encoder-decoder convolutional neural network", "Inverse problem"], "abstract": "Encoder-decoder convolutional neural networks  (CNN)  have been extensively used for various  inverse problems. However,  their prediction error  for unseen test data is difficult to estimate  a priori, since the neural networks are trained using only selected data and their architectures are largely considered blackboxes. This poses a fundamental  challenge in improving the performance of  neural networks. Recently, it was shown that Stein\u2019s unbiased risk estimator (SURE) can be used as an unbiased estimator of the prediction error for denoising problems. However, the computation of the divergence term in SURE is difficult to implement in a neural network framework, and the condition to avoid trivial identity mapping is not well defined. In this paper, inspired by the finding that  an encoder-decoder CNN can be expressed as a piecewise linear representation, we provide a  close form expression of  the unbiased estimator for the prediction error. The close form representation leads to a novel boosting scheme to prevent a neural network from converging to an identity mapping so that it can enhance the performance. Experimental results show that the proposed algorithm provides consistent improvement in various inverse problems.", "pdf": "/pdf/8bc038924aef9dc0a4d004adf50f08a472fe6d60.pdf", "paperhash": "cha|boosting_encoderdecoder_cnn_for_inverse_problems", "original_pdf": "/attachment/ac0989bae63a902dfc9d9f2629f316e0ba7307e8.pdf", "_bibtex": "@misc{\ncha2020boosting,\ntitle={{\\{}BOOSTING{\\}} {\\{}ENCODER{\\}}-{\\{}DECODER{\\}} {\\{}CNN{\\}} {\\{}FOR{\\}} {\\{}INVERSE{\\}} {\\{}PROBLEMS{\\}}},\nauthor={Eunju Cha and Jaeduck Jang and Junho Lee and Eunha Lee and Jong Chul Ye},\nyear={2020},\nurl={https://openreview.net/forum?id=BJevihVtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJevihVtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper154/Authors", "ICLR.cc/2020/Conference/Paper154/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper154/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper154/Reviewers", "ICLR.cc/2020/Conference/Paper154/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper154/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper154/Authors|ICLR.cc/2020/Conference/Paper154/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175551, "tmdate": 1576860548605, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper154/Authors", "ICLR.cc/2020/Conference/Paper154/Reviewers", "ICLR.cc/2020/Conference/Paper154/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper154/-/Official_Comment"}}}, {"id": "Bkg9o73nKH", "original": null, "number": 1, "cdate": 1571763105685, "ddate": null, "tcdate": 1571763105685, "tmdate": 1572972632017, "tddate": null, "forum": "BJevihVtwB", "replyto": "BJevihVtwB", "invitation": "ICLR.cc/2020/Conference/Paper154/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "1. Summary\nThe authors address the problem of efficiently employing the SURE estimator as a network training regularizer. They show that for CNN autoencoders this can be efficiently computed. Their other contribution is a bagging/boosting technique which is proved to avoid trivial solutions. The proposed architecture, motivated by the theoretical statements, is shown to outperform classic and 2019 state of the art image reconstruction algorithms in MRI and EDX.\n2. Decision and arguments\nUnfortunately this paper is outside my expertise so I can\u2019t evaluate the novelty of the theoretical accomplishments. However taking that as a given, they well-motive the proposed architecture and achieve impressive experimental results. The experiments are well described.\n3. Questions \na) Why do Table 1 and Figure 3 provide different PSNR and SSIM values?\nb) Is there any way to measure accuracy to ground-truth with the EDX data? Or are the results just qualitative? \nc) With respect to Figure 2, and in general for autoencoders, the input and output have the same dimension. So how do you reconcile this with undersampled MRI and EDX data? I understand you train on fully sampled data\u2014then how do you input undersampled data? Are the unknown samples set to zero?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper154/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper154/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "BOOSTING ENCODER-DECODER CNN FOR INVERSE PROBLEMS", "authors": ["Eunju Cha", "Jaeduck Jang", "Junho Lee", "Eunha Lee", "Jong Chul Ye"], "authorids": ["eunju.cha@kaist.ac.kr", "jduck.jang@samsung.com", "jh0325.lee@samsung.com", "eunhayo.lee@samsung.com", "jong.ye@kaist.ac.kr"], "keywords": ["Prediction error", "Boosting", "Encoder-decoder convolutional neural network", "Inverse problem"], "abstract": "Encoder-decoder convolutional neural networks  (CNN)  have been extensively used for various  inverse problems. However,  their prediction error  for unseen test data is difficult to estimate  a priori, since the neural networks are trained using only selected data and their architectures are largely considered blackboxes. This poses a fundamental  challenge in improving the performance of  neural networks. Recently, it was shown that Stein\u2019s unbiased risk estimator (SURE) can be used as an unbiased estimator of the prediction error for denoising problems. However, the computation of the divergence term in SURE is difficult to implement in a neural network framework, and the condition to avoid trivial identity mapping is not well defined. In this paper, inspired by the finding that  an encoder-decoder CNN can be expressed as a piecewise linear representation, we provide a  close form expression of  the unbiased estimator for the prediction error. The close form representation leads to a novel boosting scheme to prevent a neural network from converging to an identity mapping so that it can enhance the performance. Experimental results show that the proposed algorithm provides consistent improvement in various inverse problems.", "pdf": "/pdf/8bc038924aef9dc0a4d004adf50f08a472fe6d60.pdf", "paperhash": "cha|boosting_encoderdecoder_cnn_for_inverse_problems", "original_pdf": "/attachment/ac0989bae63a902dfc9d9f2629f316e0ba7307e8.pdf", "_bibtex": "@misc{\ncha2020boosting,\ntitle={{\\{}BOOSTING{\\}} {\\{}ENCODER{\\}}-{\\{}DECODER{\\}} {\\{}CNN{\\}} {\\{}FOR{\\}} {\\{}INVERSE{\\}} {\\{}PROBLEMS{\\}}},\nauthor={Eunju Cha and Jaeduck Jang and Junho Lee and Eunha Lee and Jong Chul Ye},\nyear={2020},\nurl={https://openreview.net/forum?id=BJevihVtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJevihVtwB", "replyto": "BJevihVtwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper154/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper154/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576553221731, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper154/Reviewers"], "noninvitees": [], "tcdate": 1570237756227, "tmdate": 1576553221745, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper154/-/Official_Review"}}}, {"id": "HJejsuDCtH", "original": null, "number": 2, "cdate": 1571874979292, "ddate": null, "tcdate": 1571874979292, "tmdate": 1572972631968, "tddate": null, "forum": "BJevihVtwB", "replyto": "BJevihVtwB", "invitation": "ICLR.cc/2020/Conference/Paper154/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary: The authors consider an encoder decoder setup for linear deblurring problem and propose efficient boosting estimators. Specifically, they use the Stein's unbiased risk estimator for the problem when the noise is gaussian. In the case when the encoder and decoder is represented by a convolutional neural network with RELU activations, they show how they can exploit the recent theoretical results that show the kernel type results to make their procedure efficient. They then propose using a set of models (boosting) and prove that the boosted loss function lower bounds the \"nonboosted\" loss function.\n\n1. I think Proposition 1 has minor errors, there is no need to apply Jensen's inequality since there's nothing random, but I think the claim is correct -- it is trivial. In experiments, they use attention network which is not a CNN, so I'm not sure how any of the theory applies to this case, can you please clarify?\n\n2. Experimental focus of the paper is to analyze biomedical datasets -- HCP, EDX and the authors compared their method to *only* one baseline. I suggest that they perform some more comparisons on natural images like http://vllab.ucmerced.edu/wlai24/cvpr16_deblur_study/"}, "signatures": ["ICLR.cc/2020/Conference/Paper154/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper154/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "BOOSTING ENCODER-DECODER CNN FOR INVERSE PROBLEMS", "authors": ["Eunju Cha", "Jaeduck Jang", "Junho Lee", "Eunha Lee", "Jong Chul Ye"], "authorids": ["eunju.cha@kaist.ac.kr", "jduck.jang@samsung.com", "jh0325.lee@samsung.com", "eunhayo.lee@samsung.com", "jong.ye@kaist.ac.kr"], "keywords": ["Prediction error", "Boosting", "Encoder-decoder convolutional neural network", "Inverse problem"], "abstract": "Encoder-decoder convolutional neural networks  (CNN)  have been extensively used for various  inverse problems. However,  their prediction error  for unseen test data is difficult to estimate  a priori, since the neural networks are trained using only selected data and their architectures are largely considered blackboxes. This poses a fundamental  challenge in improving the performance of  neural networks. Recently, it was shown that Stein\u2019s unbiased risk estimator (SURE) can be used as an unbiased estimator of the prediction error for denoising problems. However, the computation of the divergence term in SURE is difficult to implement in a neural network framework, and the condition to avoid trivial identity mapping is not well defined. In this paper, inspired by the finding that  an encoder-decoder CNN can be expressed as a piecewise linear representation, we provide a  close form expression of  the unbiased estimator for the prediction error. The close form representation leads to a novel boosting scheme to prevent a neural network from converging to an identity mapping so that it can enhance the performance. Experimental results show that the proposed algorithm provides consistent improvement in various inverse problems.", "pdf": "/pdf/8bc038924aef9dc0a4d004adf50f08a472fe6d60.pdf", "paperhash": "cha|boosting_encoderdecoder_cnn_for_inverse_problems", "original_pdf": "/attachment/ac0989bae63a902dfc9d9f2629f316e0ba7307e8.pdf", "_bibtex": "@misc{\ncha2020boosting,\ntitle={{\\{}BOOSTING{\\}} {\\{}ENCODER{\\}}-{\\{}DECODER{\\}} {\\{}CNN{\\}} {\\{}FOR{\\}} {\\{}INVERSE{\\}} {\\{}PROBLEMS{\\}}},\nauthor={Eunju Cha and Jaeduck Jang and Junho Lee and Eunha Lee and Jong Chul Ye},\nyear={2020},\nurl={https://openreview.net/forum?id=BJevihVtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJevihVtwB", "replyto": "BJevihVtwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper154/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper154/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576553221731, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper154/Reviewers"], "noninvitees": [], "tcdate": 1570237756227, "tmdate": 1576553221745, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper154/-/Official_Review"}}}, {"id": "SyegTqoCFr", "original": null, "number": 3, "cdate": 1571891896281, "ddate": null, "tcdate": 1571891896281, "tmdate": 1572972631920, "tddate": null, "forum": "BJevihVtwB", "replyto": "BJevihVtwB", "invitation": "ICLR.cc/2020/Conference/Paper154/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposed a piecewise linear close form expression for the Stein\u2019s unbiased risk estimator and use this formulation to construct a new Encoder-decoder convolutional neural network. The author claimed that this closely related to bagging. Improved experimental results on two inverse problems are presented. Overall, the experiment results are encouraging but the paper need clarification on a few points.\n\n1. In the model description part, the intuition behind the attention modules is never mentioned. It will be nice to explain the intuition and possibly attached the derivation of the loss function the attention modules. \n\n2. the author seems misunderstand the difference between boosting and bagging. The way described in the paper is bagging and in order to do boosting, a sequential type of network structure probably need to be proposed.\n\n3. How will be model performance compared with a simple bagging for the baseline compared in the experiment part?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper154/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper154/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "BOOSTING ENCODER-DECODER CNN FOR INVERSE PROBLEMS", "authors": ["Eunju Cha", "Jaeduck Jang", "Junho Lee", "Eunha Lee", "Jong Chul Ye"], "authorids": ["eunju.cha@kaist.ac.kr", "jduck.jang@samsung.com", "jh0325.lee@samsung.com", "eunhayo.lee@samsung.com", "jong.ye@kaist.ac.kr"], "keywords": ["Prediction error", "Boosting", "Encoder-decoder convolutional neural network", "Inverse problem"], "abstract": "Encoder-decoder convolutional neural networks  (CNN)  have been extensively used for various  inverse problems. However,  their prediction error  for unseen test data is difficult to estimate  a priori, since the neural networks are trained using only selected data and their architectures are largely considered blackboxes. This poses a fundamental  challenge in improving the performance of  neural networks. Recently, it was shown that Stein\u2019s unbiased risk estimator (SURE) can be used as an unbiased estimator of the prediction error for denoising problems. However, the computation of the divergence term in SURE is difficult to implement in a neural network framework, and the condition to avoid trivial identity mapping is not well defined. In this paper, inspired by the finding that  an encoder-decoder CNN can be expressed as a piecewise linear representation, we provide a  close form expression of  the unbiased estimator for the prediction error. The close form representation leads to a novel boosting scheme to prevent a neural network from converging to an identity mapping so that it can enhance the performance. Experimental results show that the proposed algorithm provides consistent improvement in various inverse problems.", "pdf": "/pdf/8bc038924aef9dc0a4d004adf50f08a472fe6d60.pdf", "paperhash": "cha|boosting_encoderdecoder_cnn_for_inverse_problems", "original_pdf": "/attachment/ac0989bae63a902dfc9d9f2629f316e0ba7307e8.pdf", "_bibtex": "@misc{\ncha2020boosting,\ntitle={{\\{}BOOSTING{\\}} {\\{}ENCODER{\\}}-{\\{}DECODER{\\}} {\\{}CNN{\\}} {\\{}FOR{\\}} {\\{}INVERSE{\\}} {\\{}PROBLEMS{\\}}},\nauthor={Eunju Cha and Jaeduck Jang and Junho Lee and Eunha Lee and Jong Chul Ye},\nyear={2020},\nurl={https://openreview.net/forum?id=BJevihVtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJevihVtwB", "replyto": "BJevihVtwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper154/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper154/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576553221731, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper154/Reviewers"], "noninvitees": [], "tcdate": 1570237756227, "tmdate": 1576553221745, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper154/-/Official_Review"}}}], "count": 9}