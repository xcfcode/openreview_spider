{"notes": [{"id": "McYsRk9-rso", "original": "oiV1RVIwwe", "number": 1671, "cdate": 1601308184967, "ddate": null, "tcdate": 1601308184967, "tmdate": 1614985773216, "tddate": null, "forum": "McYsRk9-rso", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Reducing Implicit Bias in Latent Domain Learning", "authorids": ["~Lucas_Deecke1", "~Timothy_Hospedales1", "~Hakan_Bilen1"], "authors": ["Lucas Deecke", "Timothy Hospedales", "Hakan Bilen"], "keywords": ["Latent Domain Learning", "CNN Architectures"], "abstract": "A fundamental shortcoming of deep neural networks is their specialization to a single task and domain. While recent techniques in multi-domain learning enable the learning of more domain-agnostic features, their success relies firmly on the presence of domain labels, typically requiring manual annotation and careful curation of datasets. Here we focus on latent domain learning, a highly realistic, yet less explored scenario: learning from data from different domains, without access to domain annotations. This is a particularly challenging problem, since standard models exhibit an implicit bias toward learning only the large domains in data, while disregarding smaller ones. To address this issue, we propose dynamic residual adapters that adaptively account for latent domains, and weighted domain transfer \u2013 a novel augmentation strategy designed specifically for this setting. Our techniques are evaluated on image classification tasks containing multiple unannotated domains, and we demonstrate they enhance performance, in particular, on the smallest of these.", "one-sentence_summary": "We demonstrate that standard models suppress underrepresented latent domains, and formulate novel strategies to limit this behavior.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deecke|reducing_implicit_bias_in_latent_domain_learning", "pdf": "/pdf/492a085a079e0510023ccd898f834aab15947d02.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iqOQ7bIUbc", "_bibtex": "@misc{\ndeecke2021reducing,\ntitle={Reducing Implicit Bias in Latent Domain Learning},\nauthor={Lucas Deecke and Timothy Hospedales and Hakan Bilen},\nyear={2021},\nurl={https://openreview.net/forum?id=McYsRk9-rso}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "13Y-Rfnb9bb", "original": null, "number": 1, "cdate": 1610040358346, "ddate": null, "tcdate": 1610040358346, "tmdate": 1610473948236, "tddate": null, "forum": "McYsRk9-rso", "replyto": "McYsRk9-rso", "invitation": "ICLR.cc/2021/Conference/Paper1671/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "While all reviewers agree that the topic is interesting and the work has merit, several issues have been pointed out, especially by R1 and R3, that indicate that the work is not  ready for acceptance at this stage. the authors are strongly encouraged to continue to work on this topic, taking into account the feedback received."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reducing Implicit Bias in Latent Domain Learning", "authorids": ["~Lucas_Deecke1", "~Timothy_Hospedales1", "~Hakan_Bilen1"], "authors": ["Lucas Deecke", "Timothy Hospedales", "Hakan Bilen"], "keywords": ["Latent Domain Learning", "CNN Architectures"], "abstract": "A fundamental shortcoming of deep neural networks is their specialization to a single task and domain. While recent techniques in multi-domain learning enable the learning of more domain-agnostic features, their success relies firmly on the presence of domain labels, typically requiring manual annotation and careful curation of datasets. Here we focus on latent domain learning, a highly realistic, yet less explored scenario: learning from data from different domains, without access to domain annotations. This is a particularly challenging problem, since standard models exhibit an implicit bias toward learning only the large domains in data, while disregarding smaller ones. To address this issue, we propose dynamic residual adapters that adaptively account for latent domains, and weighted domain transfer \u2013 a novel augmentation strategy designed specifically for this setting. Our techniques are evaluated on image classification tasks containing multiple unannotated domains, and we demonstrate they enhance performance, in particular, on the smallest of these.", "one-sentence_summary": "We demonstrate that standard models suppress underrepresented latent domains, and formulate novel strategies to limit this behavior.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deecke|reducing_implicit_bias_in_latent_domain_learning", "pdf": "/pdf/492a085a079e0510023ccd898f834aab15947d02.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iqOQ7bIUbc", "_bibtex": "@misc{\ndeecke2021reducing,\ntitle={Reducing Implicit Bias in Latent Domain Learning},\nauthor={Lucas Deecke and Timothy Hospedales and Hakan Bilen},\nyear={2021},\nurl={https://openreview.net/forum?id=McYsRk9-rso}\n}"}, "tags": [], "invitation": {"reply": {"forum": "McYsRk9-rso", "replyto": "McYsRk9-rso", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040358333, "tmdate": 1610473948216, "id": "ICLR.cc/2021/Conference/Paper1671/-/Decision"}}}, {"id": "qJ1d_DXvR5A", "original": null, "number": 3, "cdate": 1603891260943, "ddate": null, "tcdate": 1603891260943, "tmdate": 1606312697232, "tddate": null, "forum": "McYsRk9-rso", "replyto": "McYsRk9-rso", "invitation": "ICLR.cc/2021/Conference/Paper1671/-/Official_Review", "content": {"title": "Review #1", "review": "------ Update after discussion with authors ---------\n\nI would like to thanks the author for their efforts by adding additional experiments, which surely enhances the significance of the proposed approach. Based on these, I increased my score to 5.\n\nI have re-checked the final revised version, I think the current version still *requires proper organizations and justifications*. For example, the added experiments still talked about the accuracy, the in-depth analysis seems lacking. I think a substantial revision of the paper in terms of structure, idea presentation, and analysis is still needed. Based on this, my final score is 5.\n\n-----------------------------------\nSummary: \n\nThis paper studied how to learn a neural network with multiple domains without knowing the exact domain label (by merging all the domains as a large domain). Then they proposed dynamic residual adapters and weighted domain transfer to address this issue. The empirical results showed its practical benefits.\n\n------------------------------------------------------\n\nOverall review \n\nPros:\n\n[1] This paper is well-motivated. I like the analyzed scenario and I think it can have a strong practical utility. \n\n[2] The high level of proposed ideas is technically sound. \n\nCons:\n\n[1] The submitted version seems to be a preliminary version with many missing and unclear elements.\n\n[2] As an **empirical** paper, the experimental results are not sufficient for ICLR.\n\n[3] Some technical details need better justifications and discussions.\n\nBased on these, I recommend a rejection at this time but encourage a major revision for resubmission.\n\n----------------------------------------------------\n\nDetailed explanations\n\n[1] Missing elements\n\n[a] I am rather confused and unclear about the whole learning procedure. It seems the author used DRA in the residual module. However, the role of WDT is unclear. What is the global training loss in the proposed approach? WDT is a part of the loss or used for analyzing the problem?  I would like to see a pseudocode/protocol for the whole algorithm or a clear network structure for illustrating the idea in Sec 3.3-3.4.\n\n[b] The mathematical notations defined in this paper are presented oddly (particularly in sec 3) for example:\n\n[b1] Equation (1), $\\alpha$ and $\\theta$ are scalars or vectors? what is meaning for $|\\alpha|\\ll |\\theta|$? I guess it is $\\text{dim}(\\alpha)$ but it makes me rather confused.\n\n[b2] The same problem for eq(2) and $\\epsilon$\n \n[b3] In WDT, the same problem for $\\delta$, $\\delta_i$ and $\\delta_j$\n\nThese confusions make it more difficult to understand the approach.\n\n[2] The empirical results\n\nThe current empirical results only compare MLFN, which is not sufficient. \n\nI noticed the author claimed, \u201cNote the goal here is not to compare to the performance of existing multi-domain approaches that Visual Decathlon was designed for, but to show that deep networks struggle with learning small latent domains when no domain annotations are provided.\u201d\n\nI agree with this opinion if the paper aims to only analyze this scenario (generally from a theoretical perspective).  These kinds of experiments are sufficient. \n\nBy contrast, the current version aims at **proposing a new empirical approach for the real-world practice**, which is not sufficient. I would like to see a **strong practical result** either outperforming the recent baselines or applying in many real-world problems.\n\n[3] Technical details\n\n[a] I suggest not using the term \u201cdomain labels\u201d since it can be confusing to label $y$ information in the unsupervised domain adaptation. I think \u201cdomain index\u201d or \u201ctask index\u201d are better choices.\n\n[b] The visualization of $\\delta$ sounds interesting but I can not understand the meaning. A better explanation is expected. \n\n[c] Fig (1),(2) why PCA visualization? Why not Tsne?\n\n[d] The benefits of self-attention are unclear. More analysis (not numerical accuracy) is expected.\n\n--------------------------------------------\n\nSuggestions \n\nI suggest a major revision on the proposed approach, empirical results, and more analysis (not accuracy) on the benefits of the idea.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1671/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1671/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reducing Implicit Bias in Latent Domain Learning", "authorids": ["~Lucas_Deecke1", "~Timothy_Hospedales1", "~Hakan_Bilen1"], "authors": ["Lucas Deecke", "Timothy Hospedales", "Hakan Bilen"], "keywords": ["Latent Domain Learning", "CNN Architectures"], "abstract": "A fundamental shortcoming of deep neural networks is their specialization to a single task and domain. While recent techniques in multi-domain learning enable the learning of more domain-agnostic features, their success relies firmly on the presence of domain labels, typically requiring manual annotation and careful curation of datasets. Here we focus on latent domain learning, a highly realistic, yet less explored scenario: learning from data from different domains, without access to domain annotations. This is a particularly challenging problem, since standard models exhibit an implicit bias toward learning only the large domains in data, while disregarding smaller ones. To address this issue, we propose dynamic residual adapters that adaptively account for latent domains, and weighted domain transfer \u2013 a novel augmentation strategy designed specifically for this setting. Our techniques are evaluated on image classification tasks containing multiple unannotated domains, and we demonstrate they enhance performance, in particular, on the smallest of these.", "one-sentence_summary": "We demonstrate that standard models suppress underrepresented latent domains, and formulate novel strategies to limit this behavior.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deecke|reducing_implicit_bias_in_latent_domain_learning", "pdf": "/pdf/492a085a079e0510023ccd898f834aab15947d02.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iqOQ7bIUbc", "_bibtex": "@misc{\ndeecke2021reducing,\ntitle={Reducing Implicit Bias in Latent Domain Learning},\nauthor={Lucas Deecke and Timothy Hospedales and Hakan Bilen},\nyear={2021},\nurl={https://openreview.net/forum?id=McYsRk9-rso}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "McYsRk9-rso", "replyto": "McYsRk9-rso", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1671/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538113391, "tmdate": 1606915760746, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1671/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1671/-/Official_Review"}}}, {"id": "sjOvgkXDGI4", "original": null, "number": 11, "cdate": 1606302603167, "ddate": null, "tcdate": 1606302603167, "tmdate": 1606302603167, "tddate": null, "forum": "McYsRk9-rso", "replyto": "La7gHAJsd_1", "invitation": "ICLR.cc/2021/Conference/Paper1671/-/Official_Comment", "content": {"title": "Second response, PCA for ResNet26", "comment": "> I found the motivation around WDT still unclear. You mentioned that \"for latent domains, we do not want to exchange between samples that are visually too similar\". But then what will the interpolation be like if you are exchanging information from two very distinct images? Also in cases where you have distinct label set for different latent domains, learning concepts of objects (cats, dogs) that are invariant to the domain is not well-defined.\n\nOne of the challenges in latent domain learning is *domain leakage*: in multi-domain learning there are domain annotations, so the probability of classifying some f(x) to a wrong domain is *zero*. Remember, this is *not* the case here.\n\nThis is exactly where WDT helps: by interpolating between domains the model learns to look at more than just domain-specific (low-level) features, and becomes focused on the (high-level) semantic features relevant for object classification.\nWhat we mean with \"we do not want to exchange between samples that are visually too similar\": we simply point out that if you exchange too aggressively, you eventually end up removing features that *are* crucial to detecting semantic categories.\n\n> For the PCA part, what I was thinking is that in the case where the label set of different latent domains are distinct, their input distributions are usually very disjoint as well (for example, images from Omniglot should look very different from aircraft). In this situation, the original ResNet may already give you a separated embedding space.\n\nThanks for the clarification, we understand your request better now. We have added a PCA over ResNet26 for VD in the Appendix. As one can see there, indeed Omniglot is singled out (due to its low-level differences). Higher level concepts are however not picked up on by the ResNet, which *otherwise exhibits no latent domain structure*.\n\nAgain, note that our main goal is not to identify the original domains but to automatically find the optimal latent domains for our learning objective, i.e. classification. Think of a case where two domains are *annotated as different*, but are actually from the *same distributions* \u2013 traditional multi-domain models will not be efficient in this setting!\n\nFurthermore, we already show that using the ground-truth domain information (RA in Table 2) or using a clustering method (K-means in Table 2) obtains lower performance than our end-to-end methodology.\n\nFig. 2&3 show that the gating output encodes discriminative information about the original domains. In particular, they provide evidence that models *separate latent domains internally* when learning end-to-end \u2013 i.e. models \"agree\" that domain separation makes sense (they *could* simply ignore latent domains, after all), however we can also see that parameter sharing between *some* domains is crucial to maximizing model performance.\n\n> I still think DRA is an instantiation of MoE.\n\nWe never disagreed, as MoE is an *extremely general concept*. Note however that any self-conditional attention mechanism \u2013 from a formal viewpoint \u2013 can be viewed as a modular instantiation of MoE. We believe that research (where appropriate) should make connections between current ideas & past ones. Pointing out a *clear* connection between MoE and self-attention is doing just that.\n\nWe thank you for your second response and your suggestions, which have been very helpful in improving this manuscript.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1671/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1671/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reducing Implicit Bias in Latent Domain Learning", "authorids": ["~Lucas_Deecke1", "~Timothy_Hospedales1", "~Hakan_Bilen1"], "authors": ["Lucas Deecke", "Timothy Hospedales", "Hakan Bilen"], "keywords": ["Latent Domain Learning", "CNN Architectures"], "abstract": "A fundamental shortcoming of deep neural networks is their specialization to a single task and domain. While recent techniques in multi-domain learning enable the learning of more domain-agnostic features, their success relies firmly on the presence of domain labels, typically requiring manual annotation and careful curation of datasets. Here we focus on latent domain learning, a highly realistic, yet less explored scenario: learning from data from different domains, without access to domain annotations. This is a particularly challenging problem, since standard models exhibit an implicit bias toward learning only the large domains in data, while disregarding smaller ones. To address this issue, we propose dynamic residual adapters that adaptively account for latent domains, and weighted domain transfer \u2013 a novel augmentation strategy designed specifically for this setting. Our techniques are evaluated on image classification tasks containing multiple unannotated domains, and we demonstrate they enhance performance, in particular, on the smallest of these.", "one-sentence_summary": "We demonstrate that standard models suppress underrepresented latent domains, and formulate novel strategies to limit this behavior.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deecke|reducing_implicit_bias_in_latent_domain_learning", "pdf": "/pdf/492a085a079e0510023ccd898f834aab15947d02.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iqOQ7bIUbc", "_bibtex": "@misc{\ndeecke2021reducing,\ntitle={Reducing Implicit Bias in Latent Domain Learning},\nauthor={Lucas Deecke and Timothy Hospedales and Hakan Bilen},\nyear={2021},\nurl={https://openreview.net/forum?id=McYsRk9-rso}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "McYsRk9-rso", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1671/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1671/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1671/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1671/Authors|ICLR.cc/2021/Conference/Paper1671/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1671/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857068, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1671/-/Official_Comment"}}}, {"id": "La7gHAJsd_1", "original": null, "number": 10, "cdate": 1606279897889, "ddate": null, "tcdate": 1606279897889, "tmdate": 1606279897889, "tddate": null, "forum": "McYsRk9-rso", "replyto": "AXtR7i6mlV", "invitation": "ICLR.cc/2021/Conference/Paper1671/-/Official_Comment", "content": {"title": "reply to the rebuttal", "comment": "Thanks for your responses.  I do strongly agree that the setting is very interesting and practical. However, I still have several comments:\n\n1. I found the motivation around WDT still unclear. You mentioned that \"for latent domains, we do not want to exchange between samples that are visually too similar\". But then what will the interpolation be like if you are exchanging information from two very distinct images? Also in cases where you have distinct label set for different latent domains, learning concepts of objects (cats, dogs) that are invariant to the domain is not well-defined.\n\n2. For the PCA part, what I was thinking is that in the case where the label set of different latent domains are distinct, their input distributions are usually very disjoint as well (for example, images from Omniglot should look very different from aircraft). In this situation, the original ResNet may already give you a separated embedding space. Applying clustering over this embedding space and run distributionally robust optimization may give you a very strong baseline.\n\n3. I still think DRA is an instantiation of MoE.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1671/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1671/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reducing Implicit Bias in Latent Domain Learning", "authorids": ["~Lucas_Deecke1", "~Timothy_Hospedales1", "~Hakan_Bilen1"], "authors": ["Lucas Deecke", "Timothy Hospedales", "Hakan Bilen"], "keywords": ["Latent Domain Learning", "CNN Architectures"], "abstract": "A fundamental shortcoming of deep neural networks is their specialization to a single task and domain. While recent techniques in multi-domain learning enable the learning of more domain-agnostic features, their success relies firmly on the presence of domain labels, typically requiring manual annotation and careful curation of datasets. Here we focus on latent domain learning, a highly realistic, yet less explored scenario: learning from data from different domains, without access to domain annotations. This is a particularly challenging problem, since standard models exhibit an implicit bias toward learning only the large domains in data, while disregarding smaller ones. To address this issue, we propose dynamic residual adapters that adaptively account for latent domains, and weighted domain transfer \u2013 a novel augmentation strategy designed specifically for this setting. Our techniques are evaluated on image classification tasks containing multiple unannotated domains, and we demonstrate they enhance performance, in particular, on the smallest of these.", "one-sentence_summary": "We demonstrate that standard models suppress underrepresented latent domains, and formulate novel strategies to limit this behavior.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deecke|reducing_implicit_bias_in_latent_domain_learning", "pdf": "/pdf/492a085a079e0510023ccd898f834aab15947d02.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iqOQ7bIUbc", "_bibtex": "@misc{\ndeecke2021reducing,\ntitle={Reducing Implicit Bias in Latent Domain Learning},\nauthor={Lucas Deecke and Timothy Hospedales and Hakan Bilen},\nyear={2021},\nurl={https://openreview.net/forum?id=McYsRk9-rso}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "McYsRk9-rso", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1671/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1671/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1671/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1671/Authors|ICLR.cc/2021/Conference/Paper1671/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1671/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857068, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1671/-/Official_Comment"}}}, {"id": "mvfSOYshVdv", "original": null, "number": 9, "cdate": 1606248598327, "ddate": null, "tcdate": 1606248598327, "tmdate": 1606248598327, "tddate": null, "forum": "McYsRk9-rso", "replyto": "McYsRk9-rso", "invitation": "ICLR.cc/2021/Conference/Paper1671/-/Official_Comment", "content": {"title": "Overview of changes for rebuttal", "comment": "We have made use of the extended 9 page limit for the rebuttal and incorporated content that was previously discussed in the Appendix:\n* We highlight that latent domain suppression is a *core problem for ML generalization*. Our experiments with undersampled CIFAR-10 classes (Table 4) show that standard models (ResNet26, ResNet56) struggle to generalize in standard classification settings. We show DRA is a very effective counter against this.\n* Added results for single datasets. DRA improves performance, especially for highly varied (multi-modal) datasets such as CIFAR-100. This makes sense as a highly modal dataset is \u2013 under its surface \u2013 a latent domain problem.\n* Revised notation and motivation for WDT, which should now be much clearer.\n* Added results for PACS with ResNet56. It\u2019s poor performance shows: *deeper models are not a remedy for latent domain suppression*, which instead requires customized solutions like DRA and WDT.\n\nIn addition, we added new results for:\n* **NLP**- demonstrating that latent domain suppression is a *general problem for different types of data*. We couple DRA+VDCNN and show it boosts performance in topic classification.\n* **Medical images**- showing DRA boosts performance on multi-modal medical image benchmarks."}, "signatures": ["ICLR.cc/2021/Conference/Paper1671/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1671/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reducing Implicit Bias in Latent Domain Learning", "authorids": ["~Lucas_Deecke1", "~Timothy_Hospedales1", "~Hakan_Bilen1"], "authors": ["Lucas Deecke", "Timothy Hospedales", "Hakan Bilen"], "keywords": ["Latent Domain Learning", "CNN Architectures"], "abstract": "A fundamental shortcoming of deep neural networks is their specialization to a single task and domain. While recent techniques in multi-domain learning enable the learning of more domain-agnostic features, their success relies firmly on the presence of domain labels, typically requiring manual annotation and careful curation of datasets. Here we focus on latent domain learning, a highly realistic, yet less explored scenario: learning from data from different domains, without access to domain annotations. This is a particularly challenging problem, since standard models exhibit an implicit bias toward learning only the large domains in data, while disregarding smaller ones. To address this issue, we propose dynamic residual adapters that adaptively account for latent domains, and weighted domain transfer \u2013 a novel augmentation strategy designed specifically for this setting. Our techniques are evaluated on image classification tasks containing multiple unannotated domains, and we demonstrate they enhance performance, in particular, on the smallest of these.", "one-sentence_summary": "We demonstrate that standard models suppress underrepresented latent domains, and formulate novel strategies to limit this behavior.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deecke|reducing_implicit_bias_in_latent_domain_learning", "pdf": "/pdf/492a085a079e0510023ccd898f834aab15947d02.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iqOQ7bIUbc", "_bibtex": "@misc{\ndeecke2021reducing,\ntitle={Reducing Implicit Bias in Latent Domain Learning},\nauthor={Lucas Deecke and Timothy Hospedales and Hakan Bilen},\nyear={2021},\nurl={https://openreview.net/forum?id=McYsRk9-rso}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "McYsRk9-rso", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1671/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1671/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1671/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1671/Authors|ICLR.cc/2021/Conference/Paper1671/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1671/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857068, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1671/-/Official_Comment"}}}, {"id": "Ws7Md-BeYRw", "original": null, "number": 8, "cdate": 1606248341804, "ddate": null, "tcdate": 1606248341804, "tmdate": 1606248341804, "tddate": null, "forum": "McYsRk9-rso", "replyto": "WZFzaLot_ws", "invitation": "ICLR.cc/2021/Conference/Paper1671/-/Official_Comment", "content": {"title": "Second response, new NLP & medical imaging experiments", "comment": "Thank you for your quick response, which we much appreciated!\n\n> 1. What is the global training loss in the proposed approach\n> 2. a pseudocode/protocol for the whole algorithm or a clear network structure for illustrating the idea in Sec 3.3-3.4.\n\nBecause there are no domain labels, global training occurs via standard cross-entropy. This is the very point of designing an end-to-end method, as we can therefore optimize our methods using standard minimization of $\\nabla\\big(\\sum_i L(y_i,f(x_i))\\big)$.\n\nThe only important bit to watch out for is that (I) images are modified via WDT (because we are dealing with latent domain problems here) and (II) because we have DRA, we only optimize the model weights $\\alpha$.\n\n> a pseudocode/protocol for the whole algorithm or a clear network structure for illustrating the idea in Sec 3.3-3.4.\n\nWe have added pseudo-code at your request (see Algorithm 1, page 14) and will release source code and models alongside a final version. As this makes clear, our methods are of general purpose and **can be combined easily with existing methods** \u2013 this aspect was a central motivation to our work.\n\n> Current results mainly focus on image problems on the benchmark.\n\nThe use of these benchmarks is really complete standard protocol. Exactly the same benchmarks are used in recent works in the multi-domain literature:\n* Rebuffi et al. (2017, 2018) NeurIPS & CVPR.\n* Guo et al. (2019) AAAI.\n* Liu et al. (2019) CVPR.\n\n> For example, is your approach still valid in NLP dataset?\n\nWe have added two new results to address this question:\n* Sentiment analysis on airlines Tweets (Table 8) under shrinking domain sizes. Main result: we can again correlate smaller domains directly with a drop in performance, just as for images.\n* Topic classification with VDCNN (baseline) and DRA inserted into the network (Table 9). We follow the experimental setup in Section 4.4 and subsample 2 out of 4 topics to create an unbalanced version. **DRA improves over VDCNN**\n\nThe above results confirm that there *exists no reason to believe that latent domain suppression only occurs in images*. That being said, as the absence of any modular/architectural solutions for latent domains in the NLP literature indicates, adding small domain robustness to NLP models is a research question in it\u2019s own right that requires significant effort (for an indication of this see e.g. the NLP multi-task work of Stickland & Murray (2019)), but this is something we hope to address adequately in future work!\n\n> I would like to see a real-world application such as medical image\n\nWe followed your suggestion and have added results for coupling our approach with a ResNet18 (Table 10) on medical images, where we show that DRA yields a *robust increase in performance on multi-modal medical image data*."}, "signatures": ["ICLR.cc/2021/Conference/Paper1671/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1671/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reducing Implicit Bias in Latent Domain Learning", "authorids": ["~Lucas_Deecke1", "~Timothy_Hospedales1", "~Hakan_Bilen1"], "authors": ["Lucas Deecke", "Timothy Hospedales", "Hakan Bilen"], "keywords": ["Latent Domain Learning", "CNN Architectures"], "abstract": "A fundamental shortcoming of deep neural networks is their specialization to a single task and domain. While recent techniques in multi-domain learning enable the learning of more domain-agnostic features, their success relies firmly on the presence of domain labels, typically requiring manual annotation and careful curation of datasets. Here we focus on latent domain learning, a highly realistic, yet less explored scenario: learning from data from different domains, without access to domain annotations. This is a particularly challenging problem, since standard models exhibit an implicit bias toward learning only the large domains in data, while disregarding smaller ones. To address this issue, we propose dynamic residual adapters that adaptively account for latent domains, and weighted domain transfer \u2013 a novel augmentation strategy designed specifically for this setting. Our techniques are evaluated on image classification tasks containing multiple unannotated domains, and we demonstrate they enhance performance, in particular, on the smallest of these.", "one-sentence_summary": "We demonstrate that standard models suppress underrepresented latent domains, and formulate novel strategies to limit this behavior.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deecke|reducing_implicit_bias_in_latent_domain_learning", "pdf": "/pdf/492a085a079e0510023ccd898f834aab15947d02.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iqOQ7bIUbc", "_bibtex": "@misc{\ndeecke2021reducing,\ntitle={Reducing Implicit Bias in Latent Domain Learning},\nauthor={Lucas Deecke and Timothy Hospedales and Hakan Bilen},\nyear={2021},\nurl={https://openreview.net/forum?id=McYsRk9-rso}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "McYsRk9-rso", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1671/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1671/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1671/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1671/Authors|ICLR.cc/2021/Conference/Paper1671/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1671/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857068, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1671/-/Official_Comment"}}}, {"id": "WZFzaLot_ws", "original": null, "number": 7, "cdate": 1605933307649, "ddate": null, "tcdate": 1605933307649, "tmdate": 1605933626020, "tddate": null, "forum": "McYsRk9-rso", "replyto": "3KM9LNsF5T2", "invitation": "ICLR.cc/2021/Conference/Paper1671/-/Official_Comment", "content": {"title": "Response for the rebuttal", "comment": "Thanks for your responses. It surely clarifies my several confusions!  However, I still have several questions that require properly addressed in the paper. The following are my comments:\n> WDT is an augmentation strategy\n\nThanks for your clarification! The following questions still exist\n\n1. *What is the global training loss in the proposed approach*\n\n2. *a pseudocode/protocol for the whole algorithm or a clear network structure for illustrating the idea in Sec 3.3-3.4.* \n\nI think this is quite important for understanding your approach. e.g. the step-by-step description of the proposed approach. Since this paper currently does not provide source code, this is particularly important.\n\n> clarified notation further in our revision\n\nThanks, I have checked the revised version. It is much better.\n\n>  to make a first step toward models that learn reliably in the presence of latent domains\n\nThanks for your remark. But as an *empirical paper*, the current empirical studies are indeed not sufficient for ICLR. For example, is your approach still valid in NLP dataset? Current results mainly focus on image problems *on the benchmark*.  I would like to see a real-world application such as medical image.\n\nAlternatively, the authors can provide a theoretical analysis of the proposed approach, to enhance the contribution of the paper.\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1671/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1671/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reducing Implicit Bias in Latent Domain Learning", "authorids": ["~Lucas_Deecke1", "~Timothy_Hospedales1", "~Hakan_Bilen1"], "authors": ["Lucas Deecke", "Timothy Hospedales", "Hakan Bilen"], "keywords": ["Latent Domain Learning", "CNN Architectures"], "abstract": "A fundamental shortcoming of deep neural networks is their specialization to a single task and domain. While recent techniques in multi-domain learning enable the learning of more domain-agnostic features, their success relies firmly on the presence of domain labels, typically requiring manual annotation and careful curation of datasets. Here we focus on latent domain learning, a highly realistic, yet less explored scenario: learning from data from different domains, without access to domain annotations. This is a particularly challenging problem, since standard models exhibit an implicit bias toward learning only the large domains in data, while disregarding smaller ones. To address this issue, we propose dynamic residual adapters that adaptively account for latent domains, and weighted domain transfer \u2013 a novel augmentation strategy designed specifically for this setting. Our techniques are evaluated on image classification tasks containing multiple unannotated domains, and we demonstrate they enhance performance, in particular, on the smallest of these.", "one-sentence_summary": "We demonstrate that standard models suppress underrepresented latent domains, and formulate novel strategies to limit this behavior.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deecke|reducing_implicit_bias_in_latent_domain_learning", "pdf": "/pdf/492a085a079e0510023ccd898f834aab15947d02.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iqOQ7bIUbc", "_bibtex": "@misc{\ndeecke2021reducing,\ntitle={Reducing Implicit Bias in Latent Domain Learning},\nauthor={Lucas Deecke and Timothy Hospedales and Hakan Bilen},\nyear={2021},\nurl={https://openreview.net/forum?id=McYsRk9-rso}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "McYsRk9-rso", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1671/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1671/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1671/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1671/Authors|ICLR.cc/2021/Conference/Paper1671/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1671/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857068, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1671/-/Official_Comment"}}}, {"id": "olSXfr8QRq", "original": null, "number": 6, "cdate": 1605899001696, "ddate": null, "tcdate": 1605899001696, "tmdate": 1605899001696, "tddate": null, "forum": "McYsRk9-rso", "replyto": "5j3JXy0LiYh", "invitation": "ICLR.cc/2021/Conference/Paper1671/-/Official_Comment", "content": {"title": "Clarifications around WDT & ResNet56 experiments", "comment": "Dear Reviewer, many thanks for your suggestions. We address them point-by-point below.\n\n*\"Skipped the math that yields the equation (3) in the section 3.\"*\n\nAs requested, we have reorganized this section to clarify those points. Please let us know in case any concerns remain!\n\n\n*\"In Table 1, there is a data domain size metric: $\\pi_d$. Can you add how this is computed? Also, how small is the smallest data in sheer number of examples?\"*\n\n$\\pi_d$ indicates the relative share of each (hidden) domain, i.e. we compute it as $N_d/N$, where $N_d$ denotes the number of examples belonging to latent domain $d$, and $N$ is the sum over all domains (we also mention this in the first paragraph of Section 3.1). Note that we only compute $\\pi_d$ for the analysis \u2013 the model has absolutely no knowledge of which domains are large/small, nor how many there are.\n\nThe smallest domains are: 1.6k (PACS-photo) and 2k (Vgg-Flowers).\n\n\n*\"Can you explain what \"inactive\" here means?\"*\n\nWDT is a pairwise augmentation, and the exchange strength is a measure of how similar each latent domain is. In other words: how much parameter sharing occurs between some latent domain (e.g. VD-Omniglot) and the other ones.\n\nFor example: PACS-sketch is relatively isolated in feature space (see Fig. 3 left), and \"inactive\" here means the average exchange strength is relatively low, i.e. **a lower average \u03b4 in WDT = fewer parameters shared with other PACS domains = an inactive domain.**\n\n*\"Table 1 has RestNet56, but Table 2 doesn't. Why did you make this choice of experiment design?\"*\n\nThat\u2019s a great suggestion, we have added results for ResNet56 to Table 2. However, the improvement over ResNet26 is only marginal (92.70 -> 93.35) and significantly smaller than the improvement via DRA (92.70 -> 94.76). Note DRA also uses around 40% less parameters than ResNet56.\n\nThis result once again confirms that **performance on small latent domains is suppressed**, and **adding more layers** \u2013 the preferred option for standard classification \u2013 **doesn\u2019t solve this problem**.\n\nPlease let us know of any concerns that remain, so that we may address them."}, "signatures": ["ICLR.cc/2021/Conference/Paper1671/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1671/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reducing Implicit Bias in Latent Domain Learning", "authorids": ["~Lucas_Deecke1", "~Timothy_Hospedales1", "~Hakan_Bilen1"], "authors": ["Lucas Deecke", "Timothy Hospedales", "Hakan Bilen"], "keywords": ["Latent Domain Learning", "CNN Architectures"], "abstract": "A fundamental shortcoming of deep neural networks is their specialization to a single task and domain. While recent techniques in multi-domain learning enable the learning of more domain-agnostic features, their success relies firmly on the presence of domain labels, typically requiring manual annotation and careful curation of datasets. Here we focus on latent domain learning, a highly realistic, yet less explored scenario: learning from data from different domains, without access to domain annotations. This is a particularly challenging problem, since standard models exhibit an implicit bias toward learning only the large domains in data, while disregarding smaller ones. To address this issue, we propose dynamic residual adapters that adaptively account for latent domains, and weighted domain transfer \u2013 a novel augmentation strategy designed specifically for this setting. Our techniques are evaluated on image classification tasks containing multiple unannotated domains, and we demonstrate they enhance performance, in particular, on the smallest of these.", "one-sentence_summary": "We demonstrate that standard models suppress underrepresented latent domains, and formulate novel strategies to limit this behavior.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deecke|reducing_implicit_bias_in_latent_domain_learning", "pdf": "/pdf/492a085a079e0510023ccd898f834aab15947d02.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iqOQ7bIUbc", "_bibtex": "@misc{\ndeecke2021reducing,\ntitle={Reducing Implicit Bias in Latent Domain Learning},\nauthor={Lucas Deecke and Timothy Hospedales and Hakan Bilen},\nyear={2021},\nurl={https://openreview.net/forum?id=McYsRk9-rso}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "McYsRk9-rso", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1671/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1671/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1671/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1671/Authors|ICLR.cc/2021/Conference/Paper1671/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1671/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857068, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1671/-/Official_Comment"}}}, {"id": "AXtR7i6mlV", "original": null, "number": 5, "cdate": 1605898398822, "ddate": null, "tcdate": 1605898398822, "tmdate": 1605898424137, "tddate": null, "forum": "McYsRk9-rso", "replyto": "Kl_QO5GxJqm", "invitation": "ICLR.cc/2021/Conference/Paper1671/-/Official_Comment", "content": {"title": "Clarification of novel contributions made & qualitative analysis", "comment": "Dear Reviewer, thank you for your comments, which we address point by point below.\n\n*\u201cMaybe I missed something, but I don't there are new insights in the paper. The proposed dynamic residual adapter is just an instance of MoE [1] with adapters, which I think should be a baseline in the experiments.\u201d*\n\n\nWe make several novel contributions in this paper.\n\n1. The most important insight is the **identification of a new problem** that is pervasive yet not generally known by the community. Specifically, we present clear empirical evidence that **standard deep models underperform when latent domains are present due to suppressing small domains**. This is a *fundamental problem with the generalization of deep architectures*, and we have included results (page 9 of our revision) that demonstrate it even extends to standard classification problems, e.g. on CIFAR-10 (see Table 4).\n\n2. We find that standard augmentation strategies do not remedy this problem. Because of this, **we propose WDT, a new augmentation strategy that reduces bias against small latent domains**.\n\n3. As per MoE: this is simply an elegant way of extending multi-domain approaches to latent domains, where we cannot use off-the-shelf multi-domain methods (since there are no domain labels to speak of, see page 4 paragraph 3). Because there is no domain information, we **propose DRA, an alternative way to target corrections purpose-built for latent domain learning**. Please note DRA is not necessarily an instance of (I) MoE \u2013 we explored several alternatives, such as (II) Gumbel-softmax mechanisms that we compare to in Table 5, or (III) fixed assignments (e.g. after applying latent domain discovery e.g. through k-means, see Table 2).\n\n\n*\"Here you want to interpolate between $x_i$ and $x_j$. But why do you compute the difference between the input $x_i$ and the feature $\\mu_i$ in equation 3?\"*\n\nWe discuss the motivation around WDT on page 5 paragraph 2: for latent domains, we do not want to exchange between samples that are visually too similar. This has a straightforward motivation: exchanging too much information between samples *can* reduce the number of discriminative features for samples from the same class. What we want instead is to encourage information exchange between different (hidden) domains, so as to end up with a model that learns concepts of objects (cats, dogs, etc.) that are invariant to the (latent) domain.\n\nPlease note the difference $x_i-\\mu_i$ is further scaled by the standard deviation after which $\\mu_j$ is added. This process is crucial and exactly responsible for the *transfer of statistical information from $x_j\\to x_i$*.\n\n\n*\"I think the introduction describes the problem too much, leaving it no space to expand your idea and intuition. For example, you start describing your idea at the very last paragraph.\"*\n\nWe maintain a longer than usual introduction is needed, because an important aspect in this paper is to **establish latent domain learning as a novel learning setting**. This requires we explain the latent domain scenario in sufficient detail, in large part so that future work won\u2019t have to. Feel free to check out the long introduction in the initial work on multi-task learning by e.g. Caruana (1997). While it might be a bit obvious nowadays, it is still recommended literature for anyone that is starting out in the field.\n\n\n*\"When creating the augmented examples, can you leverage the gate information that you produced from the MoE?\"*\n\nWe agree this is an interesting idea, as having access to richer conditional information within DRA could potentially enhance the internal clustering of latent domains. However, in doing so the method would no longer be end-to-end, as we would require a full pass through the network to collect gate activations. The goal of our manuscript \u2013 as stated on page 2 paragraph 1 \u2013 is to contribute a principled end-to-end mechanism that doesn\u2019t have a large negative impact on runtimes (e.g. we want to avoid 9x models). Requiring two passes would have moved the paper in a very different direction.\n\n\n*\"It will be more helpful to understand the DRA component if you can provide PCA over the original activations.\"*\n\nPlease note in Figure 2 we do not display PCA over activations. We collect *only the gate activations* across the ResNet26, and then reduce this to two dimensions with a linear mechanism (i.e. PCA). This shows:\n* individual residual adaptations *can* be attributed to latent domains.\n* **DRA shares between similar domains**, which provides an **explanation** of its much better performance than RA etc.\n\nWe went ahead and added some clarification around this in the final paragraphs of page 6.\n\nPlease let us know in case there\u2019s anything else you would like us to revisit."}, "signatures": ["ICLR.cc/2021/Conference/Paper1671/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1671/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reducing Implicit Bias in Latent Domain Learning", "authorids": ["~Lucas_Deecke1", "~Timothy_Hospedales1", "~Hakan_Bilen1"], "authors": ["Lucas Deecke", "Timothy Hospedales", "Hakan Bilen"], "keywords": ["Latent Domain Learning", "CNN Architectures"], "abstract": "A fundamental shortcoming of deep neural networks is their specialization to a single task and domain. While recent techniques in multi-domain learning enable the learning of more domain-agnostic features, their success relies firmly on the presence of domain labels, typically requiring manual annotation and careful curation of datasets. Here we focus on latent domain learning, a highly realistic, yet less explored scenario: learning from data from different domains, without access to domain annotations. This is a particularly challenging problem, since standard models exhibit an implicit bias toward learning only the large domains in data, while disregarding smaller ones. To address this issue, we propose dynamic residual adapters that adaptively account for latent domains, and weighted domain transfer \u2013 a novel augmentation strategy designed specifically for this setting. Our techniques are evaluated on image classification tasks containing multiple unannotated domains, and we demonstrate they enhance performance, in particular, on the smallest of these.", "one-sentence_summary": "We demonstrate that standard models suppress underrepresented latent domains, and formulate novel strategies to limit this behavior.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deecke|reducing_implicit_bias_in_latent_domain_learning", "pdf": "/pdf/492a085a079e0510023ccd898f834aab15947d02.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iqOQ7bIUbc", "_bibtex": "@misc{\ndeecke2021reducing,\ntitle={Reducing Implicit Bias in Latent Domain Learning},\nauthor={Lucas Deecke and Timothy Hospedales and Hakan Bilen},\nyear={2021},\nurl={https://openreview.net/forum?id=McYsRk9-rso}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "McYsRk9-rso", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1671/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1671/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1671/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1671/Authors|ICLR.cc/2021/Conference/Paper1671/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1671/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857068, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1671/-/Official_Comment"}}}, {"id": "4oRzT6tXXzv", "original": null, "number": 3, "cdate": 1605895603868, "ddate": null, "tcdate": 1605895603868, "tmdate": 1605896454106, "tddate": null, "forum": "McYsRk9-rso", "replyto": "LN5mj9TH940", "invitation": "ICLR.cc/2021/Conference/Paper1671/-/Official_Comment", "content": {"title": "Included additional experiments & clarifications", "comment": "Thank you for your constructive feedback. We address each point below.\n\n*\u201cThe authors do not describe any processing they have done of the data. This should be clearly included in the methods section.\u201d*\n\nWe do discuss this on page 6 paragraph 3. As we mention there, we only use standard augmentations (flipping, normalization) in *all* of our experiments, and otherwise make no changes to the benchmark data.\n\n\n*\"The authors should explain the results in more detail in the text and include their interpretation of the results.\"*\n\nWe extensively discuss our results, and provide a large set of experiments for two latent domain settings, with (I) joint and (II) disjoint label spaces. Moreover, for both settings, we provide (III) qualitative insights, as well as (IV) thorough discussions of the computational requirements. Please note the 8 page limit prevented us from including additional analysis in the main paper, but we included an extensive study of DRA around (V) single dataset performance on CIFAR-10 and CIFAR-100 (Table 3), (VI) robustness to class imbalance (Table 4), as well as (VII) ablations (Tables 5-8) in the appended pages.\n\nWe have taken advantage of the 9 page limit for the revision, and following your suggestion included additional analysis (V)+(VI) (which were previously in the appendix) in the main parts of the paper. On page 9 we demonstrate that the benefits of DRA even extend to standard classification tasks, and DRA brings *significant* performance advantages when some classes are underrepresented in the data. The last case addresses a particularly severe issue for standard deep learning models, as we display in Table 4 for ResNet26+56.\n\n*\"In this part, the authors introduce the algorithm. Since there are many formulas in this section, additional explanations on the learning procedure would help understand the proposed method.\"*\n\nWe have reorganized the section, and added additional discussions around WDT in our revision.\n\nThank you again for your constructive feedback. Please point out any remaining concerns so that we may address them.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1671/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1671/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reducing Implicit Bias in Latent Domain Learning", "authorids": ["~Lucas_Deecke1", "~Timothy_Hospedales1", "~Hakan_Bilen1"], "authors": ["Lucas Deecke", "Timothy Hospedales", "Hakan Bilen"], "keywords": ["Latent Domain Learning", "CNN Architectures"], "abstract": "A fundamental shortcoming of deep neural networks is their specialization to a single task and domain. While recent techniques in multi-domain learning enable the learning of more domain-agnostic features, their success relies firmly on the presence of domain labels, typically requiring manual annotation and careful curation of datasets. Here we focus on latent domain learning, a highly realistic, yet less explored scenario: learning from data from different domains, without access to domain annotations. This is a particularly challenging problem, since standard models exhibit an implicit bias toward learning only the large domains in data, while disregarding smaller ones. To address this issue, we propose dynamic residual adapters that adaptively account for latent domains, and weighted domain transfer \u2013 a novel augmentation strategy designed specifically for this setting. Our techniques are evaluated on image classification tasks containing multiple unannotated domains, and we demonstrate they enhance performance, in particular, on the smallest of these.", "one-sentence_summary": "We demonstrate that standard models suppress underrepresented latent domains, and formulate novel strategies to limit this behavior.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deecke|reducing_implicit_bias_in_latent_domain_learning", "pdf": "/pdf/492a085a079e0510023ccd898f834aab15947d02.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iqOQ7bIUbc", "_bibtex": "@misc{\ndeecke2021reducing,\ntitle={Reducing Implicit Bias in Latent Domain Learning},\nauthor={Lucas Deecke and Timothy Hospedales and Hakan Bilen},\nyear={2021},\nurl={https://openreview.net/forum?id=McYsRk9-rso}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "McYsRk9-rso", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1671/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1671/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1671/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1671/Authors|ICLR.cc/2021/Conference/Paper1671/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1671/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857068, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1671/-/Official_Comment"}}}, {"id": "3KM9LNsF5T2", "original": null, "number": 4, "cdate": 1605896378603, "ddate": null, "tcdate": 1605896378603, "tmdate": 1605896378603, "tddate": null, "forum": "McYsRk9-rso", "replyto": "qJ1d_DXvR5A", "invitation": "ICLR.cc/2021/Conference/Paper1671/-/Official_Comment", "content": {"title": "Clarifications around WDT, latent domain learning & revision of notation", "comment": "Dear reviewer, many thanks for your review. We go through your comments point by point.\n\n*\u201cI am rather confused and unclear about the whole learning procedure. [.. ] WDT is a part of the loss or used for analyzing the problem?\u201d*\n\nWe have pointed this out at various points throughout the paper \u2013 *including* the abstract: WDT is an augmentation strategy. Just like other strategies (grayscaling, flipping, or MixUp) this does not explicitly appear in the loss function. WDT is needed in latent domain learning because strategies that work well for standard classification problems (such as MixUp) do not work here (see Table 5).\n\n\n*\u201cThe mathematical notations defined in this paper are presented oddly [..]\u201d*\n\nWe follow standard ML notation for all parts of our paper, distances \u03b4 for example are always scalar by their definition; \u03f5 is a channel-sized vector (as we state on page 4). At your request, we did go ahead and clarified notation further in our revision, e.g. replaced |**x**| with dim(**x**), and have introduced bold symbols for vector-valued objects such as **x**.\n\n\n*\u201cThe current empirical results only compare MLFN, which is not sufficient [..] I would like to see a strong practical result [..] outperforming recent baselines or applying in many real-world problems.\u201d*\n\nAs we state on page 7 paragraph 4, there currently exist no customized end-to-end solutions for latent domain learning in the literature. If there is any approach you feel we\u2019ve overlooked here, please point them out.\n\nMoreover, the goal of this paper is very clear: to **make a *first* step toward models that learn reliably in the presence of latent domains**, by preserving small latent domains in the dataset. Contrary to other work that expands on previous learning settings (multi-domain, domain adaptation, etc.), this requires a significant extra effort in terms of motivating the problem. We do this here through an extended introduction, and furthermore present multiple qualitative insights. It is **standard protocol** in the multi-domain literature (compare Rebuffi et al. (2017, 2018), Guo et al. (2019), Liu et al. (2019), etc.) to use datasets like PACS or Visual Decathlon as a proxy for the real-world. As such, the criticism that there is no \u201creal world experiments\u201d has to be put into perspective \u2013 we are not using toy data after all, and e.g. Visual Decathlon is a **very** complex dataset.\n\n\n*\u201cThe visualization of \u03b4 sounds interesting but I can not understand the meaning. A better explanation is expected.\u201d*\n\nWDT is a pairwise augmentation. This figure visualizes how much exchange occurs on average by each latent domain. In other words: how much parameter sharing happens between some latent domain (e.g. VD-Omniglot) and the other ones.\n\nFor example: some domains (e.g. PACS-sketch) are quite isolated in feature space (see Fig. 3 left), and we show that this directly translates to a **lower average \u03b4 in WDT = less parameter sharing with other PACS domains**. These visualizations confirm that WDT does what it was designed for.\n\n\n*\u201cFig (1),(2) why PCA visualization? Why not Tsne?\u201d*\n\nBecause when features are separable by the most simple, *linear* unsupervised mechanism (which is PCA, not the *nonlinear* t-SNE), then this is a much stronger statement that there is *meaningful separation* within DRA.\n\n\n*\u201cThe benefits of self-attention are unclear. More analysis (not numerical accuracy) is expected.\u201d*\n\nWe firmly establish the benefits of self-attention. We analyze alternatives to self-attention in Table 5, and in particular find that Gumbel-softmax negatively impacts performance. We also compare to K=1, i.e. having no self-attention.\n\nWe hope we were able to remedy your concerns in our updated revision. If there\u2019s anything else you would like us to revisit, please let us know.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1671/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1671/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reducing Implicit Bias in Latent Domain Learning", "authorids": ["~Lucas_Deecke1", "~Timothy_Hospedales1", "~Hakan_Bilen1"], "authors": ["Lucas Deecke", "Timothy Hospedales", "Hakan Bilen"], "keywords": ["Latent Domain Learning", "CNN Architectures"], "abstract": "A fundamental shortcoming of deep neural networks is their specialization to a single task and domain. While recent techniques in multi-domain learning enable the learning of more domain-agnostic features, their success relies firmly on the presence of domain labels, typically requiring manual annotation and careful curation of datasets. Here we focus on latent domain learning, a highly realistic, yet less explored scenario: learning from data from different domains, without access to domain annotations. This is a particularly challenging problem, since standard models exhibit an implicit bias toward learning only the large domains in data, while disregarding smaller ones. To address this issue, we propose dynamic residual adapters that adaptively account for latent domains, and weighted domain transfer \u2013 a novel augmentation strategy designed specifically for this setting. Our techniques are evaluated on image classification tasks containing multiple unannotated domains, and we demonstrate they enhance performance, in particular, on the smallest of these.", "one-sentence_summary": "We demonstrate that standard models suppress underrepresented latent domains, and formulate novel strategies to limit this behavior.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deecke|reducing_implicit_bias_in_latent_domain_learning", "pdf": "/pdf/492a085a079e0510023ccd898f834aab15947d02.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iqOQ7bIUbc", "_bibtex": "@misc{\ndeecke2021reducing,\ntitle={Reducing Implicit Bias in Latent Domain Learning},\nauthor={Lucas Deecke and Timothy Hospedales and Hakan Bilen},\nyear={2021},\nurl={https://openreview.net/forum?id=McYsRk9-rso}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "McYsRk9-rso", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1671/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1671/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1671/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1671/Authors|ICLR.cc/2021/Conference/Paper1671/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1671/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857068, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1671/-/Official_Comment"}}}, {"id": "5j3JXy0LiYh", "original": null, "number": 1, "cdate": 1603611630409, "ddate": null, "tcdate": 1603611630409, "tmdate": 1605024386751, "tddate": null, "forum": "McYsRk9-rso", "replyto": "McYsRk9-rso", "invitation": "ICLR.cc/2021/Conference/Paper1671/-/Official_Review", "content": {"title": "Impressive results on PACS (single task, multi-domain) for latent domain learning with a gate", "review": "1. Summarize what the paper claims to contribute. Be positive and generous.\nThe paper claims to contribute a new method *dynamic residual adapters (DRA)* coupled with *weighted domain transfer (WDT)* to tackle *latent domain learning.* The proposed method improves the model performance against small domain datasets without hurting the model performance against large domain datasets in two different settings:\n  (1) multi-domain setting (10 different tasks with 10 different domain per task)\n  (2) multi-style setting (1 task with 4 different styles)\nImpressive empirical results! I especially enjoyed reading PCA graphs and its qualitative analyses.\n\n2. List strong and weak points of the paper. Be as comprehensive as possible.\n  (1) Strengths\n    a. Exhaustive empirical analyses. (ablation tests and tests with varying K value)\n    b. Qualitative analyses backed by graphs like PCA and example images. \n    c. Impressive accuracy improvements.\n    d. Intuitive theoretical explanation. Cool idea to use a gate to make RA dynamic.\n  (2) Weaknesses\n    a. Skipped the math that yields the equation (3) in the section 3. Would be nice if the steps are attached as an Appendix.\n    b. In Table 1, there is a data domain size metric: $\\pi_{d}$ . Can you add how this is computed? Also, how small is the smallest data in sheer number of examples?\n    c. Table 1 has RestNet56, but Table 2 doesn't. Why did you make this choice of experiment design?\n    d. With PACS dataset, you have experimented with the model (k-means+RA). I didn't quite understand the model setup and the motivation. In my understanding, the model learned the latent domain labels via k-means. And, then, based on this pseudo domain labels, the model is fine-tuned with Residual Adapter applied. Did I understand the model correctly? What is the motivation of doing this? I am not sure if this is a fair comparison between the RA and the proposed DRA+WDT methods. It seems rather a comparison between DRA+WDT and K-Means.\n    e. In Figure 3, the paper says \"Middle: WDT exchange or different domains, sketch is particularly inactive.\" I had a bit of difficulty parsing what \"inactive\" means here because the \"Middle\" figure is about \"WDT Exchange Strength\" and because \"sketch\" has the largest strength. Can you explain what \"inactive\" here means?\n    f. Based on Table 3, the positive effect of WDT is not strong. I wonder if WDT is necessary.\n    g. It seems to me that the strength of the proposed method is much more evident in the second problem type (PACS) where the task is the same across different domains. In the first problem type (Visual Decathlon), the DRA+WDT's performance boost is not consistent across different domains. I see that DRA+WDT hurts the performance compared to the baseline ResNet26/56 on a few different domains, such as Daim., Gtsrb, Omn., and Svhn. $\\pi_{d}$ values of the domains of PACS are greater than those of Visual Decathlon, excluding svhn. Why do you think that is? When should one use or not use DRA+WDT in order to avoid hurting the model performance?\n    h. Minor formatting issues. See 6 below.\n\n3. Clearly state your recommendation (accept or reject) with one or two key reasons for this choice.\nAccept because of the impressive performance of the proposed method against PACS (single task, multi domain) with clear visual analyses. Meantime, 2.(2).g requires more explanation to make the paper's claim stronger.\n\n4. Provide supporting arguments for your recommendation.\nSee 2.(1) Strengths, 3, and 2.(2).g above.\n\n5. Ask questions you would like answered by the authors to help you clarify your understanding of the paper and provide the additional evidence you need to be confident in your assessment. \nSee 2.(2) Weaknesses above.\n\n6. Provide additional feedback with the aim to improve the paper. Make it clear that these points are here to help, and not necessarily part of your decision assessment.\n  (1) The indentation of Table 5 seems to be inconsistent with the rest of the tables in the paper.\n  (2) In Figure 3, the paper mentions $P_{k}$ without the denotation explained explicitly in any of the main body of the text. I had to re-read the paper to find a footnote 5 to finally understand what this denotation meant. It would be good to briefly explain this denotation in the same description of Figure 3.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper1671/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1671/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reducing Implicit Bias in Latent Domain Learning", "authorids": ["~Lucas_Deecke1", "~Timothy_Hospedales1", "~Hakan_Bilen1"], "authors": ["Lucas Deecke", "Timothy Hospedales", "Hakan Bilen"], "keywords": ["Latent Domain Learning", "CNN Architectures"], "abstract": "A fundamental shortcoming of deep neural networks is their specialization to a single task and domain. While recent techniques in multi-domain learning enable the learning of more domain-agnostic features, their success relies firmly on the presence of domain labels, typically requiring manual annotation and careful curation of datasets. Here we focus on latent domain learning, a highly realistic, yet less explored scenario: learning from data from different domains, without access to domain annotations. This is a particularly challenging problem, since standard models exhibit an implicit bias toward learning only the large domains in data, while disregarding smaller ones. To address this issue, we propose dynamic residual adapters that adaptively account for latent domains, and weighted domain transfer \u2013 a novel augmentation strategy designed specifically for this setting. Our techniques are evaluated on image classification tasks containing multiple unannotated domains, and we demonstrate they enhance performance, in particular, on the smallest of these.", "one-sentence_summary": "We demonstrate that standard models suppress underrepresented latent domains, and formulate novel strategies to limit this behavior.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deecke|reducing_implicit_bias_in_latent_domain_learning", "pdf": "/pdf/492a085a079e0510023ccd898f834aab15947d02.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iqOQ7bIUbc", "_bibtex": "@misc{\ndeecke2021reducing,\ntitle={Reducing Implicit Bias in Latent Domain Learning},\nauthor={Lucas Deecke and Timothy Hospedales and Hakan Bilen},\nyear={2021},\nurl={https://openreview.net/forum?id=McYsRk9-rso}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "McYsRk9-rso", "replyto": "McYsRk9-rso", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1671/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538113391, "tmdate": 1606915760746, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1671/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1671/-/Official_Review"}}}, {"id": "Kl_QO5GxJqm", "original": null, "number": 2, "cdate": 1603765879865, "ddate": null, "tcdate": 1603765879865, "tmdate": 1605024386682, "tddate": null, "forum": "McYsRk9-rso", "replyto": "McYsRk9-rso", "invitation": "ICLR.cc/2021/Conference/Paper1671/-/Official_Review", "content": {"title": "interesting problem, but lack of new insights", "review": "Summary:\nThe authors propose a method for latent domain learning, where input data come from different domains and the domain labels are unknown. The proposed method consists of two parts: dynamic residual adapter and weighted domain transfer. The dynamic residual adapter acts as a mixture of expert layer. And the weighted domain transfer which augments the dataset by interpolating between different input pairs. Empirical results show that when combined together, the proposed method perform better than training a regular model.\n\nPros:\n1. Latent domain discovery is a very interesting topic. \n2. Empirical results show that the method brings improvement to minority domains.\n\nCons:\n1. Maybe I missed something, but I don't there are new insights in the paper. The proposed dynamic residual adapter is just an instance of MoE [1] with adapters, which I think should be a baseline in the experiments.\n2. \"Section 3.4 Weighted Domain Transfer\" is not well-motivated and very confusing. Here you want to interpolate between x_i and x_j. But why do you compute the difference between the input x_i and the feature \\mu_i in equation 3? Are they comparable with each other? What is the goal that you want to achieve here?\n\nOther comments:\n1. I think the introduction describes the problem too much, leaving it no space to expand your idea and intuition. For example, you start describing your idea at the very last paragraph.\n2. When creating the augmented examples, can you leverage the gate information that you produced from the MoE?\n3. It will be more helpful to understand the DRA component if you can provide PCA over the original activations.\n\n\n[1] OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1671/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1671/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reducing Implicit Bias in Latent Domain Learning", "authorids": ["~Lucas_Deecke1", "~Timothy_Hospedales1", "~Hakan_Bilen1"], "authors": ["Lucas Deecke", "Timothy Hospedales", "Hakan Bilen"], "keywords": ["Latent Domain Learning", "CNN Architectures"], "abstract": "A fundamental shortcoming of deep neural networks is their specialization to a single task and domain. While recent techniques in multi-domain learning enable the learning of more domain-agnostic features, their success relies firmly on the presence of domain labels, typically requiring manual annotation and careful curation of datasets. Here we focus on latent domain learning, a highly realistic, yet less explored scenario: learning from data from different domains, without access to domain annotations. This is a particularly challenging problem, since standard models exhibit an implicit bias toward learning only the large domains in data, while disregarding smaller ones. To address this issue, we propose dynamic residual adapters that adaptively account for latent domains, and weighted domain transfer \u2013 a novel augmentation strategy designed specifically for this setting. Our techniques are evaluated on image classification tasks containing multiple unannotated domains, and we demonstrate they enhance performance, in particular, on the smallest of these.", "one-sentence_summary": "We demonstrate that standard models suppress underrepresented latent domains, and formulate novel strategies to limit this behavior.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deecke|reducing_implicit_bias_in_latent_domain_learning", "pdf": "/pdf/492a085a079e0510023ccd898f834aab15947d02.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iqOQ7bIUbc", "_bibtex": "@misc{\ndeecke2021reducing,\ntitle={Reducing Implicit Bias in Latent Domain Learning},\nauthor={Lucas Deecke and Timothy Hospedales and Hakan Bilen},\nyear={2021},\nurl={https://openreview.net/forum?id=McYsRk9-rso}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "McYsRk9-rso", "replyto": "McYsRk9-rso", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1671/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538113391, "tmdate": 1606915760746, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1671/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1671/-/Official_Review"}}}, {"id": "LN5mj9TH940", "original": null, "number": 4, "cdate": 1604472984040, "ddate": null, "tcdate": 1604472984040, "tmdate": 1605024386538, "tddate": null, "forum": "McYsRk9-rso", "replyto": "McYsRk9-rso", "invitation": "ICLR.cc/2021/Conference/Paper1671/-/Official_Review", "content": {"title": "This paper is well written. The motivation of this paper is clear and the proposed framework does break the limitation of the existing deep learning methods.", "review": "The paper describes dynamic residual adapters designed to adaptively account for latent domains, and weighted domain transfer. This framework injects adaptivity into networks, preventing them from overfitting to the largest domains in distributions, a failure mode of traditional models that are exposed in latent domain learning. The approach closes a large amount of the performance gap to domain-supervised solutions. \n\nThis paper is well written. The motivation of this paper is clear and the proposed framework does break the limitation of the existing deep learning methods. Below I present some suggestions, which hopefully can help the authors improve their study:\n\nThe authors do not describe any processing they have done of the data. This should be clearly included in the methods section. More experimental details and insightful discussions should be provided. I suggest the authors repeat the benchmarking using a selection of datasets more similar to those used in current studies.\nThe benchmarking results are insufficiently described in the text and can only really be seen in the tables/figures. The authors should explain the results in more detail in the text and include their interpretation of the results.\nSection 3. In this part, the authors introduce the algorithm. Since there are many formulas in this section, additional explanations on the learning procedure would help understand the proposed method.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1671/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1671/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reducing Implicit Bias in Latent Domain Learning", "authorids": ["~Lucas_Deecke1", "~Timothy_Hospedales1", "~Hakan_Bilen1"], "authors": ["Lucas Deecke", "Timothy Hospedales", "Hakan Bilen"], "keywords": ["Latent Domain Learning", "CNN Architectures"], "abstract": "A fundamental shortcoming of deep neural networks is their specialization to a single task and domain. While recent techniques in multi-domain learning enable the learning of more domain-agnostic features, their success relies firmly on the presence of domain labels, typically requiring manual annotation and careful curation of datasets. Here we focus on latent domain learning, a highly realistic, yet less explored scenario: learning from data from different domains, without access to domain annotations. This is a particularly challenging problem, since standard models exhibit an implicit bias toward learning only the large domains in data, while disregarding smaller ones. To address this issue, we propose dynamic residual adapters that adaptively account for latent domains, and weighted domain transfer \u2013 a novel augmentation strategy designed specifically for this setting. Our techniques are evaluated on image classification tasks containing multiple unannotated domains, and we demonstrate they enhance performance, in particular, on the smallest of these.", "one-sentence_summary": "We demonstrate that standard models suppress underrepresented latent domains, and formulate novel strategies to limit this behavior.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deecke|reducing_implicit_bias_in_latent_domain_learning", "pdf": "/pdf/492a085a079e0510023ccd898f834aab15947d02.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iqOQ7bIUbc", "_bibtex": "@misc{\ndeecke2021reducing,\ntitle={Reducing Implicit Bias in Latent Domain Learning},\nauthor={Lucas Deecke and Timothy Hospedales and Hakan Bilen},\nyear={2021},\nurl={https://openreview.net/forum?id=McYsRk9-rso}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "McYsRk9-rso", "replyto": "McYsRk9-rso", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1671/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538113391, "tmdate": 1606915760746, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1671/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1671/-/Official_Review"}}}], "count": 15}