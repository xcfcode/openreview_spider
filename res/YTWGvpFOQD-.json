{"notes": [{"id": "YTWGvpFOQD-", "original": "CowXrVXVuP", "number": 1641, "cdate": 1601308181894, "ddate": null, "tcdate": 1601308181894, "tmdate": 1613616674990, "tddate": null, "forum": "YTWGvpFOQD-", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Differentially Private Learning Needs Better Features (or Much More Data)", "authorids": ["~Florian_Tramer1", "~Dan_Boneh1"], "authors": ["Florian Tramer", "Dan Boneh"], "keywords": ["Differential Privacy", "Privacy", "Deep Learning"], "abstract": "We demonstrate that differentially private machine learning has not yet reached its ''AlexNet moment'' on many canonical vision tasks: linear models trained on handcrafted features significantly outperform end-to-end deep neural networks for moderate privacy budgets.\nTo exceed the performance of handcrafted features, we show that private learning requires either much more private data, or access to features learned on public data from a similar domain.\nOur work introduces simple yet strong baselines for differentially private learning that can inform the evaluation of future progress in this area.", "one-sentence_summary": "Linear models with handcrafted features outperform end-to-end CNNs for differentially private learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tramer|differentially_private_learning_needs_better_features_or_much_more_data", "supplementary_material": "", "pdf": "/pdf/63107901e325896b18874aad193314befc47c7ae.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ntramer2021differentially,\ntitle={Differentially Private Learning Needs Better Features (or Much More Data)},\nauthor={Florian Tramer and Dan Boneh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=YTWGvpFOQD-}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 16, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "5O7SXjEiRsK", "original": null, "number": 1, "cdate": 1610040432631, "ddate": null, "tcdate": 1610040432631, "tmdate": 1610474032846, "tddate": null, "forum": "YTWGvpFOQD-", "replyto": "YTWGvpFOQD-", "invitation": "ICLR.cc/2021/Conference/Paper1641/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Spotlight)", "comment": "This paper presents a very interesting investigation. While deep neural networks are typically best in non-private settings, the authors show that linear models with handcrafted features (ScatterNets) perform better in certain settings of the privacy parameter. The reviewers all found this to be important and insightful, with a thorough investigation, and I tend to agree, recommending acceptance."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentially Private Learning Needs Better Features (or Much More Data)", "authorids": ["~Florian_Tramer1", "~Dan_Boneh1"], "authors": ["Florian Tramer", "Dan Boneh"], "keywords": ["Differential Privacy", "Privacy", "Deep Learning"], "abstract": "We demonstrate that differentially private machine learning has not yet reached its ''AlexNet moment'' on many canonical vision tasks: linear models trained on handcrafted features significantly outperform end-to-end deep neural networks for moderate privacy budgets.\nTo exceed the performance of handcrafted features, we show that private learning requires either much more private data, or access to features learned on public data from a similar domain.\nOur work introduces simple yet strong baselines for differentially private learning that can inform the evaluation of future progress in this area.", "one-sentence_summary": "Linear models with handcrafted features outperform end-to-end CNNs for differentially private learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tramer|differentially_private_learning_needs_better_features_or_much_more_data", "supplementary_material": "", "pdf": "/pdf/63107901e325896b18874aad193314befc47c7ae.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ntramer2021differentially,\ntitle={Differentially Private Learning Needs Better Features (or Much More Data)},\nauthor={Florian Tramer and Dan Boneh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=YTWGvpFOQD-}\n}"}, "tags": [], "invitation": {"reply": {"forum": "YTWGvpFOQD-", "replyto": "YTWGvpFOQD-", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040432618, "tmdate": 1610474032829, "id": "ICLR.cc/2021/Conference/Paper1641/-/Decision"}}}, {"id": "zw03-casx-A", "original": null, "number": 12, "cdate": 1606152382112, "ddate": null, "tcdate": 1606152382112, "tmdate": 1606152382112, "tddate": null, "forum": "YTWGvpFOQD-", "replyto": "j0bzv-F-qj4", "invitation": "ICLR.cc/2021/Conference/Paper1641/-/Official_Comment", "content": {"title": "Response", "comment": "Thank you for your appreciation, and for the fruitful discussion which will help us improve the presentation of our results."}, "signatures": ["ICLR.cc/2021/Conference/Paper1641/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1641/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentially Private Learning Needs Better Features (or Much More Data)", "authorids": ["~Florian_Tramer1", "~Dan_Boneh1"], "authors": ["Florian Tramer", "Dan Boneh"], "keywords": ["Differential Privacy", "Privacy", "Deep Learning"], "abstract": "We demonstrate that differentially private machine learning has not yet reached its ''AlexNet moment'' on many canonical vision tasks: linear models trained on handcrafted features significantly outperform end-to-end deep neural networks for moderate privacy budgets.\nTo exceed the performance of handcrafted features, we show that private learning requires either much more private data, or access to features learned on public data from a similar domain.\nOur work introduces simple yet strong baselines for differentially private learning that can inform the evaluation of future progress in this area.", "one-sentence_summary": "Linear models with handcrafted features outperform end-to-end CNNs for differentially private learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tramer|differentially_private_learning_needs_better_features_or_much_more_data", "supplementary_material": "", "pdf": "/pdf/63107901e325896b18874aad193314befc47c7ae.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ntramer2021differentially,\ntitle={Differentially Private Learning Needs Better Features (or Much More Data)},\nauthor={Florian Tramer and Dan Boneh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=YTWGvpFOQD-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "YTWGvpFOQD-", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1641/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1641/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1641/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1641/Authors|ICLR.cc/2021/Conference/Paper1641/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1641/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857402, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1641/-/Official_Comment"}}}, {"id": "U5WeIre4ggR", "original": null, "number": 1, "cdate": 1603576503918, "ddate": null, "tcdate": 1603576503918, "tmdate": 1606151640259, "tddate": null, "forum": "YTWGvpFOQD-", "replyto": "YTWGvpFOQD-", "invitation": "ICLR.cc/2021/Conference/Paper1641/-/Official_Review", "content": {"title": "Review", "review": "The paper presents an analysis of differential privacy in machine learning, with a focus on neural networks trained via differentially private stochastic gradient descent (DPSGD). The main focus and the message in the paper is that the handcrafted features work better compared to learned features during training of NNs and having more training data results in better outcomes (i.e. a better privacy-utility trade-off).\n\nStarting with the latter, this is apparent from the noise formulation in DPSGD, where the noise is reduced via sampling probability, which decreases as the data size grows. Hence, I do not consider this as a new insight or a contribution. Unless, I have misunderstood something, in which case, please do explain.\n\nFor the former, as the final model used (Table 3 and Figure 1) is a linear classifier, it outperforming an end-to-end CNN based model is intuitive, as it has far fewer number of parameters (which improve the noise scale, due to smaller gradient norm). This is slightly touched upon in subsection \"Training CNNs on handcrafted features\", where the comparison is made using CNNs on the handcrafted features, however there are no detailed results presented in the paper, I would have liked to see a similar table and figures as earlier.\n\nThe presentation of results (Table 3) is a bit strange. I would have further liked to see the comparison of both models on the *same* set of hyperparameters. Also, instead of stating that hyperparameter search's privacy budget was not accounted for as in prior works, it would have been nice to see some analysis, such as the section D(Appendix) in Abadi et al.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1641/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1641/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentially Private Learning Needs Better Features (or Much More Data)", "authorids": ["~Florian_Tramer1", "~Dan_Boneh1"], "authors": ["Florian Tramer", "Dan Boneh"], "keywords": ["Differential Privacy", "Privacy", "Deep Learning"], "abstract": "We demonstrate that differentially private machine learning has not yet reached its ''AlexNet moment'' on many canonical vision tasks: linear models trained on handcrafted features significantly outperform end-to-end deep neural networks for moderate privacy budgets.\nTo exceed the performance of handcrafted features, we show that private learning requires either much more private data, or access to features learned on public data from a similar domain.\nOur work introduces simple yet strong baselines for differentially private learning that can inform the evaluation of future progress in this area.", "one-sentence_summary": "Linear models with handcrafted features outperform end-to-end CNNs for differentially private learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tramer|differentially_private_learning_needs_better_features_or_much_more_data", "supplementary_material": "", "pdf": "/pdf/63107901e325896b18874aad193314befc47c7ae.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ntramer2021differentially,\ntitle={Differentially Private Learning Needs Better Features (or Much More Data)},\nauthor={Florian Tramer and Dan Boneh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=YTWGvpFOQD-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "YTWGvpFOQD-", "replyto": "YTWGvpFOQD-", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1641/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538114042, "tmdate": 1606915781642, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1641/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1641/-/Official_Review"}}}, {"id": "j0bzv-F-qj4", "original": null, "number": 11, "cdate": 1606151618698, "ddate": null, "tcdate": 1606151618698, "tmdate": 1606151618698, "tddate": null, "forum": "YTWGvpFOQD-", "replyto": "gBHXMRi-jEP", "invitation": "ICLR.cc/2021/Conference/Paper1641/-/Official_Comment", "content": {"title": "Response", "comment": "Thank you for all the answers. I have updated my score accordingly."}, "signatures": ["ICLR.cc/2021/Conference/Paper1641/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1641/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentially Private Learning Needs Better Features (or Much More Data)", "authorids": ["~Florian_Tramer1", "~Dan_Boneh1"], "authors": ["Florian Tramer", "Dan Boneh"], "keywords": ["Differential Privacy", "Privacy", "Deep Learning"], "abstract": "We demonstrate that differentially private machine learning has not yet reached its ''AlexNet moment'' on many canonical vision tasks: linear models trained on handcrafted features significantly outperform end-to-end deep neural networks for moderate privacy budgets.\nTo exceed the performance of handcrafted features, we show that private learning requires either much more private data, or access to features learned on public data from a similar domain.\nOur work introduces simple yet strong baselines for differentially private learning that can inform the evaluation of future progress in this area.", "one-sentence_summary": "Linear models with handcrafted features outperform end-to-end CNNs for differentially private learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tramer|differentially_private_learning_needs_better_features_or_much_more_data", "supplementary_material": "", "pdf": "/pdf/63107901e325896b18874aad193314befc47c7ae.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ntramer2021differentially,\ntitle={Differentially Private Learning Needs Better Features (or Much More Data)},\nauthor={Florian Tramer and Dan Boneh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=YTWGvpFOQD-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "YTWGvpFOQD-", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1641/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1641/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1641/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1641/Authors|ICLR.cc/2021/Conference/Paper1641/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1641/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857402, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1641/-/Official_Comment"}}}, {"id": "gBHXMRi-jEP", "original": null, "number": 10, "cdate": 1606151309557, "ddate": null, "tcdate": 1606151309557, "tmdate": 1606151448437, "tddate": null, "forum": "YTWGvpFOQD-", "replyto": "DN7kxmHiRRh", "invitation": "ICLR.cc/2021/Conference/Paper1641/-/Official_Comment", "content": {"title": "Great questions", "comment": "Thanks for the response, we are glad that we could clarify these points.\n\nAnd thank you for raising these additional insightful questions. We will also clarify these points in our writeup.\n\n**Q2:** The ScatterNet transform we use was indeed designed for images, as it aims to capture invariants that are specific to natural image data (e.g., invariance under small rotations and translations).\n\nAlternative transforms have been proposed for other domains such as speech (https://arxiv.org/abs/1304.6763) and chemistry (https://math.msu.edu/user_content/docs/m10754520190706225610788.pdf).\n\nFor domains with tabular data, the Scattering transform is probably not directly applicable. Nevertheless, whenever we have priors on the data domain, domain-specific handcrafted features may be available, and *our thesis is that these handcrafted features are beneficial for private learning*.\n\nFor example, we are very interested in exploring extensions of our work to natural language processing, a domain with a rich history of task-specific handcrafted features, which may also prove useful to improve private deep learning.\n\n**Q5:** All our experiments are performed on a single TITAN xp GPU with 12GB of memory. \n\nAs DP-SGD requires computing per-example gradients, we indeed cannot fit all gradients in memory at the same time. \nTo solve this, we make use of the \"virtual step\" feature in opacus (https://github.com/pytorch/opacus). Basically, we split each batch into \"mini batches\" that fit in memory (we use mini batches of size 256 in all our experiments). For each mini-batch, we compute per-example gradients, clip them, and sum them up. Once we've processed an entire batch, we add the noise and take the actual update step. Note that this \"caching\" approach has no influence on the privacy analysis of DP-SGD.\n\nThe process is very similar to how standard SGD can be scaled to large batch sizes that do not fit in GPU memory.\n\nWe have uploaded code to reproduce all our experiments. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1641/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1641/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentially Private Learning Needs Better Features (or Much More Data)", "authorids": ["~Florian_Tramer1", "~Dan_Boneh1"], "authors": ["Florian Tramer", "Dan Boneh"], "keywords": ["Differential Privacy", "Privacy", "Deep Learning"], "abstract": "We demonstrate that differentially private machine learning has not yet reached its ''AlexNet moment'' on many canonical vision tasks: linear models trained on handcrafted features significantly outperform end-to-end deep neural networks for moderate privacy budgets.\nTo exceed the performance of handcrafted features, we show that private learning requires either much more private data, or access to features learned on public data from a similar domain.\nOur work introduces simple yet strong baselines for differentially private learning that can inform the evaluation of future progress in this area.", "one-sentence_summary": "Linear models with handcrafted features outperform end-to-end CNNs for differentially private learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tramer|differentially_private_learning_needs_better_features_or_much_more_data", "supplementary_material": "", "pdf": "/pdf/63107901e325896b18874aad193314befc47c7ae.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ntramer2021differentially,\ntitle={Differentially Private Learning Needs Better Features (or Much More Data)},\nauthor={Florian Tramer and Dan Boneh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=YTWGvpFOQD-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "YTWGvpFOQD-", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1641/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1641/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1641/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1641/Authors|ICLR.cc/2021/Conference/Paper1641/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1641/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857402, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1641/-/Official_Comment"}}}, {"id": "DN7kxmHiRRh", "original": null, "number": 9, "cdate": 1606148495403, "ddate": null, "tcdate": 1606148495403, "tmdate": 1606148495403, "tddate": null, "forum": "YTWGvpFOQD-", "replyto": "d7ooO0edGw2", "invitation": "ICLR.cc/2021/Conference/Paper1641/-/Official_Comment", "content": {"title": "Response", "comment": "Q2. Thank you for the explanation. I had missed the scatternet transform's property of \"increasing dimensionality\".  This makes sense. I was under the impression that automated feature extraction would result in lower dimensionality than the original dimensions. \n\nDo you have any comment on the applicability of the proposed tricks on tabular data? As I assume the inherent differences between image and tabular data would result in significantly different feature extraction and/or the subspace on which they reside.\n\nQ4. I agree. \n\nQ5. Yes, that is much better. Can you please provide the details on the computing infrastructure used for the experiments? As I understand, increasing batch size takes a heavy toll on DPSGD's performance.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1641/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1641/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentially Private Learning Needs Better Features (or Much More Data)", "authorids": ["~Florian_Tramer1", "~Dan_Boneh1"], "authors": ["Florian Tramer", "Dan Boneh"], "keywords": ["Differential Privacy", "Privacy", "Deep Learning"], "abstract": "We demonstrate that differentially private machine learning has not yet reached its ''AlexNet moment'' on many canonical vision tasks: linear models trained on handcrafted features significantly outperform end-to-end deep neural networks for moderate privacy budgets.\nTo exceed the performance of handcrafted features, we show that private learning requires either much more private data, or access to features learned on public data from a similar domain.\nOur work introduces simple yet strong baselines for differentially private learning that can inform the evaluation of future progress in this area.", "one-sentence_summary": "Linear models with handcrafted features outperform end-to-end CNNs for differentially private learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tramer|differentially_private_learning_needs_better_features_or_much_more_data", "supplementary_material": "", "pdf": "/pdf/63107901e325896b18874aad193314befc47c7ae.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ntramer2021differentially,\ntitle={Differentially Private Learning Needs Better Features (or Much More Data)},\nauthor={Florian Tramer and Dan Boneh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=YTWGvpFOQD-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "YTWGvpFOQD-", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1641/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1641/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1641/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1641/Authors|ICLR.cc/2021/Conference/Paper1641/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1641/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857402, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1641/-/Official_Comment"}}}, {"id": "d7ooO0edGw2", "original": null, "number": 8, "cdate": 1606084877463, "ddate": null, "tcdate": 1606084877463, "tmdate": 1606087312271, "tddate": null, "forum": "YTWGvpFOQD-", "replyto": "ZdJvSFgQjzu", "invitation": "ICLR.cc/2021/Conference/Paper1641/-/Official_Comment", "content": {"title": "Response", "comment": "Thank you for the follow-up to your review and for the clarifications.\n\n**Q2:** There are three factors that explain this fact: (1) the ScatterNet transform is slightly expansive; (2) Convolutions have fewer parameters than dense layers due to weight sharing; (3) Deep networks typically downsample the input (e.g., via maxpooling or convolutions of stride > 1) before the final dense layers.\n\nFirst, note that CNNs having fewer parameters than linear models is very common. E.g., on ImageNet with images of 224x224 pixels, a Logistic regression model would have approximately 150M parameters (224\\*224\\*3\\*1000 + 1000). In contrast, a ResNet-50 has only about 25M parameters.\n\nLet us illustrate with concrete numbers for the models we use for MNIST and Fashion-MNIST:\n- The input size is 28x28x1=784. A linear model on top of pixels (which we don't consider in our paper) would have 7850 parameters.\n- The ScatterNet transform outputs features of dimension 7x7x81 (i.e., the transform combines spatial downsampling and channel upsampling). **The linear ScatterNet classifiers have 39700 parameters** (7\\*7\\*81\\*10 + 10).\n- The end-to-end CNN has two convolutional layers, each followed by a maxpool layer, and then two dense layers. Due to the weight-sharing of convolutions and the spatial downsampling of both the convolutions and maxpool layers, this model only has **26010 parameters.**\n-  The ScatterNet+CNN model also has maxpool layers that downsample the features before the dense layers. This model has **33066 parameters.**\n\nIf we had used deep *dense* models, then those would indeed have more parameters than the linear models.\n\n**Q4:** Thank you for this clarification. We agree that re-using the same set of hyperparameters as in a prior work would be a good approach. Unfortunately, prior work rarely provides these details. E.g., the best source of comparison would be the work of Papernot et al. which proposed the CNNs we use, but their paper does not list the hyperparameters considered. The work of Abadi et al. gave more explicit hyperparameters, but their setup differs too significantly from ours to allow for a direct comparison (Abadi et al. use private PCA before training MNIST models, and they use transfer-learning for CIFAR-10).\n\nWe hope that our approach of listing all hyperparameter searches performed will encourage future work in this area to also provide these important details.\n\n**Q5:** Apologies, we should clarify what we meant by this. We agree that, all other parameters being fixed, the choice of batch-size is important for DP-SGD. *But the same is also true for regular SGD!* \n\nIndeed, there is a large literature on batch size scaling in the non-private case, that shows that when multiplying the batch size by a factor k, the learning rate should also be multiplied by k to keep the performance constant. As we show in Appendix D.1, the same simple linear scaling rule also holds for DP-SGD. \n\nIn Appendix D.1, we further show formally that if we scale both the learning rate and the batch size, and adjust the noise level according to the analysis of Abadi et al., then the amount of noise that DP-SGD injects per epoch remains the same. Our experimental results (Figure 7) confirm this analysis: model convergence stays the same as we increase the batch size from 512 to 4096 with a linearly scaled learning rate (and all other parameters fixed).\n\nSo a more precise statement would be: *with all other parameters fixed, tuning the batch size of DP-SGD is similar to tuning its learning rate*. As tuning the learning rate is much more common with regular SGD, we recommend to do the same with DP-SGD."}, "signatures": ["ICLR.cc/2021/Conference/Paper1641/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1641/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentially Private Learning Needs Better Features (or Much More Data)", "authorids": ["~Florian_Tramer1", "~Dan_Boneh1"], "authors": ["Florian Tramer", "Dan Boneh"], "keywords": ["Differential Privacy", "Privacy", "Deep Learning"], "abstract": "We demonstrate that differentially private machine learning has not yet reached its ''AlexNet moment'' on many canonical vision tasks: linear models trained on handcrafted features significantly outperform end-to-end deep neural networks for moderate privacy budgets.\nTo exceed the performance of handcrafted features, we show that private learning requires either much more private data, or access to features learned on public data from a similar domain.\nOur work introduces simple yet strong baselines for differentially private learning that can inform the evaluation of future progress in this area.", "one-sentence_summary": "Linear models with handcrafted features outperform end-to-end CNNs for differentially private learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tramer|differentially_private_learning_needs_better_features_or_much_more_data", "supplementary_material": "", "pdf": "/pdf/63107901e325896b18874aad193314befc47c7ae.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ntramer2021differentially,\ntitle={Differentially Private Learning Needs Better Features (or Much More Data)},\nauthor={Florian Tramer and Dan Boneh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=YTWGvpFOQD-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "YTWGvpFOQD-", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1641/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1641/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1641/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1641/Authors|ICLR.cc/2021/Conference/Paper1641/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1641/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857402, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1641/-/Official_Comment"}}}, {"id": "ZdJvSFgQjzu", "original": null, "number": 7, "cdate": 1606071664637, "ddate": null, "tcdate": 1606071664637, "tmdate": 1606071664637, "tddate": null, "forum": "YTWGvpFOQD-", "replyto": "o9rcf8gAqo", "invitation": "ICLR.cc/2021/Conference/Paper1641/-/Official_Comment", "content": {"title": "Response to authors", "comment": "Thank you. I very much appreciate the detailed response.\n\nStarting with Q2, I still do not understand how the linear classifier has more parameters than a deep CNN. Please correct me if I am wrong: Linear model = logistic regression(or a single layer NN) on top of extracted features via scatter net? if that is the case, then the number of parameters should be smaller, no? i.e. a single layer NN compared to multilayer NN. Or am I missing any obvious details?\n\nQ4: Thanks, that remark makes it clear. I meant was to use the set of hyperparameters from a previous study for competitors and run your model with a similar set of hyperparameters.\n\nQ5: Thanks. Choice of batch size is not very important in DPSGD: I disagree with the statement as we can see from theorem 1 of Abadi et al. that the batch size is directly proportional to the noise addition. Although this is countered by the summation step (before adding noise), but making such a blanket statement is not correct as in practice, we see a significant difference, given everything else is fixed.\n\n\n\n\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1641/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1641/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentially Private Learning Needs Better Features (or Much More Data)", "authorids": ["~Florian_Tramer1", "~Dan_Boneh1"], "authors": ["Florian Tramer", "Dan Boneh"], "keywords": ["Differential Privacy", "Privacy", "Deep Learning"], "abstract": "We demonstrate that differentially private machine learning has not yet reached its ''AlexNet moment'' on many canonical vision tasks: linear models trained on handcrafted features significantly outperform end-to-end deep neural networks for moderate privacy budgets.\nTo exceed the performance of handcrafted features, we show that private learning requires either much more private data, or access to features learned on public data from a similar domain.\nOur work introduces simple yet strong baselines for differentially private learning that can inform the evaluation of future progress in this area.", "one-sentence_summary": "Linear models with handcrafted features outperform end-to-end CNNs for differentially private learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tramer|differentially_private_learning_needs_better_features_or_much_more_data", "supplementary_material": "", "pdf": "/pdf/63107901e325896b18874aad193314befc47c7ae.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ntramer2021differentially,\ntitle={Differentially Private Learning Needs Better Features (or Much More Data)},\nauthor={Florian Tramer and Dan Boneh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=YTWGvpFOQD-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "YTWGvpFOQD-", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1641/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1641/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1641/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1641/Authors|ICLR.cc/2021/Conference/Paper1641/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1641/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857402, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1641/-/Official_Comment"}}}, {"id": "o9rcf8gAqo", "original": null, "number": 6, "cdate": 1605641250424, "ddate": null, "tcdate": 1605641250424, "tmdate": 1605652631110, "tddate": null, "forum": "YTWGvpFOQD-", "replyto": "U5WeIre4ggR", "invitation": "ICLR.cc/2021/Conference/Paper1641/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "We thank the reviewer for their insightful comments.\n\n**1)    Q:** *...having more training data results in better outcomes [...] I do not consider this as a new insight or a contribution. Unless, I have misunderstood something, in which case, please do explain.*\n\n**A:** As many reviewers raised a similar concern about expectedness of our results when training with more data, we provide a detailed response to these concerns in a meta-comment above.\n\n*In summary, while the tradeoff we show is indeed expected qualitatively, we are interested in a quantitative assessment: how much more private data is needed for private end-to-end deep learning to outperform handcrafted features?*\n\nOur result shows that improving private deep learning simply by collecting more data might be very expensive for canonical vision tasks like CIFAR-10: we need about one order of magnitude more data before the end-to-end CNN outperforms our handcrafted baselines. This motivates the design of better private learning algorithms in low data regimes.\n\n**2)    Q:** *as the final model used (Table 3 and Figure 1) is a linear classifier, it outperforming an end-to-end CNN based model is intuitive, as it has far fewer number of parameters*\n\n**A:** **The claim that our linear classifiers have fewer parameters than end-to-end CNNs is incorrect!** This is the point of our analysis in Section 4: \u201cSmaller models are not easier to train privately.\u201d\n\nWe find that ScatterNet models outperform end-to-end CNNs *despite having more trainable parameters!* As the reviewer correctly notes, the noise analysis of DP-SGD would suggest the opposite, **so this result is surprising.**\nWe have clarified this in our introduction.\n\nAs we show in Section 4, handcrafted features give rise to an \u201ceasier\u201d learning task, where convergence occurs much faster *despite the higher noise.*\n\n**3)    Q:**  *there are no detailed results presented in the paper, I would have liked to see a similar table and figures as earlier*\n\n**A:** It is unclear to us what the reviewer means by a lack of \u201cdetailed results\u201d for CNNs trained on ScatterNet features. Figure 2 is analogous to the earlier Figure 1 and directly compares the ScatterNet+linear, ScatterNet+CNN and end-to-end CNN models for all privacy budgets we consider.\nWe have also added results with ScatterNet+CNN models to Table 1 and Table 3, which in particular includes some improved results for the ScatterNet+CNN model on CIFAR-10, following a suggestion of reviewer 3.\nAre there other results that we could add to the paper to better compare the ScatterNet and end-to-end results?\n\n**4)    Q:** *The presentation of results (Table 3) is a bit strange. I would have further liked to see the comparison of both models on the same set of hyperparameters*\n\n**A:** We are not sure what the reviewer means by a \u201ccomparison of both models on the same set of hyperparameters\u201d. It seems to us that this is exactly what is shown in Figure 1, which shows the best performance achieved by both linear ScattterNet models and end-to-end CNNs for each privacy budget, across the entire hyper-parameter set.\n\nIf the reviewer meant comparing these models for a *single* assignment of hyper-parameters, this is somewhat tricky to do in a fair way. It is unclear how to choose those hyper-parameters so as not to favor one of the two models. \nWe note that for CIFAR-10 this issue is moot anyhow: *the best hyper-parameter assignment for end-to-end CNNs is outperformed by the worst assignment of hyper-parameters for linear ScatterNet models (Table 3).*\n\n**4)    Q:** *instead of stating that hyper-parameter search's privacy budget was not accounted for as in prior works, it would have been nice to see some analysis, such as the section D(Appendix) in Abadi et al.*\n\n**A:** We can definitely perform an analysis of the cost of hyper-parameter search as in Abadi et al. Ultimately though, none of the prior works that we compare against in Table 1 do this, so it is hard to perform a fair comparison that accounts for this cost. If we solely consider our comparison of end-to-end CNNs and ScatterNet models, then the hyper-parameter sets are identical so the analysis of Abadi et al. would yield the same cost in both cases. Moreover, as we show in Table 2 and Appendices D.1 and D.2, our hyper-parameter search was anyhow somewhat \u201cexcessive\u201d, especially for linear ScatterNets. We would obtain similar results by considering a much smaller set of parameters (e.g., we find that contrary to what was claimed in prior work, the choice of batch size is not very important in DP-SGD, as long as the learning rate is set adequately).\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1641/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1641/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentially Private Learning Needs Better Features (or Much More Data)", "authorids": ["~Florian_Tramer1", "~Dan_Boneh1"], "authors": ["Florian Tramer", "Dan Boneh"], "keywords": ["Differential Privacy", "Privacy", "Deep Learning"], "abstract": "We demonstrate that differentially private machine learning has not yet reached its ''AlexNet moment'' on many canonical vision tasks: linear models trained on handcrafted features significantly outperform end-to-end deep neural networks for moderate privacy budgets.\nTo exceed the performance of handcrafted features, we show that private learning requires either much more private data, or access to features learned on public data from a similar domain.\nOur work introduces simple yet strong baselines for differentially private learning that can inform the evaluation of future progress in this area.", "one-sentence_summary": "Linear models with handcrafted features outperform end-to-end CNNs for differentially private learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tramer|differentially_private_learning_needs_better_features_or_much_more_data", "supplementary_material": "", "pdf": "/pdf/63107901e325896b18874aad193314befc47c7ae.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ntramer2021differentially,\ntitle={Differentially Private Learning Needs Better Features (or Much More Data)},\nauthor={Florian Tramer and Dan Boneh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=YTWGvpFOQD-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "YTWGvpFOQD-", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1641/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1641/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1641/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1641/Authors|ICLR.cc/2021/Conference/Paper1641/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1641/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857402, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1641/-/Official_Comment"}}}, {"id": "-CiCLaeoab", "original": null, "number": 2, "cdate": 1605640179593, "ddate": null, "tcdate": 1605640179593, "tmdate": 1605652197079, "tddate": null, "forum": "YTWGvpFOQD-", "replyto": "YTWGvpFOQD-", "invitation": "ICLR.cc/2021/Conference/Paper1641/-/Official_Comment", "content": {"title": "Clarification of contributions and changes to submission", "comment": "The main contribution of our paper is to show that in standard data regimes, private end-to-end deep learning is outperformed by simpler (linear) models on top of handcrafted features.\n\nGiven this state of affairs, we further explore two natural avenues to bridge the gap between standard and private learning: collecting more private data, and transfer learning from public data.\nThe question we aim to answer here is: *what is the cost of DP\u2019s \u201cAlexNet\u201d moment?* \nI.e., what do we need in order for private learning to outperform handcrafted features?\n\nMultiple reviewers have raised a similar concern, namely that it is not very surprising that these two avenues---collecting more private data, and transfer learning---lead to improved results. \n\n**We remark that these results were not meant to be *qualitatively* surprising! Rather, our aim was to provide *quantitative* results to compare the viability of different natural approaches for improving private learning.**\n\nThese approaches, while known, haven\u2019t been rigorously evaluated in past work.\nFrom our experiments, we extract quantitative results that were not obvious a priori:\n- Beating handcrafted features with end-to-end deep learning requires about *one order of magnitude more private training data* on CIFAR-10.\n- With transfer learning from a state-of-the-art source model, differential privacy comes at only a small cost in accuracy, and significantly outperforms handcrafted features.\n\nFirst, our results show that the data-complexity of private deep learning is currently much higher than for standard learning, and this motivates the design of better private learning algorithms in low data regimes (such as our approach with handcrafted features).\nIf the results had been quantitatively different (e.g., 10% additional data suffices to bridge the gap), our conclusions would of course be very different.\n\nSecond, our strong results with transfer learning may seem obvious in hindsight, and yet prior results in this setting have reported much worse results than ours, primarily because they built upon weak non-private transfer learning baselines.\nThis has led to a significant amount of confusion in the literature about the limitations of private (transfer) learning, which our paper rectifies.\nAs with the other results in our paper, we hope that our improved transfer learning results can act as strong baselines to compare future approaches for private learning.\n\n**We have amended our submission to clarify the above points. We have also added improved results for the ScatterNet+CNN models on CIFAR-10, following a suggestion from reviewer 2. We find that CNNs with handcrafted features can slightly improve over the linear models. This does not change our main conclusion---i.e, handcrafted features help for private learning.**\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1641/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1641/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentially Private Learning Needs Better Features (or Much More Data)", "authorids": ["~Florian_Tramer1", "~Dan_Boneh1"], "authors": ["Florian Tramer", "Dan Boneh"], "keywords": ["Differential Privacy", "Privacy", "Deep Learning"], "abstract": "We demonstrate that differentially private machine learning has not yet reached its ''AlexNet moment'' on many canonical vision tasks: linear models trained on handcrafted features significantly outperform end-to-end deep neural networks for moderate privacy budgets.\nTo exceed the performance of handcrafted features, we show that private learning requires either much more private data, or access to features learned on public data from a similar domain.\nOur work introduces simple yet strong baselines for differentially private learning that can inform the evaluation of future progress in this area.", "one-sentence_summary": "Linear models with handcrafted features outperform end-to-end CNNs for differentially private learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tramer|differentially_private_learning_needs_better_features_or_much_more_data", "supplementary_material": "", "pdf": "/pdf/63107901e325896b18874aad193314befc47c7ae.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ntramer2021differentially,\ntitle={Differentially Private Learning Needs Better Features (or Much More Data)},\nauthor={Florian Tramer and Dan Boneh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=YTWGvpFOQD-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "YTWGvpFOQD-", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1641/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1641/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1641/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1641/Authors|ICLR.cc/2021/Conference/Paper1641/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1641/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857402, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1641/-/Official_Comment"}}}, {"id": "AsGnKqZh5Gc", "original": null, "number": 5, "cdate": 1605640712224, "ddate": null, "tcdate": 1605640712224, "tmdate": 1605641583083, "tddate": null, "forum": "YTWGvpFOQD-", "replyto": "ZYyQCdSCMT", "invitation": "ICLR.cc/2021/Conference/Paper1641/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "We thank the reviewer for their insightful comments.\n\n**1)    Q:** *However, I feel like the results for having more data / transfer learning is not so surprising, though the experiments with models different from previous work are valuable.*\n\n**A:** As many reviewers raised a similar concern about expectedness of our results when training with more data or with transfer learning, we provide a detailed response to these concerns in a meta-comment above.\n\nIn summary, while the data tradeoff we show is indeed expected *qualitatively*, we are interested in a *quantitative* assessment: how much more private data is needed for private end-to-end deep learning to work?\nSimilarly, our experiments with transfer learning aim to rectify confusion in the literature about the quantitative limitations of DP in this setting, which seem largely due to prior work only evaluating private transfer learning with very weak public source models (as the reviewer notes, the only difference in our approach is the use of a state-of-the-art source model. It is unclear to us why prior work has not followed this approach).\n\n**2)    Q:** *Maybe you could try something deeper than linear but shallower than the CNN to see if there is a sweet spot in between.*\n\n**A:** We agree with the reviewer that the non-convexity or larger capacity of neural networks are likely a cause for their slower convergence (with or without ScatterNet features). Following the reviewers' great suggestion, we experimented with shallower CNN architectures on MNIST, Fashion-MNIST and CIFAR-10, by varying the number of convolutional layers and dense layers.\nWe observe no improvement for the end-to-end CNNs (this is not very surprising, as the original CNNs we use were the result of an architecture search tailored to DP-SGD performed by Papernot et al.)\n**However, for the Scat+CNN model on CIFAR-10, we do see some improvements with shallower architectures!**\n\nSpecifically, our best Scat+CNN model slightly outperforms our best linear model on CIFAR-10 (we get 69% accuracy at eps=3). \nThis is exciting! We have added these improved results to our submission, and we plan to perform a more thorough architecture search for ScatterNet models.\nFor MNIST and Fashion-MNIST, we could not improve the Scat+CNN results further by varying the model depth. The architectures we use are already very shallow (2 convolutions followed by 2 dense layers) and further reducing the model size reduces private and non-private accuracy alike.\n\nThe main reason for this discrepancy between (Fashion)-MNIST and CIFAR-10 is that on the simpler MNIST tasks, the linear ScatterNet model achieves near optimal accuracy in the non-private setting. Therefore, deeper architectures have little room for improvement, and since they converge more slowly, they perform worse in the private setting. On CIFAR-10 however, the linear ScatterNet model achieves about 71% non-private accuracy, which is far from state-of-the-art. Our shallow Scat+CNN model achieves about 74% non-private accuracy, so even though it converges a bit slower, the extra base accuracy is sufficient to slightly outperform the linear model in the private setting. The end-to-end CNN is at the other end of this spectrum: it achieves over 80% accuracy in the non-private setting, but converges too slowly to compete in the private setting. These results are included in our updated submission.\n\nOur main conclusion remains that handcrafted features are highly beneficial for private learning in low data regimes.\n\n**3)    Q:** *In Sec 5.2, you showed results that are much better than previous results with transfer learning. It seems like the main difference is the model architecture. Is that the case? Do you have any comment on that?*\n\n**A:** This indeed seems to be the main difference (e.g., the model used by Papernot et al. achieves 75% accuracy in the non-private setting). Our results thus show that as in the non-private case, the performance of private transfer learning is highly dependent on the choice of a good source model. A thorough analysis of differentially private transfer learning is out of scope of our paper, but our results provide strong baselines to inform such a study in the future.\n\n**4)    Q:** *I would love to see more details about ScatterNet*\n\n**A:** We do provide some details on ScatterNets in Appendix C.1. Are there additional aspects of these networks that would be helpful for us to include there?\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1641/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1641/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentially Private Learning Needs Better Features (or Much More Data)", "authorids": ["~Florian_Tramer1", "~Dan_Boneh1"], "authors": ["Florian Tramer", "Dan Boneh"], "keywords": ["Differential Privacy", "Privacy", "Deep Learning"], "abstract": "We demonstrate that differentially private machine learning has not yet reached its ''AlexNet moment'' on many canonical vision tasks: linear models trained on handcrafted features significantly outperform end-to-end deep neural networks for moderate privacy budgets.\nTo exceed the performance of handcrafted features, we show that private learning requires either much more private data, or access to features learned on public data from a similar domain.\nOur work introduces simple yet strong baselines for differentially private learning that can inform the evaluation of future progress in this area.", "one-sentence_summary": "Linear models with handcrafted features outperform end-to-end CNNs for differentially private learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tramer|differentially_private_learning_needs_better_features_or_much_more_data", "supplementary_material": "", "pdf": "/pdf/63107901e325896b18874aad193314befc47c7ae.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ntramer2021differentially,\ntitle={Differentially Private Learning Needs Better Features (or Much More Data)},\nauthor={Florian Tramer and Dan Boneh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=YTWGvpFOQD-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "YTWGvpFOQD-", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1641/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1641/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1641/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1641/Authors|ICLR.cc/2021/Conference/Paper1641/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1641/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857402, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1641/-/Official_Comment"}}}, {"id": "EE8WiZ40J7", "original": null, "number": 4, "cdate": 1605640491982, "ddate": null, "tcdate": 1605640491982, "tmdate": 1605641541037, "tddate": null, "forum": "YTWGvpFOQD-", "replyto": "uSXag5Z679O", "invitation": "ICLR.cc/2021/Conference/Paper1641/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "We thank the reviewer for their insightful comments.\n\n**1)    Q:** *Lower bounds for this degradation have been shown theoretically (e.g. Bassily et al., 2014), and there has recently been also work on circumventing this issue*\n\n**A:** Thank you for pointing us to the works of Kairouz et al. and Banerjee et al. We added a reference to these works in our paper.\nThere has indeed been a lot of work on trying to improve the performance of DP-SGD (e.g., all the works listed in Table 1). As our results show however, proposed improvements have typically been evaluated in sub-par settings, which makes it hard to convincingly argue which techniques will generalize to better models.\nFor example, Banerjee et al. achieve 85% accuracy on MNIST, which is far below ours and prior results. \nWe hope that our work can provide simple and strong baselines for private learning, which can be used to assess future proposed improvements to DP-SGD.\n\n**2)    Q:** *As expected, these 'handcrafted data-independent feature extractors' of Scatter Networks cannot beat CNN+DP-SGD when more private data is available, or when features can be extracted from public image data*\n\n**A:** As many reviewers raised a similar concern about expectedness of our results when training with more data or with transfer learning, we provide a detailed response to these concerns in a meta-comment above.\n\nIn summary, while the data tradeoff we show is indeed expected *qualitatively*, we are interested in a *quantitative* assessment: how much more private data is needed for private end-to-end deep learning to work?\nSimilarly, our experiments with transfer learning aim to rectify confusion in the literature about the quantitative limitations of DP in this setting, which seem largely due to prior work only evaluating private transfer learning with very weak public source models.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1641/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1641/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentially Private Learning Needs Better Features (or Much More Data)", "authorids": ["~Florian_Tramer1", "~Dan_Boneh1"], "authors": ["Florian Tramer", "Dan Boneh"], "keywords": ["Differential Privacy", "Privacy", "Deep Learning"], "abstract": "We demonstrate that differentially private machine learning has not yet reached its ''AlexNet moment'' on many canonical vision tasks: linear models trained on handcrafted features significantly outperform end-to-end deep neural networks for moderate privacy budgets.\nTo exceed the performance of handcrafted features, we show that private learning requires either much more private data, or access to features learned on public data from a similar domain.\nOur work introduces simple yet strong baselines for differentially private learning that can inform the evaluation of future progress in this area.", "one-sentence_summary": "Linear models with handcrafted features outperform end-to-end CNNs for differentially private learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tramer|differentially_private_learning_needs_better_features_or_much_more_data", "supplementary_material": "", "pdf": "/pdf/63107901e325896b18874aad193314befc47c7ae.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ntramer2021differentially,\ntitle={Differentially Private Learning Needs Better Features (or Much More Data)},\nauthor={Florian Tramer and Dan Boneh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=YTWGvpFOQD-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "YTWGvpFOQD-", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1641/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1641/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1641/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1641/Authors|ICLR.cc/2021/Conference/Paper1641/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1641/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857402, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1641/-/Official_Comment"}}}, {"id": "9mVDmIEJVu8", "original": null, "number": 3, "cdate": 1605640364179, "ddate": null, "tcdate": 1605640364179, "tmdate": 1605641453285, "tddate": null, "forum": "YTWGvpFOQD-", "replyto": "qiypVXHx8FG", "invitation": "ICLR.cc/2021/Conference/Paper1641/-/Official_Comment", "content": {"title": "Response to Reviewer 4", "comment": "We thank the reviewer for their insightful comments.\n\n**1)    Q:** *A possible criticism is that in principle the \"hand crafted\" features may have been built based on empirical work on MNIST and CIFAR-10*\n\n**A:** The design of ScatterNet may indeed have been partially influenced by existing vision datasets. However, the core scattering transform originates from a purely theoretical work of Mallat. The architecture proposed by Oyallon and Mallat was indeed originally evaluated on CIFAR-10, but not on MNIST or Fashion-MNIST. The latter two datasets are qualitatively very different from CIFAR-10, so this speaks to the generality of the approach. \nNote that the Fashion-MNIST dataset (2017) was indeed proposed after the ScatterNet paper (2014).\nWe agree that the choice of CNN architectures might lead to some privacy leakage, but this only strengthens our main result that linear models with handcrafted features outperform deep neural nets for moderate privacy budgets.\n\n\n**2)    Q:** *It is also argued that having much more data similarly improves the trade-off, but this is unsurprising and, it seems, has been observed before by McMahan et al.*\n\n**A:** As many reviewers raised a similar concern about expectedness of our results when training with more data, we provide a detailed response to these concerns in a meta-comment above.\n\nIn summary, while the tradeoff we show is indeed expected *qualitatively*, we are interested in a *quantitative* assessment: how much more private data is needed for private end-to-end deep learning to work?  \n\n\n**3)    Q:** *Two final comments:*\n\n**A:**\n- Yes, thank you for pointing out this misnomer about \u201cunlearning\u201d \n- The label-private setting is indeed very interesting to consider. We are unaware of any work in this direction. The standard DP-SGD algorithm does not seem well suited to exploit such a weaker privacy model. Indeed, DP-SGD provides privacy at the level of gradients, which depend on both the input and label.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1641/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1641/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentially Private Learning Needs Better Features (or Much More Data)", "authorids": ["~Florian_Tramer1", "~Dan_Boneh1"], "authors": ["Florian Tramer", "Dan Boneh"], "keywords": ["Differential Privacy", "Privacy", "Deep Learning"], "abstract": "We demonstrate that differentially private machine learning has not yet reached its ''AlexNet moment'' on many canonical vision tasks: linear models trained on handcrafted features significantly outperform end-to-end deep neural networks for moderate privacy budgets.\nTo exceed the performance of handcrafted features, we show that private learning requires either much more private data, or access to features learned on public data from a similar domain.\nOur work introduces simple yet strong baselines for differentially private learning that can inform the evaluation of future progress in this area.", "one-sentence_summary": "Linear models with handcrafted features outperform end-to-end CNNs for differentially private learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tramer|differentially_private_learning_needs_better_features_or_much_more_data", "supplementary_material": "", "pdf": "/pdf/63107901e325896b18874aad193314befc47c7ae.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ntramer2021differentially,\ntitle={Differentially Private Learning Needs Better Features (or Much More Data)},\nauthor={Florian Tramer and Dan Boneh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=YTWGvpFOQD-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "YTWGvpFOQD-", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1641/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1641/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1641/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1641/Authors|ICLR.cc/2021/Conference/Paper1641/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1641/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857402, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1641/-/Official_Comment"}}}, {"id": "ZYyQCdSCMT", "original": null, "number": 2, "cdate": 1603843095554, "ddate": null, "tcdate": 1603843095554, "tmdate": 1605024393956, "tddate": null, "forum": "YTWGvpFOQD-", "replyto": "YTWGvpFOQD-", "invitation": "ICLR.cc/2021/Conference/Paper1641/-/Official_Review", "content": {"title": "Nice experiments", "review": "The paper shows that linear model on top of ScatterNet can outperform CNN for DPSGD training on a few generic image classification tasks. It analyzed the results, provides hypotheses to explain it, and concludes that more data / better feature is needed for DPSGD training.\n\nPeople have been searching for good models for DPSGD training, so it is nice to see a new baseline (ScatterNet + linear model) that is simple but performs better. This can be pretty valuable for researchers and practitioners in the field.\nThe paper also did some quite interesting experiments to explain the advantage of ScatterNet + linear model and to suggest directions to improve DPSGD training. The experiments and discussions on the learning rate are quite interesting and inspiring to me. However, I feel like the results for having more data / transfer learning is not so surprising, though the experiments with models different from previous work are valuable.\n\nMore detailed comments:\n- In Sec 4 \"smaller models are not easier to train privately\", you mentioned that the CNN is smaller than the linear model, so dimensionality is not an explanation for ScatterNet + linear's better performance. But I guess convex model (or maybe shallow model) might have some fundamental difference from nonconvex (or maybe deeper model). Maybe you could try something deeper than linear but shallower than the CNN to see if there is a sweet spot in between.\n- In Sec 4 \"Models with handcrafted features converge faster without privacy\", I guess the results can be explained by the fact that simpler model (linear) has a lower capacity than more complicated model (CNN) so requires less training time even with lower learning rate. So maybe again it would worth trying something in between linear and CNN.\n- In Sec 5.2, you showed results that are much better than previous results with transfer learning. It seems like the main difference is the model architecture. Is that the case? Do you have any comment on that?\n\nThe presentation is clear in general. I would love to see more details about ScatterNet as that is the important component of the proposed method.\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1641/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1641/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentially Private Learning Needs Better Features (or Much More Data)", "authorids": ["~Florian_Tramer1", "~Dan_Boneh1"], "authors": ["Florian Tramer", "Dan Boneh"], "keywords": ["Differential Privacy", "Privacy", "Deep Learning"], "abstract": "We demonstrate that differentially private machine learning has not yet reached its ''AlexNet moment'' on many canonical vision tasks: linear models trained on handcrafted features significantly outperform end-to-end deep neural networks for moderate privacy budgets.\nTo exceed the performance of handcrafted features, we show that private learning requires either much more private data, or access to features learned on public data from a similar domain.\nOur work introduces simple yet strong baselines for differentially private learning that can inform the evaluation of future progress in this area.", "one-sentence_summary": "Linear models with handcrafted features outperform end-to-end CNNs for differentially private learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tramer|differentially_private_learning_needs_better_features_or_much_more_data", "supplementary_material": "", "pdf": "/pdf/63107901e325896b18874aad193314befc47c7ae.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ntramer2021differentially,\ntitle={Differentially Private Learning Needs Better Features (or Much More Data)},\nauthor={Florian Tramer and Dan Boneh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=YTWGvpFOQD-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "YTWGvpFOQD-", "replyto": "YTWGvpFOQD-", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1641/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538114042, "tmdate": 1606915781642, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1641/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1641/-/Official_Review"}}}, {"id": "uSXag5Z679O", "original": null, "number": 3, "cdate": 1603915277468, "ddate": null, "tcdate": 1603915277468, "tmdate": 1605024393894, "tddate": null, "forum": "YTWGvpFOQD-", "replyto": "YTWGvpFOQD-", "invitation": "ICLR.cc/2021/Conference/Paper1641/-/Official_Review", "content": {"title": "New baselines for differentially private vision tasks using handcrafted feature extractors", "review": "This article is about a topical issue: performance degradation of of deep learning models trained with differential privacy (DP). Clipping of the gradients and addition of the noise, required to obtain DP guarantees, blur the models such that for moderate privacy guarantees (eps~7.0) CIFAR-10 test accuracy baseline is currently ~66% (Papernot et al., 2020b).\n\nLower bounds for this degradation have been shown theoretically (e.g. Bassily et al., 2014), and there has recently been also work on circumventing this issue, see e.g.\n\nKairouz, P., Ribero, M., Rush, K. and Thakurta, A., 2020. Dimension Independence in Unconstrained Private ERM via Adaptive Preconditioning. arXiv preprint arXiv:2008.06570,\nYingxue Zhou, Zhiwei Steven Wu, and Arindam Banerjee. Bypassing the ambient dimension: Private sgd with gradient subspace identification. arXiv preprint arXiv:2007.03813, 2020\n(these references are not included in the paper). \n\nAlthough this paper does not introduce fundamentally anything new for DP learning (DP-SGD + R\u00e9nyi DP accountant for obtaining eps,delta-guarantees are used), it does clearly beat the state-of-the-art for small epsilon values (eps up to 3.0) for MNIST, Fashion-MNIST and CIFAR-10. This is obtained by using so called Scattering Networks (Oyallon and Mallat, 2015), which have the property of converging very fast without privacy. This phenomenon is transferred to DP learning and thus high accuracies for shorter DP-SGD runs (i.e. smaller epsilons) are obtained. \n\nAs expected, these 'handcrafted data-independent feature extractors' of Scatter Networks cannot beat CNN+DP-SGD when more private data is available, or when features can be extracted from public image data.\n\nAll in all, although I think the gist of the paper is simply combining these handcrafted feature extractors (ScatterNets) and DP-SGD, it does improve the baseline for DP CIFAR-10 for small / moderate eps-values (up to 3.0) and does provide new ideas / questions on how to improve DP learning (e.g. by accelerated convergence) also outside of image domain (handcrafted feature extraction for non-vision tasks).\n\nThe paper is very well written. A tiny remark:\nYou write \"Gaussian noise of variance sigma^2 C^2 is added to the mean gradient.\"\nNotice that sigma^2 C^2 - noise is added to the summed gradients, and sigma^2 C^2 / B^2 to the mean.\n\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1641/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1641/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentially Private Learning Needs Better Features (or Much More Data)", "authorids": ["~Florian_Tramer1", "~Dan_Boneh1"], "authors": ["Florian Tramer", "Dan Boneh"], "keywords": ["Differential Privacy", "Privacy", "Deep Learning"], "abstract": "We demonstrate that differentially private machine learning has not yet reached its ''AlexNet moment'' on many canonical vision tasks: linear models trained on handcrafted features significantly outperform end-to-end deep neural networks for moderate privacy budgets.\nTo exceed the performance of handcrafted features, we show that private learning requires either much more private data, or access to features learned on public data from a similar domain.\nOur work introduces simple yet strong baselines for differentially private learning that can inform the evaluation of future progress in this area.", "one-sentence_summary": "Linear models with handcrafted features outperform end-to-end CNNs for differentially private learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tramer|differentially_private_learning_needs_better_features_or_much_more_data", "supplementary_material": "", "pdf": "/pdf/63107901e325896b18874aad193314befc47c7ae.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ntramer2021differentially,\ntitle={Differentially Private Learning Needs Better Features (or Much More Data)},\nauthor={Florian Tramer and Dan Boneh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=YTWGvpFOQD-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "YTWGvpFOQD-", "replyto": "YTWGvpFOQD-", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1641/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538114042, "tmdate": 1606915781642, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1641/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1641/-/Official_Review"}}}, {"id": "qiypVXHx8FG", "original": null, "number": 4, "cdate": 1603990535104, "ddate": null, "tcdate": 1603990535104, "tmdate": 1605024393832, "tddate": null, "forum": "YTWGvpFOQD-", "replyto": "YTWGvpFOQD-", "invitation": "ICLR.cc/2021/Conference/Paper1641/-/Official_Review", "content": {"title": "Interesting experimental study of improved, differentially private image classification", "review": "The paper considers ways of improving private versions of SGD in the context of image classification. The main finding is that providing \"hand crafted\" features can significantly improve the privacy/accuracy trade-off. In some cases, even a linear model built on top of such features (like those produced by ScatterNet), can improve over differentially private SGD. A plausible explanation for this phenomenon is that extra features can reduce the number of iterations required in SGD, resulting in better privacy and/or less noise. (It is also argued that having much more data similarly improves the trade-off, but this is unsurprising and, it seems, has been observed before by McMahan et al.)\n\nThe paper is quite well-written, and I found it easy to follow even though this is not my area of expertise. I also like that it presents a number of possible directions for further improving private SGD, including transfer learning from related, public data sets, and second-order optimization.\n\nA possible criticism is that in principle the \"hand crafted\" features may have been built based on empirical work on MNIST and CIFAR-10, and the same goes for the architecture choices, so in theory there could be some privacy leakage from these choices. It would have been more impressive to demonstrate effectiveness of a newer data set, not known when ScatterNet and the used CNN architectures were proposed.\n\nTwo final comments:\n- \"Unlearned\" usually means that you have (deliberately) forgotten something, so it is not the same as \"not learned\".\n- It would be interesting to consider the setting where just the image *label* is private. Has DP SGD been considered in that setting?", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper1641/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1641/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentially Private Learning Needs Better Features (or Much More Data)", "authorids": ["~Florian_Tramer1", "~Dan_Boneh1"], "authors": ["Florian Tramer", "Dan Boneh"], "keywords": ["Differential Privacy", "Privacy", "Deep Learning"], "abstract": "We demonstrate that differentially private machine learning has not yet reached its ''AlexNet moment'' on many canonical vision tasks: linear models trained on handcrafted features significantly outperform end-to-end deep neural networks for moderate privacy budgets.\nTo exceed the performance of handcrafted features, we show that private learning requires either much more private data, or access to features learned on public data from a similar domain.\nOur work introduces simple yet strong baselines for differentially private learning that can inform the evaluation of future progress in this area.", "one-sentence_summary": "Linear models with handcrafted features outperform end-to-end CNNs for differentially private learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tramer|differentially_private_learning_needs_better_features_or_much_more_data", "supplementary_material": "", "pdf": "/pdf/63107901e325896b18874aad193314befc47c7ae.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ntramer2021differentially,\ntitle={Differentially Private Learning Needs Better Features (or Much More Data)},\nauthor={Florian Tramer and Dan Boneh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=YTWGvpFOQD-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "YTWGvpFOQD-", "replyto": "YTWGvpFOQD-", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1641/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538114042, "tmdate": 1606915781642, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1641/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1641/-/Official_Review"}}}], "count": 17}