{"notes": [{"id": "99M-4QlinPr", "original": "qr0nQHfBuo7", "number": 2219, "cdate": 1601308244449, "ddate": null, "tcdate": 1601308244449, "tmdate": 1614985638313, "tddate": null, "forum": "99M-4QlinPr", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Efficient Competitive Self-Play Policy Optimization", "authorids": ["~Yuanyi_Zhong1", "~Yuan_Zhou1", "~Jian_Peng1"], "authors": ["Yuanyi Zhong", "Yuan Zhou", "Jian Peng"], "keywords": ["self-play", "policy optimization", "two-player zero-sum game", "multiagent"], "abstract": "Reinforcement learning from self-play has recently reported many successes. Self-play, where the agents compete with themselves, is often used to generate training data for iterative policy improvement. In previous work, heuristic rules are designed to choose an opponent for the current learner. Typical rules include choosing the latest agent, the best agent, or a random historical agent. However, these rules may be inefficient in practice and sometimes do not guarantee convergence even in the simplest matrix games. This paper proposes a new algorithmic framework for competitive self-play reinforcement learning in two-player zero-sum games. We recognize the fact that the Nash equilibrium coincides with the saddle point of the stochastic payoff function, which motivates us to borrow ideas from classical saddle point optimization literature. Our method simultaneously trains several agents and intelligently takes each other as opponents based on a simple adversarial rule derived from a principled perturbation-based saddle optimization method. We prove theoretically that our algorithm converges to an approximate equilibrium with high probability in convex-concave games under standard assumptions. Beyond the theory, we further show the empirical superiority of our method over baseline methods relying on the aforementioned opponent-selection heuristics in matrix games, grid-world soccer, Gomoku, and simulated robot sumo, with neural net policy function approximators.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhong|efficient_competitive_selfplay_policy_optimization", "one-sentence_summary": "We present a population-based self-play policy optimization algorithm with a principled opponent-selection rule.", "supplementary_material": "/attachment/00dbe72abaed677675e23547c2d73eba50334181.zip", "pdf": "/pdf/4bc505dd96e2a3bc262fae922603d4b3fdfc7743.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=gQhNkQiDb", "_bibtex": "@misc{\nzhong2021efficient,\ntitle={Efficient Competitive Self-Play Policy Optimization},\nauthor={Yuanyi Zhong and Yuan Zhou and Jian Peng},\nyear={2021},\nurl={https://openreview.net/forum?id=99M-4QlinPr}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "WWSpvlD1C2", "original": null, "number": 1, "cdate": 1610040523240, "ddate": null, "tcdate": 1610040523240, "tmdate": 1610474132180, "tddate": null, "forum": "99M-4QlinPr", "replyto": "99M-4QlinPr", "invitation": "ICLR.cc/2021/Conference/Paper2219/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper investigate an interesting problem of multi-agent RL with self-play. We agree with the reviewers that the paper requires more work before it can be presented at a top conference.  We would  encourage the authors to use the reviewers' feedback to improve the paper and resubmit to one of the upcoming conferences.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Competitive Self-Play Policy Optimization", "authorids": ["~Yuanyi_Zhong1", "~Yuan_Zhou1", "~Jian_Peng1"], "authors": ["Yuanyi Zhong", "Yuan Zhou", "Jian Peng"], "keywords": ["self-play", "policy optimization", "two-player zero-sum game", "multiagent"], "abstract": "Reinforcement learning from self-play has recently reported many successes. Self-play, where the agents compete with themselves, is often used to generate training data for iterative policy improvement. In previous work, heuristic rules are designed to choose an opponent for the current learner. Typical rules include choosing the latest agent, the best agent, or a random historical agent. However, these rules may be inefficient in practice and sometimes do not guarantee convergence even in the simplest matrix games. This paper proposes a new algorithmic framework for competitive self-play reinforcement learning in two-player zero-sum games. We recognize the fact that the Nash equilibrium coincides with the saddle point of the stochastic payoff function, which motivates us to borrow ideas from classical saddle point optimization literature. Our method simultaneously trains several agents and intelligently takes each other as opponents based on a simple adversarial rule derived from a principled perturbation-based saddle optimization method. We prove theoretically that our algorithm converges to an approximate equilibrium with high probability in convex-concave games under standard assumptions. Beyond the theory, we further show the empirical superiority of our method over baseline methods relying on the aforementioned opponent-selection heuristics in matrix games, grid-world soccer, Gomoku, and simulated robot sumo, with neural net policy function approximators.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhong|efficient_competitive_selfplay_policy_optimization", "one-sentence_summary": "We present a population-based self-play policy optimization algorithm with a principled opponent-selection rule.", "supplementary_material": "/attachment/00dbe72abaed677675e23547c2d73eba50334181.zip", "pdf": "/pdf/4bc505dd96e2a3bc262fae922603d4b3fdfc7743.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=gQhNkQiDb", "_bibtex": "@misc{\nzhong2021efficient,\ntitle={Efficient Competitive Self-Play Policy Optimization},\nauthor={Yuanyi Zhong and Yuan Zhou and Jian Peng},\nyear={2021},\nurl={https://openreview.net/forum?id=99M-4QlinPr}\n}"}, "tags": [], "invitation": {"reply": {"forum": "99M-4QlinPr", "replyto": "99M-4QlinPr", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040523227, "tmdate": 1610474132164, "id": "ICLR.cc/2021/Conference/Paper2219/-/Decision"}}}, {"id": "SWZ6QotRgkq", "original": null, "number": 1, "cdate": 1603491562256, "ddate": null, "tcdate": 1603491562256, "tmdate": 1605024261094, "tddate": null, "forum": "99M-4QlinPr", "replyto": "99M-4QlinPr", "invitation": "ICLR.cc/2021/Conference/Paper2219/-/Official_Review", "content": {"title": "A well-executed justification and evaluation of an intuitive opponent-selection rule", "review": "In this paper, the authors present a rule for selecting opponents for self-play training in zero-sum games: Train each agent i against the agent j that is \"hardest\" for i (in the sense that i's payoff is least among all candidate opponents when playing against j).  This principle is justified by appeal to \"perturbation-based subgradient methods\" in saddle-point optimization.  Last-iterate (as opposed to average-iterate) convergence to equilibrium is demonstrated both analytically and experimentally.\n\nThe evaluation is very thorough.  The empirical baselines are appropriately chosen.  The principled grounding of the selection procedure in saddle-point optimization was especially interesting, as I have not encountered that corner of optimization before.  The exposition is clearly written and well organized.\n\nI have somewhat mixed feelings about this paper.  On the one hand, it's hard to believe that \"train against the toughest opponent\" as a selection rule has never been tried before.  On the other hand, the argument for the method is extremely convincing.  Overall I am in favor of acceptance. \n\nIt would be valuable for the authors to explicitly highlight the novel aspects of their contribution compared to [Balduzzi et al 2019], \"Open-ended Learning in Symmetric Zero-sum Games\", which rejects the \"play against toughest (for me) opponent\" approach explicitly because it collapses to equilibrium play.  \n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2219/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2219/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Competitive Self-Play Policy Optimization", "authorids": ["~Yuanyi_Zhong1", "~Yuan_Zhou1", "~Jian_Peng1"], "authors": ["Yuanyi Zhong", "Yuan Zhou", "Jian Peng"], "keywords": ["self-play", "policy optimization", "two-player zero-sum game", "multiagent"], "abstract": "Reinforcement learning from self-play has recently reported many successes. Self-play, where the agents compete with themselves, is often used to generate training data for iterative policy improvement. In previous work, heuristic rules are designed to choose an opponent for the current learner. Typical rules include choosing the latest agent, the best agent, or a random historical agent. However, these rules may be inefficient in practice and sometimes do not guarantee convergence even in the simplest matrix games. This paper proposes a new algorithmic framework for competitive self-play reinforcement learning in two-player zero-sum games. We recognize the fact that the Nash equilibrium coincides with the saddle point of the stochastic payoff function, which motivates us to borrow ideas from classical saddle point optimization literature. Our method simultaneously trains several agents and intelligently takes each other as opponents based on a simple adversarial rule derived from a principled perturbation-based saddle optimization method. We prove theoretically that our algorithm converges to an approximate equilibrium with high probability in convex-concave games under standard assumptions. Beyond the theory, we further show the empirical superiority of our method over baseline methods relying on the aforementioned opponent-selection heuristics in matrix games, grid-world soccer, Gomoku, and simulated robot sumo, with neural net policy function approximators.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhong|efficient_competitive_selfplay_policy_optimization", "one-sentence_summary": "We present a population-based self-play policy optimization algorithm with a principled opponent-selection rule.", "supplementary_material": "/attachment/00dbe72abaed677675e23547c2d73eba50334181.zip", "pdf": "/pdf/4bc505dd96e2a3bc262fae922603d4b3fdfc7743.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=gQhNkQiDb", "_bibtex": "@misc{\nzhong2021efficient,\ntitle={Efficient Competitive Self-Play Policy Optimization},\nauthor={Yuanyi Zhong and Yuan Zhou and Jian Peng},\nyear={2021},\nurl={https://openreview.net/forum?id=99M-4QlinPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "99M-4QlinPr", "replyto": "99M-4QlinPr", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2219/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538101313, "tmdate": 1606915806624, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2219/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2219/-/Official_Review"}}}, {"id": "1oETMjfNMvr", "original": null, "number": 2, "cdate": 1603812702496, "ddate": null, "tcdate": 1603812702496, "tmdate": 1605024261019, "tddate": null, "forum": "99M-4QlinPr", "replyto": "99M-4QlinPr", "invitation": "ICLR.cc/2021/Conference/Paper2219/-/Official_Review", "content": {"title": "I acknowledge the paper and its results to be of interest for self-play in RL. However, in my opinion, the paper fails to properly account for the weakness of its approach. (I believe the paper to become stronger, if the efficiency of the approach was discussed critically.)", "review": "Summary:\nThe paper \u201cEfficient Competitive Self-Play Policy Optimization\u201d introduces a new self-play scheme for solving zero-sum two-player games. It is suggested to train a population of N agents in parallel, where each agent is matched against the comparatively strongest opponent in the next round of training. As baselines, the paper considers self-play against the best, the latest and random snapshots from the training history of only a single agent.\n \nStrong:\nThe submission is well motivated and seems to state relevant related work.\nThe proposed algorithm seems straightforward and is presented in a clear and understandable way.\nThe proposed scheme solves for the Nash equilibrium for non-transitive matrix games with MC targets.\nThis is shown empirically and amended with a convergence proof.\nThe paper includes deep RL experiments for which the proposed algorithm results in better sample efficiency per agent.\n \nWeak:\nThe last positive point also highlights the main weakness of the paper: I am not convinced, that the efficiency of RL self-play is best measured per agent. In the appendix, it is rightfully argued, that part of the training could be parallelized. However, the conclusion that the baseline experiments thus could be repeated N times, seems to ignore that the additional compute could be used to train stronger opponents also for the baselines. The experiments don\u2019t account for this. \nFurther, the algorithm requires the evaluation of each agent-match-up combination for each round to choose the next opponent and thus involves policy roll-outs that are quadratic in the population size. This seems very expensive especially for larger populations. To highlight the efficiency of the approach in the title of the paper thus might be a misnomer.\n \nRecommendation:\nIn its current form, I thus vote for a weak reject.\n \nSupport:\nI acknowledge the paper and its results to be of interest for self-play in RL. However, in my opinion, the paper fails to properly account for the weakness of its approach. (I believe the paper to become stronger, if the efficiency of the approach was discussed critically.)\n \nRating: 5 out of 10\n \nConfidence: 3 out of 5\n \nCoE: I don\u2019t see the paper in violation of the ICLR\u2019s Code of Ethics.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2219/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2219/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Competitive Self-Play Policy Optimization", "authorids": ["~Yuanyi_Zhong1", "~Yuan_Zhou1", "~Jian_Peng1"], "authors": ["Yuanyi Zhong", "Yuan Zhou", "Jian Peng"], "keywords": ["self-play", "policy optimization", "two-player zero-sum game", "multiagent"], "abstract": "Reinforcement learning from self-play has recently reported many successes. Self-play, where the agents compete with themselves, is often used to generate training data for iterative policy improvement. In previous work, heuristic rules are designed to choose an opponent for the current learner. Typical rules include choosing the latest agent, the best agent, or a random historical agent. However, these rules may be inefficient in practice and sometimes do not guarantee convergence even in the simplest matrix games. This paper proposes a new algorithmic framework for competitive self-play reinforcement learning in two-player zero-sum games. We recognize the fact that the Nash equilibrium coincides with the saddle point of the stochastic payoff function, which motivates us to borrow ideas from classical saddle point optimization literature. Our method simultaneously trains several agents and intelligently takes each other as opponents based on a simple adversarial rule derived from a principled perturbation-based saddle optimization method. We prove theoretically that our algorithm converges to an approximate equilibrium with high probability in convex-concave games under standard assumptions. Beyond the theory, we further show the empirical superiority of our method over baseline methods relying on the aforementioned opponent-selection heuristics in matrix games, grid-world soccer, Gomoku, and simulated robot sumo, with neural net policy function approximators.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhong|efficient_competitive_selfplay_policy_optimization", "one-sentence_summary": "We present a population-based self-play policy optimization algorithm with a principled opponent-selection rule.", "supplementary_material": "/attachment/00dbe72abaed677675e23547c2d73eba50334181.zip", "pdf": "/pdf/4bc505dd96e2a3bc262fae922603d4b3fdfc7743.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=gQhNkQiDb", "_bibtex": "@misc{\nzhong2021efficient,\ntitle={Efficient Competitive Self-Play Policy Optimization},\nauthor={Yuanyi Zhong and Yuan Zhou and Jian Peng},\nyear={2021},\nurl={https://openreview.net/forum?id=99M-4QlinPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "99M-4QlinPr", "replyto": "99M-4QlinPr", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2219/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538101313, "tmdate": 1606915806624, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2219/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2219/-/Official_Review"}}}, {"id": "NsA2U2MdX1c", "original": null, "number": 3, "cdate": 1603909813354, "ddate": null, "tcdate": 1603909813354, "tmdate": 1605024260941, "tddate": null, "forum": "99M-4QlinPr", "replyto": "99M-4QlinPr", "invitation": "ICLR.cc/2021/Conference/Paper2219/-/Official_Review", "content": {"title": "I highly encourage the authors to improve the clarity of their paper, and provide comparison with existing methods.", "review": "In this paper, the authors study the problem of optimization in two-player competitive zero-sum sequential games where the objective function is convex-concave in players' strategies. They relax the convex-concave assumption for the empirical study.\n\nThis problem is an important and challenging problem in the field of reinforcement learning.\n\nThe authors propose a policy gradient-based method and argue that it converges to the game's Nash equilibrium. They also conduct a series of empirical studies to show the significance of their method. \n\nWhile this is a challenging problem, my judgment is that the paper is not ready for publication yet.\n\n1) The presentation of theorem 1:\nThe authors state that if \"a sequence of (x^k,y^k)->(xhat,yhat) \\wedge f(x^k,y^k) -f(u^k,y^k)-> 0 implies (xhat,yhat) is a saddle point\". \nI am not sure I follow this sentence. \n1-a) First, please clear up the notation when you use \\wedge. For example, say ((x^k,y^k)->(xhat,yhat)) \\wedge (f(x^k,y^k) -f(u^k,y^k)-> 0) since \\wedge someitmes also means minimum. \n1-b) In thm1, do the authors mean \nif \"a\" sequence (x^k,y^k) for k>0, satisfies the following conditions ((x^k,y^k)->(xhat,yhat)) and (f(x^k,y^k) -f(u^k,y^k)-> 0), then (xhat,yhat) is a saddle point?\nIf that is the case, please consider rewording the thm1. \nAlso, if that is the case, for what \\nu^k and u^k?\n\n1-c) Algorithm 1 seems to produce a sequence of sets rather than (x^k,y^k). Please clarify it in your theorem.\n\n1-d) If \\nu^k and u^k are some quantities that are produced by your algorithm, then the theorem seems not to be well-stated. Because I am not sure what sequence of \\nu^k and u^k the algorithm produces in order for me to see what space of X and Y you are dealing with. \n\n1-e) I see the same lack of clarity in the proof of thm1. \n\n2) In assumption 1, you state that the sets C_x^k and C_y^k are compact subsets of X and Y. However, these are what your algorithm produces. So probably, you may want to design your algorithm such it guarantees such a requirement. \n\n3) E_k seems to be not defined. \n\nAs the authors stated, the main contribution of this paper is not the theory. And the theoretical study in this paper is to motivate their algorithm.\n4) Empirical study. \nI found the empirical study in this paper, somewhat not convincing. \nIf the contribution is empirical, it would be useful to compare it with existing methods.\nI realized that the authors did not mention the work of \n\"Learning with Opponent-Learning Awareness\" which proposes LOLA. Also, the authors mention the work of \"Competitive policy optimization\". \nI strongly encourage the authors to make a clear empirical study and compare their method with prior works.\n\nSince the proposed algorithm seems like a well-known traditional per-step best response method, such comparisons are helpful for future submission.\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2219/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2219/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Competitive Self-Play Policy Optimization", "authorids": ["~Yuanyi_Zhong1", "~Yuan_Zhou1", "~Jian_Peng1"], "authors": ["Yuanyi Zhong", "Yuan Zhou", "Jian Peng"], "keywords": ["self-play", "policy optimization", "two-player zero-sum game", "multiagent"], "abstract": "Reinforcement learning from self-play has recently reported many successes. Self-play, where the agents compete with themselves, is often used to generate training data for iterative policy improvement. In previous work, heuristic rules are designed to choose an opponent for the current learner. Typical rules include choosing the latest agent, the best agent, or a random historical agent. However, these rules may be inefficient in practice and sometimes do not guarantee convergence even in the simplest matrix games. This paper proposes a new algorithmic framework for competitive self-play reinforcement learning in two-player zero-sum games. We recognize the fact that the Nash equilibrium coincides with the saddle point of the stochastic payoff function, which motivates us to borrow ideas from classical saddle point optimization literature. Our method simultaneously trains several agents and intelligently takes each other as opponents based on a simple adversarial rule derived from a principled perturbation-based saddle optimization method. We prove theoretically that our algorithm converges to an approximate equilibrium with high probability in convex-concave games under standard assumptions. Beyond the theory, we further show the empirical superiority of our method over baseline methods relying on the aforementioned opponent-selection heuristics in matrix games, grid-world soccer, Gomoku, and simulated robot sumo, with neural net policy function approximators.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhong|efficient_competitive_selfplay_policy_optimization", "one-sentence_summary": "We present a population-based self-play policy optimization algorithm with a principled opponent-selection rule.", "supplementary_material": "/attachment/00dbe72abaed677675e23547c2d73eba50334181.zip", "pdf": "/pdf/4bc505dd96e2a3bc262fae922603d4b3fdfc7743.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=gQhNkQiDb", "_bibtex": "@misc{\nzhong2021efficient,\ntitle={Efficient Competitive Self-Play Policy Optimization},\nauthor={Yuanyi Zhong and Yuan Zhou and Jian Peng},\nyear={2021},\nurl={https://openreview.net/forum?id=99M-4QlinPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "99M-4QlinPr", "replyto": "99M-4QlinPr", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2219/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538101313, "tmdate": 1606915806624, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2219/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2219/-/Official_Review"}}}, {"id": "D8RuTeBT1H_", "original": null, "number": 4, "cdate": 1604200696678, "ddate": null, "tcdate": 1604200696678, "tmdate": 1605024260882, "tddate": null, "forum": "99M-4QlinPr", "replyto": "99M-4QlinPr", "invitation": "ICLR.cc/2021/Conference/Paper2219/-/Official_Review", "content": {"title": "Interesting paper", "review": "\n1. Summary:\n\nThis paper addressed an interesting topic on competitive self-play reinforcement learning on two-player zero-sum games. Typically, many self-play methods are only average-case optimal and self-play with latest agent fails to converge. I believe this is a big issue for many self-play because averaging past policies for each player during self-play iterations is very difficult especially for large-scale games.\nThe author tried to solve this problem by perturbation-based subgradient method because of its three advantages:\n(1) it only requires to know the subgradients rather than the game dynamics;\n(2) it's guaranteed to converge in its last iterate rather than the average iterate for convex-concave functions;\n(3) it's very simple to select the opponent in the self-play training.\nThe authors evaluate this method on some small games and the experimental settings/results are well.\nOverall, I like this paper. However, there are some issues or weakness in the current version, I tend to vote minor reject. \nNote, if the author can address the key issues well, i can increase my score.\n\n2. Some Concerns/weakness:\n\n(1) For convex-concave functions, the perturbation-based subgradient method is guaranteed to converge in its last iterate. Does its convergence hold for deep learning settings?\n\n(2) I'd like to see the performance on large-scale settings.\nI think the strength of this method is to solve large-scale game, because it's expensive to average policies for the past iterations for many self-play methods. The author only evaluates the method on small settings, it's not very convincing because when solving small games, self-play with average policies is not expensive.\n\n(3) The author missed some important related papers, such as neural counterfactual regret minimization [1, 2] and exploitability descent [3]. These methods also address the problem of average-case optimal. It will benefit readers if you can talk about these methods.\n\n(4) the baseline is so weak, it will be better to compare against counterfactual regret minimization method and Johannes Heinrich's deep fictitious self-play method.\n\n3. Questions:\n\n(1) For large-scale games, does it need large population size $n$?\n\n(2) Does it time-consuming to solve perturbed $v^{k}_i$ and $u^{k}_i$ for step 6 in Algorithm 1?\n\n(3) Why not use exploitability to evaluate different methods? It's a standard metric in imperfect information games. It will be good if you can evaluate the method on one of poker game, such as leduc.\n\n(4) \"Four colors correspond to the 4 agents in the population with 4 initial points\" in Figure 1, do you mean there are the population size is 4 and there are 4 different initial policies?\n\n(5) what's exact gradient in \"On the other hand, our method enjoys approximate last-iterate convergence with both exact and policy gradients.\"\n\n(6) page 4, Algorithm 1, $N^0$ iterations, $N^0$ inner updates.  $N^0$ is a typo?\n\n\nSome key references:\n\n[1] Brown, N., Lerer, A., Gross, S. and Sandholm, T., 2019, May. Deep counterfactual regret minimization. In International conference on machine learning (pp. 793-802).\n\n[2] Li, H., Hu, K., Zhang, S., Qi, Y. and Song, L., 2018. Double neural counterfactual regret minimization. ICLR, 2020. https://openreview.net/pdf?id=ByedzkrKvH\n\n[3] Lockhart, E., Lanctot, M., P\u00e9rolat, J., Lespiau, J.B., Morrill, D., Timbers, F. and Tuyls, K., 2019. Computing approximate equilibria in sequential adversarial games by exploitability descent. arXiv preprint arXiv:1903.05614.\n\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2219/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2219/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Competitive Self-Play Policy Optimization", "authorids": ["~Yuanyi_Zhong1", "~Yuan_Zhou1", "~Jian_Peng1"], "authors": ["Yuanyi Zhong", "Yuan Zhou", "Jian Peng"], "keywords": ["self-play", "policy optimization", "two-player zero-sum game", "multiagent"], "abstract": "Reinforcement learning from self-play has recently reported many successes. Self-play, where the agents compete with themselves, is often used to generate training data for iterative policy improvement. In previous work, heuristic rules are designed to choose an opponent for the current learner. Typical rules include choosing the latest agent, the best agent, or a random historical agent. However, these rules may be inefficient in practice and sometimes do not guarantee convergence even in the simplest matrix games. This paper proposes a new algorithmic framework for competitive self-play reinforcement learning in two-player zero-sum games. We recognize the fact that the Nash equilibrium coincides with the saddle point of the stochastic payoff function, which motivates us to borrow ideas from classical saddle point optimization literature. Our method simultaneously trains several agents and intelligently takes each other as opponents based on a simple adversarial rule derived from a principled perturbation-based saddle optimization method. We prove theoretically that our algorithm converges to an approximate equilibrium with high probability in convex-concave games under standard assumptions. Beyond the theory, we further show the empirical superiority of our method over baseline methods relying on the aforementioned opponent-selection heuristics in matrix games, grid-world soccer, Gomoku, and simulated robot sumo, with neural net policy function approximators.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhong|efficient_competitive_selfplay_policy_optimization", "one-sentence_summary": "We present a population-based self-play policy optimization algorithm with a principled opponent-selection rule.", "supplementary_material": "/attachment/00dbe72abaed677675e23547c2d73eba50334181.zip", "pdf": "/pdf/4bc505dd96e2a3bc262fae922603d4b3fdfc7743.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=gQhNkQiDb", "_bibtex": "@misc{\nzhong2021efficient,\ntitle={Efficient Competitive Self-Play Policy Optimization},\nauthor={Yuanyi Zhong and Yuan Zhou and Jian Peng},\nyear={2021},\nurl={https://openreview.net/forum?id=99M-4QlinPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "99M-4QlinPr", "replyto": "99M-4QlinPr", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2219/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538101313, "tmdate": 1606915806624, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2219/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2219/-/Official_Review"}}}], "count": 6}