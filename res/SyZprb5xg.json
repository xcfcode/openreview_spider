{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396409876, "tcdate": 1486396409876, "number": 1, "id": "SJfZ2GU_x", "invitation": "ICLR.cc/2017/conference/-/paper169/acceptance", "forum": "SyZprb5xg", "replyto": "SyZprb5xg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The paper makes a solid technical contribution in establishing a universal approximation theorem for the boolean hypercube. They characterize the class of boolean functions that can be efficiently approximated by a two-layer network.\n \n We like the idea of noise stability, and it could explain why in practice, perturbation techniques such as dropout are effective. Moreover, humans can identify images, despite corruptions, and hence, intuitively the concepts we aim to learn should be robust.\n \n However, the framework of the paper deviated from the networks and data structures that are the norm in practice. In practice, we rarely have boolean functions. And it is well known that boolean functions can behave quite differently from continuous functions. \n \n We recommend that the authors widen the scope of their work, and attempt to connect their findings to practical networks and functions. Moreover, we recommend that they do a more thorough literature survey. For instance, the idea of robust concepts has appeared before\n http://www.pnas.org/content/113/48/E7655.full\n And nice connections to elastic learning have been made in the above paper.\n \n We recommend a workshop presentation since this will enable more interaction with people in the area, and people who are working on practical networks.", "decision": "Invite to Workshop Track"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Robust Concepts and Small Neural Nets", "abstract": "The universal approximation theorem for neural networks says that any reasonable function is well-approximated by a two-layer neural network with sigmoid gates but it does not provide good bounds on the number of hidden-layer nodes or the weights. However, robust concepts often have small neural networks in practice. We show an efficient analog of the universal approximation theorem on the boolean hypercube in this context.\n\nWe prove that any noise-stable boolean function on n boolean-valued input variables can be well-approximated by a two-layer linear threshold circuit with a small number of hidden-layer nodes and small weights, that depend only on the noise-stability and approximation parameters, and are independent of n. We also give a polynomial time learning algorithm that outputs a small two-layer linear threshold circuit that approximates such a given function. We also show weaker generalizations of this to noise-stable polynomial threshold functions and noise-stable boolean functions in general.", "pdf": "/pdf/31c447ffbb60b8212b2e965e2db44d961cf1b7bb.pdf", "TL;DR": "an efficient analog of the universal approximation theorem for neural networks over the boolean hypercube", "paperhash": "deshpande|on_robust_concepts_and_small_neural_nets", "keywords": ["Theory"], "conflicts": ["microsoft.com", "cs.utexas.edu"], "authorids": ["amitdesh@microsoft.com", "sushrutk@cs.utexas.edu"], "authors": ["Amit Deshpande", "Sushrut Karmalkar"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396410377, "id": "ICLR.cc/2017/conference/-/paper169/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "SyZprb5xg", "replyto": "SyZprb5xg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396410377}}}, {"tddate": null, "tmdate": 1484407502336, "tcdate": 1481829101579, "number": 1, "id": "B18esDeVe", "invitation": "ICLR.cc/2017/conference/-/paper169/official/review", "forum": "SyZprb5xg", "replyto": "SyZprb5xg", "signatures": ["ICLR.cc/2017/conference/paper169/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper169/AnonReviewer1"], "content": {"title": "This paper provides an analog of the universal approximation theorem where the size of the network depends on a notion of noise-stability instead of the dimension.", "rating": "5: Marginally below acceptance threshold", "review": "The approximation capabilities of neural networks have been studied before for approximating different classes of functions. The goal of this paper is to provide an analog of the approximation theorem for the class of noise-stable functions. The class of functions that are noise-stable and their output does not significantly depend on an individual input seems an interesting class and therefore I find the problem definition interesting.  The paper is well-written and it is easy to follow the proofs and arguments. \n\nI have two major comments:\n\n1- Presentation: The way I understand this arguments is that the noise-stability measures the \"true\" dimensionality of the data based on the dependence of the function on different dimensions. Therefore, it is possible to restate and prove an analog to the approximation theorems based on \"true\" dimensionality of data. It is also unclear when the stability based bounds are tighter than dimension based bounds as both of them grow exponentially. I find these discussions interesting but unfortunately, the authors present the result as some bound that does not depend on the dimension and a constant (!??) that grows exponentially with (1/eps). This is not entirely the right picture because the epsilon in the stability could itself depend on the dimension. I believe in most problems (1/epsilon) grows with the dimension. \n\n2- Contribution: Even though the connection is new and interesting, the contribution of the paper is not significant enough. The presented results are direct applications of previous works and most of the lemmas in the paper are restating the known results. I believe more discussions and results need to be added to make this a complete work.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Robust Concepts and Small Neural Nets", "abstract": "The universal approximation theorem for neural networks says that any reasonable function is well-approximated by a two-layer neural network with sigmoid gates but it does not provide good bounds on the number of hidden-layer nodes or the weights. However, robust concepts often have small neural networks in practice. We show an efficient analog of the universal approximation theorem on the boolean hypercube in this context.\n\nWe prove that any noise-stable boolean function on n boolean-valued input variables can be well-approximated by a two-layer linear threshold circuit with a small number of hidden-layer nodes and small weights, that depend only on the noise-stability and approximation parameters, and are independent of n. We also give a polynomial time learning algorithm that outputs a small two-layer linear threshold circuit that approximates such a given function. We also show weaker generalizations of this to noise-stable polynomial threshold functions and noise-stable boolean functions in general.", "pdf": "/pdf/31c447ffbb60b8212b2e965e2db44d961cf1b7bb.pdf", "TL;DR": "an efficient analog of the universal approximation theorem for neural networks over the boolean hypercube", "paperhash": "deshpande|on_robust_concepts_and_small_neural_nets", "keywords": ["Theory"], "conflicts": ["microsoft.com", "cs.utexas.edu"], "authorids": ["amitdesh@microsoft.com", "sushrutk@cs.utexas.edu"], "authors": ["Amit Deshpande", "Sushrut Karmalkar"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512675716, "id": "ICLR.cc/2017/conference/-/paper169/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper169/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper169/AnonReviewer1", "ICLR.cc/2017/conference/paper169/AnonReviewer4", "ICLR.cc/2017/conference/paper169/AnonReviewer3"], "reply": {"forum": "SyZprb5xg", "replyto": "SyZprb5xg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper169/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper169/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512675716}}}, {"tddate": null, "tmdate": 1484371986230, "tcdate": 1484371986230, "number": 6, "id": "Hk5MdEvUx", "invitation": "ICLR.cc/2017/conference/-/paper169/public/comment", "forum": "SyZprb5xg", "replyto": "r1O7mnrVg", "signatures": ["~Amit_Deshpande1"], "readers": ["everyone"], "writers": ["~Amit_Deshpande1"], "content": {"title": "response to AnonReviewer4's comments on the limits of our work", "comment": "Thank you for your comments and suggestions. We have a detailed discussion in Subsection 2.1 on obstacles to extending our approach and ideas to a more general setting. The definition of robust concepts can be generalized as those having a small probability of label-flipping (from +ve to -ve or vice versa) when a small random gaussian perturbation is applied to a data point picked from an underlying distribution. However, proving similar results would require very different ideas than what works for the Boolean hypercube. We are interested in this question, and we do have a detailed discussion of the obstacles to improvement in Subsection 2.1."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Robust Concepts and Small Neural Nets", "abstract": "The universal approximation theorem for neural networks says that any reasonable function is well-approximated by a two-layer neural network with sigmoid gates but it does not provide good bounds on the number of hidden-layer nodes or the weights. However, robust concepts often have small neural networks in practice. We show an efficient analog of the universal approximation theorem on the boolean hypercube in this context.\n\nWe prove that any noise-stable boolean function on n boolean-valued input variables can be well-approximated by a two-layer linear threshold circuit with a small number of hidden-layer nodes and small weights, that depend only on the noise-stability and approximation parameters, and are independent of n. We also give a polynomial time learning algorithm that outputs a small two-layer linear threshold circuit that approximates such a given function. We also show weaker generalizations of this to noise-stable polynomial threshold functions and noise-stable boolean functions in general.", "pdf": "/pdf/31c447ffbb60b8212b2e965e2db44d961cf1b7bb.pdf", "TL;DR": "an efficient analog of the universal approximation theorem for neural networks over the boolean hypercube", "paperhash": "deshpande|on_robust_concepts_and_small_neural_nets", "keywords": ["Theory"], "conflicts": ["microsoft.com", "cs.utexas.edu"], "authorids": ["amitdesh@microsoft.com", "sushrutk@cs.utexas.edu"], "authors": ["Amit Deshpande", "Sushrut Karmalkar"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287702226, "id": "ICLR.cc/2017/conference/-/paper169/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SyZprb5xg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper169/reviewers", "ICLR.cc/2017/conference/paper169/areachairs"], "cdate": 1485287702226}}}, {"tddate": null, "tmdate": 1484371717214, "tcdate": 1484371717214, "number": 5, "id": "r1abwEvLx", "invitation": "ICLR.cc/2017/conference/-/paper169/public/comment", "forum": "SyZprb5xg", "replyto": "r1tGF0HEe", "signatures": ["~Amit_Deshpande1"], "readers": ["everyone"], "writers": ["~Amit_Deshpande1"], "content": {"title": "response to AnonReviewer3's comments", "comment": "Thank you for your comments and suggestions. We will add references such as Wenzel et al. on hyperplane arrangements and their comparison or discussion.\n\nWe do mention interesting classes of (\\epsilon, \\delta)-noise-stable functions where 1/\\epsilon and 1/\\delta are constants INDEPENDENT of the dimension, e.g., constant-margin halfspaces and submodular functions. Please contrast this with arbitrary halfspaces of the d-dimensional hypercube where margin can be as small as exp(-d \\log d) in the worst case. In this sense, noise-stability as the \"true\" dimension can be much smaller than the actual dimension. As you suggested, we will add a few examples of simple noise-stable functions to improve readability.\n\nThe confusion about the constants arises from Bourgain's junta theorem where the same \\delta appears as closeness of approximation as well as in the size of the junta (as 1/\\delta). We have not been able to decouple these two, and as far as we know, any such resolution would be of independent interest in the analysis of Boolean function. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Robust Concepts and Small Neural Nets", "abstract": "The universal approximation theorem for neural networks says that any reasonable function is well-approximated by a two-layer neural network with sigmoid gates but it does not provide good bounds on the number of hidden-layer nodes or the weights. However, robust concepts often have small neural networks in practice. We show an efficient analog of the universal approximation theorem on the boolean hypercube in this context.\n\nWe prove that any noise-stable boolean function on n boolean-valued input variables can be well-approximated by a two-layer linear threshold circuit with a small number of hidden-layer nodes and small weights, that depend only on the noise-stability and approximation parameters, and are independent of n. We also give a polynomial time learning algorithm that outputs a small two-layer linear threshold circuit that approximates such a given function. We also show weaker generalizations of this to noise-stable polynomial threshold functions and noise-stable boolean functions in general.", "pdf": "/pdf/31c447ffbb60b8212b2e965e2db44d961cf1b7bb.pdf", "TL;DR": "an efficient analog of the universal approximation theorem for neural networks over the boolean hypercube", "paperhash": "deshpande|on_robust_concepts_and_small_neural_nets", "keywords": ["Theory"], "conflicts": ["microsoft.com", "cs.utexas.edu"], "authorids": ["amitdesh@microsoft.com", "sushrutk@cs.utexas.edu"], "authors": ["Amit Deshpande", "Sushrut Karmalkar"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287702226, "id": "ICLR.cc/2017/conference/-/paper169/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SyZprb5xg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper169/reviewers", "ICLR.cc/2017/conference/paper169/areachairs"], "cdate": 1485287702226}}}, {"tddate": null, "tmdate": 1482204027291, "tcdate": 1482161680238, "number": 4, "id": "SkOfC_BNe", "invitation": "ICLR.cc/2017/conference/-/paper169/public/comment", "forum": "SyZprb5xg", "replyto": "B18esDeVe", "signatures": ["~Amit_Deshpande1"], "readers": ["everyone"], "writers": ["~Amit_Deshpande1"], "content": {"title": "response to AnonReviewer1's comment on comparison of our bounds with dimension-based bounds", "comment": "Thank you for your comments and suggestions for improvement. Our paper does mention interesting classes of (\\epsilon, \\delta)-noise-stable functions where 1/\\epsilon and 1/\\delta are constants INDEPENDENT of the dimension, e.g., constant-margin halfspaces and submodular functions. Please contrast this with arbitrary halfspaces of the d-dimensional hypercube where margin can be as small as exp(-d \\log d) in the worst case. In this sense, noise-stability as the \"true\" dimension can be much smaller than the actual dimension.\n\nWe do mention that our results are a direct combination of known results (a) Bourgain's junta theorem and its more efficient version for halfspaces and (b) size-depth-weight trade-offs for linear threshold circuits. We did look for more general or surprising results beyond these. The definition of robust concepts can be generalized as those having a small probability of label-flipping (from +ve to -ve or vice versa) when a small random gaussian perturbation is applied to a data point picked from an underlying distribution. However, proving similar results would require very different ideas. We have a detailed discussion of the obstacles to improvement in Subsection 2.1."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Robust Concepts and Small Neural Nets", "abstract": "The universal approximation theorem for neural networks says that any reasonable function is well-approximated by a two-layer neural network with sigmoid gates but it does not provide good bounds on the number of hidden-layer nodes or the weights. However, robust concepts often have small neural networks in practice. We show an efficient analog of the universal approximation theorem on the boolean hypercube in this context.\n\nWe prove that any noise-stable boolean function on n boolean-valued input variables can be well-approximated by a two-layer linear threshold circuit with a small number of hidden-layer nodes and small weights, that depend only on the noise-stability and approximation parameters, and are independent of n. We also give a polynomial time learning algorithm that outputs a small two-layer linear threshold circuit that approximates such a given function. We also show weaker generalizations of this to noise-stable polynomial threshold functions and noise-stable boolean functions in general.", "pdf": "/pdf/31c447ffbb60b8212b2e965e2db44d961cf1b7bb.pdf", "TL;DR": "an efficient analog of the universal approximation theorem for neural networks over the boolean hypercube", "paperhash": "deshpande|on_robust_concepts_and_small_neural_nets", "keywords": ["Theory"], "conflicts": ["microsoft.com", "cs.utexas.edu"], "authorids": ["amitdesh@microsoft.com", "sushrutk@cs.utexas.edu"], "authors": ["Amit Deshpande", "Sushrut Karmalkar"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287702226, "id": "ICLR.cc/2017/conference/-/paper169/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SyZprb5xg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper169/reviewers", "ICLR.cc/2017/conference/paper169/areachairs"], "cdate": 1485287702226}}}, {"tddate": null, "tmdate": 1482184976838, "tcdate": 1482184976838, "number": 3, "id": "r1tGF0HEe", "invitation": "ICLR.cc/2017/conference/-/paper169/official/review", "forum": "SyZprb5xg", "replyto": "SyZprb5xg", "signatures": ["ICLR.cc/2017/conference/paper169/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper169/AnonReviewer3"], "content": {"title": "review of ``ON ROBUST CONCEPTS AND SMALL NEURAL NETS''", "rating": "5: Marginally below acceptance threshold", "review": "SUMMARY \nThis paper presents a study of the number of hidden units and training examples needed to learn functions from a particular class. \nThis class is defined as those Boolean functions with an upper bound on the variability of the outputs. \n\nPROS\nThe paper promotes interesting results from the theoretical computer science community to investigate the efficiency of representation of functions with limited variability in terms of shallow feedforward networks with linear threshold units. \n\nCONS \nThe analysis is limited to shallow networks. The analysis is based on piecing together interesting results, however without contributing significant innovations. \nThe presentation of the main results and conclusions is somewhat obscure, as the therein appearing terms/constants do not express a clear relation between increased robustness and decreasing number of required hidden units. \n\nCOMMENTS \n\n- In the abstract one reads ``The universal approximation theorem for neural networks says that any reasonable function is well-approximated by a two-layer neural network with sigmoid gates but it does not provide good bounds on the number of hidden-layer nodes or the weights.'' \n\nIn page 1 the paper points the reader to a review article. It could be a good idea to include also more recent references. \n\nGiven the motivation presented in the abstract of the paper it would be a good idea to also comment of works discussing the classes of Boolean functions representable by linear threshold networks. \nFor instance the paper [Hyperplane Arrangements Separating Arbitrary Vertex Classes in n-Cubes. Wenzel, Ay, Paseman] discusses various classes of functions that can be represented by shallow linear threshold networks and provides upper and lower bounds on the number of hidden units needed for representing various types of Boolean functions. In particular that paper also provides lower bounds on the number of hidden units needed to define a universal approximator. \n\n- It certainly would be a good idea to discuss the results on the learning complexity in terms of measures such as the VC-dimension. \n\n- Thank you for the explanations regarding the constants.  \nSo if the noise sensitivity is kept constant, larger values of epsilon are associated with a smaller value of delta and of 1/epsilon. \nNonetheless, the description in Theorem 2 is in terms of poly(1/epsilon, 1/delta), which still could increase? \nAlso, in Lemma 1 reducing the sensitivity at a constant noise increases the bound on k? \n\n- The fact that the descriptions are independent of n seems to be related to the definition of the noise sensitivity as an expectation over all inputs. This certainly deserves more discussion. One good start could be to discuss examples of functions with an upper bound on the noise sensitivity (aside from the linear threshold functions discussed in Lemma 2). \nAlso, reverse statements to Lemma 1 would be interesting, describing the noise sensitivity of juntas specifically, even if only as simple examples. \n\n- On page 3 ``...variables is polynomial in the noise-sensitivity parameters'' should be inverse of?\n\nMINOR COMMENTS\n\nOn page 5 Proposition 1 should be Lemma 1? \n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Robust Concepts and Small Neural Nets", "abstract": "The universal approximation theorem for neural networks says that any reasonable function is well-approximated by a two-layer neural network with sigmoid gates but it does not provide good bounds on the number of hidden-layer nodes or the weights. However, robust concepts often have small neural networks in practice. We show an efficient analog of the universal approximation theorem on the boolean hypercube in this context.\n\nWe prove that any noise-stable boolean function on n boolean-valued input variables can be well-approximated by a two-layer linear threshold circuit with a small number of hidden-layer nodes and small weights, that depend only on the noise-stability and approximation parameters, and are independent of n. We also give a polynomial time learning algorithm that outputs a small two-layer linear threshold circuit that approximates such a given function. We also show weaker generalizations of this to noise-stable polynomial threshold functions and noise-stable boolean functions in general.", "pdf": "/pdf/31c447ffbb60b8212b2e965e2db44d961cf1b7bb.pdf", "TL;DR": "an efficient analog of the universal approximation theorem for neural networks over the boolean hypercube", "paperhash": "deshpande|on_robust_concepts_and_small_neural_nets", "keywords": ["Theory"], "conflicts": ["microsoft.com", "cs.utexas.edu"], "authorids": ["amitdesh@microsoft.com", "sushrutk@cs.utexas.edu"], "authors": ["Amit Deshpande", "Sushrut Karmalkar"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512675716, "id": "ICLR.cc/2017/conference/-/paper169/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper169/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper169/AnonReviewer1", "ICLR.cc/2017/conference/paper169/AnonReviewer4", "ICLR.cc/2017/conference/paper169/AnonReviewer3"], "reply": {"forum": "SyZprb5xg", "replyto": "SyZprb5xg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper169/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper169/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512675716}}}, {"tddate": null, "tmdate": 1482175263958, "tcdate": 1482175263958, "number": 2, "id": "r1O7mnrVg", "invitation": "ICLR.cc/2017/conference/-/paper169/official/review", "forum": "SyZprb5xg", "replyto": "SyZprb5xg", "signatures": ["ICLR.cc/2017/conference/paper169/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper169/AnonReviewer4"], "content": {"title": "A work that finds connections between existing theoretical results and the universal approximation theorem", "rating": "6: Marginally above acceptance threshold", "review": "This work finds a connection between Bourgain's junta problem, the existing results in circuit complexity, and the approximation of a boolean function using two-layer neural net. I think that finding connections between different fields and applying the insights gained is a valid contribution. For this reason, I recommend acceptance.\n\nBut my current major concern is that this work is only constrained to the domain of boolean hypercube, which is far from what is done in practice (continuous domain). Indeed, the authors could argue that understanding the former is a first step, but if the connection is only suitable for this case and not adaptable to more general scenarios, then it probably would have limited interest.", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Robust Concepts and Small Neural Nets", "abstract": "The universal approximation theorem for neural networks says that any reasonable function is well-approximated by a two-layer neural network with sigmoid gates but it does not provide good bounds on the number of hidden-layer nodes or the weights. However, robust concepts often have small neural networks in practice. We show an efficient analog of the universal approximation theorem on the boolean hypercube in this context.\n\nWe prove that any noise-stable boolean function on n boolean-valued input variables can be well-approximated by a two-layer linear threshold circuit with a small number of hidden-layer nodes and small weights, that depend only on the noise-stability and approximation parameters, and are independent of n. We also give a polynomial time learning algorithm that outputs a small two-layer linear threshold circuit that approximates such a given function. We also show weaker generalizations of this to noise-stable polynomial threshold functions and noise-stable boolean functions in general.", "pdf": "/pdf/31c447ffbb60b8212b2e965e2db44d961cf1b7bb.pdf", "TL;DR": "an efficient analog of the universal approximation theorem for neural networks over the boolean hypercube", "paperhash": "deshpande|on_robust_concepts_and_small_neural_nets", "keywords": ["Theory"], "conflicts": ["microsoft.com", "cs.utexas.edu"], "authorids": ["amitdesh@microsoft.com", "sushrutk@cs.utexas.edu"], "authors": ["Amit Deshpande", "Sushrut Karmalkar"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512675716, "id": "ICLR.cc/2017/conference/-/paper169/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper169/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper169/AnonReviewer1", "ICLR.cc/2017/conference/paper169/AnonReviewer4", "ICLR.cc/2017/conference/paper169/AnonReviewer3"], "reply": {"forum": "SyZprb5xg", "replyto": "SyZprb5xg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper169/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper169/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512675716}}}, {"tddate": null, "tmdate": 1480911271740, "tcdate": 1480911271734, "number": 3, "id": "r1xhYwGQl", "invitation": "ICLR.cc/2017/conference/-/paper169/public/comment", "forum": "SyZprb5xg", "replyto": "By82ej1Xg", "signatures": ["~Sushrut_Karmalkar1"], "readers": ["everyone"], "writers": ["~Sushrut_Karmalkar1"], "content": {"title": "Infinite-dimension kernel setting", "comment": "Theorem 2 gives a tighter result than Theorem 1, where the neural net size is polynomial in 1/\\epsilon and 1/\\delta. Our bounds depend on the noise-stability of concepts and are independent of the dimension, which is very useful in an infinite-dimensional kernel setting."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Robust Concepts and Small Neural Nets", "abstract": "The universal approximation theorem for neural networks says that any reasonable function is well-approximated by a two-layer neural network with sigmoid gates but it does not provide good bounds on the number of hidden-layer nodes or the weights. However, robust concepts often have small neural networks in practice. We show an efficient analog of the universal approximation theorem on the boolean hypercube in this context.\n\nWe prove that any noise-stable boolean function on n boolean-valued input variables can be well-approximated by a two-layer linear threshold circuit with a small number of hidden-layer nodes and small weights, that depend only on the noise-stability and approximation parameters, and are independent of n. We also give a polynomial time learning algorithm that outputs a small two-layer linear threshold circuit that approximates such a given function. We also show weaker generalizations of this to noise-stable polynomial threshold functions and noise-stable boolean functions in general.", "pdf": "/pdf/31c447ffbb60b8212b2e965e2db44d961cf1b7bb.pdf", "TL;DR": "an efficient analog of the universal approximation theorem for neural networks over the boolean hypercube", "paperhash": "deshpande|on_robust_concepts_and_small_neural_nets", "keywords": ["Theory"], "conflicts": ["microsoft.com", "cs.utexas.edu"], "authorids": ["amitdesh@microsoft.com", "sushrutk@cs.utexas.edu"], "authors": ["Amit Deshpande", "Sushrut Karmalkar"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287702226, "id": "ICLR.cc/2017/conference/-/paper169/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SyZprb5xg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper169/reviewers", "ICLR.cc/2017/conference/paper169/areachairs"], "cdate": 1485287702226}}}, {"tddate": null, "tmdate": 1480911193605, "tcdate": 1480911193600, "number": 2, "id": "H1fPKPfQl", "invitation": "ICLR.cc/2017/conference/-/paper169/public/comment", "forum": "SyZprb5xg", "replyto": "ryuKz2xXe", "signatures": ["~Sushrut_Karmalkar1"], "readers": ["everyone"], "writers": ["~Sushrut_Karmalkar1"], "content": {"title": "Observation new", "comment": "Yes, our results are technically simple, and obtained by combining Bourgain's junta theorem and size-weight-depth trade-offs from circuit complexity in a fairly straightforward manner. To our knowledge, this observation is new. The main contribution of our paper is in making this connection, trying to formulate an efficient universal approximation theorem for neural nets, and exploring its limitations."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Robust Concepts and Small Neural Nets", "abstract": "The universal approximation theorem for neural networks says that any reasonable function is well-approximated by a two-layer neural network with sigmoid gates but it does not provide good bounds on the number of hidden-layer nodes or the weights. However, robust concepts often have small neural networks in practice. We show an efficient analog of the universal approximation theorem on the boolean hypercube in this context.\n\nWe prove that any noise-stable boolean function on n boolean-valued input variables can be well-approximated by a two-layer linear threshold circuit with a small number of hidden-layer nodes and small weights, that depend only on the noise-stability and approximation parameters, and are independent of n. We also give a polynomial time learning algorithm that outputs a small two-layer linear threshold circuit that approximates such a given function. We also show weaker generalizations of this to noise-stable polynomial threshold functions and noise-stable boolean functions in general.", "pdf": "/pdf/31c447ffbb60b8212b2e965e2db44d961cf1b7bb.pdf", "TL;DR": "an efficient analog of the universal approximation theorem for neural networks over the boolean hypercube", "paperhash": "deshpande|on_robust_concepts_and_small_neural_nets", "keywords": ["Theory"], "conflicts": ["microsoft.com", "cs.utexas.edu"], "authorids": ["amitdesh@microsoft.com", "sushrutk@cs.utexas.edu"], "authors": ["Amit Deshpande", "Sushrut Karmalkar"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287702226, "id": "ICLR.cc/2017/conference/-/paper169/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SyZprb5xg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper169/reviewers", "ICLR.cc/2017/conference/paper169/areachairs"], "cdate": 1485287702226}}}, {"tddate": null, "tmdate": 1480911091135, "tcdate": 1480911091130, "number": 1, "id": "HJseKwzXl", "invitation": "ICLR.cc/2017/conference/-/paper169/public/comment", "forum": "SyZprb5xg", "replyto": "SkGJFByQg", "signatures": ["~Sushrut_Karmalkar1"], "readers": ["everyone"], "writers": ["~Sushrut_Karmalkar1"], "content": {"title": "Sensitivity vs size", "comment": "In Bourgain's theorem, the same \\delta appears as the noise sensitivity parameter and the closeness of approximation. It is difficult to decouple both. Our theorem does give a statement like \"more robust concepts have smaller neural nets\". Consider the closeness of approximation as a fixed constant, say, 0.01. That is, we are looking for a neural net that approximates the given concept on 1% of the inputs. For (\\epsilon, 0.01)-noise stable concepts, the more robust concepts are the ones that are noise stable even under large \\epsilon-perturbations. Our theorem implies that these concepts are approximated by a small poly(1/\\epsilon)-sized neural net on 99% of the inputs. Theorem 2 gives a better bound than Theorem 1 where the constants are polynomial in 1/\\epsilon and 1/\\delta."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Robust Concepts and Small Neural Nets", "abstract": "The universal approximation theorem for neural networks says that any reasonable function is well-approximated by a two-layer neural network with sigmoid gates but it does not provide good bounds on the number of hidden-layer nodes or the weights. However, robust concepts often have small neural networks in practice. We show an efficient analog of the universal approximation theorem on the boolean hypercube in this context.\n\nWe prove that any noise-stable boolean function on n boolean-valued input variables can be well-approximated by a two-layer linear threshold circuit with a small number of hidden-layer nodes and small weights, that depend only on the noise-stability and approximation parameters, and are independent of n. We also give a polynomial time learning algorithm that outputs a small two-layer linear threshold circuit that approximates such a given function. We also show weaker generalizations of this to noise-stable polynomial threshold functions and noise-stable boolean functions in general.", "pdf": "/pdf/31c447ffbb60b8212b2e965e2db44d961cf1b7bb.pdf", "TL;DR": "an efficient analog of the universal approximation theorem for neural networks over the boolean hypercube", "paperhash": "deshpande|on_robust_concepts_and_small_neural_nets", "keywords": ["Theory"], "conflicts": ["microsoft.com", "cs.utexas.edu"], "authorids": ["amitdesh@microsoft.com", "sushrutk@cs.utexas.edu"], "authors": ["Amit Deshpande", "Sushrut Karmalkar"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287702226, "id": "ICLR.cc/2017/conference/-/paper169/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SyZprb5xg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper169/reviewers", "ICLR.cc/2017/conference/paper169/areachairs"], "cdate": 1485287702226}}}, {"tddate": null, "tmdate": 1480798848469, "tcdate": 1480798848464, "number": 3, "id": "ryuKz2xXe", "invitation": "ICLR.cc/2017/conference/-/paper169/pre-review/question", "forum": "SyZprb5xg", "replyto": "SyZprb5xg", "signatures": ["ICLR.cc/2017/conference/paper169/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper169/AnonReviewer4"], "content": {"title": "Application of previous results?", "question": "The results of the paper seem to be a straightforward application of results of previous work. Besides being able to connect results from different domains (the Junta problem, circuit complexity) to the two-layer neural net, can the authors justify their own contributions in their analysis?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Robust Concepts and Small Neural Nets", "abstract": "The universal approximation theorem for neural networks says that any reasonable function is well-approximated by a two-layer neural network with sigmoid gates but it does not provide good bounds on the number of hidden-layer nodes or the weights. However, robust concepts often have small neural networks in practice. We show an efficient analog of the universal approximation theorem on the boolean hypercube in this context.\n\nWe prove that any noise-stable boolean function on n boolean-valued input variables can be well-approximated by a two-layer linear threshold circuit with a small number of hidden-layer nodes and small weights, that depend only on the noise-stability and approximation parameters, and are independent of n. We also give a polynomial time learning algorithm that outputs a small two-layer linear threshold circuit that approximates such a given function. We also show weaker generalizations of this to noise-stable polynomial threshold functions and noise-stable boolean functions in general.", "pdf": "/pdf/31c447ffbb60b8212b2e965e2db44d961cf1b7bb.pdf", "TL;DR": "an efficient analog of the universal approximation theorem for neural networks over the boolean hypercube", "paperhash": "deshpande|on_robust_concepts_and_small_neural_nets", "keywords": ["Theory"], "conflicts": ["microsoft.com", "cs.utexas.edu"], "authorids": ["amitdesh@microsoft.com", "sushrutk@cs.utexas.edu"], "authors": ["Amit Deshpande", "Sushrut Karmalkar"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959426872, "id": "ICLR.cc/2017/conference/-/paper169/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper169/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper169/AnonReviewer3", "ICLR.cc/2017/conference/paper169/AnonReviewer1", "ICLR.cc/2017/conference/paper169/AnonReviewer4"], "reply": {"forum": "SyZprb5xg", "replyto": "SyZprb5xg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper169/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper169/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959426872}}}, {"tddate": null, "tmdate": 1480728749985, "tcdate": 1480728749978, "number": 2, "id": "By82ej1Xg", "invitation": "ICLR.cc/2017/conference/-/paper169/pre-review/question", "forum": "SyZprb5xg", "replyto": "SyZprb5xg", "signatures": ["ICLR.cc/2017/conference/paper169/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper169/AnonReviewer1"], "content": {"title": "Exponential dependence on epsilon", "question": "You have talked about small neural networks but there is an exponential dependence on 1/eps . How is this more interesting (or tighter) than the dependence on the dimension? Is it possible to combine this with the results that only depends on the dimension to get something tighter?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Robust Concepts and Small Neural Nets", "abstract": "The universal approximation theorem for neural networks says that any reasonable function is well-approximated by a two-layer neural network with sigmoid gates but it does not provide good bounds on the number of hidden-layer nodes or the weights. However, robust concepts often have small neural networks in practice. We show an efficient analog of the universal approximation theorem on the boolean hypercube in this context.\n\nWe prove that any noise-stable boolean function on n boolean-valued input variables can be well-approximated by a two-layer linear threshold circuit with a small number of hidden-layer nodes and small weights, that depend only on the noise-stability and approximation parameters, and are independent of n. We also give a polynomial time learning algorithm that outputs a small two-layer linear threshold circuit that approximates such a given function. We also show weaker generalizations of this to noise-stable polynomial threshold functions and noise-stable boolean functions in general.", "pdf": "/pdf/31c447ffbb60b8212b2e965e2db44d961cf1b7bb.pdf", "TL;DR": "an efficient analog of the universal approximation theorem for neural networks over the boolean hypercube", "paperhash": "deshpande|on_robust_concepts_and_small_neural_nets", "keywords": ["Theory"], "conflicts": ["microsoft.com", "cs.utexas.edu"], "authorids": ["amitdesh@microsoft.com", "sushrutk@cs.utexas.edu"], "authors": ["Amit Deshpande", "Sushrut Karmalkar"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959426872, "id": "ICLR.cc/2017/conference/-/paper169/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper169/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper169/AnonReviewer3", "ICLR.cc/2017/conference/paper169/AnonReviewer1", "ICLR.cc/2017/conference/paper169/AnonReviewer4"], "reply": {"forum": "SyZprb5xg", "replyto": "SyZprb5xg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper169/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper169/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959426872}}}, {"tddate": null, "tmdate": 1480706266384, "tcdate": 1480706266380, "number": 1, "id": "SkGJFByQg", "invitation": "ICLR.cc/2017/conference/-/paper169/pre-review/question", "forum": "SyZprb5xg", "replyto": "SyZprb5xg", "signatures": ["ICLR.cc/2017/conference/paper169/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper169/AnonReviewer3"], "content": {"title": "Sensitivity vs size", "question": "The constants appear to grow with smaller \\delta and \\epsilon. \nAre the presented results not contradictory when smaller sensitivity to noise goes with larger number of units? \nAre the presented results not contradictory when the same sensitivity at larger noise goes with a larger number of units? \nHow different is Theorem 3 from what could be obtained using Theorem 1 alone and not Theorem 2? "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Robust Concepts and Small Neural Nets", "abstract": "The universal approximation theorem for neural networks says that any reasonable function is well-approximated by a two-layer neural network with sigmoid gates but it does not provide good bounds on the number of hidden-layer nodes or the weights. However, robust concepts often have small neural networks in practice. We show an efficient analog of the universal approximation theorem on the boolean hypercube in this context.\n\nWe prove that any noise-stable boolean function on n boolean-valued input variables can be well-approximated by a two-layer linear threshold circuit with a small number of hidden-layer nodes and small weights, that depend only on the noise-stability and approximation parameters, and are independent of n. We also give a polynomial time learning algorithm that outputs a small two-layer linear threshold circuit that approximates such a given function. We also show weaker generalizations of this to noise-stable polynomial threshold functions and noise-stable boolean functions in general.", "pdf": "/pdf/31c447ffbb60b8212b2e965e2db44d961cf1b7bb.pdf", "TL;DR": "an efficient analog of the universal approximation theorem for neural networks over the boolean hypercube", "paperhash": "deshpande|on_robust_concepts_and_small_neural_nets", "keywords": ["Theory"], "conflicts": ["microsoft.com", "cs.utexas.edu"], "authorids": ["amitdesh@microsoft.com", "sushrutk@cs.utexas.edu"], "authors": ["Amit Deshpande", "Sushrut Karmalkar"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959426872, "id": "ICLR.cc/2017/conference/-/paper169/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper169/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper169/AnonReviewer3", "ICLR.cc/2017/conference/paper169/AnonReviewer1", "ICLR.cc/2017/conference/paper169/AnonReviewer4"], "reply": {"forum": "SyZprb5xg", "replyto": "SyZprb5xg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper169/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper169/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959426872}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1478264656503, "tcdate": 1478264249585, "number": 169, "id": "SyZprb5xg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "SyZprb5xg", "signatures": ["~Amit_Deshpande1"], "readers": ["everyone"], "content": {"title": "On Robust Concepts and Small Neural Nets", "abstract": "The universal approximation theorem for neural networks says that any reasonable function is well-approximated by a two-layer neural network with sigmoid gates but it does not provide good bounds on the number of hidden-layer nodes or the weights. However, robust concepts often have small neural networks in practice. We show an efficient analog of the universal approximation theorem on the boolean hypercube in this context.\n\nWe prove that any noise-stable boolean function on n boolean-valued input variables can be well-approximated by a two-layer linear threshold circuit with a small number of hidden-layer nodes and small weights, that depend only on the noise-stability and approximation parameters, and are independent of n. We also give a polynomial time learning algorithm that outputs a small two-layer linear threshold circuit that approximates such a given function. We also show weaker generalizations of this to noise-stable polynomial threshold functions and noise-stable boolean functions in general.", "pdf": "/pdf/31c447ffbb60b8212b2e965e2db44d961cf1b7bb.pdf", "TL;DR": "an efficient analog of the universal approximation theorem for neural networks over the boolean hypercube", "paperhash": "deshpande|on_robust_concepts_and_small_neural_nets", "keywords": ["Theory"], "conflicts": ["microsoft.com", "cs.utexas.edu"], "authorids": ["amitdesh@microsoft.com", "sushrutk@cs.utexas.edu"], "authors": ["Amit Deshpande", "Sushrut Karmalkar"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 13, "writable": false, "overwriting": ["ByUg_SNte"], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}], "count": 14}