{"notes": [{"id": "HklFUlBKPB", "original": "HygQdcetwS", "number": 2331, "cdate": 1569439825057, "ddate": null, "tcdate": 1569439825057, "tmdate": 1577168244059, "tddate": null, "forum": "HklFUlBKPB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["drolnick@seas.upenn.edu", "koerding@gmail.com"], "title": "Identifying Weights and Architectures of Unknown ReLU Networks", "authors": ["David Rolnick", "Konrad P. Kording"], "pdf": "/pdf/0998379b513ba311f63f9adaab03d545b9eb37a4.pdf", "TL;DR": "We show that in many cases it is possible to reconstruct the architecture, weights, and biases of a deep ReLU network given the network's output for specified inputs.", "abstract": "The output of a neural network depends on its parameters in a highly nonlinear way, and it is widely assumed that a network's parameters cannot be identified from its outputs. Here, we show that in many cases it is possible to reconstruct the architecture, weights, and biases of a deep ReLU network given the ability to query the network. ReLU networks are piecewise linear and the boundaries between pieces correspond to inputs for which one of the ReLUs switches between inactive and active states. Thus, first-layer ReLUs can be identified (up to sign and scaling) based on the orientation of their associated hyperplanes. Later-layer ReLU boundaries bend when they cross earlier-layer boundaries and the extent of bending reveals the weights between them. Our algorithm uses this to identify the units in the network and weights connecting them (up to isomorphism). The fact that considerable parts of deep networks can be identified from their outputs has implications for security, neuroscience, and our understanding of neural networks.", "code": "https://osf.io/rf4jt/?view_only=e367ddfd3ad84a44b7abeddd27624a88", "keywords": ["deep neural network", "ReLU", "piecewise linear function", "linear region", "activation region", "weights", "parameters", "architecture"], "paperhash": "rolnick|identifying_weights_and_architectures_of_unknown_relu_networks", "original_pdf": "/attachment/8a1d030b050d8fb8fd13fd217db8ad3070a13d94.pdf", "_bibtex": "@misc{\nrolnick2020identifying,\ntitle={Identifying Weights and Architectures of Unknown Re{\\{}LU{\\}} Networks},\nauthor={David Rolnick and Konrad P. Kording},\nyear={2020},\nurl={https://openreview.net/forum?id=HklFUlBKPB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "IBVseua3oI", "original": null, "number": 1, "cdate": 1576798746351, "ddate": null, "tcdate": 1576798746351, "tmdate": 1576800889766, "tddate": null, "forum": "HklFUlBKPB", "replyto": "HklFUlBKPB", "invitation": "ICLR.cc/2020/Conference/Paper2331/-/Decision", "content": {"decision": "Reject", "comment": "This article studies the identifiability of architecture and weights of a ReLU network from the values of the computed functions, and presents an algorithm to do this. This is a very interesting problem with diverse implications. The reviewers raised concerns about the completeness of various parts of the proposed algorithm and the complexity analysis, some of which were addressed in the author's response. Another concern raised was that the experiments were limited to small networks, with a proof of concept on more realistic networks missing. The revision added experiments with MNIST. Other concerns (which in my opinion could be studied separately) include possible limitations of the approach to networks with no shared weights nor pooling. The reviewers agree that the article concerns an interesting topic that has not been studied in much detail yet. Still, the article would benefit from a more transparent presentation of the algorithm and theoretical analysis, as well as more extensive experiments. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["drolnick@seas.upenn.edu", "koerding@gmail.com"], "title": "Identifying Weights and Architectures of Unknown ReLU Networks", "authors": ["David Rolnick", "Konrad P. Kording"], "pdf": "/pdf/0998379b513ba311f63f9adaab03d545b9eb37a4.pdf", "TL;DR": "We show that in many cases it is possible to reconstruct the architecture, weights, and biases of a deep ReLU network given the network's output for specified inputs.", "abstract": "The output of a neural network depends on its parameters in a highly nonlinear way, and it is widely assumed that a network's parameters cannot be identified from its outputs. Here, we show that in many cases it is possible to reconstruct the architecture, weights, and biases of a deep ReLU network given the ability to query the network. ReLU networks are piecewise linear and the boundaries between pieces correspond to inputs for which one of the ReLUs switches between inactive and active states. Thus, first-layer ReLUs can be identified (up to sign and scaling) based on the orientation of their associated hyperplanes. Later-layer ReLU boundaries bend when they cross earlier-layer boundaries and the extent of bending reveals the weights between them. Our algorithm uses this to identify the units in the network and weights connecting them (up to isomorphism). The fact that considerable parts of deep networks can be identified from their outputs has implications for security, neuroscience, and our understanding of neural networks.", "code": "https://osf.io/rf4jt/?view_only=e367ddfd3ad84a44b7abeddd27624a88", "keywords": ["deep neural network", "ReLU", "piecewise linear function", "linear region", "activation region", "weights", "parameters", "architecture"], "paperhash": "rolnick|identifying_weights_and_architectures_of_unknown_relu_networks", "original_pdf": "/attachment/8a1d030b050d8fb8fd13fd217db8ad3070a13d94.pdf", "_bibtex": "@misc{\nrolnick2020identifying,\ntitle={Identifying Weights and Architectures of Unknown Re{\\{}LU{\\}} Networks},\nauthor={David Rolnick and Konrad P. Kording},\nyear={2020},\nurl={https://openreview.net/forum?id=HklFUlBKPB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HklFUlBKPB", "replyto": "HklFUlBKPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795714969, "tmdate": 1576800264776, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2331/-/Decision"}}}, {"id": "rJl2PVMsoS", "original": null, "number": 6, "cdate": 1573753955903, "ddate": null, "tcdate": 1573753955903, "tmdate": 1573753955903, "tddate": null, "forum": "HklFUlBKPB", "replyto": "SygDMCksoB", "invitation": "ICLR.cc/2020/Conference/Paper2331/-/Official_Comment", "content": {"title": "Thank you for you reply", "comment": "I have read your rebuttal, and most of my questions are well addressed. I maintain my original rating on this paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper2331/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2331/AnonReviewer4", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["drolnick@seas.upenn.edu", "koerding@gmail.com"], "title": "Identifying Weights and Architectures of Unknown ReLU Networks", "authors": ["David Rolnick", "Konrad P. Kording"], "pdf": "/pdf/0998379b513ba311f63f9adaab03d545b9eb37a4.pdf", "TL;DR": "We show that in many cases it is possible to reconstruct the architecture, weights, and biases of a deep ReLU network given the network's output for specified inputs.", "abstract": "The output of a neural network depends on its parameters in a highly nonlinear way, and it is widely assumed that a network's parameters cannot be identified from its outputs. Here, we show that in many cases it is possible to reconstruct the architecture, weights, and biases of a deep ReLU network given the ability to query the network. ReLU networks are piecewise linear and the boundaries between pieces correspond to inputs for which one of the ReLUs switches between inactive and active states. Thus, first-layer ReLUs can be identified (up to sign and scaling) based on the orientation of their associated hyperplanes. Later-layer ReLU boundaries bend when they cross earlier-layer boundaries and the extent of bending reveals the weights between them. Our algorithm uses this to identify the units in the network and weights connecting them (up to isomorphism). The fact that considerable parts of deep networks can be identified from their outputs has implications for security, neuroscience, and our understanding of neural networks.", "code": "https://osf.io/rf4jt/?view_only=e367ddfd3ad84a44b7abeddd27624a88", "keywords": ["deep neural network", "ReLU", "piecewise linear function", "linear region", "activation region", "weights", "parameters", "architecture"], "paperhash": "rolnick|identifying_weights_and_architectures_of_unknown_relu_networks", "original_pdf": "/attachment/8a1d030b050d8fb8fd13fd217db8ad3070a13d94.pdf", "_bibtex": "@misc{\nrolnick2020identifying,\ntitle={Identifying Weights and Architectures of Unknown Re{\\{}LU{\\}} Networks},\nauthor={David Rolnick and Konrad P. Kording},\nyear={2020},\nurl={https://openreview.net/forum?id=HklFUlBKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HklFUlBKPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2331/Authors", "ICLR.cc/2020/Conference/Paper2331/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2331/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2331/Reviewers", "ICLR.cc/2020/Conference/Paper2331/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2331/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2331/Authors|ICLR.cc/2020/Conference/Paper2331/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142951, "tmdate": 1576860549555, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2331/Authors", "ICLR.cc/2020/Conference/Paper2331/Reviewers", "ICLR.cc/2020/Conference/Paper2331/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2331/-/Official_Comment"}}}, {"id": "HJguCCkijB", "original": null, "number": 5, "cdate": 1573744336405, "ddate": null, "tcdate": 1573744336405, "tmdate": 1573744336405, "tddate": null, "forum": "HklFUlBKPB", "replyto": "SyxqKXM6KS", "invitation": "ICLR.cc/2020/Conference/Paper2331/-/Official_Comment", "content": {"title": "Response to Review #1", "comment": "Thank you for the careful review and feedback.  To respond to the questions raised:\n\n- Detail on algorithmic primitives. We have clarified the text. The algorithm PointsOnLine is able to perform binary search on multiple points simultaneously. Our asymptotic analysis is correct because we know the expected number of boundary points that will be discovered along the line is linear in the total number of neurons of the network (Hanin and Rolnick 2019). For TestHyperplane, the goal is to determine whether all points on a hyperplane do lie within the boundary (since then the hyperplane arises from the boundary of a layer-1 neuron); to do this, we test random points far along the hyperplane - if all these points are indeed on the boundary, then we conclude that the hyperplane is contained in the boundary.\n\n- ResNets. We apologize for the lack of clarity, which we have corrected in the text; we intended to emphasize that our algorithm can be modified to learn skip connections, rather than specialty layers that may also occur in a ResNet. Here is the intuition for such modification: In the case of skip connections, each boundary is still given by a bent hyperplane which bends when it intersects the bent hyperplanes associated with neurons at earlier layers. However, potential weights must in this case be considered between any two neurons in different layers. Deriving such skip weights is somewhat more complex than for MLPs, as the \u201cbend\u201d is influenced not merely by the skip connection but by the weights along all other paths between the two neurons through the network. Thus, it is necessary to \u201cmove backward\u201d through the network - for a neuron in layer k, one must first derive the weights in the preceding layer k-1, then at k-2, and so on. If the reviewers believe that such intuition would not confuse the main argument, we are happy to include it in an appendix.\n\n- Evaluation on more complex networks.  We have now added experimental verification of our first-layer method on MLPs trained on MNIST and on networks with 3 and 4 layers.  Please see Figure 3.\n\n- Scaling of weights. As described in 3.2, it is mathematically impossible for any algorithm to learn the \u201ctrue\u201d scaling of the weights in a network, since this scaling can be arbitrarily changed without in any way affecting the underlying function. As for how we compared the approximated weights with the true weights, we rescaled both sets of weights vectors to norm 1 for comparison (again, since the scaling is arbitrary).\n\n- Figure 3, # of queries. The number of queries shown in the figure is *per parameter learned* - therefore, for larger networks, the number of queries goes up, but not by as much as the number of parameters inferred goes up.\n\n- Number of queries for additional layers. Depending on the approach taken to explore intersections between hyperplanes, the number of queries required can grow linearly in the number of parameters inferred, as each weight can be inferred by examining a single intersection between boundaries.\n\n- Choice of parameters. All choices of parameters are presented in our publicly available code. The length of line segments used in sampling does not significantly affect the results; nor does the radius.\n\nNo prior work has, to our knowledge, been able to deduce even the first layer of an MLP with 2 hidden layers.  We show empirically that our algorithm is able to deduce the first layer of 2-, 3-, and 4-layer networks, as well as the second layer of 2-layer networks. The mathematical justification for our algorithm holds for any number of layers.  We believe that each of these contributions significantly advances the state-of-the-art."}, "signatures": ["ICLR.cc/2020/Conference/Paper2331/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2331/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["drolnick@seas.upenn.edu", "koerding@gmail.com"], "title": "Identifying Weights and Architectures of Unknown ReLU Networks", "authors": ["David Rolnick", "Konrad P. Kording"], "pdf": "/pdf/0998379b513ba311f63f9adaab03d545b9eb37a4.pdf", "TL;DR": "We show that in many cases it is possible to reconstruct the architecture, weights, and biases of a deep ReLU network given the network's output for specified inputs.", "abstract": "The output of a neural network depends on its parameters in a highly nonlinear way, and it is widely assumed that a network's parameters cannot be identified from its outputs. Here, we show that in many cases it is possible to reconstruct the architecture, weights, and biases of a deep ReLU network given the ability to query the network. ReLU networks are piecewise linear and the boundaries between pieces correspond to inputs for which one of the ReLUs switches between inactive and active states. Thus, first-layer ReLUs can be identified (up to sign and scaling) based on the orientation of their associated hyperplanes. Later-layer ReLU boundaries bend when they cross earlier-layer boundaries and the extent of bending reveals the weights between them. Our algorithm uses this to identify the units in the network and weights connecting them (up to isomorphism). The fact that considerable parts of deep networks can be identified from their outputs has implications for security, neuroscience, and our understanding of neural networks.", "code": "https://osf.io/rf4jt/?view_only=e367ddfd3ad84a44b7abeddd27624a88", "keywords": ["deep neural network", "ReLU", "piecewise linear function", "linear region", "activation region", "weights", "parameters", "architecture"], "paperhash": "rolnick|identifying_weights_and_architectures_of_unknown_relu_networks", "original_pdf": "/attachment/8a1d030b050d8fb8fd13fd217db8ad3070a13d94.pdf", "_bibtex": "@misc{\nrolnick2020identifying,\ntitle={Identifying Weights and Architectures of Unknown Re{\\{}LU{\\}} Networks},\nauthor={David Rolnick and Konrad P. Kording},\nyear={2020},\nurl={https://openreview.net/forum?id=HklFUlBKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HklFUlBKPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2331/Authors", "ICLR.cc/2020/Conference/Paper2331/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2331/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2331/Reviewers", "ICLR.cc/2020/Conference/Paper2331/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2331/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2331/Authors|ICLR.cc/2020/Conference/Paper2331/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142951, "tmdate": 1576860549555, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2331/Authors", "ICLR.cc/2020/Conference/Paper2331/Reviewers", "ICLR.cc/2020/Conference/Paper2331/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2331/-/Official_Comment"}}}, {"id": "S1efi0yioS", "original": null, "number": 4, "cdate": 1573744282438, "ddate": null, "tcdate": 1573744282438, "tmdate": 1573744282438, "tddate": null, "forum": "HklFUlBKPB", "replyto": "SJeEalATKB", "invitation": "ICLR.cc/2020/Conference/Paper2331/-/Official_Comment", "content": {"title": "Response to Review #3", "comment": "Thank you for the careful review and feedback.  To respond to the questions raised:\n\n- We do suppose a completely black box condition. We first deduce the first layer (and describe mathematically why our algorithm works). We then deduce the next layer recursively using our deduction of the previous layer (and again describe mathematically why our algorithm works). We invite the reviewer to verify in our publicly available code that we are not violating the black box condition in the smallest particular.\n\n- We have clarified the description of our experimental setup in Section 6; in particular, we have spelled out that the memorization task involves training an MLP for 1000 epochs using Adam optimizer on a dataset consisting of 1000 ten-dimensional vectors with i.i.d. random coordinates drawn from a unit Gaussian, given arbitrary binary labels.  The network must memorize these points, in keeping with the literature on memorization and generalization (e.g. Zhang et al. 2016).\n\n- We now, as requested, show the success of our method for a network trained on MNIST. Please see Figure 3.\n\n- We have likewise, as requested, added experimental verification of our method for 3-layered MLPs and also 4-layered MLPs. Please see Figure 3.\n\n- We have added a reference to the work of Oh et al. Thank you for calling this work to our attention.\n\nNo prior work has, to our knowledge, been able to deduce even the first layer of an MLP with 2 hidden layers.  We show empirically that our algorithm is able to deduce the first layer of 2-, 3-, and 4-layer networks, as well as the second layer of 2-layer networks. The mathematical justification for our algorithm holds for any number of layers. We believe that each of these contributions significantly advances the state-of-the-art."}, "signatures": ["ICLR.cc/2020/Conference/Paper2331/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2331/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["drolnick@seas.upenn.edu", "koerding@gmail.com"], "title": "Identifying Weights and Architectures of Unknown ReLU Networks", "authors": ["David Rolnick", "Konrad P. Kording"], "pdf": "/pdf/0998379b513ba311f63f9adaab03d545b9eb37a4.pdf", "TL;DR": "We show that in many cases it is possible to reconstruct the architecture, weights, and biases of a deep ReLU network given the network's output for specified inputs.", "abstract": "The output of a neural network depends on its parameters in a highly nonlinear way, and it is widely assumed that a network's parameters cannot be identified from its outputs. Here, we show that in many cases it is possible to reconstruct the architecture, weights, and biases of a deep ReLU network given the ability to query the network. ReLU networks are piecewise linear and the boundaries between pieces correspond to inputs for which one of the ReLUs switches between inactive and active states. Thus, first-layer ReLUs can be identified (up to sign and scaling) based on the orientation of their associated hyperplanes. Later-layer ReLU boundaries bend when they cross earlier-layer boundaries and the extent of bending reveals the weights between them. Our algorithm uses this to identify the units in the network and weights connecting them (up to isomorphism). The fact that considerable parts of deep networks can be identified from their outputs has implications for security, neuroscience, and our understanding of neural networks.", "code": "https://osf.io/rf4jt/?view_only=e367ddfd3ad84a44b7abeddd27624a88", "keywords": ["deep neural network", "ReLU", "piecewise linear function", "linear region", "activation region", "weights", "parameters", "architecture"], "paperhash": "rolnick|identifying_weights_and_architectures_of_unknown_relu_networks", "original_pdf": "/attachment/8a1d030b050d8fb8fd13fd217db8ad3070a13d94.pdf", "_bibtex": "@misc{\nrolnick2020identifying,\ntitle={Identifying Weights and Architectures of Unknown Re{\\{}LU{\\}} Networks},\nauthor={David Rolnick and Konrad P. Kording},\nyear={2020},\nurl={https://openreview.net/forum?id=HklFUlBKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HklFUlBKPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2331/Authors", "ICLR.cc/2020/Conference/Paper2331/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2331/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2331/Reviewers", "ICLR.cc/2020/Conference/Paper2331/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2331/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2331/Authors|ICLR.cc/2020/Conference/Paper2331/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142951, "tmdate": 1576860549555, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2331/Authors", "ICLR.cc/2020/Conference/Paper2331/Reviewers", "ICLR.cc/2020/Conference/Paper2331/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2331/-/Official_Comment"}}}, {"id": "BkgCE01ioH", "original": null, "number": 3, "cdate": 1573744182508, "ddate": null, "tcdate": 1573744182508, "tmdate": 1573744182508, "tddate": null, "forum": "HklFUlBKPB", "replyto": "BJxjoEJUcB", "invitation": "ICLR.cc/2020/Conference/Paper2331/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "Thank you for the careful review and feedback.  To respond to the questions raised:\n\n- Larger networks.  We have now added experimental verification of our first-layer method on MLPs trained on MNIST and on networks with 3 and 4 layers.  Please see Figure 3.\n\n- Number of queries for additional layers. Depending on the approach taken to explore intersections between hyperplanes, the number of queries required can grow linearly in the number of parameters inferred, as each weight can be inferred by examining a single intersection between boundaries.\n\n- ResNets. In the case of skip connections, each boundary is still given by a bent hyperplane which bends when it intersects the bent hyperplanes associated with neurons at earlier layers. However, potential weights must in this case be considered between any two neurons in different layers. Deriving such skip weights is somewhat more complex than for MLPs, as the \u201cbend\u201d is influenced not merely by the skip connection but by the weights along all other paths between the two neurons through the network. Thus, it is necessary to \u201cmove backward\u201d through the network - for a neuron in layer k, one must first derive the weights in the preceding layer k-1, then at k-2, and so on. If the reviewers believe that such intuition would not confuse the main argument, we are happy to include it in an appendix.\n\nNo prior work has, to our knowledge, been able to deduce even the first layer of an MLP with 2 hidden layers. We show empirically that our algorithm is able to deduce the first layer of 2-, 3-, and 4-layer networks, as well as the second layer of 2-layer networks. The mathematical justification for our algorithm holds for any number of layers.  We believe that each of these contributions significantly advances the state-of-the-art."}, "signatures": ["ICLR.cc/2020/Conference/Paper2331/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2331/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["drolnick@seas.upenn.edu", "koerding@gmail.com"], "title": "Identifying Weights and Architectures of Unknown ReLU Networks", "authors": ["David Rolnick", "Konrad P. Kording"], "pdf": "/pdf/0998379b513ba311f63f9adaab03d545b9eb37a4.pdf", "TL;DR": "We show that in many cases it is possible to reconstruct the architecture, weights, and biases of a deep ReLU network given the network's output for specified inputs.", "abstract": "The output of a neural network depends on its parameters in a highly nonlinear way, and it is widely assumed that a network's parameters cannot be identified from its outputs. Here, we show that in many cases it is possible to reconstruct the architecture, weights, and biases of a deep ReLU network given the ability to query the network. ReLU networks are piecewise linear and the boundaries between pieces correspond to inputs for which one of the ReLUs switches between inactive and active states. Thus, first-layer ReLUs can be identified (up to sign and scaling) based on the orientation of their associated hyperplanes. Later-layer ReLU boundaries bend when they cross earlier-layer boundaries and the extent of bending reveals the weights between them. Our algorithm uses this to identify the units in the network and weights connecting them (up to isomorphism). The fact that considerable parts of deep networks can be identified from their outputs has implications for security, neuroscience, and our understanding of neural networks.", "code": "https://osf.io/rf4jt/?view_only=e367ddfd3ad84a44b7abeddd27624a88", "keywords": ["deep neural network", "ReLU", "piecewise linear function", "linear region", "activation region", "weights", "parameters", "architecture"], "paperhash": "rolnick|identifying_weights_and_architectures_of_unknown_relu_networks", "original_pdf": "/attachment/8a1d030b050d8fb8fd13fd217db8ad3070a13d94.pdf", "_bibtex": "@misc{\nrolnick2020identifying,\ntitle={Identifying Weights and Architectures of Unknown Re{\\{}LU{\\}} Networks},\nauthor={David Rolnick and Konrad P. Kording},\nyear={2020},\nurl={https://openreview.net/forum?id=HklFUlBKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HklFUlBKPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2331/Authors", "ICLR.cc/2020/Conference/Paper2331/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2331/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2331/Reviewers", "ICLR.cc/2020/Conference/Paper2331/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2331/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2331/Authors|ICLR.cc/2020/Conference/Paper2331/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142951, "tmdate": 1576860549555, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2331/Authors", "ICLR.cc/2020/Conference/Paper2331/Reviewers", "ICLR.cc/2020/Conference/Paper2331/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2331/-/Official_Comment"}}}, {"id": "SygDMCksoB", "original": null, "number": 2, "cdate": 1573744143322, "ddate": null, "tcdate": 1573744143322, "tmdate": 1573744143322, "tddate": null, "forum": "HklFUlBKPB", "replyto": "HJx7Jukc9H", "invitation": "ICLR.cc/2020/Conference/Paper2331/-/Official_Comment", "content": {"title": "Response to Review #4", "comment": "Thank you for the careful review and feedback.  To respond to the questions raised:\n\n1. We have clarified the presentation of Figure 1. The middle panel of the figure shows the output N(x,y) of the network as a function of the two inputs given to the network N. The function N is defined by the network shown on the left panel (where the weights were chosen randomly according to the standard initialization procedure described in Experiments); the network itself is the most succinct description of this function. Regarding the partition of input space, each part within the partition corresponds to the set of input points on which a particular subset of the ReLUs in the network is active, and crossing between two parts within the partition means that (at least) one neuron flips from active to inactive ReLU. Once again, the simplest closed form expression of this partition is given by the network itself - it is not a regular tiling or any other kind of partition that lends itself to succinct description. In some sense, this is the power of the neural network, that it is able to fit complicated functions that cannot be described in other ways.\n\n2. As we note in the text (see Section 5), there are cases where our algorithm fails - if certain bent hyperplanes coincide exactly or if the boundaries associated with two neurons never intersect.  The former case is vanishingly unlikely for real networks - in particular, slightly perturbing the weights makes it possible for the algorithm to succeed once again, and one will never encounter such a brittle setting as the result of a noisy learning rule.  In the latter case, as we describe in the text, it is possible for the \u201cfailure\u201d of the algorithm to actually be a consequence of the network being ill-determined from the start, with several possible isomorphic settings of the weights. Regarding computational complexity, we include several related hardness results in our Related Work section (e.g. Goel, Kanade, et al. 2017).\n\n3. Reconstructing the input based on the output depends on the nature of the function in question. For example, if the output is of lower dimension than the input, then any continuous mapping will be non-injective - i.e. it will be impossible to recover the input from the output. In cases where the function is injective, it is an excellent question to ask, but one which is likely unrelated to the methods we propose here. Regarding reconstructing the training data based on the model, there is extensive interesting literature on membership inference attacks, such as Shokri et al. 2017, Song et al. 2017, and Carlini et al. 2019.\n\nWe have now added experimental verification of our first-layer method on MLPs trained on MNIST and on networks with 3 and 4 layers.  Please see Figure 3. No prior work has, to our knowledge, been able to deduce even the first layer of an MLP with 2 hidden layers.  We show empirically that our algorithm is able to deduce the first layer of 2-, 3-, and 4-layer networks, as well as the second layer of 2-layer networks. The mathematical justification for our algorithm holds for any number of layers.  We believe that each of these contributions significantly advances the state-of-the-art."}, "signatures": ["ICLR.cc/2020/Conference/Paper2331/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2331/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["drolnick@seas.upenn.edu", "koerding@gmail.com"], "title": "Identifying Weights and Architectures of Unknown ReLU Networks", "authors": ["David Rolnick", "Konrad P. Kording"], "pdf": "/pdf/0998379b513ba311f63f9adaab03d545b9eb37a4.pdf", "TL;DR": "We show that in many cases it is possible to reconstruct the architecture, weights, and biases of a deep ReLU network given the network's output for specified inputs.", "abstract": "The output of a neural network depends on its parameters in a highly nonlinear way, and it is widely assumed that a network's parameters cannot be identified from its outputs. Here, we show that in many cases it is possible to reconstruct the architecture, weights, and biases of a deep ReLU network given the ability to query the network. ReLU networks are piecewise linear and the boundaries between pieces correspond to inputs for which one of the ReLUs switches between inactive and active states. Thus, first-layer ReLUs can be identified (up to sign and scaling) based on the orientation of their associated hyperplanes. Later-layer ReLU boundaries bend when they cross earlier-layer boundaries and the extent of bending reveals the weights between them. Our algorithm uses this to identify the units in the network and weights connecting them (up to isomorphism). The fact that considerable parts of deep networks can be identified from their outputs has implications for security, neuroscience, and our understanding of neural networks.", "code": "https://osf.io/rf4jt/?view_only=e367ddfd3ad84a44b7abeddd27624a88", "keywords": ["deep neural network", "ReLU", "piecewise linear function", "linear region", "activation region", "weights", "parameters", "architecture"], "paperhash": "rolnick|identifying_weights_and_architectures_of_unknown_relu_networks", "original_pdf": "/attachment/8a1d030b050d8fb8fd13fd217db8ad3070a13d94.pdf", "_bibtex": "@misc{\nrolnick2020identifying,\ntitle={Identifying Weights and Architectures of Unknown Re{\\{}LU{\\}} Networks},\nauthor={David Rolnick and Konrad P. Kording},\nyear={2020},\nurl={https://openreview.net/forum?id=HklFUlBKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HklFUlBKPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2331/Authors", "ICLR.cc/2020/Conference/Paper2331/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2331/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2331/Reviewers", "ICLR.cc/2020/Conference/Paper2331/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2331/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2331/Authors|ICLR.cc/2020/Conference/Paper2331/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142951, "tmdate": 1576860549555, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2331/Authors", "ICLR.cc/2020/Conference/Paper2331/Reviewers", "ICLR.cc/2020/Conference/Paper2331/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2331/-/Official_Comment"}}}, {"id": "SyxqKXM6KS", "original": null, "number": 1, "cdate": 1571787649714, "ddate": null, "tcdate": 1571787649714, "tmdate": 1572972352725, "tddate": null, "forum": "HklFUlBKPB", "replyto": "HklFUlBKPB", "invitation": "ICLR.cc/2020/Conference/Paper2331/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper introduces an approach to recover weights of ReLU neural networks by querying the network with specifically constructed inputs. The authors notice that the decision regions of such networks are piece-wise linear corresponding to activations of individual neurons. This allows to identify hyperplanes that constitute the decision boundary and find intersection points of the decision boundaries corresponding to neurons at different layers of the network. However, weights can be recovered only up to permutations of neurons in each layer and up to a constant scaling factor for each layer.\n\nThe algorithm consists of two parts: Identifying parameters of the first layer and subsequent layers. First they sample a lot of line segments and find their intersections with the decision boundary, i.e. where pre-activations equal 0. Then they sample some points around the intersections and estimate the hyperplanes up to a scaling factor and a sign. For the first layer they check whether some of the hyperplanes belong to it. For the consecutive layers they proceed by moving from the intersection points along the hyperplanes until the decision boundary bends. Again, by identifying which of the bends correspond to the intersection of the current layer's hyperplanes with the previous layers' ones, they are able to recover parameters of the current layer by computing the angles of these intersections.\n\nThis paper tackles a very interesting and important problem that might have huge implications for security and many other aspects. However, I'm leaning towards Reject for the following reasons:\n\n1. The algorithm's description is either incomplete or unclear. There are such core functions as PointsOnLine and TestHyperplane, whose pseudo-code would be very helpful for understanding. For example, the authors say that PointsOnLine performs a \"binary search,\" but then this function can find only one (arbitrary) intersection of a line segment with a decision boundary, while each sampled line can intersect multiple ones. If it is not binary search, then the asymptotic analysis given in the end of Sec. 4.2 is incorrect. Even more mysterious is TestHyperplane, from the provided intuition I do not understand how it is possible to distinguish hyperplanes corresponding to the first layer vs. the other layers. In Sec. 4.3, second paragraph, the choice of R is unclear. How to chose it to make sure that the closest boundary intersects it?\n\nThe authors consider a very limited setting of only fully-connected (linear) layers with ReLU activations. In this case it is easy to see that the resulting decision boundary is indeed piece-wise linear with a lot of bending. Authors themselves notice, that \"the algorithm does not account for weight sharing.\" For CNN this will lead to each virtual neuron in each channel to have its own kernel weights, although there must be one kernel per channel. Also the authors admit, that pooling layers affect partitioning of the activation regions, making the proposed approach inapplicable. The authors did not discuss whether the proposed approach can handle batchnorm layers. Such non-linear transformations could pose serious problems. All this rules out applications to, for example, all CNN-based architectures, that prevail in computer vision. The authors mention, that their \"algorithm holds with slight modification\" for ResNets, but as mentioned earlier convolutional, pooling and batchnorm layers make it not so trivial (if at all possible).\n\n2. Experimental evaluation is extremely limited: It is all contained in just one paragraph. Although it is mentioned that \"it is often possible to recover parameters of deep ReLU networks,\" they evaluated their approach on very shallow and narrow networks (only 2 layers, 10 to 50 neurons in each). The immediate question here is why this algorithm is not applied to sufficiently deep NN? At least a network that could classify MNIST reasonably well. Actually, this would be a better proof-of-concept: Given a pre-trained MNIST classifier, apply the proposed method, recover the weights and check if you get the same output as from the original network. Whereas here the evaluation is given as a normalized relative error of the estimated vs. the true weights. Which raises the question of how the scaling factor was chosen? Recall, that the proposed method estimates network's parameters only up to an arbitrary scaling factor. My guess, is that for the Figures 3 and 4 (both right) the estimated weights were re-scaled optimally to minimize the relative error. But in the end, one is interested in recovering the original weights of the network, not relative ones.\n\nI am very confused by Fig. 3 left: Why is the number of queries going down as the number of neurons increases? Should it not be that with more neurons the ambiguity also increases, requiring more queries? Again, this analysis is very limited, it would be very interesting to see, how many more queries one needs for deeper layers of the network. But for this experiments with deeper than 2 layers networks are necessary.\n\n3. The choice of parameters is unclear and not discussed. How long should the line segments be, how many of them. How many points are sampled and within which radius to identify hyperplanes, how to choose 'R'. And how all these choices affect accuracy and performance.\n\nOverall, the paper looks rather incomplete to me and requires a major revision. It will definitely benefit if the \"slight modification\" for the case of ResNets is included. Also, experimental evaluation should be completely re-done and extended."}, "signatures": ["ICLR.cc/2020/Conference/Paper2331/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2331/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["drolnick@seas.upenn.edu", "koerding@gmail.com"], "title": "Identifying Weights and Architectures of Unknown ReLU Networks", "authors": ["David Rolnick", "Konrad P. Kording"], "pdf": "/pdf/0998379b513ba311f63f9adaab03d545b9eb37a4.pdf", "TL;DR": "We show that in many cases it is possible to reconstruct the architecture, weights, and biases of a deep ReLU network given the network's output for specified inputs.", "abstract": "The output of a neural network depends on its parameters in a highly nonlinear way, and it is widely assumed that a network's parameters cannot be identified from its outputs. Here, we show that in many cases it is possible to reconstruct the architecture, weights, and biases of a deep ReLU network given the ability to query the network. ReLU networks are piecewise linear and the boundaries between pieces correspond to inputs for which one of the ReLUs switches between inactive and active states. Thus, first-layer ReLUs can be identified (up to sign and scaling) based on the orientation of their associated hyperplanes. Later-layer ReLU boundaries bend when they cross earlier-layer boundaries and the extent of bending reveals the weights between them. Our algorithm uses this to identify the units in the network and weights connecting them (up to isomorphism). The fact that considerable parts of deep networks can be identified from their outputs has implications for security, neuroscience, and our understanding of neural networks.", "code": "https://osf.io/rf4jt/?view_only=e367ddfd3ad84a44b7abeddd27624a88", "keywords": ["deep neural network", "ReLU", "piecewise linear function", "linear region", "activation region", "weights", "parameters", "architecture"], "paperhash": "rolnick|identifying_weights_and_architectures_of_unknown_relu_networks", "original_pdf": "/attachment/8a1d030b050d8fb8fd13fd217db8ad3070a13d94.pdf", "_bibtex": "@misc{\nrolnick2020identifying,\ntitle={Identifying Weights and Architectures of Unknown Re{\\{}LU{\\}} Networks},\nauthor={David Rolnick and Konrad P. Kording},\nyear={2020},\nurl={https://openreview.net/forum?id=HklFUlBKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HklFUlBKPB", "replyto": "HklFUlBKPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2331/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2331/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575771375882, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2331/Reviewers"], "noninvitees": [], "tcdate": 1570237724361, "tmdate": 1575771375894, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2331/-/Official_Review"}}}, {"id": "SJeEalATKB", "original": null, "number": 2, "cdate": 1571836091970, "ddate": null, "tcdate": 1571836091970, "tmdate": 1572972352673, "tddate": null, "forum": "HklFUlBKPB", "replyto": "HklFUlBKPB", "invitation": "ICLR.cc/2020/Conference/Paper2331/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2331", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nMain contribution of the paper\n- The paper proposes a new method to recover the unknown structure of  the network by utilizing the piecewise linearity of ReLU network.\n- Some theoretical explanation of the method is provided.\n\nNote & Questions\n- As far as the author understands, the algorithm does not suppose a fully black-box condition. By seeing the section 4.1 and 4.2, it seems possible to access neurons in the intermediate layers.\n- Also, the proposed method seems to target only a MLP.\n\n\nStrong-points\n- This field is not that thoroughly investigated, and the author proposes a creative method to infer the hidden statistics of the neuron.\n\nConcerns\n- Most of all, the information the experiments conveys is too small to convince the argument of the author. The reviewer could not find the dataset they train (in the Experiment section), and the graph only shows the case of two-layered networks. Moreover, the reviewer couldn't find the explanation of the graph, including their legend (for example, Memorization).\nThe author suggests that this method can be applied to various networks. Still, the reviewer couldn't find any clue that the method actually worked for various settings: different activations, convolutional networks, and so on. More experimental results supporting the argument of the authors are required.\n- Assuming that the network was trained by MNIST and we infer the weight of the networks by the proposed method. Can the recovered network classify the number as well? Then, how the accuracy change?\nMore quantitative results regarding the asking are required.\n- Experimental results for more-than-two layered networks should be provided.\n- Oh.et.al (https://arxiv.org/abs/1711.01768) proposed a blackbox reverse-engineering method and provided experimental settings as well. The author should clarify the novelty and the strong-points of the works compared to the mentioned work.\n\nConclusion\n- The author proposes a new method to recover the weight and bias of the network.\n- The reviewer could not find much clue supporting the author's argument from the experiment section.\n\ninquiries\n- See the Concerns parts."}, "signatures": ["ICLR.cc/2020/Conference/Paper2331/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2331/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["drolnick@seas.upenn.edu", "koerding@gmail.com"], "title": "Identifying Weights and Architectures of Unknown ReLU Networks", "authors": ["David Rolnick", "Konrad P. Kording"], "pdf": "/pdf/0998379b513ba311f63f9adaab03d545b9eb37a4.pdf", "TL;DR": "We show that in many cases it is possible to reconstruct the architecture, weights, and biases of a deep ReLU network given the network's output for specified inputs.", "abstract": "The output of a neural network depends on its parameters in a highly nonlinear way, and it is widely assumed that a network's parameters cannot be identified from its outputs. Here, we show that in many cases it is possible to reconstruct the architecture, weights, and biases of a deep ReLU network given the ability to query the network. ReLU networks are piecewise linear and the boundaries between pieces correspond to inputs for which one of the ReLUs switches between inactive and active states. Thus, first-layer ReLUs can be identified (up to sign and scaling) based on the orientation of their associated hyperplanes. Later-layer ReLU boundaries bend when they cross earlier-layer boundaries and the extent of bending reveals the weights between them. Our algorithm uses this to identify the units in the network and weights connecting them (up to isomorphism). The fact that considerable parts of deep networks can be identified from their outputs has implications for security, neuroscience, and our understanding of neural networks.", "code": "https://osf.io/rf4jt/?view_only=e367ddfd3ad84a44b7abeddd27624a88", "keywords": ["deep neural network", "ReLU", "piecewise linear function", "linear region", "activation region", "weights", "parameters", "architecture"], "paperhash": "rolnick|identifying_weights_and_architectures_of_unknown_relu_networks", "original_pdf": "/attachment/8a1d030b050d8fb8fd13fd217db8ad3070a13d94.pdf", "_bibtex": "@misc{\nrolnick2020identifying,\ntitle={Identifying Weights and Architectures of Unknown Re{\\{}LU{\\}} Networks},\nauthor={David Rolnick and Konrad P. Kording},\nyear={2020},\nurl={https://openreview.net/forum?id=HklFUlBKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HklFUlBKPB", "replyto": "HklFUlBKPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2331/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2331/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575771375882, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2331/Reviewers"], "noninvitees": [], "tcdate": 1570237724361, "tmdate": 1575771375894, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2331/-/Official_Review"}}}, {"id": "BJxjoEJUcB", "original": null, "number": 3, "cdate": 1572365475175, "ddate": null, "tcdate": 1572365475175, "tmdate": 1572972352624, "tddate": null, "forum": "HklFUlBKPB", "replyto": "HklFUlBKPB", "invitation": "ICLR.cc/2020/Conference/Paper2331/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper introduces a procedure for reconstructing the architecture and weights of deep ReLU network, given only the ability to query the network (observe network outputs for a sequence of inputs).  The algorithm takes advantage of the piecewise linearity of ReLU networks and an analysis by [Hanin and Rolnick, 2019b] of the boundaries between linear regions as bent hyperplanes.  The observation that a boundary bends only for other boundaries corresponding to neurons in earlier network layers leads to a recursive layer-by-layer procedure for recovering network parameters.  Experiments show ability to recover both random networks and networks trained for a memorization task.  The method is currently limited to ReLU networks and does not account for any parameter-sharing structure, such as that found in convolutional networks.\n\nThe networks used in experiments appear to be substantially smaller (e.g. input/output dimensions on the order of 10 neurons) than those used in real applications.  Is the proposed approach practical to apply to networks used in actual applications?  How does the number of queries per parameter scale? (page 5 mentions sample complexity for recovering the first layer, but it would be helpful to clarify the situation for subsequent layers).\n\nPage 7 states that the proposed algorithm also holds for ResNets, with slight modifications, but defers details to future work.  If the modifications are indeed slight, it would better to include them here as this is an important special case and would increase the potential impact of the paper.\n\nOverall, while the paper does appear to rely heavily on developments made by [Hanin and Rolnick, 2019b], there is a potentially interesting contribution here.  I would appreciate clarification on concerns over practicality and the extension to ResNets.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2331/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2331/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["drolnick@seas.upenn.edu", "koerding@gmail.com"], "title": "Identifying Weights and Architectures of Unknown ReLU Networks", "authors": ["David Rolnick", "Konrad P. Kording"], "pdf": "/pdf/0998379b513ba311f63f9adaab03d545b9eb37a4.pdf", "TL;DR": "We show that in many cases it is possible to reconstruct the architecture, weights, and biases of a deep ReLU network given the network's output for specified inputs.", "abstract": "The output of a neural network depends on its parameters in a highly nonlinear way, and it is widely assumed that a network's parameters cannot be identified from its outputs. Here, we show that in many cases it is possible to reconstruct the architecture, weights, and biases of a deep ReLU network given the ability to query the network. ReLU networks are piecewise linear and the boundaries between pieces correspond to inputs for which one of the ReLUs switches between inactive and active states. Thus, first-layer ReLUs can be identified (up to sign and scaling) based on the orientation of their associated hyperplanes. Later-layer ReLU boundaries bend when they cross earlier-layer boundaries and the extent of bending reveals the weights between them. Our algorithm uses this to identify the units in the network and weights connecting them (up to isomorphism). The fact that considerable parts of deep networks can be identified from their outputs has implications for security, neuroscience, and our understanding of neural networks.", "code": "https://osf.io/rf4jt/?view_only=e367ddfd3ad84a44b7abeddd27624a88", "keywords": ["deep neural network", "ReLU", "piecewise linear function", "linear region", "activation region", "weights", "parameters", "architecture"], "paperhash": "rolnick|identifying_weights_and_architectures_of_unknown_relu_networks", "original_pdf": "/attachment/8a1d030b050d8fb8fd13fd217db8ad3070a13d94.pdf", "_bibtex": "@misc{\nrolnick2020identifying,\ntitle={Identifying Weights and Architectures of Unknown Re{\\{}LU{\\}} Networks},\nauthor={David Rolnick and Konrad P. Kording},\nyear={2020},\nurl={https://openreview.net/forum?id=HklFUlBKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HklFUlBKPB", "replyto": "HklFUlBKPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2331/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2331/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575771375882, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2331/Reviewers"], "noninvitees": [], "tcdate": 1570237724361, "tmdate": 1575771375894, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2331/-/Official_Review"}}}, {"id": "HJx7Jukc9H", "original": null, "number": 4, "cdate": 1572628442715, "ddate": null, "tcdate": 1572628442715, "tmdate": 1572972352578, "tddate": null, "forum": "HklFUlBKPB", "replyto": "HklFUlBKPB", "invitation": "ICLR.cc/2020/Conference/Paper2331/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "In this paper, the authors showed that in many cases it is possible to reconstruct the architecture, weights, and biases of a deep ReLU network given the ability to query the network. The studied problem is very interesting. I have the following questions about this paper:\n\n1. Can the authors provide detailed explanation of Figure 1? For instance start from input (x_1, x_2), and the weight in layer 1 and layer 2, what is the exact form of the function plotted in the middle panel? Also, how the input space is partitioned? I appreciate the authors provide this simple example, but detailed math will help readers to understand this easily.\n\n2. How about the efficiency of the proposed method? Is it NP-hard? I would like to see some analysis of the computational complexity and also some related experimental results.\n\n3. If the ReLU network can be reconstructed, can the input also be reconstructed based on the output? It would be very interesting to show a few example on reconstructing the input. Also, is that possible to even reconstruct the training data based on the released model?"}, "signatures": ["ICLR.cc/2020/Conference/Paper2331/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2331/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["drolnick@seas.upenn.edu", "koerding@gmail.com"], "title": "Identifying Weights and Architectures of Unknown ReLU Networks", "authors": ["David Rolnick", "Konrad P. Kording"], "pdf": "/pdf/0998379b513ba311f63f9adaab03d545b9eb37a4.pdf", "TL;DR": "We show that in many cases it is possible to reconstruct the architecture, weights, and biases of a deep ReLU network given the network's output for specified inputs.", "abstract": "The output of a neural network depends on its parameters in a highly nonlinear way, and it is widely assumed that a network's parameters cannot be identified from its outputs. Here, we show that in many cases it is possible to reconstruct the architecture, weights, and biases of a deep ReLU network given the ability to query the network. ReLU networks are piecewise linear and the boundaries between pieces correspond to inputs for which one of the ReLUs switches between inactive and active states. Thus, first-layer ReLUs can be identified (up to sign and scaling) based on the orientation of their associated hyperplanes. Later-layer ReLU boundaries bend when they cross earlier-layer boundaries and the extent of bending reveals the weights between them. Our algorithm uses this to identify the units in the network and weights connecting them (up to isomorphism). The fact that considerable parts of deep networks can be identified from their outputs has implications for security, neuroscience, and our understanding of neural networks.", "code": "https://osf.io/rf4jt/?view_only=e367ddfd3ad84a44b7abeddd27624a88", "keywords": ["deep neural network", "ReLU", "piecewise linear function", "linear region", "activation region", "weights", "parameters", "architecture"], "paperhash": "rolnick|identifying_weights_and_architectures_of_unknown_relu_networks", "original_pdf": "/attachment/8a1d030b050d8fb8fd13fd217db8ad3070a13d94.pdf", "_bibtex": "@misc{\nrolnick2020identifying,\ntitle={Identifying Weights and Architectures of Unknown Re{\\{}LU{\\}} Networks},\nauthor={David Rolnick and Konrad P. Kording},\nyear={2020},\nurl={https://openreview.net/forum?id=HklFUlBKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HklFUlBKPB", "replyto": "HklFUlBKPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2331/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2331/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575771375882, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2331/Reviewers"], "noninvitees": [], "tcdate": 1570237724361, "tmdate": 1575771375894, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2331/-/Official_Review"}}}, {"id": "BJx_nit4Fr", "original": null, "number": 1, "cdate": 1571228591570, "ddate": null, "tcdate": 1571228591570, "tmdate": 1571228591570, "tddate": null, "forum": "HklFUlBKPB", "replyto": "Hkl8me6QtH", "invitation": "ICLR.cc/2020/Conference/Paper2331/-/Official_Comment", "content": {"comment": "Great question. Our implementation is not intended to optimize for the number of queries - its efficiency can be improved greatly at the expense of simplicity.  For example, in the current implementation, some neurons in the second layer are estimated repeatedly - using several different points on their associated boundaries - before their full weight vectors are determined.  Sharing information between these iterations would reduce the number of queries needed.\n\nA more subtle optimization approach would have an even greater effect: Suppose that boundary B bends when it intersects boundary B', so that B is given by the two hyperplanes H_1 and H_2 on the two sides of B'.  If H_1 and B' are known, then H_2 is actually almost completely known already - since the \"bend\" occurs along the intersection of H_1 and B'.  Only a single scalar needs to be determined. Our implementation recalculates the entire hyperplane H_2, but this is in fact unnecessary and leads to a great increase in queries if the input dimension is high.  Reusing this information is straightforward and should certainly be included if the goal is speed over simplicity.\n\nWe did not know of the paper you mention until after submission, and will in our revision describe their approach for one-layer ReLU networks - as well your excellent recent paper: https://arxiv.org/abs/1909.01838", "title": "Query efficiency at second hidden layer"}, "signatures": ["ICLR.cc/2020/Conference/Paper2331/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2331/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["drolnick@seas.upenn.edu", "koerding@gmail.com"], "title": "Identifying Weights and Architectures of Unknown ReLU Networks", "authors": ["David Rolnick", "Konrad P. Kording"], "pdf": "/pdf/0998379b513ba311f63f9adaab03d545b9eb37a4.pdf", "TL;DR": "We show that in many cases it is possible to reconstruct the architecture, weights, and biases of a deep ReLU network given the network's output for specified inputs.", "abstract": "The output of a neural network depends on its parameters in a highly nonlinear way, and it is widely assumed that a network's parameters cannot be identified from its outputs. Here, we show that in many cases it is possible to reconstruct the architecture, weights, and biases of a deep ReLU network given the ability to query the network. ReLU networks are piecewise linear and the boundaries between pieces correspond to inputs for which one of the ReLUs switches between inactive and active states. Thus, first-layer ReLUs can be identified (up to sign and scaling) based on the orientation of their associated hyperplanes. Later-layer ReLU boundaries bend when they cross earlier-layer boundaries and the extent of bending reveals the weights between them. Our algorithm uses this to identify the units in the network and weights connecting them (up to isomorphism). The fact that considerable parts of deep networks can be identified from their outputs has implications for security, neuroscience, and our understanding of neural networks.", "code": "https://osf.io/rf4jt/?view_only=e367ddfd3ad84a44b7abeddd27624a88", "keywords": ["deep neural network", "ReLU", "piecewise linear function", "linear region", "activation region", "weights", "parameters", "architecture"], "paperhash": "rolnick|identifying_weights_and_architectures_of_unknown_relu_networks", "original_pdf": "/attachment/8a1d030b050d8fb8fd13fd217db8ad3070a13d94.pdf", "_bibtex": "@misc{\nrolnick2020identifying,\ntitle={Identifying Weights and Architectures of Unknown Re{\\{}LU{\\}} Networks},\nauthor={David Rolnick and Konrad P. Kording},\nyear={2020},\nurl={https://openreview.net/forum?id=HklFUlBKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HklFUlBKPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2331/Authors", "ICLR.cc/2020/Conference/Paper2331/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2331/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2331/Reviewers", "ICLR.cc/2020/Conference/Paper2331/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2331/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2331/Authors|ICLR.cc/2020/Conference/Paper2331/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142951, "tmdate": 1576860549555, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2331/Authors", "ICLR.cc/2020/Conference/Paper2331/Reviewers", "ICLR.cc/2020/Conference/Paper2331/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2331/-/Official_Comment"}}}, {"id": "Hkl8me6QtH", "original": null, "number": 1, "cdate": 1571176478189, "ddate": null, "tcdate": 1571176478189, "tmdate": 1571176478189, "tddate": null, "forum": "HklFUlBKPB", "replyto": "HklFUlBKPB", "invitation": "ICLR.cc/2020/Conference/Paper2331/-/Public_Comment", "content": {"comment": "Thank you very much for making the code of your algorithm available! It really helps to make the algorithm understandable.\n\nWhen I run the code to identify the weights of a network with an architecture 10-20-30-1 (so that there are 20 units in the first ReLU layer, and 30 units in the second ReLU layer) it takes 70 million queries to identifying the second layer. \n\nIs this to be expected? For context, this is roughly a hundred thousand queries per trainable parameter. In contrast, it takes under 100,000 queries for the first layer (~400 queries per parameter, in line with Figure 3).\n\n(You may also be interested in https://arxiv.org/abs/1807.05185 which gives a very similar algorithm to yours for the case of one-layer neural networks.)", "title": "Query efficiency of the algorithm"}, "signatures": ["~Nicholas_Carlini1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Nicholas_Carlini1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["drolnick@seas.upenn.edu", "koerding@gmail.com"], "title": "Identifying Weights and Architectures of Unknown ReLU Networks", "authors": ["David Rolnick", "Konrad P. Kording"], "pdf": "/pdf/0998379b513ba311f63f9adaab03d545b9eb37a4.pdf", "TL;DR": "We show that in many cases it is possible to reconstruct the architecture, weights, and biases of a deep ReLU network given the network's output for specified inputs.", "abstract": "The output of a neural network depends on its parameters in a highly nonlinear way, and it is widely assumed that a network's parameters cannot be identified from its outputs. Here, we show that in many cases it is possible to reconstruct the architecture, weights, and biases of a deep ReLU network given the ability to query the network. ReLU networks are piecewise linear and the boundaries between pieces correspond to inputs for which one of the ReLUs switches between inactive and active states. Thus, first-layer ReLUs can be identified (up to sign and scaling) based on the orientation of their associated hyperplanes. Later-layer ReLU boundaries bend when they cross earlier-layer boundaries and the extent of bending reveals the weights between them. Our algorithm uses this to identify the units in the network and weights connecting them (up to isomorphism). The fact that considerable parts of deep networks can be identified from their outputs has implications for security, neuroscience, and our understanding of neural networks.", "code": "https://osf.io/rf4jt/?view_only=e367ddfd3ad84a44b7abeddd27624a88", "keywords": ["deep neural network", "ReLU", "piecewise linear function", "linear region", "activation region", "weights", "parameters", "architecture"], "paperhash": "rolnick|identifying_weights_and_architectures_of_unknown_relu_networks", "original_pdf": "/attachment/8a1d030b050d8fb8fd13fd217db8ad3070a13d94.pdf", "_bibtex": "@misc{\nrolnick2020identifying,\ntitle={Identifying Weights and Architectures of Unknown Re{\\{}LU{\\}} Networks},\nauthor={David Rolnick and Konrad P. Kording},\nyear={2020},\nurl={https://openreview.net/forum?id=HklFUlBKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HklFUlBKPB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504181835, "tmdate": 1576860582841, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2331/Authors", "ICLR.cc/2020/Conference/Paper2331/Reviewers", "ICLR.cc/2020/Conference/Paper2331/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2331/-/Public_Comment"}}}], "count": 13}