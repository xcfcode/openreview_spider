{"notes": [{"id": "tbwjUvUzQRU", "original": "uycdvTTarEZ", "number": 1799, "cdate": 1601308198508, "ddate": null, "tcdate": 1601308198508, "tmdate": 1614985770696, "tddate": null, "forum": "tbwjUvUzQRU", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "A Communication Efficient Federated Kernel $k$-Means", "authorids": ["~Xiaochen_Zhou2", "wxudong@sjtu.edu.cn"], "authors": ["Xiaochen Zhou", "Xudong Wang"], "keywords": ["federated learning", "kernel $k$-means", "communication efficient"], "abstract": "A federated kernel $k$-means algorithm is developed in this paper. This algorithm resolves two challenging issues: 1) how to distributedly solve the optimization problem of kernel $k$-means under federated settings; 2) how to maintain communication efficiency in the algorithm. To tackle the first challenge, a distributed stochastic proximal gradient descent (DSPGD) algorithm is developed to determine an approximated solution to the optimization problem of kernel $k$-means. To tackle the second challenge, a communication efficient mechanism (CEM) is designed to reduce the communication cost. Besides, the federated kernel $k$-means provides two levels of privacy preservation: 1) users\u2019 local data are not exposed to the cloud server; 2) the cloud server cannot recover users\u2019 local data from the local computational results via matrix operations. Theoretical analysis shows: 1) DSPGD with CEM converges with an $O(1/T)$ rate, where $T$ is the number of iterations; 2) the communication cost of DSPGD with CEM is unrelated to the number of data samples; 3) the clustering quality of the federated kernel $k$-means approaches that of the standard kernel $k$-means, with a $(1+\\epsilon)$ approximate ratio. The experimental results show that the federated kernel $k$-means achieves the highest clustering quality with the communication cost reduced by more than $60\\%$ in most cases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|a_communication_efficient_federated_kernel_kmeans", "supplementary_material": "/attachment/226a3d584fed29f35f9dfff1b45b782aac8df0d5.zip", "pdf": "/pdf/918fbbf03a84f52157cc40ec901359434a10734c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=IyB_e1BjGJ", "_bibtex": "@misc{\nzhou2021a,\ntitle={A Communication Efficient Federated Kernel {\\$}k{\\$}-Means},\nauthor={Xiaochen Zhou and Xudong Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=tbwjUvUzQRU}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 17, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "xTnVJGwsC-4", "original": null, "number": 1, "cdate": 1610040361764, "ddate": null, "tcdate": 1610040361764, "tmdate": 1610473951973, "tddate": null, "forum": "tbwjUvUzQRU", "replyto": "tbwjUvUzQRU", "invitation": "ICLR.cc/2021/Conference/Paper1799/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper presents a approach to the distributed kernel k-means problem using a combination of random features to efficiently approximate the kernel matrix, a distributed stochastic proximal gradient algorithm which calls a distributed lanczos algorithm as a primitive to find a low-rank approximation to the kernel matrix, and additional compression to reduce the cost of the communications. \n\nThe algorithm is a novel combination of prior ideas, and empirically works well. However, the claimed theoretical convergence rate is not convincing: e.g., the convergence rate depends on the Frobenius norm of the error in approximating the kernel with random feature maps, which is O(n^2) for a problem with n data samples. This implies that O(n^2) iterations must be used in the algorithm, which is already slower than a naive approach to kernel k-means.\n\nThis paper takes a promising approach to the problem, but as the potential contribution lies in combining prior ideas in order to obtain a provably guaranteed approximate solution to the distributed kernel k-means problem, and the proposed algorithm was not shown to satisfy this promise, the recommendation is to reject."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Communication Efficient Federated Kernel $k$-Means", "authorids": ["~Xiaochen_Zhou2", "wxudong@sjtu.edu.cn"], "authors": ["Xiaochen Zhou", "Xudong Wang"], "keywords": ["federated learning", "kernel $k$-means", "communication efficient"], "abstract": "A federated kernel $k$-means algorithm is developed in this paper. This algorithm resolves two challenging issues: 1) how to distributedly solve the optimization problem of kernel $k$-means under federated settings; 2) how to maintain communication efficiency in the algorithm. To tackle the first challenge, a distributed stochastic proximal gradient descent (DSPGD) algorithm is developed to determine an approximated solution to the optimization problem of kernel $k$-means. To tackle the second challenge, a communication efficient mechanism (CEM) is designed to reduce the communication cost. Besides, the federated kernel $k$-means provides two levels of privacy preservation: 1) users\u2019 local data are not exposed to the cloud server; 2) the cloud server cannot recover users\u2019 local data from the local computational results via matrix operations. Theoretical analysis shows: 1) DSPGD with CEM converges with an $O(1/T)$ rate, where $T$ is the number of iterations; 2) the communication cost of DSPGD with CEM is unrelated to the number of data samples; 3) the clustering quality of the federated kernel $k$-means approaches that of the standard kernel $k$-means, with a $(1+\\epsilon)$ approximate ratio. The experimental results show that the federated kernel $k$-means achieves the highest clustering quality with the communication cost reduced by more than $60\\%$ in most cases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|a_communication_efficient_federated_kernel_kmeans", "supplementary_material": "/attachment/226a3d584fed29f35f9dfff1b45b782aac8df0d5.zip", "pdf": "/pdf/918fbbf03a84f52157cc40ec901359434a10734c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=IyB_e1BjGJ", "_bibtex": "@misc{\nzhou2021a,\ntitle={A Communication Efficient Federated Kernel {\\$}k{\\$}-Means},\nauthor={Xiaochen Zhou and Xudong Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=tbwjUvUzQRU}\n}"}, "tags": [], "invitation": {"reply": {"forum": "tbwjUvUzQRU", "replyto": "tbwjUvUzQRU", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040361749, "tmdate": 1610473951956, "id": "ICLR.cc/2021/Conference/Paper1799/-/Decision"}}}, {"id": "-bzpPnS8Vmp", "original": null, "number": 1, "cdate": 1603878231846, "ddate": null, "tcdate": 1603878231846, "tmdate": 1606807716488, "tddate": null, "forum": "tbwjUvUzQRU", "replyto": "tbwjUvUzQRU", "invitation": "ICLR.cc/2021/Conference/Paper1799/-/Official_Review", "content": {"title": "Review", "review": "Summary:\n\nThis paper proposes a federated kernel k-means algorithm (FK k-means). The algorithm consists of two parts: a distributed stochastic proximal gradient descent (DSPGD) update rule, and a communication efficient mechanism (CEM) to reduce the communication cost. Instead of solving the original integer programming problem, the authors consider the relaxed convex stochastic composite optimization (SCO) problem and extends it to a distributed setting. \n\nThe main contribution is that the authors show, both theoretically and experimentally, that their proposed algorithm converges to the true solution of the SCO problem at O(1/T) rate. It is also proved that the communication cost does not grow with the number of samples N. In addition the authors characterize the error ratio between their algorithm and the original k-means. At the end the authors prove that the central server cannot recover the feature matrices. The authors compare their algorithm with other k-means algorithms on several datasets. \n\n\nPros:\n\n- The federated setting of k-means with privacy preservation property is interesting and relatively new. \n\n- The presentation of the theorems is very clear. The authors put interpretation after each statement and it is easy to follow most of the time. In particular it is shown that the proposed algorithm approaches the baseline scalable kernel k-means algorithm as T increases, both theoretically and experimentally.\n\n- I appreciate the discussion of the motivation and related works in the first two section of the paper. \n\n\nCons:\n\n- The whole Section 4 of the paper is filled with technical details and hard to follow. Considering the page limit, why not put the algorithms here, and put the details in the appendix? In addition the authors can consider putting important observations as lemmas. \n\n- It seems the proof techniques are somehow standard. The whole proof of Theorem 1 feels like an extension to [Zhang et al., 2016] in a distributed setting. I did not check the other proofs though. \n\n- My biggest concern is about novelty. Currently the proposed algorithm is heavily influenced by [Zhang et al., 2016] and [Wang et al., 2019]. If that is not the case, the authors can consider including a table to illustrate the difference between the algorithms. \n\n------------------------------\n\nPost-rebuttal:\n\nI appreciate the authors' feedbacks. However the authors' response to the proof of Theorem 1 is not the most convincing, which is a big part of the claimed contribution. ", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper1799/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1799/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Communication Efficient Federated Kernel $k$-Means", "authorids": ["~Xiaochen_Zhou2", "wxudong@sjtu.edu.cn"], "authors": ["Xiaochen Zhou", "Xudong Wang"], "keywords": ["federated learning", "kernel $k$-means", "communication efficient"], "abstract": "A federated kernel $k$-means algorithm is developed in this paper. This algorithm resolves two challenging issues: 1) how to distributedly solve the optimization problem of kernel $k$-means under federated settings; 2) how to maintain communication efficiency in the algorithm. To tackle the first challenge, a distributed stochastic proximal gradient descent (DSPGD) algorithm is developed to determine an approximated solution to the optimization problem of kernel $k$-means. To tackle the second challenge, a communication efficient mechanism (CEM) is designed to reduce the communication cost. Besides, the federated kernel $k$-means provides two levels of privacy preservation: 1) users\u2019 local data are not exposed to the cloud server; 2) the cloud server cannot recover users\u2019 local data from the local computational results via matrix operations. Theoretical analysis shows: 1) DSPGD with CEM converges with an $O(1/T)$ rate, where $T$ is the number of iterations; 2) the communication cost of DSPGD with CEM is unrelated to the number of data samples; 3) the clustering quality of the federated kernel $k$-means approaches that of the standard kernel $k$-means, with a $(1+\\epsilon)$ approximate ratio. The experimental results show that the federated kernel $k$-means achieves the highest clustering quality with the communication cost reduced by more than $60\\%$ in most cases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|a_communication_efficient_federated_kernel_kmeans", "supplementary_material": "/attachment/226a3d584fed29f35f9dfff1b45b782aac8df0d5.zip", "pdf": "/pdf/918fbbf03a84f52157cc40ec901359434a10734c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=IyB_e1BjGJ", "_bibtex": "@misc{\nzhou2021a,\ntitle={A Communication Efficient Federated Kernel {\\$}k{\\$}-Means},\nauthor={Xiaochen Zhou and Xudong Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=tbwjUvUzQRU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "tbwjUvUzQRU", "replyto": "tbwjUvUzQRU", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1799/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538110470, "tmdate": 1606915761703, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1799/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1799/-/Official_Review"}}}, {"id": "FG_d-Jy3fm7", "original": null, "number": 2, "cdate": 1603900772790, "ddate": null, "tcdate": 1603900772790, "tmdate": 1606739965242, "tddate": null, "forum": "tbwjUvUzQRU", "replyto": "tbwjUvUzQRU", "invitation": "ICLR.cc/2021/Conference/Paper1799/-/Official_Review", "content": {"title": "The paper is not novel by itself but is technically solid and proposed a new distributed clustering algorithm.", "review": "The paper proposed a new distributed kernel k-means algorithm that has lower communication overhead compared to the available methods while does not require transmitting the data samples from agents to the master node and therefore claims to preserve the privacy of agents. The paper is rather difficult to follow and the novelty of the paper is not very clear to me. It seems to me that the major contribution of the paper is to come up with efficient tricks during the implementation of the distributed Lanczos algorithm (DLA) to find the eigenvalues of the kernel approximated by using the well-known random Fourier features (Rahimi, nips 2008). So the end result is not something utterly novel but rather an efficient way of utilizing available tools to design a new distributed algorithm.\n\nThere are several points that I think need to be clarified:\n1. The literature review does not clearly illustrate the contribution of the paper. For instance, in Section 2.1, the author says \"However, these algorithms are designed with an assumption that they are executed at the cloud server where the constraint on transmissions of the raw data samples does not exist.\" Does this mean that your work is the first to constrain the transmission of data samples? If so you should clearly say so. Besides, why transmitting the raw data sample is important here? Is it due to privacy issues? Because the way I see it, one does not need to transmit the raw data samples but rather its local kernel matrix K, and doing so does not necessarily endanger the privacy of an agent, since one cannot easily recover data samples from K.\n\n2. In Section 6.2, the author says \"According to the results in the four subfigures, it is shown that CEM can reduce communication cost of DSPGD by more than 95%.\" It would be interesting to explain what values of r_t, Q_0, and Q_1 leads to this improvement and why. \n\n3. The 60% communication cost reduction mentioned throughout the paper seems a bit exaggerated as it does not consistently happen. It only happens for the MNIST dataset in Figure 3(b). Do you know why? My guess is that due to the high sparsity level of MNIST samples, the kernel matrix might contain a lot of zeros leading to such behavior. \n\n4. What about the drawbacks of the proposed method? What I see is a very complicated algorithm that requires heavy computational resources at each agent, which makes it unsuitable for the toy example explained in the introduction about smartphones.\n\nAfter rebuttal: I thank the author for their response but I have to lower my rating by one step after reading the comments of Reviewer2. ", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1799/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1799/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Communication Efficient Federated Kernel $k$-Means", "authorids": ["~Xiaochen_Zhou2", "wxudong@sjtu.edu.cn"], "authors": ["Xiaochen Zhou", "Xudong Wang"], "keywords": ["federated learning", "kernel $k$-means", "communication efficient"], "abstract": "A federated kernel $k$-means algorithm is developed in this paper. This algorithm resolves two challenging issues: 1) how to distributedly solve the optimization problem of kernel $k$-means under federated settings; 2) how to maintain communication efficiency in the algorithm. To tackle the first challenge, a distributed stochastic proximal gradient descent (DSPGD) algorithm is developed to determine an approximated solution to the optimization problem of kernel $k$-means. To tackle the second challenge, a communication efficient mechanism (CEM) is designed to reduce the communication cost. Besides, the federated kernel $k$-means provides two levels of privacy preservation: 1) users\u2019 local data are not exposed to the cloud server; 2) the cloud server cannot recover users\u2019 local data from the local computational results via matrix operations. Theoretical analysis shows: 1) DSPGD with CEM converges with an $O(1/T)$ rate, where $T$ is the number of iterations; 2) the communication cost of DSPGD with CEM is unrelated to the number of data samples; 3) the clustering quality of the federated kernel $k$-means approaches that of the standard kernel $k$-means, with a $(1+\\epsilon)$ approximate ratio. The experimental results show that the federated kernel $k$-means achieves the highest clustering quality with the communication cost reduced by more than $60\\%$ in most cases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|a_communication_efficient_federated_kernel_kmeans", "supplementary_material": "/attachment/226a3d584fed29f35f9dfff1b45b782aac8df0d5.zip", "pdf": "/pdf/918fbbf03a84f52157cc40ec901359434a10734c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=IyB_e1BjGJ", "_bibtex": "@misc{\nzhou2021a,\ntitle={A Communication Efficient Federated Kernel {\\$}k{\\$}-Means},\nauthor={Xiaochen Zhou and Xudong Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=tbwjUvUzQRU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "tbwjUvUzQRU", "replyto": "tbwjUvUzQRU", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1799/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538110470, "tmdate": 1606915761703, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1799/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1799/-/Official_Review"}}}, {"id": "JF3IaHcgkSd", "original": null, "number": 3, "cdate": 1603909720928, "ddate": null, "tcdate": 1603909720928, "tmdate": 1606316628360, "tddate": null, "forum": "tbwjUvUzQRU", "replyto": "tbwjUvUzQRU", "invitation": "ICLR.cc/2021/Conference/Paper1799/-/Official_Review", "content": {"title": "Not better than baseline; main theorem is wrong", "review": "##########################################################################\n\nSummary:\n\nThis paper proposes a new method and two algorithms for solving kernel k-means. The contribution is that the algorithms converge to the optimal solution. The downsides are 1) the method is not better than a simple baseline (i.e., random features + distributed power method) and 2) the main theorem (Theorem 3) is wrong.\n\n\n##########################################################################\n\nReasons for score:\n\nI vote for rejection for two reasons. First, the proposed method appears not useful. The same problem can be solved in a much simpler and faster way. Second, the main theorem, Theorem 3, is wrong.\n\n\n##########################################################################\n\nPros:\n\n+ This paper develops a new method of distributed kernel k-means. The method is new, although I do not find it very useful.\n\n+ This paper proves that two algorithms can correctly solve the trace-norm regularized problem in Section 4.1.\n\n\n##########################################################################\n\nCons:\n\n1. First and foremost, I do not see a good reason for using the proposed algorithm. The goal of the algorithm is to find the top singular vectors of the random features, $A$.\n\n    - The solution to the trace norm regularized problem, $Z^*$, has the same singular vectors as K. By finding $Z^*$, you can find the eigenvectors of $K$.\n\n    - However, the same goal could be achieved in an easier and less expensive way, i.e., random features + distributed power method. Random features are naturally distributed among the clients. Their truncated SVD could be found by the distributed power method or Krylov subspace methods. Truncated SVD is easier than solved the proposed trace norm regularized problem because the latter uses SVS which repeatedly performs SVD.\n\n2. I am very surprised that Theorem 3 does not reply on $D$ (the number of random features). So I checked some of the proofs. I found Theorem 3, which is the main theorem, is wrong.\n\n    - $Z^*$ has the same top eigenvectors as $\\xi$. But $Z^*$ may not have the same as $K$.\n\n    - The proof of Theorem 3 relies on that $Z^*$ has the same eigenvectors as $K$. This is wrong.\n\n\n3. The description of the algorithm is difficult to follow. I\u2019d suggest splitting algorithm description into 3 paragraphs: 1) Client-side computation, 2) server-side computation, and 3) communications.\n\n\n\n\n\nTypos:\n\n17th page: \"The following two lemmas will be used in the proof of Theorem 4.\u201d Do you mean Theorem 3?\n\n##########################################################################\n\nUpdates after discussing with the authors\n\n1. The paper is not very clearly written, and I had misunderstandings. Some of my comments above are not right.\n\n2. However, I will not change my rating. I found the convergence rates stated in the paper are misleading. The paper claims $O(1/T)$ convergence rate. In fact, this is WRONG. The authors assume the Frobenius and trace norms of $n\\times n$ matrices are CONSTANTS. This is not possible. The norms are $O(n)$. Simple arguments can show $|| \\xi ||_F = G$ is $O(n)$.\n\n3. Based on the right assumption that $|| \\xi ||_F = G = O(n)$, the required number of iterations is $T = O(n^2)$. The algorithm is not communication-efficient. It is more expensive than communicating the $n\\times n$ kernel matrices.\n\n4. After reading my comments, the authors changed their notation from $G$ to $\\gamma$, $C$, $G$, and $H$. They are also Frobenius and trace norms of $n\\times n$ matrices. The authors assume $\\gamma$, $C$, $G$ and $H$ are constants. This is WRONG. They are $O(n)$. \n\n    - For example, if they use the bound of Rahimi and Recht,  then $|| \\xi - K ||_F^2 = G^2$ is $O(n^2)$.  A bound as good as $|| \\xi - K ||_F^2 = O(n)$ would surprise me; if the authors know such a bound, please let me know. \n\n    - Let me strengthen my point again: IT IS WRONG TO ASSUME MATRIX NORMS ARE CONSTANTS! If the authors can prove they are constants, they need to show me the proofs. If they cannot, they should assume Frobenius norm and trace norm are $O(n)$.\n\n\n\n", "rating": "1: Trivial or wrong", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1799/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1799/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Communication Efficient Federated Kernel $k$-Means", "authorids": ["~Xiaochen_Zhou2", "wxudong@sjtu.edu.cn"], "authors": ["Xiaochen Zhou", "Xudong Wang"], "keywords": ["federated learning", "kernel $k$-means", "communication efficient"], "abstract": "A federated kernel $k$-means algorithm is developed in this paper. This algorithm resolves two challenging issues: 1) how to distributedly solve the optimization problem of kernel $k$-means under federated settings; 2) how to maintain communication efficiency in the algorithm. To tackle the first challenge, a distributed stochastic proximal gradient descent (DSPGD) algorithm is developed to determine an approximated solution to the optimization problem of kernel $k$-means. To tackle the second challenge, a communication efficient mechanism (CEM) is designed to reduce the communication cost. Besides, the federated kernel $k$-means provides two levels of privacy preservation: 1) users\u2019 local data are not exposed to the cloud server; 2) the cloud server cannot recover users\u2019 local data from the local computational results via matrix operations. Theoretical analysis shows: 1) DSPGD with CEM converges with an $O(1/T)$ rate, where $T$ is the number of iterations; 2) the communication cost of DSPGD with CEM is unrelated to the number of data samples; 3) the clustering quality of the federated kernel $k$-means approaches that of the standard kernel $k$-means, with a $(1+\\epsilon)$ approximate ratio. The experimental results show that the federated kernel $k$-means achieves the highest clustering quality with the communication cost reduced by more than $60\\%$ in most cases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|a_communication_efficient_federated_kernel_kmeans", "supplementary_material": "/attachment/226a3d584fed29f35f9dfff1b45b782aac8df0d5.zip", "pdf": "/pdf/918fbbf03a84f52157cc40ec901359434a10734c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=IyB_e1BjGJ", "_bibtex": "@misc{\nzhou2021a,\ntitle={A Communication Efficient Federated Kernel {\\$}k{\\$}-Means},\nauthor={Xiaochen Zhou and Xudong Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=tbwjUvUzQRU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "tbwjUvUzQRU", "replyto": "tbwjUvUzQRU", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1799/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538110470, "tmdate": 1606915761703, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1799/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1799/-/Official_Review"}}}, {"id": "7qPVijr0R6r", "original": null, "number": 12, "cdate": 1606301263948, "ddate": null, "tcdate": 1606301263948, "tmdate": 1606305458697, "tddate": null, "forum": "tbwjUvUzQRU", "replyto": "xYnrYVSzlYM", "invitation": "ICLR.cc/2021/Conference/Paper1799/-/Official_Comment", "content": {"title": "Clarification to the convergence rate and responses to Q1~Q5", "comment": "Thanks a lot for your constructive feedback.\n\nWe agree that the assumption that $||\\mathrm{\\xi}||_F \\leq L$ is not appropriate. In fact, we can eliminate this assumption without affecting the conclusions of Theorems 1 and 3, as explained below.\n\nAfter removing this assumption, the result of Theorem 1 becomes\n$$\n||\\mathrm{Z}\\_{T+1} - \\mathrm{Z}\\_{T}||\\_F^2 \\leq \\frac{4}{T}{\\left( C^2 + \\lambda \\gamma + 2G^2\\tau + \\frac{2}{3}GH\\tau +GH\\right)},\n$$\n\nwhere $\\gamma = \\max\\_{t \\in [T]}{||\\mathrm{Z}\\_t||\\_\\star}$, $C^2=\\max\\_{t\\in[T]}{||\\mathrm{Z}\\_{t} - \\mathrm{\\xi}\\_{t}||\\_F^2}$,  and assume that $||\\mathrm{\\xi}\\_{t} - \\mathrm{K}||\\_F \\leq G$ and $||\\mathrm{Z}\\_t - \\mathrm{Z}^\\star||\\_F \\leq H$ for all $t>2$.\n\nFrom this result we can see that given $T$, $C$, $\\gamma$, $G$, and $H$ are all constants, so during the $T$ iterations the asymptotic convergence rate of DSPGD is still $O(1/T)$, and thus the result in Theorem 3 also holds.\n\nHere are the responses to the new comments\n\n1.Assume the statements in the authors' response are correct. Theorem 3 is not correctly stated. Theorem 3 does not say it depends on any assumption other than Def 1. I suppose the theorem assumes $E[\\xi] = K$ and $||\\xi||_F < L$, etc. Such assumptions must not be hidden. If you made the assumptions explicit, I would not have the misunderstandings.\n\nFor Theorem 1 and Theorem 3, we have added the assumptions used in their proofs to the statements of the corresponding theorems.\n\n2.Theorem 1 and Theorem 3 implicitly assume $||\\xi||_F < L$ is a constant. This is wrong. $\\xi$ has $n^2$\n elements; the elements are unbiased estimates of kernel functions which are constants. Thus $||\\xi||_F^2 = O(n^2)$. $L$ is $O(n)$, not a constant.\n\nWe agree with this comment so we have removed the assumption $||\\xi||_F < L$ from Theorem 1 and revised the result of Theorem 1.\n\n3.Because $L=O(n)$, the convergence rate in Theorem 1 is not $O(1/T)$. Instead, the rate is $O(n^2/T)$\n. It means you have to set $T=O(n^2)$. Then the overall communication complexity is worse than sending $n \\times n$ kernel matrices. I am wondering if the algorithm is useful.\n\nAccording to the new result in Theorem 1, we can see that the convergence rate of DSPGD is still $O(1/T)$. Thus, we do not need to set $T=O(n^2)$ for the convergence of DSPGD\n\n4.The same problem is with Theorem 3. Setting $T$ to $O(s)$ is not enough. You need to set $T$ to $O(sn^2)$.\n\nSince convergence rate of DSPGD is still $O(1/T)$, setting $T$ to $O(s)$ is enough for FK $k$-means\n\n5.There are still many typos in the proofs. You need to carefully proofread the proofs. Otherwise, I could not assume the proofs are correct.\n\nWe have proofread our paper and revised these typos."}, "signatures": ["ICLR.cc/2021/Conference/Paper1799/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1799/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Communication Efficient Federated Kernel $k$-Means", "authorids": ["~Xiaochen_Zhou2", "wxudong@sjtu.edu.cn"], "authors": ["Xiaochen Zhou", "Xudong Wang"], "keywords": ["federated learning", "kernel $k$-means", "communication efficient"], "abstract": "A federated kernel $k$-means algorithm is developed in this paper. This algorithm resolves two challenging issues: 1) how to distributedly solve the optimization problem of kernel $k$-means under federated settings; 2) how to maintain communication efficiency in the algorithm. To tackle the first challenge, a distributed stochastic proximal gradient descent (DSPGD) algorithm is developed to determine an approximated solution to the optimization problem of kernel $k$-means. To tackle the second challenge, a communication efficient mechanism (CEM) is designed to reduce the communication cost. Besides, the federated kernel $k$-means provides two levels of privacy preservation: 1) users\u2019 local data are not exposed to the cloud server; 2) the cloud server cannot recover users\u2019 local data from the local computational results via matrix operations. Theoretical analysis shows: 1) DSPGD with CEM converges with an $O(1/T)$ rate, where $T$ is the number of iterations; 2) the communication cost of DSPGD with CEM is unrelated to the number of data samples; 3) the clustering quality of the federated kernel $k$-means approaches that of the standard kernel $k$-means, with a $(1+\\epsilon)$ approximate ratio. The experimental results show that the federated kernel $k$-means achieves the highest clustering quality with the communication cost reduced by more than $60\\%$ in most cases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|a_communication_efficient_federated_kernel_kmeans", "supplementary_material": "/attachment/226a3d584fed29f35f9dfff1b45b782aac8df0d5.zip", "pdf": "/pdf/918fbbf03a84f52157cc40ec901359434a10734c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=IyB_e1BjGJ", "_bibtex": "@misc{\nzhou2021a,\ntitle={A Communication Efficient Federated Kernel {\\$}k{\\$}-Means},\nauthor={Xiaochen Zhou and Xudong Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=tbwjUvUzQRU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "tbwjUvUzQRU", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1799/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1799/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1799/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1799/Authors|ICLR.cc/2021/Conference/Paper1799/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1799/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855621, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1799/-/Official_Comment"}}}, {"id": "6VfuZqHQsGD", "original": null, "number": 13, "cdate": 1606304805030, "ddate": null, "tcdate": 1606304805030, "tmdate": 1606304805030, "tddate": null, "forum": "tbwjUvUzQRU", "replyto": "tbwjUvUzQRU", "invitation": "ICLR.cc/2021/Conference/Paper1799/-/Official_Comment", "content": {"title": "Summary of the revision of our paper", "comment": "We thank all the reviewers for their constructive suggestions on our paper.\n\nWe have resolved all the issues mentioned in the reviewers' comments. In addition, we have revised our paper as follows (marked by the red words in the revised version of our paper).\n\n1.We have added the missing information to the abstract and revised the confusing statements in the abstract.\n\n2.We have revised the description of FK $k$-means in the introduction by pointing out the key problems of designing FK $k$-means and how we solve these problems step by step.\n\n3.We have added the comparison between our method with stochastic PCA and scalable kernel $k$-means in Section 2.\n\n4.We have reorganized the description of FK $k$-means in Section 4.1 and Section 4.2, and have moved the pseudo code back to Section 4.2.\n\n5.We have revised the main results of Theorem 1.\n\n6.We have reorganized the presentation of theoretical results and experimental results to match the challenges on designing FK $k$-means."}, "signatures": ["ICLR.cc/2021/Conference/Paper1799/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1799/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Communication Efficient Federated Kernel $k$-Means", "authorids": ["~Xiaochen_Zhou2", "wxudong@sjtu.edu.cn"], "authors": ["Xiaochen Zhou", "Xudong Wang"], "keywords": ["federated learning", "kernel $k$-means", "communication efficient"], "abstract": "A federated kernel $k$-means algorithm is developed in this paper. This algorithm resolves two challenging issues: 1) how to distributedly solve the optimization problem of kernel $k$-means under federated settings; 2) how to maintain communication efficiency in the algorithm. To tackle the first challenge, a distributed stochastic proximal gradient descent (DSPGD) algorithm is developed to determine an approximated solution to the optimization problem of kernel $k$-means. To tackle the second challenge, a communication efficient mechanism (CEM) is designed to reduce the communication cost. Besides, the federated kernel $k$-means provides two levels of privacy preservation: 1) users\u2019 local data are not exposed to the cloud server; 2) the cloud server cannot recover users\u2019 local data from the local computational results via matrix operations. Theoretical analysis shows: 1) DSPGD with CEM converges with an $O(1/T)$ rate, where $T$ is the number of iterations; 2) the communication cost of DSPGD with CEM is unrelated to the number of data samples; 3) the clustering quality of the federated kernel $k$-means approaches that of the standard kernel $k$-means, with a $(1+\\epsilon)$ approximate ratio. The experimental results show that the federated kernel $k$-means achieves the highest clustering quality with the communication cost reduced by more than $60\\%$ in most cases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|a_communication_efficient_federated_kernel_kmeans", "supplementary_material": "/attachment/226a3d584fed29f35f9dfff1b45b782aac8df0d5.zip", "pdf": "/pdf/918fbbf03a84f52157cc40ec901359434a10734c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=IyB_e1BjGJ", "_bibtex": "@misc{\nzhou2021a,\ntitle={A Communication Efficient Federated Kernel {\\$}k{\\$}-Means},\nauthor={Xiaochen Zhou and Xudong Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=tbwjUvUzQRU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "tbwjUvUzQRU", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1799/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1799/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1799/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1799/Authors|ICLR.cc/2021/Conference/Paper1799/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1799/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855621, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1799/-/Official_Comment"}}}, {"id": "xYnrYVSzlYM", "original": null, "number": 11, "cdate": 1606282300940, "ddate": null, "tcdate": 1606282300940, "tmdate": 1606282300940, "tddate": null, "forum": "tbwjUvUzQRU", "replyto": "10xrRBZCqTc", "invitation": "ICLR.cc/2021/Conference/Paper1799/-/Official_Comment", "content": {"title": "The actual convergence rate appears very different from the claimed", "comment": "Thanks for the clarifications. I admit that I had some misunderstandings. I examined the pointed theorems and the proofs more carefully and found more problems.\n\n1. Assume the statements in the authors' response are correct. Theorem 3 is not correctly stated. Theorem 3 does not say it depends on any assumption other than Def 1. I suppose the theorem assumes $E[\\xi] = K$ and $|| \\xi ||_F < L$, etc. Such assumptions must not be hidden. If you made the assumptions explicit, I would not have the misunderstandings.\n\n2. Theorem 1 and Theorem 3 implicitly assume $|| \\xi ||_F < L$ is a constant. This is wrong. \n$\\xi$ has $n^2$ elements; the elements are unbiased estimates of kernel functions which are constants. Thus $|| \\xi ||_F^2 = O(n^2)$. $L$ is $O(n)$, not a constant.\n\n3. Because $L=O(n)$, the convergence rate in Theorem 1 is not $O(1/T)$. Instead, the rate is $O(n^2 / T)$. It means you have to set $T = O(n^2)$. Then the overall communication complexity is worse than sending $n\\times n$ kernel matrices. I am wondering if the algorithm is useful.\n\n4. The same problem is with Theorem 3. Setting $T$ to $O(s)$ is not enough. You need to set $T$ to $O(s n^2)$.\n\n5. There are still many typos in the proofs. You need to carefully proofread the proofs. Otherwise, I could not assume the proofs are correct. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1799/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1799/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Communication Efficient Federated Kernel $k$-Means", "authorids": ["~Xiaochen_Zhou2", "wxudong@sjtu.edu.cn"], "authors": ["Xiaochen Zhou", "Xudong Wang"], "keywords": ["federated learning", "kernel $k$-means", "communication efficient"], "abstract": "A federated kernel $k$-means algorithm is developed in this paper. This algorithm resolves two challenging issues: 1) how to distributedly solve the optimization problem of kernel $k$-means under federated settings; 2) how to maintain communication efficiency in the algorithm. To tackle the first challenge, a distributed stochastic proximal gradient descent (DSPGD) algorithm is developed to determine an approximated solution to the optimization problem of kernel $k$-means. To tackle the second challenge, a communication efficient mechanism (CEM) is designed to reduce the communication cost. Besides, the federated kernel $k$-means provides two levels of privacy preservation: 1) users\u2019 local data are not exposed to the cloud server; 2) the cloud server cannot recover users\u2019 local data from the local computational results via matrix operations. Theoretical analysis shows: 1) DSPGD with CEM converges with an $O(1/T)$ rate, where $T$ is the number of iterations; 2) the communication cost of DSPGD with CEM is unrelated to the number of data samples; 3) the clustering quality of the federated kernel $k$-means approaches that of the standard kernel $k$-means, with a $(1+\\epsilon)$ approximate ratio. The experimental results show that the federated kernel $k$-means achieves the highest clustering quality with the communication cost reduced by more than $60\\%$ in most cases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|a_communication_efficient_federated_kernel_kmeans", "supplementary_material": "/attachment/226a3d584fed29f35f9dfff1b45b782aac8df0d5.zip", "pdf": "/pdf/918fbbf03a84f52157cc40ec901359434a10734c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=IyB_e1BjGJ", "_bibtex": "@misc{\nzhou2021a,\ntitle={A Communication Efficient Federated Kernel {\\$}k{\\$}-Means},\nauthor={Xiaochen Zhou and Xudong Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=tbwjUvUzQRU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "tbwjUvUzQRU", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1799/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1799/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1799/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1799/Authors|ICLR.cc/2021/Conference/Paper1799/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1799/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855621, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1799/-/Official_Comment"}}}, {"id": "KSlRzgmVvc_", "original": null, "number": 7, "cdate": 1606218563515, "ddate": null, "tcdate": 1606218563515, "tmdate": 1606219654163, "tddate": null, "forum": "tbwjUvUzQRU", "replyto": "rm0QbwUdiYW", "invitation": "ICLR.cc/2021/Conference/Paper1799/-/Official_Comment", "content": {"title": "Response to Q4 and Q5", "comment": "4. The $60$% communication cost reduction mentioned throughout the paper seems a bit exaggerated as it does not consistently happen. It only happens for the MNIST dataset in Figure 3(b). Do you know why? My guess is that due to the high sparsity level of MNIST samples, the kernel matrix might contain a lot of zeros leading to such behavior.\n\nIndeed the $60$% communication cost reduction does not hold for the Covtype dataset. \n  However, we would like to point out that the $60\\%$ communication cost reduction not only holds for the MNIST dataset, but also holds for the Mushrooms dataset and the Smartphone dataset, which can be seen from Figure 3(a) and 3(d). \n\n  The reasons for the $60$% communication cost reduction are explained as follows. First, for each of the three datasets, the sparsity of the data samples leads to many zero items in its kernel matrix $\\mathrm{K}$. As a result, the remaining non-zero items in $\\mathrm{K}$ can be approached with only a small $D$ and a small number of iterations of DSPGD. Second, among the top eigenvalues of $\\mathrm{K}$, the gap between two adjacent eigenvalues (e.g., the $i$-th eigenvalues and the $i+1$-th eigenvalues) is large. As a result, these top eigenvalues are approached with only a small number of Lanczos iterations, which reduces the communication cost in each iteration of DSPGD.\n\n5. What about the drawbacks of the proposed method? What I see is a very complicated algorithm that requires heavy computational resources at each agent, which makes it unsuitable for the toy example explained in the introduction about smartphones.\n\nThe drawbacks of FK $k$-means mainly come from two aspects. First, FK $k$-means is a synchronous algorithm. The efficiency of the algorithm can be affected by stragglers. Second, FK $k$-means requires all the users to participate the whole training process. If one user drops out halfway, the algorithm may have to be restarted.\n\n  In FK $k$-means (we consider the communication efficient mechanism (CEM) here), the algorithm conducted at each user's device has low computational cost, and it is easy for the common smartphone CPUs with tens of GFLOPS to execute the algorithm. The computational cost is analyzed as follows. In the $t$-th iteration of DSPGD with CEM, the $m$-th user with $n_m$ data samples needs to compute ${\\mathrm{g}}_q = {\\mathrm{W}}_t[m]^\\top {\\mathrm{W}}_t[m] {\\mathrm{c}}_q$ for $Q_t$ iterations. ${\\mathrm{W}}_t[m]^\\top {\\mathrm{W}}_t[m]$ can be only computed once with $(2 n_m -1)(r_t + D)^2$ floating-point operations. Computing one ${\\mathrm{g}}_q$ requires $2(r_t + D)^2 - (r_t + D)$ floating-point operations. Thus, one iteration of DSPGD with CEM requires $(2 n_m + 2 Q_t -1)(r_t + D)^2 - Q_t (r_t + D)$ floating-point operations at the $m$-th user's device. Among the four datasets, the MNIST dataset requires the largest communication cost. We then computes an upper bound of its communication cost by setting $n_m$, $r_t$, $D$, and $Q_t$ to their maximal values or upper bound values, i.e., $n_m=12000$, $r_t=20$, $D=200$, and $Q_t=26$. As a result, the computational cost of one iteration of DSPGD with CEM approximates $1$ giga floating-point operations that can be completed by common smartphone CPUs within tens of milliseconds.\n\nReference\n\n[1] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.\nCommunication-efficient learning of deep networks from decentralized data. In Proceedings of\nthe 20th International Conference on Artificial Intelligence and Statistics, pp. 1273\u20131282, 2017."}, "signatures": ["ICLR.cc/2021/Conference/Paper1799/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1799/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Communication Efficient Federated Kernel $k$-Means", "authorids": ["~Xiaochen_Zhou2", "wxudong@sjtu.edu.cn"], "authors": ["Xiaochen Zhou", "Xudong Wang"], "keywords": ["federated learning", "kernel $k$-means", "communication efficient"], "abstract": "A federated kernel $k$-means algorithm is developed in this paper. This algorithm resolves two challenging issues: 1) how to distributedly solve the optimization problem of kernel $k$-means under federated settings; 2) how to maintain communication efficiency in the algorithm. To tackle the first challenge, a distributed stochastic proximal gradient descent (DSPGD) algorithm is developed to determine an approximated solution to the optimization problem of kernel $k$-means. To tackle the second challenge, a communication efficient mechanism (CEM) is designed to reduce the communication cost. Besides, the federated kernel $k$-means provides two levels of privacy preservation: 1) users\u2019 local data are not exposed to the cloud server; 2) the cloud server cannot recover users\u2019 local data from the local computational results via matrix operations. Theoretical analysis shows: 1) DSPGD with CEM converges with an $O(1/T)$ rate, where $T$ is the number of iterations; 2) the communication cost of DSPGD with CEM is unrelated to the number of data samples; 3) the clustering quality of the federated kernel $k$-means approaches that of the standard kernel $k$-means, with a $(1+\\epsilon)$ approximate ratio. The experimental results show that the federated kernel $k$-means achieves the highest clustering quality with the communication cost reduced by more than $60\\%$ in most cases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|a_communication_efficient_federated_kernel_kmeans", "supplementary_material": "/attachment/226a3d584fed29f35f9dfff1b45b782aac8df0d5.zip", "pdf": "/pdf/918fbbf03a84f52157cc40ec901359434a10734c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=IyB_e1BjGJ", "_bibtex": "@misc{\nzhou2021a,\ntitle={A Communication Efficient Federated Kernel {\\$}k{\\$}-Means},\nauthor={Xiaochen Zhou and Xudong Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=tbwjUvUzQRU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "tbwjUvUzQRU", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1799/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1799/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1799/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1799/Authors|ICLR.cc/2021/Conference/Paper1799/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1799/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855621, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1799/-/Official_Comment"}}}, {"id": "rm0QbwUdiYW", "original": null, "number": 6, "cdate": 1606218423600, "ddate": null, "tcdate": 1606218423600, "tmdate": 1606219619034, "tddate": null, "forum": "tbwjUvUzQRU", "replyto": "w_nEcBQSDwL", "invitation": "ICLR.cc/2021/Conference/Paper1799/-/Official_Comment", "content": {"title": "Response to Q2 ~Q3", "comment": "2. Besides, why transmitting the raw data sample is important here? Is it due to privacy issues? Because the way I see it, one does not need to transmit the raw data samples but rather its local kernel matrix $\\mathrm{K}_m$, and doing so does not necessarily endanger the privacy of an agent, since one cannot easily recover data samples from $\\mathrm{K}_m$.\n\nNot uploading the raw data to the cloud server is important for two reasons. First, the transmission of the raw data can consume large communication bandwidth. Second, uploading the raw data can lead to privacy issues.\n\n  If each user $m$ uploads its local kernel matrix $\\mathrm{K}_m$ to the cloud server, indeed it is difficult for the cloud server to recover users' data samples from $\\mathrm{K}_m$. However, it is non-trivial to conduct the kernel $k$-means clustering using $\\{ \\mathrm{K}_m, m=1,...,M\\}$. Based on $\\{ \\mathrm{K}_m, m=1,...,M\\}$, the cloud server can construct a block-diagonal matrix $\\tilde{\\mathrm{K}}$.\n\n\nNote that $\\tilde{\\mathrm{K}}$ is far from the true kernel matrix $\\mathrm{K}$. To the best of our knowledge, none of existing schemes can accurately approach the top eigenvectors of $\\mathrm{K}$ based on $\\tilde{\\mathrm{K}}$. Besides, uploading $\\mathrm{K}_m$ can lead to even larger communication cost than uploading users' raw data. Assume each user $m$ has $n_m$ $d$-dimension data samples, and each dimension is represented by a $4$-byte floating-point number. Thus, the communication cost of uploading the raw data is $4 n_m d$ bytes for each user $m$. \n  Since $\\mathrm{K}_m$ is a symmetric matrix, the upper triangular portion of $\\mathrm{K}_m$ actually contains all the information of $\\mathrm{K}_m$. Thus, the communication cost of uploading $\\mathrm{K}_m$ is $2 n_m (n_m + 1) $ bytes. If $n_m > 2d-1$, which is common for a dataset, the communication cost of uploading $\\mathrm{K}_m$ is larger than that of uploading the raw data. Thus, it is not communication efficient to upload $\\mathrm{K}_m$ to the cloud server.\n\n3. In Section 6.2, the author says ``According to the results in the four subfigures, it is shown that CEM can reduce communication cost of DSPGD by more than $95$%.'' It would be interesting to explain what values of $r_t$, $Q_0$, and $Q_1$ leads to this improvement and why.\n\nThe communication ratio (i.e., the ratio of the communication cost of DSPGD without CEM to that of DSPGD with CEM) equals $\\frac{(N+MD)Q_0}{M(r_t+D)Q_1}$ according to Theorem 2. In this expression, the ratio $\\frac{Q_0}{Q_1} \\approx 1$, which can be seen in Figure 4 that is newly added to the appendix. As a result, the communication ratio mainly relies on $\\frac{N+MD}{M(r_t+D)}$.\n\n  We set $r_t$ to its upper bound to compute a lower bound of $\\frac{N+MD}{M(r_t+D)}$. According to Figure 5 in the appendix, the value of $r_t$ is set to $12$, $20$, $22$, $10$ for the Mushroom dataset, the MNIST dataset, the Covtype dataset, the Smartphone dataset, respectively. For other variables $\\frac{N+MD}{M(r_t+D)}$, the values of $N$ and $D$ are given in Table 1, and $M=5$. As a result, the lower bound of $\\frac{N+MD}{M(r_t+D)}$ equals $60.73$, $55.45$, $2235.23$, $1142.76$ for the Mushroom dataset, the MNIST dataset, the Covtype dataset, the Smartphone dataset, respectively. The statement ``CEM can reduce communication cost of DSPGD by more than $95$%'' is equivalent to the communication ratio larger than $20$. Since the lower bound of $\\frac{N+MD}{M(r_t+D)}$ is larger than $20$ for each dataset, it is concluded that CEM can reduce communication cost of DSPGD by more than $95$%.\n\n  Note that the statement ``more than $95\\%$'' is conservative. According to the lower bound of $\\frac{N+MD}{M(r_t+D)}$, CEM can actually reduce the communication cost of DSPGD by more than $98$%, which is also consistent with the results in Figure 2. We have revised the corresponding statements in the revised paper.\n\n\n\n\n\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1799/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1799/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Communication Efficient Federated Kernel $k$-Means", "authorids": ["~Xiaochen_Zhou2", "wxudong@sjtu.edu.cn"], "authors": ["Xiaochen Zhou", "Xudong Wang"], "keywords": ["federated learning", "kernel $k$-means", "communication efficient"], "abstract": "A federated kernel $k$-means algorithm is developed in this paper. This algorithm resolves two challenging issues: 1) how to distributedly solve the optimization problem of kernel $k$-means under federated settings; 2) how to maintain communication efficiency in the algorithm. To tackle the first challenge, a distributed stochastic proximal gradient descent (DSPGD) algorithm is developed to determine an approximated solution to the optimization problem of kernel $k$-means. To tackle the second challenge, a communication efficient mechanism (CEM) is designed to reduce the communication cost. Besides, the federated kernel $k$-means provides two levels of privacy preservation: 1) users\u2019 local data are not exposed to the cloud server; 2) the cloud server cannot recover users\u2019 local data from the local computational results via matrix operations. Theoretical analysis shows: 1) DSPGD with CEM converges with an $O(1/T)$ rate, where $T$ is the number of iterations; 2) the communication cost of DSPGD with CEM is unrelated to the number of data samples; 3) the clustering quality of the federated kernel $k$-means approaches that of the standard kernel $k$-means, with a $(1+\\epsilon)$ approximate ratio. The experimental results show that the federated kernel $k$-means achieves the highest clustering quality with the communication cost reduced by more than $60\\%$ in most cases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|a_communication_efficient_federated_kernel_kmeans", "supplementary_material": "/attachment/226a3d584fed29f35f9dfff1b45b782aac8df0d5.zip", "pdf": "/pdf/918fbbf03a84f52157cc40ec901359434a10734c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=IyB_e1BjGJ", "_bibtex": "@misc{\nzhou2021a,\ntitle={A Communication Efficient Federated Kernel {\\$}k{\\$}-Means},\nauthor={Xiaochen Zhou and Xudong Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=tbwjUvUzQRU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "tbwjUvUzQRU", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1799/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1799/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1799/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1799/Authors|ICLR.cc/2021/Conference/Paper1799/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1799/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855621, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1799/-/Official_Comment"}}}, {"id": "w_nEcBQSDwL", "original": null, "number": 5, "cdate": 1606218037937, "ddate": null, "tcdate": 1606218037937, "tmdate": 1606219587556, "tddate": null, "forum": "tbwjUvUzQRU", "replyto": "FG_d-Jy3fm7", "invitation": "ICLR.cc/2021/Conference/Paper1799/-/Official_Comment", "content": {"title": "Response to summary and Q1", "comment": "We thank the reviewer for the constructive suggestions on our paper.\n\nSummary: The paper is rather difficult to follow and the novelty of the paper is not very clear to me. It seems to me that the major contribution of the paper is to come up with efficient tricks during the implementation of the distributed Lanczos algorithm (DLA) to find the eigenvalues of the kernel approximated by using the well-known random Fourier features (Rahimi, nips 2008).\n\nIn Section 4.1, we first point out that the key problem for designing FK $k$-means is to obtain the top eigenpair of the kernel matrix $\\mathrm{K}$ in a distributed manner. To solve this problem, we design a distributed stochastic proximal gradient descent (DSPGD) algorithm. We then present the challenges on solving the key problem and how we resolve these challenges. \n Since $\\mathrm{K}$ is not available under federated settings. To this end, an estimate of $\\mathrm{K}$, denoted as $\\mathrm{\\xi}$, is constructed jointly at users' devices based on random features [1] of local data samples.\n  Since $\\mathrm{\\xi}$ is distributed among different devices, it is processed by the distributed Lanczos algorithm (DLA) [3] to obtain an estimate of $\\mathrm{K}$, i.e., $\\mathrm{Z}$, at the cloud server. Afterwards, an approximate version of the top eigenpairs of $\\mathrm{K}$ can be obtained from $\\mathrm{Z}$ through singular value decomposition (SVD). \n To improve the accuracy of approximation, $\\mathrm{Z}$ is iteratively updated by DSPGD.\n\n  More specifically, in the $t$-th iteration, an estimate $\\mathrm{\\xi}\\_{t}$ is constructed at users' devices, and then the estimate $\\mathrm{Z}\\_{t}$ at the cloud server is updated to $\\mathrm{Z}\\_{t+1}$ by applying DLA to a weighted sum of $\\mathrm{Z}\\_{t}$ and $\\mathrm{\\xi}\\_{t}$. \n\n  In Section 4.2, we first point out that the process of obtaining an updated $\\mathrm{Z}_{t}$ results in high communication cost, because DLA is applied to matrices with the number of rows/columns equals the number of data samples. To this end, we design a communication efficient mechanism (CEM) so that DLA is applied to a new type of matrices. We then present the design of the new type of matrices and the procedure of obtaining the top eigenpairs of $\\mathrm{K}$ in CEM. \n\n  The major novelty of our work is that we design a new distributed stochastic proximal gradient descent (DSPGD) algorithm to approximate the top-$s$ eigenpairs of the kernel matrix $\\mathrm{K}$ under the federated settings where raw data are maintained by users and the cloud cannot access the raw data. Besides, we design a communication efficient mechanism (CEM) that can highly reduce the communication cost of DSPGD. In addition, we derive the convergence rate of DSPGD, analyze the communication cost of DSPGD before and after employing CEM, and analyze the clustering quality of FK $k$-means.\n\n1. The literature review does not clearly illustrate the contribution of the paper. For instance, in Section 2.1, the author says ``However, these algorithms are designed with an assumption that they are executed at the cloud server where the constraint on transmissions of the raw data samples does not exist.'' Does this mean that your work is the first to constrain the transmission of data samples? If so you should clearly say so.\n\nWe have revised the literature review and clearly pointed out the differences between our work and the existing schemes.\n\nIn Section 2.1, the clause ``where the constraint on transmissions of the raw data samples does not exist'' is indeed confusing. We actually want to express that the distributed kernel $k$-means schemes mentioned in Section 2 are designed for the cloud server where users' raw data are collected. In contrast to these distributed kernel $k$-means schemes, FK $k$-means is the first scheme to conduct the kernel $k$-means clustering without uploading users' raw data to the cloud server. Besides, FK $k$-means also considers the communication efficiency during the clustering task.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1799/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1799/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Communication Efficient Federated Kernel $k$-Means", "authorids": ["~Xiaochen_Zhou2", "wxudong@sjtu.edu.cn"], "authors": ["Xiaochen Zhou", "Xudong Wang"], "keywords": ["federated learning", "kernel $k$-means", "communication efficient"], "abstract": "A federated kernel $k$-means algorithm is developed in this paper. This algorithm resolves two challenging issues: 1) how to distributedly solve the optimization problem of kernel $k$-means under federated settings; 2) how to maintain communication efficiency in the algorithm. To tackle the first challenge, a distributed stochastic proximal gradient descent (DSPGD) algorithm is developed to determine an approximated solution to the optimization problem of kernel $k$-means. To tackle the second challenge, a communication efficient mechanism (CEM) is designed to reduce the communication cost. Besides, the federated kernel $k$-means provides two levels of privacy preservation: 1) users\u2019 local data are not exposed to the cloud server; 2) the cloud server cannot recover users\u2019 local data from the local computational results via matrix operations. Theoretical analysis shows: 1) DSPGD with CEM converges with an $O(1/T)$ rate, where $T$ is the number of iterations; 2) the communication cost of DSPGD with CEM is unrelated to the number of data samples; 3) the clustering quality of the federated kernel $k$-means approaches that of the standard kernel $k$-means, with a $(1+\\epsilon)$ approximate ratio. The experimental results show that the federated kernel $k$-means achieves the highest clustering quality with the communication cost reduced by more than $60\\%$ in most cases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|a_communication_efficient_federated_kernel_kmeans", "supplementary_material": "/attachment/226a3d584fed29f35f9dfff1b45b782aac8df0d5.zip", "pdf": "/pdf/918fbbf03a84f52157cc40ec901359434a10734c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=IyB_e1BjGJ", "_bibtex": "@misc{\nzhou2021a,\ntitle={A Communication Efficient Federated Kernel {\\$}k{\\$}-Means},\nauthor={Xiaochen Zhou and Xudong Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=tbwjUvUzQRU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "tbwjUvUzQRU", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1799/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1799/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1799/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1799/Authors|ICLR.cc/2021/Conference/Paper1799/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1799/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855621, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1799/-/Official_Comment"}}}, {"id": "D8AkCtziVcq", "original": null, "number": 4, "cdate": 1606217771738, "ddate": null, "tcdate": 1606217771738, "tmdate": 1606219401270, "tddate": null, "forum": "tbwjUvUzQRU", "replyto": "10xrRBZCqTc", "invitation": "ICLR.cc/2021/Conference/Paper1799/-/Official_Comment", "content": {"title": "Response to Q2~Q5", "comment": "2. I am very surprised that Theorem 3 does not rely on $D$ (the number of random features).\n\n$D$ actually affects the convergence rate of DSPGD, and furthermore affects the approximation ratio in Theorem 3, which is explained as follows. $D$ affects the variance of $\\mathrm{\\xi}_t$ according to [1]: the larger is $D$, the smaller is the variance of $\\mathrm{\\xi}_t$. Thus, a larger $D$ leads to a smaller $G$ for the inequality $||\\mathrm{\\xi}_t - \\mathrm{K}||_F \\leq G$ in Lemma 3.\n\nAccording to the following inequality in Lemma 3\n $$\n||\\mathrm{Z}_{T+1} - \\mathrm{Z}^*||_F^2 \\leq \\frac{4}{T}\\left( C^2 + \\lambda \\gamma + 2G^2\\tau + \\frac{2}{3}GH\\tau +GH\\right),\n$$\n\nwe can see that a smaller $G$ leads to $\\mathrm{Z}\\_{T+1}$ closer to the optimal solution $\\mathrm{Z}^*$, which means that $\\mathrm{Z}_{T+1}$ converges faster to $\\mathrm{Z}^*$. The inequality is then used in Lemma 5 to compute an upper bound of $\\varepsilon$ in the approximate ratio (i.e., $1+\\varepsilon+\\frac{k}{s}$). According to Lemma 5, given $T$, the smaller is the bound, the smaller is $\\varepsilon$, which concludes that $D$ affects the approximation ratio in Theorem 3\n\nHowever,  we can also see that different values of $D$ do not change the asymptotic convergence rate of DSPGD, i.e., $O(1/T)$. As a result, different values of $D$ also do not change the asymptotic convergence rate of $\\varepsilon$, i.e., $O(\\sqrt{s/T})$.\n\n3. $\\mathrm{Z}^*$ has the same top eigenvectors as $\\mathrm{\\xi}$. But $\\mathrm{Z}^*$ may not have the same as $\\mathrm{K}$. The proof of Theorem 3 relies on that $\\mathrm{Z}^*$ has the same eigenvectors as $\\mathrm{K}$.\n\nWe would like to clarify that $\\mathrm{Z}^*$ actually has the same top eigenvectors as $\\mathrm{K}$ rather than as $\\mathrm{\\xi}$. Since $\\mathrm{Z}^*$ is the optimal solution to SCO problem, it can be proved that as long as $\\lambda$ is set to a proper value, $\\mathrm{Z}^*$ has the same top eigenvectors as $\\mathrm{K}$. However, the random matrix $\\mathrm{\\xi}_t$ is constructed by a random feature method in the $t$-th iteration of DSPGD (in DSPGD, $\\mathrm{\\xi}$ is replaced by $\\mathrm{\\xi}_t$). The top eigenvectors of $\\mathrm{\\xi}_t$ is unrelated to that of $\\mathrm{Z}^*$.\n\n4. The description of the algorithm is difficult to follow. I\u2019d suggest splitting algorithm description into 3 paragraphs: 1) Client-side computation, 2) server-side computation, and 3) communications.\n\nTo improve the clarity of algorithm description, we have marked each step with a number and pointed out the conductor (the cloud server or the users' devices) for each step. Please refer to the last paragraphs in Section 4.1 and Section 4.2.\n\n5. 17th page: ``The following two lemmas will be used in the proof of Theorem 4.'' Do you mean Theorem 3?\n\nIndeed here Theorem 4 should be replaced with Theorem 3. We have corrected this typo in our revised version of paper.\n\nReference\n\n[1] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in\nNeural Information Processing Systems 20, pp. 1177\u20131184, 2008.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1799/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1799/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Communication Efficient Federated Kernel $k$-Means", "authorids": ["~Xiaochen_Zhou2", "wxudong@sjtu.edu.cn"], "authors": ["Xiaochen Zhou", "Xudong Wang"], "keywords": ["federated learning", "kernel $k$-means", "communication efficient"], "abstract": "A federated kernel $k$-means algorithm is developed in this paper. This algorithm resolves two challenging issues: 1) how to distributedly solve the optimization problem of kernel $k$-means under federated settings; 2) how to maintain communication efficiency in the algorithm. To tackle the first challenge, a distributed stochastic proximal gradient descent (DSPGD) algorithm is developed to determine an approximated solution to the optimization problem of kernel $k$-means. To tackle the second challenge, a communication efficient mechanism (CEM) is designed to reduce the communication cost. Besides, the federated kernel $k$-means provides two levels of privacy preservation: 1) users\u2019 local data are not exposed to the cloud server; 2) the cloud server cannot recover users\u2019 local data from the local computational results via matrix operations. Theoretical analysis shows: 1) DSPGD with CEM converges with an $O(1/T)$ rate, where $T$ is the number of iterations; 2) the communication cost of DSPGD with CEM is unrelated to the number of data samples; 3) the clustering quality of the federated kernel $k$-means approaches that of the standard kernel $k$-means, with a $(1+\\epsilon)$ approximate ratio. The experimental results show that the federated kernel $k$-means achieves the highest clustering quality with the communication cost reduced by more than $60\\%$ in most cases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|a_communication_efficient_federated_kernel_kmeans", "supplementary_material": "/attachment/226a3d584fed29f35f9dfff1b45b782aac8df0d5.zip", "pdf": "/pdf/918fbbf03a84f52157cc40ec901359434a10734c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=IyB_e1BjGJ", "_bibtex": "@misc{\nzhou2021a,\ntitle={A Communication Efficient Federated Kernel {\\$}k{\\$}-Means},\nauthor={Xiaochen Zhou and Xudong Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=tbwjUvUzQRU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "tbwjUvUzQRU", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1799/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1799/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1799/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1799/Authors|ICLR.cc/2021/Conference/Paper1799/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1799/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855621, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1799/-/Official_Comment"}}}, {"id": "10xrRBZCqTc", "original": null, "number": 3, "cdate": 1606217445017, "ddate": null, "tcdate": 1606217445017, "tmdate": 1606219333819, "tddate": null, "forum": "tbwjUvUzQRU", "replyto": "JF3IaHcgkSd", "invitation": "ICLR.cc/2021/Conference/Paper1799/-/Official_Comment", "content": {"title": "Response to summary and Q1", "comment": "We thank the reviewer for the constructive suggestions on our paper.\n\nSummary: This paper proposes a new method and two algorithms for solving kernel k-means. The contribution is that the algorithms converge to the optimal solution. The downsides are 1) the method is not better than a simple baseline (i.e., random features + distributed power method) and 2) the main theorem (Theorem 3) is wrong.\n\nThe contribution of our work is as follows.\n  We design a new distributed stochastic proximal gradient descent (DSPGD) algorithm to approximate the top-$s$ eigenpairs of the kernel matrix $\\mathrm{K}$ under federated settings where raw data are maintained by users and the cloud cannot access the raw data. Besides, we design a communication efficient mechanism (CEM) that can highly reduce the communication cost of DSPGD. In addition, we theoretically analyze the performance of FK $k$-means and its components in three aspects: the convergence rate of DSPGD; the communication cost of DSPGD before and after employing CEM; the clustering quality of FK $k$-means.\n\n  We would like to point out that our method (i.e., distributed stochastic proximal gradient descent (DSPGD)) is actually better than the baseline (i.e., random features + distributed power method). The baseline is actually a special case of the DSPGD. If DSPGD adopts the same value of $D$ as that in the baseline and is only executed for one iteration, then DSPGD is reduced to the baseline. For the comparison between DSPGD and the baseline, please refer to the response to comment 2.\n\n  Besides, we have checked the proof of Theorem 3 and confirmed that Theorem 3 is correct. For the detailed clarification, please refer to the responses to comment 3 and comment 4.\n\n1. The goal of the algorithm is to find the top singular vectors of the random features $\\mathrm{A}$. However, the same goal could be achieved in an easier and less expensive way, i.e., random features + distributed power method. Random features are naturally distributed among the clients. Their truncated SVD could be found by the distributed power method or Krylov subspace methods. Truncated SVD is easier than solved the proposed trace norm regularized problem because the latter uses SVS which repeatedly performs SVD.\n\nThe goal of the distributed stochastic proximal gradient descent (DSPGD) is actually to approach the top eigenpairs of the kernel matrix $\\mathrm{K}$, not the the top singular vectors and the corresponding singular values of the random features $\\mathrm{A}$.\n  In FK $k$-means, DSPGD is designed to solve a stochastic composite optimization (SCO) problem in a distributed manner:\n$$\n\\min_{\\mathrm{Z} \\in \\mathbb{R}^{n \\times n}}{\\frac{1}{2}\\mathbb{E}[||\\mathrm{Z} - \\mathrm{\\xi}||_F^2]+\\lambda ||\\mathrm{Z}||*}.\n$$\n\nThe optimal solution $\\mathrm{Z}^*$ to SCO problem is\n$$\n\\mathrm{Z}^* = \\sum_{i:\\lambda_i > \\lambda}{(\\lambda_i - \\lambda) \\mathrm{u}_i \\mathrm{u}_i^\\top},\n$$\n\nwhere ${\\mathrm{u}}_i$ and $\\lambda_i$ are the $i$-th eigenvector and the $i$-th eigenvalue of $\\mathrm{K}$, respectively. Assume that the rank of $\\mathrm{Z}^*$ equals $s$.\n  By applying SVD to $\\mathrm{Z}^*$, its eigenvectors $\\{{\\mathrm{u}}_i, i=1,...,s\\}$ of $\\mathrm{K}$ and eigenvalues $\\{\\lambda_i - \\lambda, i=1,...,s\\}$ are determined. The eigenvectors of $\\mathrm{Z}^*$ are exactly the top-$s$ eigenvectors of $\\mathrm{K}$. Moreover, by adding $\\lambda$ to the eigenvalues of $\\mathrm{Z}^*$, the top-$s$ eigenvalues of $\\mathrm{K}$ are obtained. \n  Thus, the top eigenpairs of $\\mathrm{K}$ are approached by DSPGD.\n\nThe baseline can indeed be used to approximate the top eigenvectors of $\\mathrm{K}$. However, the number of random features $D$ should be set to a very large value in order to obtain an accurate estimate of $\\mathrm{K}$ according to [1]. This method is feasible only if users' devices have enough memory to maintain these random feature vectors. Compared with the baseline, DSPGD is a generic method that provides a tradeoff between memory and computational time: the larger is $D$, the fewer iterations are required (an extreme case is that only one iteration is required) to approach the top eigenvectors of $\\mathrm{K}$. Thus, DSPGD can be adapted to devices with different memory space.\n "}, "signatures": ["ICLR.cc/2021/Conference/Paper1799/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1799/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Communication Efficient Federated Kernel $k$-Means", "authorids": ["~Xiaochen_Zhou2", "wxudong@sjtu.edu.cn"], "authors": ["Xiaochen Zhou", "Xudong Wang"], "keywords": ["federated learning", "kernel $k$-means", "communication efficient"], "abstract": "A federated kernel $k$-means algorithm is developed in this paper. This algorithm resolves two challenging issues: 1) how to distributedly solve the optimization problem of kernel $k$-means under federated settings; 2) how to maintain communication efficiency in the algorithm. To tackle the first challenge, a distributed stochastic proximal gradient descent (DSPGD) algorithm is developed to determine an approximated solution to the optimization problem of kernel $k$-means. To tackle the second challenge, a communication efficient mechanism (CEM) is designed to reduce the communication cost. Besides, the federated kernel $k$-means provides two levels of privacy preservation: 1) users\u2019 local data are not exposed to the cloud server; 2) the cloud server cannot recover users\u2019 local data from the local computational results via matrix operations. Theoretical analysis shows: 1) DSPGD with CEM converges with an $O(1/T)$ rate, where $T$ is the number of iterations; 2) the communication cost of DSPGD with CEM is unrelated to the number of data samples; 3) the clustering quality of the federated kernel $k$-means approaches that of the standard kernel $k$-means, with a $(1+\\epsilon)$ approximate ratio. The experimental results show that the federated kernel $k$-means achieves the highest clustering quality with the communication cost reduced by more than $60\\%$ in most cases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|a_communication_efficient_federated_kernel_kmeans", "supplementary_material": "/attachment/226a3d584fed29f35f9dfff1b45b782aac8df0d5.zip", "pdf": "/pdf/918fbbf03a84f52157cc40ec901359434a10734c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=IyB_e1BjGJ", "_bibtex": "@misc{\nzhou2021a,\ntitle={A Communication Efficient Federated Kernel {\\$}k{\\$}-Means},\nauthor={Xiaochen Zhou and Xudong Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=tbwjUvUzQRU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "tbwjUvUzQRU", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1799/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1799/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1799/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1799/Authors|ICLR.cc/2021/Conference/Paper1799/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1799/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855621, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1799/-/Official_Comment"}}}, {"id": "sTZMI9AMvEc", "original": null, "number": 10, "cdate": 1606219030216, "ddate": null, "tcdate": 1606219030216, "tmdate": 1606219030216, "tddate": null, "forum": "tbwjUvUzQRU", "replyto": "cFIBNaP_aNg", "invitation": "ICLR.cc/2021/Conference/Paper1799/-/Official_Comment", "content": {"title": "Response to Q9", "comment": "9. The way the result is presented makes it look like the proposed method is a concatenation of other results, rather than the solution of a technical challenge in the problem.\n\nWe have reorganized the presentation of the results to match the technical challenges.\n\n  We have added the following paragraph at the beginning of Section 5 to match the theoretical results with the challenges:\n  The convergence of DSPGD with CEM is analyzed in Section 5.1. The communication cost of CEM is analyzed in Section 5.2, which shows CEM is important for FK $k$-means to maintain the communication efficiency. It is then proved that the clustering results of FK $k$-means can approach that of the standard kernel $k$-means in Section 5.3. Besides, the privacy preservation provided by FK $k$-means is analyzed in Section 5.4.\n\n  We have also added the following paragraph in Section~6.2 to match the numerical results with the challenges:\n  The experimental results are presented in three aspects. First, the convergence results of DSPGD is shown in Figure 1 to verify its convergence rate. Second, the average communication cost per iteration of the two versions of DSPGD is provided in Figure 2 to show that CEM highly reduces the communication cost of DSPGD. Third, in Figure 3, FK $k$-means is compared with the cloud-based kernel $k$-means schemes in terms of clustering quality to show FK $k$-means can achieve the comparable clustering results as that of the cloud-based schemes; FK $k$-means is also compared with the existing distributed kernel $k$-means schemes under federated settings to show the higher communication efficiency of FK $k$-means."}, "signatures": ["ICLR.cc/2021/Conference/Paper1799/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1799/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Communication Efficient Federated Kernel $k$-Means", "authorids": ["~Xiaochen_Zhou2", "wxudong@sjtu.edu.cn"], "authors": ["Xiaochen Zhou", "Xudong Wang"], "keywords": ["federated learning", "kernel $k$-means", "communication efficient"], "abstract": "A federated kernel $k$-means algorithm is developed in this paper. This algorithm resolves two challenging issues: 1) how to distributedly solve the optimization problem of kernel $k$-means under federated settings; 2) how to maintain communication efficiency in the algorithm. To tackle the first challenge, a distributed stochastic proximal gradient descent (DSPGD) algorithm is developed to determine an approximated solution to the optimization problem of kernel $k$-means. To tackle the second challenge, a communication efficient mechanism (CEM) is designed to reduce the communication cost. Besides, the federated kernel $k$-means provides two levels of privacy preservation: 1) users\u2019 local data are not exposed to the cloud server; 2) the cloud server cannot recover users\u2019 local data from the local computational results via matrix operations. Theoretical analysis shows: 1) DSPGD with CEM converges with an $O(1/T)$ rate, where $T$ is the number of iterations; 2) the communication cost of DSPGD with CEM is unrelated to the number of data samples; 3) the clustering quality of the federated kernel $k$-means approaches that of the standard kernel $k$-means, with a $(1+\\epsilon)$ approximate ratio. The experimental results show that the federated kernel $k$-means achieves the highest clustering quality with the communication cost reduced by more than $60\\%$ in most cases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|a_communication_efficient_federated_kernel_kmeans", "supplementary_material": "/attachment/226a3d584fed29f35f9dfff1b45b782aac8df0d5.zip", "pdf": "/pdf/918fbbf03a84f52157cc40ec901359434a10734c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=IyB_e1BjGJ", "_bibtex": "@misc{\nzhou2021a,\ntitle={A Communication Efficient Federated Kernel {\\$}k{\\$}-Means},\nauthor={Xiaochen Zhou and Xudong Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=tbwjUvUzQRU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "tbwjUvUzQRU", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1799/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1799/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1799/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1799/Authors|ICLR.cc/2021/Conference/Paper1799/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1799/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855621, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1799/-/Official_Comment"}}}, {"id": "cFIBNaP_aNg", "original": null, "number": 9, "cdate": 1606218991950, "ddate": null, "tcdate": 1606218991950, "tmdate": 1606218991950, "tddate": null, "forum": "tbwjUvUzQRU", "replyto": "VrOOrkjXp4H", "invitation": "ICLR.cc/2021/Conference/Paper1799/-/Official_Comment", "content": {"title": "Response to Q3~Q8", "comment": "3.  Problem 1 seems to be an integer programming problem, thus with very high computation complexity. It is not clear how this is solved.\n\nProblem~1 is an NP-hard problem [4]. Thus, an approximate solution is required. One efficient method to determine the approximate solution is as follows [5]. The kernel matrix $\\mathrm{K}$ is first decomposed as $\\mathrm{K}={\\mathrm{U}}{\\mathrm{\\Lambda}}{\\mathrm{U}}^\\top$ via SVD, and then a linear $k$-means algorithm is applied to the rows of the matrix ${\\mathrm{H}}={\\mathrm{U}}{\\mathrm{\\Lambda}}^{\\frac{1}{2}}$ to obtain the approximate solution. \n\n4. In the abstract please let me know what are those two levels of privacy you are talking about.\n\nThe FK $k$-means provides two levels of privacy preservation: 1) users\u2019 local data are not exposed to the cloud server; 2) the cloud server cannot recover users\u2019 local data from the local computational results via matrix operations. \n\n5. In the abstract, what does it mean that the clustering loss of the distributed method approaches the centralized one, please elaborate.\n\nIn this statement we want to express that the clustering result of FK $k$-means approaches that of the standard kernel $k$-means, with an approximate ratio $(1+\\varepsilon)$. The original statement ``the clustering loss of the distributed method approaches the centralized one'' is indeed confusing, and we have revised it in the abstract.\n\n6. Why developing a federated learning algorithm is a promising approach? Please elaborate.\n\nCompared with the standard kernel $k$-means, the federated kernel $k$-means has two advantages: 1) users' raw data are not uploaded to the cloud server, which provides a basic level of privacy preservation; 2) it is usually more communication efficient to upload the local computational results than to upload the raw data to the cloud server. Thus, a federated kernel $k$-means algorithm is a promising approach.\n\n7.  The second part of the intro turns into a detail technical analysis of the algorithm components, and so far we haven\u2019t seen the algorithm so it all remains a technical abstract discussion that takes away the main messages.\n\nWe have removed the detail technical analysis of the algorithm components from the second part of the introduction. Besides, we have revised the second part of the introduction as follows.\n\n  For the first challenging issue, we point out the key problem for designing FK $k$-means is to obtain the top eigenpair of the kernel matrix $\\mathrm{K}$ in a distributed manner. To solve this problem, we design a distributed stochastic proximal gradient descent (DSPGD) algorithm. We then present two challenges on solving the key problem and how we resolve these challenges. \n \nThe first challenge is that $\\mathrm{K}$ is not available under federated settings. To this end, an estimate of $\\mathrm{K}$, denoted as $\\mathrm{\\xi}$, is jointly constructed at users' devices based on random features [1] of local data samples. Based on $\\mathrm{\\xi}$, an estimate of $\\mathrm{K}$, denoted as $\\mathrm{Z}$ can be obtained at the cloud server. Afterwards, an approximate version of the top eigenpairs of $\\mathrm{K}$ can be obtained from $\\mathrm{Z}$ through singular value decomposition (SVD).\n \n  For the second challenging issue, we also point out how to improve the accuracy of approximation. To this end, $\\mathrm{Z}$ is iteratively updated by DSPGD.\n  More specifically, in the $t$-th iteration, an estimate $\\mathrm{\\xi}\\_{t}$ is constructed at users' devices, and then the estimate $\\mathrm{Z}\\_{t}$ at the cloud server is updated to $\\mathrm{Z}\\_{t+1}$ by applying DLA to a weighted sum of $\\mathrm{Z}\\_{t}$ and $\\mathrm{\\xi}\\_{t}$.\n\n8. The algorithm is in the appendix, so the description and analysis is made on an item that has not been presented in the main text.\n\nWe have moved the pseudo code of the algorithm back to Section 4.2.\n\n\n\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1799/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1799/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Communication Efficient Federated Kernel $k$-Means", "authorids": ["~Xiaochen_Zhou2", "wxudong@sjtu.edu.cn"], "authors": ["Xiaochen Zhou", "Xudong Wang"], "keywords": ["federated learning", "kernel $k$-means", "communication efficient"], "abstract": "A federated kernel $k$-means algorithm is developed in this paper. This algorithm resolves two challenging issues: 1) how to distributedly solve the optimization problem of kernel $k$-means under federated settings; 2) how to maintain communication efficiency in the algorithm. To tackle the first challenge, a distributed stochastic proximal gradient descent (DSPGD) algorithm is developed to determine an approximated solution to the optimization problem of kernel $k$-means. To tackle the second challenge, a communication efficient mechanism (CEM) is designed to reduce the communication cost. Besides, the federated kernel $k$-means provides two levels of privacy preservation: 1) users\u2019 local data are not exposed to the cloud server; 2) the cloud server cannot recover users\u2019 local data from the local computational results via matrix operations. Theoretical analysis shows: 1) DSPGD with CEM converges with an $O(1/T)$ rate, where $T$ is the number of iterations; 2) the communication cost of DSPGD with CEM is unrelated to the number of data samples; 3) the clustering quality of the federated kernel $k$-means approaches that of the standard kernel $k$-means, with a $(1+\\epsilon)$ approximate ratio. The experimental results show that the federated kernel $k$-means achieves the highest clustering quality with the communication cost reduced by more than $60\\%$ in most cases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|a_communication_efficient_federated_kernel_kmeans", "supplementary_material": "/attachment/226a3d584fed29f35f9dfff1b45b782aac8df0d5.zip", "pdf": "/pdf/918fbbf03a84f52157cc40ec901359434a10734c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=IyB_e1BjGJ", "_bibtex": "@misc{\nzhou2021a,\ntitle={A Communication Efficient Federated Kernel {\\$}k{\\$}-Means},\nauthor={Xiaochen Zhou and Xudong Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=tbwjUvUzQRU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "tbwjUvUzQRU", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1799/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1799/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1799/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1799/Authors|ICLR.cc/2021/Conference/Paper1799/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1799/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855621, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1799/-/Official_Comment"}}}, {"id": "VrOOrkjXp4H", "original": null, "number": 8, "cdate": 1606218754210, "ddate": null, "tcdate": 1606218754210, "tmdate": 1606218754210, "tddate": null, "forum": "tbwjUvUzQRU", "replyto": "XJEFrCiFDDP", "invitation": "ICLR.cc/2021/Conference/Paper1799/-/Official_Comment", "content": {"title": "Response to Q1 and Q2", "comment": "We thank the reviewer for the constructive suggestions on our paper.\n\n1. This paper seems provide a new algorithm for distributed clustering. However, the way the algorithm is presented look like a patch of a number of things coming together one after the other with no general structure. This might be caused by the fact that the algorithm is only presented in the appendix.\n\nTo improve the presentation of the algorithm, we have revised the paper as follows. We have added an overview of federated kernel $k$-means (denoted as FK $k$-means) at the beginning of Section 4.\n\n  In Section 4.1, we first point out that the key problem for designing FK $k$-means is to obtain the top eigenpair of the kernel matrix $\\mathrm{K}$ in a distributed manner. To solve this problem, we design a distributed stochastic proximal gradient descent (DSPGD) algorithm. We then present two challenges on solving the key problem and how we resolve these challenges. \n The first challenge is that $\\mathrm{K}$ is not available under federated settings. To this end, an estimate of $\\mathrm{K}$, denoted as $\\mathrm{\\xi}$, is jointly constructed at users' devices based on random features [1] of local data samples. Based on $\\mathrm{\\xi}$, an estimate of $\\mathrm{K}$, denoted as $\\mathrm{Z}$ can be obtained at the cloud server. Afterwards, an approximate version of the top eigenpairs of $\\mathrm{K}$ can be obtained from $\\mathrm{Z}$ through singular value decomposition (SVD).\n\nThe second challenge is how to improve the accuracy of approximation. To this end, $\\mathrm{Z}$ is iteratively updated by DSPGD.\n  More specifically, in the $t$-th iteration, an estimate $\\mathrm{\\xi}\\_{t}$ is constructed at users' devices, and then the estimate $\\mathrm{Z}\\_{t}$ at the cloud server is updated to $\\mathrm{Z}\\_{t+1}$ by applying DLA to a weighted sum of $\\mathrm{Z}\\_{t}$ and $\\mathrm{\\xi}\\_{t}$. \n\n In Section 4.2, we first point out that the process of obtaining an updated $\\mathrm{Z}_{t}$ results in high communication cost, because DLA is applied to matrices with the number of rows/columns equal to the number of data samples. To this end, we design a communication efficient mechanism (CEM) so that DLA is applied to a new type of matrices with reduced dimensions. We then present the design of the new type of matrices and the procedure of obtaining the top eigenpairs of $\\mathrm{K}$ in CEM. \n\n  In addition, to improve the clarity of the algorithm, we have moved the pseudo code of the algorithm back to the end of Section 4.2. \n\n2. The paper is written in a convoluted manner. This is the main limitation, at some point, we are talking about $k$-means, SVS, DLA, DSPGD, EVD, SPGD, a bunch of other methods that are coupled together towards the main approach.\n\nWe have revised the introduction and also Section 4.1 to clarify the relationship between our method with the bunch of other methods. For the detailed revision of the introduction, please refer to the response to comment 7.\n\nwe first point out that the key problem for designing FK $k$-means is to obtain the top eigenpair of the kernel matrix $\\mathrm{K}$ in a distributed manner. To solve this problem, we design a distributed stochastic proximal gradient descent (DSPGD) algorithm. We then present two challenges on solving the key problem and how we resolve these challenges. \n  \n  The first challenge is that $\\mathrm{K}$ is not available under federated settings. To this end, an estimate of $\\mathrm{K}$, denoted as $\\mathrm{\\xi}$, is jointly constructed at users' devices based on random features [1] of local data samples. Based on $\\mathrm{\\xi}$, an estimate of $\\mathrm{K}$, denoted as $\\mathrm{Z}$ can be obtained at the cloud server. Afterwards, an approximate version of the top eigenpairs of $\\mathrm{K}$ can be obtained from $\\mathrm{Z}$ through singular value decomposition (SVD).\n \n  The second challenge is how to improve the accuracy of approximation. To this end, $\\mathrm{Z}$ is iteratively updated by DSPGD.\n  More specifically, in the $t$-th iteration, an estimate $\\mathrm{\\xi}\\_{t}$ is constructed at users' devices, and then the estimate $\\mathrm{Z}\\_{t}$ at the cloud server is updated to $\\mathrm{Z}\\_{t+1}$ by applying DLA to a weighted sum of $\\mathrm{Z}\\_{t}$ and $\\mathrm{\\xi}\\_{t}$.\n  \n  After the revision, the relationship between FK $k$-means and other methods is clear, and it is easier to see the novelty and contribution of our work from the revised algorithm description.\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1799/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1799/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Communication Efficient Federated Kernel $k$-Means", "authorids": ["~Xiaochen_Zhou2", "wxudong@sjtu.edu.cn"], "authors": ["Xiaochen Zhou", "Xudong Wang"], "keywords": ["federated learning", "kernel $k$-means", "communication efficient"], "abstract": "A federated kernel $k$-means algorithm is developed in this paper. This algorithm resolves two challenging issues: 1) how to distributedly solve the optimization problem of kernel $k$-means under federated settings; 2) how to maintain communication efficiency in the algorithm. To tackle the first challenge, a distributed stochastic proximal gradient descent (DSPGD) algorithm is developed to determine an approximated solution to the optimization problem of kernel $k$-means. To tackle the second challenge, a communication efficient mechanism (CEM) is designed to reduce the communication cost. Besides, the federated kernel $k$-means provides two levels of privacy preservation: 1) users\u2019 local data are not exposed to the cloud server; 2) the cloud server cannot recover users\u2019 local data from the local computational results via matrix operations. Theoretical analysis shows: 1) DSPGD with CEM converges with an $O(1/T)$ rate, where $T$ is the number of iterations; 2) the communication cost of DSPGD with CEM is unrelated to the number of data samples; 3) the clustering quality of the federated kernel $k$-means approaches that of the standard kernel $k$-means, with a $(1+\\epsilon)$ approximate ratio. The experimental results show that the federated kernel $k$-means achieves the highest clustering quality with the communication cost reduced by more than $60\\%$ in most cases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|a_communication_efficient_federated_kernel_kmeans", "supplementary_material": "/attachment/226a3d584fed29f35f9dfff1b45b782aac8df0d5.zip", "pdf": "/pdf/918fbbf03a84f52157cc40ec901359434a10734c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=IyB_e1BjGJ", "_bibtex": "@misc{\nzhou2021a,\ntitle={A Communication Efficient Federated Kernel {\\$}k{\\$}-Means},\nauthor={Xiaochen Zhou and Xudong Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=tbwjUvUzQRU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "tbwjUvUzQRU", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1799/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1799/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1799/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1799/Authors|ICLR.cc/2021/Conference/Paper1799/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1799/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855621, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1799/-/Official_Comment"}}}, {"id": "HYQw5gSUZwt", "original": null, "number": 2, "cdate": 1606216870662, "ddate": null, "tcdate": 1606216870662, "tmdate": 1606216870662, "tddate": null, "forum": "tbwjUvUzQRU", "replyto": "-bzpPnS8Vmp", "invitation": "ICLR.cc/2021/Conference/Paper1799/-/Official_Comment", "content": {"title": "Responses to Q1~Q3", "comment": "We thank the reviewer for the positive comments and constructive suggestions on our paper.\n\n1. The whole Section 4 of the paper is filled with technical details and hard to follow. Considering the page limit, why not put the algorithms here, and put the details in the appendix?\n\nWe have moved the pseudo code of the algorithm back to Section 4.2, and moved some technical details to the appendix.\n\n\n2. It seems the proof techniques are somehow standard. The whole proof of Theorem 1 feels like an extension to [Zhang et al., 2016] in a distributed setting.\n\nThe difference between the proof in [Zhang et al., 2016] and our proof is as follows. \n  In [Zhang et al., 2016], the $(t+1)$-th solution $\\mathrm{Z}_{t+1}^*$ determined by the stochastic proximal gradient descent (SPGD) is the exact solution to the following problem\n\n$$\n\\min_{\\mathrm{Z} \\in \\mathbb{R}^{n \\times n}}{\\frac{1}{2}||\\mathrm{Z} - \\mathrm{Z}_{t}||_F^2 + \\eta_t \\langle \\mathrm{Z}}-\\mathrm{Z}_t,  \\mathrm{Z}_t - \\mathrm{\\xi}_t \\rangle + \\eta_t \\lambda ||\\mathrm{Z}||*\n$$\n\nHowever, in FK $k$-means, to reduce the communication cost, the $(t+1)$-th solution $\\mathrm{Z}_{t+1}$ determined by the distributed stochastic proximal gradient descent (DSPGD) is an approximate solution.\nSince it is unknown how these approximate solutions affect the convergence of DSPGD, we need to prove that the approximate solutions do not affect the convergence of DSPGD. For the details of the proof, please refer to the proof of Theorem~1 (from the equation (6) to equation (15)).\n\n\n\n3. My biggest concern is about novelty. Currently the proposed algorithm is heavily influenced by [Zhang et al., 2016] and [Wang et al., 2019]. If that is not the case, the authors can consider including a table to illustrate the difference between the algorithms.\n\nThe novelties of FK $k$-means are as follows. First, we design DSPGD to approach the top-$s$ eigenpairs of the kernel matrix $\\mathrm{K}$ under federated settings where raw data are maintained by users and the cloud cannot access the raw data. Second, a communication efficient mechanism (CEM) is designed to highly reduce the communication cost of DSPGD. \n\nAmong these two novelties, DSPGD is a counterpart of the stochastic proximal gradient descent (SPGD) algorithm in stochastic PCA [Zhang et al., 2016]. However, DSPGD is distinct from SPGD in the following three features.\n\nDSPGD is conducted under federated settings. However, SPGD is conducted in a centralized manner where users' raw data are collected at the cloud server.\n\nThe communication cost is considered in the design of DSPGD, which results in CEM, while the communication cost is not considered in SPGD.\n\nAlthough both DSPGD and SPGD aim at approaching the top eigenpairs of $\\mathrm{K}$, in the t-th iteration, DSPGD only needs to obtain an approximate solution $\\mathrm{Z}\\_{t+1}$ instead of the exact solution $\\mathrm{Z}_{t+1}^*$ to the same problem like SPGD, which leads to less communication cost under federated settings.\n\noreover, FK $k$-means is distinct from the scalable kernel $k$-means [Wang et al., 2019] (denoted as SK $k$-means) in the following two features.\n\nFK $k$-means approaches the top eigenvectors without collecting users' raw data or users' random features at the cloud server. However, SK $k$-means requires users to upload some of their raw data or random features to the cloud server to approximate the kernel matrix $\\mathrm{K}$.\n\nFK $k$-means is an iterative algorithm, and it can approach the top eigenvectors of $\\mathrm{K}$ more accurately by employing more iterations. In contrast to FK $k$-means, SK $k$-means is a one-shot algorithm, i.e., it only determines an approximate kernel matrix $\\tilde{\\mathrm{K}}$ once and then applies SVD to $\\tilde{\\mathrm{K}}$ to approach the top eigenvectors of $\\mathrm{K}$. Thus, the accuracy of these top eigenvectors is limited by the number of data samples or random features uploaded by users.\n\nReference\n\n[Zhang et al., 2016] Lijun Zhang, Tianbao Yang, Jinfeng Yi, Rong Jin, and Zhi-Hua Zhou. Stochastic optimization for kernel PCA. In Proceedings of the 30th AAAI Conference on Artificial Intelligence, pp. 2316\u2013\n2322, 2016.\n\n[Wang et al., 2019] Shusen Wang, Alex Gittens, and Michael W Mahoney. Scalable kernel k-means clustering with Nystr{\\\"o}m approximation: relative-error bounds. The Journal of Machine Learning Research, 20\n(1):431\u2013479, 2019\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1799/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1799/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Communication Efficient Federated Kernel $k$-Means", "authorids": ["~Xiaochen_Zhou2", "wxudong@sjtu.edu.cn"], "authors": ["Xiaochen Zhou", "Xudong Wang"], "keywords": ["federated learning", "kernel $k$-means", "communication efficient"], "abstract": "A federated kernel $k$-means algorithm is developed in this paper. This algorithm resolves two challenging issues: 1) how to distributedly solve the optimization problem of kernel $k$-means under federated settings; 2) how to maintain communication efficiency in the algorithm. To tackle the first challenge, a distributed stochastic proximal gradient descent (DSPGD) algorithm is developed to determine an approximated solution to the optimization problem of kernel $k$-means. To tackle the second challenge, a communication efficient mechanism (CEM) is designed to reduce the communication cost. Besides, the federated kernel $k$-means provides two levels of privacy preservation: 1) users\u2019 local data are not exposed to the cloud server; 2) the cloud server cannot recover users\u2019 local data from the local computational results via matrix operations. Theoretical analysis shows: 1) DSPGD with CEM converges with an $O(1/T)$ rate, where $T$ is the number of iterations; 2) the communication cost of DSPGD with CEM is unrelated to the number of data samples; 3) the clustering quality of the federated kernel $k$-means approaches that of the standard kernel $k$-means, with a $(1+\\epsilon)$ approximate ratio. The experimental results show that the federated kernel $k$-means achieves the highest clustering quality with the communication cost reduced by more than $60\\%$ in most cases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|a_communication_efficient_federated_kernel_kmeans", "supplementary_material": "/attachment/226a3d584fed29f35f9dfff1b45b782aac8df0d5.zip", "pdf": "/pdf/918fbbf03a84f52157cc40ec901359434a10734c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=IyB_e1BjGJ", "_bibtex": "@misc{\nzhou2021a,\ntitle={A Communication Efficient Federated Kernel {\\$}k{\\$}-Means},\nauthor={Xiaochen Zhou and Xudong Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=tbwjUvUzQRU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "tbwjUvUzQRU", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1799/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1799/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1799/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1799/Authors|ICLR.cc/2021/Conference/Paper1799/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1799/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855621, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1799/-/Official_Comment"}}}, {"id": "XJEFrCiFDDP", "original": null, "number": 4, "cdate": 1603984782640, "ddate": null, "tcdate": 1603984782640, "tmdate": 1605024355251, "tddate": null, "forum": "tbwjUvUzQRU", "replyto": "tbwjUvUzQRU", "invitation": "ICLR.cc/2021/Conference/Paper1799/-/Official_Review", "content": {"title": "A distributed method for kernel k means with some performance guarantees", "review": "Summary:\n\nThis paper proposes a distributed version of kernel k-means clustering where some federated structure is used to do distributed processing on the data. Privacy and communication issues are also studied. Numerical results are provided.\n\nReasons for the score:\n\nThis paper seems provide a new algorithm for distributed clustering. However, the way the algorithm is presented look like a patch of a number of things coming together one after the other with no general structure. This might be caused by the fact that the algorithm is only presented in the appendix. \n\nThe paper is written in a convoluted manner. This is the main limitation, at some point, we are talking about k means, SVS, DLA,DSPGD, EVD, SPGD, a bunch of other methods that are coupled together towards the main approach.\n\nProblem 1 seems to be an integer programming problem, thus with very high computation complexity. It is not clear how this is solved.\n\nIn the abstract please let me know what are those two levels of privacy you are talking about.\n\nIn the abstract, what does it mean that the clustering loss of the distributed method approaches the centralized one, please elaborate.\n\nWhy developing a federated learning algorithm is a promising approach? Please elaborate.\n\nThe second part of the intro turns into a detail technical analysis of the algorithm components, and so far we haven\u2019t seen the algorithm so it all remains a technical abstract  discussion that takes away the main messages.\n\nThe algorithm is in the appendix, so the description and analysis is made on an item that has not been presented in the main text.\n\nThe way the result is presented makes it look like the proposed method is a concatenation of other results, rather than the solution of a technical challenge in the problem.\n\nNumerical results are well presented,", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1799/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1799/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Communication Efficient Federated Kernel $k$-Means", "authorids": ["~Xiaochen_Zhou2", "wxudong@sjtu.edu.cn"], "authors": ["Xiaochen Zhou", "Xudong Wang"], "keywords": ["federated learning", "kernel $k$-means", "communication efficient"], "abstract": "A federated kernel $k$-means algorithm is developed in this paper. This algorithm resolves two challenging issues: 1) how to distributedly solve the optimization problem of kernel $k$-means under federated settings; 2) how to maintain communication efficiency in the algorithm. To tackle the first challenge, a distributed stochastic proximal gradient descent (DSPGD) algorithm is developed to determine an approximated solution to the optimization problem of kernel $k$-means. To tackle the second challenge, a communication efficient mechanism (CEM) is designed to reduce the communication cost. Besides, the federated kernel $k$-means provides two levels of privacy preservation: 1) users\u2019 local data are not exposed to the cloud server; 2) the cloud server cannot recover users\u2019 local data from the local computational results via matrix operations. Theoretical analysis shows: 1) DSPGD with CEM converges with an $O(1/T)$ rate, where $T$ is the number of iterations; 2) the communication cost of DSPGD with CEM is unrelated to the number of data samples; 3) the clustering quality of the federated kernel $k$-means approaches that of the standard kernel $k$-means, with a $(1+\\epsilon)$ approximate ratio. The experimental results show that the federated kernel $k$-means achieves the highest clustering quality with the communication cost reduced by more than $60\\%$ in most cases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|a_communication_efficient_federated_kernel_kmeans", "supplementary_material": "/attachment/226a3d584fed29f35f9dfff1b45b782aac8df0d5.zip", "pdf": "/pdf/918fbbf03a84f52157cc40ec901359434a10734c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=IyB_e1BjGJ", "_bibtex": "@misc{\nzhou2021a,\ntitle={A Communication Efficient Federated Kernel {\\$}k{\\$}-Means},\nauthor={Xiaochen Zhou and Xudong Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=tbwjUvUzQRU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "tbwjUvUzQRU", "replyto": "tbwjUvUzQRU", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1799/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538110470, "tmdate": 1606915761703, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1799/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1799/-/Official_Review"}}}], "count": 18}