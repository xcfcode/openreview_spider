{"notes": [{"id": "rygFmh0cKm", "original": "H1lqdpT9KX", "number": 1379, "cdate": 1538087969052, "ddate": null, "tcdate": 1538087969052, "tmdate": 1545355380052, "tddate": null, "forum": "rygFmh0cKm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "On Difficulties of Probability Distillation", "abstract": "Probability distillation has recently been of interest to deep learning practitioners as it presents a practical solution for sampling from autoregressive models for deployment in real-time applications. We identify a pathological optimization issue with the commonly adopted stochastic minimization of the (reverse) KL divergence, owing to sparse gradient signal from the teacher model due to curse of dimensionality. We also explore alternative principles for distillation, and show that one can achieve qualitatively better results than with KL minimization. \n", "keywords": ["Probability distillation", "Autoregressive models", "normalizing flows", "wavenet", "pixelcnn"], "authorids": ["chin-wei.huang@umontreal.ca", "faruk.ahmed.91@gmail.com", "kundankumar2510@gmail.com", "allac@elementai.com", "aaron.courville@gmail.com"], "authors": ["Chin-Wei Huang", "Faruk Ahmed", "Kundan Kumar", "Alexandre Lacoste", "Aaron Courville"], "TL;DR": "We point out an optimization issue of distillation with KL divergence, and explore different alternatives", "pdf": "/pdf/e76a476f62ce1f41c61c5595d1fb17db3a42c936.pdf", "paperhash": "huang|on_difficulties_of_probability_distillation", "_bibtex": "@misc{\nhuang2019on,\ntitle={On Difficulties of Probability Distillation},\nauthor={Chin-Wei Huang and Faruk Ahmed and Kundan Kumar and Alexandre Lacoste and Aaron Courville},\nyear={2019},\nurl={https://openreview.net/forum?id=rygFmh0cKm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "SJxLAQjbxN", "original": null, "number": 1, "cdate": 1544823758242, "ddate": null, "tcdate": 1544823758242, "tmdate": 1545354529385, "tddate": null, "forum": "rygFmh0cKm", "replyto": "rygFmh0cKm", "invitation": "ICLR.cc/2019/Conference/-/Paper1379/Meta_Review", "content": {"metareview": "The paper proposes new methods for optimization of optimization of KL(student_model||teacher_model). \n\nThe topic is relevant. The paper also contains interesting ideas and the proposed methods are interesting; they are elegant and seems to work reasonably well on the tasks tried.\n\nHowever, the reviewers do not all agree that the paper is well written. The reviewers have pointed out several issues that need to be addresses before the paper can be accepted.\n\n\n\n\n", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Meta-Review"}, "signatures": ["ICLR.cc/2019/Conference/Paper1379/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1379/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Difficulties of Probability Distillation", "abstract": "Probability distillation has recently been of interest to deep learning practitioners as it presents a practical solution for sampling from autoregressive models for deployment in real-time applications. We identify a pathological optimization issue with the commonly adopted stochastic minimization of the (reverse) KL divergence, owing to sparse gradient signal from the teacher model due to curse of dimensionality. We also explore alternative principles for distillation, and show that one can achieve qualitatively better results than with KL minimization. \n", "keywords": ["Probability distillation", "Autoregressive models", "normalizing flows", "wavenet", "pixelcnn"], "authorids": ["chin-wei.huang@umontreal.ca", "faruk.ahmed.91@gmail.com", "kundankumar2510@gmail.com", "allac@elementai.com", "aaron.courville@gmail.com"], "authors": ["Chin-Wei Huang", "Faruk Ahmed", "Kundan Kumar", "Alexandre Lacoste", "Aaron Courville"], "TL;DR": "We point out an optimization issue of distillation with KL divergence, and explore different alternatives", "pdf": "/pdf/e76a476f62ce1f41c61c5595d1fb17db3a42c936.pdf", "paperhash": "huang|on_difficulties_of_probability_distillation", "_bibtex": "@misc{\nhuang2019on,\ntitle={On Difficulties of Probability Distillation},\nauthor={Chin-Wei Huang and Faruk Ahmed and Kundan Kumar and Alexandre Lacoste and Aaron Courville},\nyear={2019},\nurl={https://openreview.net/forum?id=rygFmh0cKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1379/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352861559, "tddate": null, "super": null, "final": null, "reply": {"forum": "rygFmh0cKm", "replyto": "rygFmh0cKm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1379/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1379/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1379/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352861559}}}, {"id": "HkeCRRt7RX", "original": null, "number": 3, "cdate": 1542852310067, "ddate": null, "tcdate": 1542852310067, "tmdate": 1542852310067, "tddate": null, "forum": "rygFmh0cKm", "replyto": "SygyL6Otnm", "invitation": "ICLR.cc/2019/Conference/-/Paper1379/Official_Comment", "content": {"title": "Response to AnonReviewer3 ", "comment": "Thank you for the constructive feedback! We address the concerns below:\n\n1. Thanks for spotting the typo! We\u2019ve fixed it.\n\n2. We used c initially to express the formula generally, for any vector c. This is a stylistic choice, but we agree that it might be confusing for some readers, and for the sake of readability, we\u2019ve changed the c to rho.\n\n3. We've updated Sec3.1 in the hope to better motivate and explain the analysis. Here's some clarification. \n    a. We\u2019ve included a derivation and explanation of the path derivative in the appendix; in short, the path derivative consists of the gradient direction wrt the sample x, which directly affects how the parameters of the distribution will be updated to change the shape of p_S. \n    b. p_S is optimal when p_S=p_T (iff KL=0). In practice, what we found is that when the algorithm tries to minimize KL, p_S tends to fit to the mode of p_T and the mass is overly concentrated. This \u201cmode collapse\u201d problem, as manifested in practical issues such as the whispering characteristic present in the reverse-KL trained Parallel Wavenet, is indeed the key issue motivating our study!\n    c. p_T being a high dimensional gaussian is simply a \u201cmodel of the problem\u201d to demonstrate how stochastic optimization can be inefficient due to the \u201cunbalanced\u201d gradient distribution. Assume now p_S is sharper than p_T, the gradient in expectation should point to a direction that expands the probability mass of p_S (along the high density valley under p_T). Our analysis suggests that even when this is true in expectation, one might have exponentially low probability to sample a gradient with the required expansion signal: e.g. more contractive gradients with smaller magnitude and less expansive gradients with larger magnitude. \n\n4. Thanks for spotting the typo! We\u2019ve fixed it to set $\\mu=[2,...,2]^\\top$ to be a vector of $T$ $2$'s\n\n5. We\u2019ve updated our submission to include more details, which we also present here: For the vocoder experiment, we used the L1 loss, Gaussian IAF as in ClariNet. Each flow consists of 10 residual dilated convolution blocks (these are the standard blocks used in Parallel WaveNet) with kernel-width 2 and 64 output channels. We compute the regularized KL in closed form (for the baseline experiments with KL), using vocoder as input. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1379/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1379/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1379/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Difficulties of Probability Distillation", "abstract": "Probability distillation has recently been of interest to deep learning practitioners as it presents a practical solution for sampling from autoregressive models for deployment in real-time applications. We identify a pathological optimization issue with the commonly adopted stochastic minimization of the (reverse) KL divergence, owing to sparse gradient signal from the teacher model due to curse of dimensionality. We also explore alternative principles for distillation, and show that one can achieve qualitatively better results than with KL minimization. \n", "keywords": ["Probability distillation", "Autoregressive models", "normalizing flows", "wavenet", "pixelcnn"], "authorids": ["chin-wei.huang@umontreal.ca", "faruk.ahmed.91@gmail.com", "kundankumar2510@gmail.com", "allac@elementai.com", "aaron.courville@gmail.com"], "authors": ["Chin-Wei Huang", "Faruk Ahmed", "Kundan Kumar", "Alexandre Lacoste", "Aaron Courville"], "TL;DR": "We point out an optimization issue of distillation with KL divergence, and explore different alternatives", "pdf": "/pdf/e76a476f62ce1f41c61c5595d1fb17db3a42c936.pdf", "paperhash": "huang|on_difficulties_of_probability_distillation", "_bibtex": "@misc{\nhuang2019on,\ntitle={On Difficulties of Probability Distillation},\nauthor={Chin-Wei Huang and Faruk Ahmed and Kundan Kumar and Alexandre Lacoste and Aaron Courville},\nyear={2019},\nurl={https://openreview.net/forum?id=rygFmh0cKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1379/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616239, "tddate": null, "super": null, "final": null, "reply": {"forum": "rygFmh0cKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1379/Authors", "ICLR.cc/2019/Conference/Paper1379/Reviewers", "ICLR.cc/2019/Conference/Paper1379/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1379/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1379/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1379/Authors|ICLR.cc/2019/Conference/Paper1379/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1379/Reviewers", "ICLR.cc/2019/Conference/Paper1379/Authors", "ICLR.cc/2019/Conference/Paper1379/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616239}}}, {"id": "rkejbRFXA7", "original": null, "number": 2, "cdate": 1542852098898, "ddate": null, "tcdate": 1542852098898, "tmdate": 1542852098898, "tddate": null, "forum": "rygFmh0cKm", "replyto": "BJxBtwJqhX", "invitation": "ICLR.cc/2019/Conference/-/Paper1379/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "Thank you for your positive review and feedback! We address the comments below:\n\n1. Reverse KL minimization has been used in many other contexts other than the recent application of distilling an autoregressive model which we focus on in this paper. Most common applications have been in areas such as variational inference [1], variational continual learning [2], energy based GAN [3], policy based reinforcement learning [4], etc.\n\n2. The point of Proposition 2 is to show that z-recon loss behaves like a distance between T^{-1}(z) and S(z). So when z-recon is minimized, it implies S gets closer to T^{-1} in the sense of the induced metric. We\u2019ve updated the paper to explicitly state this. \n\n3. Thanks for the suggestion, but we believe you mean Sec 3.2 (IIUC). We added the pointer there in the new version.\n\nThank you again for the feedback and interest. \n\n[1] Auto-Encoding Variational Bayes\n[2] Variational Continual Learning\n[3] Calibrating Energy-based Generative Adversarial Networks\n[4] Latent Space Policies for Hierarchical Reinforcement Learning\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1379/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1379/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1379/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Difficulties of Probability Distillation", "abstract": "Probability distillation has recently been of interest to deep learning practitioners as it presents a practical solution for sampling from autoregressive models for deployment in real-time applications. We identify a pathological optimization issue with the commonly adopted stochastic minimization of the (reverse) KL divergence, owing to sparse gradient signal from the teacher model due to curse of dimensionality. We also explore alternative principles for distillation, and show that one can achieve qualitatively better results than with KL minimization. \n", "keywords": ["Probability distillation", "Autoregressive models", "normalizing flows", "wavenet", "pixelcnn"], "authorids": ["chin-wei.huang@umontreal.ca", "faruk.ahmed.91@gmail.com", "kundankumar2510@gmail.com", "allac@elementai.com", "aaron.courville@gmail.com"], "authors": ["Chin-Wei Huang", "Faruk Ahmed", "Kundan Kumar", "Alexandre Lacoste", "Aaron Courville"], "TL;DR": "We point out an optimization issue of distillation with KL divergence, and explore different alternatives", "pdf": "/pdf/e76a476f62ce1f41c61c5595d1fb17db3a42c936.pdf", "paperhash": "huang|on_difficulties_of_probability_distillation", "_bibtex": "@misc{\nhuang2019on,\ntitle={On Difficulties of Probability Distillation},\nauthor={Chin-Wei Huang and Faruk Ahmed and Kundan Kumar and Alexandre Lacoste and Aaron Courville},\nyear={2019},\nurl={https://openreview.net/forum?id=rygFmh0cKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1379/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616239, "tddate": null, "super": null, "final": null, "reply": {"forum": "rygFmh0cKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1379/Authors", "ICLR.cc/2019/Conference/Paper1379/Reviewers", "ICLR.cc/2019/Conference/Paper1379/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1379/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1379/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1379/Authors|ICLR.cc/2019/Conference/Paper1379/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1379/Reviewers", "ICLR.cc/2019/Conference/Paper1379/Authors", "ICLR.cc/2019/Conference/Paper1379/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616239}}}, {"id": "B1lvYTFX0X", "original": null, "number": 1, "cdate": 1542851967436, "ddate": null, "tcdate": 1542851967436, "tmdate": 1542851967436, "tddate": null, "forum": "rygFmh0cKm", "replyto": "rJllYM08p7", "invitation": "ICLR.cc/2019/Conference/-/Paper1379/Official_Comment", "content": {"title": "Response to AnonReviewer4", "comment": "Thank you for the close reading and detailed feedback! We address the comments below:\n\n1. Indeed, by \u201csparsity\u201c, we mean the probability of effective gradient signals that point away from the mode of the teacher is small, the complement of which are the gradient signals that are either zero or point towards the model of the teacher (when this is viewed binarily: point-toward being 0 and point-away being 1, it means signal 1 is sparse). We have tried to make it clearer in the paper by paraphrasing it as the gradient distribution being skewed, or imbalanced over the orientation of push with respect to the origin.\n\n2. Depending on the chosen family of the student, this might have different effects. (we've updated the paper to include this discussion and better motivate the analysis in Sec3.1)\n    a. If p_S is independent gaussian (mean field assumption), the best student will be extremely concentrated at the mode of the teacher in high dimension. Here the optimality according to the reverse KL will more likely sacrifice the norm of the samples drawn from the student (see Fig1.3 of [1]). Even in this case, the probability of point-away signal will still not be the same as the probability of point-toward signal. They are only equal when weighted by the magnitude (which means the expected gradient is zero at optimality). \n    b. If p_S is multivariate gaussian,\n        (i) One can rotate and rescale both p_T and p_S according to the covariance matrix of p_S, such that the latter once again becomes standard normal. Doing this is to show that our analysis is without loss of generality: the probability of receiving a point-away gradient signal is determined by the \u201crelative\u201d covariance of p_T (after transformation under covariance of p_S) with respect to the standard normal. \n        (ii) As we argue in the paper, as long as there is correlation present in the now transformed p_T, the condition coefficient (eq3) can be extremely small due to the exponential decrease in the volume of hyper-cone (shaded area of Fig1b). The training algorithm can constantly make progress \u201cin expectation\u201d, but since we\u2019re using SGD in practice, the rate at which p_S becomes better now depends on how likely it is to get a point-away signal, assuming p_S fits to the mode of p_T first. The latter assumption and gradient sparsity posing a problem for optimization were also validated by our experiment (Fig 2a). \n\n3. The Neural Vocoder experiment used closed-form KL and the rest used monte carlo estimate. \n\n4. We\u2019ve updated our paper to include more experimental details on the neural vocoder experiment. In particular, we have clarified that we use closed form reverse KL proposed in ClariNet. Each of our flow consists of 10 residual dilated convolution block with kernel width of 2 and 64 output channels.\n\n5. We\u2019ve added one more comparison with reconstruction loss + power loss, which is included in the following link: https://soundcloud.com/inverse-matching/sets/samples-for-inverse-matching\n\nAgain, we thank you for your constructive feedback. \n\n[1] Two problems with variational expectation maximisation for time-series models\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1379/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1379/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1379/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Difficulties of Probability Distillation", "abstract": "Probability distillation has recently been of interest to deep learning practitioners as it presents a practical solution for sampling from autoregressive models for deployment in real-time applications. We identify a pathological optimization issue with the commonly adopted stochastic minimization of the (reverse) KL divergence, owing to sparse gradient signal from the teacher model due to curse of dimensionality. We also explore alternative principles for distillation, and show that one can achieve qualitatively better results than with KL minimization. \n", "keywords": ["Probability distillation", "Autoregressive models", "normalizing flows", "wavenet", "pixelcnn"], "authorids": ["chin-wei.huang@umontreal.ca", "faruk.ahmed.91@gmail.com", "kundankumar2510@gmail.com", "allac@elementai.com", "aaron.courville@gmail.com"], "authors": ["Chin-Wei Huang", "Faruk Ahmed", "Kundan Kumar", "Alexandre Lacoste", "Aaron Courville"], "TL;DR": "We point out an optimization issue of distillation with KL divergence, and explore different alternatives", "pdf": "/pdf/e76a476f62ce1f41c61c5595d1fb17db3a42c936.pdf", "paperhash": "huang|on_difficulties_of_probability_distillation", "_bibtex": "@misc{\nhuang2019on,\ntitle={On Difficulties of Probability Distillation},\nauthor={Chin-Wei Huang and Faruk Ahmed and Kundan Kumar and Alexandre Lacoste and Aaron Courville},\nyear={2019},\nurl={https://openreview.net/forum?id=rygFmh0cKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1379/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616239, "tddate": null, "super": null, "final": null, "reply": {"forum": "rygFmh0cKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1379/Authors", "ICLR.cc/2019/Conference/Paper1379/Reviewers", "ICLR.cc/2019/Conference/Paper1379/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1379/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1379/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1379/Authors|ICLR.cc/2019/Conference/Paper1379/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1379/Reviewers", "ICLR.cc/2019/Conference/Paper1379/Authors", "ICLR.cc/2019/Conference/Paper1379/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616239}}}, {"id": "rJllYM08p7", "original": null, "number": 3, "cdate": 1542017656361, "ddate": null, "tcdate": 1542017656361, "tmdate": 1542017656361, "tddate": null, "forum": "rygFmh0cKm", "replyto": "rygFmh0cKm", "invitation": "ICLR.cc/2019/Conference/-/Paper1379/Official_Review", "content": {"title": "Interesting ideas but the paper has some issues", "review": "This paper proposes new methods for distilling a feed-forward generative model (student) from an autoregressive generative model (teacher) as an alternative to the reverse-KL divergence. The first part of the paper analyses optimization issues with the reverse KL divergence while in the second part of the paper alternatives are proposed (x-reconstruction and z-reconstruction).\n\nDetailed comments:\n\n1.\nIn abstract and other places: \"sparse gradient signal from the teacher\".\nSparsity implies that many of the values are exactly zero, while Section 3.1 seems to imply that some of the values might be small (or pointing towards the origin).\n\n2.\nIn Section 3.1 and 3.2 the authors discuss a potential failure mode of the reverse KL:\n\nBut, proposition 1 boils down to the fact that if the student's mass is more spread out than the teacher is some direction, that it should shrink that mass closer to zero as well.\n\nIn the example of the paper: if an eigenvalue of T is smaller than 1, it would mean that the student which is spherical Gaussian, would adjust its probability mass to also be smaller in that eigenvector's direction.\n\nAs training progresses, the students mass would be much closer to the teacher and the probability of 'pointing away' from the origin would be about as likely as pointing towards.\n\nSo it's not clear at all that the described property is problematic for optimization, as it could as well be interpreted as the student trying to fit the teacher's distribution better.\n\n3.\nWas the KL between P_S(x_i | z_<i) and P_T(x_i | x_<i) computed analytically? If these conditional distributions are Gaussian (which they are in many of the examples) this should be trivial.\n\n4.\nSection 4 about the neural vocoder needs to be expanded: many details are missing here and although it's one of the more important experiments in the paper it's relatively neglected compared to the other parts of the paper.\n\n5.\nIn the Section 4: the experiment with reverse-KL is a straw man comparison: For audio the reverse KL was only proposed in combination with the power loss (Oord et al). Two additional experiments would make the result a lot stronger: KL+power-loss and X-recon+power-loss. Because if the x-recon method does not work well together with the power-loss, its practical applicability seems limited.\n\n\nThe proposed methods are interesting, because they are elegant and seems to work reasonably well on the tasks tried. The first part of the paper about gradient sparsity/orientation needs to be addressed. Section 4 should be expanded and an additional comparison should be made.\n\nI would change my rating if these issues were addressed.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1379/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Difficulties of Probability Distillation", "abstract": "Probability distillation has recently been of interest to deep learning practitioners as it presents a practical solution for sampling from autoregressive models for deployment in real-time applications. We identify a pathological optimization issue with the commonly adopted stochastic minimization of the (reverse) KL divergence, owing to sparse gradient signal from the teacher model due to curse of dimensionality. We also explore alternative principles for distillation, and show that one can achieve qualitatively better results than with KL minimization. \n", "keywords": ["Probability distillation", "Autoregressive models", "normalizing flows", "wavenet", "pixelcnn"], "authorids": ["chin-wei.huang@umontreal.ca", "faruk.ahmed.91@gmail.com", "kundankumar2510@gmail.com", "allac@elementai.com", "aaron.courville@gmail.com"], "authors": ["Chin-Wei Huang", "Faruk Ahmed", "Kundan Kumar", "Alexandre Lacoste", "Aaron Courville"], "TL;DR": "We point out an optimization issue of distillation with KL divergence, and explore different alternatives", "pdf": "/pdf/e76a476f62ce1f41c61c5595d1fb17db3a42c936.pdf", "paperhash": "huang|on_difficulties_of_probability_distillation", "_bibtex": "@misc{\nhuang2019on,\ntitle={On Difficulties of Probability Distillation},\nauthor={Chin-Wei Huang and Faruk Ahmed and Kundan Kumar and Alexandre Lacoste and Aaron Courville},\nyear={2019},\nurl={https://openreview.net/forum?id=rygFmh0cKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1379/Official_Review", "cdate": 1542234242552, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rygFmh0cKm", "replyto": "rygFmh0cKm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1379/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335934037, "tmdate": 1552335934037, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1379/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJxBtwJqhX", "original": null, "number": 2, "cdate": 1541171068559, "ddate": null, "tcdate": 1541171068559, "tmdate": 1541533183220, "tddate": null, "forum": "rygFmh0cKm", "replyto": "rygFmh0cKm", "invitation": "ICLR.cc/2019/Conference/-/Paper1379/Official_Review", "content": {"title": "Convincing paper about of a potentially not-too-widespread technical issue.", "review": "The paper studies the problem of distilling a student probabilistic model (that\nis easy to sample from) from a complex teacher model (for which sampling is\nslow).  The authors identify a technical issue with a recent distillation\ntechnique, namely that positive gradient signals become increasingly unlikely\nas the dimensionality of the teacher model increases.  They then propose two\nalternative technique that sidestep this issue.\n\nThe topic is definitely relevant.  The paper focus on a single method for\nprobability distillation, which limits the significance of the contribution.\n\nThe paper is very well written and well structured.  Section 4 is may be a bit\ntoo dense for the uninitiated; it may make sense to clarify that calT and calS\nrefer to the teacher and student models---it is only obvious while reading this\nsection for the second time around.\n\nAll contributions seem novel.  The fact that the (reverse) KL can lead to bad\nmodels is known; the issue identified in this paper, however, seems novel.\n\nI could not spot any major flaws with the paper.\n\nThe evaluation is satisfactory.  The issue of KL-based training is very clear,\nas is the advantage of the encoder-decoder alternatives.\n\nI especially appreciated the link between distillation and encoder-decoder\narchitectures.\n\nDetailed comments:\n\n1 - How widespread is the issue identified in this paper?  In other words, is\nreverse KL realistically used in applications other than probability\ndistillation?\n\n2 - It is unclear to me why Proposition 2 is important.  This should be\nexplicitly stated.\n\n3 - It would make sense to add a forward pointer to Figure 3c in Section 3.1,\nto provide another example of mode-seeking.", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper1379/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Difficulties of Probability Distillation", "abstract": "Probability distillation has recently been of interest to deep learning practitioners as it presents a practical solution for sampling from autoregressive models for deployment in real-time applications. We identify a pathological optimization issue with the commonly adopted stochastic minimization of the (reverse) KL divergence, owing to sparse gradient signal from the teacher model due to curse of dimensionality. We also explore alternative principles for distillation, and show that one can achieve qualitatively better results than with KL minimization. \n", "keywords": ["Probability distillation", "Autoregressive models", "normalizing flows", "wavenet", "pixelcnn"], "authorids": ["chin-wei.huang@umontreal.ca", "faruk.ahmed.91@gmail.com", "kundankumar2510@gmail.com", "allac@elementai.com", "aaron.courville@gmail.com"], "authors": ["Chin-Wei Huang", "Faruk Ahmed", "Kundan Kumar", "Alexandre Lacoste", "Aaron Courville"], "TL;DR": "We point out an optimization issue of distillation with KL divergence, and explore different alternatives", "pdf": "/pdf/e76a476f62ce1f41c61c5595d1fb17db3a42c936.pdf", "paperhash": "huang|on_difficulties_of_probability_distillation", "_bibtex": "@misc{\nhuang2019on,\ntitle={On Difficulties of Probability Distillation},\nauthor={Chin-Wei Huang and Faruk Ahmed and Kundan Kumar and Alexandre Lacoste and Aaron Courville},\nyear={2019},\nurl={https://openreview.net/forum?id=rygFmh0cKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1379/Official_Review", "cdate": 1542234242552, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rygFmh0cKm", "replyto": "rygFmh0cKm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1379/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335934037, "tmdate": 1552335934037, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1379/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SygyL6Otnm", "original": null, "number": 1, "cdate": 1541143879233, "ddate": null, "tcdate": 1541143879233, "tmdate": 1541533182969, "tddate": null, "forum": "rygFmh0cKm", "replyto": "rygFmh0cKm", "invitation": "ICLR.cc/2019/Conference/-/Paper1379/Official_Review", "content": {"title": "Interesting idea & fair results", "review": "This paper analyzes the limitation of probability density distillation with reverse KL divergence, and proposes two practical methods for probability distillation.\n\nDetailed comments:\n\n1) Typo: should be WaveNet, not Wavenet.\n\n2) In Proposition 1. $c_i$ should be $\\rho_i$.\n\n3) One may explain \u201cpath derivative\u201d with more details. Also, I am really confused by Proposition 1 and its underlying implication. Given p_s and p_t are centered at the origin, isn\u2019t p_s(x) already the optimal if it\u2019s just a unit Gaussian. Why do we need a derivative pointing away from the origin? At least, one need parameterize p_s as N(0, \\phi)?\n\n4) In section 3.2, \u201cset $\\mu = [2, 2]^T$\u201d? Isn\u2019t $\\mu$ a T dimensional vector?\n\n5) A lot of important details are missing in neural vocoder experiment. For x-reconstruction, do you use L1 or L2 loss?  For student model, do you use Gaussian IAF with WaveNet architecture as in ClariNet, or Logistic IAF as in Parallel WaveNet? Following this question, do you compute KLD in closed-form? Do you use the regularization term introduced in ClariNet? Student with KL loss and power loss outperforms x-reconstruction. Did you try x-reconstruction along with power loss?\n\nPros:\nCertainly, there are some interesting ideas in this paper. \n\nCons:\nThe experiment results are not good enough. The paper is poorly written. A lot of important details are missing.  \n\nHowever, I would like to raise my rating to 6, if these comments can be properly addressed.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1379/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Difficulties of Probability Distillation", "abstract": "Probability distillation has recently been of interest to deep learning practitioners as it presents a practical solution for sampling from autoregressive models for deployment in real-time applications. We identify a pathological optimization issue with the commonly adopted stochastic minimization of the (reverse) KL divergence, owing to sparse gradient signal from the teacher model due to curse of dimensionality. We also explore alternative principles for distillation, and show that one can achieve qualitatively better results than with KL minimization. \n", "keywords": ["Probability distillation", "Autoregressive models", "normalizing flows", "wavenet", "pixelcnn"], "authorids": ["chin-wei.huang@umontreal.ca", "faruk.ahmed.91@gmail.com", "kundankumar2510@gmail.com", "allac@elementai.com", "aaron.courville@gmail.com"], "authors": ["Chin-Wei Huang", "Faruk Ahmed", "Kundan Kumar", "Alexandre Lacoste", "Aaron Courville"], "TL;DR": "We point out an optimization issue of distillation with KL divergence, and explore different alternatives", "pdf": "/pdf/e76a476f62ce1f41c61c5595d1fb17db3a42c936.pdf", "paperhash": "huang|on_difficulties_of_probability_distillation", "_bibtex": "@misc{\nhuang2019on,\ntitle={On Difficulties of Probability Distillation},\nauthor={Chin-Wei Huang and Faruk Ahmed and Kundan Kumar and Alexandre Lacoste and Aaron Courville},\nyear={2019},\nurl={https://openreview.net/forum?id=rygFmh0cKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1379/Official_Review", "cdate": 1542234242552, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rygFmh0cKm", "replyto": "rygFmh0cKm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1379/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335934037, "tmdate": 1552335934037, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1379/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 8}