{"notes": [{"id": "HJeO7RNKPr", "original": "B1lxuqHOPr", "number": 1043, "cdate": 1569439263889, "ddate": null, "tcdate": 1569439263889, "tmdate": 1583912047772, "tddate": null, "forum": "HJeO7RNKPr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["zteed@princeton.edu", "jiadeng@princeton.edu"], "title": "DeepV2D: Video to Depth with Differentiable Structure from Motion", "authors": ["Zachary Teed", "Jia Deng"], "pdf": "/pdf/f6187767d5b3cf159a7455235ee961feeb044e87.pdf", "TL;DR": "DeepV2D predicts depth from a video clip by composing elements of classical SfM into a fully differentiable network.", "abstract": "We propose DeepV2D, an end-to-end deep learning architecture for predicting depth from video.  DeepV2D combines the representation ability of neural networks with the geometric principles governing image formation. We compose a collection of classical geometric algorithms, which are converted into trainable modules and combined into an end-to-end differentiable architecture. DeepV2D interleaves two stages: motion estimation and depth estimation. During inference, motion and depth estimation are alternated and converge to accurate depth. ", "keywords": ["Structure-from-Motion", "Video to Depth", "Dense Depth Estimation"], "paperhash": "teed|deepv2d_video_to_depth_with_differentiable_structure_from_motion", "_bibtex": "@inproceedings{\nTeed2020DeepV2D:,\ntitle={DeepV2D: Video to Depth with Differentiable Structure from Motion},\nauthor={Zachary Teed and Jia Deng},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeO7RNKPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/15280540ca4827718d9a8ea56a8671e22bf14a2a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "eNIoRUovnm", "original": null, "number": 1, "cdate": 1576798713052, "ddate": null, "tcdate": 1576798713052, "tmdate": 1576800923384, "tddate": null, "forum": "HJeO7RNKPr", "replyto": "HJeO7RNKPr", "invitation": "ICLR.cc/2020/Conference/Paper1043/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This work proposes a CNN architecture for joint depth and camera motion estimation from videos. The paper presents a differentiable formulation of the problem to allow its end-to-end learning, and the reviewers unanimously find the proposed approach reasonable and agree that this is a solid paper. Some of the reviewers find the method itself to be too mechanical, but they all agree that this is a well-engineered solution.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zteed@princeton.edu", "jiadeng@princeton.edu"], "title": "DeepV2D: Video to Depth with Differentiable Structure from Motion", "authors": ["Zachary Teed", "Jia Deng"], "pdf": "/pdf/f6187767d5b3cf159a7455235ee961feeb044e87.pdf", "TL;DR": "DeepV2D predicts depth from a video clip by composing elements of classical SfM into a fully differentiable network.", "abstract": "We propose DeepV2D, an end-to-end deep learning architecture for predicting depth from video.  DeepV2D combines the representation ability of neural networks with the geometric principles governing image formation. We compose a collection of classical geometric algorithms, which are converted into trainable modules and combined into an end-to-end differentiable architecture. DeepV2D interleaves two stages: motion estimation and depth estimation. During inference, motion and depth estimation are alternated and converge to accurate depth. ", "keywords": ["Structure-from-Motion", "Video to Depth", "Dense Depth Estimation"], "paperhash": "teed|deepv2d_video_to_depth_with_differentiable_structure_from_motion", "_bibtex": "@inproceedings{\nTeed2020DeepV2D:,\ntitle={DeepV2D: Video to Depth with Differentiable Structure from Motion},\nauthor={Zachary Teed and Jia Deng},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeO7RNKPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/15280540ca4827718d9a8ea56a8671e22bf14a2a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJeO7RNKPr", "replyto": "HJeO7RNKPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795706986, "tmdate": 1576800255128, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1043/-/Decision"}}}, {"id": "Bke2zsVniH", "original": null, "number": 1, "cdate": 1573829395725, "ddate": null, "tcdate": 1573829395725, "tmdate": 1573833729591, "tddate": null, "forum": "HJeO7RNKPr", "replyto": "B1x0lB_6qH", "invitation": "ICLR.cc/2020/Conference/Paper1043/-/Official_Comment", "content": {"title": "Thank you for your review and suggestions.", "comment": "Thank you for your review and suggestions. We have submitted a revised version following your suggestions. Below we address individual points. \n\n1. Our overall approach is quite modular and components can easily be swapped depending on the application. We add Appendix C in our revision which demonstrates several versions along with parameter counts, timing information, peak memory usage, and depth accuracy. In terms of raw parameter counts, our method uses fewer parameters than the single-image depth baseline (Laina et al., 2016). \nFollowing your suggestions, we implement a simpler version of our model (1-HG) where we replace the feature extractor with a single 2d-hourglass network and replace the stereo network with a single 3d-hourglass network. The results in Table 6 show that this causes Abs-Rel to increase from 0.065 to 0.071, which is still significantly better than prior work.\n\nWe also test a version where we replace the 3d stereo network with a correlation layer and 2d encoder-decoder (Ours (corr) Table 6). We take the correlation between features over the same depth range as we use to build the 3D cost volume, then concatenate the correlation response with features from the keyframe image, similar to DispNet (Mayer et al., 2016). The correlation version performs worse,  increasing Abs-Rel from 0.065 to 0.135. This is consistent with prior work which has demonstrated that 3D cost volumes give better performance than direct correlation (Kendall et al., 2017; Chang & Chen, 2018).\n\n2. \u201cComparison to DSO\u201d.  We\u2019ve added pose results from DSO to Table 2. We test DSO on ScanNet because the TUM-RGBD dataset is captured using a rolling shutter camera . DSO fails to initialize or diverges on 335/2000 of the videos, so we only report results on the test sequences where tracking is successful. Using single-image depth for initialization improves the performance of DSO, reducing the failure rate from 335 to 271 of the 2000 videos and giving slightly better pose metrics, but the estimated poses are still less accurate than our method; the rotation and translational angular error of DSO is (R: 0.946, T: 19.238) while our method gets significantly more accurate poses (R: 0.628, T: 10.800).\n\n3. Thank you for this suggestion, we\u2019ve updated the derivation in Eq. 12 to use the adjoint.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1043/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1043/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zteed@princeton.edu", "jiadeng@princeton.edu"], "title": "DeepV2D: Video to Depth with Differentiable Structure from Motion", "authors": ["Zachary Teed", "Jia Deng"], "pdf": "/pdf/f6187767d5b3cf159a7455235ee961feeb044e87.pdf", "TL;DR": "DeepV2D predicts depth from a video clip by composing elements of classical SfM into a fully differentiable network.", "abstract": "We propose DeepV2D, an end-to-end deep learning architecture for predicting depth from video.  DeepV2D combines the representation ability of neural networks with the geometric principles governing image formation. We compose a collection of classical geometric algorithms, which are converted into trainable modules and combined into an end-to-end differentiable architecture. DeepV2D interleaves two stages: motion estimation and depth estimation. During inference, motion and depth estimation are alternated and converge to accurate depth. ", "keywords": ["Structure-from-Motion", "Video to Depth", "Dense Depth Estimation"], "paperhash": "teed|deepv2d_video_to_depth_with_differentiable_structure_from_motion", "_bibtex": "@inproceedings{\nTeed2020DeepV2D:,\ntitle={DeepV2D: Video to Depth with Differentiable Structure from Motion},\nauthor={Zachary Teed and Jia Deng},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeO7RNKPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/15280540ca4827718d9a8ea56a8671e22bf14a2a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJeO7RNKPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1043/Authors", "ICLR.cc/2020/Conference/Paper1043/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1043/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1043/Reviewers", "ICLR.cc/2020/Conference/Paper1043/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1043/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1043/Authors|ICLR.cc/2020/Conference/Paper1043/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162137, "tmdate": 1576860557538, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1043/Authors", "ICLR.cc/2020/Conference/Paper1043/Reviewers", "ICLR.cc/2020/Conference/Paper1043/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1043/-/Official_Comment"}}}, {"id": "SJxr1aEhir", "original": null, "number": 4, "cdate": 1573829852751, "ddate": null, "tcdate": 1573829852751, "tmdate": 1573829852751, "tddate": null, "forum": "HJeO7RNKPr", "replyto": "BkgjyQ7ccr", "invitation": "ICLR.cc/2020/Conference/Paper1043/-/Official_Comment", "content": {"title": "Thank you for your review", "comment": "Thank you for your review and comments."}, "signatures": ["ICLR.cc/2020/Conference/Paper1043/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1043/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zteed@princeton.edu", "jiadeng@princeton.edu"], "title": "DeepV2D: Video to Depth with Differentiable Structure from Motion", "authors": ["Zachary Teed", "Jia Deng"], "pdf": "/pdf/f6187767d5b3cf159a7455235ee961feeb044e87.pdf", "TL;DR": "DeepV2D predicts depth from a video clip by composing elements of classical SfM into a fully differentiable network.", "abstract": "We propose DeepV2D, an end-to-end deep learning architecture for predicting depth from video.  DeepV2D combines the representation ability of neural networks with the geometric principles governing image formation. We compose a collection of classical geometric algorithms, which are converted into trainable modules and combined into an end-to-end differentiable architecture. DeepV2D interleaves two stages: motion estimation and depth estimation. During inference, motion and depth estimation are alternated and converge to accurate depth. ", "keywords": ["Structure-from-Motion", "Video to Depth", "Dense Depth Estimation"], "paperhash": "teed|deepv2d_video_to_depth_with_differentiable_structure_from_motion", "_bibtex": "@inproceedings{\nTeed2020DeepV2D:,\ntitle={DeepV2D: Video to Depth with Differentiable Structure from Motion},\nauthor={Zachary Teed and Jia Deng},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeO7RNKPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/15280540ca4827718d9a8ea56a8671e22bf14a2a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJeO7RNKPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1043/Authors", "ICLR.cc/2020/Conference/Paper1043/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1043/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1043/Reviewers", "ICLR.cc/2020/Conference/Paper1043/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1043/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1043/Authors|ICLR.cc/2020/Conference/Paper1043/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162137, "tmdate": 1576860557538, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1043/Authors", "ICLR.cc/2020/Conference/Paper1043/Reviewers", "ICLR.cc/2020/Conference/Paper1043/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1043/-/Official_Comment"}}}, {"id": "rJxjhoEhsH", "original": null, "number": 3, "cdate": 1573829554701, "ddate": null, "tcdate": 1573829554701, "tmdate": 1573829554701, "tddate": null, "forum": "HJeO7RNKPr", "replyto": "SJlDS4yrcS", "invitation": "ICLR.cc/2020/Conference/Paper1043/-/Official_Comment", "content": {"title": "Thank you for your review", "comment": " In the revision, we\u2019ve added more analysis of the experiments to provide more understanding of where the performance gain is coming from. The performance of our method is from a combination of factors, including improvements in both depth and motion, and how they are combined into a single network. The improvement in motion comes from embedding a least-squares optimization layer in the network which jointly updates the poses of all cameras, which gives better results than using generic network layers to predict camera motion (Table 5).  We achieve good depth by adapting a stereo network (Kendall et al., 2017) for monocular video, and show that alternating motion and depth updates at inference time converges to accurate depth and motion (Tables 1-4).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1043/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1043/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zteed@princeton.edu", "jiadeng@princeton.edu"], "title": "DeepV2D: Video to Depth with Differentiable Structure from Motion", "authors": ["Zachary Teed", "Jia Deng"], "pdf": "/pdf/f6187767d5b3cf159a7455235ee961feeb044e87.pdf", "TL;DR": "DeepV2D predicts depth from a video clip by composing elements of classical SfM into a fully differentiable network.", "abstract": "We propose DeepV2D, an end-to-end deep learning architecture for predicting depth from video.  DeepV2D combines the representation ability of neural networks with the geometric principles governing image formation. We compose a collection of classical geometric algorithms, which are converted into trainable modules and combined into an end-to-end differentiable architecture. DeepV2D interleaves two stages: motion estimation and depth estimation. During inference, motion and depth estimation are alternated and converge to accurate depth. ", "keywords": ["Structure-from-Motion", "Video to Depth", "Dense Depth Estimation"], "paperhash": "teed|deepv2d_video_to_depth_with_differentiable_structure_from_motion", "_bibtex": "@inproceedings{\nTeed2020DeepV2D:,\ntitle={DeepV2D: Video to Depth with Differentiable Structure from Motion},\nauthor={Zachary Teed and Jia Deng},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeO7RNKPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/15280540ca4827718d9a8ea56a8671e22bf14a2a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJeO7RNKPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1043/Authors", "ICLR.cc/2020/Conference/Paper1043/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1043/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1043/Reviewers", "ICLR.cc/2020/Conference/Paper1043/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1043/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1043/Authors|ICLR.cc/2020/Conference/Paper1043/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162137, "tmdate": 1576860557538, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1043/Authors", "ICLR.cc/2020/Conference/Paper1043/Reviewers", "ICLR.cc/2020/Conference/Paper1043/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1043/-/Official_Comment"}}}, {"id": "ByeyusN2jH", "original": null, "number": 2, "cdate": 1573829478949, "ddate": null, "tcdate": 1573829478949, "tmdate": 1573829478949, "tddate": null, "forum": "HJeO7RNKPr", "replyto": "BJe8GmY2FS", "invitation": "ICLR.cc/2020/Conference/Paper1043/-/Official_Comment", "content": {"title": "Thank you for your review and comments.", "comment": "The minor issues are corrected in the revision, thank you for pointing us to these typos.\n\nQ. What happens if more Gauss-Newton steps are made?\n\nUsing more Gauss-Newton steps actually doesn\u2019t give any increase in performance (increasing from 1 to 3 gives the same MRE on NYU). The reason is that minimizing geometric reprojection error is a relatively easy optimization problem. When camera motion is small, the second order approximation of the objective function is very good, so a single step is sufficient.\n\nHowever, as shown in Figure 5 (left), we can improve efficiency with a slight loss of accuracy by using fewer global iterations (where each iteration is a full pass of the motion and depth networks). \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1043/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1043/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zteed@princeton.edu", "jiadeng@princeton.edu"], "title": "DeepV2D: Video to Depth with Differentiable Structure from Motion", "authors": ["Zachary Teed", "Jia Deng"], "pdf": "/pdf/f6187767d5b3cf159a7455235ee961feeb044e87.pdf", "TL;DR": "DeepV2D predicts depth from a video clip by composing elements of classical SfM into a fully differentiable network.", "abstract": "We propose DeepV2D, an end-to-end deep learning architecture for predicting depth from video.  DeepV2D combines the representation ability of neural networks with the geometric principles governing image formation. We compose a collection of classical geometric algorithms, which are converted into trainable modules and combined into an end-to-end differentiable architecture. DeepV2D interleaves two stages: motion estimation and depth estimation. During inference, motion and depth estimation are alternated and converge to accurate depth. ", "keywords": ["Structure-from-Motion", "Video to Depth", "Dense Depth Estimation"], "paperhash": "teed|deepv2d_video_to_depth_with_differentiable_structure_from_motion", "_bibtex": "@inproceedings{\nTeed2020DeepV2D:,\ntitle={DeepV2D: Video to Depth with Differentiable Structure from Motion},\nauthor={Zachary Teed and Jia Deng},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeO7RNKPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/15280540ca4827718d9a8ea56a8671e22bf14a2a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJeO7RNKPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1043/Authors", "ICLR.cc/2020/Conference/Paper1043/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1043/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1043/Reviewers", "ICLR.cc/2020/Conference/Paper1043/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1043/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1043/Authors|ICLR.cc/2020/Conference/Paper1043/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162137, "tmdate": 1576860557538, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1043/Authors", "ICLR.cc/2020/Conference/Paper1043/Reviewers", "ICLR.cc/2020/Conference/Paper1043/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1043/-/Official_Comment"}}}, {"id": "BJe8GmY2FS", "original": null, "number": 1, "cdate": 1571750669540, "ddate": null, "tcdate": 1571750669540, "tmdate": 1572972519759, "tddate": null, "forum": "HJeO7RNKPr", "replyto": "HJeO7RNKPr", "invitation": "ICLR.cc/2020/Conference/Paper1043/-/Official_Review", "content": {"rating": "8: Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This work proposes a neural network architecture for joint depth and camera motion estimation on video sequences. The authors propose an architecture that incorporates classic principles from SfM, namely depth computation based on cost volumes and motion estimation based on the reprojection error of features. Extensive experiments on a variety of datasets are shown and support that this approach provides strong results.\n\n+ Principled approach that marries the best aspects of deep learning with classic principles from multi-view geometry.\n\n+ Well-written paper\n\n+ Generalizes well\n\n+ Significantly outperforms the state of the art\n\n+ Clearly shows that more views help\n\n+ Seems to be robust to initialization\n\n\nQuestion: What happens if more Gauss-Newton steps are made? Could one trade computation for quality here?\n\n\nMinor issues:\n\n- Equation (1), right: f_z should probably be f_x\n- Paragraph between Eq. (1) and (2): There seems to be something wrong with the typesetting of x^i=... (e.g. the equals sign)\n- Same for the paragraph before Eq. (1) and x=(u,v)\n\n\nSummary: This paper presents a well-engineered and non-trivial system that leverages principles from different fields in a very reasonable way. The results look great both qualitatively and quantitatively. The experiments are extensive and show a clear improvement over the state-of-the-art.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1043/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1043/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zteed@princeton.edu", "jiadeng@princeton.edu"], "title": "DeepV2D: Video to Depth with Differentiable Structure from Motion", "authors": ["Zachary Teed", "Jia Deng"], "pdf": "/pdf/f6187767d5b3cf159a7455235ee961feeb044e87.pdf", "TL;DR": "DeepV2D predicts depth from a video clip by composing elements of classical SfM into a fully differentiable network.", "abstract": "We propose DeepV2D, an end-to-end deep learning architecture for predicting depth from video.  DeepV2D combines the representation ability of neural networks with the geometric principles governing image formation. We compose a collection of classical geometric algorithms, which are converted into trainable modules and combined into an end-to-end differentiable architecture. DeepV2D interleaves two stages: motion estimation and depth estimation. During inference, motion and depth estimation are alternated and converge to accurate depth. ", "keywords": ["Structure-from-Motion", "Video to Depth", "Dense Depth Estimation"], "paperhash": "teed|deepv2d_video_to_depth_with_differentiable_structure_from_motion", "_bibtex": "@inproceedings{\nTeed2020DeepV2D:,\ntitle={DeepV2D: Video to Depth with Differentiable Structure from Motion},\nauthor={Zachary Teed and Jia Deng},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeO7RNKPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/15280540ca4827718d9a8ea56a8671e22bf14a2a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJeO7RNKPr", "replyto": "HJeO7RNKPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1043/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1043/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575666850786, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1043/Reviewers"], "noninvitees": [], "tcdate": 1570237743232, "tmdate": 1575666850799, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1043/-/Official_Review"}}}, {"id": "SJlDS4yrcS", "original": null, "number": 2, "cdate": 1572299839422, "ddate": null, "tcdate": 1572299839422, "tmdate": 1572972519725, "tddate": null, "forum": "HJeO7RNKPr", "replyto": "HJeO7RNKPr", "invitation": "ICLR.cc/2020/Conference/Paper1043/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors proposed to estimate depth from a video sequence. In the model, the pipeline iteratively estimates motion and depth by separate modules, which can be trained in an end-to-end fashion. The numerical results show better scores than the other SOTA methods.  Overall, I think this is a useful work and may be considered for publishing.\n\nThe following are the detailed comments:\n1. The introduction of the related works is well written.\n2. The empirical comparison is quite thorough and demonstrates the proposed method using sequence and multi-frame is very useful.\n3. The method has good generalization.\n\n4. Besides the empirical results, I found the paper reads quite mechanical and provides very limited understanding if any. It would be much better if the authors can make further effort to understand where exactly the performance comes from. With a more complete story, the paper would have a better potential to last longer."}, "signatures": ["ICLR.cc/2020/Conference/Paper1043/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1043/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zteed@princeton.edu", "jiadeng@princeton.edu"], "title": "DeepV2D: Video to Depth with Differentiable Structure from Motion", "authors": ["Zachary Teed", "Jia Deng"], "pdf": "/pdf/f6187767d5b3cf159a7455235ee961feeb044e87.pdf", "TL;DR": "DeepV2D predicts depth from a video clip by composing elements of classical SfM into a fully differentiable network.", "abstract": "We propose DeepV2D, an end-to-end deep learning architecture for predicting depth from video.  DeepV2D combines the representation ability of neural networks with the geometric principles governing image formation. We compose a collection of classical geometric algorithms, which are converted into trainable modules and combined into an end-to-end differentiable architecture. DeepV2D interleaves two stages: motion estimation and depth estimation. During inference, motion and depth estimation are alternated and converge to accurate depth. ", "keywords": ["Structure-from-Motion", "Video to Depth", "Dense Depth Estimation"], "paperhash": "teed|deepv2d_video_to_depth_with_differentiable_structure_from_motion", "_bibtex": "@inproceedings{\nTeed2020DeepV2D:,\ntitle={DeepV2D: Video to Depth with Differentiable Structure from Motion},\nauthor={Zachary Teed and Jia Deng},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeO7RNKPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/15280540ca4827718d9a8ea56a8671e22bf14a2a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJeO7RNKPr", "replyto": "HJeO7RNKPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1043/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1043/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575666850786, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1043/Reviewers"], "noninvitees": [], "tcdate": 1570237743232, "tmdate": 1575666850799, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1043/-/Official_Review"}}}, {"id": "BkgjyQ7ccr", "original": null, "number": 3, "cdate": 1572643555087, "ddate": null, "tcdate": 1572643555087, "tmdate": 1572972519681, "tddate": null, "forum": "HJeO7RNKPr", "replyto": "HJeO7RNKPr", "invitation": "ICLR.cc/2020/Conference/Paper1043/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes a framework for training machine learning models that simultaneously estimates depth of objects and poses of a single camera in a sequence of images from a single camera, in others words, a video.\nIn a video, to estimate depth of objects, which is a main objective of this paper, we need to know the relative position and rotation information of a single camera in a sequence of images in a video (motional information). However, to estimate the relative position and rotation information of the camera, we need to know a ground truth depth information of each objects in each image.\nThe main idea works just like EM or alternating optimization. The depth module estimates depth of objects in a sequence of images assuming the relative position and rotation information of a single camera is given. The motion module estimates motional information of a camera assuming the depth of each object is given.\nThe authors formulate the aforementioned two modules as neural networks, so that they can be trained end-to-end, and proposes various way of initializing the two modules.\n\nWhile the idea is simple, I think the paper is well-written and the experiments show a superior performance over existing approaches.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1043/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1043/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zteed@princeton.edu", "jiadeng@princeton.edu"], "title": "DeepV2D: Video to Depth with Differentiable Structure from Motion", "authors": ["Zachary Teed", "Jia Deng"], "pdf": "/pdf/f6187767d5b3cf159a7455235ee961feeb044e87.pdf", "TL;DR": "DeepV2D predicts depth from a video clip by composing elements of classical SfM into a fully differentiable network.", "abstract": "We propose DeepV2D, an end-to-end deep learning architecture for predicting depth from video.  DeepV2D combines the representation ability of neural networks with the geometric principles governing image formation. We compose a collection of classical geometric algorithms, which are converted into trainable modules and combined into an end-to-end differentiable architecture. DeepV2D interleaves two stages: motion estimation and depth estimation. During inference, motion and depth estimation are alternated and converge to accurate depth. ", "keywords": ["Structure-from-Motion", "Video to Depth", "Dense Depth Estimation"], "paperhash": "teed|deepv2d_video_to_depth_with_differentiable_structure_from_motion", "_bibtex": "@inproceedings{\nTeed2020DeepV2D:,\ntitle={DeepV2D: Video to Depth with Differentiable Structure from Motion},\nauthor={Zachary Teed and Jia Deng},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeO7RNKPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/15280540ca4827718d9a8ea56a8671e22bf14a2a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJeO7RNKPr", "replyto": "HJeO7RNKPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1043/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1043/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575666850786, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1043/Reviewers"], "noninvitees": [], "tcdate": 1570237743232, "tmdate": 1575666850799, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1043/-/Official_Review"}}}, {"id": "B1x0lB_6qH", "original": null, "number": 4, "cdate": 1572861174210, "ddate": null, "tcdate": 1572861174210, "tmdate": 1572972519638, "tddate": null, "forum": "HJeO7RNKPr", "replyto": "HJeO7RNKPr", "invitation": "ICLR.cc/2020/Conference/Paper1043/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper pushes forward the research of deep learning based video 3D reconstruction by decomposing the problem in two-stages:\n1. Depth estimation from multi-view stereo\n2. Camera pose estimation from optical flow estimation and PnP SE3 pose,\nwhich turns out to achieve state-of-the-art performance on public datasets. \n\nIn general, the iterative procedure of this paper is similar to DeepTAM (Zhou et al., 2018), the major difference is that the camera pose is estimated by PnP with estimated Flow but not directly predicted from sub-network structures, which contributes to the higher tracking accuracy and further improves the accuracy of multi-view stereo depth estimation. \n\nA multi-view camera pose estimation (the global pose optimization in the paper) is also proposed to utilize the relative relation between all video frame pairs, which is difficult for fully CNN based pipeline and maximizes the use of the explicit PnP optimization. \n\nAfter that, a residual flow field is predicted using CNNs and camera poses is further refined by minimizing the geometric re-projection error.\n\nSo this paper focuses more on the camera pose estimation than the depth, which is a good start point to achieve better multi-view capabilities in a CNN framework.\n\n\nHowever, I still have several concerns for this paper:\n\n1. The full system is highly engineered and complicated. For example, the feature map is extracted from two hour-glass networks, which seems over-complicated for feature extraction, and the multi-view stereo network using four 3D hour-glass networks, which consumes a large amount of memories. So I would like to see the authors demonstrate the performance from in simpler settings, e.g. the feature map can be a single encoder-decoder and the multi-view stereo is done by correlation and 2D convolution. So I would like to see the inference time, peak memory consumption and the model size. Ablation studies will also help the readers to understand whether performance gain is from the pose estimation or the network capacities, which is unclear in the current paper (Appendix.D). \n\n2. The comparison with state-of-the-art conventional system is missing. For example, is the camera pose estimation better than initializing the DSO (Engel et al., 2018) with a monocular depth estimation? In real applications, if the performance gain is insignificant, the conventional method will still be a better choice because the CNN based methods are computationally expensive on platforms without powerful GPUs. I will not downgrade the rating if the performance gain is insignificant, but it is necessary to see the comparison.\n\n\n3. Even the difference is ignorable for performance, I hope the author could use adjoint when deriving the derivatives in Eq.(11) of Appendix A.1. The author can refer to this tutorial http://ethaneade.com/lie.pdf or the text book https://www.eecis.udel.edu/~cer/arv/readings/old_mkss.pdf.\n\nPS: For the current version with 10 pages, I lean to a borderline score, but I select 6 because 5 is not an option. I will keep or raise the current score if my concerns are addressed during rebuttal. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1043/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1043/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zteed@princeton.edu", "jiadeng@princeton.edu"], "title": "DeepV2D: Video to Depth with Differentiable Structure from Motion", "authors": ["Zachary Teed", "Jia Deng"], "pdf": "/pdf/f6187767d5b3cf159a7455235ee961feeb044e87.pdf", "TL;DR": "DeepV2D predicts depth from a video clip by composing elements of classical SfM into a fully differentiable network.", "abstract": "We propose DeepV2D, an end-to-end deep learning architecture for predicting depth from video.  DeepV2D combines the representation ability of neural networks with the geometric principles governing image formation. We compose a collection of classical geometric algorithms, which are converted into trainable modules and combined into an end-to-end differentiable architecture. DeepV2D interleaves two stages: motion estimation and depth estimation. During inference, motion and depth estimation are alternated and converge to accurate depth. ", "keywords": ["Structure-from-Motion", "Video to Depth", "Dense Depth Estimation"], "paperhash": "teed|deepv2d_video_to_depth_with_differentiable_structure_from_motion", "_bibtex": "@inproceedings{\nTeed2020DeepV2D:,\ntitle={DeepV2D: Video to Depth with Differentiable Structure from Motion},\nauthor={Zachary Teed and Jia Deng},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeO7RNKPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/15280540ca4827718d9a8ea56a8671e22bf14a2a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJeO7RNKPr", "replyto": "HJeO7RNKPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1043/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1043/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575666850786, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1043/Reviewers"], "noninvitees": [], "tcdate": 1570237743232, "tmdate": 1575666850799, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1043/-/Official_Review"}}}], "count": 10}