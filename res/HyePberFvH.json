{"notes": [{"id": "HyePberFvH", "original": "rJfbtggFDB", "number": 2140, "cdate": 1569439743385, "ddate": null, "tcdate": 1569439743385, "tmdate": 1577168225405, "tddate": null, "forum": "HyePberFvH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["julian.faraone@sydney.edu.au", "philip.leong@sydney.edu.au"], "title": "Monte Carlo Deep Neural Network Arithmetic", "authors": ["Julian Faraone", "Philip Leong"], "pdf": "/pdf/5cbe0a087592d00d6cde7285273883d9ff46d291.pdf", "TL;DR": "Determining the sensitivity of Deep Neural Networks to floating point rounding error using Monte Carlo Methods", "abstract": "Quantization is a crucial technique for achieving low-power, low latency and high throughput hardware implementations of Deep Neural Networks. Quantized floating point representations have received recent interest due to their hardware efficiency benefits and ability to represent a higher dynamic range than fixed point representations, leading to improvements in accuracy.  We present a novel technique, Monte Carlo Deep Neural Network Arithmetic (MCA), for determining the sensitivity of Deep Neural Networks to quantization in floating point arithmetic.We do this by applying Monte Carlo Arithmetic to the inference computation and analyzing the relative standard deviation of the neural network loss.  The method makes no assumptions regarding the underlying parameter distributions. We evaluate our method on pre-trained image classification models on the CIFAR10 andImageNet datasets.  For the same network topology and dataset, we demonstrate the ability to gain the equivalent of bits of precision by simply choosing weight parameter sets which demonstrate a lower loss of significance from the Monte Carlo trials.   Additionally,  we can apply MCA to compare the sensitivity of different network topologies to quantization effects.", "keywords": ["deep learning", "quantization", "floating point", "monte carlo methods"], "paperhash": "faraone|monte_carlo_deep_neural_network_arithmetic", "original_pdf": "/attachment/4326dbe4f951679bd5bd2a34a84ac6c4672778e1.pdf", "_bibtex": "@misc{\nfaraone2020monte,\ntitle={Monte Carlo Deep Neural Network Arithmetic},\nauthor={Julian Faraone and Philip Leong},\nyear={2020},\nurl={https://openreview.net/forum?id=HyePberFvH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "gDoYF23FX", "original": null, "number": 1, "cdate": 1576798741519, "ddate": null, "tcdate": 1576798741519, "tmdate": 1576800894722, "tddate": null, "forum": "HyePberFvH", "replyto": "HyePberFvH", "invitation": "ICLR.cc/2020/Conference/Paper2140/-/Decision", "content": {"decision": "Reject", "comment": "The paper studies the impact of rounding errors on deep neural networks. The                                                       \nauthors apply Monte Carlos arithmetics to standard DNN operations.                                                                 \nTheir results indeed show catastrophic cancellation in DNNs and that the resulting loss of                                         \nsignificance in the number representation correlates with decrease in validation                                                   \nperformance, indicating that DNN performances are sensitive to rounding errors.                                                    \n                                                                                                                                   \nAlthough recognizing that the paper addresses an important problem (quantized /                                                    \nfinite precision neural networks), the reviewers point out the contribution of                                                     \nthe paper is somewhat incremental.                                                                                                 \nDuring the rebuttal, the authors made an effort to improve the manuscript based                                                    \non reviewer suggestions, however review scores were not increased.                                                                 \n                                                                                                                                   \nThe paper is slightly below acceptance threshold, based on reviews and my own                                                      \nreading, as the method is mostly restricted to diagnostics and cannot yet be used                                                  \nto help training low-precision neural networks.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["julian.faraone@sydney.edu.au", "philip.leong@sydney.edu.au"], "title": "Monte Carlo Deep Neural Network Arithmetic", "authors": ["Julian Faraone", "Philip Leong"], "pdf": "/pdf/5cbe0a087592d00d6cde7285273883d9ff46d291.pdf", "TL;DR": "Determining the sensitivity of Deep Neural Networks to floating point rounding error using Monte Carlo Methods", "abstract": "Quantization is a crucial technique for achieving low-power, low latency and high throughput hardware implementations of Deep Neural Networks. Quantized floating point representations have received recent interest due to their hardware efficiency benefits and ability to represent a higher dynamic range than fixed point representations, leading to improvements in accuracy.  We present a novel technique, Monte Carlo Deep Neural Network Arithmetic (MCA), for determining the sensitivity of Deep Neural Networks to quantization in floating point arithmetic.We do this by applying Monte Carlo Arithmetic to the inference computation and analyzing the relative standard deviation of the neural network loss.  The method makes no assumptions regarding the underlying parameter distributions. We evaluate our method on pre-trained image classification models on the CIFAR10 andImageNet datasets.  For the same network topology and dataset, we demonstrate the ability to gain the equivalent of bits of precision by simply choosing weight parameter sets which demonstrate a lower loss of significance from the Monte Carlo trials.   Additionally,  we can apply MCA to compare the sensitivity of different network topologies to quantization effects.", "keywords": ["deep learning", "quantization", "floating point", "monte carlo methods"], "paperhash": "faraone|monte_carlo_deep_neural_network_arithmetic", "original_pdf": "/attachment/4326dbe4f951679bd5bd2a34a84ac6c4672778e1.pdf", "_bibtex": "@misc{\nfaraone2020monte,\ntitle={Monte Carlo Deep Neural Network Arithmetic},\nauthor={Julian Faraone and Philip Leong},\nyear={2020},\nurl={https://openreview.net/forum?id=HyePberFvH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HyePberFvH", "replyto": "HyePberFvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795716615, "tmdate": 1576800266799, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2140/-/Decision"}}}, {"id": "SygYoBI2oS", "original": null, "number": 4, "cdate": 1573836192959, "ddate": null, "tcdate": 1573836192959, "tmdate": 1573836315533, "tddate": null, "forum": "HyePberFvH", "replyto": "HyePberFvH", "invitation": "ICLR.cc/2020/Conference/Paper2140/-/Official_Comment", "content": {"title": "Summary Of Changes Made", "comment": "We thank all the reviewers for the constructive comments and valuable suggestions. We have uploaded a revised version of our paper following the suggestions. In the revised paper, we have highlighted the main changes in blue/aqua. The changes for each section can be summarized as follows:\n\nIn Section 1, we updated Figure 1 to show a more insightful operator comparison. We now directly compare fixed and floating point of the same precision width of 8bits. This more clearly shows that a drop of a single bit of precision in floating point, from 8->7, can have significant hardware benefits and shows improvement over the 8-bit fixed point operator. We also make a clear distinguishment between our work 'MCDA' and previous work 'MCA'. \n\nIn section 2, we further discuss the literature and contrast our work to others on Bayesian NNs.\n\nIn section 3, we fix some of the notation and wording for correctness and add an equation to more clearly describe how we calculated the loss of significance value, K.\n\nIn section 4, we fix some of the notation and the wording for the issues of applying traditional MCA to DNNs. We also remove an equation which we deemed unnecessary. \n\nIn section 5, we provide much greater detail on how we ran MCDA experiments, including a reference to the MCALIB repository which we used to produce regression plots for calculating K and t_min. We also fix the scale and labelling of the axes in Figure 3 and we order the networks in Figure 5 based on their respective K values. This makes the figures more clear and coherent.\n\nWe also added an appendix which describes how MCALIB calculates k and t_min specifically. Additionally, we add more detail on the quantization function used to produce our quantized floating point representations.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2140/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2140/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["julian.faraone@sydney.edu.au", "philip.leong@sydney.edu.au"], "title": "Monte Carlo Deep Neural Network Arithmetic", "authors": ["Julian Faraone", "Philip Leong"], "pdf": "/pdf/5cbe0a087592d00d6cde7285273883d9ff46d291.pdf", "TL;DR": "Determining the sensitivity of Deep Neural Networks to floating point rounding error using Monte Carlo Methods", "abstract": "Quantization is a crucial technique for achieving low-power, low latency and high throughput hardware implementations of Deep Neural Networks. Quantized floating point representations have received recent interest due to their hardware efficiency benefits and ability to represent a higher dynamic range than fixed point representations, leading to improvements in accuracy.  We present a novel technique, Monte Carlo Deep Neural Network Arithmetic (MCA), for determining the sensitivity of Deep Neural Networks to quantization in floating point arithmetic.We do this by applying Monte Carlo Arithmetic to the inference computation and analyzing the relative standard deviation of the neural network loss.  The method makes no assumptions regarding the underlying parameter distributions. We evaluate our method on pre-trained image classification models on the CIFAR10 andImageNet datasets.  For the same network topology and dataset, we demonstrate the ability to gain the equivalent of bits of precision by simply choosing weight parameter sets which demonstrate a lower loss of significance from the Monte Carlo trials.   Additionally,  we can apply MCA to compare the sensitivity of different network topologies to quantization effects.", "keywords": ["deep learning", "quantization", "floating point", "monte carlo methods"], "paperhash": "faraone|monte_carlo_deep_neural_network_arithmetic", "original_pdf": "/attachment/4326dbe4f951679bd5bd2a34a84ac6c4672778e1.pdf", "_bibtex": "@misc{\nfaraone2020monte,\ntitle={Monte Carlo Deep Neural Network Arithmetic},\nauthor={Julian Faraone and Philip Leong},\nyear={2020},\nurl={https://openreview.net/forum?id=HyePberFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyePberFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2140/Authors", "ICLR.cc/2020/Conference/Paper2140/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2140/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2140/Reviewers", "ICLR.cc/2020/Conference/Paper2140/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2140/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2140/Authors|ICLR.cc/2020/Conference/Paper2140/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145748, "tmdate": 1576860557085, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2140/Authors", "ICLR.cc/2020/Conference/Paper2140/Reviewers", "ICLR.cc/2020/Conference/Paper2140/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2140/-/Official_Comment"}}}, {"id": "B1xsDNhPjS", "original": null, "number": 3, "cdate": 1573532771357, "ddate": null, "tcdate": 1573532771357, "tmdate": 1573532771357, "tddate": null, "forum": "HyePberFvH", "replyto": "rJxUTXU9tS", "invitation": "ICLR.cc/2020/Conference/Paper2140/-/Official_Comment", "content": {"title": "Response To Reviewer 3", "comment": "Thank you for your review. We have provided an updated version of the paper with the main changes highlighted to address all reviewers comments.\n\nContribution:\n\nDeeper theory and making connections with Bayesian networks are indeed interesting lines of research but we believe that it is beyond the scope of this paper. We hope that other researchers can take our work in these and other directions.  \n\nAlthough related, our work is complementary to work using variational Bayes or Monte Carlo methods to estimate a posterior. We have updated Section 2 with a brief review of key papers and a statement regarding the new aspects of the present work.\n\nOriginality:\nWe agree that our references to \u201cMCA\u201d are misleading to the reader in regards to where the novelty of the paper lies.  We have altered Paragraph 4 of the introduction. This allows us to distinguish between our method \u201cMCDA\u201d and the known method \u201cMCA\u201d, which we have now updated throughout the paper.\n\nWriting Quality:\nWe thank you for your suggestions on what to improve, it has made the updated paper more clear and concise.\n\nTechnical Quality:\n\nWe have added more detail to the experiments (Section 5) on how the experiments and analysis was performed.\nIn answer to your particular questions:\n-Each Monte Carlo trial is done on the same batch of images in the training set. This has now been stated explicitly in Section 5.\n-In sections 5.1 and 5.2, linear regression analysis is used just as in 5.3. Different t are combined for all our experiments by averaging all the K values when t>t_min. Calculating t_min and K was done using the MCALIB library [1]. We have now added equation (7) and a more detailed description in Section 3.3 and Section 5 to make this more clear. \n\nPreviously we had only cited the MCALIB paper, we have now also cited the paper and github repository in Section 5. In the appendix we have also added the linear regression curves for all models used in Sections 5.1 and 5.2 and we have explained explicitly in detail how MCALIB calculates K and t_min. This is all in the updated version of the paper. \n-We have now explicitly stated which of train and validation accuracy we are referring to at each instance.\n-We have now added an explanation of the quantization with stochastic rounding in Appendix A.3 and we reference this in Section 5.\n-In 5.2, the model chosen as the baseline was done based on whichever achieved the highest single-precision (unquantized) validation accuracy.  We have stated this more clearly in the title of Table 1 and the text in 5.2. \n\nSpecific suggestions for improvement:\n\n-The citation format has now been fixed using \\citep\n\n-We have replaced the Fixed(2,8) results with Fixed(8,8) results in Figure 1 as we believe this helps provide clarity for our paper motivations. We have also modified the comments on this in paragraph 3 of the Introduction. \n\n-We have fixed the grammatical areas in Section 2 and in other parts of the paper.\n\n-We have now added more papers to the related work section and have modified some of our descriptions of previous literature to be more specific. Namely, we have cited 5 more papers in relation to Monte Carlo methods for Bayesian Neural Networks, 3 of them relating specifically to quantization/compression methods. As mentioned, we have updated Section 2 with a brief review of key papers and a statement regarding the new aspects of the present work.\n\n-Section 3:\n\u2014We have now clarified this.\n\u2014We have now fixed this\n\u2014We have now fixed this\n\u2014yes it should be \ud835\udeff, we have now fixed this\n\u2014we have removed the wording \u201cfrom numerical analysis literature\u201d and have reworded the sentence.\nIn equation (4) is now represented as (1+m) for clarity and consistency with equation (1). When subbing equation (1) into (4), it has to be reminded that the sign bit (-1)^s is not relevant to the random variable which we have defined as U~(-0.5,0.5) and hence it is discarded. Once you remove this, subbing this in does yield the same result.\n\u2014we have removed this sentence \n\u2014In IEEE floating point arithmetic, operators are implemented such that the error must be bounded by \ud835\udeff. The inequality needed to be flipped, this has now been done.\n\u2014We have now fixed this.\n\u2014This is correct, we have now fixed this.\n\u2014We have now fixed this.\n\nSection 4.1\n\u2014We have now fixed this notation error\n\u2014We have fixed equation 8 by using |X| to represent the size. We also changed the \u201cL\u201d to \u201closs\u201d in the right hand part of the equation and in the text so that the total network loss and loss function are distinguishable.\n\u2014This grammatical error has now been fixed.\n\nOur explanation in the second bullet point of 4.1 was incorrect. As the reviewer points out, the reason for MCA not working well is that the output is discrete. This means that, for high values of t, Monte Carlo results across different t become indistinguishable. This has now been corrected.\n\nAll relevant figures and grammatical errors have been updated to address reviewers concerns."}, "signatures": ["ICLR.cc/2020/Conference/Paper2140/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2140/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["julian.faraone@sydney.edu.au", "philip.leong@sydney.edu.au"], "title": "Monte Carlo Deep Neural Network Arithmetic", "authors": ["Julian Faraone", "Philip Leong"], "pdf": "/pdf/5cbe0a087592d00d6cde7285273883d9ff46d291.pdf", "TL;DR": "Determining the sensitivity of Deep Neural Networks to floating point rounding error using Monte Carlo Methods", "abstract": "Quantization is a crucial technique for achieving low-power, low latency and high throughput hardware implementations of Deep Neural Networks. Quantized floating point representations have received recent interest due to their hardware efficiency benefits and ability to represent a higher dynamic range than fixed point representations, leading to improvements in accuracy.  We present a novel technique, Monte Carlo Deep Neural Network Arithmetic (MCA), for determining the sensitivity of Deep Neural Networks to quantization in floating point arithmetic.We do this by applying Monte Carlo Arithmetic to the inference computation and analyzing the relative standard deviation of the neural network loss.  The method makes no assumptions regarding the underlying parameter distributions. We evaluate our method on pre-trained image classification models on the CIFAR10 andImageNet datasets.  For the same network topology and dataset, we demonstrate the ability to gain the equivalent of bits of precision by simply choosing weight parameter sets which demonstrate a lower loss of significance from the Monte Carlo trials.   Additionally,  we can apply MCA to compare the sensitivity of different network topologies to quantization effects.", "keywords": ["deep learning", "quantization", "floating point", "monte carlo methods"], "paperhash": "faraone|monte_carlo_deep_neural_network_arithmetic", "original_pdf": "/attachment/4326dbe4f951679bd5bd2a34a84ac6c4672778e1.pdf", "_bibtex": "@misc{\nfaraone2020monte,\ntitle={Monte Carlo Deep Neural Network Arithmetic},\nauthor={Julian Faraone and Philip Leong},\nyear={2020},\nurl={https://openreview.net/forum?id=HyePberFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyePberFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2140/Authors", "ICLR.cc/2020/Conference/Paper2140/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2140/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2140/Reviewers", "ICLR.cc/2020/Conference/Paper2140/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2140/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2140/Authors|ICLR.cc/2020/Conference/Paper2140/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145748, "tmdate": 1576860557085, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2140/Authors", "ICLR.cc/2020/Conference/Paper2140/Reviewers", "ICLR.cc/2020/Conference/Paper2140/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2140/-/Official_Comment"}}}, {"id": "rJeM1WhwiS", "original": null, "number": 2, "cdate": 1573531866349, "ddate": null, "tcdate": 1573531866349, "tmdate": 1573531866349, "tddate": null, "forum": "HyePberFvH", "replyto": "S1xgzttTFH", "invitation": "ICLR.cc/2020/Conference/Paper2140/-/Official_Comment", "content": {"title": "Response To Reviewer 2", "comment": "Thank you for your review. We have provided an updated version of the paper with the main changes highlighted to address all reviewers comments.\n\nWeaknesses:\n- The straightforward application of MCA to NN inference on conventional GPU/CPU machines precludes most optimizations used in GEMM, and would result in a significant performance loss (explained in Section 4.1 and 4.2). Thus a key idea of this paper is to provide a technique which demonstrates how to overcome these issues for applying MCA to NN (which we call MCDA). In doing so, we gain useful insights into the sensitivity of the NN to rounding, and demonstrate that we can rank different networks, and choose good ones among those with the same loss score.\n\n-We hope our work is a precursor to further research in areas such as end-to-end low precision NN training, informative priors for Bayesian NN, training methods for weight-insensitive inference, etc. We have added additional papers on Bayesian Neural Networks to Section 2. \n\nOther Comments:\n-As pointed out by reviewer 3, a better explanation of the issue in the second bullet point of 4.1 is that the output was a discrete value. To overcome this, we use the loss function as the instead, which is a continuous value. Thus for high values of t (small perturbations), it will still present an observable change. We have explicitly stated this by modifying the second bullet point in 4.1 and the first paragraph of 4.2. in the updated version.\n\n-The statistical methods used in MCALIB (the MCA library used for our regression analysis) assume that results are normally distributed to provide the summary statistics from which K and t_min are computed. Within MCALIB, a normality test is applied, and failing indicates that changes in rounding lead to unusual changes in the (scalar) output and t_min is strictly larger than the precision for which this occurs. No assumptions regarding the distribution of the inputs nor anything about the computational graph or loss metric are made.\n\n-Different t does give different K. We now use K_t to describe K for each different t in equation (6) to make this more clear. The K for a given network is reported throughout our experiment section as the average of K values for t > t_min. We have now added equation (7) to make this more clear. \n\nFor each model in Figure 3, using MCDA, we ran 1000 trials for all t in {1,2,3\u2026,16} and calculated the relative standard deviation (RSD). We have now explicitly stated this detail, in the experiments (Beginning of Section 5). We have added to the Appendix, all the linear regression plots for all the models in Figure 3 which demonstrate this analytically. Additionally we have added further explanations and equations to the Appendix regarding how MCALIB calculates t_min and K in the regression analysis.\n\n[1] Michael Frechtling and Philip H. W. Leong.  Mcalib:  Measuring sensitivity to rounding error with monte carlo programming. ACM Trans. Program. Lang. Syst., 37(2):5:1\u20135:25, April 2015. ISSN 0164-0925. doi:10.1145/2665073. URL http://doi.acm.org/10.1145/2665073\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2140/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2140/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["julian.faraone@sydney.edu.au", "philip.leong@sydney.edu.au"], "title": "Monte Carlo Deep Neural Network Arithmetic", "authors": ["Julian Faraone", "Philip Leong"], "pdf": "/pdf/5cbe0a087592d00d6cde7285273883d9ff46d291.pdf", "TL;DR": "Determining the sensitivity of Deep Neural Networks to floating point rounding error using Monte Carlo Methods", "abstract": "Quantization is a crucial technique for achieving low-power, low latency and high throughput hardware implementations of Deep Neural Networks. Quantized floating point representations have received recent interest due to their hardware efficiency benefits and ability to represent a higher dynamic range than fixed point representations, leading to improvements in accuracy.  We present a novel technique, Monte Carlo Deep Neural Network Arithmetic (MCA), for determining the sensitivity of Deep Neural Networks to quantization in floating point arithmetic.We do this by applying Monte Carlo Arithmetic to the inference computation and analyzing the relative standard deviation of the neural network loss.  The method makes no assumptions regarding the underlying parameter distributions. We evaluate our method on pre-trained image classification models on the CIFAR10 andImageNet datasets.  For the same network topology and dataset, we demonstrate the ability to gain the equivalent of bits of precision by simply choosing weight parameter sets which demonstrate a lower loss of significance from the Monte Carlo trials.   Additionally,  we can apply MCA to compare the sensitivity of different network topologies to quantization effects.", "keywords": ["deep learning", "quantization", "floating point", "monte carlo methods"], "paperhash": "faraone|monte_carlo_deep_neural_network_arithmetic", "original_pdf": "/attachment/4326dbe4f951679bd5bd2a34a84ac6c4672778e1.pdf", "_bibtex": "@misc{\nfaraone2020monte,\ntitle={Monte Carlo Deep Neural Network Arithmetic},\nauthor={Julian Faraone and Philip Leong},\nyear={2020},\nurl={https://openreview.net/forum?id=HyePberFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyePberFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2140/Authors", "ICLR.cc/2020/Conference/Paper2140/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2140/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2140/Reviewers", "ICLR.cc/2020/Conference/Paper2140/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2140/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2140/Authors|ICLR.cc/2020/Conference/Paper2140/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145748, "tmdate": 1576860557085, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2140/Authors", "ICLR.cc/2020/Conference/Paper2140/Reviewers", "ICLR.cc/2020/Conference/Paper2140/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2140/-/Official_Comment"}}}, {"id": "S1gD9l3wiS", "original": null, "number": 1, "cdate": 1573531791153, "ddate": null, "tcdate": 1573531791153, "tmdate": 1573531791153, "tddate": null, "forum": "HyePberFvH", "replyto": "rkgOn7jaYS", "invitation": "ICLR.cc/2020/Conference/Paper2140/-/Official_Comment", "content": {"title": "Response To Reviewer 1", "comment": "Thank you for your review. We have provided an updated version of the paper with the main changes highlighted to address all reviewers comments.\n\nThe idea of combining MCA with Bayesian Neural Networks is indeed interesting, but beyond the scope of this paper. We hope that this paper will provide an initial direction for further research in MCA for NNs which enables deeper understanding of quantization in inference and training. In response to your comments and those of the other reviewers, we have cited an additional 5 papers concerning Bayesian Neural Networks in Section 2. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2140/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2140/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["julian.faraone@sydney.edu.au", "philip.leong@sydney.edu.au"], "title": "Monte Carlo Deep Neural Network Arithmetic", "authors": ["Julian Faraone", "Philip Leong"], "pdf": "/pdf/5cbe0a087592d00d6cde7285273883d9ff46d291.pdf", "TL;DR": "Determining the sensitivity of Deep Neural Networks to floating point rounding error using Monte Carlo Methods", "abstract": "Quantization is a crucial technique for achieving low-power, low latency and high throughput hardware implementations of Deep Neural Networks. Quantized floating point representations have received recent interest due to their hardware efficiency benefits and ability to represent a higher dynamic range than fixed point representations, leading to improvements in accuracy.  We present a novel technique, Monte Carlo Deep Neural Network Arithmetic (MCA), for determining the sensitivity of Deep Neural Networks to quantization in floating point arithmetic.We do this by applying Monte Carlo Arithmetic to the inference computation and analyzing the relative standard deviation of the neural network loss.  The method makes no assumptions regarding the underlying parameter distributions. We evaluate our method on pre-trained image classification models on the CIFAR10 andImageNet datasets.  For the same network topology and dataset, we demonstrate the ability to gain the equivalent of bits of precision by simply choosing weight parameter sets which demonstrate a lower loss of significance from the Monte Carlo trials.   Additionally,  we can apply MCA to compare the sensitivity of different network topologies to quantization effects.", "keywords": ["deep learning", "quantization", "floating point", "monte carlo methods"], "paperhash": "faraone|monte_carlo_deep_neural_network_arithmetic", "original_pdf": "/attachment/4326dbe4f951679bd5bd2a34a84ac6c4672778e1.pdf", "_bibtex": "@misc{\nfaraone2020monte,\ntitle={Monte Carlo Deep Neural Network Arithmetic},\nauthor={Julian Faraone and Philip Leong},\nyear={2020},\nurl={https://openreview.net/forum?id=HyePberFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyePberFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2140/Authors", "ICLR.cc/2020/Conference/Paper2140/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2140/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2140/Reviewers", "ICLR.cc/2020/Conference/Paper2140/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2140/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2140/Authors|ICLR.cc/2020/Conference/Paper2140/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145748, "tmdate": 1576860557085, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2140/Authors", "ICLR.cc/2020/Conference/Paper2140/Reviewers", "ICLR.cc/2020/Conference/Paper2140/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2140/-/Official_Comment"}}}, {"id": "rJxUTXU9tS", "original": null, "number": 1, "cdate": 1571607486256, "ddate": null, "tcdate": 1571607486256, "tmdate": 1572972377904, "tddate": null, "forum": "HyePberFvH", "replyto": "HyePberFvH", "invitation": "ICLR.cc/2020/Conference/Paper2140/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary:\n\nThe paper studies the sensitivity of a neural network with respect to quantizing its weights and activations. The idea is to use Monte Carlo Arithmetic (MCA) in order to calculate the number of significant bits in the training loss (e.g. cross entropy) that are lost due to floating-point arithmetic. The results show that the number of significant bits lost correlates with the reduction in classification accuracy when quantizing the weights and activations of the neural network.\n\nDecision:\n\nOverall, this is an interesting paper with interesting results. However, I think there is considerable room for improvement, and that more details are needed in order to assess the significance of the results, as I detail in the rest of my review. For these reasons, I recommend weak reject for now, but I encourage the authors to continue working on improving the paper and to provide more details in the updated version.\n\nContribution:\n\nThe paper considers an important problem, that of quantizing the weights and activations of a neural network in order to reduce computational and memory cost, while maintaining the machine-learning performance as high as possible. \n\nIn my opinion, the main contribution of the paper is the experimental findings, and in particular that the sensitivity of the training loss with respect to the precision of the weights and activations correlates with the accuracy of the network. It seems to me that these results may relate to work on Bayesian neural networks, sharp vs flat minima, and minimum-description length approaches to variational inference. Work on these areas has also shown that sensitivity of the training loss with respect to the precision of the weights (which intuitively happens when the network is at a \"sharp\" local minimum vs a \"flat\" one) is related to poor generalization performance, and vice versa. I would encourage the authors to explore the potential relationship of their work with these areas, and possibly discuss them in an updated version of the paper.\n\nOriginality:\n\nThe paper describes a method for assessing the sensitivity of a neural network with respect to the precision of the weights and activations. The method is a straightforward application of Monte Carlo Arithmetic (MCA) to neural networks. I believe that the application of MCA to neural networks for this particular purpose is novel, and that the results are original. However, the introduction of the paper gives the impression that the proposed method is brand new, and even uses the acronym MCA to refer to the proposed method, which can be confusing to readers. I would suggest to the authors to rewrite the introduction so as to reflect more accurately that the contribution is not a brand-new method, but rather the application of an existing method (MCA) in a novel way.\n\nWriting quality:\n\nThe paper is generally easy to read, but there is considerable room for improvement. There are mistakes, and often the writing is sloppy and imprecise. I give some more specific suggestions on what to improve later on.\n\nTechnical quality:\n\nThe method is well motivated and the experiments seem reasonable. However, there is very little detail on the experiments, which makes it hard to assess their correctness/significance. I would suggest to the authors to rewrite the experimental section with full detail, or put more details in an appendix. In particular:\n- Is each Monte Carlo trial done on the same batch of training images or a different one? If different, how are the trials averaged, and does that mean that the standard deviation over trials also includes a contribution due to different batches?\n- In sections 5.1 and 5.2, how were the results for different t combined/aggregated? Did you use linear-regression analysis as in section 5.3?\n- When you say \"accuracy\", do you mean accuracy on the training set, validation set, or test set? This is particularly important for assessing the significance of the results, and is something that is currently missing from the description of the experiments.\n- How was the quantization of the neural networks performed? It would be good to explain this at least on a high level, in addition to citing Wang et al., (2018).\n- In section 5.2, how was the model selection for each method performed exactly? In the baseline method, was the model to be quantized selected based on validation performance before quantization or after quantization?\n\nSpecific suggestions for improvement:\n\nThe citation format, i.e. (Smith et al., (2019)), is unusual and uses unnecessarily many parentheses. Use \\citep for (Smith et al., 2019), and \\citet for Smith et al. (2019).\n\nThe illustration of fig. 1 is not fully convincing as a motivation for floating-point arithmetic. Even though it makes the case that Float(7, 7) is more efficient than Float(8, 8) and Float (9, 9), the comparison between Float(7, 7) and Fixed(12, 12) is hard to interpret, as we can't conclude whether the efficiency gain is due to reducing the number of bits or to switching from fixed-point to floating-point arithmetic. A more convincing illustration would compare fixed-point with floating-point arithmetic using the same number of bits.\n\nIt would be better if fig. 1 were 2D, as 3D doesn't add anything but makes it harder to compare sizes visually.\n\nPlease avoid exaggerations, such as \"exquisitely sensitive\" or \"extremely sensitive\", when \"sensitive\" would suffice.\n\nSection 2 is grammatically sloppy:\n- arihtmetic --> arithmetic\n- Last line of page 2 seems to be missing a verb.\n- this has lead --> this has led\n\nThe related-work section is too short and in many cases it doesn't explain what previous work has actually done. For example, \"rounding of inexact values to their nearest FP approximation has been studied in several publications\" is vague: what exactly these publication have done? This lack of detail makes it hard to assess the originality of the current paper, and how it differs from existing work.\n\nSection 3 is often unclear with imprecise mathematical notation:\n- \"e is the base-2 exponent of x in binary floating point arithmetic\": surely, the exponent is represented as an integer?\n- (bs, be1, be2, ..., bex, bm1, bm2, ..., bmx) is sloppy, as it indicates that the indices run from 1 to x.\n- Bx = sx + ex + mx  is also sloppy; what is meant here is the number of bits to represent sx, ex, mx and not the values themselves.\n- F(x) = x(1 + \u03b8), shouldn't \u03b8 be \u03b4?\n- \"which is typically the cause of horrific numerical inaccuracy from numerical analysis literature\", the phrase \"from numerical analysis literature\" doesn't make much sense here.\n- In eq. (4), substituting the expression for x from eq. (1) doesn't yield the same result.\n- \"The number of trials is an important consideration because [...] it can produce adverse effects on results\". What is meant by \"adverse effects\"? Do you mean that with few trials Monte Carlo doesn't give accurate results? Please be more specific.\n- \"we can determine the expected number of significant binary digits available from a p-digit FP system as p \u2265 \u2212log2(\u03b4)\". I'm unable to follow this statement, please explain further. Also, from applying logs to \u03b4 \u2264 2^{\u2212p} one gets an inequality that doesn't match the one in p \u2265 \u2212log2(\u03b4).\n- \"The relative error of an MCA operation is, for virtual precision t, is \u03b4 \u2264 2^-t\", \"is\" is used twice here.\n- \"the expected number of significant binary digits in a t-digit MCA operations is at least t\", operations --> operation. Also, shouldn't it be at most t, otherwise K becomes negative?\n-  is discussed in the section --> is discussed in the next section.\n\nSome mistakes in section 4.1:\n- y = (x; w) --> y = f(x; w)\n- Eq. (8) is sloppy, it uses X for both the set and its size. Use |X| or something similar for the size.\n- In the caption of fig. 2, baes --> base\n\nI'm not convinced by the second bullet point in section 4.1, that the averaging over many images used to obtain the accuracy is the reason why MCA doesn't work well. Surely, the training loss (cross entropy) is also an average over many images? To me it would seem more plausible that the main reason MCA works with training loss but not accuracy is because accuracy is discrete, whereas training loss is continuous.\n\nFig. 3 would be much easier to read if the axes were labelled, and if the axes had the same range (so that different plots can be compared visually).\n\nFig. 5 would be easier to read if the networks were sorted with respect to K.\n\nIn section 5, CIFAR-10 is sometimes written as CIFAR10.\n\nAppendix A is empty, so it should be removed.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2140/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2140/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["julian.faraone@sydney.edu.au", "philip.leong@sydney.edu.au"], "title": "Monte Carlo Deep Neural Network Arithmetic", "authors": ["Julian Faraone", "Philip Leong"], "pdf": "/pdf/5cbe0a087592d00d6cde7285273883d9ff46d291.pdf", "TL;DR": "Determining the sensitivity of Deep Neural Networks to floating point rounding error using Monte Carlo Methods", "abstract": "Quantization is a crucial technique for achieving low-power, low latency and high throughput hardware implementations of Deep Neural Networks. Quantized floating point representations have received recent interest due to their hardware efficiency benefits and ability to represent a higher dynamic range than fixed point representations, leading to improvements in accuracy.  We present a novel technique, Monte Carlo Deep Neural Network Arithmetic (MCA), for determining the sensitivity of Deep Neural Networks to quantization in floating point arithmetic.We do this by applying Monte Carlo Arithmetic to the inference computation and analyzing the relative standard deviation of the neural network loss.  The method makes no assumptions regarding the underlying parameter distributions. We evaluate our method on pre-trained image classification models on the CIFAR10 andImageNet datasets.  For the same network topology and dataset, we demonstrate the ability to gain the equivalent of bits of precision by simply choosing weight parameter sets which demonstrate a lower loss of significance from the Monte Carlo trials.   Additionally,  we can apply MCA to compare the sensitivity of different network topologies to quantization effects.", "keywords": ["deep learning", "quantization", "floating point", "monte carlo methods"], "paperhash": "faraone|monte_carlo_deep_neural_network_arithmetic", "original_pdf": "/attachment/4326dbe4f951679bd5bd2a34a84ac6c4672778e1.pdf", "_bibtex": "@misc{\nfaraone2020monte,\ntitle={Monte Carlo Deep Neural Network Arithmetic},\nauthor={Julian Faraone and Philip Leong},\nyear={2020},\nurl={https://openreview.net/forum?id=HyePberFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyePberFvH", "replyto": "HyePberFvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2140/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2140/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575483889200, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2140/Reviewers"], "noninvitees": [], "tcdate": 1570237727133, "tmdate": 1575483889213, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2140/-/Official_Review"}}}, {"id": "S1xgzttTFH", "original": null, "number": 2, "cdate": 1571817735986, "ddate": null, "tcdate": 1571817735986, "tmdate": 1572972377867, "tddate": null, "forum": "HyePberFvH", "replyto": "HyePberFvH", "invitation": "ICLR.cc/2020/Conference/Paper2140/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose a scalable method based on Monte Carlo arithmetic for quantifying the sensitivity of trained neural networks to floating point rounding errors. They demonstrate that the loss of significance metric K estimated from the process can be used for selecting networks that are more robust to quantization, and compare popular architectures (AlexNet, ResNet etc.) for their varying sensitivities.\n\nStrengths:\n- The paper tackles an important problem of analyzing sensitivity of networks to quantization and offers a well-correlated metric that can be computed without actually running models on quantized mode\n- Experiments cover a wide range of architectures in image recognition\n\nWeaknesses:\n- The proposed method in Section 4.2 appears to be a straightforward modification to MCA for NN\n- Experiments only demonstrate model selection and evaluating trained networks. Can this metric be used in optimization? For example, can you optimize for lowering K (say with fixed t) during training, so you can find a well-performing weight that also is robust to quantization? 1000 random samples interleaved in training may be slow, but perhaps you can use coarse approximation. This could significantly improve the impact of the paper. Some Bayesian NN literatures may be relevant (dropout, SGLD etc). \n\nOther Comments:\n- How is the second bullet point in Section 4.1 addressed in the proposed method?\n- Can you make this metric task-agnostic or input-distribution-agnostic (e.g. just based on variance in predictions over some input datasets)? (e.g. you may pick a difference loss function or different test distribution to evaluate afterwards) \n- Does different t give different K? If so, what\u2019s the K reported? (are those points on Figure 3)?"}, "signatures": ["ICLR.cc/2020/Conference/Paper2140/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2140/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["julian.faraone@sydney.edu.au", "philip.leong@sydney.edu.au"], "title": "Monte Carlo Deep Neural Network Arithmetic", "authors": ["Julian Faraone", "Philip Leong"], "pdf": "/pdf/5cbe0a087592d00d6cde7285273883d9ff46d291.pdf", "TL;DR": "Determining the sensitivity of Deep Neural Networks to floating point rounding error using Monte Carlo Methods", "abstract": "Quantization is a crucial technique for achieving low-power, low latency and high throughput hardware implementations of Deep Neural Networks. Quantized floating point representations have received recent interest due to their hardware efficiency benefits and ability to represent a higher dynamic range than fixed point representations, leading to improvements in accuracy.  We present a novel technique, Monte Carlo Deep Neural Network Arithmetic (MCA), for determining the sensitivity of Deep Neural Networks to quantization in floating point arithmetic.We do this by applying Monte Carlo Arithmetic to the inference computation and analyzing the relative standard deviation of the neural network loss.  The method makes no assumptions regarding the underlying parameter distributions. We evaluate our method on pre-trained image classification models on the CIFAR10 andImageNet datasets.  For the same network topology and dataset, we demonstrate the ability to gain the equivalent of bits of precision by simply choosing weight parameter sets which demonstrate a lower loss of significance from the Monte Carlo trials.   Additionally,  we can apply MCA to compare the sensitivity of different network topologies to quantization effects.", "keywords": ["deep learning", "quantization", "floating point", "monte carlo methods"], "paperhash": "faraone|monte_carlo_deep_neural_network_arithmetic", "original_pdf": "/attachment/4326dbe4f951679bd5bd2a34a84ac6c4672778e1.pdf", "_bibtex": "@misc{\nfaraone2020monte,\ntitle={Monte Carlo Deep Neural Network Arithmetic},\nauthor={Julian Faraone and Philip Leong},\nyear={2020},\nurl={https://openreview.net/forum?id=HyePberFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyePberFvH", "replyto": "HyePberFvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2140/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2140/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575483889200, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2140/Reviewers"], "noninvitees": [], "tcdate": 1570237727133, "tmdate": 1575483889213, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2140/-/Official_Review"}}}, {"id": "rkgOn7jaYS", "original": null, "number": 3, "cdate": 1571824559771, "ddate": null, "tcdate": 1571824559771, "tmdate": 1572972377820, "tddate": null, "forum": "HyePberFvH", "replyto": "HyePberFvH", "invitation": "ICLR.cc/2020/Conference/Paper2140/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The premise of this paper is that quantization plays an important role in the deployment of deep neural networks; ie in the inference stage. However, errors due to quantization affect different neural architectures differently. It would be useful if we could predict ahead of time which models are more amenable to quantization. I think this is a very interesting premise and the paper is very well motivated.\n\nThe paper is also very clear and well written, making the claims precise and backing these up with experiments.\n\nAt the heart of the paper is the replacement of floating point numbers with inexact values, which are treated as random variables and defined precisely in equation 4. This definition enables the authors to apply Monte Carlo methods to obtain network predictions as shown in equation (10) and figure 2, and subsequently carry out sensitivity analysis. The experiments show that a measure of sensitivity (K) is indeed a good augmentation to cross-validation for model selection for the purpose of trading-off accuracy and resource consumption when launching deep neural networks with floating point rounding errors.\n\nOne question I have for the authors is the following: There has been a large body of literature on Monte Carlo methods for Bayesian neural networks. Could those works have something to say in addressing some of the challenges posed in Section 4.1? \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2140/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2140/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["julian.faraone@sydney.edu.au", "philip.leong@sydney.edu.au"], "title": "Monte Carlo Deep Neural Network Arithmetic", "authors": ["Julian Faraone", "Philip Leong"], "pdf": "/pdf/5cbe0a087592d00d6cde7285273883d9ff46d291.pdf", "TL;DR": "Determining the sensitivity of Deep Neural Networks to floating point rounding error using Monte Carlo Methods", "abstract": "Quantization is a crucial technique for achieving low-power, low latency and high throughput hardware implementations of Deep Neural Networks. Quantized floating point representations have received recent interest due to their hardware efficiency benefits and ability to represent a higher dynamic range than fixed point representations, leading to improvements in accuracy.  We present a novel technique, Monte Carlo Deep Neural Network Arithmetic (MCA), for determining the sensitivity of Deep Neural Networks to quantization in floating point arithmetic.We do this by applying Monte Carlo Arithmetic to the inference computation and analyzing the relative standard deviation of the neural network loss.  The method makes no assumptions regarding the underlying parameter distributions. We evaluate our method on pre-trained image classification models on the CIFAR10 andImageNet datasets.  For the same network topology and dataset, we demonstrate the ability to gain the equivalent of bits of precision by simply choosing weight parameter sets which demonstrate a lower loss of significance from the Monte Carlo trials.   Additionally,  we can apply MCA to compare the sensitivity of different network topologies to quantization effects.", "keywords": ["deep learning", "quantization", "floating point", "monte carlo methods"], "paperhash": "faraone|monte_carlo_deep_neural_network_arithmetic", "original_pdf": "/attachment/4326dbe4f951679bd5bd2a34a84ac6c4672778e1.pdf", "_bibtex": "@misc{\nfaraone2020monte,\ntitle={Monte Carlo Deep Neural Network Arithmetic},\nauthor={Julian Faraone and Philip Leong},\nyear={2020},\nurl={https://openreview.net/forum?id=HyePberFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyePberFvH", "replyto": "HyePberFvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2140/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2140/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575483889200, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2140/Reviewers"], "noninvitees": [], "tcdate": 1570237727133, "tmdate": 1575483889213, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2140/-/Official_Review"}}}], "count": 9}