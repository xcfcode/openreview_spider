{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124435264, "tcdate": 1518472653143, "number": 333, "cdate": 1518472653143, "id": "BJHRaK1PG", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "BJHRaK1PG", "signatures": ["~Remi_Tachet_des_Combes1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Learning Invariances for Policy Generalization", "abstract": "While recent progress has spawned very powerful machine learning systems, those agents remain extremely specialized and fail to transfer the knowledge they gain to similar yet unseen tasks. In this paper, we study a simple reinforcement learning problem and focus on learning policies that encode the proper invariances for generalization to different settings. We evaluate three potential methods for policy generalization: data augmentation, meta-learning and adversarial training. We find our data augmentation method to be effective, and study the potential of meta-learning and adversarial learning as alternative task-agnostic approaches.", "paperhash": "combes|learning_invariances_for_policy_generalization", "_bibtex": "@misc{\n  combes2018learning,\n  title={Learning Invariances for Policy Generalization},\n  author={Remi Tachet des Combes and Philip Bachman and Harm van Seijen},\n  year={2018},\n  url={https://openreview.net/forum?id=BJHRaK1PG}\n}", "authorids": ["retachet@microsoft.com", "phbachma@microsoft.com", "havansei@microsoft.com"], "authors": ["Remi Tachet des Combes", "Philip Bachman", "Harm van Seijen"], "keywords": ["reinforcement learning", "generalization", "zero-shot learning", "data augmentation", "meta-learning", "adversarial training"], "pdf": "/pdf/d70a52773d08f0b6d831f8e2ed09853704a5943f.pdf"}, "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582930215, "tcdate": 1520330460912, "number": 1, "cdate": 1520330460912, "id": "rJBkD1n_M", "invitation": "ICLR.cc/2018/Workshop/-/Paper333/Official_Review", "forum": "BJHRaK1PG", "replyto": "BJHRaK1PG", "signatures": ["ICLR.cc/2018/Workshop/Paper333/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper333/AnonReviewer3"], "content": {"title": "Case study on policy generalization", "rating": "7: Good paper, accept", "review": "The problem is chosen to be controlling an avatar and making it jump over an obstacle. The authors compare data augmentation, meta learning and adversarial training.\nPros\n* Clear presentation and up to date references\n* Interesting and difficult question\n* promising results\nCons\n* The chosen data augmentation method generates situations very similar to the testing problems which somehow biased the results\n* The paper does not give hints for improving meta learning and adversarial training", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Invariances for Policy Generalization", "abstract": "While recent progress has spawned very powerful machine learning systems, those agents remain extremely specialized and fail to transfer the knowledge they gain to similar yet unseen tasks. In this paper, we study a simple reinforcement learning problem and focus on learning policies that encode the proper invariances for generalization to different settings. We evaluate three potential methods for policy generalization: data augmentation, meta-learning and adversarial training. We find our data augmentation method to be effective, and study the potential of meta-learning and adversarial learning as alternative task-agnostic approaches.", "paperhash": "combes|learning_invariances_for_policy_generalization", "_bibtex": "@misc{\n  combes2018learning,\n  title={Learning Invariances for Policy Generalization},\n  author={Remi Tachet des Combes and Philip Bachman and Harm van Seijen},\n  year={2018},\n  url={https://openreview.net/forum?id=BJHRaK1PG}\n}", "authorids": ["retachet@microsoft.com", "phbachma@microsoft.com", "havansei@microsoft.com"], "authors": ["Remi Tachet des Combes", "Philip Bachman", "Harm van Seijen"], "keywords": ["reinforcement learning", "generalization", "zero-shot learning", "data augmentation", "meta-learning", "adversarial training"], "pdf": "/pdf/d70a52773d08f0b6d831f8e2ed09853704a5943f.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582930027, "id": "ICLR.cc/2018/Workshop/-/Paper333/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper333/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper333/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper333/AnonReviewer1"], "reply": {"forum": "BJHRaK1PG", "replyto": "BJHRaK1PG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper333/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper333/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582930027}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582893595, "tcdate": 1520516883988, "number": 2, "cdate": 1520516883988, "id": "By3GypCuz", "invitation": "ICLR.cc/2018/Workshop/-/Paper333/Official_Review", "forum": "BJHRaK1PG", "replyto": "BJHRaK1PG", "signatures": ["ICLR.cc/2018/Workshop/Paper333/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper333/AnonReviewer1"], "content": {"title": "Nice experimental paper", "rating": "7: Good paper, accept", "review": "The paper studies the problem of learning a RL agent which policy is invariant to certain transformation of the task and thus is potentially able to generalize to new variants of the task.\nAuthors introduce a simple task where a number of parameters can vary and conduct generalization experiments with double Q-learning agent as a model. The results show that the simplest data augmentation techniques improve generalization better than more complicated and principled approaches such as meta- or adversarial learning.\nThe core problem that is being studied seems very important to me and it is interesting that even such a simple \"jumping\" task presents a challenge. However, to gain more insights for the research community from the negative results being presented, the experiments should certainly contain more information. I understand that the short paper format maybe does not allow this fully, but maybe authors could use the appendix to specify the network architecture, the exact way they used more advanced techniques, how the double DQN agent has been trained exactly etc. This way we could know more if the negative result has some fundamental reasons or can potentially be fixed with more careful training regime.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Invariances for Policy Generalization", "abstract": "While recent progress has spawned very powerful machine learning systems, those agents remain extremely specialized and fail to transfer the knowledge they gain to similar yet unseen tasks. In this paper, we study a simple reinforcement learning problem and focus on learning policies that encode the proper invariances for generalization to different settings. We evaluate three potential methods for policy generalization: data augmentation, meta-learning and adversarial training. We find our data augmentation method to be effective, and study the potential of meta-learning and adversarial learning as alternative task-agnostic approaches.", "paperhash": "combes|learning_invariances_for_policy_generalization", "_bibtex": "@misc{\n  combes2018learning,\n  title={Learning Invariances for Policy Generalization},\n  author={Remi Tachet des Combes and Philip Bachman and Harm van Seijen},\n  year={2018},\n  url={https://openreview.net/forum?id=BJHRaK1PG}\n}", "authorids": ["retachet@microsoft.com", "phbachma@microsoft.com", "havansei@microsoft.com"], "authors": ["Remi Tachet des Combes", "Philip Bachman", "Harm van Seijen"], "keywords": ["reinforcement learning", "generalization", "zero-shot learning", "data augmentation", "meta-learning", "adversarial training"], "pdf": "/pdf/d70a52773d08f0b6d831f8e2ed09853704a5943f.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582930027, "id": "ICLR.cc/2018/Workshop/-/Paper333/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper333/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper333/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper333/AnonReviewer1"], "reply": {"forum": "BJHRaK1PG", "replyto": "BJHRaK1PG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper333/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper333/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582930027}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573554967, "tcdate": 1521573554967, "number": 52, "cdate": 1521573554628, "id": "H1ohACRtG", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "BJHRaK1PG", "replyto": "BJHRaK1PG", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Invariances for Policy Generalization", "abstract": "While recent progress has spawned very powerful machine learning systems, those agents remain extremely specialized and fail to transfer the knowledge they gain to similar yet unseen tasks. In this paper, we study a simple reinforcement learning problem and focus on learning policies that encode the proper invariances for generalization to different settings. We evaluate three potential methods for policy generalization: data augmentation, meta-learning and adversarial training. We find our data augmentation method to be effective, and study the potential of meta-learning and adversarial learning as alternative task-agnostic approaches.", "paperhash": "combes|learning_invariances_for_policy_generalization", "_bibtex": "@misc{\n  combes2018learning,\n  title={Learning Invariances for Policy Generalization},\n  author={Remi Tachet des Combes and Philip Bachman and Harm van Seijen},\n  year={2018},\n  url={https://openreview.net/forum?id=BJHRaK1PG}\n}", "authorids": ["retachet@microsoft.com", "phbachma@microsoft.com", "havansei@microsoft.com"], "authors": ["Remi Tachet des Combes", "Philip Bachman", "Harm van Seijen"], "keywords": ["reinforcement learning", "generalization", "zero-shot learning", "data augmentation", "meta-learning", "adversarial training"], "pdf": "/pdf/d70a52773d08f0b6d831f8e2ed09853704a5943f.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 4}