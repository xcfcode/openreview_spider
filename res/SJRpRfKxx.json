{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1486807020966, "tcdate": 1478205125887, "number": 83, "id": "SJRpRfKxx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "SJRpRfKxx", "signatures": ["~Loris_Bazzani1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Recurrent Mixture Density Network for Spatiotemporal Visual Attention", "abstract": "In many computer vision tasks, the relevant information to solve the problem at hand is mixed to irrelevant, distracting information. This has motivated researchers to design attentional models that can dynamically focus on parts of images or videos that are salient, e.g., by down-weighting irrelevant pixels. In this work, we propose a spatiotemporal attentional model that learns where to look in a video directly from human fixation data. We model visual attention with a mixture of Gaussians at each frame. This distribution is used to express the probability of saliency for each pixel. Time consistency in videos is modeled hierarchically by: 1) deep 3D convolutional features to represent spatial and short-term time relations and 2) a long short-term memory network on top that aggregates the clip-level representation of sequential clips and therefore expands the temporal domain from few frames to seconds. The parameters of the proposed model are optimized via maximum likelihood estimation using human fixations as training data, without knowledge of the action in each video. Our experiments on Hollywood2 show state-of-the-art performance on saliency prediction for video. We also show that  our attentional model trained on Hollywood2 generalizes well to UCF101 and it can be leveraged to improve action classification accuracy on both datasets.", "pdf": "/pdf/581303df88ca3bbe6ae8ed1186fcbb3aaa8a6356.pdf", "paperhash": "bazzani|recurrent_mixture_density_network_for_spatiotemporal_visual_attention", "conflicts": ["dartmouth.edu", "amazon.com", "twitter.com", "usherbrooke.ca", "umontreal.ca", "google.com", "fb.com"], "keywords": ["Computer vision", "Deep learning", "Applications"], "authors": ["Loris Bazzani", "Hugo Larochelle", "Lorenzo Torresani"], "authorids": ["loris.bazzani@gmail.com", "hugo.larochelle@usherbrooke.ca", "lt@dartmouth.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 20, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396346410, "tcdate": 1486396346410, "number": 1, "id": "r1Majz8dl", "invitation": "ICLR.cc/2017/conference/-/paper83/acceptance", "forum": "SJRpRfKxx", "replyto": "SJRpRfKxx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The paper describes a model for video saliency prediction using a combination of spatio-temporal ConvNet features and LSTM. The proposed method outperforms the state of the art on the saliency prediction task and is shown to improve the performance of a baseline action classification model.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Recurrent Mixture Density Network for Spatiotemporal Visual Attention", "abstract": "In many computer vision tasks, the relevant information to solve the problem at hand is mixed to irrelevant, distracting information. This has motivated researchers to design attentional models that can dynamically focus on parts of images or videos that are salient, e.g., by down-weighting irrelevant pixels. In this work, we propose a spatiotemporal attentional model that learns where to look in a video directly from human fixation data. We model visual attention with a mixture of Gaussians at each frame. This distribution is used to express the probability of saliency for each pixel. Time consistency in videos is modeled hierarchically by: 1) deep 3D convolutional features to represent spatial and short-term time relations and 2) a long short-term memory network on top that aggregates the clip-level representation of sequential clips and therefore expands the temporal domain from few frames to seconds. The parameters of the proposed model are optimized via maximum likelihood estimation using human fixations as training data, without knowledge of the action in each video. Our experiments on Hollywood2 show state-of-the-art performance on saliency prediction for video. We also show that  our attentional model trained on Hollywood2 generalizes well to UCF101 and it can be leveraged to improve action classification accuracy on both datasets.", "pdf": "/pdf/581303df88ca3bbe6ae8ed1186fcbb3aaa8a6356.pdf", "paperhash": "bazzani|recurrent_mixture_density_network_for_spatiotemporal_visual_attention", "conflicts": ["dartmouth.edu", "amazon.com", "twitter.com", "usherbrooke.ca", "umontreal.ca", "google.com", "fb.com"], "keywords": ["Computer vision", "Deep learning", "Applications"], "authors": ["Loris Bazzani", "Hugo Larochelle", "Lorenzo Torresani"], "authorids": ["loris.bazzani@gmail.com", "hugo.larochelle@usherbrooke.ca", "lt@dartmouth.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396346989, "id": "ICLR.cc/2017/conference/-/paper83/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "SJRpRfKxx", "replyto": "SJRpRfKxx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396346989}}}, {"tddate": null, "tmdate": 1484685603144, "tcdate": 1484685603144, "number": 11, "id": "B1imb-hIg", "invitation": "ICLR.cc/2017/conference/-/paper83/public/comment", "forum": "SJRpRfKxx", "replyto": "SkEeMQnHl", "signatures": ["~Loris_Bazzani1"], "readers": ["everyone"], "writers": ["~Loris_Bazzani1"], "content": {"title": "Ablation Experiment", "comment": "We run the ablation study where the recurrent link between time t-1 and time t is removed from our model. The results in terms of AUC are 1.2% and 2.4% lower with respect to the RMDN which uses RNN and LSTM, respectively. We reported this result in the paper at page 7, second paragraph."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Recurrent Mixture Density Network for Spatiotemporal Visual Attention", "abstract": "In many computer vision tasks, the relevant information to solve the problem at hand is mixed to irrelevant, distracting information. This has motivated researchers to design attentional models that can dynamically focus on parts of images or videos that are salient, e.g., by down-weighting irrelevant pixels. In this work, we propose a spatiotemporal attentional model that learns where to look in a video directly from human fixation data. We model visual attention with a mixture of Gaussians at each frame. This distribution is used to express the probability of saliency for each pixel. Time consistency in videos is modeled hierarchically by: 1) deep 3D convolutional features to represent spatial and short-term time relations and 2) a long short-term memory network on top that aggregates the clip-level representation of sequential clips and therefore expands the temporal domain from few frames to seconds. The parameters of the proposed model are optimized via maximum likelihood estimation using human fixations as training data, without knowledge of the action in each video. Our experiments on Hollywood2 show state-of-the-art performance on saliency prediction for video. We also show that  our attentional model trained on Hollywood2 generalizes well to UCF101 and it can be leveraged to improve action classification accuracy on both datasets.", "pdf": "/pdf/581303df88ca3bbe6ae8ed1186fcbb3aaa8a6356.pdf", "paperhash": "bazzani|recurrent_mixture_density_network_for_spatiotemporal_visual_attention", "conflicts": ["dartmouth.edu", "amazon.com", "twitter.com", "usherbrooke.ca", "umontreal.ca", "google.com", "fb.com"], "keywords": ["Computer vision", "Deep learning", "Applications"], "authors": ["Loris Bazzani", "Hugo Larochelle", "Lorenzo Torresani"], "authorids": ["loris.bazzani@gmail.com", "hugo.larochelle@usherbrooke.ca", "lt@dartmouth.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287735589, "id": "ICLR.cc/2017/conference/-/paper83/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJRpRfKxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper83/reviewers", "ICLR.cc/2017/conference/paper83/areachairs"], "cdate": 1485287735589}}}, {"tddate": null, "tmdate": 1484085392923, "tcdate": 1484085392923, "number": 10, "id": "B1Y5uRM8x", "invitation": "ICLR.cc/2017/conference/-/paper83/public/comment", "forum": "SJRpRfKxx", "replyto": "rylbemnBx", "signatures": ["~Loris_Bazzani1"], "readers": ["everyone"], "writers": ["~Loris_Bazzani1"], "content": {"title": "Experiments CONV5 when multiplying the input with the saliency map", "comment": "In the new version of the manuscript, we added the experiments that use CONV5 when multiplying the input with the saliency map (second row of Table 3). We also modified the text to discuss the new results (second and fourth paragraph of Sec. 4.2).\n\nWe called \"feature mode\" when the convolutional representation is multiplied by the saliency, and \"clip mode\" when the saliency maps are used to weight the input video pixels. In summary: 1) the clip mode preforms better than the feature mode and 2) CONV5 and FC6 give similar results when considering the clip mode, but FC6 is preferred since it provides a more compact representation."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Recurrent Mixture Density Network for Spatiotemporal Visual Attention", "abstract": "In many computer vision tasks, the relevant information to solve the problem at hand is mixed to irrelevant, distracting information. This has motivated researchers to design attentional models that can dynamically focus on parts of images or videos that are salient, e.g., by down-weighting irrelevant pixels. In this work, we propose a spatiotemporal attentional model that learns where to look in a video directly from human fixation data. We model visual attention with a mixture of Gaussians at each frame. This distribution is used to express the probability of saliency for each pixel. Time consistency in videos is modeled hierarchically by: 1) deep 3D convolutional features to represent spatial and short-term time relations and 2) a long short-term memory network on top that aggregates the clip-level representation of sequential clips and therefore expands the temporal domain from few frames to seconds. The parameters of the proposed model are optimized via maximum likelihood estimation using human fixations as training data, without knowledge of the action in each video. Our experiments on Hollywood2 show state-of-the-art performance on saliency prediction for video. We also show that  our attentional model trained on Hollywood2 generalizes well to UCF101 and it can be leveraged to improve action classification accuracy on both datasets.", "pdf": "/pdf/581303df88ca3bbe6ae8ed1186fcbb3aaa8a6356.pdf", "paperhash": "bazzani|recurrent_mixture_density_network_for_spatiotemporal_visual_attention", "conflicts": ["dartmouth.edu", "amazon.com", "twitter.com", "usherbrooke.ca", "umontreal.ca", "google.com", "fb.com"], "keywords": ["Computer vision", "Deep learning", "Applications"], "authors": ["Loris Bazzani", "Hugo Larochelle", "Lorenzo Torresani"], "authorids": ["loris.bazzani@gmail.com", "hugo.larochelle@usherbrooke.ca", "lt@dartmouth.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287735589, "id": "ICLR.cc/2017/conference/-/paper83/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJRpRfKxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper83/reviewers", "ICLR.cc/2017/conference/paper83/areachairs"], "cdate": 1485287735589}}}, {"tddate": null, "tmdate": 1483645419945, "tcdate": 1483645419945, "number": 9, "id": "SkEeMQnHl", "invitation": "ICLR.cc/2017/conference/-/paper83/public/comment", "forum": "SJRpRfKxx", "replyto": "HkmLrvg4x", "signatures": ["~Loris_Bazzani1"], "readers": ["everyone"], "writers": ["~Loris_Bazzani1"], "content": {"title": "Answer to no title", "comment": "We agree with the reviewer that it is instructive to separately assess the importance of the recurrent part of our model. However, we believe that comparing our model with a fully convolutional network or a deconvolutional network would not provide an answer to this question, since these are methods that differ considerably from our proposed model, not only in their lack of recurrence. If the reviewer agrees, we propose instead to run a more informative ablation study, involving the removal of the recurrent link between time t-1 and time t from our model. This will provide a more direct answer concerning the usefulness of recurrency. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Recurrent Mixture Density Network for Spatiotemporal Visual Attention", "abstract": "In many computer vision tasks, the relevant information to solve the problem at hand is mixed to irrelevant, distracting information. This has motivated researchers to design attentional models that can dynamically focus on parts of images or videos that are salient, e.g., by down-weighting irrelevant pixels. In this work, we propose a spatiotemporal attentional model that learns where to look in a video directly from human fixation data. We model visual attention with a mixture of Gaussians at each frame. This distribution is used to express the probability of saliency for each pixel. Time consistency in videos is modeled hierarchically by: 1) deep 3D convolutional features to represent spatial and short-term time relations and 2) a long short-term memory network on top that aggregates the clip-level representation of sequential clips and therefore expands the temporal domain from few frames to seconds. The parameters of the proposed model are optimized via maximum likelihood estimation using human fixations as training data, without knowledge of the action in each video. Our experiments on Hollywood2 show state-of-the-art performance on saliency prediction for video. We also show that  our attentional model trained on Hollywood2 generalizes well to UCF101 and it can be leveraged to improve action classification accuracy on both datasets.", "pdf": "/pdf/581303df88ca3bbe6ae8ed1186fcbb3aaa8a6356.pdf", "paperhash": "bazzani|recurrent_mixture_density_network_for_spatiotemporal_visual_attention", "conflicts": ["dartmouth.edu", "amazon.com", "twitter.com", "usherbrooke.ca", "umontreal.ca", "google.com", "fb.com"], "keywords": ["Computer vision", "Deep learning", "Applications"], "authors": ["Loris Bazzani", "Hugo Larochelle", "Lorenzo Torresani"], "authorids": ["loris.bazzani@gmail.com", "hugo.larochelle@usherbrooke.ca", "lt@dartmouth.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287735589, "id": "ICLR.cc/2017/conference/-/paper83/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJRpRfKxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper83/reviewers", "ICLR.cc/2017/conference/paper83/areachairs"], "cdate": 1485287735589}}}, {"tddate": null, "tmdate": 1483645296602, "tcdate": 1483645296602, "number": 8, "id": "B1_O-XhBx", "invitation": "ICLR.cc/2017/conference/-/paper83/public/comment", "forum": "SJRpRfKxx", "replyto": "SyMt0S-4g", "signatures": ["~Loris_Bazzani1"], "readers": ["everyone"], "writers": ["~Loris_Bazzani1"], "content": {"title": "Answer to Review", "comment": "The context and attention parts are capturing slightly different information (as confirmed empirically in our study). The context features represent the video in its entirety. Therefore they may contain elements from the background, which may be useful for action categorization but at times also potentially distracting. Instead, the attention part focuses only on the most salient spatiotemporal volumes in the video. We agree that there are many other ways to use the saliency maps produced by our model. For example, the saliency map could be used to sample higher-resolution crops of the frames. We opted for the simplest model in order to show that what is contributing to the improvement in action categorization is truly coming from the saliency map and not from a more complex model.\n\n\u201cComparison between [(1), (2)] and (3) seems unfair.\u201d As we mentioned in the pre-review answer \u201cAnswer to few questions, comments\u201d, we ran an experiment with PCA to yield the same feature dimensionality as (1) in order to have a more fair comparison. The resulting testing mAP of this compressed descriptor (using both context and attention) is 51.82%, which is quite a bit better than method (1) (based on context only), despite having the same dimensionality. We added this relevant result to the paper (last paragraph in page 8).\n\nThe number of fixations are randomly subsampled (first paragraph, Sec. 3.1) if greater than A. Conversely, they are  randomly duplicated if less than A. \n\nIn our experiments, we froze the layers of the C3D. We ran several fine-tuning experiments for action recognition using the Hollywood2 dataset, however we have never obtained any significant improvement. We think that this might be due to the small size of the dataset and to the fact that the C3D features are already representative and general since they were trained on a huge dataset. This discouraged us to run fine-tuning experiments for saliency prediction.\n\nAbout feature concatenation: for each clip, we stacked the d-dimensional context feature vector on top of the d-dimensional attention-based feature vector to form a (2*d)-dimensional descriptor.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Recurrent Mixture Density Network for Spatiotemporal Visual Attention", "abstract": "In many computer vision tasks, the relevant information to solve the problem at hand is mixed to irrelevant, distracting information. This has motivated researchers to design attentional models that can dynamically focus on parts of images or videos that are salient, e.g., by down-weighting irrelevant pixels. In this work, we propose a spatiotemporal attentional model that learns where to look in a video directly from human fixation data. We model visual attention with a mixture of Gaussians at each frame. This distribution is used to express the probability of saliency for each pixel. Time consistency in videos is modeled hierarchically by: 1) deep 3D convolutional features to represent spatial and short-term time relations and 2) a long short-term memory network on top that aggregates the clip-level representation of sequential clips and therefore expands the temporal domain from few frames to seconds. The parameters of the proposed model are optimized via maximum likelihood estimation using human fixations as training data, without knowledge of the action in each video. Our experiments on Hollywood2 show state-of-the-art performance on saliency prediction for video. We also show that  our attentional model trained on Hollywood2 generalizes well to UCF101 and it can be leveraged to improve action classification accuracy on both datasets.", "pdf": "/pdf/581303df88ca3bbe6ae8ed1186fcbb3aaa8a6356.pdf", "paperhash": "bazzani|recurrent_mixture_density_network_for_spatiotemporal_visual_attention", "conflicts": ["dartmouth.edu", "amazon.com", "twitter.com", "usherbrooke.ca", "umontreal.ca", "google.com", "fb.com"], "keywords": ["Computer vision", "Deep learning", "Applications"], "authors": ["Loris Bazzani", "Hugo Larochelle", "Lorenzo Torresani"], "authorids": ["loris.bazzani@gmail.com", "hugo.larochelle@usherbrooke.ca", "lt@dartmouth.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287735589, "id": "ICLR.cc/2017/conference/-/paper83/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJRpRfKxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper83/reviewers", "ICLR.cc/2017/conference/paper83/areachairs"], "cdate": 1485287735589}}}, {"tddate": null, "tmdate": 1483644920544, "tcdate": 1483644920544, "number": 7, "id": "rylbemnBx", "invitation": "ICLR.cc/2017/conference/-/paper83/public/comment", "forum": "SJRpRfKxx", "replyto": "BkxeboW4g", "signatures": ["~Loris_Bazzani1"], "readers": ["everyone"], "writers": ["~Loris_Bazzani1"], "content": {"title": "Answer to Interesting paper, some evaluation issues", "comment": "As mentioned in the pre-review answer \u201cAnswer to Central Bias\u201d, the trained central bias is reported in Table 1 while Table 2 reports the manually-created central bias from Mathe & Sminchisescu, 2015. The trained central bias is used as a comparative simple baseline that does not consider the content of the frames.\nHuman performance is computed as described below in the pre-review answer \u201cAnswer to empirical distribution\u201d. Since the human performance uses fixations to create the saliency maps, we believe that Mathe & Sminchisescu, 2015 tuned the parameters \\sigma and p (Sec. 4.1). However, this information is missing from their paper.\nWe clarified these points in the new version of the manuscript (last paragraph of Sec. 4.1).\n\nOur results on Table 2 are in between the human performance and the trained central bias. We agree that AUC is not the best metric for evaluation because of the saturation problem. In fact we used also NSS, CC and Sim to evaluate the variants of our method. However, we needed to use the AUC to compare with the methods of Mathe & Sminchisescu, 2015.\n\nAbout the evaluation in Table 3, we wanted to evaluate two different ways to merge the saliency map. We thank the reviewer for pointing out that one relevant baseline is missing: CONV5 when multiplying the input with the saliency map. We are in the process of running this experiment. We will report the result as soon as we obtain it.\n\nK\u00fcmmerer et. al 2015 reference was misplaced by mistake. In fact, their model is trained on fixations for saliency prediction in images. We removed it from the second paragraph of Sec. 2 and correctly placed it in the third paragraph of Sec. 2. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Recurrent Mixture Density Network for Spatiotemporal Visual Attention", "abstract": "In many computer vision tasks, the relevant information to solve the problem at hand is mixed to irrelevant, distracting information. This has motivated researchers to design attentional models that can dynamically focus on parts of images or videos that are salient, e.g., by down-weighting irrelevant pixels. In this work, we propose a spatiotemporal attentional model that learns where to look in a video directly from human fixation data. We model visual attention with a mixture of Gaussians at each frame. This distribution is used to express the probability of saliency for each pixel. Time consistency in videos is modeled hierarchically by: 1) deep 3D convolutional features to represent spatial and short-term time relations and 2) a long short-term memory network on top that aggregates the clip-level representation of sequential clips and therefore expands the temporal domain from few frames to seconds. The parameters of the proposed model are optimized via maximum likelihood estimation using human fixations as training data, without knowledge of the action in each video. Our experiments on Hollywood2 show state-of-the-art performance on saliency prediction for video. We also show that  our attentional model trained on Hollywood2 generalizes well to UCF101 and it can be leveraged to improve action classification accuracy on both datasets.", "pdf": "/pdf/581303df88ca3bbe6ae8ed1186fcbb3aaa8a6356.pdf", "paperhash": "bazzani|recurrent_mixture_density_network_for_spatiotemporal_visual_attention", "conflicts": ["dartmouth.edu", "amazon.com", "twitter.com", "usherbrooke.ca", "umontreal.ca", "google.com", "fb.com"], "keywords": ["Computer vision", "Deep learning", "Applications"], "authors": ["Loris Bazzani", "Hugo Larochelle", "Lorenzo Torresani"], "authorids": ["loris.bazzani@gmail.com", "hugo.larochelle@usherbrooke.ca", "lt@dartmouth.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287735589, "id": "ICLR.cc/2017/conference/-/paper83/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJRpRfKxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper83/reviewers", "ICLR.cc/2017/conference/paper83/areachairs"], "cdate": 1485287735589}}}, {"tddate": null, "tmdate": 1482840639230, "tcdate": 1482840639230, "number": 6, "id": "BkPBcCyHe", "invitation": "ICLR.cc/2017/conference/-/paper83/public/comment", "forum": "SJRpRfKxx", "replyto": "Hk3lLBg4e", "signatures": ["~Loris_Bazzani1"], "readers": ["everyone"], "writers": ["~Loris_Bazzani1"], "content": {"title": "Answer to empirical distribution", "comment": "The human performance reported in Table 2 has been taken from Mathe & Sminchisescu, 2015 for comparisons. We can easily put the AUC in Table 1, however we do not have the NSS, the CC and the Sim measurements for the human performance. \n\nHuman performance (Mathe & Sminchisescu, 2015) is computed by deriving a saliency map from half of our human subjects and is evaluated with respect to fixations of the remaining ones. From their paper, it seems that the saliency map is computed in the same way as the ground truth saliency map, but considering half of the subjects.\n\nQualitative results that show how peaked are the predicted distributions can be found in the video available at https://youtu.be/aXOwc17nx_s. Can the reviewer clarify why the log-likelihood would be a good performance metric rather than the well-established measures (AUC, NSS, ...) for saliency prediction?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Recurrent Mixture Density Network for Spatiotemporal Visual Attention", "abstract": "In many computer vision tasks, the relevant information to solve the problem at hand is mixed to irrelevant, distracting information. This has motivated researchers to design attentional models that can dynamically focus on parts of images or videos that are salient, e.g., by down-weighting irrelevant pixels. In this work, we propose a spatiotemporal attentional model that learns where to look in a video directly from human fixation data. We model visual attention with a mixture of Gaussians at each frame. This distribution is used to express the probability of saliency for each pixel. Time consistency in videos is modeled hierarchically by: 1) deep 3D convolutional features to represent spatial and short-term time relations and 2) a long short-term memory network on top that aggregates the clip-level representation of sequential clips and therefore expands the temporal domain from few frames to seconds. The parameters of the proposed model are optimized via maximum likelihood estimation using human fixations as training data, without knowledge of the action in each video. Our experiments on Hollywood2 show state-of-the-art performance on saliency prediction for video. We also show that  our attentional model trained on Hollywood2 generalizes well to UCF101 and it can be leveraged to improve action classification accuracy on both datasets.", "pdf": "/pdf/581303df88ca3bbe6ae8ed1186fcbb3aaa8a6356.pdf", "paperhash": "bazzani|recurrent_mixture_density_network_for_spatiotemporal_visual_attention", "conflicts": ["dartmouth.edu", "amazon.com", "twitter.com", "usherbrooke.ca", "umontreal.ca", "google.com", "fb.com"], "keywords": ["Computer vision", "Deep learning", "Applications"], "authors": ["Loris Bazzani", "Hugo Larochelle", "Lorenzo Torresani"], "authorids": ["loris.bazzani@gmail.com", "hugo.larochelle@usherbrooke.ca", "lt@dartmouth.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287735589, "id": "ICLR.cc/2017/conference/-/paper83/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJRpRfKxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper83/reviewers", "ICLR.cc/2017/conference/paper83/areachairs"], "cdate": 1485287735589}}}, {"tddate": null, "tmdate": 1481985828119, "tcdate": 1481985828119, "number": 5, "id": "Hkhm1AGNx", "invitation": "ICLR.cc/2017/conference/-/paper83/public/comment", "forum": "SJRpRfKxx", "replyto": "HylMOrx4g", "signatures": ["~Loris_Bazzani1"], "readers": ["everyone"], "writers": ["~Loris_Bazzani1"], "content": {"title": "Answer to Central Bias", "comment": "The trained central bias of Table 1 has been trained using all the fixations of the training set (see third paragraph of sec. 4.1 for details). Instead the central bias of Table 2 is manually created by Mathe & Sminchisescu, 2015 and there is no training at all.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Recurrent Mixture Density Network for Spatiotemporal Visual Attention", "abstract": "In many computer vision tasks, the relevant information to solve the problem at hand is mixed to irrelevant, distracting information. This has motivated researchers to design attentional models that can dynamically focus on parts of images or videos that are salient, e.g., by down-weighting irrelevant pixels. In this work, we propose a spatiotemporal attentional model that learns where to look in a video directly from human fixation data. We model visual attention with a mixture of Gaussians at each frame. This distribution is used to express the probability of saliency for each pixel. Time consistency in videos is modeled hierarchically by: 1) deep 3D convolutional features to represent spatial and short-term time relations and 2) a long short-term memory network on top that aggregates the clip-level representation of sequential clips and therefore expands the temporal domain from few frames to seconds. The parameters of the proposed model are optimized via maximum likelihood estimation using human fixations as training data, without knowledge of the action in each video. Our experiments on Hollywood2 show state-of-the-art performance on saliency prediction for video. We also show that  our attentional model trained on Hollywood2 generalizes well to UCF101 and it can be leveraged to improve action classification accuracy on both datasets.", "pdf": "/pdf/581303df88ca3bbe6ae8ed1186fcbb3aaa8a6356.pdf", "paperhash": "bazzani|recurrent_mixture_density_network_for_spatiotemporal_visual_attention", "conflicts": ["dartmouth.edu", "amazon.com", "twitter.com", "usherbrooke.ca", "umontreal.ca", "google.com", "fb.com"], "keywords": ["Computer vision", "Deep learning", "Applications"], "authors": ["Loris Bazzani", "Hugo Larochelle", "Lorenzo Torresani"], "authorids": ["loris.bazzani@gmail.com", "hugo.larochelle@usherbrooke.ca", "lt@dartmouth.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287735589, "id": "ICLR.cc/2017/conference/-/paper83/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJRpRfKxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper83/reviewers", "ICLR.cc/2017/conference/paper83/areachairs"], "cdate": 1485287735589}}}, {"tddate": null, "tmdate": 1481908456005, "tcdate": 1481908456005, "number": 3, "id": "BkxeboW4g", "invitation": "ICLR.cc/2017/conference/-/paper83/official/review", "forum": "SJRpRfKxx", "replyto": "SJRpRfKxx", "signatures": ["ICLR.cc/2017/conference/paper83/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper83/AnonReviewer1"], "content": {"title": "Interesting paper, some evaluation issues", "rating": "7: Good paper, accept", "review": "The authors formulate a recurrent deep neural network to predict human fixation locations in videos as a mixture of Gaussians. They train the model using maximum likelihood with actual fixation data. Apart from evaluating how good the model performs at predicting fixations, they combine the saliency predictions with the C3D features for action recognition.\n\nquality: I am missing a more thorough evaluation of the fixation prediction performance. The center bias performance in Table 1 differs significantly from the on in Table 2. All the state-of-the-art models reported in Table 2 have a performance worse than the center bias performance reported in Table 1. Is there really no other model better than the center bias? Additionally I am missing details on how central bias and human performance are modelled. Is human performance cross-validated?\n\nYou claim that your \"results are very close to human performance (the difference is only 3.2%). This difference is actually larger than the difference between Central Bias and your model reported in Table 1. Apart from this, it is dangerous to compare AUC performance differences due to e.g. saturation issues.\n\nclarity: the explanation for Table 3 is a bit confusing, also it is not clear why the CONV5 and the FC6 models differ in how the saliency map is used. At least one should also evaluate the CONV5 model when multiplying the input with the saliency map to see how much of the difference comes from the different ways to use the saliency map and how much from the different features.\n\nOther issues:\n\nYou cite K\u00fcmmerer et. al 2015 as a model which \"learns ... indirectly rather than from explicit information of where humans look\", however the their model has been trained on fixation data using maximum-likelihood.\n\nApart from these issues, I think the paper make a very interesting contribution to spatio-temporal fixation prediction. If the evaluation issues given above are sorted out, I will happily improve my rating.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Recurrent Mixture Density Network for Spatiotemporal Visual Attention", "abstract": "In many computer vision tasks, the relevant information to solve the problem at hand is mixed to irrelevant, distracting information. This has motivated researchers to design attentional models that can dynamically focus on parts of images or videos that are salient, e.g., by down-weighting irrelevant pixels. In this work, we propose a spatiotemporal attentional model that learns where to look in a video directly from human fixation data. We model visual attention with a mixture of Gaussians at each frame. This distribution is used to express the probability of saliency for each pixel. Time consistency in videos is modeled hierarchically by: 1) deep 3D convolutional features to represent spatial and short-term time relations and 2) a long short-term memory network on top that aggregates the clip-level representation of sequential clips and therefore expands the temporal domain from few frames to seconds. The parameters of the proposed model are optimized via maximum likelihood estimation using human fixations as training data, without knowledge of the action in each video. Our experiments on Hollywood2 show state-of-the-art performance on saliency prediction for video. We also show that  our attentional model trained on Hollywood2 generalizes well to UCF101 and it can be leveraged to improve action classification accuracy on both datasets.", "pdf": "/pdf/581303df88ca3bbe6ae8ed1186fcbb3aaa8a6356.pdf", "paperhash": "bazzani|recurrent_mixture_density_network_for_spatiotemporal_visual_attention", "conflicts": ["dartmouth.edu", "amazon.com", "twitter.com", "usherbrooke.ca", "umontreal.ca", "google.com", "fb.com"], "keywords": ["Computer vision", "Deep learning", "Applications"], "authors": ["Loris Bazzani", "Hugo Larochelle", "Lorenzo Torresani"], "authorids": ["loris.bazzani@gmail.com", "hugo.larochelle@usherbrooke.ca", "lt@dartmouth.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512703541, "id": "ICLR.cc/2017/conference/-/paper83/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper83/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper83/AnonReviewer3", "ICLR.cc/2017/conference/paper83/AnonReviewer2", "ICLR.cc/2017/conference/paper83/AnonReviewer1"], "reply": {"forum": "SJRpRfKxx", "replyto": "SJRpRfKxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper83/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper83/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512703541}}}, {"tddate": null, "tmdate": 1481887353712, "tcdate": 1481887353712, "number": 2, "id": "SyMt0S-4g", "invitation": "ICLR.cc/2017/conference/-/paper83/official/review", "forum": "SJRpRfKxx", "replyto": "SJRpRfKxx", "signatures": ["ICLR.cc/2017/conference/paper83/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper83/AnonReviewer2"], "content": {"title": "Review", "rating": "6: Marginally above acceptance threshold", "review": "This work proposes to a spatiotemporal saliency network that is able to mimic human fixation patterns,\nthus helping to prune irrelevant information from the video and improve action recognition.\n\nThe work is interesting and has shown state-of-the-art results on predicting human attention on action videos.\nIt has also shown promise for helping action clip classification.\n\nThe paper would benefit from a discussion on the role of context in attention.\nFor instance, if context is important, and people give attention to context, why is it not incorporated automatically in your model?\n\nOne weak point is the action recognition section, where the comparison between the two (1)(2) and (3) seems unfair.\nThe attention weighted feature maps in fact reduce the classification performance, and only improve performance when doubling the feature and associated model complexity by concatenating the weighted maps with the original features.\n\nIs there a way to combine the context and attention without concatenation?\nThe rational for concatenating the features extracted from the original clip,\nand the features extracted from the saliency weighted clip seems to contradict the initial hypothesis that `eliminating or down-weighting pixels that are not important' will improve performance.\n\nThe authors should also mention the current state-of-the-art results in Table 4, for comparison.\n\n# Other comments:\n# Abstract\n- Typo: `mixed with irrelevant ...'\n``Time consistency in videos ... expands the temporal domain from few frames to seconds'' - These two points are not clear, probably need a re-write.\n\n# Contributions\n- 1) `The model can be trained without having to engineer spatiotemporal features' - you would need to collect training data from humans though.. \n\n# Section 3.1\nThe number of fixation points is controlled to be fixed for each frame - how is this done?\n\nIn practice we freeze the layers of the C3D network to values pretrained by Tran etal.\nWhat happens when you allow gradients to flow back to the C3D layers?\nIs it not better to allow the features to be best tuned for the final task?\n\nThe precise way in which the features are concatenated needs to be clarified in section 3.4.\n\nMinor typo:\n`we added them trained central bias'", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Recurrent Mixture Density Network for Spatiotemporal Visual Attention", "abstract": "In many computer vision tasks, the relevant information to solve the problem at hand is mixed to irrelevant, distracting information. This has motivated researchers to design attentional models that can dynamically focus on parts of images or videos that are salient, e.g., by down-weighting irrelevant pixels. In this work, we propose a spatiotemporal attentional model that learns where to look in a video directly from human fixation data. We model visual attention with a mixture of Gaussians at each frame. This distribution is used to express the probability of saliency for each pixel. Time consistency in videos is modeled hierarchically by: 1) deep 3D convolutional features to represent spatial and short-term time relations and 2) a long short-term memory network on top that aggregates the clip-level representation of sequential clips and therefore expands the temporal domain from few frames to seconds. The parameters of the proposed model are optimized via maximum likelihood estimation using human fixations as training data, without knowledge of the action in each video. Our experiments on Hollywood2 show state-of-the-art performance on saliency prediction for video. We also show that  our attentional model trained on Hollywood2 generalizes well to UCF101 and it can be leveraged to improve action classification accuracy on both datasets.", "pdf": "/pdf/581303df88ca3bbe6ae8ed1186fcbb3aaa8a6356.pdf", "paperhash": "bazzani|recurrent_mixture_density_network_for_spatiotemporal_visual_attention", "conflicts": ["dartmouth.edu", "amazon.com", "twitter.com", "usherbrooke.ca", "umontreal.ca", "google.com", "fb.com"], "keywords": ["Computer vision", "Deep learning", "Applications"], "authors": ["Loris Bazzani", "Hugo Larochelle", "Lorenzo Torresani"], "authorids": ["loris.bazzani@gmail.com", "hugo.larochelle@usherbrooke.ca", "lt@dartmouth.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512703541, "id": "ICLR.cc/2017/conference/-/paper83/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper83/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper83/AnonReviewer3", "ICLR.cc/2017/conference/paper83/AnonReviewer2", "ICLR.cc/2017/conference/paper83/AnonReviewer1"], "reply": {"forum": "SJRpRfKxx", "replyto": "SJRpRfKxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper83/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper83/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512703541}}}, {"tddate": null, "tmdate": 1481827659236, "tcdate": 1481827659231, "number": 1, "id": "HkmLrvg4x", "invitation": "ICLR.cc/2017/conference/-/paper83/official/review", "forum": "SJRpRfKxx", "replyto": "SJRpRfKxx", "signatures": ["ICLR.cc/2017/conference/paper83/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper83/AnonReviewer3"], "content": {"title": "", "rating": "6: Marginally above acceptance threshold", "review": "This paper proposes a new method for estimating visual attention in videos. The input clip is first processed by a convnet (in particular, C3D) to extract visual features. The visual features are then passed to LSTM. The hidden state at each time step in LSTM is used to generate the parameters in a Gaussian mixture model. Finally, the visual attention map is generated from the Gaussian mixture model.\n\nOverall, the idea in this paper is reasonable and the paper is well written. RNN/LSTM has been used in lots of vision problem where the outputs are discrete sequences, there has not been much work on using RNN/LSTM for problems where the output is continuous like in this paper.\n\nThe experimental results have demonstrated the effectiveness of the proposed approach. In particular, it outperforms other state-of-the-art on the saliency prediction task on the Hollywood2 datasets. It also shows improvement over baselines (e.g. C3D + SVM) on the action recognition task.\n\nMy only \"gripe\" of this paper is that this paper is missing some important baseline comparisons. In particular, it does not seem to show how the \"recurrent\" part help the overall performance. Although Table 2 shows RMDN outperforms other state-of-the-art, it might be due to the fact that it uses strong C3D features (while other methods in Table 2 use traditional handcrafted features). Since saliency prediction is essentially a dense image labeling problem (similar to semantic segmentation). For dense image labeling, there has been lots of methods proposed in the past two years, e.g. fully convolution neural network (FCN) or deconvnet. A straightforward baseline is to simply take FCN and apply it on each frame. If the proposed method still outperforms this baseline, we can know that the \"recurrent\" part really helps.\n\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Recurrent Mixture Density Network for Spatiotemporal Visual Attention", "abstract": "In many computer vision tasks, the relevant information to solve the problem at hand is mixed to irrelevant, distracting information. This has motivated researchers to design attentional models that can dynamically focus on parts of images or videos that are salient, e.g., by down-weighting irrelevant pixels. In this work, we propose a spatiotemporal attentional model that learns where to look in a video directly from human fixation data. We model visual attention with a mixture of Gaussians at each frame. This distribution is used to express the probability of saliency for each pixel. Time consistency in videos is modeled hierarchically by: 1) deep 3D convolutional features to represent spatial and short-term time relations and 2) a long short-term memory network on top that aggregates the clip-level representation of sequential clips and therefore expands the temporal domain from few frames to seconds. The parameters of the proposed model are optimized via maximum likelihood estimation using human fixations as training data, without knowledge of the action in each video. Our experiments on Hollywood2 show state-of-the-art performance on saliency prediction for video. We also show that  our attentional model trained on Hollywood2 generalizes well to UCF101 and it can be leveraged to improve action classification accuracy on both datasets.", "pdf": "/pdf/581303df88ca3bbe6ae8ed1186fcbb3aaa8a6356.pdf", "paperhash": "bazzani|recurrent_mixture_density_network_for_spatiotemporal_visual_attention", "conflicts": ["dartmouth.edu", "amazon.com", "twitter.com", "usherbrooke.ca", "umontreal.ca", "google.com", "fb.com"], "keywords": ["Computer vision", "Deep learning", "Applications"], "authors": ["Loris Bazzani", "Hugo Larochelle", "Lorenzo Torresani"], "authorids": ["loris.bazzani@gmail.com", "hugo.larochelle@usherbrooke.ca", "lt@dartmouth.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512703541, "id": "ICLR.cc/2017/conference/-/paper83/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper83/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper83/AnonReviewer3", "ICLR.cc/2017/conference/paper83/AnonReviewer2", "ICLR.cc/2017/conference/paper83/AnonReviewer1"], "reply": {"forum": "SJRpRfKxx", "replyto": "SJRpRfKxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper83/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper83/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512703541}}}, {"tddate": null, "tmdate": 1481820167901, "tcdate": 1481820167897, "number": 2, "id": "HylMOrx4g", "invitation": "ICLR.cc/2017/conference/-/paper83/official/comment", "forum": "SJRpRfKxx", "replyto": "H14QjKyQg", "signatures": ["ICLR.cc/2017/conference/paper83/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper83/AnonReviewer1"], "content": {"title": "Central Bias", "comment": "In Table 1 you report a Central Bias AUC performance of 87.25%, in Table 2 the Central Bias AUC performance for the same dataset is only 84%. What am I missing?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Recurrent Mixture Density Network for Spatiotemporal Visual Attention", "abstract": "In many computer vision tasks, the relevant information to solve the problem at hand is mixed to irrelevant, distracting information. This has motivated researchers to design attentional models that can dynamically focus on parts of images or videos that are salient, e.g., by down-weighting irrelevant pixels. In this work, we propose a spatiotemporal attentional model that learns where to look in a video directly from human fixation data. We model visual attention with a mixture of Gaussians at each frame. This distribution is used to express the probability of saliency for each pixel. Time consistency in videos is modeled hierarchically by: 1) deep 3D convolutional features to represent spatial and short-term time relations and 2) a long short-term memory network on top that aggregates the clip-level representation of sequential clips and therefore expands the temporal domain from few frames to seconds. The parameters of the proposed model are optimized via maximum likelihood estimation using human fixations as training data, without knowledge of the action in each video. Our experiments on Hollywood2 show state-of-the-art performance on saliency prediction for video. We also show that  our attentional model trained on Hollywood2 generalizes well to UCF101 and it can be leveraged to improve action classification accuracy on both datasets.", "pdf": "/pdf/581303df88ca3bbe6ae8ed1186fcbb3aaa8a6356.pdf", "paperhash": "bazzani|recurrent_mixture_density_network_for_spatiotemporal_visual_attention", "conflicts": ["dartmouth.edu", "amazon.com", "twitter.com", "usherbrooke.ca", "umontreal.ca", "google.com", "fb.com"], "keywords": ["Computer vision", "Deep learning", "Applications"], "authors": ["Loris Bazzani", "Hugo Larochelle", "Lorenzo Torresani"], "authorids": ["loris.bazzani@gmail.com", "hugo.larochelle@usherbrooke.ca", "lt@dartmouth.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287735465, "id": "ICLR.cc/2017/conference/-/paper83/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "SJRpRfKxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper83/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper83/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper83/reviewers", "ICLR.cc/2017/conference/paper83/areachairs"], "cdate": 1485287735465}}}, {"tddate": null, "tmdate": 1481819635680, "tcdate": 1481819635675, "number": 1, "id": "Hk3lLBg4e", "invitation": "ICLR.cc/2017/conference/-/paper83/official/comment", "forum": "SJRpRfKxx", "replyto": "H14QjKyQg", "signatures": ["ICLR.cc/2017/conference/paper83/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper83/AnonReviewer1"], "content": {"title": "Empirical distribution", "comment": "Thank you for your answer. You are right, Human performance is exactly what I was refering by emperical distribution. I missed that it is given in Table 2 -- I still think it might make sense to have it in Table 1, too.\n\nHow is the model for human performance formulated?\n\nLog-likelihoods would make a good performance metric because they measure how good the model actually is at prediction fixation locations (as opposed to the standard metrics like AUC) and give you a feeling for how peaked the true distribution and the model predictions are."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Recurrent Mixture Density Network for Spatiotemporal Visual Attention", "abstract": "In many computer vision tasks, the relevant information to solve the problem at hand is mixed to irrelevant, distracting information. This has motivated researchers to design attentional models that can dynamically focus on parts of images or videos that are salient, e.g., by down-weighting irrelevant pixels. In this work, we propose a spatiotemporal attentional model that learns where to look in a video directly from human fixation data. We model visual attention with a mixture of Gaussians at each frame. This distribution is used to express the probability of saliency for each pixel. Time consistency in videos is modeled hierarchically by: 1) deep 3D convolutional features to represent spatial and short-term time relations and 2) a long short-term memory network on top that aggregates the clip-level representation of sequential clips and therefore expands the temporal domain from few frames to seconds. The parameters of the proposed model are optimized via maximum likelihood estimation using human fixations as training data, without knowledge of the action in each video. Our experiments on Hollywood2 show state-of-the-art performance on saliency prediction for video. We also show that  our attentional model trained on Hollywood2 generalizes well to UCF101 and it can be leveraged to improve action classification accuracy on both datasets.", "pdf": "/pdf/581303df88ca3bbe6ae8ed1186fcbb3aaa8a6356.pdf", "paperhash": "bazzani|recurrent_mixture_density_network_for_spatiotemporal_visual_attention", "conflicts": ["dartmouth.edu", "amazon.com", "twitter.com", "usherbrooke.ca", "umontreal.ca", "google.com", "fb.com"], "keywords": ["Computer vision", "Deep learning", "Applications"], "authors": ["Loris Bazzani", "Hugo Larochelle", "Lorenzo Torresani"], "authorids": ["loris.bazzani@gmail.com", "hugo.larochelle@usherbrooke.ca", "lt@dartmouth.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287735465, "id": "ICLR.cc/2017/conference/-/paper83/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "SJRpRfKxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper83/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper83/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper83/reviewers", "ICLR.cc/2017/conference/paper83/areachairs"], "cdate": 1485287735465}}}, {"tddate": null, "tmdate": 1481301026051, "tcdate": 1481301026047, "number": 4, "id": "Hkc7hLdXg", "invitation": "ICLR.cc/2017/conference/-/paper83/public/comment", "forum": "SJRpRfKxx", "replyto": "SyWgpHnGg", "signatures": ["~Loris_Bazzani1"], "readers": ["everyone"], "writers": ["~Loris_Bazzani1"], "content": {"title": "Update about computational timing question and references", "comment": "The authors of the method we compared in Sec. 4.1 replied to our question about the runtimes. Unfortunately, they did not store this information, since measuring computational cost was not the focus of their work.\n\nAlso, we added the suggested references to the new version of the manuscript."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Recurrent Mixture Density Network for Spatiotemporal Visual Attention", "abstract": "In many computer vision tasks, the relevant information to solve the problem at hand is mixed to irrelevant, distracting information. This has motivated researchers to design attentional models that can dynamically focus on parts of images or videos that are salient, e.g., by down-weighting irrelevant pixels. In this work, we propose a spatiotemporal attentional model that learns where to look in a video directly from human fixation data. We model visual attention with a mixture of Gaussians at each frame. This distribution is used to express the probability of saliency for each pixel. Time consistency in videos is modeled hierarchically by: 1) deep 3D convolutional features to represent spatial and short-term time relations and 2) a long short-term memory network on top that aggregates the clip-level representation of sequential clips and therefore expands the temporal domain from few frames to seconds. The parameters of the proposed model are optimized via maximum likelihood estimation using human fixations as training data, without knowledge of the action in each video. Our experiments on Hollywood2 show state-of-the-art performance on saliency prediction for video. We also show that  our attentional model trained on Hollywood2 generalizes well to UCF101 and it can be leveraged to improve action classification accuracy on both datasets.", "pdf": "/pdf/581303df88ca3bbe6ae8ed1186fcbb3aaa8a6356.pdf", "paperhash": "bazzani|recurrent_mixture_density_network_for_spatiotemporal_visual_attention", "conflicts": ["dartmouth.edu", "amazon.com", "twitter.com", "usherbrooke.ca", "umontreal.ca", "google.com", "fb.com"], "keywords": ["Computer vision", "Deep learning", "Applications"], "authors": ["Loris Bazzani", "Hugo Larochelle", "Lorenzo Torresani"], "authorids": ["loris.bazzani@gmail.com", "hugo.larochelle@usherbrooke.ca", "lt@dartmouth.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287735589, "id": "ICLR.cc/2017/conference/-/paper83/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJRpRfKxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper83/reviewers", "ICLR.cc/2017/conference/paper83/areachairs"], "cdate": 1485287735589}}}, {"tddate": null, "tmdate": 1480723532679, "tcdate": 1480723532674, "number": 3, "id": "SkS8hF17l", "invitation": "ICLR.cc/2017/conference/-/paper83/public/comment", "forum": "SJRpRfKxx", "replyto": "SyWgpHnGg", "signatures": ["~Loris_Bazzani1"], "readers": ["everyone"], "writers": ["~Loris_Bazzani1"], "content": {"title": "Answer to few questions, comments", "comment": "Normalizing the saliency map to sum up to one does not have any major implications. We did it to facilitate evaluation and to have a normalized reweighting of the video for the action recognition task. In practice, one can use directly the learned Gaussian mixture as probability distribution, to sample fixations for example. When there is nothing salient in the image (e.g., when the ground truth human fixations are spread and do not show a clear pattern), the model still outputs a saliency map. But in such cases we empirically observed that the variances of the Gaussian components are quite large, and they collectively approximate the uniform distribution.\n\nThe concatenation of the two descriptors produces a representation that 1) includes the general context of the action in the videos and 2) down-weights pixels deemed less important, thus encouraging the recognition model to focus on parts that may be more salient. Since the RMDN is computing anyway the context features for each clip in order to estimate the saliency map, reusing them for action recognition seems straightforward and does not add computations. \n\nWe sent an email to the authors requesting their runtimes for comparison. Unfortunately, we cannot promise that they will respond in time.\n\nThe SVM model complexity for experiment (3) in Table 3 is indeed twice as large as the complexity for [(1),(2)]. The same applies when comparing (5) against [(1),(4)]. In fact, we added PCA dimensionality reduction to experiment (5) in order to match the same feature dimensionality as (1) (and (4)). Although the validation accuracies are similar, the testing mAP drops from 54.85% of experiment (5) to 51.82% of the PCA experiment. This is not surprising, since the extra dimensions provided by the use of the saliency map are not redundant with respect to the context representation (1). Therefore, concatenation seems to be the best way to make use of the saliency map.\n\nThank you for providing additional references that we missed. We will include them in newer versions of the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Recurrent Mixture Density Network for Spatiotemporal Visual Attention", "abstract": "In many computer vision tasks, the relevant information to solve the problem at hand is mixed to irrelevant, distracting information. This has motivated researchers to design attentional models that can dynamically focus on parts of images or videos that are salient, e.g., by down-weighting irrelevant pixels. In this work, we propose a spatiotemporal attentional model that learns where to look in a video directly from human fixation data. We model visual attention with a mixture of Gaussians at each frame. This distribution is used to express the probability of saliency for each pixel. Time consistency in videos is modeled hierarchically by: 1) deep 3D convolutional features to represent spatial and short-term time relations and 2) a long short-term memory network on top that aggregates the clip-level representation of sequential clips and therefore expands the temporal domain from few frames to seconds. The parameters of the proposed model are optimized via maximum likelihood estimation using human fixations as training data, without knowledge of the action in each video. Our experiments on Hollywood2 show state-of-the-art performance on saliency prediction for video. We also show that  our attentional model trained on Hollywood2 generalizes well to UCF101 and it can be leveraged to improve action classification accuracy on both datasets.", "pdf": "/pdf/581303df88ca3bbe6ae8ed1186fcbb3aaa8a6356.pdf", "paperhash": "bazzani|recurrent_mixture_density_network_for_spatiotemporal_visual_attention", "conflicts": ["dartmouth.edu", "amazon.com", "twitter.com", "usherbrooke.ca", "umontreal.ca", "google.com", "fb.com"], "keywords": ["Computer vision", "Deep learning", "Applications"], "authors": ["Loris Bazzani", "Hugo Larochelle", "Lorenzo Torresani"], "authorids": ["loris.bazzani@gmail.com", "hugo.larochelle@usherbrooke.ca", "lt@dartmouth.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287735589, "id": "ICLR.cc/2017/conference/-/paper83/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJRpRfKxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper83/reviewers", "ICLR.cc/2017/conference/paper83/areachairs"], "cdate": 1485287735589}}}, {"tddate": null, "tmdate": 1480723228134, "tcdate": 1480723228129, "number": 2, "id": "H14QjKyQg", "invitation": "ICLR.cc/2017/conference/-/paper83/public/comment", "forum": "SJRpRfKxx", "replyto": "Hycsv3sfl", "signatures": ["~Loris_Bazzani1"], "readers": ["everyone"], "writers": ["~Loris_Bazzani1"], "content": {"title": "Answer about performance measures", "comment": "We used the area under Receiver Operating Characteristic curve (AUC) commonly applied in the context of saliency estimation (Judd et al., 2012; Borji et al., 2013). (Borji et al., 2013) explain it as follows: \u201chuman fixations are then used as ground truth. By varying the threshold [on the saliency map], the ROC curve is drawn as the false positive rate vs. true positive rate, and the area under this curve indicates how well the saliency map predicts actual human eye fixations.\u201d Moreover, Fig. 7 in [A] shows a nice visualization that aids comprehension.\n\nWe are not 100% sure what you mean by \"empirical distribution\". But we guess that the reviewer is effectively after an experiment that would estimate what is the best achievable performance. Is that correct? In this case, we'd argue that the Human performance (reported in Table 2) essentially achieves this objective.\n\nAbout the log-likelihood metric, it is a good point since it is another way to compare the models with respect to the training objective. We did not report it because we were interested in the final accuracy, in order to look at the standard metrics for saliency estimation and eventually compare with other methods.\n\n[A] X. Hou, J. Harel, and Christof Koch, \u201dImage Signature: Highlighting sparse salient regions,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Recurrent Mixture Density Network for Spatiotemporal Visual Attention", "abstract": "In many computer vision tasks, the relevant information to solve the problem at hand is mixed to irrelevant, distracting information. This has motivated researchers to design attentional models that can dynamically focus on parts of images or videos that are salient, e.g., by down-weighting irrelevant pixels. In this work, we propose a spatiotemporal attentional model that learns where to look in a video directly from human fixation data. We model visual attention with a mixture of Gaussians at each frame. This distribution is used to express the probability of saliency for each pixel. Time consistency in videos is modeled hierarchically by: 1) deep 3D convolutional features to represent spatial and short-term time relations and 2) a long short-term memory network on top that aggregates the clip-level representation of sequential clips and therefore expands the temporal domain from few frames to seconds. The parameters of the proposed model are optimized via maximum likelihood estimation using human fixations as training data, without knowledge of the action in each video. Our experiments on Hollywood2 show state-of-the-art performance on saliency prediction for video. We also show that  our attentional model trained on Hollywood2 generalizes well to UCF101 and it can be leveraged to improve action classification accuracy on both datasets.", "pdf": "/pdf/581303df88ca3bbe6ae8ed1186fcbb3aaa8a6356.pdf", "paperhash": "bazzani|recurrent_mixture_density_network_for_spatiotemporal_visual_attention", "conflicts": ["dartmouth.edu", "amazon.com", "twitter.com", "usherbrooke.ca", "umontreal.ca", "google.com", "fb.com"], "keywords": ["Computer vision", "Deep learning", "Applications"], "authors": ["Loris Bazzani", "Hugo Larochelle", "Lorenzo Torresani"], "authorids": ["loris.bazzani@gmail.com", "hugo.larochelle@usherbrooke.ca", "lt@dartmouth.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287735589, "id": "ICLR.cc/2017/conference/-/paper83/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJRpRfKxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper83/reviewers", "ICLR.cc/2017/conference/paper83/areachairs"], "cdate": 1485287735589}}}, {"tddate": null, "tmdate": 1480723030462, "tcdate": 1480723030458, "number": 1, "id": "HJ0Ict1Xl", "invitation": "ICLR.cc/2017/conference/-/paper83/public/comment", "forum": "SJRpRfKxx", "replyto": "H1FXPtqfl", "signatures": ["~Loris_Bazzani1"], "readers": ["everyone"], "writers": ["~Loris_Bazzani1"], "content": {"title": "advantage over unsuperivsed attention", "comment": "The main motivation of the paper is to create a model that is able to mimic human attention. Since human fixations are not specialized for a single task but instead are functional to all forms of our visual processing, we believe that our model may serve as a general-purpose saliency estimator that can be useful for different applications. \n\nConversely, \u201cunsupervised\u201d attention models learned as part of a supervised task are more specific by design and they may be harder to transfer to other tasks.\n\nWe decided to evaluate our model on the task-agnostic task of saliency estimation. Moreover, we chose to test the model on the high-level task of action recognition because 1) it is a typical problem in the context of video, 2) we wanted to compare with Mathe & Sminchisescu, 2015 and 3) we aimed to show that we can train attention on one dataset (Hollywood2) and use it on another (UCF101). However, we believe that our model may be useful in other application scenarios, precisely because it has been trained to mimic human attention. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Recurrent Mixture Density Network for Spatiotemporal Visual Attention", "abstract": "In many computer vision tasks, the relevant information to solve the problem at hand is mixed to irrelevant, distracting information. This has motivated researchers to design attentional models that can dynamically focus on parts of images or videos that are salient, e.g., by down-weighting irrelevant pixels. In this work, we propose a spatiotemporal attentional model that learns where to look in a video directly from human fixation data. We model visual attention with a mixture of Gaussians at each frame. This distribution is used to express the probability of saliency for each pixel. Time consistency in videos is modeled hierarchically by: 1) deep 3D convolutional features to represent spatial and short-term time relations and 2) a long short-term memory network on top that aggregates the clip-level representation of sequential clips and therefore expands the temporal domain from few frames to seconds. The parameters of the proposed model are optimized via maximum likelihood estimation using human fixations as training data, without knowledge of the action in each video. Our experiments on Hollywood2 show state-of-the-art performance on saliency prediction for video. We also show that  our attentional model trained on Hollywood2 generalizes well to UCF101 and it can be leveraged to improve action classification accuracy on both datasets.", "pdf": "/pdf/581303df88ca3bbe6ae8ed1186fcbb3aaa8a6356.pdf", "paperhash": "bazzani|recurrent_mixture_density_network_for_spatiotemporal_visual_attention", "conflicts": ["dartmouth.edu", "amazon.com", "twitter.com", "usherbrooke.ca", "umontreal.ca", "google.com", "fb.com"], "keywords": ["Computer vision", "Deep learning", "Applications"], "authors": ["Loris Bazzani", "Hugo Larochelle", "Lorenzo Torresani"], "authorids": ["loris.bazzani@gmail.com", "hugo.larochelle@usherbrooke.ca", "lt@dartmouth.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287735589, "id": "ICLR.cc/2017/conference/-/paper83/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJRpRfKxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper83/reviewers", "ICLR.cc/2017/conference/paper83/areachairs"], "cdate": 1485287735589}}}, {"tddate": null, "tmdate": 1480510697116, "tcdate": 1480510697111, "number": 3, "id": "SyWgpHnGg", "invitation": "ICLR.cc/2017/conference/-/paper83/pre-review/question", "forum": "SJRpRfKxx", "replyto": "SJRpRfKxx", "signatures": ["ICLR.cc/2017/conference/paper83/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper83/AnonReviewer2"], "content": {"title": "few questions, comments", "question": "# Section 3.3\nWhat is the effect of normalising the probability map to sum to one at each time-step? How do you represent the saliency in a frame where nothing is salient? Is that an issue?\n\n# Section 3.4\nThe rational for concatenating the features extracted from the original clip,\nand the features extracted from the saliency weighted clip seems to contradict the initial hypothesis that `eliminating or down-weighting pixels that are not important' will improve performance.\nCan you elaborate on this point?\n\n# Section 4.1\nCan you send an email to the authors to get their computational timings for a comparison?\n\n# Section 4.2\nConcatenating features from two sources will double the SVM model complexity, making the comparison between [(1),(2)] and (3) in Table 3 unfair.\nThe dimensionality of the features can have a large effect on the ease of classification.\nWhat are your thoughts on that?\n\n\nMissing references in the introduction.\nParagraphs 1-2, an example of using attention modelling to improve action recognition:\nhttp://www.eleonoravig.com/papers/eccv12_action_recognition.pdf\n\nParagraph 3, an example of identifying spatio-temporal volumes which are salient to improve action recognition, and an example of joint saliency prediction and action recognition:\nhttp://www.robots.ox.ac.uk/~tvg/publications/2013/sapienza_IJCV13_accepted.pdf"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Recurrent Mixture Density Network for Spatiotemporal Visual Attention", "abstract": "In many computer vision tasks, the relevant information to solve the problem at hand is mixed to irrelevant, distracting information. This has motivated researchers to design attentional models that can dynamically focus on parts of images or videos that are salient, e.g., by down-weighting irrelevant pixels. In this work, we propose a spatiotemporal attentional model that learns where to look in a video directly from human fixation data. We model visual attention with a mixture of Gaussians at each frame. This distribution is used to express the probability of saliency for each pixel. Time consistency in videos is modeled hierarchically by: 1) deep 3D convolutional features to represent spatial and short-term time relations and 2) a long short-term memory network on top that aggregates the clip-level representation of sequential clips and therefore expands the temporal domain from few frames to seconds. The parameters of the proposed model are optimized via maximum likelihood estimation using human fixations as training data, without knowledge of the action in each video. Our experiments on Hollywood2 show state-of-the-art performance on saliency prediction for video. We also show that  our attentional model trained on Hollywood2 generalizes well to UCF101 and it can be leveraged to improve action classification accuracy on both datasets.", "pdf": "/pdf/581303df88ca3bbe6ae8ed1186fcbb3aaa8a6356.pdf", "paperhash": "bazzani|recurrent_mixture_density_network_for_spatiotemporal_visual_attention", "conflicts": ["dartmouth.edu", "amazon.com", "twitter.com", "usherbrooke.ca", "umontreal.ca", "google.com", "fb.com"], "keywords": ["Computer vision", "Deep learning", "Applications"], "authors": ["Loris Bazzani", "Hugo Larochelle", "Lorenzo Torresani"], "authorids": ["loris.bazzani@gmail.com", "hugo.larochelle@usherbrooke.ca", "lt@dartmouth.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959472400, "id": "ICLR.cc/2017/conference/-/paper83/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper83/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper83/AnonReviewer3", "ICLR.cc/2017/conference/paper83/AnonReviewer1", "ICLR.cc/2017/conference/paper83/AnonReviewer2"], "reply": {"forum": "SJRpRfKxx", "replyto": "SJRpRfKxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper83/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper83/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959472400}}}, {"tddate": null, "tmdate": 1480472482509, "tcdate": 1480472482503, "number": 2, "id": "Hycsv3sfl", "invitation": "ICLR.cc/2017/conference/-/paper83/pre-review/question", "forum": "SJRpRfKxx", "replyto": "SJRpRfKxx", "signatures": ["ICLR.cc/2017/conference/paper83/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper83/AnonReviewer1"], "content": {"title": "Question about performance measures", "question": "In Table 1 you evaluate among others the AUC metric. Could you clarify which kind of AUC metric you use? I guess it is the AUC metric with a uniform nonfixation distribution (e.g. using more or less all pixels of the saliency map as nonfixated pixels). Additionally to get a better understanding for how good the model is I think it would be very helpful to see not only the centerbias baseline, but also the performance of the empirical distribution that you show in the video you refer to. Besides this, as you train the model for log-likelihood, why don't report this metric too? I think it might be very informative, especially for getting and intuition of how predictable the data is."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Recurrent Mixture Density Network for Spatiotemporal Visual Attention", "abstract": "In many computer vision tasks, the relevant information to solve the problem at hand is mixed to irrelevant, distracting information. This has motivated researchers to design attentional models that can dynamically focus on parts of images or videos that are salient, e.g., by down-weighting irrelevant pixels. In this work, we propose a spatiotemporal attentional model that learns where to look in a video directly from human fixation data. We model visual attention with a mixture of Gaussians at each frame. This distribution is used to express the probability of saliency for each pixel. Time consistency in videos is modeled hierarchically by: 1) deep 3D convolutional features to represent spatial and short-term time relations and 2) a long short-term memory network on top that aggregates the clip-level representation of sequential clips and therefore expands the temporal domain from few frames to seconds. The parameters of the proposed model are optimized via maximum likelihood estimation using human fixations as training data, without knowledge of the action in each video. Our experiments on Hollywood2 show state-of-the-art performance on saliency prediction for video. We also show that  our attentional model trained on Hollywood2 generalizes well to UCF101 and it can be leveraged to improve action classification accuracy on both datasets.", "pdf": "/pdf/581303df88ca3bbe6ae8ed1186fcbb3aaa8a6356.pdf", "paperhash": "bazzani|recurrent_mixture_density_network_for_spatiotemporal_visual_attention", "conflicts": ["dartmouth.edu", "amazon.com", "twitter.com", "usherbrooke.ca", "umontreal.ca", "google.com", "fb.com"], "keywords": ["Computer vision", "Deep learning", "Applications"], "authors": ["Loris Bazzani", "Hugo Larochelle", "Lorenzo Torresani"], "authorids": ["loris.bazzani@gmail.com", "hugo.larochelle@usherbrooke.ca", "lt@dartmouth.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959472400, "id": "ICLR.cc/2017/conference/-/paper83/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper83/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper83/AnonReviewer3", "ICLR.cc/2017/conference/paper83/AnonReviewer1", "ICLR.cc/2017/conference/paper83/AnonReviewer2"], "reply": {"forum": "SJRpRfKxx", "replyto": "SJRpRfKxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper83/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper83/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959472400}}}, {"tddate": null, "tmdate": 1480394529508, "tcdate": 1480394529501, "number": 1, "id": "H1FXPtqfl", "invitation": "ICLR.cc/2017/conference/-/paper83/pre-review/question", "forum": "SJRpRfKxx", "replyto": "SJRpRfKxx", "signatures": ["ICLR.cc/2017/conference/paper83/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper83/AnonReviewer3"], "content": {"title": "advantage over unsuperivsed attention", "question": "Most attention-based models in deep learning are in some sense \"unsupervised\" attentions, where the attentions are some hidden variables jointly trained with the rest of the network. What is the advantage of the \"supervised\" attention in this paper, if the goal is to solve some high-level task (e.g. classification)? "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Recurrent Mixture Density Network for Spatiotemporal Visual Attention", "abstract": "In many computer vision tasks, the relevant information to solve the problem at hand is mixed to irrelevant, distracting information. This has motivated researchers to design attentional models that can dynamically focus on parts of images or videos that are salient, e.g., by down-weighting irrelevant pixels. In this work, we propose a spatiotemporal attentional model that learns where to look in a video directly from human fixation data. We model visual attention with a mixture of Gaussians at each frame. This distribution is used to express the probability of saliency for each pixel. Time consistency in videos is modeled hierarchically by: 1) deep 3D convolutional features to represent spatial and short-term time relations and 2) a long short-term memory network on top that aggregates the clip-level representation of sequential clips and therefore expands the temporal domain from few frames to seconds. The parameters of the proposed model are optimized via maximum likelihood estimation using human fixations as training data, without knowledge of the action in each video. Our experiments on Hollywood2 show state-of-the-art performance on saliency prediction for video. We also show that  our attentional model trained on Hollywood2 generalizes well to UCF101 and it can be leveraged to improve action classification accuracy on both datasets.", "pdf": "/pdf/581303df88ca3bbe6ae8ed1186fcbb3aaa8a6356.pdf", "paperhash": "bazzani|recurrent_mixture_density_network_for_spatiotemporal_visual_attention", "conflicts": ["dartmouth.edu", "amazon.com", "twitter.com", "usherbrooke.ca", "umontreal.ca", "google.com", "fb.com"], "keywords": ["Computer vision", "Deep learning", "Applications"], "authors": ["Loris Bazzani", "Hugo Larochelle", "Lorenzo Torresani"], "authorids": ["loris.bazzani@gmail.com", "hugo.larochelle@usherbrooke.ca", "lt@dartmouth.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959472400, "id": "ICLR.cc/2017/conference/-/paper83/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper83/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper83/AnonReviewer3", "ICLR.cc/2017/conference/paper83/AnonReviewer1", "ICLR.cc/2017/conference/paper83/AnonReviewer2"], "reply": {"forum": "SJRpRfKxx", "replyto": "SJRpRfKxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper83/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper83/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959472400}}}], "count": 21}