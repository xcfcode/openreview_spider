{"notes": [{"id": "fqxhxkzP0V", "original": null, "number": 3, "cdate": 1615733340777, "ddate": null, "tcdate": 1615733340777, "tmdate": 1615733340777, "tddate": null, "forum": "ixpSxO9flk3", "replyto": "dbS9Lioddyi", "invitation": "ICLR.cc/2021/Conference/Paper2046/-/Comment", "content": {"title": "some responses", "comment": "Zhijian,\n\nThe point of this work was not to completely demonize the use of MCMC for training energy-based models. MCMC-based training *can* be slow and hard to tune, making it hard to generalize to many domains (like our tabular data experiments). Your inclusive-NRF is not slow to train and gets very impressive results, but it does require the use of 2 different MCMC samplers which must be tuned. We must choose the stepsizes, the number of steps, type of sampler, and so on. These must be tuned properly or else the model will not train.\n\nWe don't claim that this makes our approach superior to yours. \n\nWe were interested in providing an alternative method for training EBMs that does not require the tuning of MCMC samplers which has been the most difficult part of EBM training in my personal experience. \n\nI hope this clarifies things."}, "signatures": ["ICLR.cc/2021/Conference/Paper2046/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "No MCMC for me: Amortized sampling for fast and stable training of energy-based models", "authorids": ["~Will_Sussman_Grathwohl2", "jacob.jin.kelly@gmail.com", "~Milad_Hashemi1", "~Mohammad_Norouzi1", "~Kevin_Swersky1", "~David_Duvenaud2"], "authors": ["Will Sussman Grathwohl", "Jacob Jin Kelly", "Milad Hashemi", "Mohammad Norouzi", "Kevin Swersky", "David Duvenaud"], "keywords": ["Generative Models", "EBM", "Energy-Based Models", "Energy Based Models", "semi-supervised learning", "JEM"], "abstract": "Energy-Based Models (EBMs) present a flexible and appealing way to represent uncertainty. Despite recent advances, training EBMs on high-dimensional data remains a challenging problem as the state-of-the-art approaches are costly, unstable, and require considerable tuning and domain expertise to apply successfully. In this work, we present a simple method for training EBMs at scale which uses an entropy-regularized generator to amortize the MCMC sampling typically used in EBM training. We improve upon prior MCMC-based entropy regularization methods with a fast variational approximation. We demonstrate the effectiveness of our approach by using it to train tractable likelihood models. Next, we apply our estimator to the recently proposed Joint Energy Model (JEM), where we match the original performance with faster and stable training. This allows us to extend JEM models to semi-supervised classification on tabular data from a variety of continuous domains.", "one-sentence_summary": "We present a new generator-based approach for training EBMs and demonstrate that it trains models which obtain high likelihood and overcomes stability issues common in EBM training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grathwohl|no_mcmc_for_me_amortized_sampling_for_fast_and_stable_training_of_energybased_models", "supplementary_material": "", "pdf": "/pdf/4bf06b7c43a66250e22fbdad3d0f492a3293efc6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngrathwohl2021no,\ntitle={No {\\{}MCMC{\\}} for me: Amortized sampling for fast and stable training of energy-based models},\nauthor={Will Sussman Grathwohl and Jacob Jin Kelly and Milad Hashemi and Mohammad Norouzi and Kevin Swersky and David Duvenaud},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ixpSxO9flk3}\n}"}, "tags": [], "invitation": {"reply": {"forum": "ixpSxO9flk3", "readers": {"values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2021/Conference/Paper2046/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2046/Authors|ICLR.cc/2021/Conference/Paper2046/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs"}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}}, "multiReply": true, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["everyone"], "tcdate": 1610649467180, "tmdate": 1610649467180, "id": "ICLR.cc/2021/Conference/Paper2046/-/Comment"}}}, {"id": "dbS9Lioddyi", "original": null, "number": 2, "cdate": 1615714537549, "ddate": null, "tcdate": 1615714537549, "tmdate": 1615714712481, "tddate": null, "forum": "ixpSxO9flk3", "replyto": "ixpSxO9flk3", "invitation": "ICLR.cc/2021/Conference/Paper2046/-/Comment", "content": {"title": "Some concerns", "comment": "Nice to read this paper. The paper addresses the important problem of improving EBM training and does a good job in demonstrating the usefulness of EBM and the new training method for a number of interesting applications. I have some concerns.\n\n1. The claim on \"no MCMC for me\" is prone to misleading. The authors compare different EBM training approaches in the Introduction and Table 1. But there are some inaccurate points. First, MCMC methods can also use auxiliary models, such as in CoopNet, Inclusive-NRF, and so on. Second, it may be too simple to category MCMC methods as slow training. In fact, the time complexity of some MCMC methods USED IN TRAINING is not at all slow. It should be stressed that based on the stochastic approximation (SA) framework of model training [a,b], we do not need to run the Markov chain for sufficiently long time to converge within one SA iteration (theoretically one step Markov move would suffice). This is unlike in applications of MCMC solely FOR INFERENCE. This understanding is often overlooked when commenting on MCMC methods for model learning. Further, if a concern of this work is to improve training time complexity, then the readers expect this paper to provide some comparisons. In fact, Inclusive-NRF [c] uses one step Stochastic Gradient Langevin Dynamics (SGLD), much faster than IGEBM [d] and NCSN [e], and obtains superior image generation results (See below).\n\n2. Note that on CIFAR-10, unsupervised Inclusive-NRF obtains IS 7.54\u00b10.10, FID 27.9\u00b10.53 [c]. In contrast, VERA uses the same CNN generator, more complexed Wide ResNet for energy function, and, remarkably, trains JEM which uses labels. Training JEM with labels is easier than training EBM. In fact, the JEM model is the supervised Inclusive-NRF in [c], and EBM is the unsupervised Inclusive-NRF. As shown in [c], supervised model significantly outperform unsupervised model. Considering these, the result of VERA in Table 4 is not so impressive. \n\n[a] Andrieu, C., Moulines, \u00c9., and Priouret, P. (2005). Stability of stochastic approximation under verifiable conditions. SIAM Journal on control and optimization, 44(1):283\u2013312.\n\n[b] Song, Q., Wu, M., and Liang, F. (2014). Weak convergence rates of population versus single-chain stochastic approximation MCMC algorithms. Advances in Applied Probability, 46(4):1059\u20131083.\n\n[c] Yunfu Song, Zhijian Ou. Learning neural random fields with inclusive auxiliary generators. arXiv:1806.0027, 2018.\n\n[d] Yilun Du and Igor Mordatch. Implicit generation and generalization in energy-based models. arXiv:1903.08689, 2019.\n\n[e] Yang Song, Stefano Ermon. Improved techniques for training score-based generative models. arXiv:2006.09011, 2020.\n\nThanks for the nice work."}, "signatures": ["~Zhijian_Ou1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Zhijian_Ou1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "No MCMC for me: Amortized sampling for fast and stable training of energy-based models", "authorids": ["~Will_Sussman_Grathwohl2", "jacob.jin.kelly@gmail.com", "~Milad_Hashemi1", "~Mohammad_Norouzi1", "~Kevin_Swersky1", "~David_Duvenaud2"], "authors": ["Will Sussman Grathwohl", "Jacob Jin Kelly", "Milad Hashemi", "Mohammad Norouzi", "Kevin Swersky", "David Duvenaud"], "keywords": ["Generative Models", "EBM", "Energy-Based Models", "Energy Based Models", "semi-supervised learning", "JEM"], "abstract": "Energy-Based Models (EBMs) present a flexible and appealing way to represent uncertainty. Despite recent advances, training EBMs on high-dimensional data remains a challenging problem as the state-of-the-art approaches are costly, unstable, and require considerable tuning and domain expertise to apply successfully. In this work, we present a simple method for training EBMs at scale which uses an entropy-regularized generator to amortize the MCMC sampling typically used in EBM training. We improve upon prior MCMC-based entropy regularization methods with a fast variational approximation. We demonstrate the effectiveness of our approach by using it to train tractable likelihood models. Next, we apply our estimator to the recently proposed Joint Energy Model (JEM), where we match the original performance with faster and stable training. This allows us to extend JEM models to semi-supervised classification on tabular data from a variety of continuous domains.", "one-sentence_summary": "We present a new generator-based approach for training EBMs and demonstrate that it trains models which obtain high likelihood and overcomes stability issues common in EBM training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grathwohl|no_mcmc_for_me_amortized_sampling_for_fast_and_stable_training_of_energybased_models", "supplementary_material": "", "pdf": "/pdf/4bf06b7c43a66250e22fbdad3d0f492a3293efc6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngrathwohl2021no,\ntitle={No {\\{}MCMC{\\}} for me: Amortized sampling for fast and stable training of energy-based models},\nauthor={Will Sussman Grathwohl and Jacob Jin Kelly and Milad Hashemi and Mohammad Norouzi and Kevin Swersky and David Duvenaud},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ixpSxO9flk3}\n}"}, "tags": [], "invitation": {"reply": {"forum": "ixpSxO9flk3", "readers": {"values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2021/Conference/Paper2046/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2046/Authors|ICLR.cc/2021/Conference/Paper2046/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs"}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}}, "multiReply": true, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["everyone"], "tcdate": 1610649467180, "tmdate": 1610649467180, "id": "ICLR.cc/2021/Conference/Paper2046/-/Comment"}}}, {"id": "ixpSxO9flk3", "original": "u4sqm2XBaEp", "number": 2046, "cdate": 1601308225385, "ddate": null, "tcdate": 1601308225385, "tmdate": 1615584060650, "tddate": null, "forum": "ixpSxO9flk3", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "No MCMC for me: Amortized sampling for fast and stable training of energy-based models", "authorids": ["~Will_Sussman_Grathwohl2", "jacob.jin.kelly@gmail.com", "~Milad_Hashemi1", "~Mohammad_Norouzi1", "~Kevin_Swersky1", "~David_Duvenaud2"], "authors": ["Will Sussman Grathwohl", "Jacob Jin Kelly", "Milad Hashemi", "Mohammad Norouzi", "Kevin Swersky", "David Duvenaud"], "keywords": ["Generative Models", "EBM", "Energy-Based Models", "Energy Based Models", "semi-supervised learning", "JEM"], "abstract": "Energy-Based Models (EBMs) present a flexible and appealing way to represent uncertainty. Despite recent advances, training EBMs on high-dimensional data remains a challenging problem as the state-of-the-art approaches are costly, unstable, and require considerable tuning and domain expertise to apply successfully. In this work, we present a simple method for training EBMs at scale which uses an entropy-regularized generator to amortize the MCMC sampling typically used in EBM training. We improve upon prior MCMC-based entropy regularization methods with a fast variational approximation. We demonstrate the effectiveness of our approach by using it to train tractable likelihood models. Next, we apply our estimator to the recently proposed Joint Energy Model (JEM), where we match the original performance with faster and stable training. This allows us to extend JEM models to semi-supervised classification on tabular data from a variety of continuous domains.", "one-sentence_summary": "We present a new generator-based approach for training EBMs and demonstrate that it trains models which obtain high likelihood and overcomes stability issues common in EBM training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grathwohl|no_mcmc_for_me_amortized_sampling_for_fast_and_stable_training_of_energybased_models", "supplementary_material": "", "pdf": "/pdf/4bf06b7c43a66250e22fbdad3d0f492a3293efc6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngrathwohl2021no,\ntitle={No {\\{}MCMC{\\}} for me: Amortized sampling for fast and stable training of energy-based models},\nauthor={Will Sussman Grathwohl and Jacob Jin Kelly and Milad Hashemi and Mohammad Norouzi and Kevin Swersky and David Duvenaud},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ixpSxO9flk3}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 34, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "WV-m_E5sPxv", "original": null, "number": 1, "cdate": 1615489767134, "ddate": null, "tcdate": 1615489767134, "tmdate": 1615489767134, "tddate": null, "forum": "ixpSxO9flk3", "replyto": "C4VuDibwoD", "invitation": "ICLR.cc/2021/Conference/Paper2046/-/Comment", "content": {"title": "CoopNets was a dupe too", "comment": "FYI, we had a paper at ICLR 2017 which also optimized this objective. It was called \"Calibrating Energy-Based Generative Adversarial Networks\". https://arxiv.org/abs/1702.01691"}, "signatures": ["~Philip_Bachman1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Philip_Bachman1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "No MCMC for me: Amortized sampling for fast and stable training of energy-based models", "authorids": ["~Will_Sussman_Grathwohl2", "jacob.jin.kelly@gmail.com", "~Milad_Hashemi1", "~Mohammad_Norouzi1", "~Kevin_Swersky1", "~David_Duvenaud2"], "authors": ["Will Sussman Grathwohl", "Jacob Jin Kelly", "Milad Hashemi", "Mohammad Norouzi", "Kevin Swersky", "David Duvenaud"], "keywords": ["Generative Models", "EBM", "Energy-Based Models", "Energy Based Models", "semi-supervised learning", "JEM"], "abstract": "Energy-Based Models (EBMs) present a flexible and appealing way to represent uncertainty. Despite recent advances, training EBMs on high-dimensional data remains a challenging problem as the state-of-the-art approaches are costly, unstable, and require considerable tuning and domain expertise to apply successfully. In this work, we present a simple method for training EBMs at scale which uses an entropy-regularized generator to amortize the MCMC sampling typically used in EBM training. We improve upon prior MCMC-based entropy regularization methods with a fast variational approximation. We demonstrate the effectiveness of our approach by using it to train tractable likelihood models. Next, we apply our estimator to the recently proposed Joint Energy Model (JEM), where we match the original performance with faster and stable training. This allows us to extend JEM models to semi-supervised classification on tabular data from a variety of continuous domains.", "one-sentence_summary": "We present a new generator-based approach for training EBMs and demonstrate that it trains models which obtain high likelihood and overcomes stability issues common in EBM training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grathwohl|no_mcmc_for_me_amortized_sampling_for_fast_and_stable_training_of_energybased_models", "supplementary_material": "", "pdf": "/pdf/4bf06b7c43a66250e22fbdad3d0f492a3293efc6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngrathwohl2021no,\ntitle={No {\\{}MCMC{\\}} for me: Amortized sampling for fast and stable training of energy-based models},\nauthor={Will Sussman Grathwohl and Jacob Jin Kelly and Milad Hashemi and Mohammad Norouzi and Kevin Swersky and David Duvenaud},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ixpSxO9flk3}\n}"}, "tags": [], "invitation": {"reply": {"forum": "ixpSxO9flk3", "readers": {"values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2021/Conference/Paper2046/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2046/Authors|ICLR.cc/2021/Conference/Paper2046/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs"}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}}, "multiReply": true, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["everyone"], "tcdate": 1610649467180, "tmdate": 1610649467180, "id": "ICLR.cc/2021/Conference/Paper2046/-/Comment"}}}, {"id": "BtLBCj-NE", "original": null, "number": 1, "cdate": 1610040504061, "ddate": null, "tcdate": 1610040504061, "tmdate": 1610474111158, "tddate": null, "forum": "ixpSxO9flk3", "replyto": "ixpSxO9flk3", "invitation": "ICLR.cc/2021/Conference/Paper2046/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "The authors proposed to train an energy based model with a hierachical\nvariational approximations. The entropy can be tricky in hierarchical\nvariational approximations.  The authors suggest using the auxillary\nsamples to guide an importance samples to compute the gradient of the\nentropy. They evaluate their approach on a slew of models. The idea is\nstraightfoward and could potentially be applied to other hierarchical\nvariational models out side of the energy-based model setting.  The\nauthors were responsive and clarified many agressive questions. I'd\nask the authors to clean up two things\n\n- Equation 8 would be easier to follow if it kept the expectation from\n  equation 6 thereby making z_0 feel like it materialize out of thin\n  air\n\n\n- A more detailed discusion of when the proposal is good and what could\n  be missed out\twhen relying on the generating z to center the proposal"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "No MCMC for me: Amortized sampling for fast and stable training of energy-based models", "authorids": ["~Will_Sussman_Grathwohl2", "jacob.jin.kelly@gmail.com", "~Milad_Hashemi1", "~Mohammad_Norouzi1", "~Kevin_Swersky1", "~David_Duvenaud2"], "authors": ["Will Sussman Grathwohl", "Jacob Jin Kelly", "Milad Hashemi", "Mohammad Norouzi", "Kevin Swersky", "David Duvenaud"], "keywords": ["Generative Models", "EBM", "Energy-Based Models", "Energy Based Models", "semi-supervised learning", "JEM"], "abstract": "Energy-Based Models (EBMs) present a flexible and appealing way to represent uncertainty. Despite recent advances, training EBMs on high-dimensional data remains a challenging problem as the state-of-the-art approaches are costly, unstable, and require considerable tuning and domain expertise to apply successfully. In this work, we present a simple method for training EBMs at scale which uses an entropy-regularized generator to amortize the MCMC sampling typically used in EBM training. We improve upon prior MCMC-based entropy regularization methods with a fast variational approximation. We demonstrate the effectiveness of our approach by using it to train tractable likelihood models. Next, we apply our estimator to the recently proposed Joint Energy Model (JEM), where we match the original performance with faster and stable training. This allows us to extend JEM models to semi-supervised classification on tabular data from a variety of continuous domains.", "one-sentence_summary": "We present a new generator-based approach for training EBMs and demonstrate that it trains models which obtain high likelihood and overcomes stability issues common in EBM training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grathwohl|no_mcmc_for_me_amortized_sampling_for_fast_and_stable_training_of_energybased_models", "supplementary_material": "", "pdf": "/pdf/4bf06b7c43a66250e22fbdad3d0f492a3293efc6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngrathwohl2021no,\ntitle={No {\\{}MCMC{\\}} for me: Amortized sampling for fast and stable training of energy-based models},\nauthor={Will Sussman Grathwohl and Jacob Jin Kelly and Milad Hashemi and Mohammad Norouzi and Kevin Swersky and David Duvenaud},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ixpSxO9flk3}\n}"}, "tags": [], "invitation": {"reply": {"forum": "ixpSxO9flk3", "replyto": "ixpSxO9flk3", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040504048, "tmdate": 1610474111143, "id": "ICLR.cc/2021/Conference/Paper2046/-/Decision"}}}, {"id": "KJEb6ZD1cHv", "original": null, "number": 1, "cdate": 1603683065907, "ddate": null, "tcdate": 1603683065907, "tmdate": 1606790935165, "tddate": null, "forum": "ixpSxO9flk3", "replyto": "ixpSxO9flk3", "invitation": "ICLR.cc/2021/Conference/Paper2046/-/Official_Review", "content": {"title": "An improved algorithm to train EBM-based models", "review": "This paper proposes an improved algorithm to train EBM-based models, called Variational Entropy Regularized Approximate maximum likelihood. The basic idea is to formulate the intractable partition function as an optimization problem with an additional entropy term. To estimate the gradient of the entropy term, the authors then propose a variational formulation for approximation.\n\nThe idea is interesting but not very exciting, and there consists of technical details that need to be carefully deal with. One of my biggest concerns is that since the algorithm relies on importance sampling to estimate the gradient of log marginal likelihood, the variance can be arbitrarily large. Specifically, the importance weight is the ration between a joint distribution and a proposed variational distribution \\xi. One can see that a sample from \\xi is a zero-mean Gaussian distribution, whereas q(z, x) should not. I suspect this would make the ratios to be vary unstable with high variance, which seems to contradict with the goal of the paper. In other words, I am not convinced why the importance sampling based method can work better than existing methods.\n\nIn addition, this importance sampling based method is introduced to avoid sampling the posterior distribution with HMC. I think this step can be considered as an efficient approximation to the  method, I am expecting HMC should perform at least as good as the proposed method but less efficient. The results in Table 2 seems to agree with this (although not completely). I wonder how is the running time comparison? Also, since HMC based method is a competitive method, why don't you consider this in other experiments such as those in Section 6?\n\nAlso, since the proposed method is claimed to outperform the recent SSM method. I think the same experiments as in the SSM paper should be conducted for comparison.\n\nBTW, the generative images in Appendix are too small to be informative.\n\n=========After rebuttal======\nAfter the rebuttal, my main concern remains. Specifically, the paper defines a variational distribution q(z|x0 via a hierarchical construction: z_0 ~ N(0, 1), z ~ N(z_0, \\eta I), which is essentially a zero-mean Gaussian. And I suspect the this is a bad variational distribution and it will induce high variance. The author said they didn't the hierarchical construction to define the variational distribution, because they fix z_0 after sampling. I don't think this is a formal way of defining a variational distribution. One reason is that even if they fix z_0, the proposal distribution will be a z_0-mean Gaussian, and the mean is randomly drawn from N(0, 1), which will not match the true posterior distribution (they only optimize the variance parameter). I think this should be make clear and investigated in more details. I will keep my initial score.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2046/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "No MCMC for me: Amortized sampling for fast and stable training of energy-based models", "authorids": ["~Will_Sussman_Grathwohl2", "jacob.jin.kelly@gmail.com", "~Milad_Hashemi1", "~Mohammad_Norouzi1", "~Kevin_Swersky1", "~David_Duvenaud2"], "authors": ["Will Sussman Grathwohl", "Jacob Jin Kelly", "Milad Hashemi", "Mohammad Norouzi", "Kevin Swersky", "David Duvenaud"], "keywords": ["Generative Models", "EBM", "Energy-Based Models", "Energy Based Models", "semi-supervised learning", "JEM"], "abstract": "Energy-Based Models (EBMs) present a flexible and appealing way to represent uncertainty. Despite recent advances, training EBMs on high-dimensional data remains a challenging problem as the state-of-the-art approaches are costly, unstable, and require considerable tuning and domain expertise to apply successfully. In this work, we present a simple method for training EBMs at scale which uses an entropy-regularized generator to amortize the MCMC sampling typically used in EBM training. We improve upon prior MCMC-based entropy regularization methods with a fast variational approximation. We demonstrate the effectiveness of our approach by using it to train tractable likelihood models. Next, we apply our estimator to the recently proposed Joint Energy Model (JEM), where we match the original performance with faster and stable training. This allows us to extend JEM models to semi-supervised classification on tabular data from a variety of continuous domains.", "one-sentence_summary": "We present a new generator-based approach for training EBMs and demonstrate that it trains models which obtain high likelihood and overcomes stability issues common in EBM training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grathwohl|no_mcmc_for_me_amortized_sampling_for_fast_and_stable_training_of_energybased_models", "supplementary_material": "", "pdf": "/pdf/4bf06b7c43a66250e22fbdad3d0f492a3293efc6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngrathwohl2021no,\ntitle={No {\\{}MCMC{\\}} for me: Amortized sampling for fast and stable training of energy-based models},\nauthor={Will Sussman Grathwohl and Jacob Jin Kelly and Milad Hashemi and Mohammad Norouzi and Kevin Swersky and David Duvenaud},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ixpSxO9flk3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ixpSxO9flk3", "replyto": "ixpSxO9flk3", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2046/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538105264, "tmdate": 1606915801246, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2046/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2046/-/Official_Review"}}}, {"id": "d5eBVKdgkuH", "original": null, "number": 3, "cdate": 1603943537699, "ddate": null, "tcdate": 1603943537699, "tmdate": 1606784097573, "tddate": null, "forum": "ixpSxO9flk3", "replyto": "ixpSxO9flk3", "invitation": "ICLR.cc/2021/Conference/Paper2046/-/Official_Review", "content": {"title": "An overall nice idea, but can be stronger with more comparison/dicussion with previous work.", "review": "This paper proposes a new method on training energy-based models with maximum likelihood. Instead of using MCMC approaches to sample from the EBM, authors follow previous work on training neural generators for faster sample generation. In particular, authors consider a special generator where the output is convolved with Gaussian noise. The score function of this generator can be estimated with self-normalized importance sampling, which is then used to estimate the entropy term through the reparameterization trick. Authors demonstrate that their new method is able to train EBMs efficiently, and improves the stability and performance of JEMs compared to MCMC-based training approaches.\n\n#### Pros\n* The method is more computationally efficient compared to MCMC-based approaches. As the title suggests, no iterative MCMC approaches are needed. Though the technique is inherently similar to Dieng et al. (2019), authors replaces the HMC sub-routine with a carefully designed variational approximation.\n\n* Experiments on JEMs are particularly interesting. Authors demonstrate that their method can train JEMs in a stable way and outperforms baselines on classification accuracy, sample quality, out-of-distribution detection and semi-supervised learning.\n\n* Writing is clear and easy to follow.\n\n#### Cons\n* From my perspective, the biggest disadvantage is related to various additional hyper-parameters. In VERA training, $\\gamma$ controls the gradient penalty and $\\lambda$ controls the contribution of the estimated entropy gradient. Both requires considerable tuning for optimal performance. However, typical MCMC-based approaches do not require gradient penalty, and tuning $\\lambda$ is unsatisfying \u2014 shouldn't $\\lambda \\equiv 1$ for real maximum likelihood training? \n\n* The idea of training a neural generator with the dual form of the likelihood objective has been explored before. Authors have compared with MEG, which uses a similar objective. I think adversarial dynamics embedding is also a necessary baseline, since it uses the same objective and has a special design of the neural generator to make entropy computing tractable. It would be better if authors will include it in both NICE and JEM experiments.\n\n* Authors cited (Song & Ermon, 2019a) when pointing out the difficulty of MI estimation. The reference below should also be included as it actually appeared earlier in 2018.\n\nMcAllester, David, and Karl Stratos. \"Formal limitations on the measurement of mutual information.\" International Conference on Artificial Intelligence and Statistics. 2020.\n\n* Authors did not include methods such as MEG in the JEM experiments. Any reason?\n\n----------------\nPost-rebuttal\n\nI appreciate the authors' response and additional comparison against previous work. I do think that proper comparison with previous work is important, as it allows us to know better when and where the proposed approach is beneficial. \n\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2046/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "No MCMC for me: Amortized sampling for fast and stable training of energy-based models", "authorids": ["~Will_Sussman_Grathwohl2", "jacob.jin.kelly@gmail.com", "~Milad_Hashemi1", "~Mohammad_Norouzi1", "~Kevin_Swersky1", "~David_Duvenaud2"], "authors": ["Will Sussman Grathwohl", "Jacob Jin Kelly", "Milad Hashemi", "Mohammad Norouzi", "Kevin Swersky", "David Duvenaud"], "keywords": ["Generative Models", "EBM", "Energy-Based Models", "Energy Based Models", "semi-supervised learning", "JEM"], "abstract": "Energy-Based Models (EBMs) present a flexible and appealing way to represent uncertainty. Despite recent advances, training EBMs on high-dimensional data remains a challenging problem as the state-of-the-art approaches are costly, unstable, and require considerable tuning and domain expertise to apply successfully. In this work, we present a simple method for training EBMs at scale which uses an entropy-regularized generator to amortize the MCMC sampling typically used in EBM training. We improve upon prior MCMC-based entropy regularization methods with a fast variational approximation. We demonstrate the effectiveness of our approach by using it to train tractable likelihood models. Next, we apply our estimator to the recently proposed Joint Energy Model (JEM), where we match the original performance with faster and stable training. This allows us to extend JEM models to semi-supervised classification on tabular data from a variety of continuous domains.", "one-sentence_summary": "We present a new generator-based approach for training EBMs and demonstrate that it trains models which obtain high likelihood and overcomes stability issues common in EBM training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grathwohl|no_mcmc_for_me_amortized_sampling_for_fast_and_stable_training_of_energybased_models", "supplementary_material": "", "pdf": "/pdf/4bf06b7c43a66250e22fbdad3d0f492a3293efc6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngrathwohl2021no,\ntitle={No {\\{}MCMC{\\}} for me: Amortized sampling for fast and stable training of energy-based models},\nauthor={Will Sussman Grathwohl and Jacob Jin Kelly and Milad Hashemi and Mohammad Norouzi and Kevin Swersky and David Duvenaud},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ixpSxO9flk3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ixpSxO9flk3", "replyto": "ixpSxO9flk3", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2046/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538105264, "tmdate": 1606915801246, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2046/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2046/-/Official_Review"}}}, {"id": "sXFEfnVd9-v", "original": null, "number": 30, "cdate": 1606281641855, "ddate": null, "tcdate": 1606281641855, "tmdate": 1606281641855, "tddate": null, "forum": "ixpSxO9flk3", "replyto": "TLkjVmO7rXD", "invitation": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment", "content": {"title": "...", "comment": "To our proposal distribution, $z_0$ is not a random variable. When we generate a sample $x$ there is exactly 1 $z_0$ which generates this $x$. There is a distribution of possible $z_0$ values which could create this $x$ but when we sample $x$ once, there is only one $z_0$. We are free to use this $z_0$ to condition our proposal distribution. This is described underneath eq. 8 in Section 4.2.\n\nThe proposal distribution is not a hierarchical process. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2046/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "No MCMC for me: Amortized sampling for fast and stable training of energy-based models", "authorids": ["~Will_Sussman_Grathwohl2", "jacob.jin.kelly@gmail.com", "~Milad_Hashemi1", "~Mohammad_Norouzi1", "~Kevin_Swersky1", "~David_Duvenaud2"], "authors": ["Will Sussman Grathwohl", "Jacob Jin Kelly", "Milad Hashemi", "Mohammad Norouzi", "Kevin Swersky", "David Duvenaud"], "keywords": ["Generative Models", "EBM", "Energy-Based Models", "Energy Based Models", "semi-supervised learning", "JEM"], "abstract": "Energy-Based Models (EBMs) present a flexible and appealing way to represent uncertainty. Despite recent advances, training EBMs on high-dimensional data remains a challenging problem as the state-of-the-art approaches are costly, unstable, and require considerable tuning and domain expertise to apply successfully. In this work, we present a simple method for training EBMs at scale which uses an entropy-regularized generator to amortize the MCMC sampling typically used in EBM training. We improve upon prior MCMC-based entropy regularization methods with a fast variational approximation. We demonstrate the effectiveness of our approach by using it to train tractable likelihood models. Next, we apply our estimator to the recently proposed Joint Energy Model (JEM), where we match the original performance with faster and stable training. This allows us to extend JEM models to semi-supervised classification on tabular data from a variety of continuous domains.", "one-sentence_summary": "We present a new generator-based approach for training EBMs and demonstrate that it trains models which obtain high likelihood and overcomes stability issues common in EBM training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grathwohl|no_mcmc_for_me_amortized_sampling_for_fast_and_stable_training_of_energybased_models", "supplementary_material": "", "pdf": "/pdf/4bf06b7c43a66250e22fbdad3d0f492a3293efc6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngrathwohl2021no,\ntitle={No {\\{}MCMC{\\}} for me: Amortized sampling for fast and stable training of energy-based models},\nauthor={Will Sussman Grathwohl and Jacob Jin Kelly and Milad Hashemi and Mohammad Norouzi and Kevin Swersky and David Duvenaud},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ixpSxO9flk3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ixpSxO9flk3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2046/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2046/Authors|ICLR.cc/2021/Conference/Paper2046/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852884, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment"}}}, {"id": "TLkjVmO7rXD", "original": null, "number": 29, "cdate": 1606280649359, "ddate": null, "tcdate": 1606280649359, "tmdate": 1606280649359, "tddate": null, "forum": "ixpSxO9flk3", "replyto": "W9hqQNeNfEH", "invitation": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment", "content": {"title": "Not correct", "comment": "I am afraid I cannot agree with the response. What you define for the proposal distribution is a hierarchical generative process. So z_0 is a random variable, saying that *we use the exact value* is not strictly correct. At the end, when talking about the proposal distribution, you have to integrate out all other random variables, which is the marginal distribution of z, and is a zero-mean gaussian. You cannot consider the conditional distribution as the proposal distribution.\n\nI also understand what you try to say in the response, and I think the current writing of the paper does not correspond to what you describe in the response. I think there are other ways to describe this but not sure how."}, "signatures": ["ICLR.cc/2021/Conference/Paper2046/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "No MCMC for me: Amortized sampling for fast and stable training of energy-based models", "authorids": ["~Will_Sussman_Grathwohl2", "jacob.jin.kelly@gmail.com", "~Milad_Hashemi1", "~Mohammad_Norouzi1", "~Kevin_Swersky1", "~David_Duvenaud2"], "authors": ["Will Sussman Grathwohl", "Jacob Jin Kelly", "Milad Hashemi", "Mohammad Norouzi", "Kevin Swersky", "David Duvenaud"], "keywords": ["Generative Models", "EBM", "Energy-Based Models", "Energy Based Models", "semi-supervised learning", "JEM"], "abstract": "Energy-Based Models (EBMs) present a flexible and appealing way to represent uncertainty. Despite recent advances, training EBMs on high-dimensional data remains a challenging problem as the state-of-the-art approaches are costly, unstable, and require considerable tuning and domain expertise to apply successfully. In this work, we present a simple method for training EBMs at scale which uses an entropy-regularized generator to amortize the MCMC sampling typically used in EBM training. We improve upon prior MCMC-based entropy regularization methods with a fast variational approximation. We demonstrate the effectiveness of our approach by using it to train tractable likelihood models. Next, we apply our estimator to the recently proposed Joint Energy Model (JEM), where we match the original performance with faster and stable training. This allows us to extend JEM models to semi-supervised classification on tabular data from a variety of continuous domains.", "one-sentence_summary": "We present a new generator-based approach for training EBMs and demonstrate that it trains models which obtain high likelihood and overcomes stability issues common in EBM training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grathwohl|no_mcmc_for_me_amortized_sampling_for_fast_and_stable_training_of_energybased_models", "supplementary_material": "", "pdf": "/pdf/4bf06b7c43a66250e22fbdad3d0f492a3293efc6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngrathwohl2021no,\ntitle={No {\\{}MCMC{\\}} for me: Amortized sampling for fast and stable training of energy-based models},\nauthor={Will Sussman Grathwohl and Jacob Jin Kelly and Milad Hashemi and Mohammad Norouzi and Kevin Swersky and David Duvenaud},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ixpSxO9flk3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ixpSxO9flk3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2046/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2046/Authors|ICLR.cc/2021/Conference/Paper2046/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852884, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment"}}}, {"id": "W9hqQNeNfEH", "original": null, "number": 28, "cdate": 1606279742375, "ddate": null, "tcdate": 1606279742375, "tmdate": 1606279951835, "tddate": null, "forum": "ixpSxO9flk3", "replyto": "LsmWWBSAoH9", "invitation": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment", "content": {"title": "Clarification", "comment": "Reviewer,\n\nWe believe that you misunderstand how our proposal distribution works. To generate a sample $x$ we first sample $z_0 \\sim N(0, I)$ and then generate $x \\sim N(g(z_0), \\sigma  I)$. We look for a proposal distribution to approximate the true posterior of $p(z|x)$. Given that we know the true $z_0$ which generated $x$, we can use this to create a more informed proposal distribution. Since $\\sigma$ is small, the space of possible $z$ values that could likely generate $x$ should concentrate near $z_0$. This indicates our knowledge of $z_0$ should be useful for building a posterior approximation.\n\nWe only use this posterior approximation to regularize the generator's entropy. For this task, we only need to create a posterior approximation on samples generated from our model. This means that we have access to the exact $z_0$ which generated each sample we see. If we were trying to build a posterior approximation for data samples that we did not generate, then our approach would not work since we do not have access to the $z_0$ which generated $x$. Thankfully, we are only interested in building posterior approximation for samples that we generate. \n\nTo reiterate, our posterior approximation is not a zero-mean Gaussian distribution. We re-use the exact $z_0$ which generated the $x$ that we see. We know this $z_0$ since we generated this $x$. We do not integrate out $z_0$, we generate it while sampling $x$ and then re-use the same value explicitly in our posterior approximation.\n\nWe think this is the main point you are misunderstanding. If we were not to re-use the $z_0$ which generated $x$ and simply re-sampled it, then the proposal would be a zero-mean Gaussian as you say but that is not what we are doing. \n\nTo address your other point about variance: \n\nJust because our estimator has larger variance than HMC does not mean it is of lower quality. As you can see in the plot above the one you mention, the estimator has notably lower bias. Variance can be dealt with (within reason) by increasing batch size but bias cannot. \n\nIf it is possible, please respond before the discussion period is over. We will do our best to respond to you before the discussion period ends although it is late where we are. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2046/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "No MCMC for me: Amortized sampling for fast and stable training of energy-based models", "authorids": ["~Will_Sussman_Grathwohl2", "jacob.jin.kelly@gmail.com", "~Milad_Hashemi1", "~Mohammad_Norouzi1", "~Kevin_Swersky1", "~David_Duvenaud2"], "authors": ["Will Sussman Grathwohl", "Jacob Jin Kelly", "Milad Hashemi", "Mohammad Norouzi", "Kevin Swersky", "David Duvenaud"], "keywords": ["Generative Models", "EBM", "Energy-Based Models", "Energy Based Models", "semi-supervised learning", "JEM"], "abstract": "Energy-Based Models (EBMs) present a flexible and appealing way to represent uncertainty. Despite recent advances, training EBMs on high-dimensional data remains a challenging problem as the state-of-the-art approaches are costly, unstable, and require considerable tuning and domain expertise to apply successfully. In this work, we present a simple method for training EBMs at scale which uses an entropy-regularized generator to amortize the MCMC sampling typically used in EBM training. We improve upon prior MCMC-based entropy regularization methods with a fast variational approximation. We demonstrate the effectiveness of our approach by using it to train tractable likelihood models. Next, we apply our estimator to the recently proposed Joint Energy Model (JEM), where we match the original performance with faster and stable training. This allows us to extend JEM models to semi-supervised classification on tabular data from a variety of continuous domains.", "one-sentence_summary": "We present a new generator-based approach for training EBMs and demonstrate that it trains models which obtain high likelihood and overcomes stability issues common in EBM training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grathwohl|no_mcmc_for_me_amortized_sampling_for_fast_and_stable_training_of_energybased_models", "supplementary_material": "", "pdf": "/pdf/4bf06b7c43a66250e22fbdad3d0f492a3293efc6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngrathwohl2021no,\ntitle={No {\\{}MCMC{\\}} for me: Amortized sampling for fast and stable training of energy-based models},\nauthor={Will Sussman Grathwohl and Jacob Jin Kelly and Milad Hashemi and Mohammad Norouzi and Kevin Swersky and David Duvenaud},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ixpSxO9flk3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ixpSxO9flk3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2046/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2046/Authors|ICLR.cc/2021/Conference/Paper2046/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852884, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment"}}}, {"id": "LsmWWBSAoH9", "original": null, "number": 26, "cdate": 1606273275817, "ddate": null, "tcdate": 1606273275817, "tmdate": 1606273275817, "tddate": null, "forum": "ixpSxO9flk3", "replyto": "Bcm_1YymGY2", "invitation": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment", "content": {"title": "Concern remains", "comment": "Thanks for the detailed rebuttal. However, after reading the rebuttal, my concern on the proposal distribution being zero-mean gaussian remains.\n\nSpecifically, you argue that the proposal distribution for q(z|x) is constructed as: z_0 ~ N(0, 1), z ~ N(z_0, \\eta I). And you argue that the proposal distribution is not zero-mean gaussian. I don't get this, because after integrating out z_0, the proposal distribution is still a zero-mean gaussian. And you propose to optimize the parameter \\eta, but that only affects the variance of the proposal distribution. As a result, this zero-mean Gaussian is likely to induce high variance of the proposed method. I think this is also verified by your additional experiments with HMC, where you show that your method has higher variance than HMC.\n\nGiven this, I think this is a critical issue that does not convince me to change my rating."}, "signatures": ["ICLR.cc/2021/Conference/Paper2046/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "No MCMC for me: Amortized sampling for fast and stable training of energy-based models", "authorids": ["~Will_Sussman_Grathwohl2", "jacob.jin.kelly@gmail.com", "~Milad_Hashemi1", "~Mohammad_Norouzi1", "~Kevin_Swersky1", "~David_Duvenaud2"], "authors": ["Will Sussman Grathwohl", "Jacob Jin Kelly", "Milad Hashemi", "Mohammad Norouzi", "Kevin Swersky", "David Duvenaud"], "keywords": ["Generative Models", "EBM", "Energy-Based Models", "Energy Based Models", "semi-supervised learning", "JEM"], "abstract": "Energy-Based Models (EBMs) present a flexible and appealing way to represent uncertainty. Despite recent advances, training EBMs on high-dimensional data remains a challenging problem as the state-of-the-art approaches are costly, unstable, and require considerable tuning and domain expertise to apply successfully. In this work, we present a simple method for training EBMs at scale which uses an entropy-regularized generator to amortize the MCMC sampling typically used in EBM training. We improve upon prior MCMC-based entropy regularization methods with a fast variational approximation. We demonstrate the effectiveness of our approach by using it to train tractable likelihood models. Next, we apply our estimator to the recently proposed Joint Energy Model (JEM), where we match the original performance with faster and stable training. This allows us to extend JEM models to semi-supervised classification on tabular data from a variety of continuous domains.", "one-sentence_summary": "We present a new generator-based approach for training EBMs and demonstrate that it trains models which obtain high likelihood and overcomes stability issues common in EBM training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grathwohl|no_mcmc_for_me_amortized_sampling_for_fast_and_stable_training_of_energybased_models", "supplementary_material": "", "pdf": "/pdf/4bf06b7c43a66250e22fbdad3d0f492a3293efc6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngrathwohl2021no,\ntitle={No {\\{}MCMC{\\}} for me: Amortized sampling for fast and stable training of energy-based models},\nauthor={Will Sussman Grathwohl and Jacob Jin Kelly and Milad Hashemi and Mohammad Norouzi and Kevin Swersky and David Duvenaud},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ixpSxO9flk3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ixpSxO9flk3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2046/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2046/Authors|ICLR.cc/2021/Conference/Paper2046/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852884, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment"}}}, {"id": "IDPXKRxmg9x", "original": null, "number": 2, "cdate": 1603841878896, "ddate": null, "tcdate": 1603841878896, "tmdate": 1606089786613, "tddate": null, "forum": "ixpSxO9flk3", "replyto": "ixpSxO9flk3", "invitation": "ICLR.cc/2021/Conference/Paper2046/-/Official_Review", "content": {"title": "An extension of previous works, but demonstrated well", "review": "**Summary**: This paper presents a method for improving training of energy-based models. Rather than drawing samples using persistent contrastive divergence / MCMC, this approach parameterizes a separate model, which is trained to directly output samples. This effectively adds an additional KL divergence to the objective. The authors use a particular form of sampling model (a latent Gaussian model), borrowing a few tricks for getting entropy estimates out of the model. Results are demonstrated on a few qualitative setups, but most of the results are centered on improved JEM on sample quality, out-of-distribution detection, and semi-supervised learning. The main benefit of the approach seems to be speed and stability, however, the authors also claim that minimal tuning is needed.\n\n**Strong Points**: Overall, the paper is fairly clear in its description of the background, method and experiments. Likewise, the tables, figures, and algorithm box provide clear demonstrations of key aspects of the approach. The descriptions in Eq. 6 are also useful for unfamiliar readers.\n\nFrom my understanding, the technical aspects of the paper appear to be largely correct. Like previous papers, the authors use an objective that includes both the energy function and the approximate sampler. This includes the entropy of the sampler (from the sampler\u2019s KL). The authors opt for a Gaussian latent variable model as the sampler. Evaluating the entropy thus entails an integral over the posterior of this model. The key technical contribution of this paper is to approximate this integral not with MCMC sampling, but instead use an importance-weighted variational approximation. This involves drawing a sample from the prior, then sampling points from a Gaussian around this point.\n\nThe experimental results in the paper are highly thorough. The authors demonstrate their method in settings where exact log-likelihood estimates are feasible (a flow-based model and probabilistic PCA). This allows the authors to demonstrate both improved samples and log-likelihood performance. In these cases, the authors compare with several relevant baselines. In the main experiments of the paper, the authors incorporate their approximate sampling scheme into JEM, a recent hybrid discriminative/generative model. Experimental results demonstrate that using the approximate sampler generally results in (slight) improvements over JEM, with the exception of semi-supervised classification, where the proposed method is substantially better. In all cases, experiments are performed using standard, benchmark datasets, and the authors compare with competitive, recent methods.\n\nThere are substantial details on the experiments in the appendix. This, combined with the experiments across multiple datasets and settings, suggests that these results are reproducible. However, given the fact that this methods requires training multiple models, exactly reproducing the results will likely require the corresponding code (which the authors did not provide).\n\nWhile the method itself is not highly novel, the thorough experimental results somewhat compensate for this aspect. In particular, at least in the \u201ctoy\u201d experiments, the authors compare with MEG, a relevant, related baseline. This demonstrates that, although the proposed method is a natural extension of previous works, the contribution is nevertheless useful.\n\n**Weak Points**: One weak point of this paper is with regard to novelty. The main contribution of this paper is in estimating the integral of the sampling model\u2019s entropy with an importance-weighted variational approximation. This is a fairly natural extension of previous works on learning sampling models, simply making the estimation procedure more efficient. On its own, this is not a highly substantial contribution. It\u2019s also not clear that this is the best set of design choices for this problem. For instance, would the issues with the entropy gradient not be obviated with an exact log-likelihood model, such as a flow-based model / autoregressive model?\n\nSome of the theoretical considerations of the setup appear to be skimmed over. The authors present the approximate sampling method with an equality in Eq. 4. However, in practice, the sampling model will not be able to reach the max, due to inherent limitations in the model class. Further, in Algorithm 1, the energy-based model and the sampling model are trained jointly, rather than training the sampling model inside of an inner loop (for the max operation). This is analogous to amortized inference in VAEs, where training the encoder and decoder jointly typically implies that the encoder cannot fully maximize the ELBO for the decoder. This phenomenon is referred to as \u201clagging inference networks,\u201d (He et al., 2019) and the performance gap is the \u201camortization gap\u201d (Cremer et al., 2018). Similarly, there will be a \u201csampling gap\u201d here due to a \u201clagging sampling model.\u201d While the method still seems to work well in practice, the fact that this is an approximation is almost entirely missing from the paper. Note that, to generate samples, the authors do indeed run additional MCMC steps (see appendix).\n\nFinally, although the experiments are comprehensive, it\u2019s not clear whether the improvements over JEM are highly substantial. In addition, in the case with the largest gap, semi-supervised learning, it\u2019s unclear why the proposed method, VERA, should dramatically outperform JEM. Given that this is the most substantial result, it seems as though further investigation should be performed to analyze the reason for the boost in performance. Further, the experiments on JEM only compare the baseline models (which uses MCMC) with the proposed (amortized) method. A more complete set of experiments would also include other learned sampling methods, such as MEG.\n\n**Accept / Reject**: Although the method is not highly novel, the experiments are fairly comprehensive in demonstrating the proposed method. Analyses are performed in multiple settings, across multiple datasets, comparing with the relevant baselines. At the very least, this paper demonstrates an efficient method that seems to outperform previous methods in simpler tasks and improve JEM on more difficult tasks. For these reasons, I would be in favor of accepting this paper. With a more detailed motivation/explanation of the design choices, discussion of the quality of the approximation, and some additional sampling model baselines on the JEM results, this paper could be further improved.\n\n**Questions**:\n\nCould you elaborate on the issue with the score function estimator in Eq. 6? Is this simply a matter of having a high-variance estimator?\n\nIn the importance weights, should $q_\\phi (x)$ be included in the denominator?\n\nIn Algorithm 1, are you using a mini-batch of data examples but only one sample from the model. If so, why not use a batch of samples?\n\nThe MNIST samples don\u2019t look great, even for VERA. Why is this the case?\n\nIs using NICE in Section 5.1 representative of energy-based models? Flow-based models tend to have their own issues associated with stability.\n\nWhy is VERA better at semi-supervised learning as compared with JEM?\n\n**Additional Feedback**:\n\nMethod:  \nI would not compare Eq. 5 with a GAN decoder, as GANs define an implicit density.  \n\u201cor simple a diagonal Gaussian\u201d \u2014> \u201cor simply a diagonal Gaussian\u201d\n\u201cto the gradient of our model\u2019s likelihood\u2026\u201d: should specify that this is the EBM.  \n\nEBM Training Experiments: \nI would put zeros in front of all decimals to be consistent.  \nNot sure if it\u2019s fair to conclude that entropy regularization is unnecessary for preventing mode collapse from this experiment. This seems like a limited setting.  \n\nSection A.2:  \n\u201cEquation equation 7\u201d \u2014> \u201cEquation 7\u201d\n\nFigure 7:  \n\u201cUnconditional CIFAR10 Samples\u201d \u2014> \u201cUnconditional MNIST Samples\u201d", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2046/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "No MCMC for me: Amortized sampling for fast and stable training of energy-based models", "authorids": ["~Will_Sussman_Grathwohl2", "jacob.jin.kelly@gmail.com", "~Milad_Hashemi1", "~Mohammad_Norouzi1", "~Kevin_Swersky1", "~David_Duvenaud2"], "authors": ["Will Sussman Grathwohl", "Jacob Jin Kelly", "Milad Hashemi", "Mohammad Norouzi", "Kevin Swersky", "David Duvenaud"], "keywords": ["Generative Models", "EBM", "Energy-Based Models", "Energy Based Models", "semi-supervised learning", "JEM"], "abstract": "Energy-Based Models (EBMs) present a flexible and appealing way to represent uncertainty. Despite recent advances, training EBMs on high-dimensional data remains a challenging problem as the state-of-the-art approaches are costly, unstable, and require considerable tuning and domain expertise to apply successfully. In this work, we present a simple method for training EBMs at scale which uses an entropy-regularized generator to amortize the MCMC sampling typically used in EBM training. We improve upon prior MCMC-based entropy regularization methods with a fast variational approximation. We demonstrate the effectiveness of our approach by using it to train tractable likelihood models. Next, we apply our estimator to the recently proposed Joint Energy Model (JEM), where we match the original performance with faster and stable training. This allows us to extend JEM models to semi-supervised classification on tabular data from a variety of continuous domains.", "one-sentence_summary": "We present a new generator-based approach for training EBMs and demonstrate that it trains models which obtain high likelihood and overcomes stability issues common in EBM training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grathwohl|no_mcmc_for_me_amortized_sampling_for_fast_and_stable_training_of_energybased_models", "supplementary_material": "", "pdf": "/pdf/4bf06b7c43a66250e22fbdad3d0f492a3293efc6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngrathwohl2021no,\ntitle={No {\\{}MCMC{\\}} for me: Amortized sampling for fast and stable training of energy-based models},\nauthor={Will Sussman Grathwohl and Jacob Jin Kelly and Milad Hashemi and Mohammad Norouzi and Kevin Swersky and David Duvenaud},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ixpSxO9flk3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ixpSxO9flk3", "replyto": "ixpSxO9flk3", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2046/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538105264, "tmdate": 1606915801246, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2046/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2046/-/Official_Review"}}}, {"id": "7qveH-NCGs", "original": null, "number": 21, "cdate": 1605626272799, "ddate": null, "tcdate": 1605626272799, "tmdate": 1605626272799, "tddate": null, "forum": "ixpSxO9flk3", "replyto": "d5eBVKdgkuH", "invitation": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment", "content": {"title": "Response to feedback", "comment": "Reviewer1,\n\nAgain we thank you for your time, thoughtful review, and defense of our work. We took your comments into consideration and we feel that we have greatly strengthened our paper. We hope that our response to your comments and the changes we have made in the paper have addressed your concerns. We look forward to hearing from you.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2046/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "No MCMC for me: Amortized sampling for fast and stable training of energy-based models", "authorids": ["~Will_Sussman_Grathwohl2", "jacob.jin.kelly@gmail.com", "~Milad_Hashemi1", "~Mohammad_Norouzi1", "~Kevin_Swersky1", "~David_Duvenaud2"], "authors": ["Will Sussman Grathwohl", "Jacob Jin Kelly", "Milad Hashemi", "Mohammad Norouzi", "Kevin Swersky", "David Duvenaud"], "keywords": ["Generative Models", "EBM", "Energy-Based Models", "Energy Based Models", "semi-supervised learning", "JEM"], "abstract": "Energy-Based Models (EBMs) present a flexible and appealing way to represent uncertainty. Despite recent advances, training EBMs on high-dimensional data remains a challenging problem as the state-of-the-art approaches are costly, unstable, and require considerable tuning and domain expertise to apply successfully. In this work, we present a simple method for training EBMs at scale which uses an entropy-regularized generator to amortize the MCMC sampling typically used in EBM training. We improve upon prior MCMC-based entropy regularization methods with a fast variational approximation. We demonstrate the effectiveness of our approach by using it to train tractable likelihood models. Next, we apply our estimator to the recently proposed Joint Energy Model (JEM), where we match the original performance with faster and stable training. This allows us to extend JEM models to semi-supervised classification on tabular data from a variety of continuous domains.", "one-sentence_summary": "We present a new generator-based approach for training EBMs and demonstrate that it trains models which obtain high likelihood and overcomes stability issues common in EBM training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grathwohl|no_mcmc_for_me_amortized_sampling_for_fast_and_stable_training_of_energybased_models", "supplementary_material": "", "pdf": "/pdf/4bf06b7c43a66250e22fbdad3d0f492a3293efc6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngrathwohl2021no,\ntitle={No {\\{}MCMC{\\}} for me: Amortized sampling for fast and stable training of energy-based models},\nauthor={Will Sussman Grathwohl and Jacob Jin Kelly and Milad Hashemi and Mohammad Norouzi and Kevin Swersky and David Duvenaud},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ixpSxO9flk3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ixpSxO9flk3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2046/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2046/Authors|ICLR.cc/2021/Conference/Paper2046/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852884, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment"}}}, {"id": "WhDI2jbyfrd", "original": null, "number": 20, "cdate": 1605626202857, "ddate": null, "tcdate": 1605626202857, "tmdate": 1605626202857, "tddate": null, "forum": "ixpSxO9flk3", "replyto": "IDPXKRxmg9x", "invitation": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment", "content": {"title": "Response to feedback", "comment": "Reviewer3,\n\nAgain we thank you for your time and thoughtful review. We took your comments into consideration and we feel that we have greatly strengthened our paper. We hope that our response to your comments and the changes we have made in the paper have addressed your concerns. We look forward to hearing from you.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2046/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "No MCMC for me: Amortized sampling for fast and stable training of energy-based models", "authorids": ["~Will_Sussman_Grathwohl2", "jacob.jin.kelly@gmail.com", "~Milad_Hashemi1", "~Mohammad_Norouzi1", "~Kevin_Swersky1", "~David_Duvenaud2"], "authors": ["Will Sussman Grathwohl", "Jacob Jin Kelly", "Milad Hashemi", "Mohammad Norouzi", "Kevin Swersky", "David Duvenaud"], "keywords": ["Generative Models", "EBM", "Energy-Based Models", "Energy Based Models", "semi-supervised learning", "JEM"], "abstract": "Energy-Based Models (EBMs) present a flexible and appealing way to represent uncertainty. Despite recent advances, training EBMs on high-dimensional data remains a challenging problem as the state-of-the-art approaches are costly, unstable, and require considerable tuning and domain expertise to apply successfully. In this work, we present a simple method for training EBMs at scale which uses an entropy-regularized generator to amortize the MCMC sampling typically used in EBM training. We improve upon prior MCMC-based entropy regularization methods with a fast variational approximation. We demonstrate the effectiveness of our approach by using it to train tractable likelihood models. Next, we apply our estimator to the recently proposed Joint Energy Model (JEM), where we match the original performance with faster and stable training. This allows us to extend JEM models to semi-supervised classification on tabular data from a variety of continuous domains.", "one-sentence_summary": "We present a new generator-based approach for training EBMs and demonstrate that it trains models which obtain high likelihood and overcomes stability issues common in EBM training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grathwohl|no_mcmc_for_me_amortized_sampling_for_fast_and_stable_training_of_energybased_models", "supplementary_material": "", "pdf": "/pdf/4bf06b7c43a66250e22fbdad3d0f492a3293efc6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngrathwohl2021no,\ntitle={No {\\{}MCMC{\\}} for me: Amortized sampling for fast and stable training of energy-based models},\nauthor={Will Sussman Grathwohl and Jacob Jin Kelly and Milad Hashemi and Mohammad Norouzi and Kevin Swersky and David Duvenaud},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ixpSxO9flk3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ixpSxO9flk3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2046/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2046/Authors|ICLR.cc/2021/Conference/Paper2046/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852884, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment"}}}, {"id": "iEQndg5hWi5", "original": null, "number": 19, "cdate": 1605626147070, "ddate": null, "tcdate": 1605626147070, "tmdate": 1605626147070, "tddate": null, "forum": "ixpSxO9flk3", "replyto": "KJEb6ZD1cHv", "invitation": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment", "content": {"title": "Response to feedback", "comment": "Reviewer2,\n\nAgain we thank you for your time and thoughtful review. We took your comments into consideration and we feel that we have greatly strengthened our paper. We hope that our response to your comments and the changes we have made in the paper have addressed your concerns. In particular, have we addressed your concerns regarding our importance sampling proposal and the variance of our estimator? We look forward to hearing from you.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2046/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "No MCMC for me: Amortized sampling for fast and stable training of energy-based models", "authorids": ["~Will_Sussman_Grathwohl2", "jacob.jin.kelly@gmail.com", "~Milad_Hashemi1", "~Mohammad_Norouzi1", "~Kevin_Swersky1", "~David_Duvenaud2"], "authors": ["Will Sussman Grathwohl", "Jacob Jin Kelly", "Milad Hashemi", "Mohammad Norouzi", "Kevin Swersky", "David Duvenaud"], "keywords": ["Generative Models", "EBM", "Energy-Based Models", "Energy Based Models", "semi-supervised learning", "JEM"], "abstract": "Energy-Based Models (EBMs) present a flexible and appealing way to represent uncertainty. Despite recent advances, training EBMs on high-dimensional data remains a challenging problem as the state-of-the-art approaches are costly, unstable, and require considerable tuning and domain expertise to apply successfully. In this work, we present a simple method for training EBMs at scale which uses an entropy-regularized generator to amortize the MCMC sampling typically used in EBM training. We improve upon prior MCMC-based entropy regularization methods with a fast variational approximation. We demonstrate the effectiveness of our approach by using it to train tractable likelihood models. Next, we apply our estimator to the recently proposed Joint Energy Model (JEM), where we match the original performance with faster and stable training. This allows us to extend JEM models to semi-supervised classification on tabular data from a variety of continuous domains.", "one-sentence_summary": "We present a new generator-based approach for training EBMs and demonstrate that it trains models which obtain high likelihood and overcomes stability issues common in EBM training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grathwohl|no_mcmc_for_me_amortized_sampling_for_fast_and_stable_training_of_energybased_models", "supplementary_material": "", "pdf": "/pdf/4bf06b7c43a66250e22fbdad3d0f492a3293efc6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngrathwohl2021no,\ntitle={No {\\{}MCMC{\\}} for me: Amortized sampling for fast and stable training of energy-based models},\nauthor={Will Sussman Grathwohl and Jacob Jin Kelly and Milad Hashemi and Mohammad Norouzi and Kevin Swersky and David Duvenaud},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ixpSxO9flk3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ixpSxO9flk3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2046/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2046/Authors|ICLR.cc/2021/Conference/Paper2046/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852884, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment"}}}, {"id": "WjuK_QrW2F0", "original": null, "number": 18, "cdate": 1605493998998, "ddate": null, "tcdate": 1605493998998, "tmdate": 1605493998998, "tddate": null, "forum": "ixpSxO9flk3", "replyto": "4cFsIrg_mWp", "invitation": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment", "content": {"title": "more related work", "comment": "Zengyi,\n\nThank you for alerting us to this work. It is quite interesting and indeed related. We have updated our paper to include a citation and a discussion comparing this work to ours. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2046/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "No MCMC for me: Amortized sampling for fast and stable training of energy-based models", "authorids": ["~Will_Sussman_Grathwohl2", "jacob.jin.kelly@gmail.com", "~Milad_Hashemi1", "~Mohammad_Norouzi1", "~Kevin_Swersky1", "~David_Duvenaud2"], "authors": ["Will Sussman Grathwohl", "Jacob Jin Kelly", "Milad Hashemi", "Mohammad Norouzi", "Kevin Swersky", "David Duvenaud"], "keywords": ["Generative Models", "EBM", "Energy-Based Models", "Energy Based Models", "semi-supervised learning", "JEM"], "abstract": "Energy-Based Models (EBMs) present a flexible and appealing way to represent uncertainty. Despite recent advances, training EBMs on high-dimensional data remains a challenging problem as the state-of-the-art approaches are costly, unstable, and require considerable tuning and domain expertise to apply successfully. In this work, we present a simple method for training EBMs at scale which uses an entropy-regularized generator to amortize the MCMC sampling typically used in EBM training. We improve upon prior MCMC-based entropy regularization methods with a fast variational approximation. We demonstrate the effectiveness of our approach by using it to train tractable likelihood models. Next, we apply our estimator to the recently proposed Joint Energy Model (JEM), where we match the original performance with faster and stable training. This allows us to extend JEM models to semi-supervised classification on tabular data from a variety of continuous domains.", "one-sentence_summary": "We present a new generator-based approach for training EBMs and demonstrate that it trains models which obtain high likelihood and overcomes stability issues common in EBM training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grathwohl|no_mcmc_for_me_amortized_sampling_for_fast_and_stable_training_of_energybased_models", "supplementary_material": "", "pdf": "/pdf/4bf06b7c43a66250e22fbdad3d0f492a3293efc6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngrathwohl2021no,\ntitle={No {\\{}MCMC{\\}} for me: Amortized sampling for fast and stable training of energy-based models},\nauthor={Will Sussman Grathwohl and Jacob Jin Kelly and Milad Hashemi and Mohammad Norouzi and Kevin Swersky and David Duvenaud},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ixpSxO9flk3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ixpSxO9flk3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2046/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2046/Authors|ICLR.cc/2021/Conference/Paper2046/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852884, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment"}}}, {"id": "4cFsIrg_mWp", "original": null, "number": 4, "cdate": 1605490995189, "ddate": null, "tcdate": 1605490995189, "tmdate": 1605490995189, "tddate": null, "forum": "ixpSxO9flk3", "replyto": "ixpSxO9flk3", "invitation": "ICLR.cc/2021/Conference/Paper2046/-/Public_Comment", "content": {"title": "Yet another suggested reference/comparison", "comment": "I know this is frustrating, but I happen to know yet another paper [Abbasnejad et al 2019], which I'm pretty sure uses the same objective as your paper. Their approach of optimizing the entropy term is indeed different, but a comparison and some discussion is probably needed here.\n\nThanks\n\n\nReference:\nAbbasnejad, M. Ehsan, et al. \"A generative adversarial density estimator.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.\n"}, "signatures": ["~ZENGYI_LI1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~ZENGYI_LI1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "No MCMC for me: Amortized sampling for fast and stable training of energy-based models", "authorids": ["~Will_Sussman_Grathwohl2", "jacob.jin.kelly@gmail.com", "~Milad_Hashemi1", "~Mohammad_Norouzi1", "~Kevin_Swersky1", "~David_Duvenaud2"], "authors": ["Will Sussman Grathwohl", "Jacob Jin Kelly", "Milad Hashemi", "Mohammad Norouzi", "Kevin Swersky", "David Duvenaud"], "keywords": ["Generative Models", "EBM", "Energy-Based Models", "Energy Based Models", "semi-supervised learning", "JEM"], "abstract": "Energy-Based Models (EBMs) present a flexible and appealing way to represent uncertainty. Despite recent advances, training EBMs on high-dimensional data remains a challenging problem as the state-of-the-art approaches are costly, unstable, and require considerable tuning and domain expertise to apply successfully. In this work, we present a simple method for training EBMs at scale which uses an entropy-regularized generator to amortize the MCMC sampling typically used in EBM training. We improve upon prior MCMC-based entropy regularization methods with a fast variational approximation. We demonstrate the effectiveness of our approach by using it to train tractable likelihood models. Next, we apply our estimator to the recently proposed Joint Energy Model (JEM), where we match the original performance with faster and stable training. This allows us to extend JEM models to semi-supervised classification on tabular data from a variety of continuous domains.", "one-sentence_summary": "We present a new generator-based approach for training EBMs and demonstrate that it trains models which obtain high likelihood and overcomes stability issues common in EBM training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grathwohl|no_mcmc_for_me_amortized_sampling_for_fast_and_stable_training_of_energybased_models", "supplementary_material": "", "pdf": "/pdf/4bf06b7c43a66250e22fbdad3d0f492a3293efc6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngrathwohl2021no,\ntitle={No {\\{}MCMC{\\}} for me: Amortized sampling for fast and stable training of energy-based models},\nauthor={Will Sussman Grathwohl and Jacob Jin Kelly and Milad Hashemi and Mohammad Norouzi and Kevin Swersky and David Duvenaud},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ixpSxO9flk3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ixpSxO9flk3", "readers": {"description": "User groups that will be able to read this comment.", "values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed."}}, "expdate": 1605630600000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Authors", "ICLR.cc/2021/Conference/Paper2046/Reviewers", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1605024966444, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2046/-/Public_Comment"}}}, {"id": "AItsMU1Q5h3", "original": null, "number": 16, "cdate": 1605386917300, "ddate": null, "tcdate": 1605386917300, "tmdate": 1605389279996, "tddate": null, "forum": "ixpSxO9flk3", "replyto": "C4VuDibwoD", "invitation": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment", "content": {"title": "Related work", "comment": "Feng,\n\nWe were first made aware of CoopNets after Yang's comment yesterday. We appreciate you as well bringing it to our attention. As we said in our response to Yang, we completely agree that CoopNets are related to our work and to the many other works which have been published in the past few years (some others even submitted to this conference) which train EBMs and alongside generators. We sincerely apologize for not originally citing and comparing with CoopNets. We have remedied this in the updated paper. \n\nBeyond this, we disagree with the tone and content of your comments. We made no claims that our work is the first to train EBMs along with a generator. You also seem to overlook the many things that make our work distinct from CoopNets. First, we use a different training objective. Second, the focus of our work is to develop an approach for training large-scale EBMs without any MCMC sampling (hence our paper's title). CoopNets require MCMC sampling to train both the EBM and the generator. Removing MCMC provides the challenge of generator entropy estimation which we tackle, leading to the main contribution of our work -- a fast, efficient, effective entropy regularizer. This contribution is completely orthogonal to CoopNets.\n\nPutting this together, we develop an MCMC-free method for training EBMs. We demonstrate the utility of this training approach in a number of distinct applications, model classes, and evaluation metrics. \n\nFor these reasons, we disagree with you about the novelty of our work. \n\nFurther, we disagree that any method which includes a generator and an EBM should be named a \"CoopNet.\" Our training objective, intended application, and many other factors differ from CoopNets, so we do not feel this warrants a name change.  As reviewer1 pointed out, CoopNets are not the first published work to do this. Further, the Inclusive-NRF of [1] is more related to CoopNets than our work and yet is not referred to as a CoopNet. Do you also propose Maximum Entropy Generators [2], Adversarial Exponential Family Dynamics Embedding [3], FLow-Contrastive Estimation [4],  and Auxiliary-variable Local Exploration [5] also be renamed as CoopNets? \n\n\nWe appreciate you and Yang bringing this work to our attention. We believe the added discussion and new experiments greatly strengthen our work. Thank you very much for that. \n\n\n\n\n[1] Song, Yunfu, and Zhijian Ou. \"Learning neural random fields with inclusive auxiliary generators.\"\n\n[2] Kumar, Rithesh, et al. \"Maximum entropy generators for energy-based models.\" \n\n[3] Dai, Bo, et al. \"Exponential family estimation via adversarial dynamics embedding.\" \n\n[4] Gao, Ruiqi, et al. \"Flow contrastive estimation of energy-based models.\" \n\n[5] Dai, Hanjun, et al. \"Learning Discrete Energy-based Models via Auxiliary-variable Local Exploration\""}, "signatures": ["ICLR.cc/2021/Conference/Paper2046/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "No MCMC for me: Amortized sampling for fast and stable training of energy-based models", "authorids": ["~Will_Sussman_Grathwohl2", "jacob.jin.kelly@gmail.com", "~Milad_Hashemi1", "~Mohammad_Norouzi1", "~Kevin_Swersky1", "~David_Duvenaud2"], "authors": ["Will Sussman Grathwohl", "Jacob Jin Kelly", "Milad Hashemi", "Mohammad Norouzi", "Kevin Swersky", "David Duvenaud"], "keywords": ["Generative Models", "EBM", "Energy-Based Models", "Energy Based Models", "semi-supervised learning", "JEM"], "abstract": "Energy-Based Models (EBMs) present a flexible and appealing way to represent uncertainty. Despite recent advances, training EBMs on high-dimensional data remains a challenging problem as the state-of-the-art approaches are costly, unstable, and require considerable tuning and domain expertise to apply successfully. In this work, we present a simple method for training EBMs at scale which uses an entropy-regularized generator to amortize the MCMC sampling typically used in EBM training. We improve upon prior MCMC-based entropy regularization methods with a fast variational approximation. We demonstrate the effectiveness of our approach by using it to train tractable likelihood models. Next, we apply our estimator to the recently proposed Joint Energy Model (JEM), where we match the original performance with faster and stable training. This allows us to extend JEM models to semi-supervised classification on tabular data from a variety of continuous domains.", "one-sentence_summary": "We present a new generator-based approach for training EBMs and demonstrate that it trains models which obtain high likelihood and overcomes stability issues common in EBM training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grathwohl|no_mcmc_for_me_amortized_sampling_for_fast_and_stable_training_of_energybased_models", "supplementary_material": "", "pdf": "/pdf/4bf06b7c43a66250e22fbdad3d0f492a3293efc6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngrathwohl2021no,\ntitle={No {\\{}MCMC{\\}} for me: Amortized sampling for fast and stable training of energy-based models},\nauthor={Will Sussman Grathwohl and Jacob Jin Kelly and Milad Hashemi and Mohammad Norouzi and Kevin Swersky and David Duvenaud},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ixpSxO9flk3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ixpSxO9flk3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2046/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2046/Authors|ICLR.cc/2021/Conference/Paper2046/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852884, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment"}}}, {"id": "yHxeXYAnbs4", "original": null, "number": 17, "cdate": 1605387790208, "ddate": null, "tcdate": 1605387790208, "tmdate": 1605387790208, "tddate": null, "forum": "ixpSxO9flk3", "replyto": "AoxFnFdiLl-", "invitation": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment", "content": {"title": "Thanks! Interesting ideas!", "comment": "Zhisheng,\n\nThanks for your interest in our work. These are interesting points you bring up. This was in a sense the goal of our NICE experiments. There we examine the ability of our approach to train high-quality, unconditional energy functions. We chose to use flows since we can easily quantify the quality of the models we learn using likelihood. We could have trained unconditional image models as well but the evaluation and comparison is a bit trickier. Most image models like this are evaluated using qualitative measures of sample quality (such as FID, IS). It's well known that sample quality does not necessarily line up well with likelihood. Further, if sample quality is the target measure, we believe other classes of models such as GANs and diffusion models are more promising. We are more interested in EBMs for their application to downstream tasks, hence the evaluation on JEM and SSL. Having said all this, I agree that it would be interesting to see these results and I may investigate that sometime soon. \n\nWe hope this makes things more clear. Thanks again for you comment. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2046/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "No MCMC for me: Amortized sampling for fast and stable training of energy-based models", "authorids": ["~Will_Sussman_Grathwohl2", "jacob.jin.kelly@gmail.com", "~Milad_Hashemi1", "~Mohammad_Norouzi1", "~Kevin_Swersky1", "~David_Duvenaud2"], "authors": ["Will Sussman Grathwohl", "Jacob Jin Kelly", "Milad Hashemi", "Mohammad Norouzi", "Kevin Swersky", "David Duvenaud"], "keywords": ["Generative Models", "EBM", "Energy-Based Models", "Energy Based Models", "semi-supervised learning", "JEM"], "abstract": "Energy-Based Models (EBMs) present a flexible and appealing way to represent uncertainty. Despite recent advances, training EBMs on high-dimensional data remains a challenging problem as the state-of-the-art approaches are costly, unstable, and require considerable tuning and domain expertise to apply successfully. In this work, we present a simple method for training EBMs at scale which uses an entropy-regularized generator to amortize the MCMC sampling typically used in EBM training. We improve upon prior MCMC-based entropy regularization methods with a fast variational approximation. We demonstrate the effectiveness of our approach by using it to train tractable likelihood models. Next, we apply our estimator to the recently proposed Joint Energy Model (JEM), where we match the original performance with faster and stable training. This allows us to extend JEM models to semi-supervised classification on tabular data from a variety of continuous domains.", "one-sentence_summary": "We present a new generator-based approach for training EBMs and demonstrate that it trains models which obtain high likelihood and overcomes stability issues common in EBM training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grathwohl|no_mcmc_for_me_amortized_sampling_for_fast_and_stable_training_of_energybased_models", "supplementary_material": "", "pdf": "/pdf/4bf06b7c43a66250e22fbdad3d0f492a3293efc6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngrathwohl2021no,\ntitle={No {\\{}MCMC{\\}} for me: Amortized sampling for fast and stable training of energy-based models},\nauthor={Will Sussman Grathwohl and Jacob Jin Kelly and Milad Hashemi and Mohammad Norouzi and Kevin Swersky and David Duvenaud},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ixpSxO9flk3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ixpSxO9flk3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2046/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2046/Authors|ICLR.cc/2021/Conference/Paper2046/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852884, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment"}}}, {"id": "AoxFnFdiLl-", "original": null, "number": 3, "cdate": 1605386382030, "ddate": null, "tcdate": 1605386382030, "tmdate": 1605386473285, "tddate": null, "forum": "ixpSxO9flk3", "replyto": "ixpSxO9flk3", "invitation": "ICLR.cc/2021/Conference/Paper2046/-/Public_Comment", "content": {"title": "Training on unconditional EBM rather than JEM?", "comment": "Hi authors! I like your new idea on training EBMs. One question: your experiments on real image dataset is training a JEM, while the result is impressive, I may wonder that training JEM is easier than training an completely unconditional EBM, as the classification part can provide strong signal to learn the parameters. Is it possible to train unconditional EBMs (say, on images with no label like LSUN/CelebA, or at least just pretending we don't have labels for cifar-10) using your method? That might be a more challenging task than JEM. I personally think that to show the effectiveness of a new deep EBM training method, it would be better to disentangle all other factors and focus on the most basic generative modeling task. EBM training paper like [1,2,3] all performs at least some unconditional generation experiments.\n\n[1]Implicit Generation and Modeling with Energy Based Models\n[2] Learning Energy-Based Models in High-Dimensional Spaces with Multi-scale Denoising Score Matching https://arxiv.org/abs/1910.07762\n[3]Training Deep Energy-Based Models with f-Divergence Minimization https://arxiv.org/abs/1910.07762"}, "signatures": ["~Zhisheng_Xiao1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Zhisheng_Xiao1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "No MCMC for me: Amortized sampling for fast and stable training of energy-based models", "authorids": ["~Will_Sussman_Grathwohl2", "jacob.jin.kelly@gmail.com", "~Milad_Hashemi1", "~Mohammad_Norouzi1", "~Kevin_Swersky1", "~David_Duvenaud2"], "authors": ["Will Sussman Grathwohl", "Jacob Jin Kelly", "Milad Hashemi", "Mohammad Norouzi", "Kevin Swersky", "David Duvenaud"], "keywords": ["Generative Models", "EBM", "Energy-Based Models", "Energy Based Models", "semi-supervised learning", "JEM"], "abstract": "Energy-Based Models (EBMs) present a flexible and appealing way to represent uncertainty. Despite recent advances, training EBMs on high-dimensional data remains a challenging problem as the state-of-the-art approaches are costly, unstable, and require considerable tuning and domain expertise to apply successfully. In this work, we present a simple method for training EBMs at scale which uses an entropy-regularized generator to amortize the MCMC sampling typically used in EBM training. We improve upon prior MCMC-based entropy regularization methods with a fast variational approximation. We demonstrate the effectiveness of our approach by using it to train tractable likelihood models. Next, we apply our estimator to the recently proposed Joint Energy Model (JEM), where we match the original performance with faster and stable training. This allows us to extend JEM models to semi-supervised classification on tabular data from a variety of continuous domains.", "one-sentence_summary": "We present a new generator-based approach for training EBMs and demonstrate that it trains models which obtain high likelihood and overcomes stability issues common in EBM training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grathwohl|no_mcmc_for_me_amortized_sampling_for_fast_and_stable_training_of_energybased_models", "supplementary_material": "", "pdf": "/pdf/4bf06b7c43a66250e22fbdad3d0f492a3293efc6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngrathwohl2021no,\ntitle={No {\\{}MCMC{\\}} for me: Amortized sampling for fast and stable training of energy-based models},\nauthor={Will Sussman Grathwohl and Jacob Jin Kelly and Milad Hashemi and Mohammad Norouzi and Kevin Swersky and David Duvenaud},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ixpSxO9flk3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ixpSxO9flk3", "readers": {"description": "User groups that will be able to read this comment.", "values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed."}}, "expdate": 1605630600000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Authors", "ICLR.cc/2021/Conference/Paper2046/Reviewers", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1605024966444, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2046/-/Public_Comment"}}}, {"id": "ZRvafFPkR_L", "original": null, "number": 15, "cdate": 1605380228085, "ddate": null, "tcdate": 1605380228085, "tmdate": 1605380228085, "tddate": null, "forum": "ixpSxO9flk3", "replyto": "KJEb6ZD1cHv", "invitation": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment", "content": {"title": "Added new baselines", "comment": "Thank you again for your thoughtful review. In response to the reviewer and public feedback, we have run a few more baselines which are now in an updated version of the paper. \n\nWe ran MEG as a baseline for our SSL experiments. We find it performs on par with the supervised baseline. We believe this is due to the entropy regularizer being less effective than our own. \n\nWe have also run CoopNets as a baseline in our NICE training section. It is a competitive approach, outperforming PCD and many SM variants, but it also has many of the same issues as PCD training. We found we needed to use considerably different hyper-parmeters than presented in the original work to stably train in this setting. Further, due to the MCMC sampling, CoopNets runs multiple times slower than VERA. We believe with more tuning or more MCMC steps, the results might be able to improve, but we leave exploring this further for future work. \n\nThanks again for your time!"}, "signatures": ["ICLR.cc/2021/Conference/Paper2046/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "No MCMC for me: Amortized sampling for fast and stable training of energy-based models", "authorids": ["~Will_Sussman_Grathwohl2", "jacob.jin.kelly@gmail.com", "~Milad_Hashemi1", "~Mohammad_Norouzi1", "~Kevin_Swersky1", "~David_Duvenaud2"], "authors": ["Will Sussman Grathwohl", "Jacob Jin Kelly", "Milad Hashemi", "Mohammad Norouzi", "Kevin Swersky", "David Duvenaud"], "keywords": ["Generative Models", "EBM", "Energy-Based Models", "Energy Based Models", "semi-supervised learning", "JEM"], "abstract": "Energy-Based Models (EBMs) present a flexible and appealing way to represent uncertainty. Despite recent advances, training EBMs on high-dimensional data remains a challenging problem as the state-of-the-art approaches are costly, unstable, and require considerable tuning and domain expertise to apply successfully. In this work, we present a simple method for training EBMs at scale which uses an entropy-regularized generator to amortize the MCMC sampling typically used in EBM training. We improve upon prior MCMC-based entropy regularization methods with a fast variational approximation. We demonstrate the effectiveness of our approach by using it to train tractable likelihood models. Next, we apply our estimator to the recently proposed Joint Energy Model (JEM), where we match the original performance with faster and stable training. This allows us to extend JEM models to semi-supervised classification on tabular data from a variety of continuous domains.", "one-sentence_summary": "We present a new generator-based approach for training EBMs and demonstrate that it trains models which obtain high likelihood and overcomes stability issues common in EBM training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grathwohl|no_mcmc_for_me_amortized_sampling_for_fast_and_stable_training_of_energybased_models", "supplementary_material": "", "pdf": "/pdf/4bf06b7c43a66250e22fbdad3d0f492a3293efc6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngrathwohl2021no,\ntitle={No {\\{}MCMC{\\}} for me: Amortized sampling for fast and stable training of energy-based models},\nauthor={Will Sussman Grathwohl and Jacob Jin Kelly and Milad Hashemi and Mohammad Norouzi and Kevin Swersky and David Duvenaud},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ixpSxO9flk3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ixpSxO9flk3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2046/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2046/Authors|ICLR.cc/2021/Conference/Paper2046/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852884, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment"}}}, {"id": "djYzxDhtRQj", "original": null, "number": 14, "cdate": 1605380175711, "ddate": null, "tcdate": 1605380175711, "tmdate": 1605380175711, "tddate": null, "forum": "ixpSxO9flk3", "replyto": "IDPXKRxmg9x", "invitation": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment", "content": {"title": "Added new baselines", "comment": "Thank you again for your thoughtful review. In response to the reviewer and public feedback, we have run a few more baselines which are now in an updated version of the paper. \n\nWe ran MEG as a baseline for our SSL experiments. We find it performs on par with the supervised baseline. We believe this is due to the entropy regularizer being less effective than our own. \n\nWe have also run CoopNets as a baseline in our NICE training section. It is a competitive approach, outperforming PCD and many SM variants, but it also has many of the same issues as PCD training. We found we needed to use considerably different hyper-parmeters than presented in the original work to stably train in this setting. Further, due to the MCMC sampling, CoopNets runs multiple times slower than VERA. We believe with more tuning or more MCMC steps, the results might be able to improve, but we leave exploring this further for future work. \n\nThanks again for your time!"}, "signatures": ["ICLR.cc/2021/Conference/Paper2046/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "No MCMC for me: Amortized sampling for fast and stable training of energy-based models", "authorids": ["~Will_Sussman_Grathwohl2", "jacob.jin.kelly@gmail.com", "~Milad_Hashemi1", "~Mohammad_Norouzi1", "~Kevin_Swersky1", "~David_Duvenaud2"], "authors": ["Will Sussman Grathwohl", "Jacob Jin Kelly", "Milad Hashemi", "Mohammad Norouzi", "Kevin Swersky", "David Duvenaud"], "keywords": ["Generative Models", "EBM", "Energy-Based Models", "Energy Based Models", "semi-supervised learning", "JEM"], "abstract": "Energy-Based Models (EBMs) present a flexible and appealing way to represent uncertainty. Despite recent advances, training EBMs on high-dimensional data remains a challenging problem as the state-of-the-art approaches are costly, unstable, and require considerable tuning and domain expertise to apply successfully. In this work, we present a simple method for training EBMs at scale which uses an entropy-regularized generator to amortize the MCMC sampling typically used in EBM training. We improve upon prior MCMC-based entropy regularization methods with a fast variational approximation. We demonstrate the effectiveness of our approach by using it to train tractable likelihood models. Next, we apply our estimator to the recently proposed Joint Energy Model (JEM), where we match the original performance with faster and stable training. This allows us to extend JEM models to semi-supervised classification on tabular data from a variety of continuous domains.", "one-sentence_summary": "We present a new generator-based approach for training EBMs and demonstrate that it trains models which obtain high likelihood and overcomes stability issues common in EBM training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grathwohl|no_mcmc_for_me_amortized_sampling_for_fast_and_stable_training_of_energybased_models", "supplementary_material": "", "pdf": "/pdf/4bf06b7c43a66250e22fbdad3d0f492a3293efc6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngrathwohl2021no,\ntitle={No {\\{}MCMC{\\}} for me: Amortized sampling for fast and stable training of energy-based models},\nauthor={Will Sussman Grathwohl and Jacob Jin Kelly and Milad Hashemi and Mohammad Norouzi and Kevin Swersky and David Duvenaud},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ixpSxO9flk3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ixpSxO9flk3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2046/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2046/Authors|ICLR.cc/2021/Conference/Paper2046/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852884, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment"}}}, {"id": "JAWjwnUAVx6", "original": null, "number": 13, "cdate": 1605380151227, "ddate": null, "tcdate": 1605380151227, "tmdate": 1605380151227, "tddate": null, "forum": "ixpSxO9flk3", "replyto": "d5eBVKdgkuH", "invitation": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment", "content": {"title": "Added new baselines", "comment": "Thank you again for your thoughtful review. In response to the reviewer and public feedback, we have run a few more baselines which are now in an updated version of the paper. \n\nWe ran MEG as a baseline for our SSL experiments. We find it performs on par with the supervised baseline. We believe this is due to the entropy regularizer being less effective than our own. \n\nWe have also run CoopNets as a baseline in our NICE training section. It is a competitive approach, outperforming PCD and many SM variants, but it also has many of the same issues as PCD training. We found we needed to use considerably different hyper-parmeters than presented in the original work to stably train in this setting. Further, due to the MCMC sampling, CoopNets runs multiple times slower than VERA. We believe with more tuning or more MCMC steps, the results might be able to improve, but we leave exploring this further for future work. \n\nThanks again for your time!"}, "signatures": ["ICLR.cc/2021/Conference/Paper2046/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "No MCMC for me: Amortized sampling for fast and stable training of energy-based models", "authorids": ["~Will_Sussman_Grathwohl2", "jacob.jin.kelly@gmail.com", "~Milad_Hashemi1", "~Mohammad_Norouzi1", "~Kevin_Swersky1", "~David_Duvenaud2"], "authors": ["Will Sussman Grathwohl", "Jacob Jin Kelly", "Milad Hashemi", "Mohammad Norouzi", "Kevin Swersky", "David Duvenaud"], "keywords": ["Generative Models", "EBM", "Energy-Based Models", "Energy Based Models", "semi-supervised learning", "JEM"], "abstract": "Energy-Based Models (EBMs) present a flexible and appealing way to represent uncertainty. Despite recent advances, training EBMs on high-dimensional data remains a challenging problem as the state-of-the-art approaches are costly, unstable, and require considerable tuning and domain expertise to apply successfully. In this work, we present a simple method for training EBMs at scale which uses an entropy-regularized generator to amortize the MCMC sampling typically used in EBM training. We improve upon prior MCMC-based entropy regularization methods with a fast variational approximation. We demonstrate the effectiveness of our approach by using it to train tractable likelihood models. Next, we apply our estimator to the recently proposed Joint Energy Model (JEM), where we match the original performance with faster and stable training. This allows us to extend JEM models to semi-supervised classification on tabular data from a variety of continuous domains.", "one-sentence_summary": "We present a new generator-based approach for training EBMs and demonstrate that it trains models which obtain high likelihood and overcomes stability issues common in EBM training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grathwohl|no_mcmc_for_me_amortized_sampling_for_fast_and_stable_training_of_energybased_models", "supplementary_material": "", "pdf": "/pdf/4bf06b7c43a66250e22fbdad3d0f492a3293efc6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngrathwohl2021no,\ntitle={No {\\{}MCMC{\\}} for me: Amortized sampling for fast and stable training of energy-based models},\nauthor={Will Sussman Grathwohl and Jacob Jin Kelly and Milad Hashemi and Mohammad Norouzi and Kevin Swersky and David Duvenaud},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ixpSxO9flk3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ixpSxO9flk3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2046/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2046/Authors|ICLR.cc/2021/Conference/Paper2046/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852884, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment"}}}, {"id": "Yf9GoupQODr", "original": null, "number": 12, "cdate": 1605379624546, "ddate": null, "tcdate": 1605379624546, "tmdate": 1605379624546, "tddate": null, "forum": "ixpSxO9flk3", "replyto": "M7JWXOoiAEo", "invitation": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment", "content": {"title": "Added comparison", "comment": "Again, thank you for your defense of our work. As recommended, we have added a comparison to CoopNets in our section training NICE flows. We hope you find this informative. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2046/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "No MCMC for me: Amortized sampling for fast and stable training of energy-based models", "authorids": ["~Will_Sussman_Grathwohl2", "jacob.jin.kelly@gmail.com", "~Milad_Hashemi1", "~Mohammad_Norouzi1", "~Kevin_Swersky1", "~David_Duvenaud2"], "authors": ["Will Sussman Grathwohl", "Jacob Jin Kelly", "Milad Hashemi", "Mohammad Norouzi", "Kevin Swersky", "David Duvenaud"], "keywords": ["Generative Models", "EBM", "Energy-Based Models", "Energy Based Models", "semi-supervised learning", "JEM"], "abstract": "Energy-Based Models (EBMs) present a flexible and appealing way to represent uncertainty. Despite recent advances, training EBMs on high-dimensional data remains a challenging problem as the state-of-the-art approaches are costly, unstable, and require considerable tuning and domain expertise to apply successfully. In this work, we present a simple method for training EBMs at scale which uses an entropy-regularized generator to amortize the MCMC sampling typically used in EBM training. We improve upon prior MCMC-based entropy regularization methods with a fast variational approximation. We demonstrate the effectiveness of our approach by using it to train tractable likelihood models. Next, we apply our estimator to the recently proposed Joint Energy Model (JEM), where we match the original performance with faster and stable training. This allows us to extend JEM models to semi-supervised classification on tabular data from a variety of continuous domains.", "one-sentence_summary": "We present a new generator-based approach for training EBMs and demonstrate that it trains models which obtain high likelihood and overcomes stability issues common in EBM training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grathwohl|no_mcmc_for_me_amortized_sampling_for_fast_and_stable_training_of_energybased_models", "supplementary_material": "", "pdf": "/pdf/4bf06b7c43a66250e22fbdad3d0f492a3293efc6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngrathwohl2021no,\ntitle={No {\\{}MCMC{\\}} for me: Amortized sampling for fast and stable training of energy-based models},\nauthor={Will Sussman Grathwohl and Jacob Jin Kelly and Milad Hashemi and Mohammad Norouzi and Kevin Swersky and David Duvenaud},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ixpSxO9flk3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ixpSxO9flk3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2046/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2046/Authors|ICLR.cc/2021/Conference/Paper2046/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852884, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment"}}}, {"id": "7xxM5a_0kBb", "original": null, "number": 11, "cdate": 1605379549322, "ddate": null, "tcdate": 1605379549322, "tmdate": 1605379549322, "tddate": null, "forum": "ixpSxO9flk3", "replyto": "58SMFKIcshW", "invitation": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment", "content": {"title": "Added comparison with Coopnets", "comment": "Yang, \n\nWe have added CoopNets as a baseline for our NICE experiments. CoopNets perform better than PCD training but seem to have some of the same issues. We leave it to future work to compare these methods further. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2046/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "No MCMC for me: Amortized sampling for fast and stable training of energy-based models", "authorids": ["~Will_Sussman_Grathwohl2", "jacob.jin.kelly@gmail.com", "~Milad_Hashemi1", "~Mohammad_Norouzi1", "~Kevin_Swersky1", "~David_Duvenaud2"], "authors": ["Will Sussman Grathwohl", "Jacob Jin Kelly", "Milad Hashemi", "Mohammad Norouzi", "Kevin Swersky", "David Duvenaud"], "keywords": ["Generative Models", "EBM", "Energy-Based Models", "Energy Based Models", "semi-supervised learning", "JEM"], "abstract": "Energy-Based Models (EBMs) present a flexible and appealing way to represent uncertainty. Despite recent advances, training EBMs on high-dimensional data remains a challenging problem as the state-of-the-art approaches are costly, unstable, and require considerable tuning and domain expertise to apply successfully. In this work, we present a simple method for training EBMs at scale which uses an entropy-regularized generator to amortize the MCMC sampling typically used in EBM training. We improve upon prior MCMC-based entropy regularization methods with a fast variational approximation. We demonstrate the effectiveness of our approach by using it to train tractable likelihood models. Next, we apply our estimator to the recently proposed Joint Energy Model (JEM), where we match the original performance with faster and stable training. This allows us to extend JEM models to semi-supervised classification on tabular data from a variety of continuous domains.", "one-sentence_summary": "We present a new generator-based approach for training EBMs and demonstrate that it trains models which obtain high likelihood and overcomes stability issues common in EBM training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grathwohl|no_mcmc_for_me_amortized_sampling_for_fast_and_stable_training_of_energybased_models", "supplementary_material": "", "pdf": "/pdf/4bf06b7c43a66250e22fbdad3d0f492a3293efc6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngrathwohl2021no,\ntitle={No {\\{}MCMC{\\}} for me: Amortized sampling for fast and stable training of energy-based models},\nauthor={Will Sussman Grathwohl and Jacob Jin Kelly and Milad Hashemi and Mohammad Norouzi and Kevin Swersky and David Duvenaud},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ixpSxO9flk3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ixpSxO9flk3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2046/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2046/Authors|ICLR.cc/2021/Conference/Paper2046/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852884, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment"}}}, {"id": "MJUZnAmjNH1", "original": null, "number": 10, "cdate": 1605363078622, "ddate": null, "tcdate": 1605363078622, "tmdate": 1605363078622, "tddate": null, "forum": "ixpSxO9flk3", "replyto": "4CxO1xrfs9K", "invitation": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment", "content": {"title": "Thank you (again) for defending our work. ", "comment": "We thank you very much for taking the time to understand the key differences between our proposed method and Coopnets. We have already updated our paper to add a citation to Coopnets and discuss the similarities and differences.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2046/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "No MCMC for me: Amortized sampling for fast and stable training of energy-based models", "authorids": ["~Will_Sussman_Grathwohl2", "jacob.jin.kelly@gmail.com", "~Milad_Hashemi1", "~Mohammad_Norouzi1", "~Kevin_Swersky1", "~David_Duvenaud2"], "authors": ["Will Sussman Grathwohl", "Jacob Jin Kelly", "Milad Hashemi", "Mohammad Norouzi", "Kevin Swersky", "David Duvenaud"], "keywords": ["Generative Models", "EBM", "Energy-Based Models", "Energy Based Models", "semi-supervised learning", "JEM"], "abstract": "Energy-Based Models (EBMs) present a flexible and appealing way to represent uncertainty. Despite recent advances, training EBMs on high-dimensional data remains a challenging problem as the state-of-the-art approaches are costly, unstable, and require considerable tuning and domain expertise to apply successfully. In this work, we present a simple method for training EBMs at scale which uses an entropy-regularized generator to amortize the MCMC sampling typically used in EBM training. We improve upon prior MCMC-based entropy regularization methods with a fast variational approximation. We demonstrate the effectiveness of our approach by using it to train tractable likelihood models. Next, we apply our estimator to the recently proposed Joint Energy Model (JEM), where we match the original performance with faster and stable training. This allows us to extend JEM models to semi-supervised classification on tabular data from a variety of continuous domains.", "one-sentence_summary": "We present a new generator-based approach for training EBMs and demonstrate that it trains models which obtain high likelihood and overcomes stability issues common in EBM training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grathwohl|no_mcmc_for_me_amortized_sampling_for_fast_and_stable_training_of_energybased_models", "supplementary_material": "", "pdf": "/pdf/4bf06b7c43a66250e22fbdad3d0f492a3293efc6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngrathwohl2021no,\ntitle={No {\\{}MCMC{\\}} for me: Amortized sampling for fast and stable training of energy-based models},\nauthor={Will Sussman Grathwohl and Jacob Jin Kelly and Milad Hashemi and Mohammad Norouzi and Kevin Swersky and David Duvenaud},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ixpSxO9flk3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ixpSxO9flk3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2046/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2046/Authors|ICLR.cc/2021/Conference/Paper2046/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852884, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment"}}}, {"id": "58SMFKIcshW", "original": null, "number": 9, "cdate": 1605360841970, "ddate": null, "tcdate": 1605360841970, "tmdate": 1605360841970, "tddate": null, "forum": "ixpSxO9flk3", "replyto": "yz6JwO736tP", "invitation": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment", "content": {"title": "Related? Yes. Reinvented...? We don't think so.", "comment": "Yang,\n\nThank you for alerting us to your work. We agree that it is indeed related to our own and we have added a reference to the work you mention in our related work section. \n\nWe disagree though with your claims that our work lacks novelty and is a \u201creinvention\u201d of your work. We make no claims that it is a novel contribution to use a generator to train EBMs. We cite multiple works that do this such as [1, 2, 3, 4] and we have added your work to this list.\n\nThere are a number of key differences which distinguish our work from yours.\n\nOur training objective differs from yours. Your EBM and generator are trained in an EM or wake-sleep fashion. You sample  $x\\sim q(x)$ from the generator, you then refine these samples using MCMC on your EBM and to obtain samples $x\u2019$ which more closely match your EBM\u2019s distribution. You use these samples for EBM training. You then optimize the parameters of q by performing maximum likelihood (forward KL-divergence minimization) updates on the refined samples $x\u2019$. This step requires posterior sampling with MCMC. This training objective is quite similar to [4, 5].\n\nIn contrast, our generator is trained to minimize the reverse KL-divergence (like [1, 3]) between our EBM and generator. The main challenge with this involves maximizing the entropy of the generator. A key contribution of our work is an efficient method for doing this which is faster and more accurate than prior work. This contribution is completely unrelated to your work. Since this is the improvement we claim to make, we compare against other methods which propose entropy regularizers for latent variable models [3, 6]\n\nAll of this is in an attempt to remove the need for MCMC sampling (re: our paper title) to train EBMs. \n\nWe hope that this has clarified our contributions and helps you better understand how our work differs from your own. \n\n\n[1] Dai, Bo, et al. \"Exponential family estimation via adversarial dynamics embedding.\" \n\n[2] Gao, Ruiqi, et al. \"Flow contrastive estimation of energy-based models.\"\n\n[3] Kumar, Rithesh, et al. \"Maximum entropy generators for energy-based models.\"\n\n[4] Song, Yunfu, and Zhijian Ou. \"Learning neural random fields with inclusive auxiliary generators.\"\n\n[5] Dai, Hanjun et al. \"Learning Discrete Energy-based Models via Auxiliary-variable Local Exploration\"\n\n[6] Dieng, Adji B., et al. \"Prescribed generative adversarial networks.\"\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2046/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "No MCMC for me: Amortized sampling for fast and stable training of energy-based models", "authorids": ["~Will_Sussman_Grathwohl2", "jacob.jin.kelly@gmail.com", "~Milad_Hashemi1", "~Mohammad_Norouzi1", "~Kevin_Swersky1", "~David_Duvenaud2"], "authors": ["Will Sussman Grathwohl", "Jacob Jin Kelly", "Milad Hashemi", "Mohammad Norouzi", "Kevin Swersky", "David Duvenaud"], "keywords": ["Generative Models", "EBM", "Energy-Based Models", "Energy Based Models", "semi-supervised learning", "JEM"], "abstract": "Energy-Based Models (EBMs) present a flexible and appealing way to represent uncertainty. Despite recent advances, training EBMs on high-dimensional data remains a challenging problem as the state-of-the-art approaches are costly, unstable, and require considerable tuning and domain expertise to apply successfully. In this work, we present a simple method for training EBMs at scale which uses an entropy-regularized generator to amortize the MCMC sampling typically used in EBM training. We improve upon prior MCMC-based entropy regularization methods with a fast variational approximation. We demonstrate the effectiveness of our approach by using it to train tractable likelihood models. Next, we apply our estimator to the recently proposed Joint Energy Model (JEM), where we match the original performance with faster and stable training. This allows us to extend JEM models to semi-supervised classification on tabular data from a variety of continuous domains.", "one-sentence_summary": "We present a new generator-based approach for training EBMs and demonstrate that it trains models which obtain high likelihood and overcomes stability issues common in EBM training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grathwohl|no_mcmc_for_me_amortized_sampling_for_fast_and_stable_training_of_energybased_models", "supplementary_material": "", "pdf": "/pdf/4bf06b7c43a66250e22fbdad3d0f492a3293efc6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngrathwohl2021no,\ntitle={No {\\{}MCMC{\\}} for me: Amortized sampling for fast and stable training of energy-based models},\nauthor={Will Sussman Grathwohl and Jacob Jin Kelly and Milad Hashemi and Mohammad Norouzi and Kevin Swersky and David Duvenaud},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ixpSxO9flk3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ixpSxO9flk3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2046/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2046/Authors|ICLR.cc/2021/Conference/Paper2046/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852884, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment"}}}, {"id": "qoG-pQMou0g", "original": null, "number": 8, "cdate": 1605359906403, "ddate": null, "tcdate": 1605359906403, "tmdate": 1605359906403, "tddate": null, "forum": "ixpSxO9flk3", "replyto": "M7JWXOoiAEo", "invitation": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment", "content": {"title": "Thank you for defending our work. ", "comment": "We thank you very much for defending our work against these comments. We agree with everything you say and we thank you for taking a stand against comments like this. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2046/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "No MCMC for me: Amortized sampling for fast and stable training of energy-based models", "authorids": ["~Will_Sussman_Grathwohl2", "jacob.jin.kelly@gmail.com", "~Milad_Hashemi1", "~Mohammad_Norouzi1", "~Kevin_Swersky1", "~David_Duvenaud2"], "authors": ["Will Sussman Grathwohl", "Jacob Jin Kelly", "Milad Hashemi", "Mohammad Norouzi", "Kevin Swersky", "David Duvenaud"], "keywords": ["Generative Models", "EBM", "Energy-Based Models", "Energy Based Models", "semi-supervised learning", "JEM"], "abstract": "Energy-Based Models (EBMs) present a flexible and appealing way to represent uncertainty. Despite recent advances, training EBMs on high-dimensional data remains a challenging problem as the state-of-the-art approaches are costly, unstable, and require considerable tuning and domain expertise to apply successfully. In this work, we present a simple method for training EBMs at scale which uses an entropy-regularized generator to amortize the MCMC sampling typically used in EBM training. We improve upon prior MCMC-based entropy regularization methods with a fast variational approximation. We demonstrate the effectiveness of our approach by using it to train tractable likelihood models. Next, we apply our estimator to the recently proposed Joint Energy Model (JEM), where we match the original performance with faster and stable training. This allows us to extend JEM models to semi-supervised classification on tabular data from a variety of continuous domains.", "one-sentence_summary": "We present a new generator-based approach for training EBMs and demonstrate that it trains models which obtain high likelihood and overcomes stability issues common in EBM training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grathwohl|no_mcmc_for_me_amortized_sampling_for_fast_and_stable_training_of_energybased_models", "supplementary_material": "", "pdf": "/pdf/4bf06b7c43a66250e22fbdad3d0f492a3293efc6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngrathwohl2021no,\ntitle={No {\\{}MCMC{\\}} for me: Amortized sampling for fast and stable training of energy-based models},\nauthor={Will Sussman Grathwohl and Jacob Jin Kelly and Milad Hashemi and Mohammad Norouzi and Kevin Swersky and David Duvenaud},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ixpSxO9flk3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ixpSxO9flk3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2046/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2046/Authors|ICLR.cc/2021/Conference/Paper2046/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852884, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment"}}}, {"id": "M7JWXOoiAEo", "original": null, "number": 7, "cdate": 1605342555506, "ddate": null, "tcdate": 1605342555506, "tmdate": 1605342847568, "tddate": null, "forum": "ixpSxO9flk3", "replyto": "yz6JwO736tP", "invitation": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment", "content": {"title": "Not exactly the same as cooperative learning", "comment": "You seem to indicate that the method proposed here is exactly the same as your \"cooperative learning framework\". Though I agree they have similarities and \"cooperative learning\" should be discussed, I do think the new method is different enough and has its own novelty. At a quick glance it seems that cooperative training is using a different objective (not the dual form of likelihood Eq. (4)), and still requires multiple Langevin MCMC steps. I strongly encourage the authors to engage in this conversation and give a more comprehensive list.\n\nAs a friendly reminder, no matter how correct you might be, comments like \"I am wondering if ICLR encourages accepting a paper that reinvents an existing idea, especially when this method has been widely used in education in graduate school, research in other fields, etc.\" are presumptuous and irritating. Please do not take advantage of the authors because they cannot fight you back with sarcasm during rebuttal."}, "signatures": ["ICLR.cc/2021/Conference/Paper2046/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "No MCMC for me: Amortized sampling for fast and stable training of energy-based models", "authorids": ["~Will_Sussman_Grathwohl2", "jacob.jin.kelly@gmail.com", "~Milad_Hashemi1", "~Mohammad_Norouzi1", "~Kevin_Swersky1", "~David_Duvenaud2"], "authors": ["Will Sussman Grathwohl", "Jacob Jin Kelly", "Milad Hashemi", "Mohammad Norouzi", "Kevin Swersky", "David Duvenaud"], "keywords": ["Generative Models", "EBM", "Energy-Based Models", "Energy Based Models", "semi-supervised learning", "JEM"], "abstract": "Energy-Based Models (EBMs) present a flexible and appealing way to represent uncertainty. Despite recent advances, training EBMs on high-dimensional data remains a challenging problem as the state-of-the-art approaches are costly, unstable, and require considerable tuning and domain expertise to apply successfully. In this work, we present a simple method for training EBMs at scale which uses an entropy-regularized generator to amortize the MCMC sampling typically used in EBM training. We improve upon prior MCMC-based entropy regularization methods with a fast variational approximation. We demonstrate the effectiveness of our approach by using it to train tractable likelihood models. Next, we apply our estimator to the recently proposed Joint Energy Model (JEM), where we match the original performance with faster and stable training. This allows us to extend JEM models to semi-supervised classification on tabular data from a variety of continuous domains.", "one-sentence_summary": "We present a new generator-based approach for training EBMs and demonstrate that it trains models which obtain high likelihood and overcomes stability issues common in EBM training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grathwohl|no_mcmc_for_me_amortized_sampling_for_fast_and_stable_training_of_energybased_models", "supplementary_material": "", "pdf": "/pdf/4bf06b7c43a66250e22fbdad3d0f492a3293efc6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngrathwohl2021no,\ntitle={No {\\{}MCMC{\\}} for me: Amortized sampling for fast and stable training of energy-based models},\nauthor={Will Sussman Grathwohl and Jacob Jin Kelly and Milad Hashemi and Mohammad Norouzi and Kevin Swersky and David Duvenaud},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ixpSxO9flk3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ixpSxO9flk3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2046/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2046/Authors|ICLR.cc/2021/Conference/Paper2046/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852884, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment"}}}, {"id": "4CxO1xrfs9K", "original": null, "number": 6, "cdate": 1605341062307, "ddate": null, "tcdate": 1605341062307, "tmdate": 1605341062307, "tddate": null, "forum": "ixpSxO9flk3", "replyto": "C4VuDibwoD", "invitation": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment", "content": {"title": "In defense of the authors", "comment": "Thanks for bringing additional related work to our attention. I agree with Feng Shi and Yang Lu that previous work on cooperative training is relevant and I think it probably should be included as a baseline for experimental comparison. \n\nThere are several points, however, that I do not fully agree with. \n\nFirst, it is pretty clear that this paper did NOT claim to be the **first one** that \"uses a generator as amortized sampling for fast training of EBMs\". Authors acknowledged that they use the same objective as in Kumar et al. (2019), Dai et al. (2019), which both train a generator to amortize sampling for EBM training. The major contribution of this paper is another method for optimizing the dual objective, not (and the authors never claimed) the idea of amortized neural generators for EBM training. On that note, authors should probably also cite Kim & Bengio (2016) and Zhai et al. (2016), which are even more relevant than CoopNet.\n\nSecond, CoopNet is not the first paper that proposes the idea of amortized sampling for EBM training. As far as I know, Kim & Bengio (2016) seems to be the earlier one that tries to accelerate EBM training with a neural generator. For this reason, I don't think it makes sense to name every related method \"xxxx_CoopNet\". Even if CoopNet was the earliest, such a naming requirement seems very weird to me.\n\nFinally, the method in this work is sufficiently different from CoopNet, and has its own novelty. CoopNet essentially uses two separate objectives for training the EBM and the generator, while this work (and previous ones on the dual form of likelihood) has a unified objective. In addition, CoopNet still needs multiple steps of Langevin MCMC despite having a generator, which is also different from this paper.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2046/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "No MCMC for me: Amortized sampling for fast and stable training of energy-based models", "authorids": ["~Will_Sussman_Grathwohl2", "jacob.jin.kelly@gmail.com", "~Milad_Hashemi1", "~Mohammad_Norouzi1", "~Kevin_Swersky1", "~David_Duvenaud2"], "authors": ["Will Sussman Grathwohl", "Jacob Jin Kelly", "Milad Hashemi", "Mohammad Norouzi", "Kevin Swersky", "David Duvenaud"], "keywords": ["Generative Models", "EBM", "Energy-Based Models", "Energy Based Models", "semi-supervised learning", "JEM"], "abstract": "Energy-Based Models (EBMs) present a flexible and appealing way to represent uncertainty. Despite recent advances, training EBMs on high-dimensional data remains a challenging problem as the state-of-the-art approaches are costly, unstable, and require considerable tuning and domain expertise to apply successfully. In this work, we present a simple method for training EBMs at scale which uses an entropy-regularized generator to amortize the MCMC sampling typically used in EBM training. We improve upon prior MCMC-based entropy regularization methods with a fast variational approximation. We demonstrate the effectiveness of our approach by using it to train tractable likelihood models. Next, we apply our estimator to the recently proposed Joint Energy Model (JEM), where we match the original performance with faster and stable training. This allows us to extend JEM models to semi-supervised classification on tabular data from a variety of continuous domains.", "one-sentence_summary": "We present a new generator-based approach for training EBMs and demonstrate that it trains models which obtain high likelihood and overcomes stability issues common in EBM training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grathwohl|no_mcmc_for_me_amortized_sampling_for_fast_and_stable_training_of_energybased_models", "supplementary_material": "", "pdf": "/pdf/4bf06b7c43a66250e22fbdad3d0f492a3293efc6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngrathwohl2021no,\ntitle={No {\\{}MCMC{\\}} for me: Amortized sampling for fast and stable training of energy-based models},\nauthor={Will Sussman Grathwohl and Jacob Jin Kelly and Milad Hashemi and Mohammad Norouzi and Kevin Swersky and David Duvenaud},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ixpSxO9flk3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ixpSxO9flk3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2046/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2046/Authors|ICLR.cc/2021/Conference/Paper2046/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852884, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment"}}}, {"id": "C4VuDibwoD", "original": null, "number": 2, "cdate": 1605338097562, "ddate": null, "tcdate": 1605338097562, "tmdate": 1605338097562, "tddate": null, "forum": "ixpSxO9flk3", "replyto": "ixpSxO9flk3", "invitation": "ICLR.cc/2021/Conference/Paper2046/-/Public_Comment", "content": {"title": "The current idea has been published as the CoopNets algorithm", "comment": "Dear authors, reviewers, and AC,\n\nThe paper claims that it proposes \u201cusing a generator as amortized sampling for fast training of Energy-based models\u201d as its main contribution. I think this is exactly what the paper \u201ccooperative network (CoopNet)\u201d[1] does.\n\nSpecifically, the cooperative network (CoopNet) in [1] jointly trains two components:  a generator G (the student net), which learns to be a fast sampler for amortizing MCMC, and a descriptor net D (the teacher net), which is an energy-based model to distill the knowledge to the generator.  The generator helps the EBM for MCMC, and the EBM teaches the generator. The cooperative training simultaneously alternates the MLE training of both EBM and the generator. Both models are parameterized by deep neural networks. Such a training scheme is well known as \u201ccooperative learning\u201d.\n\nIt is also in a textbook [2] used in UCLA for graduate course stat 202C:\n\nhttp://www.stat.ucla.edu/~sczhu/Courses/UCLA/Stat_202C/Stat_202C.html\n\nI can share some related pages about the cooperative learning in that book here since the book is not free:  https://bit.ly/mc_book_sample\n\nOther papers using Cooperative Nets include [3], which is learning a conditional generator for amortizing MCMC of a conditional energy-based model for conditional learning.\n\nI believe there are still other related research articles using such a framework in other domains. Therefore, I suspect the novelty of the current paper under review.   \n\nMy concerns and questions are:\n\nTo authors:\n\n(1)    I don\u2019t know why the authors didn\u2019t discuss, cite, or even mention the existing framework. I found that most of the current ConvNet-EBM-related papers only cited prior works from 2019, and totally ignored those papers that first proposed the original model and learning algorithm. This is just like, we will not cite Goodfellow\u2019s 2014 GAN paper anymore, but only cite BigGAN or progressive GAN when we refer to adversarial training algorithms.    \n\n(2)    If the authors were unaware of the work I mentioned above before, then given the fact that your current framework belongs to the existing cooperative training framework or its variant. I suggest the authors call your method \u201cxxx_CoopNets\u201d. Re-naming an existing framework is not encouraged. This is just like, we re-name GAN with other names. \n\n\nTo reviewers and AC:\n\n(1)    Given the fact that the idea in the current paper bears a strong similarity with the CoopNets paper [1], I think you might need to re-evaluate the novelty of the paper.\n\n(2)    If the paper is finally accepted for some reason, do you think the paper should use the name that has been used in the original paper [1] or re-name everything as a new one? This is a very general question that frequently happens to any of us. Because this is about how we should grant credits to the prior works. \n  \nReferences:\n[1] Xie et al. Cooperative Training of Descriptor and Generator Networks. (TPAMI) 2018.\nhttps://arxiv.org/pdf/1609.09408.pdf\n\n[2] Adrian Barbu, Song-Chun Zhu. Monte Carlo Methods. Springer 2020.\nhttps://bit.ly/mc_book_sample\n\n[3] Xie et al. Cooperative Training of Fast Thinking Initializer and Slow Thinking Solver for Multi-Modal Conditional Learning. https://arxiv.org/pdf/1902.02812.pdf"}, "signatures": ["~Feng_Shi1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Feng_Shi1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "No MCMC for me: Amortized sampling for fast and stable training of energy-based models", "authorids": ["~Will_Sussman_Grathwohl2", "jacob.jin.kelly@gmail.com", "~Milad_Hashemi1", "~Mohammad_Norouzi1", "~Kevin_Swersky1", "~David_Duvenaud2"], "authors": ["Will Sussman Grathwohl", "Jacob Jin Kelly", "Milad Hashemi", "Mohammad Norouzi", "Kevin Swersky", "David Duvenaud"], "keywords": ["Generative Models", "EBM", "Energy-Based Models", "Energy Based Models", "semi-supervised learning", "JEM"], "abstract": "Energy-Based Models (EBMs) present a flexible and appealing way to represent uncertainty. Despite recent advances, training EBMs on high-dimensional data remains a challenging problem as the state-of-the-art approaches are costly, unstable, and require considerable tuning and domain expertise to apply successfully. In this work, we present a simple method for training EBMs at scale which uses an entropy-regularized generator to amortize the MCMC sampling typically used in EBM training. We improve upon prior MCMC-based entropy regularization methods with a fast variational approximation. We demonstrate the effectiveness of our approach by using it to train tractable likelihood models. Next, we apply our estimator to the recently proposed Joint Energy Model (JEM), where we match the original performance with faster and stable training. This allows us to extend JEM models to semi-supervised classification on tabular data from a variety of continuous domains.", "one-sentence_summary": "We present a new generator-based approach for training EBMs and demonstrate that it trains models which obtain high likelihood and overcomes stability issues common in EBM training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grathwohl|no_mcmc_for_me_amortized_sampling_for_fast_and_stable_training_of_energybased_models", "supplementary_material": "", "pdf": "/pdf/4bf06b7c43a66250e22fbdad3d0f492a3293efc6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngrathwohl2021no,\ntitle={No {\\{}MCMC{\\}} for me: Amortized sampling for fast and stable training of energy-based models},\nauthor={Will Sussman Grathwohl and Jacob Jin Kelly and Milad Hashemi and Mohammad Norouzi and Kevin Swersky and David Duvenaud},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ixpSxO9flk3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ixpSxO9flk3", "readers": {"description": "User groups that will be able to read this comment.", "values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed."}}, "expdate": 1605630600000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Authors", "ICLR.cc/2021/Conference/Paper2046/Reviewers", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1605024966444, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2046/-/Public_Comment"}}}, {"id": "Bcm_1YymGY2", "original": null, "number": 5, "cdate": 1605238074859, "ddate": null, "tcdate": 1605238074859, "tmdate": 1605238074859, "tddate": null, "forum": "ixpSxO9flk3", "replyto": "KJEb6ZD1cHv", "invitation": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment", "content": {"title": "Thank you for the constructive feedback", "comment": "We thank you for your time and your thoughtful comments on our work. We will address your concerns in order. \n\nRegarding the variance of the importance sampling estimator:\n\nThese are important concerns you raise. You say in your review that $\\xi$ is a sample from a zero-mean Gaussian distribution and this would be a bad proposal distribution for importance sampling from $q(z|x)$. We agree that if we were using such an uninformed proposal distribution for importance sampling, then the variance of the estimator would be very high. However, as described in the text directly following equation 8, this is not the proposal distribution we use. We construct an informed proposal distribution that closely approximates $q(z|x)$ and this informed proposal is a key contribution of our work. We elaborate more below:\n\nWhen generating a sample $x \\sim q(x)$, we first sample $z \\sim q(z)$ and then $x \\sim q(x|z)$. The sample $z$ was thrown out in most previous approaches, but we do not do so. Knowing the $z$ which generated x allows us to build a much better proposal distribution. Namely, this $z$  by construction is a sample from the true posterior $q(z|x)$ and because of the smoothness of the generator network, the mass of $q(z|x)$ should concentrate near $z$. This same observation was made in [1] to motivate their use of only a few HMC samples to regularize the generator\u2019s entropy. In our work, we let our proposal distribution be a Gaussian centered around $z$ and we further refine this proposal to maximize the ELBO which allows us to learn the dimension-wise scale of the proposal. \n\nThus our proposal distribution is not a sample from a zero-mean Gaussian. It is a sample from a Gaussian centered around a true posterior sample whose dimension-wise scale is tuned to have minimal KL-divergence from the true posterior. To make this clearer, we have moved the definition of $\\xi(z|z_0)$ out of the text into its own equation. \n\nIn practice, we find importance sampling using this proposal performs very well (Table 2), even (slightly) out-performing HMC based entropy regularization. We attribute this improvement to a reduction in bias over the HMC entropy regularizer (Figure 3). We believe this gap in performance would likely close if more HMC steps were taken at training time but the number of samples used in our experiments (referring to figure 3, we used 2 burn-in steps) resulted in training which was 20x slower than our method (as mentioned in the paper).\n\nThis massive increase in runtime is why we did not run the HMC baseline during our large-scale JEM experiments. The cost would have been prohibitive with the Resnet generator architectures we used. \n\nDespite all of this, we agree that the variance of the estimator is of just as much importance as the bias as analyzed in section 5.2. For that reason, we have added additional results to this section which analyze the variance of the estimator applied to the Factor Analysis model shown in Figure 3. We compare this variance with the HMC estimator. We find the stddev of our estimator is approximately 4x higher than the HMC estimator but the bias is approximately \u2155 the size of the HMC estimator (Figure 3). Empirically we find this increased variance to be alleviated by mini-batch averaging and the decreased bias results in better model performance. We also compute the effective sample size (ESS) of the importance sampling estimators. We find that the ESS of VERA is 1.31 for our CIFAR10 model and 1.29 for our MNIST model with the 20 importance samples we use in training, indicating importance sampling is giving informative gradient estimates. Using a $\\mathcal{N}(0, 1)$ proposal gives an ESS of 1.0 (minimum possible value) for both MNIST and CIFAR10 models, indicating the uninformed proposal distribution gives uninformative gradient estimates (as expected). \n\nRegarding comparisons to experiments in SSM:\n\nWe are confused by this question. Our experimental setup for section 5.1 exactly follows [2]. We stated this in Appendix B.3, but have also updated the paper to state this in the main body as well. The only difference is that we train our models for more epochs because of the reduced learning rates we use. We re-ran some of the SSM experiments as well to follow this number of training epochs but found them to obtain their best performance (in terms of likelihood) very early in training, so this change did not impact the results. \n\nThe focus of this work was on large-scale EBM training so we do not feel that other experiments from the SSM paper are relevant to our method. \n\nImage size in the appendix:\n\nWe have increased the size of these images.\n\nWe hope this response and the changes we made have cleared up some of your concerns about our work. Thank you for your time. \n\n[1] Dieng, Adji B., et al. \"Prescribed generative adversarial networks.\"\n\n[2] Song, Yang, et al. \"Sliced score matching: A scalable approach to density and score estimation.\"\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2046/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "No MCMC for me: Amortized sampling for fast and stable training of energy-based models", "authorids": ["~Will_Sussman_Grathwohl2", "jacob.jin.kelly@gmail.com", "~Milad_Hashemi1", "~Mohammad_Norouzi1", "~Kevin_Swersky1", "~David_Duvenaud2"], "authors": ["Will Sussman Grathwohl", "Jacob Jin Kelly", "Milad Hashemi", "Mohammad Norouzi", "Kevin Swersky", "David Duvenaud"], "keywords": ["Generative Models", "EBM", "Energy-Based Models", "Energy Based Models", "semi-supervised learning", "JEM"], "abstract": "Energy-Based Models (EBMs) present a flexible and appealing way to represent uncertainty. Despite recent advances, training EBMs on high-dimensional data remains a challenging problem as the state-of-the-art approaches are costly, unstable, and require considerable tuning and domain expertise to apply successfully. In this work, we present a simple method for training EBMs at scale which uses an entropy-regularized generator to amortize the MCMC sampling typically used in EBM training. We improve upon prior MCMC-based entropy regularization methods with a fast variational approximation. We demonstrate the effectiveness of our approach by using it to train tractable likelihood models. Next, we apply our estimator to the recently proposed Joint Energy Model (JEM), where we match the original performance with faster and stable training. This allows us to extend JEM models to semi-supervised classification on tabular data from a variety of continuous domains.", "one-sentence_summary": "We present a new generator-based approach for training EBMs and demonstrate that it trains models which obtain high likelihood and overcomes stability issues common in EBM training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grathwohl|no_mcmc_for_me_amortized_sampling_for_fast_and_stable_training_of_energybased_models", "supplementary_material": "", "pdf": "/pdf/4bf06b7c43a66250e22fbdad3d0f492a3293efc6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngrathwohl2021no,\ntitle={No {\\{}MCMC{\\}} for me: Amortized sampling for fast and stable training of energy-based models},\nauthor={Will Sussman Grathwohl and Jacob Jin Kelly and Milad Hashemi and Mohammad Norouzi and Kevin Swersky and David Duvenaud},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ixpSxO9flk3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ixpSxO9flk3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2046/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2046/Authors|ICLR.cc/2021/Conference/Paper2046/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852884, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment"}}}, {"id": "zviddUYs0L_", "original": null, "number": 2, "cdate": 1605236034570, "ddate": null, "tcdate": 1605236034570, "tmdate": 1605237533109, "tddate": null, "forum": "ixpSxO9flk3", "replyto": "d5eBVKdgkuH", "invitation": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment", "content": {"title": "Thank you for the constructive feedback", "comment": "We thank you for your time and your kind words about our work. We will address your concerns in order.\n\nRegarding hyper-parameters:\n\nWhile we agree our approach introduces a new set of hyper-parameters to be tuned, we disagree that our approach \u201cadds\u201d hyper-parameters. In fact, our approach has fewer hyper-parameters than MCMC-based EBM training methods and these hyper-parameters can be set using common-sense principles.\n\nAs you point out, the main hyper-parameters to tune are the strength of the entropy regularizer $\\lambda$ and the strength of the gradient regularizer $\\gamma$. \n\nWe compare this with the PCD training used in the original JEM work [1]. In PCD training with SGLD you must specify each of: the SGLD step-size, number of MCMC steps, SGLD noise variance, buffer size, and reinitialization frequency. So PCD has 3 more hyper-parameters than VERA.\n\nFurther, we add that the gradient regularizer strength was chosen early in our experiments and not searched over. We found this choice worked well across multiple data domains and model sizes, so we recommend setting $\\gamma = .1$ and ignoring this when searching hyper-parameters. \n\nFurther still, a simple analysis can verify that the strength of the entropy regularizer $\\lambda$ has the effect of tempering the energy function. This is analogous to decoupling the step-size in SGLD from the noise variance as is common in EBM training [1, 2, 3, 4]. This has the effect of increasing the impact of the gradient of the energy function in generator training (or in sampling for PCD) which makes learning more efficient at the cost of sample quality. As in PCD, this can be tuned to be as large as possible while keeping training stable. We have added a section (B.1.1) to the appendix to explain this connection. \n\nWe also find that training with VERA allows us to remove the Gaussian noise typically added to the input data to stabilize EBM training. The scale of this noise is another important hyper-parameter to set [1, 2] and it can be removed completely when training with VERA.\n\nSo to summarize, VERA has fewer hyper-parameters than PCD training, one parameter does not need to be searched over, the other parameter can be tuned in a common-sense fashion, and we can completely remove a pre-preprocessing step (which has its own hyper-parameters) that was previously required for stable EBM training. \n\nThus we feel that the number of hyper-parameters of VERA is actually a strength of our method instead of a weakness. \n\n\nRegarding MEG in the JEM experiments:\n\nAs you can see in section 5.1 the results of MEG are near identical to training with no entropy regularization at all. Further, our experiments on mode-capturing demonstrate that entropy regularization is not necessary to achieve strong performance at this task on MNIST-sized data. Thus, we feel that the MEG entropy regularizer is not responsible for the published performance at this task (and no baseline without their regularizer was reported in the original work on MEG). This, combined with the results from 5.1 led us to believe this approach to entropy regularization would not be competitive in high dimensions so it was left out of our comparisons. \n\nRegarding Adversarial Dynamics Embedding in NICE and JEM experiments:\n\nWe did not compare against this method as it was complex, not fully described in their paper, and the code provided was not easy to use. \n\nRegarding the citation for MI estimation:\n\nWe were not aware of this work. It is quite insightful, and we have added a reference to the updated paper.\n\n[1] Grathwohl, Will, et al. \"Your classifier is secretly an energy based model and you should treat it like one.\"\n\n[2] Nijkamp, Erik, et al. \"Learning non-convergent non-persistent short-run MCMC toward energy-based model.\"\n\n[3] Nijkamp, Erik, et al. \"On the anatomy of mcmc-based maximum likelihood learning of energy-based models.\"\n\n[4] Du, Yilun, and Igor Mordatch. \"Implicit generation and generalization in energy-based models.\"\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2046/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "No MCMC for me: Amortized sampling for fast and stable training of energy-based models", "authorids": ["~Will_Sussman_Grathwohl2", "jacob.jin.kelly@gmail.com", "~Milad_Hashemi1", "~Mohammad_Norouzi1", "~Kevin_Swersky1", "~David_Duvenaud2"], "authors": ["Will Sussman Grathwohl", "Jacob Jin Kelly", "Milad Hashemi", "Mohammad Norouzi", "Kevin Swersky", "David Duvenaud"], "keywords": ["Generative Models", "EBM", "Energy-Based Models", "Energy Based Models", "semi-supervised learning", "JEM"], "abstract": "Energy-Based Models (EBMs) present a flexible and appealing way to represent uncertainty. Despite recent advances, training EBMs on high-dimensional data remains a challenging problem as the state-of-the-art approaches are costly, unstable, and require considerable tuning and domain expertise to apply successfully. In this work, we present a simple method for training EBMs at scale which uses an entropy-regularized generator to amortize the MCMC sampling typically used in EBM training. We improve upon prior MCMC-based entropy regularization methods with a fast variational approximation. We demonstrate the effectiveness of our approach by using it to train tractable likelihood models. Next, we apply our estimator to the recently proposed Joint Energy Model (JEM), where we match the original performance with faster and stable training. This allows us to extend JEM models to semi-supervised classification on tabular data from a variety of continuous domains.", "one-sentence_summary": "We present a new generator-based approach for training EBMs and demonstrate that it trains models which obtain high likelihood and overcomes stability issues common in EBM training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grathwohl|no_mcmc_for_me_amortized_sampling_for_fast_and_stable_training_of_energybased_models", "supplementary_material": "", "pdf": "/pdf/4bf06b7c43a66250e22fbdad3d0f492a3293efc6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngrathwohl2021no,\ntitle={No {\\{}MCMC{\\}} for me: Amortized sampling for fast and stable training of energy-based models},\nauthor={Will Sussman Grathwohl and Jacob Jin Kelly and Milad Hashemi and Mohammad Norouzi and Kevin Swersky and David Duvenaud},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ixpSxO9flk3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ixpSxO9flk3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2046/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2046/Authors|ICLR.cc/2021/Conference/Paper2046/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852884, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment"}}}, {"id": "u_qwz5v4_K", "original": null, "number": 4, "cdate": 1605237009148, "ddate": null, "tcdate": 1605237009148, "tmdate": 1605237516186, "tddate": null, "forum": "ixpSxO9flk3", "replyto": "RTSn79LccZP", "invitation": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment", "content": {"title": "Thank you for the constructive feedback (2/2) ", "comment": "\nYour questions:\n\n1) The issue with Eq 6 is that this entropy estimator requires the score function of q(x). Unfortunately, q(x) is a latent-variable model and the score function cannot be evaluated easily. The issue with the score function estimator in Eq 7 is that it requires posterior samples which also cannot be easily computed and we must resort to MCMC or VI techniques to approximate the expectation in Eq 7.\n\n2) We define how the importance weights are computed in the text after Eq 8. There, you can see q(x) is not used in computing the importance weights. We include q(x) in the derivation since this is how the posterior can be defined using Bayes rule. \n\n3) We intended x_g to refer to a mini-batch of samples from the generator. We have updated Algorithm 1 to make this more clear.\n\n4) These samples do not look great because they are exact samples from a NICE model trained in a way that ignores its normalizing constant. This experiment is meant to demonstrate the quality of various training methods that do not require a normalized model. If we did not care about the likelihood we could generate higher quality samples, but this was not the intent of this experiment. The main goal here was to demonstrate that approximate (generator) samples from VERA matched exact samples much more closely than approximate samples from other training procedures such as PCD and MEG. You can see MNIST samples from our VERA SSL model in Appendix C.2 which are of much higher quality.\n \n5) We agree that flow-based models have their own issues, but the point of this experiment was simply to demonstrate that models trained with VERA achieve higher likelihood than alternative training approaches that do not require a normalized model. Results like this are commonly presented when proposing a new method for training unnormalized models. We typically train a model, whose likelihood can be computed, with our approach and then evaluate the training methods using the true model likelihood. Previously linear ICA was the preferred model [3, 4, 5, 6], but normalizing flows present a more interesting class of models to explore since, unlike linear ICA, they are capable of accurately modeling complex, high dimensional data such as images. [7] evaluates their training method using these models and we felt it useful to train these models with VERA since this is the most challenging model and dataset we were aware of where these results have been benchmarked. \n\n6) We have elaborated on our SSL results above. \n\nYour additional notes: \n\n\u201cI would not compare Eq. 5 with a GAN decoder, as GANs define an implicit density.\u201d\n\nWe have removed this sentence.\n\n\u201cNot sure if it\u2019s fair to conclude that entropy regularization is unnecessary for preventing mode collapse from this experiment. This seems like a limited setting.\u201d\n\nWe\u2019ve qualified this claim with \u201cin this setting\u201d.\n\nThank you for pointing out these minor inaccuracies. We agree with all of them and have updated the paper accordingly.\n\nWith regards to your comment on reproducibility, here is an anonymized Google Drive link to our code: https://drive.google.com/drive/folders/1y1lBAtt5fzp4IgZ4URuYmUgRHCIBGag7?usp=sharing\n\n[1] Grathwohl, Will, et al. \"Your classifier is secretly an energy based model and you should treat it like one.\" \n\n[2] Kingma, Durk P., and Prafulla Dhariwal. \"Glow: Generative flow with invertible 1x1 convolutions.\"\n\n[3] Hyv\u00e4rinen, Aapo. \"Estimation of non-normalized statistical models by score matching.\"\n\n[4] Gutmann, Michael, and Aapo Hyv\u00e4rinen. \"Noise-contrastive estimation: A new estimation principle for unnormalized statistical models.\"\n\n[5] Ceylan, Ciwan, and Michael U. Gutmann. \"Conditional noise-contrastive estimation of unnormalised models.\"\n\n[6] Grathwohl, Will, et al. \"Learning the Stein Discrepancy for Training and Evaluating Energy-Based Models without Sampling\"\n\n[7] Song, Yang, et al. \"Sliced score matching: A scalable approach to density and score estimation.\""}, "signatures": ["ICLR.cc/2021/Conference/Paper2046/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "No MCMC for me: Amortized sampling for fast and stable training of energy-based models", "authorids": ["~Will_Sussman_Grathwohl2", "jacob.jin.kelly@gmail.com", "~Milad_Hashemi1", "~Mohammad_Norouzi1", "~Kevin_Swersky1", "~David_Duvenaud2"], "authors": ["Will Sussman Grathwohl", "Jacob Jin Kelly", "Milad Hashemi", "Mohammad Norouzi", "Kevin Swersky", "David Duvenaud"], "keywords": ["Generative Models", "EBM", "Energy-Based Models", "Energy Based Models", "semi-supervised learning", "JEM"], "abstract": "Energy-Based Models (EBMs) present a flexible and appealing way to represent uncertainty. Despite recent advances, training EBMs on high-dimensional data remains a challenging problem as the state-of-the-art approaches are costly, unstable, and require considerable tuning and domain expertise to apply successfully. In this work, we present a simple method for training EBMs at scale which uses an entropy-regularized generator to amortize the MCMC sampling typically used in EBM training. We improve upon prior MCMC-based entropy regularization methods with a fast variational approximation. We demonstrate the effectiveness of our approach by using it to train tractable likelihood models. Next, we apply our estimator to the recently proposed Joint Energy Model (JEM), where we match the original performance with faster and stable training. This allows us to extend JEM models to semi-supervised classification on tabular data from a variety of continuous domains.", "one-sentence_summary": "We present a new generator-based approach for training EBMs and demonstrate that it trains models which obtain high likelihood and overcomes stability issues common in EBM training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grathwohl|no_mcmc_for_me_amortized_sampling_for_fast_and_stable_training_of_energybased_models", "supplementary_material": "", "pdf": "/pdf/4bf06b7c43a66250e22fbdad3d0f492a3293efc6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngrathwohl2021no,\ntitle={No {\\{}MCMC{\\}} for me: Amortized sampling for fast and stable training of energy-based models},\nauthor={Will Sussman Grathwohl and Jacob Jin Kelly and Milad Hashemi and Mohammad Norouzi and Kevin Swersky and David Duvenaud},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ixpSxO9flk3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ixpSxO9flk3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2046/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2046/Authors|ICLR.cc/2021/Conference/Paper2046/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852884, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment"}}}, {"id": "RTSn79LccZP", "original": null, "number": 3, "cdate": 1605236943044, "ddate": null, "tcdate": 1605236943044, "tmdate": 1605236943044, "tddate": null, "forum": "ixpSxO9flk3", "replyto": "IDPXKRxmg9x", "invitation": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment", "content": {"title": "Thank you for the constructive feedback (1/2)", "comment": "We thank you for your very comprehensive, thoughtful review, and for your kind words about our work.\n\nWe will address your concerns in order:\n\nRegarding novelty: \n\nWe have developed a novel entropy regularizer for latent-variable models which is considerably faster than prior approaches, does not require the training of auxiliary MI or score networks, and outperforms prior approaches for training large scale EBMs. Our proposed training procedure is several times faster than the state-of-the-art EBM training methods and alleviates the notorious stability issues found in these state-of-the-art approaches [1]. These stability improvements allow EBMs to be applied to domains where MCMC-based training fails (section 6.1) and to perform well at new applications. We believe our work provides a novel contribution to the field. \n\nUse of flows or AR models:\n\nAutoregressive models are not useful in this setting since sampling comes at an O(D) cost and we must sample at every training iteration. The cost of sampling would dominate training. Flows are not favorable since they are much less parameter efficient and produce lower quality samples than the latent-variable models we use. The CIFAR10 Glow [2] model has approximately 100x as many parameters as our Resnet generator and is much slower to sample from. Despite this, it achieves much lower sample quality (in terms of FID). For these reasons, we felt it valuable to address the difficulties that arise when using latent-variable generators -- which lead to this work. \n\nRegarding the approximations:\n\nIn section 5.1 we explicitly state that our proposed optimization procedure maximizes an upper bound on likelihood. The objective is an upper bound exactly due to the \u201csampling gap\u201d you bring up. The results in this section were specifically added to the paper to demonstrate that while we are indeed maximizing an upper bound, the bound is tight enough to train high-quality models at scale. To make this more clear we have added a discussion of this under equation 4 in the updated paper. We hope this clarifies your concern.\n\nRegarding your point about using MCMC to generate samples:\n\nThe MCMC refinement procedure is there only to improve sample quality by making the generator samples match the distribution defined by the EBM more closely. This is not necessary and samples taken only from the generator are of high quality and competitive -- but of slightly lower quality. This can be seen in Appendix C.3.\n\nRegarding Improvements over JEM:\n\nThe main intent of this paper was to present a new method for training EBMs which is faster and more stable than MCMC-based training. We chose to apply this to JEM as JEM is a new and effective application of EBMs. Training JEM as proposed in [1] is slow to train and unstable ([1], Sections 6, H.3). The main point of these experiments was to demonstrate that we can match the performance of JEM while training much faster and relieving the instability that is common with PCD training of JEM models. The fact that we improve upon the results is even better, but we feel our point would have been made even if the results were identical to the original work on JEM.\n\nPCD training of EBMs has made much progress but almost all research in this area has focused on image models. Little attention has been paid to alternate domains. As we believed our approach is more stable and easier to tune than PCD training we wanted to focus on training models outside of the image domain. This is why we focused on a diverse range of non-image datasets for our SSL experiments. Note that SSL outside of the image domain can be especially challenging because we can\u2019t rely on strong prior knowledge of useful data augmentations. Our VERA-trained JEM models achieved strong results at SSL whereas our PCD training JEM models all performed poorly despite a considerable hyper-parameter search. We believe VERA\u2019s improved performance in this setting is due to its stability and ease of tuning. We believe it is likely that a PCD trained model could achieve similar performance but we were not able to train one to convergence.\n\nWe chose not to compare with MEG in this setting because of our results in section 5.1 which demonstrated that their entropy regularizer had little-to-no effect on MNIST sized data. \n\n----continued below------"}, "signatures": ["ICLR.cc/2021/Conference/Paper2046/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "No MCMC for me: Amortized sampling for fast and stable training of energy-based models", "authorids": ["~Will_Sussman_Grathwohl2", "jacob.jin.kelly@gmail.com", "~Milad_Hashemi1", "~Mohammad_Norouzi1", "~Kevin_Swersky1", "~David_Duvenaud2"], "authors": ["Will Sussman Grathwohl", "Jacob Jin Kelly", "Milad Hashemi", "Mohammad Norouzi", "Kevin Swersky", "David Duvenaud"], "keywords": ["Generative Models", "EBM", "Energy-Based Models", "Energy Based Models", "semi-supervised learning", "JEM"], "abstract": "Energy-Based Models (EBMs) present a flexible and appealing way to represent uncertainty. Despite recent advances, training EBMs on high-dimensional data remains a challenging problem as the state-of-the-art approaches are costly, unstable, and require considerable tuning and domain expertise to apply successfully. In this work, we present a simple method for training EBMs at scale which uses an entropy-regularized generator to amortize the MCMC sampling typically used in EBM training. We improve upon prior MCMC-based entropy regularization methods with a fast variational approximation. We demonstrate the effectiveness of our approach by using it to train tractable likelihood models. Next, we apply our estimator to the recently proposed Joint Energy Model (JEM), where we match the original performance with faster and stable training. This allows us to extend JEM models to semi-supervised classification on tabular data from a variety of continuous domains.", "one-sentence_summary": "We present a new generator-based approach for training EBMs and demonstrate that it trains models which obtain high likelihood and overcomes stability issues common in EBM training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grathwohl|no_mcmc_for_me_amortized_sampling_for_fast_and_stable_training_of_energybased_models", "supplementary_material": "", "pdf": "/pdf/4bf06b7c43a66250e22fbdad3d0f492a3293efc6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngrathwohl2021no,\ntitle={No {\\{}MCMC{\\}} for me: Amortized sampling for fast and stable training of energy-based models},\nauthor={Will Sussman Grathwohl and Jacob Jin Kelly and Milad Hashemi and Mohammad Norouzi and Kevin Swersky and David Duvenaud},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ixpSxO9flk3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ixpSxO9flk3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2046/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2046/Authors|ICLR.cc/2021/Conference/Paper2046/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2046/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852884, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2046/-/Official_Comment"}}}], "count": 35}