{"notes": [{"id": "BJxhijAcY7", "original": "r1l5M7q9KX", "number": 658, "cdate": 1538087844014, "ddate": null, "tcdate": 1538087844014, "tmdate": 1550864372772, "tddate": null, "forum": "BJxhijAcY7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "signSGD with Majority Vote is Communication Efficient and Fault Tolerant", "abstract": "Training neural networks on large datasets can be accelerated by distributing the workload over a network of machines. As datasets grow ever larger, networks of hundreds or thousands of machines become economically viable. The time cost of communicating gradients limits the effectiveness of using such large machine counts, as may the increased chance of network faults. We explore a particularly simple algorithm for robust, communication-efficient learning---signSGD. Workers transmit only the sign of their gradient vector to a server, and the overall update is decided by a majority vote. This algorithm uses 32x less communication per iteration than full-precision, distributed SGD. Under natural conditions verified by experiment, we prove that signSGD converges in the large and mini-batch settings, establishing convergence for a parameter regime of Adam as a byproduct. Aggregating sign gradients by majority vote means that no individual worker has too much power. We prove that unlike SGD, majority vote is robust when up to 50% of workers behave adversarially. The class of adversaries we consider includes as special cases those that invert or randomise their gradient estimate. On the practical side, we built our distributed training system in Pytorch. Benchmarking against the state of the art collective communications library (NCCL), our framework---with the parameter server housed entirely on one machine---led to a 25% reduction in time for training resnet50 on Imagenet when using 15 AWS p3.2xlarge machines.", "keywords": ["large-scale learning", "distributed systems", "communication efficiency", "convergence rate analysis", "robust optimisation"], "authorids": ["bernstein@caltech.edu", "jiaweizhao.zjw@qq.com", "kazizzad@uci.edu", "anima@caltech.edu"], "authors": ["Jeremy Bernstein", "Jiawei Zhao", "Kamyar Azizzadenesheli", "Anima Anandkumar"], "TL;DR": "Workers send gradient signs to the server, and the update is decided by majority vote. We show that this algorithm is convergent, communication efficient and fault tolerant, both in theory and in practice.", "pdf": "/pdf/6933f7682283061fe8e760f1b4d92c465e999e64.pdf", "paperhash": "bernstein|signsgd_with_majority_vote_is_communication_efficient_and_fault_tolerant", "_bibtex": "@inproceedings{\nbernstein2018signsgd,\ntitle={sign{SGD} with Majority Vote is Communication Efficient and Fault Tolerant},\nauthor={Jeremy Bernstein and Jiawei Zhao and Kamyar Azizzadenesheli and Anima Anandkumar},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJxhijAcY7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 15, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "SJe3qk6YlN", "original": null, "number": 16, "cdate": 1545355156307, "ddate": null, "tcdate": 1545355156307, "tmdate": 1545380416938, "tddate": null, "forum": "BJxhijAcY7", "replyto": "SkgZHKcaJE", "invitation": "ICLR.cc/2019/Conference/-/Paper658/Official_Comment", "content": {"title": "We tested higher precision QSGD", "comment": "Dear AC and AnonReviewer1,\n\nWe have finished running 2, 4 and 8 bit QSGD. Per iteration, on our CIFAR-10 benchmark, we see:\n\n- max QSGD shows a tiny (insignificant) improvement at higher precision.\n- L2 QSGD shows larger improvement but is still roughly 2x slower than Majority Vote even at 8-bit precision.\n\nTherefore the claims in the paper still stand. We will add these results to the paper."}, "signatures": ["ICLR.cc/2019/Conference/Paper658/Authors"], "readers": ["ICLR.cc/2019/Conference/Paper658/Authors", "everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper658/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper658/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "signSGD with Majority Vote is Communication Efficient and Fault Tolerant", "abstract": "Training neural networks on large datasets can be accelerated by distributing the workload over a network of machines. As datasets grow ever larger, networks of hundreds or thousands of machines become economically viable. The time cost of communicating gradients limits the effectiveness of using such large machine counts, as may the increased chance of network faults. We explore a particularly simple algorithm for robust, communication-efficient learning---signSGD. Workers transmit only the sign of their gradient vector to a server, and the overall update is decided by a majority vote. This algorithm uses 32x less communication per iteration than full-precision, distributed SGD. Under natural conditions verified by experiment, we prove that signSGD converges in the large and mini-batch settings, establishing convergence for a parameter regime of Adam as a byproduct. Aggregating sign gradients by majority vote means that no individual worker has too much power. We prove that unlike SGD, majority vote is robust when up to 50% of workers behave adversarially. The class of adversaries we consider includes as special cases those that invert or randomise their gradient estimate. On the practical side, we built our distributed training system in Pytorch. Benchmarking against the state of the art collective communications library (NCCL), our framework---with the parameter server housed entirely on one machine---led to a 25% reduction in time for training resnet50 on Imagenet when using 15 AWS p3.2xlarge machines.", "keywords": ["large-scale learning", "distributed systems", "communication efficiency", "convergence rate analysis", "robust optimisation"], "authorids": ["bernstein@caltech.edu", "jiaweizhao.zjw@qq.com", "kazizzad@uci.edu", "anima@caltech.edu"], "authors": ["Jeremy Bernstein", "Jiawei Zhao", "Kamyar Azizzadenesheli", "Anima Anandkumar"], "TL;DR": "Workers send gradient signs to the server, and the update is decided by majority vote. We show that this algorithm is convergent, communication efficient and fault tolerant, both in theory and in practice.", "pdf": "/pdf/6933f7682283061fe8e760f1b4d92c465e999e64.pdf", "paperhash": "bernstein|signsgd_with_majority_vote_is_communication_efficient_and_fault_tolerant", "_bibtex": "@inproceedings{\nbernstein2018signsgd,\ntitle={sign{SGD} with Majority Vote is Communication Efficient and Fault Tolerant},\nauthor={Jeremy Bernstein and Jiawei Zhao and Kamyar Azizzadenesheli and Anima Anandkumar},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJxhijAcY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper658/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608966, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJxhijAcY7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper658/Authors", "ICLR.cc/2019/Conference/Paper658/Reviewers", "ICLR.cc/2019/Conference/Paper658/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper658/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper658/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper658/Authors|ICLR.cc/2019/Conference/Paper658/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper658/Reviewers", "ICLR.cc/2019/Conference/Paper658/Authors", "ICLR.cc/2019/Conference/Paper658/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608966}}}, {"id": "Bylc8ckyxN", "original": null, "number": 1, "cdate": 1544645201990, "ddate": null, "tcdate": 1544645201990, "tmdate": 1545354485174, "tddate": null, "forum": "BJxhijAcY7", "replyto": "BJxhijAcY7", "invitation": "ICLR.cc/2019/Conference/-/Paper658/Meta_Review", "content": {"metareview": "The Reviewers noticed that the paper undergone many editions and raise concern about the content. They encourage improving experimental section further and strengthening the message of the paper. ", "confidence": "5: The area chair is absolutely certain", "recommendation": "Accept (Poster)", "title": "Good paper but requires revisions"}, "signatures": ["ICLR.cc/2019/Conference/Paper658/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper658/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "signSGD with Majority Vote is Communication Efficient and Fault Tolerant", "abstract": "Training neural networks on large datasets can be accelerated by distributing the workload over a network of machines. As datasets grow ever larger, networks of hundreds or thousands of machines become economically viable. The time cost of communicating gradients limits the effectiveness of using such large machine counts, as may the increased chance of network faults. We explore a particularly simple algorithm for robust, communication-efficient learning---signSGD. Workers transmit only the sign of their gradient vector to a server, and the overall update is decided by a majority vote. This algorithm uses 32x less communication per iteration than full-precision, distributed SGD. Under natural conditions verified by experiment, we prove that signSGD converges in the large and mini-batch settings, establishing convergence for a parameter regime of Adam as a byproduct. Aggregating sign gradients by majority vote means that no individual worker has too much power. We prove that unlike SGD, majority vote is robust when up to 50% of workers behave adversarially. The class of adversaries we consider includes as special cases those that invert or randomise their gradient estimate. On the practical side, we built our distributed training system in Pytorch. Benchmarking against the state of the art collective communications library (NCCL), our framework---with the parameter server housed entirely on one machine---led to a 25% reduction in time for training resnet50 on Imagenet when using 15 AWS p3.2xlarge machines.", "keywords": ["large-scale learning", "distributed systems", "communication efficiency", "convergence rate analysis", "robust optimisation"], "authorids": ["bernstein@caltech.edu", "jiaweizhao.zjw@qq.com", "kazizzad@uci.edu", "anima@caltech.edu"], "authors": ["Jeremy Bernstein", "Jiawei Zhao", "Kamyar Azizzadenesheli", "Anima Anandkumar"], "TL;DR": "Workers send gradient signs to the server, and the update is decided by majority vote. We show that this algorithm is convergent, communication efficient and fault tolerant, both in theory and in practice.", "pdf": "/pdf/6933f7682283061fe8e760f1b4d92c465e999e64.pdf", "paperhash": "bernstein|signsgd_with_majority_vote_is_communication_efficient_and_fault_tolerant", "_bibtex": "@inproceedings{\nbernstein2018signsgd,\ntitle={sign{SGD} with Majority Vote is Communication Efficient and Fault Tolerant},\nauthor={Jeremy Bernstein and Jiawei Zhao and Kamyar Azizzadenesheli and Anima Anandkumar},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJxhijAcY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper658/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353135391, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJxhijAcY7", "replyto": "BJxhijAcY7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper658/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper658/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper658/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353135391}}}, {"id": "SkgZHKcaJE", "original": null, "number": 15, "cdate": 1544558904944, "ddate": null, "tcdate": 1544558904944, "tmdate": 1544558904944, "tddate": null, "forum": "BJxhijAcY7", "replyto": "BJxhijAcY7", "invitation": "ICLR.cc/2019/Conference/-/Paper658/Official_Comment", "content": {"title": "Message to AC and AnonReviewer1", "comment": "Dear AC and AnonReviewer1,\n\nThe reviewers\u2019 scores show a consensus to accept. Still, AnonReviewer1 raises important points that we want to address here.\n\n1. QSGD precision. We agree, thanks for pointing it out. We are running experiments on 2 and 4bit QSGD and will add these to the paper.\n\n2. Bulyan. We disagree. We believe this comparison is unnecessary for the following reasons:\n\u2014\u2014\u2014(A) our comparison with Krum is \u201cgood\u201d\u2014Krum successfully detects and eliminates the adversaries in our experiments. The only drawback of Krum is that is has a requirement for total num workers \u201cn\u201d to exceed num adversaries \u201cf\u201d by n > 2f + 2, therefore for n=7, Krum already breaks down at num adversaries f=3, whereas majority vote still works at f=3.\n\u2014\u2014\u2014(B) Bulyan, on the other hand, only tolerates up to 25% adversaries, requiring n > 4f + 3. For our case of 7 workers this means it only tolerates 1 adversary (f=1). Clearly Bulyan will perform worse than Krum on these experiments.\n\u2014\u2014\u2014(TL;DR) Krum already \u201caces\u201d our experiments, except for the fact it has max security level f=2, therefore we didn\u2019t see the need to compare to Bulyan which only serves to lower the max security level to f=1.\n\u2014\u2014\u2014(extra) There is another drawback of Krum and Bulyan, in that they throw away workers even when there are no adversaries\u2014they have a \u201cparanoid\u201d regime. Majority vote does not do this. But this effect was not visible in our experiments (probably the batch size was too large to see it).\n\nWe therefore see no reason why the paper should not be accepted for this round of submission. In particular we think presenting the small batch theory (Theorem 1) would be an important and timely contribution to the understanding of adaptive gradient methods like Adam, which closely relate to signSGD. The paper may also spur further research into the combination of gradient compression and fault tolerance, which seem like a natural mix for large scale distributed learning.\n\nFinally, we want to thank all the reviewers for their thorough, critical and constructive reviews."}, "signatures": ["ICLR.cc/2019/Conference/Paper658/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper658/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper658/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "signSGD with Majority Vote is Communication Efficient and Fault Tolerant", "abstract": "Training neural networks on large datasets can be accelerated by distributing the workload over a network of machines. As datasets grow ever larger, networks of hundreds or thousands of machines become economically viable. The time cost of communicating gradients limits the effectiveness of using such large machine counts, as may the increased chance of network faults. We explore a particularly simple algorithm for robust, communication-efficient learning---signSGD. Workers transmit only the sign of their gradient vector to a server, and the overall update is decided by a majority vote. This algorithm uses 32x less communication per iteration than full-precision, distributed SGD. Under natural conditions verified by experiment, we prove that signSGD converges in the large and mini-batch settings, establishing convergence for a parameter regime of Adam as a byproduct. Aggregating sign gradients by majority vote means that no individual worker has too much power. We prove that unlike SGD, majority vote is robust when up to 50% of workers behave adversarially. The class of adversaries we consider includes as special cases those that invert or randomise their gradient estimate. On the practical side, we built our distributed training system in Pytorch. Benchmarking against the state of the art collective communications library (NCCL), our framework---with the parameter server housed entirely on one machine---led to a 25% reduction in time for training resnet50 on Imagenet when using 15 AWS p3.2xlarge machines.", "keywords": ["large-scale learning", "distributed systems", "communication efficiency", "convergence rate analysis", "robust optimisation"], "authorids": ["bernstein@caltech.edu", "jiaweizhao.zjw@qq.com", "kazizzad@uci.edu", "anima@caltech.edu"], "authors": ["Jeremy Bernstein", "Jiawei Zhao", "Kamyar Azizzadenesheli", "Anima Anandkumar"], "TL;DR": "Workers send gradient signs to the server, and the update is decided by majority vote. We show that this algorithm is convergent, communication efficient and fault tolerant, both in theory and in practice.", "pdf": "/pdf/6933f7682283061fe8e760f1b4d92c465e999e64.pdf", "paperhash": "bernstein|signsgd_with_majority_vote_is_communication_efficient_and_fault_tolerant", "_bibtex": "@inproceedings{\nbernstein2018signsgd,\ntitle={sign{SGD} with Majority Vote is Communication Efficient and Fault Tolerant},\nauthor={Jeremy Bernstein and Jiawei Zhao and Kamyar Azizzadenesheli and Anima Anandkumar},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJxhijAcY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper658/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608966, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJxhijAcY7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper658/Authors", "ICLR.cc/2019/Conference/Paper658/Reviewers", "ICLR.cc/2019/Conference/Paper658/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper658/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper658/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper658/Authors|ICLR.cc/2019/Conference/Paper658/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper658/Reviewers", "ICLR.cc/2019/Conference/Paper658/Authors", "ICLR.cc/2019/Conference/Paper658/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608966}}}, {"id": "B1eUDnU037", "original": null, "number": 3, "cdate": 1541463133785, "ddate": null, "tcdate": 1541463133785, "tmdate": 1544020278515, "tddate": null, "forum": "BJxhijAcY7", "replyto": "BJxhijAcY7", "invitation": "ICLR.cc/2019/Conference/-/Paper658/Official_Review", "content": {"title": "A distributed implementation of signSGD with majority vote as aggregation. An interesting idea, that however is lacking comparisons with state of the art.", "review": "The authors present a distributed implementation of signSGD with majority vote as aggregation. The result is a  communication efficient and byzantine robust distributed training method. This is an interesting and relevant problem. There are two parts in this paper: first the authors prove a convergence guarantee for signSGD, and then they prove that under a weak adversary attack signSGD will be robust to a constant fraction of adversarial nodes. The authors conclude with some limited experiments.\n\nOverall, the idea of combining low-communication methods with byzantine resilience is quite interesting. That is, by limiting the domain of the gradients one expects that the power of an adversary would be limited too. The application of the majority vote on the gradients is an intuitive technique that can resolve weak adversarial attacks. Overall, I found the premise quite interesting.\n\nThere are several issues that if fixed this could be a great paper, however I am not sure if there is enough time between rebuttals to achieve this for this round of submissions. I will summarize these key issues below.\n\n\n1) Although the authors claim that this is a communication efficient technique, signSGD (on its communication merit) is not compared with any state of the art communication efficient training algorithm, for example:\n- 1Bit SGD [1]\n- QSD [2]\n- TernGrad [3]\n- Deep Gradient compression [4]\nI think it is important to include at least one of those algorithms in a comparison. Due to the lack of comparisons with state of the art it is hard to argue on the relative performance of signSGD.\n\n2) Although the authors claim byzantine resilience, this is against a very weak type of adversary, eg one that only sends back the opposite sign of the local stochastic gradient. An omniscient adversary can craft attacks that are significantly more sophisticated, for which a simple majority vote would not work. Please see the results in [b1].\n\n3) The authors although reference some limited literature on byzantine ML, they do not compare with other byzantine tolerant ML methods. For example check [eg, b1-b4] below. Again, due to the lack of comparisons with state of the art it is hard to argue on the relative performance of signSGD.\n\nOverall, although the presented ideas are promising, a substantial revision is needed before this paper is accepted for publication. I think it is extremely important that an extensive comparison is carried out with respect to both communication efficient algorithms, and/or byzantine tolerant algorithms, since signSGD aims to be competitive with both of these lines of work. This is a paper that has potential, but is currently limited by its lack of appropriate comparisons.\n\n\n\n[1] https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/IS140694.pdf\n[2] https://papers.nips.cc/paper/6768-qsgd-communication-efficient-sgd-via-gradient-quantization-and-encoding.pdf\n[3] https://papers.nips.cc/paper/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning.pdf\n[4] https://arxiv.org/pdf/1712.01887.pdf\n\n[b1] https://arxiv.org/pdf/1802.07927.pdf\n[b2] https://arxiv.org/pdf/1803.01498.pdf\n[b3] https://dl.acm.org/citation.cfm?id=2933105\n[b4] https://arxiv.org/pdf/1804.10140.pdf\n[b5] https://arxiv.org/pdf/1802.10116.pdf\n\n########################\n\nI would like to commend the authors for making a significant effort in revising their manuscript. Specifically, I think adding the experiments for QSGD and Krum are an important addition. However, I still have a few major that in my opinion are significant:\n\n- The experiments for QSGD are only carried for the 1-bit version of the algorithm. It has been well observed that this is by far the least well performing variant of QSGD. That is, 4 or 8 bit QSGD seems to be significantly more accurate for a given time budget. I think the goal of the experiments should not be to compare against other 1-bit algorithms (though to be precise, 1-bit QSGD is a ternary algorithm) , but against the fastest low-communication algorithm. As such, although the authors made an effort in adding more experiments, I am still not convinced that signSGD will be faster than 4 or 8 bit QSGD. I want to also acknowledge in this comment the fact that these experiments do take time, and are not easy to run, so I commend them again for this effort.\n\n- My second comment relates to comparisons with state of the art algorithms in byzantine ML. The authors indeed did compare against Krum, however, as noted in my original review there are many works following Blanchard et al.  \n\nFor example as I noted https://arxiv.org/pdf/1802.07927.pdf (the Bulyan algorithm) shows that there exist significantly stronger defense mechanisms for byzantine attacks. I think it would have been a much stronger comparison to compare with Bulyan.\n\nOverall, I think the paper has good content, and the authors significantly revised their paper according to the reviews. However, several more experiments are needed for convincing a potential reader of the main claims of the paper, i.e., that signSGD is a state of the art communication efficient and byzantine tolerant algorithm. \n\nI will increase my score from 5 to 6, and I will not oppose the paper being rejected or accepted. My personal opinion is that a resubmission for a future venue would yield a much stronger and more convincing paper assuming more extensive and thorough comparisons are added.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper658/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "signSGD with Majority Vote is Communication Efficient and Fault Tolerant", "abstract": "Training neural networks on large datasets can be accelerated by distributing the workload over a network of machines. As datasets grow ever larger, networks of hundreds or thousands of machines become economically viable. The time cost of communicating gradients limits the effectiveness of using such large machine counts, as may the increased chance of network faults. We explore a particularly simple algorithm for robust, communication-efficient learning---signSGD. Workers transmit only the sign of their gradient vector to a server, and the overall update is decided by a majority vote. This algorithm uses 32x less communication per iteration than full-precision, distributed SGD. Under natural conditions verified by experiment, we prove that signSGD converges in the large and mini-batch settings, establishing convergence for a parameter regime of Adam as a byproduct. Aggregating sign gradients by majority vote means that no individual worker has too much power. We prove that unlike SGD, majority vote is robust when up to 50% of workers behave adversarially. The class of adversaries we consider includes as special cases those that invert or randomise their gradient estimate. On the practical side, we built our distributed training system in Pytorch. Benchmarking against the state of the art collective communications library (NCCL), our framework---with the parameter server housed entirely on one machine---led to a 25% reduction in time for training resnet50 on Imagenet when using 15 AWS p3.2xlarge machines.", "keywords": ["large-scale learning", "distributed systems", "communication efficiency", "convergence rate analysis", "robust optimisation"], "authorids": ["bernstein@caltech.edu", "jiaweizhao.zjw@qq.com", "kazizzad@uci.edu", "anima@caltech.edu"], "authors": ["Jeremy Bernstein", "Jiawei Zhao", "Kamyar Azizzadenesheli", "Anima Anandkumar"], "TL;DR": "Workers send gradient signs to the server, and the update is decided by majority vote. We show that this algorithm is convergent, communication efficient and fault tolerant, both in theory and in practice.", "pdf": "/pdf/6933f7682283061fe8e760f1b4d92c465e999e64.pdf", "paperhash": "bernstein|signsgd_with_majority_vote_is_communication_efficient_and_fault_tolerant", "_bibtex": "@inproceedings{\nbernstein2018signsgd,\ntitle={sign{SGD} with Majority Vote is Communication Efficient and Fault Tolerant},\nauthor={Jeremy Bernstein and Jiawei Zhao and Kamyar Azizzadenesheli and Anima Anandkumar},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJxhijAcY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper658/Official_Review", "cdate": 1542234409413, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJxhijAcY7", "replyto": "BJxhijAcY7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper658/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335774060, "tmdate": 1552335774060, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper658/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HkgS6925hX", "original": null, "number": 2, "cdate": 1541225149155, "ddate": null, "tcdate": 1541225149155, "tmdate": 1543718149562, "tddate": null, "forum": "BJxhijAcY7", "replyto": "BJxhijAcY7", "invitation": "ICLR.cc/2019/Conference/-/Paper658/Official_Review", "content": {"title": "good work but can be improved", "review": "This paper continues the study of the signSGD algorithm due to (Balles & Hennig, Bernstein et al), where only the sign of a stochastic gradient is used for updating. There are two main results: (1) a slightly refined analysis of two results in Bernstein et al. The authors proved that signSGD continues to converge at the 1/sqrt(T) rate even with minibatch size 1 (instead of T as in Bernstein et al), if the gradient noise is symmetric and unimodal; (2) a similar convergence rate is obtained even when half of the worker machines flip the sign of their stochastic gradients. These results appear to be relatively straightforward extensions of those in Bernstein et al.\n\nClarity: The paper is mostly nicely written, with some occasionally imprecise claims. \n\nPage 5, right before Remark 1: it is wrongly claimed that signSGD converges to a critical point of the objective. This cannot be inferred from Theorem 1. (If the authors disagree, please give the complete details on how the random sequence x_t converges to some critical point x^*. or perhaps you are using the word \"convergence\" differently from its usual meaning?)\n\nPage 6, after Lemma 1. The authors claimed that \"the bound is elegant since ... even at low SNR we still have ... <= 1/2.\" In my opinion, this is not elegant at all. This is just your symmetric assumption on the noise, nothing more...\n\nEq (1): are you assuming g_i > 0 here? this inequality is false as you need to discuss the two cases. \n\n\"Therefore signSGD cannot converge for these noise distributions, ..... point in the wrong direction.\" This is a claim based on intuitive arguments but not a proven fact. Please refrain from using definitive sentences like this.\n\nFootnote 1: where is the discussion?\n\n\nOriginality: Compared to the existing work of Bernstein et al, the novelty of the current submission is moderate. The main results appear to be relatively straightforward refinements of those in Bernstein. The observation that majority voting is Byzantine fault tolerant is perhaps not very surprising but it is certainly nice to have a formal justification.\n\nQuality: At times this submission feels like half-baked:\n-- The theoretical results are about signSGD while the experiments are about sigNUM\n-- The adversaries must send the negation of the sign? why can't they send an arbitrary bit vector?\n-- From the authors' discussion \" we will include this feature in our open source code release\", \"plan to run more extensive experiments in the immediate future and will update the paper...\", and \"should be possible to extend the result to the mini-batch setting by combining ...\"\n\nSignificance: This paper is certainly a nice addition to our understanding of signSGD. However, the current obtained results are not very significant compared to the existing results: Theorem 1 is a minor refinement of the two results in Bernstein et al, while Theorem 2 at its current form is not very interesting, as it heavily restricts what an adversary worker machine can do. It would be more realistic if the adversaries can send random bits (still non-cooperated though).\n\n\n\n##### added after author response #####\nI appreciate the authors' efforts in trying to improve the draft by incorporating the reviewers' comments. While I do like the authors' continued study of signSGD, the submission has gone through some significant revision (more complete experiments + stronger adversary). ", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper658/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "signSGD with Majority Vote is Communication Efficient and Fault Tolerant", "abstract": "Training neural networks on large datasets can be accelerated by distributing the workload over a network of machines. As datasets grow ever larger, networks of hundreds or thousands of machines become economically viable. The time cost of communicating gradients limits the effectiveness of using such large machine counts, as may the increased chance of network faults. We explore a particularly simple algorithm for robust, communication-efficient learning---signSGD. Workers transmit only the sign of their gradient vector to a server, and the overall update is decided by a majority vote. This algorithm uses 32x less communication per iteration than full-precision, distributed SGD. Under natural conditions verified by experiment, we prove that signSGD converges in the large and mini-batch settings, establishing convergence for a parameter regime of Adam as a byproduct. Aggregating sign gradients by majority vote means that no individual worker has too much power. We prove that unlike SGD, majority vote is robust when up to 50% of workers behave adversarially. The class of adversaries we consider includes as special cases those that invert or randomise their gradient estimate. On the practical side, we built our distributed training system in Pytorch. Benchmarking against the state of the art collective communications library (NCCL), our framework---with the parameter server housed entirely on one machine---led to a 25% reduction in time for training resnet50 on Imagenet when using 15 AWS p3.2xlarge machines.", "keywords": ["large-scale learning", "distributed systems", "communication efficiency", "convergence rate analysis", "robust optimisation"], "authorids": ["bernstein@caltech.edu", "jiaweizhao.zjw@qq.com", "kazizzad@uci.edu", "anima@caltech.edu"], "authors": ["Jeremy Bernstein", "Jiawei Zhao", "Kamyar Azizzadenesheli", "Anima Anandkumar"], "TL;DR": "Workers send gradient signs to the server, and the update is decided by majority vote. We show that this algorithm is convergent, communication efficient and fault tolerant, both in theory and in practice.", "pdf": "/pdf/6933f7682283061fe8e760f1b4d92c465e999e64.pdf", "paperhash": "bernstein|signsgd_with_majority_vote_is_communication_efficient_and_fault_tolerant", "_bibtex": "@inproceedings{\nbernstein2018signsgd,\ntitle={sign{SGD} with Majority Vote is Communication Efficient and Fault Tolerant},\nauthor={Jeremy Bernstein and Jiawei Zhao and Kamyar Azizzadenesheli and Anima Anandkumar},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJxhijAcY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper658/Official_Review", "cdate": 1542234409413, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJxhijAcY7", "replyto": "BJxhijAcY7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper658/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335774060, "tmdate": 1552335774060, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper658/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "r1lMfYylTQ", "original": null, "number": 5, "cdate": 1541564682439, "ddate": null, "tcdate": 1541564682439, "tmdate": 1543288359918, "tddate": null, "forum": "BJxhijAcY7", "replyto": "B1eUDnU037", "invitation": "ICLR.cc/2019/Conference/-/Paper658/Official_Comment", "content": {"title": "Response to AnonReviewer1", "comment": "Dear AnonReviewer1,\n\nThank you for your clear and precise review. We appreciate the comment that our work \u201ccould be a great paper\u201d if we add some comparisons during the rebuttal. We want to contest your take on the weakness of our adversarial model, yet wholeheartedly agree with the need for adequate experimental comparisons to other techniques.\n\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n>>>>>>> Comparison expts >>>>>>\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n\nWe have added comparisons to QSGD (compression) Multi-Krum (Byzantine fault tolerance). Please see the revisions in the post above.\n\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n>>>>>>> Adversarial model >>>>>>\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n\n> the adversary is \u201cvery weak\u201d since it \u201conly sends back the opposite sign of the local stochastic gradient\u201d\nWe have formulated an entire class of adversaries that our algorithm is robust to. Please see our revisions above.\n\nThank you for pointing us to the paper [b1] saying that \u201cconvergence is not enough\u201d since, for example, a powerful adversary can steer convergence to bad local minimisers. This is a great point. For this reason we do not recommend using our algorithm to protect against \u201comniscient\u201d adversaries. But for \u201cmere mortal\u201d adversaries, our results are interesting. An example of a \u201cmere mortal\u201d adversary could be a broken machine that sends random bits or stale gradients."}, "signatures": ["ICLR.cc/2019/Conference/Paper658/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper658/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper658/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "signSGD with Majority Vote is Communication Efficient and Fault Tolerant", "abstract": "Training neural networks on large datasets can be accelerated by distributing the workload over a network of machines. As datasets grow ever larger, networks of hundreds or thousands of machines become economically viable. The time cost of communicating gradients limits the effectiveness of using such large machine counts, as may the increased chance of network faults. We explore a particularly simple algorithm for robust, communication-efficient learning---signSGD. Workers transmit only the sign of their gradient vector to a server, and the overall update is decided by a majority vote. This algorithm uses 32x less communication per iteration than full-precision, distributed SGD. Under natural conditions verified by experiment, we prove that signSGD converges in the large and mini-batch settings, establishing convergence for a parameter regime of Adam as a byproduct. Aggregating sign gradients by majority vote means that no individual worker has too much power. We prove that unlike SGD, majority vote is robust when up to 50% of workers behave adversarially. The class of adversaries we consider includes as special cases those that invert or randomise their gradient estimate. On the practical side, we built our distributed training system in Pytorch. Benchmarking against the state of the art collective communications library (NCCL), our framework---with the parameter server housed entirely on one machine---led to a 25% reduction in time for training resnet50 on Imagenet when using 15 AWS p3.2xlarge machines.", "keywords": ["large-scale learning", "distributed systems", "communication efficiency", "convergence rate analysis", "robust optimisation"], "authorids": ["bernstein@caltech.edu", "jiaweizhao.zjw@qq.com", "kazizzad@uci.edu", "anima@caltech.edu"], "authors": ["Jeremy Bernstein", "Jiawei Zhao", "Kamyar Azizzadenesheli", "Anima Anandkumar"], "TL;DR": "Workers send gradient signs to the server, and the update is decided by majority vote. We show that this algorithm is convergent, communication efficient and fault tolerant, both in theory and in practice.", "pdf": "/pdf/6933f7682283061fe8e760f1b4d92c465e999e64.pdf", "paperhash": "bernstein|signsgd_with_majority_vote_is_communication_efficient_and_fault_tolerant", "_bibtex": "@inproceedings{\nbernstein2018signsgd,\ntitle={sign{SGD} with Majority Vote is Communication Efficient and Fault Tolerant},\nauthor={Jeremy Bernstein and Jiawei Zhao and Kamyar Azizzadenesheli and Anima Anandkumar},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJxhijAcY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper658/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608966, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJxhijAcY7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper658/Authors", "ICLR.cc/2019/Conference/Paper658/Reviewers", "ICLR.cc/2019/Conference/Paper658/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper658/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper658/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper658/Authors|ICLR.cc/2019/Conference/Paper658/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper658/Reviewers", "ICLR.cc/2019/Conference/Paper658/Authors", "ICLR.cc/2019/Conference/Paper658/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608966}}}, {"id": "r1lxIcJxpm", "original": null, "number": 7, "cdate": 1541564999764, "ddate": null, "tcdate": 1541564999764, "tmdate": 1543288178255, "tddate": null, "forum": "BJxhijAcY7", "replyto": "HkgS6925hX", "invitation": "ICLR.cc/2019/Conference/-/Paper658/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "Dear AnonReviewer2,\n\nThank you for your clear and thorough review. We appreciate your comment that the paper is a \u201cnice addition to our understanding of signSGD\u201d. \n\nWe will first contest the criticism about the significance of the work. We will then respond to the other comments in detail.\n\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n>>>> On matters of significance >>>\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n\n> \u201cit heavily restricts what an adversary worker machine can do\u201d\nWe have now formulated an entire class of adversaries that our algorithm is robust to. Please see our revisions above. This class contains machines that send random bits as a special case.\n\n> \u201cTheorem 1 is a minor refinement\u201d.\nWhilst \"algebraically\" the result is a minor refinement, conceptually it is a larger shift. It brings the signSGD work in line with modern machine learning practice. And we expect that it has ramifications on other active areas of ML research. For example:\n\nReddi et al. (2018) showed how bimodal noise distributions can lead to divergence of Adam. This leaves a major outstanding question in the community: if Adam generally diverges, why does it work so well in practice? Theorem 1 shows how signSGD---a special limit of Adam---may be guaranteed to converge in natural settings such as Gaussian noise distributions. It suggests that we may be able to prove convergence of Adam for Gaussian noise distributions.\n\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n>>>>>>> Minor comments >>>>>>>\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n\n> \u201csignSGD converges to a critical point of the objective\u201d\nTo clarify, we mean convergence in the sense that the gradient norm goes to zero as N increases, which is exactly what Theorem 1 tells us. Points with zero gradient norm are critical points. The mixed norm on the left hand side is unusual, but by inspection it is clear that the mixed norm shrinking to zero implies that the L2-norm shrinks to zero. We will clarify this in the paper.\n\n> \u201care you assuming g_i > 0 here\u201d\nThanks for mentioning this. We did not signpost it, but we assumed, without loss of generality, that g_i > 0. (The case that g_i < 0 follows by totally analogous reasoning.)\n\n> The claim \u201csignSGD cannot converge for these noise distributions\u201d is only \u201cbased on intuitive arguments\u201d. \nThank you for pointing this out, we decided to simplify the discussion by just giving a simple example.\n\n> \u201dThe theoretical results are about signSGD while the experiments are about sigNUM\u201d\nSee [1, Appendix, Figure A.4] for experiments across a range of momentum values. [1] also discusses the theoretical relation between Signum and signSGD. In general we suggest practitioners use Signum instead of signSGD in practice since it is only fair to give our algorithm as many hyperparameters as momentum SGD.\n\n[1] signSGD, compressed optimisation for non-convex problems https://arxiv.org/abs/1802.04434."}, "signatures": ["ICLR.cc/2019/Conference/Paper658/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper658/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper658/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "signSGD with Majority Vote is Communication Efficient and Fault Tolerant", "abstract": "Training neural networks on large datasets can be accelerated by distributing the workload over a network of machines. As datasets grow ever larger, networks of hundreds or thousands of machines become economically viable. The time cost of communicating gradients limits the effectiveness of using such large machine counts, as may the increased chance of network faults. We explore a particularly simple algorithm for robust, communication-efficient learning---signSGD. Workers transmit only the sign of their gradient vector to a server, and the overall update is decided by a majority vote. This algorithm uses 32x less communication per iteration than full-precision, distributed SGD. Under natural conditions verified by experiment, we prove that signSGD converges in the large and mini-batch settings, establishing convergence for a parameter regime of Adam as a byproduct. Aggregating sign gradients by majority vote means that no individual worker has too much power. We prove that unlike SGD, majority vote is robust when up to 50% of workers behave adversarially. The class of adversaries we consider includes as special cases those that invert or randomise their gradient estimate. On the practical side, we built our distributed training system in Pytorch. Benchmarking against the state of the art collective communications library (NCCL), our framework---with the parameter server housed entirely on one machine---led to a 25% reduction in time for training resnet50 on Imagenet when using 15 AWS p3.2xlarge machines.", "keywords": ["large-scale learning", "distributed systems", "communication efficiency", "convergence rate analysis", "robust optimisation"], "authorids": ["bernstein@caltech.edu", "jiaweizhao.zjw@qq.com", "kazizzad@uci.edu", "anima@caltech.edu"], "authors": ["Jeremy Bernstein", "Jiawei Zhao", "Kamyar Azizzadenesheli", "Anima Anandkumar"], "TL;DR": "Workers send gradient signs to the server, and the update is decided by majority vote. We show that this algorithm is convergent, communication efficient and fault tolerant, both in theory and in practice.", "pdf": "/pdf/6933f7682283061fe8e760f1b4d92c465e999e64.pdf", "paperhash": "bernstein|signsgd_with_majority_vote_is_communication_efficient_and_fault_tolerant", "_bibtex": "@inproceedings{\nbernstein2018signsgd,\ntitle={sign{SGD} with Majority Vote is Communication Efficient and Fault Tolerant},\nauthor={Jeremy Bernstein and Jiawei Zhao and Kamyar Azizzadenesheli and Anima Anandkumar},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJxhijAcY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper658/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608966, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJxhijAcY7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper658/Authors", "ICLR.cc/2019/Conference/Paper658/Reviewers", "ICLR.cc/2019/Conference/Paper658/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper658/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper658/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper658/Authors|ICLR.cc/2019/Conference/Paper658/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper658/Reviewers", "ICLR.cc/2019/Conference/Paper658/Authors", "ICLR.cc/2019/Conference/Paper658/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608966}}}, {"id": "SJg-hh75C7", "original": null, "number": 11, "cdate": 1543285928623, "ddate": null, "tcdate": 1543285928623, "tmdate": 1543287525224, "tddate": null, "forum": "BJxhijAcY7", "replyto": "r1eEYTEq2m", "invitation": "ICLR.cc/2019/Conference/-/Paper658/Official_Comment", "content": {"title": "Revised the paper", "comment": "Dear AnonReviewer3,\n\nWe have updated your individualised response, and also summarised our revisions to the paper in a post above.\n\nBest wishes,\nAnonAuthors"}, "signatures": ["ICLR.cc/2019/Conference/Paper658/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper658/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper658/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "signSGD with Majority Vote is Communication Efficient and Fault Tolerant", "abstract": "Training neural networks on large datasets can be accelerated by distributing the workload over a network of machines. As datasets grow ever larger, networks of hundreds or thousands of machines become economically viable. The time cost of communicating gradients limits the effectiveness of using such large machine counts, as may the increased chance of network faults. We explore a particularly simple algorithm for robust, communication-efficient learning---signSGD. Workers transmit only the sign of their gradient vector to a server, and the overall update is decided by a majority vote. This algorithm uses 32x less communication per iteration than full-precision, distributed SGD. Under natural conditions verified by experiment, we prove that signSGD converges in the large and mini-batch settings, establishing convergence for a parameter regime of Adam as a byproduct. Aggregating sign gradients by majority vote means that no individual worker has too much power. We prove that unlike SGD, majority vote is robust when up to 50% of workers behave adversarially. The class of adversaries we consider includes as special cases those that invert or randomise their gradient estimate. On the practical side, we built our distributed training system in Pytorch. Benchmarking against the state of the art collective communications library (NCCL), our framework---with the parameter server housed entirely on one machine---led to a 25% reduction in time for training resnet50 on Imagenet when using 15 AWS p3.2xlarge machines.", "keywords": ["large-scale learning", "distributed systems", "communication efficiency", "convergence rate analysis", "robust optimisation"], "authorids": ["bernstein@caltech.edu", "jiaweizhao.zjw@qq.com", "kazizzad@uci.edu", "anima@caltech.edu"], "authors": ["Jeremy Bernstein", "Jiawei Zhao", "Kamyar Azizzadenesheli", "Anima Anandkumar"], "TL;DR": "Workers send gradient signs to the server, and the update is decided by majority vote. We show that this algorithm is convergent, communication efficient and fault tolerant, both in theory and in practice.", "pdf": "/pdf/6933f7682283061fe8e760f1b4d92c465e999e64.pdf", "paperhash": "bernstein|signsgd_with_majority_vote_is_communication_efficient_and_fault_tolerant", "_bibtex": "@inproceedings{\nbernstein2018signsgd,\ntitle={sign{SGD} with Majority Vote is Communication Efficient and Fault Tolerant},\nauthor={Jeremy Bernstein and Jiawei Zhao and Kamyar Azizzadenesheli and Anima Anandkumar},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJxhijAcY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper658/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608966, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJxhijAcY7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper658/Authors", "ICLR.cc/2019/Conference/Paper658/Reviewers", "ICLR.cc/2019/Conference/Paper658/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper658/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper658/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper658/Authors|ICLR.cc/2019/Conference/Paper658/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper658/Reviewers", "ICLR.cc/2019/Conference/Paper658/Authors", "ICLR.cc/2019/Conference/Paper658/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608966}}}, {"id": "SJg5q27qAX", "original": null, "number": 10, "cdate": 1543285905528, "ddate": null, "tcdate": 1543285905528, "tmdate": 1543287506392, "tddate": null, "forum": "BJxhijAcY7", "replyto": "HkgS6925hX", "invitation": "ICLR.cc/2019/Conference/-/Paper658/Official_Comment", "content": {"title": "Revised the paper", "comment": "Dear AnonReviewer2,\n\nWe have updated your individualised response, and also summarised our revisions to the paper in a post above.\n\nBest wishes,\nAnonAuthors"}, "signatures": ["ICLR.cc/2019/Conference/Paper658/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper658/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper658/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "signSGD with Majority Vote is Communication Efficient and Fault Tolerant", "abstract": "Training neural networks on large datasets can be accelerated by distributing the workload over a network of machines. As datasets grow ever larger, networks of hundreds or thousands of machines become economically viable. The time cost of communicating gradients limits the effectiveness of using such large machine counts, as may the increased chance of network faults. We explore a particularly simple algorithm for robust, communication-efficient learning---signSGD. Workers transmit only the sign of their gradient vector to a server, and the overall update is decided by a majority vote. This algorithm uses 32x less communication per iteration than full-precision, distributed SGD. Under natural conditions verified by experiment, we prove that signSGD converges in the large and mini-batch settings, establishing convergence for a parameter regime of Adam as a byproduct. Aggregating sign gradients by majority vote means that no individual worker has too much power. We prove that unlike SGD, majority vote is robust when up to 50% of workers behave adversarially. The class of adversaries we consider includes as special cases those that invert or randomise their gradient estimate. On the practical side, we built our distributed training system in Pytorch. Benchmarking against the state of the art collective communications library (NCCL), our framework---with the parameter server housed entirely on one machine---led to a 25% reduction in time for training resnet50 on Imagenet when using 15 AWS p3.2xlarge machines.", "keywords": ["large-scale learning", "distributed systems", "communication efficiency", "convergence rate analysis", "robust optimisation"], "authorids": ["bernstein@caltech.edu", "jiaweizhao.zjw@qq.com", "kazizzad@uci.edu", "anima@caltech.edu"], "authors": ["Jeremy Bernstein", "Jiawei Zhao", "Kamyar Azizzadenesheli", "Anima Anandkumar"], "TL;DR": "Workers send gradient signs to the server, and the update is decided by majority vote. We show that this algorithm is convergent, communication efficient and fault tolerant, both in theory and in practice.", "pdf": "/pdf/6933f7682283061fe8e760f1b4d92c465e999e64.pdf", "paperhash": "bernstein|signsgd_with_majority_vote_is_communication_efficient_and_fault_tolerant", "_bibtex": "@inproceedings{\nbernstein2018signsgd,\ntitle={sign{SGD} with Majority Vote is Communication Efficient and Fault Tolerant},\nauthor={Jeremy Bernstein and Jiawei Zhao and Kamyar Azizzadenesheli and Anima Anandkumar},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJxhijAcY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper658/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608966, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJxhijAcY7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper658/Authors", "ICLR.cc/2019/Conference/Paper658/Reviewers", "ICLR.cc/2019/Conference/Paper658/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper658/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper658/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper658/Authors|ICLR.cc/2019/Conference/Paper658/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper658/Reviewers", "ICLR.cc/2019/Conference/Paper658/Authors", "ICLR.cc/2019/Conference/Paper658/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608966}}}, {"id": "r1g4OhQqRQ", "original": null, "number": 9, "cdate": 1543285867928, "ddate": null, "tcdate": 1543285867928, "tmdate": 1543286961620, "tddate": null, "forum": "BJxhijAcY7", "replyto": "B1eUDnU037", "invitation": "ICLR.cc/2019/Conference/-/Paper658/Official_Comment", "content": {"title": "Revised the paper", "comment": "Dear AnonReviewer1,\n\nWe have updated your individualised response, and also summarised our revisions to the paper in a post above.\n\nBest wishes,\nAnonAuthors"}, "signatures": ["ICLR.cc/2019/Conference/Paper658/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper658/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper658/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "signSGD with Majority Vote is Communication Efficient and Fault Tolerant", "abstract": "Training neural networks on large datasets can be accelerated by distributing the workload over a network of machines. As datasets grow ever larger, networks of hundreds or thousands of machines become economically viable. The time cost of communicating gradients limits the effectiveness of using such large machine counts, as may the increased chance of network faults. We explore a particularly simple algorithm for robust, communication-efficient learning---signSGD. Workers transmit only the sign of their gradient vector to a server, and the overall update is decided by a majority vote. This algorithm uses 32x less communication per iteration than full-precision, distributed SGD. Under natural conditions verified by experiment, we prove that signSGD converges in the large and mini-batch settings, establishing convergence for a parameter regime of Adam as a byproduct. Aggregating sign gradients by majority vote means that no individual worker has too much power. We prove that unlike SGD, majority vote is robust when up to 50% of workers behave adversarially. The class of adversaries we consider includes as special cases those that invert or randomise their gradient estimate. On the practical side, we built our distributed training system in Pytorch. Benchmarking against the state of the art collective communications library (NCCL), our framework---with the parameter server housed entirely on one machine---led to a 25% reduction in time for training resnet50 on Imagenet when using 15 AWS p3.2xlarge machines.", "keywords": ["large-scale learning", "distributed systems", "communication efficiency", "convergence rate analysis", "robust optimisation"], "authorids": ["bernstein@caltech.edu", "jiaweizhao.zjw@qq.com", "kazizzad@uci.edu", "anima@caltech.edu"], "authors": ["Jeremy Bernstein", "Jiawei Zhao", "Kamyar Azizzadenesheli", "Anima Anandkumar"], "TL;DR": "Workers send gradient signs to the server, and the update is decided by majority vote. We show that this algorithm is convergent, communication efficient and fault tolerant, both in theory and in practice.", "pdf": "/pdf/6933f7682283061fe8e760f1b4d92c465e999e64.pdf", "paperhash": "bernstein|signsgd_with_majority_vote_is_communication_efficient_and_fault_tolerant", "_bibtex": "@inproceedings{\nbernstein2018signsgd,\ntitle={sign{SGD} with Majority Vote is Communication Efficient and Fault Tolerant},\nauthor={Jeremy Bernstein and Jiawei Zhao and Kamyar Azizzadenesheli and Anima Anandkumar},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJxhijAcY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper658/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608966, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJxhijAcY7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper658/Authors", "ICLR.cc/2019/Conference/Paper658/Reviewers", "ICLR.cc/2019/Conference/Paper658/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper658/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper658/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper658/Authors|ICLR.cc/2019/Conference/Paper658/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper658/Reviewers", "ICLR.cc/2019/Conference/Paper658/Authors", "ICLR.cc/2019/Conference/Paper658/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608966}}}, {"id": "Byx4HczqC7", "original": null, "number": 8, "cdate": 1543281212077, "ddate": null, "tcdate": 1543281212077, "tmdate": 1543284823942, "tddate": null, "forum": "BJxhijAcY7", "replyto": "BJxhijAcY7", "invitation": "ICLR.cc/2019/Conference/-/Paper658/Official_Comment", "content": {"title": "Summary of revisions", "comment": "Dear AnonReviewers and AC,\n\nWe have updated the paper. The new version includes the following additions:\n\n1. Added comparison to the Multi-Krum [1] Byzantine fault tolerant method (p9)\n2. Added comparison to the QSGD [2] gradient compression method (p9)\n3. Added natural language task benchmark (QRNN [3] model on the Wikitext-103 dataset) (p8)\n4. Extended the robustness theorem to an entire class of adversaries that we term \"blind multiplicative adversaries\" (p7) \n\nWe are grateful to Rev1 and Rev3 for encouraging us to run the additional experiments, and for Rev2 for encouraging us to extend the robustness theorem. \n\nWe will now go into more detail:\n\n1. Multi-Krum experiment. Multi-Krum is a Byzantine fault tolerant method that defines a security level f, and always removes f workers from the gradient aggregation (even when there are no adversaries present). Majority Vote in contrast always keeps all workers. We found that when the number of adversaries exceeds f, Multi-Krum deteriorates dramatically, whereas Majority Vote deteriorates more gracefully.\n\n2. QSGD experiment. For a resnet-18 model on Cifar-10, we found that majority vote converges much faster than the \"theory version\" [2, p5] of the QSGD algorithm, but it converges at similar rate to the \"experiment version\" [2, p7] where the QSGD authors normalise by the max instead of the L2 norm. We found the max-norm version of QSGD had about 5x higher compression than the 32x compression of signSGD for this problem, but this gain represents a diminishing return since the cost of backpropagation has already started to dominate at that compression level. \n\nTo be explicit, for this network with SGD and NCCL, one epochs costs \n=========> 6 sec computing + 12 sec communicating = 18 sec\nFor signSGD a very efficient implementation should reduce communication by 32x, therefore we expect one epoch to cost\n=========> 6 sec computing + 12/32 sec communicating = 6.375 sec\nFor QSGD a very efficient implementation should reduce communication by (32x5)x, therefore one epoch should cost\n=========> 6 sec computing + 12/(32x5) sec communicating = 6.075 sec\nAnd we see the marginal gain of QSGD is small, whilst the algorithm is much more complicated.\n\n3. Natural language experiment. We found that using signSGD with majority vote to train QRNN led to a 3x speedup per epoch over Adam with NCCL. That said, there was a deterioration in the converged solution. This meant that overall the performance after 2 hours of training was very similar.\n\n4. Extended the robustness theorem. We show that Majority vote is robust to an entire class of adversaries that we call \"blind multiplicative adversaries\". This class includes adversaries that invert or randomise their gradient estimate as special cases.  We are particularly interested in randomised attacks as a model of network faults. This class of adversaries is more rigorous than the class of \"non-cooperative\" adversaries that we discussed previously.\n\n[1] https://papers.nips.cc/paper/6617-machine-learning-with-adversaries-byzantine-tolerant-gradient-descent\n[2] https://papers.nips.cc/paper/6768-qsgd-communication-efficient-sgd-via-gradient-quantization-and-encoding\n[3] https://openreview.net/forum?id=H1zJ-v5xl"}, "signatures": ["ICLR.cc/2019/Conference/Paper658/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper658/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper658/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "signSGD with Majority Vote is Communication Efficient and Fault Tolerant", "abstract": "Training neural networks on large datasets can be accelerated by distributing the workload over a network of machines. As datasets grow ever larger, networks of hundreds or thousands of machines become economically viable. The time cost of communicating gradients limits the effectiveness of using such large machine counts, as may the increased chance of network faults. We explore a particularly simple algorithm for robust, communication-efficient learning---signSGD. Workers transmit only the sign of their gradient vector to a server, and the overall update is decided by a majority vote. This algorithm uses 32x less communication per iteration than full-precision, distributed SGD. Under natural conditions verified by experiment, we prove that signSGD converges in the large and mini-batch settings, establishing convergence for a parameter regime of Adam as a byproduct. Aggregating sign gradients by majority vote means that no individual worker has too much power. We prove that unlike SGD, majority vote is robust when up to 50% of workers behave adversarially. The class of adversaries we consider includes as special cases those that invert or randomise their gradient estimate. On the practical side, we built our distributed training system in Pytorch. Benchmarking against the state of the art collective communications library (NCCL), our framework---with the parameter server housed entirely on one machine---led to a 25% reduction in time for training resnet50 on Imagenet when using 15 AWS p3.2xlarge machines.", "keywords": ["large-scale learning", "distributed systems", "communication efficiency", "convergence rate analysis", "robust optimisation"], "authorids": ["bernstein@caltech.edu", "jiaweizhao.zjw@qq.com", "kazizzad@uci.edu", "anima@caltech.edu"], "authors": ["Jeremy Bernstein", "Jiawei Zhao", "Kamyar Azizzadenesheli", "Anima Anandkumar"], "TL;DR": "Workers send gradient signs to the server, and the update is decided by majority vote. We show that this algorithm is convergent, communication efficient and fault tolerant, both in theory and in practice.", "pdf": "/pdf/6933f7682283061fe8e760f1b4d92c465e999e64.pdf", "paperhash": "bernstein|signsgd_with_majority_vote_is_communication_efficient_and_fault_tolerant", "_bibtex": "@inproceedings{\nbernstein2018signsgd,\ntitle={sign{SGD} with Majority Vote is Communication Efficient and Fault Tolerant},\nauthor={Jeremy Bernstein and Jiawei Zhao and Kamyar Azizzadenesheli and Anima Anandkumar},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJxhijAcY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper658/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608966, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJxhijAcY7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper658/Authors", "ICLR.cc/2019/Conference/Paper658/Reviewers", "ICLR.cc/2019/Conference/Paper658/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper658/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper658/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper658/Authors|ICLR.cc/2019/Conference/Paper658/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper658/Reviewers", "ICLR.cc/2019/Conference/Paper658/Authors", "ICLR.cc/2019/Conference/Paper658/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608966}}}, {"id": "S1eaXqJgpX", "original": null, "number": 6, "cdate": 1541564964704, "ddate": null, "tcdate": 1541564964704, "tmdate": 1543284132173, "tddate": null, "forum": "BJxhijAcY7", "replyto": "r1eEYTEq2m", "invitation": "ICLR.cc/2019/Conference/-/Paper658/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "Dear AnonReviewer3,\n\nThank you for your positive review. We really appreciate the remarks that our \u201cexperiments are extensive\u201d and our paper is \u201csolid and interesting\u201d.\n\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n>>>>>>> More experiments >>>>>>\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n\n> \u201cMore experiments on different tasks and DNN architectures could be performed\u201d\n\nThanks for the suggestion, we have added experiments training the QRNN language model on the Wikitext-103 dataset. Please see the revisions above\n\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n>>>>>>>> Further thoughts >>>>>>>\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n\n> \u201csome workers might be lost during one iteration\u201d\nIntuitively, dropping workers will slow down convergence but not prevent it. You can see this immediately since a dropped worker is strictly better for convergence than an adversarial worker. This is one of the reasons we are excited about our Byzantine fault tolerance results.\n\n> what \u201cregularization technique would be suitable for signed update kind of method\u201d?\nWe are particularly excited about this question for future work, thanks for suggesting it.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper658/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper658/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper658/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "signSGD with Majority Vote is Communication Efficient and Fault Tolerant", "abstract": "Training neural networks on large datasets can be accelerated by distributing the workload over a network of machines. As datasets grow ever larger, networks of hundreds or thousands of machines become economically viable. The time cost of communicating gradients limits the effectiveness of using such large machine counts, as may the increased chance of network faults. We explore a particularly simple algorithm for robust, communication-efficient learning---signSGD. Workers transmit only the sign of their gradient vector to a server, and the overall update is decided by a majority vote. This algorithm uses 32x less communication per iteration than full-precision, distributed SGD. Under natural conditions verified by experiment, we prove that signSGD converges in the large and mini-batch settings, establishing convergence for a parameter regime of Adam as a byproduct. Aggregating sign gradients by majority vote means that no individual worker has too much power. We prove that unlike SGD, majority vote is robust when up to 50% of workers behave adversarially. The class of adversaries we consider includes as special cases those that invert or randomise their gradient estimate. On the practical side, we built our distributed training system in Pytorch. Benchmarking against the state of the art collective communications library (NCCL), our framework---with the parameter server housed entirely on one machine---led to a 25% reduction in time for training resnet50 on Imagenet when using 15 AWS p3.2xlarge machines.", "keywords": ["large-scale learning", "distributed systems", "communication efficiency", "convergence rate analysis", "robust optimisation"], "authorids": ["bernstein@caltech.edu", "jiaweizhao.zjw@qq.com", "kazizzad@uci.edu", "anima@caltech.edu"], "authors": ["Jeremy Bernstein", "Jiawei Zhao", "Kamyar Azizzadenesheli", "Anima Anandkumar"], "TL;DR": "Workers send gradient signs to the server, and the update is decided by majority vote. We show that this algorithm is convergent, communication efficient and fault tolerant, both in theory and in practice.", "pdf": "/pdf/6933f7682283061fe8e760f1b4d92c465e999e64.pdf", "paperhash": "bernstein|signsgd_with_majority_vote_is_communication_efficient_and_fault_tolerant", "_bibtex": "@inproceedings{\nbernstein2018signsgd,\ntitle={sign{SGD} with Majority Vote is Communication Efficient and Fault Tolerant},\nauthor={Jeremy Bernstein and Jiawei Zhao and Kamyar Azizzadenesheli and Anima Anandkumar},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJxhijAcY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper658/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608966, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJxhijAcY7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper658/Authors", "ICLR.cc/2019/Conference/Paper658/Reviewers", "ICLR.cc/2019/Conference/Paper658/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper658/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper658/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper658/Authors|ICLR.cc/2019/Conference/Paper658/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper658/Reviewers", "ICLR.cc/2019/Conference/Paper658/Authors", "ICLR.cc/2019/Conference/Paper658/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608966}}}, {"id": "SygCGdygTQ", "original": null, "number": 4, "cdate": 1541564438504, "ddate": null, "tcdate": 1541564438504, "tmdate": 1543283365268, "tddate": null, "forum": "BJxhijAcY7", "replyto": "BJxhijAcY7", "invitation": "ICLR.cc/2019/Conference/-/Paper658/Official_Comment", "content": {"title": "Summary of reviews", "comment": "Dear AnonReviewers,\n\nThank you for your thoughtful and thorough reviews. We will summarise the content of the reviews here.\n\nFirst some high notes:\n\nRev3 says our \u201cexperiments are extensive\u201d and our paper is \u201csolid and interesting\u201d. Rev2 says the paper is a \u201cnice addition to our understanding of signSGD\u201d. Rev1 says our work \u201ccould be a great paper\u201d if we add sufficient comparisons during the rebuttal.\n\nThe reviewers\u2019 main concerns:\n\n1. Rev1 and Rev2 question the strength of the adversarial model;\n2. Rev1 asks for comparison experiments for communication and/or Byzantine property;\n3. Rev3 would like to see additional datasets and network architectures.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper658/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper658/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper658/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "signSGD with Majority Vote is Communication Efficient and Fault Tolerant", "abstract": "Training neural networks on large datasets can be accelerated by distributing the workload over a network of machines. As datasets grow ever larger, networks of hundreds or thousands of machines become economically viable. The time cost of communicating gradients limits the effectiveness of using such large machine counts, as may the increased chance of network faults. We explore a particularly simple algorithm for robust, communication-efficient learning---signSGD. Workers transmit only the sign of their gradient vector to a server, and the overall update is decided by a majority vote. This algorithm uses 32x less communication per iteration than full-precision, distributed SGD. Under natural conditions verified by experiment, we prove that signSGD converges in the large and mini-batch settings, establishing convergence for a parameter regime of Adam as a byproduct. Aggregating sign gradients by majority vote means that no individual worker has too much power. We prove that unlike SGD, majority vote is robust when up to 50% of workers behave adversarially. The class of adversaries we consider includes as special cases those that invert or randomise their gradient estimate. On the practical side, we built our distributed training system in Pytorch. Benchmarking against the state of the art collective communications library (NCCL), our framework---with the parameter server housed entirely on one machine---led to a 25% reduction in time for training resnet50 on Imagenet when using 15 AWS p3.2xlarge machines.", "keywords": ["large-scale learning", "distributed systems", "communication efficiency", "convergence rate analysis", "robust optimisation"], "authorids": ["bernstein@caltech.edu", "jiaweizhao.zjw@qq.com", "kazizzad@uci.edu", "anima@caltech.edu"], "authors": ["Jeremy Bernstein", "Jiawei Zhao", "Kamyar Azizzadenesheli", "Anima Anandkumar"], "TL;DR": "Workers send gradient signs to the server, and the update is decided by majority vote. We show that this algorithm is convergent, communication efficient and fault tolerant, both in theory and in practice.", "pdf": "/pdf/6933f7682283061fe8e760f1b4d92c465e999e64.pdf", "paperhash": "bernstein|signsgd_with_majority_vote_is_communication_efficient_and_fault_tolerant", "_bibtex": "@inproceedings{\nbernstein2018signsgd,\ntitle={sign{SGD} with Majority Vote is Communication Efficient and Fault Tolerant},\nauthor={Jeremy Bernstein and Jiawei Zhao and Kamyar Azizzadenesheli and Anima Anandkumar},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJxhijAcY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper658/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608966, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJxhijAcY7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper658/Authors", "ICLR.cc/2019/Conference/Paper658/Reviewers", "ICLR.cc/2019/Conference/Paper658/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper658/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper658/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper658/Authors|ICLR.cc/2019/Conference/Paper658/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper658/Reviewers", "ICLR.cc/2019/Conference/Paper658/Authors", "ICLR.cc/2019/Conference/Paper658/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608966}}}, {"id": "BJezmXgjt7", "original": null, "number": 1, "cdate": 1538093850502, "ddate": null, "tcdate": 1538093850502, "tmdate": 1543279008017, "tddate": null, "forum": "BJxhijAcY7", "replyto": "BJxhijAcY7", "invitation": "ICLR.cc/2019/Conference/-/Paper658/Official_Comment", "content": {"title": "Jupyter notebook", "comment": "Dear anonReviewers,\n\nHere's a Jupyter notebook in case you'd like to play with the algorithm: https://colab.research.google.com/drive/1PlD2jXoXr2a8e57aIDINCw1-7RIttRTt\n\nIt can be run in the browser, or you can just download it and run on your machine.\n\nBest wishes,\nanonAuthors"}, "signatures": ["ICLR.cc/2019/Conference/Paper658/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper658/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper658/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "signSGD with Majority Vote is Communication Efficient and Fault Tolerant", "abstract": "Training neural networks on large datasets can be accelerated by distributing the workload over a network of machines. As datasets grow ever larger, networks of hundreds or thousands of machines become economically viable. The time cost of communicating gradients limits the effectiveness of using such large machine counts, as may the increased chance of network faults. We explore a particularly simple algorithm for robust, communication-efficient learning---signSGD. Workers transmit only the sign of their gradient vector to a server, and the overall update is decided by a majority vote. This algorithm uses 32x less communication per iteration than full-precision, distributed SGD. Under natural conditions verified by experiment, we prove that signSGD converges in the large and mini-batch settings, establishing convergence for a parameter regime of Adam as a byproduct. Aggregating sign gradients by majority vote means that no individual worker has too much power. We prove that unlike SGD, majority vote is robust when up to 50% of workers behave adversarially. The class of adversaries we consider includes as special cases those that invert or randomise their gradient estimate. On the practical side, we built our distributed training system in Pytorch. Benchmarking against the state of the art collective communications library (NCCL), our framework---with the parameter server housed entirely on one machine---led to a 25% reduction in time for training resnet50 on Imagenet when using 15 AWS p3.2xlarge machines.", "keywords": ["large-scale learning", "distributed systems", "communication efficiency", "convergence rate analysis", "robust optimisation"], "authorids": ["bernstein@caltech.edu", "jiaweizhao.zjw@qq.com", "kazizzad@uci.edu", "anima@caltech.edu"], "authors": ["Jeremy Bernstein", "Jiawei Zhao", "Kamyar Azizzadenesheli", "Anima Anandkumar"], "TL;DR": "Workers send gradient signs to the server, and the update is decided by majority vote. We show that this algorithm is convergent, communication efficient and fault tolerant, both in theory and in practice.", "pdf": "/pdf/6933f7682283061fe8e760f1b4d92c465e999e64.pdf", "paperhash": "bernstein|signsgd_with_majority_vote_is_communication_efficient_and_fault_tolerant", "_bibtex": "@inproceedings{\nbernstein2018signsgd,\ntitle={sign{SGD} with Majority Vote is Communication Efficient and Fault Tolerant},\nauthor={Jeremy Bernstein and Jiawei Zhao and Kamyar Azizzadenesheli and Anima Anandkumar},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJxhijAcY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper658/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608966, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJxhijAcY7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper658/Authors", "ICLR.cc/2019/Conference/Paper658/Reviewers", "ICLR.cc/2019/Conference/Paper658/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper658/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper658/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper658/Authors|ICLR.cc/2019/Conference/Paper658/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper658/Reviewers", "ICLR.cc/2019/Conference/Paper658/Authors", "ICLR.cc/2019/Conference/Paper658/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608966}}}, {"id": "r1eEYTEq2m", "original": null, "number": 1, "cdate": 1541193084051, "ddate": null, "tcdate": 1541193084051, "tmdate": 1541533799741, "tddate": null, "forum": "BJxhijAcY7", "replyto": "BJxhijAcY7", "invitation": "ICLR.cc/2019/Conference/-/Paper658/Official_Review", "content": {"title": "interesting distributed optimization algorithm based on signSGD", "review": "The paper proposes a distributed optimization method based on signSGD. Majority vote is used when aggregating the updates from different workers.\n The method itself is naturally communication efficient. Convergence analysis is provided under certain assumptions on the gradient. It also theoretically shows that it is robust up to half of the workers behave independently adversarially. Experiments are carried out on parameter server environment and are shown to be effective in speeding up training. \n\nI find the paper to be solid and interesting. The idea of using signSGD for distributed optimization make it attractive as it is naturally communication efficient. The work provides theoretical convergence analysis under the small batch setting by further assuming the gradient is unimodal and symmetric, which is the main theoretical contribution. Another main theoretical contribution is showing it is Byzantine fault tolerant. The experiments are extensive, demonstrating running time speed-up comparison to normal SGD.  \n\nIt is interesting to see a test set gap in the experiments. It remains to be further experimented to see if the method itself inherently suffer from generalization problems or it is a result of imperfect parameter tuning. \n\nOne thing that would be interesting to explore further is to see how asynchronous updates of signSGD affect the convergence both in theory and practice. For example, some workers might be lost during one iteration, how will this affect the overall convergence.\nAlso, it would be interesting to see the comparison of the proposed method with SGD + batch normalization, especially on their generalization performance. It might be interesting to explore what kind of regularization technique would be suitable for signed update kind of method.   \n\nOverall, I think the paper proposes a novel distributed optimization algorithm that has both theoretical and experimental contribution. The presentation of the paper is clear and easy to follow. \n\nSuggestions: I feel the experiments part could still be improved as also mentioned in the paper to achieve competitive results. More experiments on different tasks and DNN architectures could be performed. \n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper658/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "signSGD with Majority Vote is Communication Efficient and Fault Tolerant", "abstract": "Training neural networks on large datasets can be accelerated by distributing the workload over a network of machines. As datasets grow ever larger, networks of hundreds or thousands of machines become economically viable. The time cost of communicating gradients limits the effectiveness of using such large machine counts, as may the increased chance of network faults. We explore a particularly simple algorithm for robust, communication-efficient learning---signSGD. Workers transmit only the sign of their gradient vector to a server, and the overall update is decided by a majority vote. This algorithm uses 32x less communication per iteration than full-precision, distributed SGD. Under natural conditions verified by experiment, we prove that signSGD converges in the large and mini-batch settings, establishing convergence for a parameter regime of Adam as a byproduct. Aggregating sign gradients by majority vote means that no individual worker has too much power. We prove that unlike SGD, majority vote is robust when up to 50% of workers behave adversarially. The class of adversaries we consider includes as special cases those that invert or randomise their gradient estimate. On the practical side, we built our distributed training system in Pytorch. Benchmarking against the state of the art collective communications library (NCCL), our framework---with the parameter server housed entirely on one machine---led to a 25% reduction in time for training resnet50 on Imagenet when using 15 AWS p3.2xlarge machines.", "keywords": ["large-scale learning", "distributed systems", "communication efficiency", "convergence rate analysis", "robust optimisation"], "authorids": ["bernstein@caltech.edu", "jiaweizhao.zjw@qq.com", "kazizzad@uci.edu", "anima@caltech.edu"], "authors": ["Jeremy Bernstein", "Jiawei Zhao", "Kamyar Azizzadenesheli", "Anima Anandkumar"], "TL;DR": "Workers send gradient signs to the server, and the update is decided by majority vote. We show that this algorithm is convergent, communication efficient and fault tolerant, both in theory and in practice.", "pdf": "/pdf/6933f7682283061fe8e760f1b4d92c465e999e64.pdf", "paperhash": "bernstein|signsgd_with_majority_vote_is_communication_efficient_and_fault_tolerant", "_bibtex": "@inproceedings{\nbernstein2018signsgd,\ntitle={sign{SGD} with Majority Vote is Communication Efficient and Fault Tolerant},\nauthor={Jeremy Bernstein and Jiawei Zhao and Kamyar Azizzadenesheli and Anima Anandkumar},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJxhijAcY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper658/Official_Review", "cdate": 1542234409413, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJxhijAcY7", "replyto": "BJxhijAcY7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper658/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335774060, "tmdate": 1552335774060, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper658/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 16}