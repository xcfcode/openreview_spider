{"notes": [{"id": "Bylnx209YX", "original": "BJxDjehqKm", "number": 1112, "cdate": 1538087923688, "ddate": null, "tcdate": 1538087923688, "tmdate": 1550827035915, "tddate": null, "forum": "Bylnx209YX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Adversarial Attacks on Graph Neural Networks via Meta Learning", "abstract": "Deep learning models for graphs have advanced the state of the art on many tasks. Despite their recent success, little is known about their robustness. We investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure.  Our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize. Our experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings. Remarkably, the perturbations created by our algorithm can misguide the graph neural networks such that they perform worse than a simple baseline that ignores all relational information. Our attacks do not assume any knowledge about or access to the target classifiers.", "keywords": ["graph mining", "adversarial attacks", "meta learning", "graph neural networks", "node classification"], "authorids": ["zuegnerd@in.tum.de", "guennemann@in.tum.de"], "authors": ["Daniel Z\u00fcgner", "Stephan G\u00fcnnemann"], "TL;DR": "We use meta-gradients to attack the training procedure of deep neural networks for graphs.", "pdf": "/pdf/9c456c2747f67ea78b03bd154b4ce729e1410d56.pdf", "paperhash": "z\u00fcgner|adversarial_attacks_on_graph_neural_networks_via_meta_learning", "_bibtex": "@inproceedings{\nz\u00fcgner2018adversarial,\ntitle={Adversarial Attacks on Graph Neural Networks via Meta Learning},\nauthor={Daniel Z\u00fcgner and Stephan G\u00fcnnemann},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Bylnx209YX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rJg9H3BvgV", "original": null, "number": 1, "cdate": 1545194561920, "ddate": null, "tcdate": 1545194561920, "tmdate": 1545354476284, "tddate": null, "forum": "Bylnx209YX", "replyto": "Bylnx209YX", "invitation": "ICLR.cc/2019/Conference/-/Paper1112/Meta_Review", "content": {"metareview": " The paper proposes an method for investigating robustness of graph neural nets for node classification problem; training-time attacks for perturbing graph structure are generated using  meta-learning approach. Reviewers agree that the contribution is novel and empirical results support the validity of the approach.\n ", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "A novel meta-learning based approach for testing robustness of grap neural nets"}, "signatures": ["ICLR.cc/2019/Conference/Paper1112/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1112/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Attacks on Graph Neural Networks via Meta Learning", "abstract": "Deep learning models for graphs have advanced the state of the art on many tasks. Despite their recent success, little is known about their robustness. We investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure.  Our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize. Our experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings. Remarkably, the perturbations created by our algorithm can misguide the graph neural networks such that they perform worse than a simple baseline that ignores all relational information. Our attacks do not assume any knowledge about or access to the target classifiers.", "keywords": ["graph mining", "adversarial attacks", "meta learning", "graph neural networks", "node classification"], "authorids": ["zuegnerd@in.tum.de", "guennemann@in.tum.de"], "authors": ["Daniel Z\u00fcgner", "Stephan G\u00fcnnemann"], "TL;DR": "We use meta-gradients to attack the training procedure of deep neural networks for graphs.", "pdf": "/pdf/9c456c2747f67ea78b03bd154b4ce729e1410d56.pdf", "paperhash": "z\u00fcgner|adversarial_attacks_on_graph_neural_networks_via_meta_learning", "_bibtex": "@inproceedings{\nz\u00fcgner2018adversarial,\ntitle={Adversarial Attacks on Graph Neural Networks via Meta Learning},\nauthor={Daniel Z\u00fcgner and Stephan G\u00fcnnemann},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Bylnx209YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1112/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352961972, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bylnx209YX", "replyto": "Bylnx209YX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1112/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1112/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1112/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352961972}}}, {"id": "SkeoiDVcAX", "original": null, "number": 8, "cdate": 1543288739493, "ddate": null, "tcdate": 1543288739493, "tmdate": 1543288739493, "tddate": null, "forum": "Bylnx209YX", "replyto": "BJxOYyT-0m", "invitation": "ICLR.cc/2019/Conference/-/Paper1112/Official_Comment", "content": {"title": "the authors have addressed my concerns", "comment": "The authors have made efforts in addressing my concerns and have improved their paper. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1112/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1112/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1112/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Attacks on Graph Neural Networks via Meta Learning", "abstract": "Deep learning models for graphs have advanced the state of the art on many tasks. Despite their recent success, little is known about their robustness. We investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure.  Our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize. Our experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings. Remarkably, the perturbations created by our algorithm can misguide the graph neural networks such that they perform worse than a simple baseline that ignores all relational information. Our attacks do not assume any knowledge about or access to the target classifiers.", "keywords": ["graph mining", "adversarial attacks", "meta learning", "graph neural networks", "node classification"], "authorids": ["zuegnerd@in.tum.de", "guennemann@in.tum.de"], "authors": ["Daniel Z\u00fcgner", "Stephan G\u00fcnnemann"], "TL;DR": "We use meta-gradients to attack the training procedure of deep neural networks for graphs.", "pdf": "/pdf/9c456c2747f67ea78b03bd154b4ce729e1410d56.pdf", "paperhash": "z\u00fcgner|adversarial_attacks_on_graph_neural_networks_via_meta_learning", "_bibtex": "@inproceedings{\nz\u00fcgner2018adversarial,\ntitle={Adversarial Attacks on Graph Neural Networks via Meta Learning},\nauthor={Daniel Z\u00fcgner and Stephan G\u00fcnnemann},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Bylnx209YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1112/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617085, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bylnx209YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1112/Authors", "ICLR.cc/2019/Conference/Paper1112/Reviewers", "ICLR.cc/2019/Conference/Paper1112/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1112/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1112/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1112/Authors|ICLR.cc/2019/Conference/Paper1112/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1112/Reviewers", "ICLR.cc/2019/Conference/Paper1112/Authors", "ICLR.cc/2019/Conference/Paper1112/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617085}}}, {"id": "HJe1c7y_27", "original": null, "number": 2, "cdate": 1541038983081, "ddate": null, "tcdate": 1541038983081, "tmdate": 1543258866953, "tddate": null, "forum": "Bylnx209YX", "replyto": "Bylnx209YX", "invitation": "ICLR.cc/2019/Conference/-/Paper1112/Official_Review", "content": {"title": "Used meta-learning by treating graph structure as hyperparameter to get the poisoned graph. Achieved reasonable results on three graph datasets.", "review": "This paper studies the problem of learning a better poisoned graph parameters that can maximize the loss of a graph neural network. The proposed using meta-learning to compute the second-order derivatives to get the meta-gradients seems reasonable. The authors also proposed approximate methods to compute the graph as learning parameters, which could be more efficient since the second-order derivatives are no longer computed. The experimental results on three graph datasets show that the proposed model could improve the misclassification rate of the unlabeled nodes.\n\nThe paper is well-written. It would be good if the authors could address the following suggestions or concerns:\n\n1) The proposed attack model assumes the only the graph structure are accessiable to the attackers, which might limit the proposed model in real applications. Joint study with the graph features would be useful to convince more audience and potentially have larger impacts.\n\n2) In the self-learning setting, in order to define l_atk, l_self is used, however, l_self is using v_u, which is the ground truth label of the test nodes based on my understanding, so this approach is using labels of the unlabeled data, which might be not applicable in real world.\n\n3) About the action space, based on the constraints of the attacker's capability, the possible attacks will be significantly smaller than O(N^2 delta), might be O(N^delta).\n\n4) Change 'treat the graph structure as a hyperparameter' to 'treat the graph structure tensor/matrix as a hyperparameter' would be earier to understand. And is the graph structure tensor with shape (NXN)? \n\n5) What's the relationship between T and S? Are T in theta_T is the same as the S in G_S?\n\n6) The title of section 4.2 is misleading. It would be better to name it as 'Greedy Computing Meta-Gradients'. \n\n7) It lacks intuition of why define S(u,v)=delta . (-2.a_uv+1). '(-2.a_uv+1)' looks lack of intuition. Please also change 'pair (i,j), we define S(u,v)' -> 'pair (u,v)'.\n\n8) In the experiments, what's the definition of meta-train? l_atk=-l_train?\n\n9) In the experiments, it would be interesting to study the impact of unnoticaability constraints on the model results.\n\n10) In figure 1, it is not surprising that when increasing the number of edges changed, the misclassification rates will increase. A graph NN considers more graph features rather than the structure is expected to show the impact of the graph structure change.\n\nI have read the authors' detailed rebuttal. Thanks.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1112/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Adversarial Attacks on Graph Neural Networks via Meta Learning", "abstract": "Deep learning models for graphs have advanced the state of the art on many tasks. Despite their recent success, little is known about their robustness. We investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure.  Our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize. Our experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings. Remarkably, the perturbations created by our algorithm can misguide the graph neural networks such that they perform worse than a simple baseline that ignores all relational information. Our attacks do not assume any knowledge about or access to the target classifiers.", "keywords": ["graph mining", "adversarial attacks", "meta learning", "graph neural networks", "node classification"], "authorids": ["zuegnerd@in.tum.de", "guennemann@in.tum.de"], "authors": ["Daniel Z\u00fcgner", "Stephan G\u00fcnnemann"], "TL;DR": "We use meta-gradients to attack the training procedure of deep neural networks for graphs.", "pdf": "/pdf/9c456c2747f67ea78b03bd154b4ce729e1410d56.pdf", "paperhash": "z\u00fcgner|adversarial_attacks_on_graph_neural_networks_via_meta_learning", "_bibtex": "@inproceedings{\nz\u00fcgner2018adversarial,\ntitle={Adversarial Attacks on Graph Neural Networks via Meta Learning},\nauthor={Daniel Z\u00fcgner and Stephan G\u00fcnnemann},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Bylnx209YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1112/Official_Review", "cdate": 1542234303399, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Bylnx209YX", "replyto": "Bylnx209YX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1112/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335875247, "tmdate": 1552335875247, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1112/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJlQtgaW0m", "original": null, "number": 5, "cdate": 1542733946682, "ddate": null, "tcdate": 1542733946682, "tmdate": 1542733946682, "tddate": null, "forum": "Bylnx209YX", "replyto": "HkeXZtLM2m", "invitation": "ICLR.cc/2019/Conference/-/Paper1112/Official_Comment", "content": {"title": "Re: Review 2", "comment": "Dear Reviewer 2,\n\nThank you for your constructive feedback and suggestions. We have run experiments on a larger dataset with roughly 20K nodes and found that our attacks are also successful in this scenario. You can find the results in Table 8 in Appendix F of the updated manuscript. Furthermore, we have included a discussion on the complexity of our approach in Appendix C in the updated manuscript. \n\nRegarding your question about the transferability to other graph embedding algorithms: We would like to point out that we already evaluate the impact of our attacks on DeepWalk. Our experiments show that our method\u2019s adversarial attacks also transfer to DeepWalk.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1112/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1112/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1112/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Attacks on Graph Neural Networks via Meta Learning", "abstract": "Deep learning models for graphs have advanced the state of the art on many tasks. Despite their recent success, little is known about their robustness. We investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure.  Our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize. Our experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings. Remarkably, the perturbations created by our algorithm can misguide the graph neural networks such that they perform worse than a simple baseline that ignores all relational information. Our attacks do not assume any knowledge about or access to the target classifiers.", "keywords": ["graph mining", "adversarial attacks", "meta learning", "graph neural networks", "node classification"], "authorids": ["zuegnerd@in.tum.de", "guennemann@in.tum.de"], "authors": ["Daniel Z\u00fcgner", "Stephan G\u00fcnnemann"], "TL;DR": "We use meta-gradients to attack the training procedure of deep neural networks for graphs.", "pdf": "/pdf/9c456c2747f67ea78b03bd154b4ce729e1410d56.pdf", "paperhash": "z\u00fcgner|adversarial_attacks_on_graph_neural_networks_via_meta_learning", "_bibtex": "@inproceedings{\nz\u00fcgner2018adversarial,\ntitle={Adversarial Attacks on Graph Neural Networks via Meta Learning},\nauthor={Daniel Z\u00fcgner and Stephan G\u00fcnnemann},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Bylnx209YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1112/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617085, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bylnx209YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1112/Authors", "ICLR.cc/2019/Conference/Paper1112/Reviewers", "ICLR.cc/2019/Conference/Paper1112/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1112/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1112/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1112/Authors|ICLR.cc/2019/Conference/Paper1112/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1112/Reviewers", "ICLR.cc/2019/Conference/Paper1112/Authors", "ICLR.cc/2019/Conference/Paper1112/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617085}}}, {"id": "B1xHHl6Z0m", "original": null, "number": 4, "cdate": 1542733884556, "ddate": null, "tcdate": 1542733884556, "tmdate": 1542733884556, "tddate": null, "forum": "Bylnx209YX", "replyto": "HJe1c7y_27", "invitation": "ICLR.cc/2019/Conference/-/Paper1112/Official_Comment", "content": {"title": "Re: Review 1", "comment": "Dear Reviewer 1,\n\nThank you for your detailed and constructive feedback. We have used your suggestions to improve the paper and have uploaded the updated manuscript.\n\nWe would like to address each point individually here:\n\n1) Based on your suggestion, we ran experiments on Citeseer where we use meta gradients to modify the graph structure and features simultaneously. We evaluated on GCN and CLN (DeepWalk does not use features) and we observed that the impact of the combined attacks is comparable but slightly lower (GCN: 38.6 vs 37.2, CLN: 35.3 vs 34.2; structure-only vs combined). We attribute this to the fact that we assign the same \u2018cost\u2019 to structure and feature changes, but arguably we expect a structure perturbation to have a stronger effect on performance than a feature perturbation. We have summarized these findings in Appendix E of the updated manuscript.\n\n2) We would like to emphasize that the attack model does *not* have access to the ground-truth labels of the unlabeled nodes V_u. We use the labels of the labeled nodes to train the surrogate classification model and predict the labels \\hat{C}_u of the unlabeled nodes. These labels are then treated as the \u2018ground truth\u2019 for the self-training loss L_self. Thus, the attack never uses or has access to the labels C_u of the unlabeled nodes.\n\n3) We agree that the set of admissible attacks is significantly smaller than O(N^{2 delta}). However, since it is challenging to derive a tighter upper bound on the size of the set of admissible perturbations, we decided to use this conservative upper bound. The main point we wanted to make (which also holds for a tighter bound) is that there is an exponential growth in the number of perturbations, i.e. exhaustive search is infeasible.\n\n4) Thank you for this suggestion. We have updated the manuscript to make this point more clear. Yes, the dimensionality of the adjacency matrix is NxN.\n\n5) T is the number of inner optimization steps (i.e., gradient descent steps of learning the surrogate model). S is the number of meta-steps on the graph structure. We have replaced G^(S) by G^(delta) in the manuscript to avoid confusion.\n\n6) Thank you for raising this point. We have changed the section title to \u2018Greedy Poisoning Attacks via Meta Gradients\u2019 in the updated manuscript.\n\n7) We have changed (i,j) to (u,v). A negative gradient in an entry (u,v) means that the target quantity (e.g. error) increases when the value is decreased. Decreasing the value is only admissible for node pairs connected by an edge, i.e. we change the adjacency matrix entry from a 1 (edge) to a 0 (no edge). To account for this, we flip the sign of gradients of node pairs connected by an edge, as achieved by multiplying by (-2a_uv+1). This enables us to use the arg max operation later. Equivalently, we could compute the maximum of the gradients where there is no edge and the minimum where the nodes are connected, and then choosing the entry with the higher absolute value as the perturbation.\n\n8) You are correct, Meta-Train uses l_atk=-l_train. \n\n9) We have added an experiment to Appendix D showing the effect of the unnoticeability constraint (see Figure 4). As shown, even when enforcing the constraints the attacks have similar impact. Thus we conclude that the constraint should always be enforced since they improve unnoticeability while at the same time our attacks remain effective.\n\n10) We agree that an increasing misclassification rate is expected when increasing the number of edges changed. Our intention in Figure 1 was to visualize this relationship and, more importantly, to show that our attacks consistently outperform the DICE baseline that has access to all class labels, i.e. more information than our method.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1112/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1112/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1112/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Attacks on Graph Neural Networks via Meta Learning", "abstract": "Deep learning models for graphs have advanced the state of the art on many tasks. Despite their recent success, little is known about their robustness. We investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure.  Our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize. Our experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings. Remarkably, the perturbations created by our algorithm can misguide the graph neural networks such that they perform worse than a simple baseline that ignores all relational information. Our attacks do not assume any knowledge about or access to the target classifiers.", "keywords": ["graph mining", "adversarial attacks", "meta learning", "graph neural networks", "node classification"], "authorids": ["zuegnerd@in.tum.de", "guennemann@in.tum.de"], "authors": ["Daniel Z\u00fcgner", "Stephan G\u00fcnnemann"], "TL;DR": "We use meta-gradients to attack the training procedure of deep neural networks for graphs.", "pdf": "/pdf/9c456c2747f67ea78b03bd154b4ce729e1410d56.pdf", "paperhash": "z\u00fcgner|adversarial_attacks_on_graph_neural_networks_via_meta_learning", "_bibtex": "@inproceedings{\nz\u00fcgner2018adversarial,\ntitle={Adversarial Attacks on Graph Neural Networks via Meta Learning},\nauthor={Daniel Z\u00fcgner and Stephan G\u00fcnnemann},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Bylnx209YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1112/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617085, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bylnx209YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1112/Authors", "ICLR.cc/2019/Conference/Paper1112/Reviewers", "ICLR.cc/2019/Conference/Paper1112/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1112/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1112/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1112/Authors|ICLR.cc/2019/Conference/Paper1112/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1112/Reviewers", "ICLR.cc/2019/Conference/Paper1112/Authors", "ICLR.cc/2019/Conference/Paper1112/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617085}}}, {"id": "BJxOYyT-0m", "original": null, "number": 3, "cdate": 1542733696152, "ddate": null, "tcdate": 1542733696152, "tmdate": 1542733696152, "tddate": null, "forum": "Bylnx209YX", "replyto": "H1lFhSrF3X", "invitation": "ICLR.cc/2019/Conference/-/Paper1112/Official_Comment", "content": {"title": "Re: Review 3", "comment": "Dear Reviewer 3,\n\nThank you for your constructive feedback and suggestions. We used your suggestions to improve the manuscript.\n(1+3) We have added an algorithm summary and complexity discussion to the appendix. \n(2) As Reviewer 1 also requested information about graph attribute attacks, we ran experiments on Citeseer where we use meta gradients to modify the graph structure and features simultaneously. We evaluated on GCN and CLN (DeepWalk does not use features) and we observed that the impact of the combined attacks is comparable but slightly lower (GCN: 38.6 vs 37.2, CLN: 35.3 vs 34.2; structure-only vs combined). We attribute this to the fact that we assign the same \u2018cost\u2019 to structure and feature changes, but arguably we expect a structure perturbation to have a stronger effect on performance than a feature perturbation. We have summarized these findings in Appendix E of the updated manuscript.\n\nRegarding your question about the benefit of meta-learning: Meta learning is a principle that enables us to directly tackle the bilevel optimization problem. That is, the meta gradient gives us an indication of how the value of the outer optimization problem will change when modifying the input to the inner optimization problem (i.e. the classifier training). This proves to be a very powerful principle for poisoning attacks (essentially a bilevel optimization problem) on node classification as we show in our work.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1112/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1112/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1112/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Attacks on Graph Neural Networks via Meta Learning", "abstract": "Deep learning models for graphs have advanced the state of the art on many tasks. Despite their recent success, little is known about their robustness. We investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure.  Our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize. Our experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings. Remarkably, the perturbations created by our algorithm can misguide the graph neural networks such that they perform worse than a simple baseline that ignores all relational information. Our attacks do not assume any knowledge about or access to the target classifiers.", "keywords": ["graph mining", "adversarial attacks", "meta learning", "graph neural networks", "node classification"], "authorids": ["zuegnerd@in.tum.de", "guennemann@in.tum.de"], "authors": ["Daniel Z\u00fcgner", "Stephan G\u00fcnnemann"], "TL;DR": "We use meta-gradients to attack the training procedure of deep neural networks for graphs.", "pdf": "/pdf/9c456c2747f67ea78b03bd154b4ce729e1410d56.pdf", "paperhash": "z\u00fcgner|adversarial_attacks_on_graph_neural_networks_via_meta_learning", "_bibtex": "@inproceedings{\nz\u00fcgner2018adversarial,\ntitle={Adversarial Attacks on Graph Neural Networks via Meta Learning},\nauthor={Daniel Z\u00fcgner and Stephan G\u00fcnnemann},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Bylnx209YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1112/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617085, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bylnx209YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1112/Authors", "ICLR.cc/2019/Conference/Paper1112/Reviewers", "ICLR.cc/2019/Conference/Paper1112/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1112/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1112/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1112/Authors|ICLR.cc/2019/Conference/Paper1112/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1112/Reviewers", "ICLR.cc/2019/Conference/Paper1112/Authors", "ICLR.cc/2019/Conference/Paper1112/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617085}}}, {"id": "Skli62kQ6Q", "original": null, "number": 1, "cdate": 1541762242711, "ddate": null, "tcdate": 1541762242711, "tmdate": 1541762242711, "tddate": null, "forum": "Bylnx209YX", "replyto": "BkltS1uMpQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1112/Official_Comment", "content": {"title": "Authors' response", "comment": "Dear commenter,\n\nWhile we appreciate any constructive feedback and questions on OpenReview, we have the impression that you have not read our paper. Still, since your comment contains various incorrect claims, we address your points here:\n\n1) Graph neural networks are NOT a special case of networks for text classification. If at all, they are generalizations. We recommend to read the broad literature on graph neural networks to clarify your confusion (references are mentioned in our paper). Here we just want to point out two important differences: (i) The neighborhood in graphs is not ordered; unlike text/images where you have before-after/left-right-up-down information. (ii) The interaction structure in graphs, i.e. the edges, is an explicit part of the data (i.e. observed) -- while in text it is NOT. Put simply: The graph structure is part of the data and, thus, can be manipulated. This is what we consider in our work.\n\n2) You are linking to a discussion which does NOT apply to our setting. (i) It talks about text classification. (ii) The discussion you are linking to claims that text classification can easily be fooled (e.g. just simple random perturbations). Simple perturbations, however, do NOT have a strong effect on graph neural networks. This result was already clearly shown by other graph attack papers (see again the references in our paper). We also compare to strong baselines (including a random one) in our work which are consistently outperformed by our method.\n\n3) Your statement \u201cit is even easier to fool graph neural networks\u201d is simply incorrect. Due to (1) you cannot make any direct conclusion from text to graphs and due to (2) it has been shown that it is NOT easy to fool graph neural networks (e.g. with random perturbations). Due to the challenging nature of achieving graph attacks, we need more advanced principles -- like the one proposed in our paper."}, "signatures": ["ICLR.cc/2019/Conference/Paper1112/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1112/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1112/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Attacks on Graph Neural Networks via Meta Learning", "abstract": "Deep learning models for graphs have advanced the state of the art on many tasks. Despite their recent success, little is known about their robustness. We investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure.  Our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize. Our experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings. Remarkably, the perturbations created by our algorithm can misguide the graph neural networks such that they perform worse than a simple baseline that ignores all relational information. Our attacks do not assume any knowledge about or access to the target classifiers.", "keywords": ["graph mining", "adversarial attacks", "meta learning", "graph neural networks", "node classification"], "authorids": ["zuegnerd@in.tum.de", "guennemann@in.tum.de"], "authors": ["Daniel Z\u00fcgner", "Stephan G\u00fcnnemann"], "TL;DR": "We use meta-gradients to attack the training procedure of deep neural networks for graphs.", "pdf": "/pdf/9c456c2747f67ea78b03bd154b4ce729e1410d56.pdf", "paperhash": "z\u00fcgner|adversarial_attacks_on_graph_neural_networks_via_meta_learning", "_bibtex": "@inproceedings{\nz\u00fcgner2018adversarial,\ntitle={Adversarial Attacks on Graph Neural Networks via Meta Learning},\nauthor={Daniel Z\u00fcgner and Stephan G\u00fcnnemann},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Bylnx209YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1112/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617085, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bylnx209YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1112/Authors", "ICLR.cc/2019/Conference/Paper1112/Reviewers", "ICLR.cc/2019/Conference/Paper1112/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1112/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1112/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1112/Authors|ICLR.cc/2019/Conference/Paper1112/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1112/Reviewers", "ICLR.cc/2019/Conference/Paper1112/Authors", "ICLR.cc/2019/Conference/Paper1112/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617085}}}, {"id": "BkltS1uMpQ", "original": null, "number": 1, "cdate": 1541730112779, "ddate": null, "tcdate": 1541730112779, "tmdate": 1541730112779, "tddate": null, "forum": "Bylnx209YX", "replyto": "Bylnx209YX", "invitation": "ICLR.cc/2019/Conference/-/Paper1112/Public_Comment", "content": {"comment": "Graph neural networks are just special cases of neural networks for classifying text (which is just a chain graph). To generate text that fools state-of-the-art classifiers one doesn't need to do much, and certainly not the method used in the paper (see e.g. the discussion in https://openreview.net/forum?id=ByghKiC5YX&noteId=B1xno5Dz6X). It is therefore quite obvious that it is even easier to fool graph neural networks, so why all the fancy methods?", "title": "Why is this problem important?"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Attacks on Graph Neural Networks via Meta Learning", "abstract": "Deep learning models for graphs have advanced the state of the art on many tasks. Despite their recent success, little is known about their robustness. We investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure.  Our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize. Our experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings. Remarkably, the perturbations created by our algorithm can misguide the graph neural networks such that they perform worse than a simple baseline that ignores all relational information. Our attacks do not assume any knowledge about or access to the target classifiers.", "keywords": ["graph mining", "adversarial attacks", "meta learning", "graph neural networks", "node classification"], "authorids": ["zuegnerd@in.tum.de", "guennemann@in.tum.de"], "authors": ["Daniel Z\u00fcgner", "Stephan G\u00fcnnemann"], "TL;DR": "We use meta-gradients to attack the training procedure of deep neural networks for graphs.", "pdf": "/pdf/9c456c2747f67ea78b03bd154b4ce729e1410d56.pdf", "paperhash": "z\u00fcgner|adversarial_attacks_on_graph_neural_networks_via_meta_learning", "_bibtex": "@inproceedings{\nz\u00fcgner2018adversarial,\ntitle={Adversarial Attacks on Graph Neural Networks via Meta Learning},\nauthor={Daniel Z\u00fcgner and Stephan G\u00fcnnemann},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Bylnx209YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1112/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311675780, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "Bylnx209YX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1112/Authors", "ICLR.cc/2019/Conference/Paper1112/Reviewers", "ICLR.cc/2019/Conference/Paper1112/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1112/Authors", "ICLR.cc/2019/Conference/Paper1112/Reviewers", "ICLR.cc/2019/Conference/Paper1112/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311675780}}}, {"id": "H1lFhSrF3X", "original": null, "number": 3, "cdate": 1541129648733, "ddate": null, "tcdate": 1541129648733, "tmdate": 1541533412215, "tddate": null, "forum": "Bylnx209YX", "replyto": "Bylnx209YX", "invitation": "ICLR.cc/2019/Conference/-/Paper1112/Official_Review", "content": {"title": "Good paper of using meta-learning to solve the bilevel optimization problem in graph attacking", "review": "This paper proposes an algorithm to alter the structure of a graph by adding/deleting edges so as to degrade the global performance of node classification. The main idea is to use the idea of meta-gradients from meta-learning to solve the bilevel optimization problem. \n\nThe paper is clearly presented. The main contribution is to use meta-learning to solve the bilevel optimization in the discrete graph data using greedy selection approach. From the experimental results, this treatment is really effective in attacking the graph learning models (GCN, CLN, DeepWalk). However, the motivation in using meta-learning to solve the bilevel optimization is not very clear to me, e.g., what are the advantages it can offer?\n\nTheoretically, the paper could have given some discussion on the optimality of the meta-gradient approach to bilevel optimization to strengthen the theoretical aspect. For the greedy selection approach in Eq (8), is there any sub-modularity for the score function used?\n\nSome minor suggestions and comments:\n1) please summarize the attacking procedures in the form of an algorithm\n2) please have some discussion on attacking the graph attributes besides the structure\n3) please have an complexity analysis and empirical evaluations of the meta-gradient computations and approximations", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1112/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Attacks on Graph Neural Networks via Meta Learning", "abstract": "Deep learning models for graphs have advanced the state of the art on many tasks. Despite their recent success, little is known about their robustness. We investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure.  Our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize. Our experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings. Remarkably, the perturbations created by our algorithm can misguide the graph neural networks such that they perform worse than a simple baseline that ignores all relational information. Our attacks do not assume any knowledge about or access to the target classifiers.", "keywords": ["graph mining", "adversarial attacks", "meta learning", "graph neural networks", "node classification"], "authorids": ["zuegnerd@in.tum.de", "guennemann@in.tum.de"], "authors": ["Daniel Z\u00fcgner", "Stephan G\u00fcnnemann"], "TL;DR": "We use meta-gradients to attack the training procedure of deep neural networks for graphs.", "pdf": "/pdf/9c456c2747f67ea78b03bd154b4ce729e1410d56.pdf", "paperhash": "z\u00fcgner|adversarial_attacks_on_graph_neural_networks_via_meta_learning", "_bibtex": "@inproceedings{\nz\u00fcgner2018adversarial,\ntitle={Adversarial Attacks on Graph Neural Networks via Meta Learning},\nauthor={Daniel Z\u00fcgner and Stephan G\u00fcnnemann},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Bylnx209YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1112/Official_Review", "cdate": 1542234303399, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Bylnx209YX", "replyto": "Bylnx209YX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1112/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335875247, "tmdate": 1552335875247, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1112/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HkeXZtLM2m", "original": null, "number": 1, "cdate": 1540675834715, "ddate": null, "tcdate": 1540675834715, "tmdate": 1541533411795, "tddate": null, "forum": "Bylnx209YX", "replyto": "Bylnx209YX", "invitation": "ICLR.cc/2019/Conference/-/Paper1112/Official_Review", "content": {"title": "interesting idea and good results", "review": "This paper studied data poisoning attacking for graph neural networks. The authors proposed treating graph structures as hyperparameters and leveraged recent progress on meta-learning for optimizing the adversarial attacks. Different from some recent work on adversarial attacks for graph neural networks (Zuigner et al. 2018; Dai et al. 2018), which focus on attacking specific nodes, this paper focuses on attacking the  overall performance of graph neural networks. Experiments on a few data sets prove the effectiveness of the proposed approach. \n\nStrength:\n- the studied problem is very important and recently attracting increasing attention\n- Experiments show that the proposed method is effective.\n\nWeakness:\n- the complexity of the proposed method seems to be very high\n- the data sets used in the experiments are too small\nDetails:\n-- the complexity of the proposed method seems to be very high. The authors should explicitly discuss the complexity of the proposed method. \n-- the data sets in the experiments are too small. Some large data sets would be much more compelling.\n-- Are the adversarial examples identified by the proposed method transferrable to other graph embedding algorithms (e.g., the unsupervised node embedding methods, DeepWalk, LINE, and node2vec)?\n-- I like Figure 3, though some concrete examples would be more intuitive. ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1112/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Attacks on Graph Neural Networks via Meta Learning", "abstract": "Deep learning models for graphs have advanced the state of the art on many tasks. Despite their recent success, little is known about their robustness. We investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure.  Our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize. Our experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings. Remarkably, the perturbations created by our algorithm can misguide the graph neural networks such that they perform worse than a simple baseline that ignores all relational information. Our attacks do not assume any knowledge about or access to the target classifiers.", "keywords": ["graph mining", "adversarial attacks", "meta learning", "graph neural networks", "node classification"], "authorids": ["zuegnerd@in.tum.de", "guennemann@in.tum.de"], "authors": ["Daniel Z\u00fcgner", "Stephan G\u00fcnnemann"], "TL;DR": "We use meta-gradients to attack the training procedure of deep neural networks for graphs.", "pdf": "/pdf/9c456c2747f67ea78b03bd154b4ce729e1410d56.pdf", "paperhash": "z\u00fcgner|adversarial_attacks_on_graph_neural_networks_via_meta_learning", "_bibtex": "@inproceedings{\nz\u00fcgner2018adversarial,\ntitle={Adversarial Attacks on Graph Neural Networks via Meta Learning},\nauthor={Daniel Z\u00fcgner and Stephan G\u00fcnnemann},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Bylnx209YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1112/Official_Review", "cdate": 1542234303399, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Bylnx209YX", "replyto": "Bylnx209YX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1112/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335875247, "tmdate": 1552335875247, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1112/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 11}