{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028587869, "tcdate": 1490028587869, "number": 1, "id": "SJEVOYpse", "invitation": "ICLR.cc/2017/workshop/-/paper82/acceptance", "forum": "BkL7bONFe", "replyto": "BkL7bONFe", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Joint Multimodal Learning with Deep Generative Models", "abstract": "We investigate deep generative models that can exchange multiple modalities bi-directionally, e.g., generating images from corresponding texts and vice versa. Recently, some studies handle multiple modalities on deep generative models. However, these models typically assume that modalities are forced to have a conditioned relation, i.e., we can only generate modalities in one direction. To achieve our objective, we should extract a joint representation that captures high-level concepts among all modalities and through which we can exchange them bi-directionally. As described herein, we propose a joint multimodal variational autoencoder (JMVAE), in which all modalities are independently conditioned on joint representation. In other words, it models a joint distribution of modalities. Furthermore, to be able to generate missing modalities from the remaining modalities properly, we develop an additional method, JMVAE-kl, that is trained by reducing the divergence between JMVAE's encoder and prepared networks of respective modalities. Our experiments show that JMVAE can generate multiple modalities bi-directionally.", "pdf": "/pdf/4aa63871a80d9226b27c9a257623615868431d92.pdf", "paperhash": "suzuki|joint_multimodal_learning_with_deep_generative_models", "conflicts": ["u-tokyo.ac.jp"], "keywords": [], "authors": ["Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["masa@weblab.t.u-tokyo.ac.jp", "k-nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028588443, "id": "ICLR.cc/2017/workshop/-/paper82/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "BkL7bONFe", "replyto": "BkL7bONFe", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028588443}}}, {"tddate": null, "tmdate": 1489361023172, "tcdate": 1489361023172, "number": 2, "id": "HkwtuUXol", "invitation": "ICLR.cc/2017/workshop/-/paper82/official/review", "forum": "BkL7bONFe", "replyto": "BkL7bONFe", "signatures": ["ICLR.cc/2017/workshop/paper82/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper82/AnonReviewer2"], "content": {"title": "A sound model with good preliminary results", "rating": "7: Good paper, accept", "review": "Paper summary\nThis paper proposes a VAE model for modeling multimodal data.\nAdditionally, a KL-divergence term is added in order to encourage the model to\nlearn a latent representation such that the same representation can be inferred\nfrom any one modality alone. This makes it possible to use the inferred\nrepresentation bidirectionally - i.e. to go from any one modality to another.\n\nPros:\n- The models gets better or comparable log probs when compared to relevant baselines.\n- The analysis of the learned representation is well presented.\n\nCons:\n- A comparison of JMVAE and JMVAE-kl is not made.\n- A description of the network architecture is not given. Presumably this looks\n  like two pathways (one for each modality) fused together at the top (similar\nto the ones in Ngiam et al. 2011) to make it easy to split the parameters into\n\\theta_x and \\theta_w. Some description of this network should be added.\n\nMinor comments\n- Please check the legend in Fig 1(b). It seems that the green dots should be \"Base\". Also some of the images seem to be repeated.\n- Please reconsider the use of `w' to represent a modality. w is often\n  associated with the weights of a network so this makes it a little jarring to\nread.\n- \"then the inferred latent variable becomes incomplete and generated samples might collapse\" : not clear what this means. Please explain.\n\nOverall, the paper describes a sound model with good preliminary results.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Joint Multimodal Learning with Deep Generative Models", "abstract": "We investigate deep generative models that can exchange multiple modalities bi-directionally, e.g., generating images from corresponding texts and vice versa. Recently, some studies handle multiple modalities on deep generative models. However, these models typically assume that modalities are forced to have a conditioned relation, i.e., we can only generate modalities in one direction. To achieve our objective, we should extract a joint representation that captures high-level concepts among all modalities and through which we can exchange them bi-directionally. As described herein, we propose a joint multimodal variational autoencoder (JMVAE), in which all modalities are independently conditioned on joint representation. In other words, it models a joint distribution of modalities. Furthermore, to be able to generate missing modalities from the remaining modalities properly, we develop an additional method, JMVAE-kl, that is trained by reducing the divergence between JMVAE's encoder and prepared networks of respective modalities. Our experiments show that JMVAE can generate multiple modalities bi-directionally.", "pdf": "/pdf/4aa63871a80d9226b27c9a257623615868431d92.pdf", "paperhash": "suzuki|joint_multimodal_learning_with_deep_generative_models", "conflicts": ["u-tokyo.ac.jp"], "keywords": [], "authors": ["Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["masa@weblab.t.u-tokyo.ac.jp", "k-nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489361023971, "id": "ICLR.cc/2017/workshop/-/paper82/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper82/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper82/AnonReviewer1", "ICLR.cc/2017/workshop/paper82/AnonReviewer2"], "reply": {"forum": "BkL7bONFe", "replyto": "BkL7bONFe", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper82/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper82/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489361023971}}}, {"tddate": null, "tmdate": 1489172690621, "tcdate": 1489172690621, "number": 1, "id": "rJiAuulsx", "invitation": "ICLR.cc/2017/workshop/-/paper82/official/review", "forum": "BkL7bONFe", "replyto": "BkL7bONFe", "signatures": ["ICLR.cc/2017/workshop/paper82/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper82/AnonReviewer1"], "content": {"title": "VAE for learning the joint density of images and image-attributes", "rating": "6: Marginally above acceptance threshold", "review": "Summary\n\nThis paper investigates two versions of the VAE model in the context of learning the joint density of images and image-attributes. The authors introduce auxiliary conditional densities to help with the task of sampling images conditioned on text and vice-versa.\n\n\nNovelty\n\nWhile images and text may seem like different things, from a mathematical perspective of modeling their joint densities they are not. The assumption that data comes as a pair (x, w) or as a single vector x makes no difference from the model's perspective.\nFor instance, the authors propose a factorized structure p(x,w|z) = p(x|z)p(w|z). This assumption by itself is not novel as already in the VAE model pixels are assumed to be independent of each other given z. \nThe main contribution of this work is in proposing the JMVAE-kl model, which introduces the uni-modal conditional densities p(z|x) and p(z|w) and a modified loss to make these densities close to the variational posterior q(z|x,w).\n\n\nClarity\n\nAlthough the specific model architectures are not explained, the text is overall clear.\n\n\nSignificance\n\nThe results shown in this paper are very specific. But this work introduces an alternative way of learning conditional VAEs worth of further investigations. \n\n\nMinor points:\nIt would be good to have confidence intervals for the numbers on Table 1.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Joint Multimodal Learning with Deep Generative Models", "abstract": "We investigate deep generative models that can exchange multiple modalities bi-directionally, e.g., generating images from corresponding texts and vice versa. Recently, some studies handle multiple modalities on deep generative models. However, these models typically assume that modalities are forced to have a conditioned relation, i.e., we can only generate modalities in one direction. To achieve our objective, we should extract a joint representation that captures high-level concepts among all modalities and through which we can exchange them bi-directionally. As described herein, we propose a joint multimodal variational autoencoder (JMVAE), in which all modalities are independently conditioned on joint representation. In other words, it models a joint distribution of modalities. Furthermore, to be able to generate missing modalities from the remaining modalities properly, we develop an additional method, JMVAE-kl, that is trained by reducing the divergence between JMVAE's encoder and prepared networks of respective modalities. Our experiments show that JMVAE can generate multiple modalities bi-directionally.", "pdf": "/pdf/4aa63871a80d9226b27c9a257623615868431d92.pdf", "paperhash": "suzuki|joint_multimodal_learning_with_deep_generative_models", "conflicts": ["u-tokyo.ac.jp"], "keywords": [], "authors": ["Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["masa@weblab.t.u-tokyo.ac.jp", "k-nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489361023971, "id": "ICLR.cc/2017/workshop/-/paper82/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper82/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper82/AnonReviewer1", "ICLR.cc/2017/workshop/paper82/AnonReviewer2"], "reply": {"forum": "BkL7bONFe", "replyto": "BkL7bONFe", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper82/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper82/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489361023971}}}, {"tddate": null, "replyto": null, "nonreaders": null, "ddate": null, "writable": true, "revisions": true, "tmdate": 1487719902965, "tcdate": 1487335710433, "number": 82, "replyCount": 0, "id": "BkL7bONFe", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "BkL7bONFe", "original": "Hk8rlUqge", "signatures": ["~Masahiro_Suzuki1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Joint Multimodal Learning with Deep Generative Models", "abstract": "We investigate deep generative models that can exchange multiple modalities bi-directionally, e.g., generating images from corresponding texts and vice versa. Recently, some studies handle multiple modalities on deep generative models. However, these models typically assume that modalities are forced to have a conditioned relation, i.e., we can only generate modalities in one direction. To achieve our objective, we should extract a joint representation that captures high-level concepts among all modalities and through which we can exchange them bi-directionally. As described herein, we propose a joint multimodal variational autoencoder (JMVAE), in which all modalities are independently conditioned on joint representation. In other words, it models a joint distribution of modalities. Furthermore, to be able to generate missing modalities from the remaining modalities properly, we develop an additional method, JMVAE-kl, that is trained by reducing the divergence between JMVAE's encoder and prepared networks of respective modalities. Our experiments show that JMVAE can generate multiple modalities bi-directionally.", "pdf": "/pdf/4aa63871a80d9226b27c9a257623615868431d92.pdf", "paperhash": "suzuki|joint_multimodal_learning_with_deep_generative_models", "conflicts": ["u-tokyo.ac.jp"], "keywords": [], "authors": ["Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["masa@weblab.t.u-tokyo.ac.jp", "k-nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "writers": [], "tags": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "original": {"tddate": null, "replyto": null, "ddate": null, "active": true, "tmdate": 1481719347822, "tcdate": 1478283325857, "number": 284, "id": "Hk8rlUqge", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "Hk8rlUqge", "signatures": ["~Masahiro_Suzuki1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Joint Multimodal Learning with Deep Generative Models", "abstract": "We investigate deep generative models that can exchange multiple modalities bi-directionally, e.g., generating images from corresponding texts and vice versa. Recently, some studies handle multiple modalities on deep generative models, such as variational autoencoders (VAEs). However, these models typically assume that modalities are forced to have a conditioned relation, i.e., we can only generate modalities in one direction. To achieve our objective, we should extract a joint representation that captures high-level concepts among all modalities and through which we can exchange them bi-directionally. As described herein, we propose a joint multimodal variational autoencoder (JMVAE), in which all modalities are independently conditioned on joint representation. In other words, it models a joint distribution of modalities. Furthermore, to be able to generate missing modalities from the remaining modalities properly, we develop an additional method, JMVAE-kl, that is trained by reducing the divergence between JMVAE's encoder and prepared networks of respective modalities. Our experiments show that our proposed method can obtain appropriate joint representation from multiple modalities and that it can generate and reconstruct them more properly than conventional VAEs. We further demonstrate that JMVAE can generate multiple modalities bi-directionally.\n", "pdf": "/pdf/cb01f96bb430eeac976c1e4ea3a12095c97881a8.pdf", "paperhash": "suzuki|joint_multimodal_learning_with_deep_generative_models", "conflicts": ["u-tokyo.ac.jp"], "keywords": [], "authors": ["Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["masa@weblab.t.u-tokyo.ac.jp", "k-nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "writers": [], "nonreaders": []}, "originalWritable": false, "originalInvitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}, "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}], "count": 4}