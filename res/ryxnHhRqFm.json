{"notes": [{"id": "ryxnHhRqFm", "original": "ryl8EsY9tm", "number": 1581, "cdate": 1538088004311, "ddate": null, "tcdate": 1538088004311, "tmdate": 1547543031921, "tddate": null, "forum": "ryxnHhRqFm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Global-to-local Memory Pointer Networks for Task-Oriented Dialogue", "abstract": "End-to-end task-oriented dialogue is challenging since knowledge bases are usually large, dynamic and hard to incorporate into a learning framework. We propose the global-to-local memory pointer (GLMP) networks to address this issue. In our model, a global memory encoder and a local memory decoder are proposed to share external knowledge. The encoder encodes dialogue history, modifies global contextual representation, and generates a global memory pointer. The decoder first generates a sketch response with unfilled slots. Next, it passes the global memory pointer to filter the external knowledge for relevant information, then instantiates the slots via the local memory pointers. We empirically show that our model can improve copy accuracy and mitigate the common out-of-vocabulary problem. As a result, GLMP is able to improve over the previous state-of-the-art models in both simulated bAbI Dialogue dataset and human-human Stanford Multi-domain Dialogue dataset on automatic and human evaluation.", "keywords": ["pointer networks", "memory networks", "task-oriented dialogue systems", "natural language processing"], "authorids": ["jason.wu@connect.ust.hk", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chien-Sheng Wu", "Richard Socher", "Caiming Xiong"], "TL;DR": "GLMP: Global memory encoder (context RNN, global pointer) and local memory decoder (sketch RNN, local pointer) that share external knowledge (MemNN) are proposed to strengthen response generation in task-oriented dialogue.", "pdf": "/pdf/db8acf203bd7c5a73dd104a07690d8f6f8d85757.pdf", "paperhash": "wu|globaltolocal_memory_pointer_networks_for_taskoriented_dialogue", "_bibtex": "@inproceedings{\nwu2018globaltolocal,\ntitle={Global-to-local Memory Pointer Networks for Task-Oriented Dialogue},\nauthor={Chien-Sheng Wu and Richard Socher and Caiming Xiong},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxnHhRqFm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "r1gSR9IA2X", "original": null, "number": 1, "cdate": 1541462732575, "ddate": null, "tcdate": 1541462732575, "tmdate": 1545354472664, "tddate": null, "forum": "ryxnHhRqFm", "replyto": "ryxnHhRqFm", "invitation": "ICLR.cc/2019/Conference/-/Paper1581/Meta_Review", "content": {"metareview": "Interesting paper applying memory networks that encode external knowledge (represented in the form of triples) and conversation context for task oriented dialogues. Experiments demonstrate improvements over the state of the art on two public datasets. \nNotation and presentation in the first version of the paper were not very clear, hence many question and answers were exchanged during the reviews. \n", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "novel architecture for task oriented dialogue systems"}, "signatures": ["ICLR.cc/2019/Conference/Paper1581/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1581/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Global-to-local Memory Pointer Networks for Task-Oriented Dialogue", "abstract": "End-to-end task-oriented dialogue is challenging since knowledge bases are usually large, dynamic and hard to incorporate into a learning framework. We propose the global-to-local memory pointer (GLMP) networks to address this issue. In our model, a global memory encoder and a local memory decoder are proposed to share external knowledge. The encoder encodes dialogue history, modifies global contextual representation, and generates a global memory pointer. The decoder first generates a sketch response with unfilled slots. Next, it passes the global memory pointer to filter the external knowledge for relevant information, then instantiates the slots via the local memory pointers. We empirically show that our model can improve copy accuracy and mitigate the common out-of-vocabulary problem. As a result, GLMP is able to improve over the previous state-of-the-art models in both simulated bAbI Dialogue dataset and human-human Stanford Multi-domain Dialogue dataset on automatic and human evaluation.", "keywords": ["pointer networks", "memory networks", "task-oriented dialogue systems", "natural language processing"], "authorids": ["jason.wu@connect.ust.hk", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chien-Sheng Wu", "Richard Socher", "Caiming Xiong"], "TL;DR": "GLMP: Global memory encoder (context RNN, global pointer) and local memory decoder (sketch RNN, local pointer) that share external knowledge (MemNN) are proposed to strengthen response generation in task-oriented dialogue.", "pdf": "/pdf/db8acf203bd7c5a73dd104a07690d8f6f8d85757.pdf", "paperhash": "wu|globaltolocal_memory_pointer_networks_for_taskoriented_dialogue", "_bibtex": "@inproceedings{\nwu2018globaltolocal,\ntitle={Global-to-local Memory Pointer Networks for Task-Oriented Dialogue},\nauthor={Chien-Sheng Wu and Richard Socher and Caiming Xiong},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxnHhRqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1581/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352784761, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryxnHhRqFm", "replyto": "ryxnHhRqFm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1581/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1581/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1581/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352784761}}}, {"id": "rJlOcp5YRm", "original": null, "number": 8, "cdate": 1543249295775, "ddate": null, "tcdate": 1543249295775, "tmdate": 1543249295775, "tddate": null, "forum": "ryxnHhRqFm", "replyto": "BJeSLd9FRQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1581/Official_Comment", "content": {"title": "Covered", "comment": "Many thanks for the most detailed reply. It was most enlightening. Yes please, do add that to the discussion. I believe many people in the field would be interested in your point of view. Many thanks again!"}, "signatures": ["ICLR.cc/2019/Conference/Paper1581/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1581/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1581/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Global-to-local Memory Pointer Networks for Task-Oriented Dialogue", "abstract": "End-to-end task-oriented dialogue is challenging since knowledge bases are usually large, dynamic and hard to incorporate into a learning framework. We propose the global-to-local memory pointer (GLMP) networks to address this issue. In our model, a global memory encoder and a local memory decoder are proposed to share external knowledge. The encoder encodes dialogue history, modifies global contextual representation, and generates a global memory pointer. The decoder first generates a sketch response with unfilled slots. Next, it passes the global memory pointer to filter the external knowledge for relevant information, then instantiates the slots via the local memory pointers. We empirically show that our model can improve copy accuracy and mitigate the common out-of-vocabulary problem. As a result, GLMP is able to improve over the previous state-of-the-art models in both simulated bAbI Dialogue dataset and human-human Stanford Multi-domain Dialogue dataset on automatic and human evaluation.", "keywords": ["pointer networks", "memory networks", "task-oriented dialogue systems", "natural language processing"], "authorids": ["jason.wu@connect.ust.hk", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chien-Sheng Wu", "Richard Socher", "Caiming Xiong"], "TL;DR": "GLMP: Global memory encoder (context RNN, global pointer) and local memory decoder (sketch RNN, local pointer) that share external knowledge (MemNN) are proposed to strengthen response generation in task-oriented dialogue.", "pdf": "/pdf/db8acf203bd7c5a73dd104a07690d8f6f8d85757.pdf", "paperhash": "wu|globaltolocal_memory_pointer_networks_for_taskoriented_dialogue", "_bibtex": "@inproceedings{\nwu2018globaltolocal,\ntitle={Global-to-local Memory Pointer Networks for Task-Oriented Dialogue},\nauthor={Chien-Sheng Wu and Richard Socher and Caiming Xiong},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxnHhRqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1581/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618165, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryxnHhRqFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1581/Authors", "ICLR.cc/2019/Conference/Paper1581/Reviewers", "ICLR.cc/2019/Conference/Paper1581/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1581/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1581/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1581/Authors|ICLR.cc/2019/Conference/Paper1581/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1581/Reviewers", "ICLR.cc/2019/Conference/Paper1581/Authors", "ICLR.cc/2019/Conference/Paper1581/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618165}}}, {"id": "SkgRl95FR7", "original": null, "number": 7, "cdate": 1543248373980, "ddate": null, "tcdate": 1543248373980, "tmdate": 1543248373980, "tddate": null, "forum": "ryxnHhRqFm", "replyto": "BJgs2DPSR7", "invitation": "ICLR.cc/2019/Conference/-/Paper1581/Official_Comment", "content": {"title": "Thank you for raising your concerns", "comment": "Please let us reply you as below:\n\n1) The entity type information we mentioned is the slot type representation (embedding) for each slot. For example, to the best of our knowledge, in the MemNN for bAbi dialogue, they have 7 special words (embeddings) for 7 different slot types that they then added to all the words that are related to them. In this way, for example, the representation of \u201cParis\u201d can include the information of \u201clocation\u201d. \n\nIn our case, our model does not have the explicit information in the \u201cParis embedding\u201d that it is a \u201clocation\u201d. Please note that when we encode the dialogue history, we used the plain text input, that is, the \u201cParis\u201d embedding does not include the \u201clocation\u201d type embedding. Even if an OOV word comes into place, our model did not add the type information as the others did, instead, we added the hidden states of the RNN encoder. The sketch response is only used while \"decoding\", not \"encoding\". During the encoding stage, all the input are plain texts, not the sketch sentences. Hope this makes it clear about your question one and two. \n\n2) We total understood your concern about the KB. Please note that for each \u201cnode\u201d, we summed up the embeddings of (Subject, Relation, Object), then we assume that every time the \u201cnode\u201d is pointed to, we copy the Object word out (it\u2019s our own rule we defined). Therefore, there is no constraint that what needs to be a subject and what needs to be an object. The only thing important in our task is we need to be able to copy every entity that may exist in a response. Thus, we need at least a node that we can copy the \u201cname of entity\u201d out, for example, the restaurant name. That is, either we decide to copy the Subject for the entity names, or we just simply represent the name of the entity as an Object in one node. This is a matter of design. We do this is because it is easy for us to maintain our code. Therefore, hope this explains your question three. \n\nIn addition, as we mentioned in the last post, there are many different ways to represent the KB information, some may use flat KB as we did (ex: mem2seq), some may use the hierarchical one. Although flat KB might not be the most effective one (because the hierarchical one is easier for machine to reason KB, the nodes are assumed to connect by the entity names), we choose this preprocessing strategy and left the ability of connecting the nodes to our system, so does some previous works, is because it is simple and fast. The comparison between these two could be an interesting future work. \n\n3) Lastly, we will release our code if our work is published. If you have any further question about the preprocessing or model architecture, etc, we hope that can make you more clear. \n\nThank you again for your interests in our work. Very happy to hear that."}, "signatures": ["ICLR.cc/2019/Conference/Paper1581/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1581/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1581/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Global-to-local Memory Pointer Networks for Task-Oriented Dialogue", "abstract": "End-to-end task-oriented dialogue is challenging since knowledge bases are usually large, dynamic and hard to incorporate into a learning framework. We propose the global-to-local memory pointer (GLMP) networks to address this issue. In our model, a global memory encoder and a local memory decoder are proposed to share external knowledge. The encoder encodes dialogue history, modifies global contextual representation, and generates a global memory pointer. The decoder first generates a sketch response with unfilled slots. Next, it passes the global memory pointer to filter the external knowledge for relevant information, then instantiates the slots via the local memory pointers. We empirically show that our model can improve copy accuracy and mitigate the common out-of-vocabulary problem. As a result, GLMP is able to improve over the previous state-of-the-art models in both simulated bAbI Dialogue dataset and human-human Stanford Multi-domain Dialogue dataset on automatic and human evaluation.", "keywords": ["pointer networks", "memory networks", "task-oriented dialogue systems", "natural language processing"], "authorids": ["jason.wu@connect.ust.hk", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chien-Sheng Wu", "Richard Socher", "Caiming Xiong"], "TL;DR": "GLMP: Global memory encoder (context RNN, global pointer) and local memory decoder (sketch RNN, local pointer) that share external knowledge (MemNN) are proposed to strengthen response generation in task-oriented dialogue.", "pdf": "/pdf/db8acf203bd7c5a73dd104a07690d8f6f8d85757.pdf", "paperhash": "wu|globaltolocal_memory_pointer_networks_for_taskoriented_dialogue", "_bibtex": "@inproceedings{\nwu2018globaltolocal,\ntitle={Global-to-local Memory Pointer Networks for Task-Oriented Dialogue},\nauthor={Chien-Sheng Wu and Richard Socher and Caiming Xiong},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxnHhRqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1581/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618165, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryxnHhRqFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1581/Authors", "ICLR.cc/2019/Conference/Paper1581/Reviewers", "ICLR.cc/2019/Conference/Paper1581/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1581/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1581/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1581/Authors|ICLR.cc/2019/Conference/Paper1581/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1581/Reviewers", "ICLR.cc/2019/Conference/Paper1581/Authors", "ICLR.cc/2019/Conference/Paper1581/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618165}}}, {"id": "BJeSLd9FRQ", "original": null, "number": 6, "cdate": 1543247948997, "ddate": null, "tcdate": 1543247948997, "tmdate": 1543247948997, "tddate": null, "forum": "ryxnHhRqFm", "replyto": "ByxLNlPEAQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1581/Official_Comment", "content": {"title": "Re: Reviewer3", "comment": "Yes, we agree with you that it will be interesting to have a comparison of the end-to-end systems with the modularized systems. However, please let us show some difficulties to design a system like that using pydial in the SMD and bAbI datasets we used in our paper:\n\nTo the best of our knowledge, in the pydial framework, it requires to have the dialogue act\u2019s labels for the NLU module and the belief states\u2019 labels for the belief tracker module. The biggest challenge here is we do not have such labels in the SMD and bAbI datasets we used. Moreover, the semi tracker in pydial is rule-based (ex: self.slot_vocab[\"pricerange\"] = \"(price|cost)(\\ ?range)*\"), which need to re-write rules whenever it encounters a new domain or new datasets. Even its dialogue management module could be a learning solution like policy networks, the input of the policy network is still the hand-crafted state features and labels. Therefore, without the rules and labels predefined in the NLU and belief tracker modules, pydial couldn\u2019t learn a good policy network. \n\nLastly, for now, based on the data we have (not very big size) and the current SOTA machine learning algorithms and models, we believe that a well and carefully constructed task-oriented dialogue system (like pydial) in a known domain using human rules (in NLU and Belief Tracker) with policy networks may outperform the end-to-end systems. However, in this paper, without additional human labels and human rules, we want to explore the potential and the advantage of end-to-end systems. Besides easy to train, for multi-domain cases, or even zero-shot domain cases, we believe end-to-end approaches will have better adaptability compared to any rule-based systems. We will include this discussion in our paper. \n \nThank you again for your feedback and we really appreciate it. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1581/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1581/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1581/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Global-to-local Memory Pointer Networks for Task-Oriented Dialogue", "abstract": "End-to-end task-oriented dialogue is challenging since knowledge bases are usually large, dynamic and hard to incorporate into a learning framework. We propose the global-to-local memory pointer (GLMP) networks to address this issue. In our model, a global memory encoder and a local memory decoder are proposed to share external knowledge. The encoder encodes dialogue history, modifies global contextual representation, and generates a global memory pointer. The decoder first generates a sketch response with unfilled slots. Next, it passes the global memory pointer to filter the external knowledge for relevant information, then instantiates the slots via the local memory pointers. We empirically show that our model can improve copy accuracy and mitigate the common out-of-vocabulary problem. As a result, GLMP is able to improve over the previous state-of-the-art models in both simulated bAbI Dialogue dataset and human-human Stanford Multi-domain Dialogue dataset on automatic and human evaluation.", "keywords": ["pointer networks", "memory networks", "task-oriented dialogue systems", "natural language processing"], "authorids": ["jason.wu@connect.ust.hk", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chien-Sheng Wu", "Richard Socher", "Caiming Xiong"], "TL;DR": "GLMP: Global memory encoder (context RNN, global pointer) and local memory decoder (sketch RNN, local pointer) that share external knowledge (MemNN) are proposed to strengthen response generation in task-oriented dialogue.", "pdf": "/pdf/db8acf203bd7c5a73dd104a07690d8f6f8d85757.pdf", "paperhash": "wu|globaltolocal_memory_pointer_networks_for_taskoriented_dialogue", "_bibtex": "@inproceedings{\nwu2018globaltolocal,\ntitle={Global-to-local Memory Pointer Networks for Task-Oriented Dialogue},\nauthor={Chien-Sheng Wu and Richard Socher and Caiming Xiong},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxnHhRqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1581/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618165, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryxnHhRqFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1581/Authors", "ICLR.cc/2019/Conference/Paper1581/Reviewers", "ICLR.cc/2019/Conference/Paper1581/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1581/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1581/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1581/Authors|ICLR.cc/2019/Conference/Paper1581/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1581/Reviewers", "ICLR.cc/2019/Conference/Paper1581/Authors", "ICLR.cc/2019/Conference/Paper1581/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618165}}}, {"id": "BJgs2DPSR7", "original": null, "number": 2, "cdate": 1542973363403, "ddate": null, "tcdate": 1542973363403, "tmdate": 1542973363403, "tddate": null, "forum": "ryxnHhRqFm", "replyto": "B1xRlvQppm", "invitation": "ICLR.cc/2019/Conference/-/Paper1581/Public_Comment", "content": {"comment": "Thank you for your reply, but some of my concerns are still unclear.\n\nEntity Type Information:\nThank you for clearly stating that GLMP uses entity type information to create sketch responses. You have mentioned GLMP did not add the \u201centity type\u201d information into the word representation, but neither did the existing approaches that use match feature (QRN, MemNN and Gated MemNN).\n\nAt a high level, there exists one set of approaches that use entity type information (in the model), and GLMP also uses the same information (in a different way). However, you have only compared against the models that do not use this information at all. This does not feel like a fair comparison.\n\nDataset Preparation:\nThank you for explaining the preprocessing in the SMD dataset, we felt it was not consistent with the explanation in section (2.1) which states \"each element bi \u2208 B is represented in the triplet format as (Subject, Relation, Object) structure\". Any comments about the preprocessing in the bAbI dataset? The reason for posing the question (Q3) in the initial comment is that we strongly feel that removing the preprocessing would considerably reduce the accuracy as well as task completion rate for T3-OOV and T5-OOV. \n\nWe would appreciate if you can answer the specific questions we raised. Your silence, suggests that the answers are likely, \n(Q1) very close performance\n(Q2) not very good\n(Q3) would not work very well \n", "title": "a few concerns"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Global-to-local Memory Pointer Networks for Task-Oriented Dialogue", "abstract": "End-to-end task-oriented dialogue is challenging since knowledge bases are usually large, dynamic and hard to incorporate into a learning framework. We propose the global-to-local memory pointer (GLMP) networks to address this issue. In our model, a global memory encoder and a local memory decoder are proposed to share external knowledge. The encoder encodes dialogue history, modifies global contextual representation, and generates a global memory pointer. The decoder first generates a sketch response with unfilled slots. Next, it passes the global memory pointer to filter the external knowledge for relevant information, then instantiates the slots via the local memory pointers. We empirically show that our model can improve copy accuracy and mitigate the common out-of-vocabulary problem. As a result, GLMP is able to improve over the previous state-of-the-art models in both simulated bAbI Dialogue dataset and human-human Stanford Multi-domain Dialogue dataset on automatic and human evaluation.", "keywords": ["pointer networks", "memory networks", "task-oriented dialogue systems", "natural language processing"], "authorids": ["jason.wu@connect.ust.hk", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chien-Sheng Wu", "Richard Socher", "Caiming Xiong"], "TL;DR": "GLMP: Global memory encoder (context RNN, global pointer) and local memory decoder (sketch RNN, local pointer) that share external knowledge (MemNN) are proposed to strengthen response generation in task-oriented dialogue.", "pdf": "/pdf/db8acf203bd7c5a73dd104a07690d8f6f8d85757.pdf", "paperhash": "wu|globaltolocal_memory_pointer_networks_for_taskoriented_dialogue", "_bibtex": "@inproceedings{\nwu2018globaltolocal,\ntitle={Global-to-local Memory Pointer Networks for Task-Oriented Dialogue},\nauthor={Chien-Sheng Wu and Richard Socher and Caiming Xiong},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxnHhRqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1581/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311563547, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "ryxnHhRqFm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1581/Authors", "ICLR.cc/2019/Conference/Paper1581/Reviewers", "ICLR.cc/2019/Conference/Paper1581/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1581/Authors", "ICLR.cc/2019/Conference/Paper1581/Reviewers", "ICLR.cc/2019/Conference/Paper1581/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311563547}}}, {"id": "ByxLNlPEAQ", "original": null, "number": 5, "cdate": 1542905902459, "ddate": null, "tcdate": 1542905902459, "tmdate": 1542905902459, "tddate": null, "forum": "ryxnHhRqFm", "replyto": "S1g_lF4U6X", "invitation": "ICLR.cc/2019/Conference/-/Paper1581/Official_Comment", "content": {"title": "Thanks - just one point", "comment": "For 9. I would still be interested (if possible and straight-forward) to see how you compare to pydial (that is not an encoder-decoder approach), since pydial policy manager is also NN (and not rule-based) as the Eric et al. 2017. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1581/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1581/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1581/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Global-to-local Memory Pointer Networks for Task-Oriented Dialogue", "abstract": "End-to-end task-oriented dialogue is challenging since knowledge bases are usually large, dynamic and hard to incorporate into a learning framework. We propose the global-to-local memory pointer (GLMP) networks to address this issue. In our model, a global memory encoder and a local memory decoder are proposed to share external knowledge. The encoder encodes dialogue history, modifies global contextual representation, and generates a global memory pointer. The decoder first generates a sketch response with unfilled slots. Next, it passes the global memory pointer to filter the external knowledge for relevant information, then instantiates the slots via the local memory pointers. We empirically show that our model can improve copy accuracy and mitigate the common out-of-vocabulary problem. As a result, GLMP is able to improve over the previous state-of-the-art models in both simulated bAbI Dialogue dataset and human-human Stanford Multi-domain Dialogue dataset on automatic and human evaluation.", "keywords": ["pointer networks", "memory networks", "task-oriented dialogue systems", "natural language processing"], "authorids": ["jason.wu@connect.ust.hk", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chien-Sheng Wu", "Richard Socher", "Caiming Xiong"], "TL;DR": "GLMP: Global memory encoder (context RNN, global pointer) and local memory decoder (sketch RNN, local pointer) that share external knowledge (MemNN) are proposed to strengthen response generation in task-oriented dialogue.", "pdf": "/pdf/db8acf203bd7c5a73dd104a07690d8f6f8d85757.pdf", "paperhash": "wu|globaltolocal_memory_pointer_networks_for_taskoriented_dialogue", "_bibtex": "@inproceedings{\nwu2018globaltolocal,\ntitle={Global-to-local Memory Pointer Networks for Task-Oriented Dialogue},\nauthor={Chien-Sheng Wu and Richard Socher and Caiming Xiong},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxnHhRqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1581/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618165, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryxnHhRqFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1581/Authors", "ICLR.cc/2019/Conference/Paper1581/Reviewers", "ICLR.cc/2019/Conference/Paper1581/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1581/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1581/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1581/Authors|ICLR.cc/2019/Conference/Paper1581/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1581/Reviewers", "ICLR.cc/2019/Conference/Paper1581/Authors", "ICLR.cc/2019/Conference/Paper1581/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618165}}}, {"id": "B1xRlvQppm", "original": null, "number": 4, "cdate": 1542432501875, "ddate": null, "tcdate": 1542432501875, "tmdate": 1542432847695, "tddate": null, "forum": "ryxnHhRqFm", "replyto": "BJetM8J3aX", "invitation": "ICLR.cc/2019/Conference/-/Paper1581/Official_Comment", "content": {"title": "Thank you for your feedback and your clear summary of our contribution", "comment": "Please let me reply to you below:\n\nFirst, in our experiment, we did not add the \u201centity type\u201d information into the word representation, which is same as the previous works such as Mem2Seq, MemNN, QRN, etc. Therefore, the comparison is fair. The step we did related to entity type was the sketch response preprocessing, based on the provided entity table (or the NER if the table is not provided), we can obtain our gold sketch responses for training. The local memory pointer is then learned to copy words to replace the generated sketch tags. Note that all the word-level representations in the external knowledge are not included the \u201ctype embedding\u201d.\n\nSecond, yes we followed the same preprocessing as in the Mem2Seq paper to represent our KB tuples. If you look into the original KB in the SMD dataset, it is not represented as the triplet format. Therefore, there are many different ways to represent the KB information, some may use flat KB as we did, some may use the hierarchical one, or even the input the table-like KB. Although it might not be the most effective one, we choose this preprocessing strategy, so does the previous works, is because it is simple and fast. There are some related works have tried different ways to represent KB information, but it may need additional attention calculation for entity copying. The comparison between these KB structures are interesting and could be our future works.\n\nThank you again for your interest in our work. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1581/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1581/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1581/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Global-to-local Memory Pointer Networks for Task-Oriented Dialogue", "abstract": "End-to-end task-oriented dialogue is challenging since knowledge bases are usually large, dynamic and hard to incorporate into a learning framework. We propose the global-to-local memory pointer (GLMP) networks to address this issue. In our model, a global memory encoder and a local memory decoder are proposed to share external knowledge. The encoder encodes dialogue history, modifies global contextual representation, and generates a global memory pointer. The decoder first generates a sketch response with unfilled slots. Next, it passes the global memory pointer to filter the external knowledge for relevant information, then instantiates the slots via the local memory pointers. We empirically show that our model can improve copy accuracy and mitigate the common out-of-vocabulary problem. As a result, GLMP is able to improve over the previous state-of-the-art models in both simulated bAbI Dialogue dataset and human-human Stanford Multi-domain Dialogue dataset on automatic and human evaluation.", "keywords": ["pointer networks", "memory networks", "task-oriented dialogue systems", "natural language processing"], "authorids": ["jason.wu@connect.ust.hk", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chien-Sheng Wu", "Richard Socher", "Caiming Xiong"], "TL;DR": "GLMP: Global memory encoder (context RNN, global pointer) and local memory decoder (sketch RNN, local pointer) that share external knowledge (MemNN) are proposed to strengthen response generation in task-oriented dialogue.", "pdf": "/pdf/db8acf203bd7c5a73dd104a07690d8f6f8d85757.pdf", "paperhash": "wu|globaltolocal_memory_pointer_networks_for_taskoriented_dialogue", "_bibtex": "@inproceedings{\nwu2018globaltolocal,\ntitle={Global-to-local Memory Pointer Networks for Task-Oriented Dialogue},\nauthor={Chien-Sheng Wu and Richard Socher and Caiming Xiong},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxnHhRqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1581/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618165, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryxnHhRqFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1581/Authors", "ICLR.cc/2019/Conference/Paper1581/Reviewers", "ICLR.cc/2019/Conference/Paper1581/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1581/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1581/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1581/Authors|ICLR.cc/2019/Conference/Paper1581/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1581/Reviewers", "ICLR.cc/2019/Conference/Paper1581/Authors", "ICLR.cc/2019/Conference/Paper1581/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618165}}}, {"id": "BJetM8J3aX", "original": null, "number": 1, "cdate": 1542350352692, "ddate": null, "tcdate": 1542350352692, "tmdate": 1542350352692, "tddate": null, "forum": "ryxnHhRqFm", "replyto": "ryxnHhRqFm", "invitation": "ICLR.cc/2019/Conference/-/Paper1581/Public_Comment", "content": {"comment": "This paper builds upon Mem2Seq (Madotto et al 2018) to incorporate large external KBs for task-oriented dialogs. To the best of my understanding, the innovations over Mem2Seq are: (1) use of context RNN (instead of just last utterance) as the query in MN encoder, (2) addition of the hidden state of context RNN to the dialog memory (equation 3), (3) the use of global memory pointer 4) additional loss components (Loss_g and Loss_l in equation 11) and (5) two step decoding using sketch tags\n\nTo me the biggest pro of the paper is its impressive result on SMD corpus. I appreciate the authors performing a human evaluation of the generated response for SMD.  However, I am quite concerned about two main issues. The first issue is in experimental rigor, and second is in dataset preparation, which is also related to experimental rigor but also exposes certain model weaknesses. I elaborate below.\n\nExperimental Rigor\nThis paper uses \"entity type\" information in Local Memory Decoder (Section 2.3), but compares against all previous work that does NOT use this information. This makes the comparison not sound. In fact, the original Memory Net paper (Bordes & Weston 2017) and many extensions performed two sets of experiments, one vanilla and one with a \"match\" feature, which had access to the entity type information. This paper compares against all previous papers in the settings that do NOT use this \"match\" feature. So, it is not clear, whether the improvement is coming from the specific changes in the model, or just by using additional information at training time. For example, QRN paper (Seo et al 2017) with Match feature reports an average OOV error (across 5 bAbI tasks) of 2.3%, which is fairly close the reported results in Table 2 in this paper. In my opinion, this careful experimental comparison is essential before having a clear assessment of this paper.\n\nDataset Preparation\nThis is a comment on this paper and also the Mem2Seq paper. Both these seem to have CHANGED the original training/test datasets to suit their model. In particular, all KB tuples in bAbI follow the format (restaurant_name, relation, value of relation), e.g., (olive_garden, rating, 5), however, Mem2Seq had reversed just the rating-relation tuples (5, rating, olive_garden), because its model allowed it to copy only from the object location, and it needed to copy restaurant name in the dialog. A similar example can be seen in this paper Figure 3 (row 5) where an ARTIFICIAL tuple (chinese_restaurant no_traffic 6_miles, poi, tai_pan) has been added to the SMD KB. Notice that values for different relations (namely poi_type, traffic_info, and distance) for the entity tai_pan have been concatenated in this artificial tuple at subject location and the entity name appears as object. This is just so that it can be copied by the model. Such tuples don't usually exist in normal KBs.\n\nI believe that this changing of datasets makes its comparisons with other models unfair. Even if other models were re-trained with this new modified dataset, it is still a severe limitation, because in practice, such tuples may not be found in the KB, and in such situations this model (and Mem2Seq) will not perform well. \n\n\nQuestions to authors\n1) how does your model compare with \"+match\" extension of previous models? \n2) Say we are in a more reasonable setting where we are not given entity type information, but say we are given whether it is a KB entity or not. How well will your model perform then?\n3) Suppose we remove the reverse relations in bAbI and the concatenated poi relations in SMD. How well will your model perform then?", "title": "Interesting Work, Need Clarity on Experiements"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Global-to-local Memory Pointer Networks for Task-Oriented Dialogue", "abstract": "End-to-end task-oriented dialogue is challenging since knowledge bases are usually large, dynamic and hard to incorporate into a learning framework. We propose the global-to-local memory pointer (GLMP) networks to address this issue. In our model, a global memory encoder and a local memory decoder are proposed to share external knowledge. The encoder encodes dialogue history, modifies global contextual representation, and generates a global memory pointer. The decoder first generates a sketch response with unfilled slots. Next, it passes the global memory pointer to filter the external knowledge for relevant information, then instantiates the slots via the local memory pointers. We empirically show that our model can improve copy accuracy and mitigate the common out-of-vocabulary problem. As a result, GLMP is able to improve over the previous state-of-the-art models in both simulated bAbI Dialogue dataset and human-human Stanford Multi-domain Dialogue dataset on automatic and human evaluation.", "keywords": ["pointer networks", "memory networks", "task-oriented dialogue systems", "natural language processing"], "authorids": ["jason.wu@connect.ust.hk", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chien-Sheng Wu", "Richard Socher", "Caiming Xiong"], "TL;DR": "GLMP: Global memory encoder (context RNN, global pointer) and local memory decoder (sketch RNN, local pointer) that share external knowledge (MemNN) are proposed to strengthen response generation in task-oriented dialogue.", "pdf": "/pdf/db8acf203bd7c5a73dd104a07690d8f6f8d85757.pdf", "paperhash": "wu|globaltolocal_memory_pointer_networks_for_taskoriented_dialogue", "_bibtex": "@inproceedings{\nwu2018globaltolocal,\ntitle={Global-to-local Memory Pointer Networks for Task-Oriented Dialogue},\nauthor={Chien-Sheng Wu and Richard Socher and Caiming Xiong},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxnHhRqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1581/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311563547, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "ryxnHhRqFm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1581/Authors", "ICLR.cc/2019/Conference/Paper1581/Reviewers", "ICLR.cc/2019/Conference/Paper1581/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1581/Authors", "ICLR.cc/2019/Conference/Paper1581/Reviewers", "ICLR.cc/2019/Conference/Paper1581/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311563547}}}, {"id": "BklMNKNLaQ", "original": null, "number": 3, "cdate": 1541978409604, "ddate": null, "tcdate": 1541978409604, "tmdate": 1541978409604, "tddate": null, "forum": "ryxnHhRqFm", "replyto": "rJljYi39nX", "invitation": "ICLR.cc/2019/Conference/-/Paper1581/Official_Comment", "content": {"title": "Re: Reviewer2", "comment": "Thank you for your review and feedback. The question which you mentioned, the replies are as followed : \n\n1. You describe the auxiliary loss on the global pointer, and mention an ablation study that show that this improves performance. Maybe I am overlooking something, but I cannot find this ablation in the paper or appendix. It would be nice to see how large the effect is.\nReply: \nThe ablation study of our global memory pointer G is in Table 4, the GLMP w/o G. For SMD dataset, without G gave us around 8.3% additional loss.\n\n2. Following on the above, why no similar auxiliary losses on additional components, e.g. the template generation? Were these tried and deemed unnecessary or vice-versa (i.e. the default was no auxiliary loss and they were only added when needed)? \nReply: \nOur model has three loss functions, Loss_g for global memory pointer, Loss_v for sketch response generation and Loss_l for local memory pointer. The template generation loss you mentioned is included as Loss_v, which is a standard cross-entropy loss. \n\n3. I really appreciate that you run a human eval. But why not have humans evaluate objective \"correctness\" as well?\nReply: \nIn our evaluation setting, we combine the correctness and the appropriateness, as the criteria we mentioned in the appendix A.3. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1581/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1581/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1581/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Global-to-local Memory Pointer Networks for Task-Oriented Dialogue", "abstract": "End-to-end task-oriented dialogue is challenging since knowledge bases are usually large, dynamic and hard to incorporate into a learning framework. We propose the global-to-local memory pointer (GLMP) networks to address this issue. In our model, a global memory encoder and a local memory decoder are proposed to share external knowledge. The encoder encodes dialogue history, modifies global contextual representation, and generates a global memory pointer. The decoder first generates a sketch response with unfilled slots. Next, it passes the global memory pointer to filter the external knowledge for relevant information, then instantiates the slots via the local memory pointers. We empirically show that our model can improve copy accuracy and mitigate the common out-of-vocabulary problem. As a result, GLMP is able to improve over the previous state-of-the-art models in both simulated bAbI Dialogue dataset and human-human Stanford Multi-domain Dialogue dataset on automatic and human evaluation.", "keywords": ["pointer networks", "memory networks", "task-oriented dialogue systems", "natural language processing"], "authorids": ["jason.wu@connect.ust.hk", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chien-Sheng Wu", "Richard Socher", "Caiming Xiong"], "TL;DR": "GLMP: Global memory encoder (context RNN, global pointer) and local memory decoder (sketch RNN, local pointer) that share external knowledge (MemNN) are proposed to strengthen response generation in task-oriented dialogue.", "pdf": "/pdf/db8acf203bd7c5a73dd104a07690d8f6f8d85757.pdf", "paperhash": "wu|globaltolocal_memory_pointer_networks_for_taskoriented_dialogue", "_bibtex": "@inproceedings{\nwu2018globaltolocal,\ntitle={Global-to-local Memory Pointer Networks for Task-Oriented Dialogue},\nauthor={Chien-Sheng Wu and Richard Socher and Caiming Xiong},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxnHhRqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1581/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618165, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryxnHhRqFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1581/Authors", "ICLR.cc/2019/Conference/Paper1581/Reviewers", "ICLR.cc/2019/Conference/Paper1581/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1581/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1581/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1581/Authors|ICLR.cc/2019/Conference/Paper1581/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1581/Reviewers", "ICLR.cc/2019/Conference/Paper1581/Authors", "ICLR.cc/2019/Conference/Paper1581/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618165}}}, {"id": "S1g_lF4U6X", "original": null, "number": 2, "cdate": 1541978352464, "ddate": null, "tcdate": 1541978352464, "tmdate": 1541978352464, "tddate": null, "forum": "ryxnHhRqFm", "replyto": "BJl5tkoN2X", "invitation": "ICLR.cc/2019/Conference/-/Paper1581/Official_Comment", "content": {"title": "Re: Reviewer3", "comment": "Thank you for your review and feedback. The question which you mentioned, the replies are as followed : \n\n1. In Section 2.1 I am not sure all the symbols are clearly defined.\nReply: \nWe will make the definitions more appropriate and consistent. \n\n2. I am also confused about the loss function. Which loss function is used when?\nReply: \nOur model has three loss functions: Loss_g for global memory pointer, Loss_v for sketch response generation and Loss_l for local memory pointer. During training, they are summed and optimized simultaneously.\n\n3. I am missing one more figure. From Fig 2 it's not so straightforward to see how the encoder/decoder along with the shared KB work at the same time (i.e. not independently)\nReply: \nAs shown in the block diagram Fig 1(a), first, the global memory encoder encodes dialogue history and writes its hidden states into the external knowledge. Then the last hidden state is used to read the external knowledge and generate the global memory pointer at the same time. Later during the decoding stage, the local memory decoder generates sketch responses. Then the global memory pointer and the sketch RNN hidden state are passed to the external knowledge, which returns the local memory pointer that can copy plain text to replace the sketch tags and obtain the final system response.\n\n4. In Section 2.3, it's not clear to me how the expected output word will be picked up from the local memory pointer. Same goes for the entity table.\nReply: \nSorry that we did not make it clear. As the visualization in Fig 3, the right column is the local memory pointers for time step 0 to 3. For example, in step 3, when our sketch RNN generated tags such as\u201c@address\u201d, the word will be picked out from the learned local memory pointer, which points to the memory node \u201c[783_arcadia_pl] address chevron\u201d. Therefore, we took the Object word \u201c783_arcadia_pl\u201d out as the real address. Otherwise, the output word is generated from the vocabulary\n\n5. How can you guarantee that that position n+l+1 is a null token?\nReply: \nWe manually assign token of \u201cn+l+1\u201d position to be \u201cNULL\u201d during preprocessing.\n\n6. What was the initial query vector and how did you initialise that? Did different initialisations had any effect on performance?\nReply: \nThe query vector is the vector to query the external knowledge. In the encoder, the query vector is the last hidden state of context RNN. In the decoder, the query vectors are the hidden states of the sketch RNN. \n\n7. If you can please provide an example of a memory position.\nReply: \nThe example of memory position is shown in the left part of Fig 3, as you can see, our external knowledge includes the kB and the dialogue history.\n\n8. Also, i would like to see a description of how the OOV tasks are handled.\nReply:\nSorry that we did not make it clear. In Sec 2.2, we explain that our model can mitigate the OOV problem is because we use the context RNN hidden states as the global contextual representation, and feed into the external knowledge. Therefore, the embedding of each token includes its RNN hidden state, including embeddings of OOV tokens. \n\n9. Finally, your method is a NN end-to-end one and I was wondering how do you compare not with other end-to-end approaches, but with a traditional approach, such as pydial?\nReply: \nWe mainly followed previous works to compare end-to-end models without human feature engineer efforts. In Table 3, results of the rule-based system from the Eric et al., 2017 are reported, we can observe the improvement over the traditional pipeline solution on the SMD human-human dialogue dataset.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1581/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1581/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1581/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Global-to-local Memory Pointer Networks for Task-Oriented Dialogue", "abstract": "End-to-end task-oriented dialogue is challenging since knowledge bases are usually large, dynamic and hard to incorporate into a learning framework. We propose the global-to-local memory pointer (GLMP) networks to address this issue. In our model, a global memory encoder and a local memory decoder are proposed to share external knowledge. The encoder encodes dialogue history, modifies global contextual representation, and generates a global memory pointer. The decoder first generates a sketch response with unfilled slots. Next, it passes the global memory pointer to filter the external knowledge for relevant information, then instantiates the slots via the local memory pointers. We empirically show that our model can improve copy accuracy and mitigate the common out-of-vocabulary problem. As a result, GLMP is able to improve over the previous state-of-the-art models in both simulated bAbI Dialogue dataset and human-human Stanford Multi-domain Dialogue dataset on automatic and human evaluation.", "keywords": ["pointer networks", "memory networks", "task-oriented dialogue systems", "natural language processing"], "authorids": ["jason.wu@connect.ust.hk", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chien-Sheng Wu", "Richard Socher", "Caiming Xiong"], "TL;DR": "GLMP: Global memory encoder (context RNN, global pointer) and local memory decoder (sketch RNN, local pointer) that share external knowledge (MemNN) are proposed to strengthen response generation in task-oriented dialogue.", "pdf": "/pdf/db8acf203bd7c5a73dd104a07690d8f6f8d85757.pdf", "paperhash": "wu|globaltolocal_memory_pointer_networks_for_taskoriented_dialogue", "_bibtex": "@inproceedings{\nwu2018globaltolocal,\ntitle={Global-to-local Memory Pointer Networks for Task-Oriented Dialogue},\nauthor={Chien-Sheng Wu and Richard Socher and Caiming Xiong},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxnHhRqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1581/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618165, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryxnHhRqFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1581/Authors", "ICLR.cc/2019/Conference/Paper1581/Reviewers", "ICLR.cc/2019/Conference/Paper1581/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1581/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1581/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1581/Authors|ICLR.cc/2019/Conference/Paper1581/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1581/Reviewers", "ICLR.cc/2019/Conference/Paper1581/Authors", "ICLR.cc/2019/Conference/Paper1581/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618165}}}, {"id": "BkeK5uELpm", "original": null, "number": 1, "cdate": 1541978256627, "ddate": null, "tcdate": 1541978256627, "tmdate": 1541978256627, "tddate": null, "forum": "ryxnHhRqFm", "replyto": "ryeDCdQ-jQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1581/Official_Comment", "content": {"title": "Re: Reviewer1", "comment": "Thank you for your review and feedback.  The question which you mentioned, the replies are as followed : \n\n1. In global memory pointer, the users employ non-normalized probability (non-softmax). What is the difference in performance if one uses softmax?\nReply: \nSorry that we did not make it clear. We treat the training of global memory pointer as a multi-label learning problem, instead of a multi-class classification problem. For example, if the system generates a response like \u201cStarbucks is 4_miles away\u201d, both \u201cStarbucks\u201d and \u201c4_miles\u201d are model\u2019s outputs. \n\n2. In (11), there's no linear weights. Will higher weights in global/local help?\nReply: \nIn our experiments, we tried to add the linear weights in (11) as hyper-parameters, the results are similar. \n\n3. As pointed out in ablation study, it's weird that in task5 global memory pointer does not help.\nReply: \nNote that our ablation study on bAbI dialogue is on its OOV setting, which could be considered as a zero-shot scenario. Therefore, we observe a performance tradeoff between the normal setting and the OOV setting, that is, if we increase the dropout ratio, the accuracy in normal setting may decrease and the accuracy in OOV setting may increase. Therefore, after we rerun the ablation study on task5 OOV with the same dropout rate, the performance difference between with and without global memory pointer is very little. We will include this explanation in the paper. \n\n4. The main competitor of this algorithm is mem2seq. While mem2seq includes DSTC2 and In-car Assistant, and especially in-car assistant provides the first example dialogue, why does the paper not include expeirments on these two datasets?\nReply: \nSorry that we did not make it clear. The \u201cSMD\u201d dataset in our experiment is exactly the same as the \u201cIn-Car Assistant\u201d dataset in the Mem2Seq paper (different naming), both came from the paper Eric et al, 2017. Therefore, the results are comparable. Second, We did not include the DSTC2 in our paper is because it is a \u201chuman-machine\u201d dataset which is originally designed as a DST task, not a response generation task. That dataset has many noisy system responses as well. We take one of the dialogues as an example, one can observe that the system responses are not properly collected.\n...\nUser: <SILENCE>\t\nSystem: the_lucky_star is a nice place in the south of town and the prices are cheap\nUser: what else is there\t\nSystem: the_lucky_star is a great restaurant\nUser: anything else\t\nSystem: The post code of the_lucky_star is the_lucky_star_post_code\n...\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1581/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1581/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1581/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Global-to-local Memory Pointer Networks for Task-Oriented Dialogue", "abstract": "End-to-end task-oriented dialogue is challenging since knowledge bases are usually large, dynamic and hard to incorporate into a learning framework. We propose the global-to-local memory pointer (GLMP) networks to address this issue. In our model, a global memory encoder and a local memory decoder are proposed to share external knowledge. The encoder encodes dialogue history, modifies global contextual representation, and generates a global memory pointer. The decoder first generates a sketch response with unfilled slots. Next, it passes the global memory pointer to filter the external knowledge for relevant information, then instantiates the slots via the local memory pointers. We empirically show that our model can improve copy accuracy and mitigate the common out-of-vocabulary problem. As a result, GLMP is able to improve over the previous state-of-the-art models in both simulated bAbI Dialogue dataset and human-human Stanford Multi-domain Dialogue dataset on automatic and human evaluation.", "keywords": ["pointer networks", "memory networks", "task-oriented dialogue systems", "natural language processing"], "authorids": ["jason.wu@connect.ust.hk", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chien-Sheng Wu", "Richard Socher", "Caiming Xiong"], "TL;DR": "GLMP: Global memory encoder (context RNN, global pointer) and local memory decoder (sketch RNN, local pointer) that share external knowledge (MemNN) are proposed to strengthen response generation in task-oriented dialogue.", "pdf": "/pdf/db8acf203bd7c5a73dd104a07690d8f6f8d85757.pdf", "paperhash": "wu|globaltolocal_memory_pointer_networks_for_taskoriented_dialogue", "_bibtex": "@inproceedings{\nwu2018globaltolocal,\ntitle={Global-to-local Memory Pointer Networks for Task-Oriented Dialogue},\nauthor={Chien-Sheng Wu and Richard Socher and Caiming Xiong},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxnHhRqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1581/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618165, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryxnHhRqFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1581/Authors", "ICLR.cc/2019/Conference/Paper1581/Reviewers", "ICLR.cc/2019/Conference/Paper1581/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1581/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1581/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1581/Authors|ICLR.cc/2019/Conference/Paper1581/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1581/Reviewers", "ICLR.cc/2019/Conference/Paper1581/Authors", "ICLR.cc/2019/Conference/Paper1581/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618165}}}, {"id": "rJljYi39nX", "original": null, "number": 3, "cdate": 1541225346803, "ddate": null, "tcdate": 1541225346803, "tmdate": 1541533016298, "tddate": null, "forum": "ryxnHhRqFm", "replyto": "ryxnHhRqFm", "invitation": "ICLR.cc/2019/Conference/-/Paper1581/Official_Review", "content": {"title": "nicely motivated architecture and thorough evaluation, aimed at an interesting and difficult task", "review": "The paper presents a new model for reading and writing memory in the context of task-oriented dialogue. The model contains three main components: an encoder, a decoder, and an external KB. The external KB is in the format of an SVO triple store. The encoder encodes the dialogue history and, in doing so, writes its hidden states to memory and generates a \"global memory pointer\" as its last hidden state. The decoder takes as input the global memory pointer, the encoded dialogue state history, and the external KB and then generates a response using a two-step process in which it 1) generates a template response using tags to designate slots that need filling and 2) looks up the correct filler for each slot using the template+global memory pointer as a query. The authors evaluate the model on a simulated dialogue dataset (bAbI) and on a human-human dataset (Stanford Multi-domain Dialogue or SMD) as well as in a human eval. They show substantial improvements over existing models on SMD (the more interesting of the datasets) in terms of entity F1--i.e. the number of correctly-generated entities in the response. They also show improvement on bAbI specifically on cases involving OOVs. On the human evaluation, they show improvements in terms of both \"appropriateness\" and \"human-likeness\". \n\nOverall, I think this is a nice and well-motivated model. I very much appreciate the thoroughness of the evaluation (two different datasets, plus a human evaluation). The level of analysis of the model was also good, although there (inevitably) could have been more. Since it is such a complex model, I would have liked to see more thorough ablations or at least better descriptions of the baselines, in order to better understand which specific pieces of the model yield which types of gains. A few particular questions below:\n\n- You describe the auxiliary loss on the global pointer, and mention an ablation study that show that this improves performance. Maybe I am overlooking something, but I cannot find this ablation in the paper or appendix. It would be nice to see how large the effect is. \n- Following on the above, why no similar auxiliary losses on additional components, e.g. the template generation? Were these tried and deemed unnecessary or vice-versa (i.e. the default was no auxiliary loss and they were only added when needed)? Either way, it would be nice to better communicate the experiments/intuitions that motivated the particular architecture you arrived at.\n- I really appreciate that you run a human eval. But why not have humans evaluate objective \"correctness\" as well? It seems trivial to ask people to say whether or not the answer is correct/communicates the same information as the gold.\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper1581/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Global-to-local Memory Pointer Networks for Task-Oriented Dialogue", "abstract": "End-to-end task-oriented dialogue is challenging since knowledge bases are usually large, dynamic and hard to incorporate into a learning framework. We propose the global-to-local memory pointer (GLMP) networks to address this issue. In our model, a global memory encoder and a local memory decoder are proposed to share external knowledge. The encoder encodes dialogue history, modifies global contextual representation, and generates a global memory pointer. The decoder first generates a sketch response with unfilled slots. Next, it passes the global memory pointer to filter the external knowledge for relevant information, then instantiates the slots via the local memory pointers. We empirically show that our model can improve copy accuracy and mitigate the common out-of-vocabulary problem. As a result, GLMP is able to improve over the previous state-of-the-art models in both simulated bAbI Dialogue dataset and human-human Stanford Multi-domain Dialogue dataset on automatic and human evaluation.", "keywords": ["pointer networks", "memory networks", "task-oriented dialogue systems", "natural language processing"], "authorids": ["jason.wu@connect.ust.hk", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chien-Sheng Wu", "Richard Socher", "Caiming Xiong"], "TL;DR": "GLMP: Global memory encoder (context RNN, global pointer) and local memory decoder (sketch RNN, local pointer) that share external knowledge (MemNN) are proposed to strengthen response generation in task-oriented dialogue.", "pdf": "/pdf/db8acf203bd7c5a73dd104a07690d8f6f8d85757.pdf", "paperhash": "wu|globaltolocal_memory_pointer_networks_for_taskoriented_dialogue", "_bibtex": "@inproceedings{\nwu2018globaltolocal,\ntitle={Global-to-local Memory Pointer Networks for Task-Oriented Dialogue},\nauthor={Chien-Sheng Wu and Richard Socher and Caiming Xiong},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxnHhRqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1581/Official_Review", "cdate": 1542234199095, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryxnHhRqFm", "replyto": "ryxnHhRqFm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1581/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335977249, "tmdate": 1552335977249, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1581/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJl5tkoN2X", "original": null, "number": 2, "cdate": 1540824962279, "ddate": null, "tcdate": 1540824962279, "tmdate": 1541533016090, "tddate": null, "forum": "ryxnHhRqFm", "replyto": "ryxnHhRqFm", "invitation": "ICLR.cc/2019/Conference/-/Paper1581/Official_Review", "content": {"title": "End-to-end task oriented system: An encoder-decoder approach with a shared external knowledge base", "review": "This is, in general, a well-written paper with extensive experimentation. \n\nThe authors tried to describe their architecture both with equations as well as graphically. However, I would like to mention the following: \n\nIn Section 2.1 I am not sure all the symbols are clearly defined. For example, I could not locate the definitions of n, l etc. Even if they are easy to assume, I am fond of appropriate definitions. Also, I suspect that some symbols, like n, are not used consistently across the manuscript.\n\nI am also confused about the loss function. Which loss function is used when?\n\nI am missing one more figure. From Fig 2 it's not so straightforward to see how the encoder/decoder along with the shared KB work at the same time (i.e. not independently)\n\nIn Section 2.3, it's not clear to me how the expected output word will be picked up from the local memory pointer. Same goes for the entity table.\n\nHow can you guarantee that that position n+l+1 is a null token?\n\nWhat was the initial query vector and how did you initialise that? Did different initialisations had any effect on performance?\n\nIf you can please provide an example of a memory position.\n\nAlso, i would like to see a description of how the OOV tasks are handled.\n\nFinally, your method is a NN end-to-end one and I was wondering how do you compare not with other end-to-end approaches, but with a traditional approach, such as pydial?\n\n\nAnd some minor suggestions:\n\nNot all the abbreviations are defined. For example QRN, GMN, KVR. It would also be nice to have the references of the respective methods included in the Tables or their captions.\n\nParts of Figs. 1&2 are pixelised. It would be nice to have everything vectorised.\n\n I would prefer to see the training details (in fact, I would even be favorable of having more of those) in the main body of the manuscript, rather than in the appendix.\n\nThere are some minor typos, such as \"our approach that utilizing the recurrent\" or \"in each datasets\"", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper1581/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Global-to-local Memory Pointer Networks for Task-Oriented Dialogue", "abstract": "End-to-end task-oriented dialogue is challenging since knowledge bases are usually large, dynamic and hard to incorporate into a learning framework. We propose the global-to-local memory pointer (GLMP) networks to address this issue. In our model, a global memory encoder and a local memory decoder are proposed to share external knowledge. The encoder encodes dialogue history, modifies global contextual representation, and generates a global memory pointer. The decoder first generates a sketch response with unfilled slots. Next, it passes the global memory pointer to filter the external knowledge for relevant information, then instantiates the slots via the local memory pointers. We empirically show that our model can improve copy accuracy and mitigate the common out-of-vocabulary problem. As a result, GLMP is able to improve over the previous state-of-the-art models in both simulated bAbI Dialogue dataset and human-human Stanford Multi-domain Dialogue dataset on automatic and human evaluation.", "keywords": ["pointer networks", "memory networks", "task-oriented dialogue systems", "natural language processing"], "authorids": ["jason.wu@connect.ust.hk", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chien-Sheng Wu", "Richard Socher", "Caiming Xiong"], "TL;DR": "GLMP: Global memory encoder (context RNN, global pointer) and local memory decoder (sketch RNN, local pointer) that share external knowledge (MemNN) are proposed to strengthen response generation in task-oriented dialogue.", "pdf": "/pdf/db8acf203bd7c5a73dd104a07690d8f6f8d85757.pdf", "paperhash": "wu|globaltolocal_memory_pointer_networks_for_taskoriented_dialogue", "_bibtex": "@inproceedings{\nwu2018globaltolocal,\ntitle={Global-to-local Memory Pointer Networks for Task-Oriented Dialogue},\nauthor={Chien-Sheng Wu and Richard Socher and Caiming Xiong},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxnHhRqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1581/Official_Review", "cdate": 1542234199095, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryxnHhRqFm", "replyto": "ryxnHhRqFm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1581/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335977249, "tmdate": 1552335977249, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1581/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "ryeDCdQ-jQ", "original": null, "number": 1, "cdate": 1539549391023, "ddate": null, "tcdate": 1539549391023, "tmdate": 1541533015883, "tddate": null, "forum": "ryxnHhRqFm", "replyto": "ryxnHhRqFm", "invitation": "ICLR.cc/2019/Conference/-/Paper1581/Official_Review", "content": {"title": "Expect more experiments", "review": "This paper puts forward a new global+local memory pointer network to tackle task-oriented dialogue problem.\n\nThe idea of introducing global memory is novel and experimental results show its effectiveness to encode external knowledge in most cases.\n\nHere're some comments:\n1. In global memory pointer, the users employ non-normalized probability (non-softmax). What is the difference in performance if one uses softmax?\n\n2. In (11), there's no linear weights. Will higher weights in global/local help?\n\n3. As pointed out in ablation study, it's weird that in task5 global memory pointer does not help.\n\n4. The main competitor of this algorithm is mem2seq. While mem2seq includes DSTC2 and In-car Assistant, and especially in-car assistant provides the first example dialogue, why does the paper not include expeirments on these two datasets?", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1581/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Global-to-local Memory Pointer Networks for Task-Oriented Dialogue", "abstract": "End-to-end task-oriented dialogue is challenging since knowledge bases are usually large, dynamic and hard to incorporate into a learning framework. We propose the global-to-local memory pointer (GLMP) networks to address this issue. In our model, a global memory encoder and a local memory decoder are proposed to share external knowledge. The encoder encodes dialogue history, modifies global contextual representation, and generates a global memory pointer. The decoder first generates a sketch response with unfilled slots. Next, it passes the global memory pointer to filter the external knowledge for relevant information, then instantiates the slots via the local memory pointers. We empirically show that our model can improve copy accuracy and mitigate the common out-of-vocabulary problem. As a result, GLMP is able to improve over the previous state-of-the-art models in both simulated bAbI Dialogue dataset and human-human Stanford Multi-domain Dialogue dataset on automatic and human evaluation.", "keywords": ["pointer networks", "memory networks", "task-oriented dialogue systems", "natural language processing"], "authorids": ["jason.wu@connect.ust.hk", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chien-Sheng Wu", "Richard Socher", "Caiming Xiong"], "TL;DR": "GLMP: Global memory encoder (context RNN, global pointer) and local memory decoder (sketch RNN, local pointer) that share external knowledge (MemNN) are proposed to strengthen response generation in task-oriented dialogue.", "pdf": "/pdf/db8acf203bd7c5a73dd104a07690d8f6f8d85757.pdf", "paperhash": "wu|globaltolocal_memory_pointer_networks_for_taskoriented_dialogue", "_bibtex": "@inproceedings{\nwu2018globaltolocal,\ntitle={Global-to-local Memory Pointer Networks for Task-Oriented Dialogue},\nauthor={Chien-Sheng Wu and Richard Socher and Caiming Xiong},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxnHhRqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1581/Official_Review", "cdate": 1542234199095, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryxnHhRqFm", "replyto": "ryxnHhRqFm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1581/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335977249, "tmdate": 1552335977249, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1581/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 15}