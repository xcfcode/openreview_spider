{"notes": [{"id": "YgrdmztE4OY", "original": "BTR56XBy8kP", "number": 1092, "cdate": 1601308122828, "ddate": null, "tcdate": 1601308122828, "tmdate": 1614985740498, "tddate": null, "forum": "YgrdmztE4OY", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Federated Mixture of Experts", "authorids": ["~Matthias_Reisser1", "~Christos_Louizos1", "~Efstratios_Gavves1", "~Max_Welling1"], "authors": ["Matthias Reisser", "Christos Louizos", "Efstratios Gavves", "Max Welling"], "keywords": ["Federated Learning", "personalized models", "non-i.i.d data"], "abstract": "Federated learning (FL) has emerged as the predominant approach for collaborative training of neural network models across multiple users, without the need to gather the data at a central location. One of the important challenges in this setting is data heterogeneity; different users have different data characteristics. For this reason, training and using a single global model might be suboptimal when considering the performance of each of the individual user\u2019s data. In this work, we tackle this problem via Federated Mixture of Experts, FedMix, a framework that allows us to train an ensemble of specialized models. FedMix adaptively selects and trains a user-specific selection of the ensemble members. We show that users with similar data characteristics select the same members and therefore share statistical strength while mitigating the effect of non-i.i.d data. Empirically, we show through an extensive experimental evaluation that FedMix improves performance compared to using a single global model while requiring similar or less communication costs.", "one-sentence_summary": "We propose Federated Mixture of Experts to tackle the non-i.i.d data problem in federated learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "reisser|federated_mixture_of_experts", "pdf": "/pdf/ea76c45ea010f0bbf4b97fb5a54dd42d495f3bad.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OoLI4R7-lL", "_bibtex": "@misc{\nreisser2021federated,\ntitle={Federated Mixture of Experts},\nauthor={Matthias Reisser and Christos Louizos and Efstratios Gavves and Max Welling},\nyear={2021},\nurl={https://openreview.net/forum?id=YgrdmztE4OY}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 17, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "8Fe8NJZwcf", "original": null, "number": 1, "cdate": 1610040397731, "ddate": null, "tcdate": 1610040397731, "tmdate": 1610473993176, "tddate": null, "forum": "YgrdmztE4OY", "replyto": "YgrdmztE4OY", "invitation": "ICLR.cc/2021/Conference/Paper1092/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper presents an approach, FedMix, for federated learning using mixture of experts (MoE). The basic idea is to learn an ensemble of models and user-specific combination weights (mixing proportions).\n\nThe reviewers appreciated the MoE formulation for federated learning. However, there were multiple concerns from the reviewers, which include lack of clarity (regarding the variational lower bound that is being used), significant communication cost and privacy concerns (the server can infer the users' label distributions), weak experimental results, and lack of any theoretical support (which isn't that big an issue if the paper were stronger on other aspects). The author feedback was considered and the reviewers engaged in some discussions (also with the authors). In the end, however, the reviewer were still not convinced that the paper is ready to be published in its current state. Based on my own reading of the paper, the reviews, and the author response, I agree with this assessment.\n\nThe authors are advised to take into account the feedback from the reviewers and resubmit to another venue."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Federated Mixture of Experts", "authorids": ["~Matthias_Reisser1", "~Christos_Louizos1", "~Efstratios_Gavves1", "~Max_Welling1"], "authors": ["Matthias Reisser", "Christos Louizos", "Efstratios Gavves", "Max Welling"], "keywords": ["Federated Learning", "personalized models", "non-i.i.d data"], "abstract": "Federated learning (FL) has emerged as the predominant approach for collaborative training of neural network models across multiple users, without the need to gather the data at a central location. One of the important challenges in this setting is data heterogeneity; different users have different data characteristics. For this reason, training and using a single global model might be suboptimal when considering the performance of each of the individual user\u2019s data. In this work, we tackle this problem via Federated Mixture of Experts, FedMix, a framework that allows us to train an ensemble of specialized models. FedMix adaptively selects and trains a user-specific selection of the ensemble members. We show that users with similar data characteristics select the same members and therefore share statistical strength while mitigating the effect of non-i.i.d data. Empirically, we show through an extensive experimental evaluation that FedMix improves performance compared to using a single global model while requiring similar or less communication costs.", "one-sentence_summary": "We propose Federated Mixture of Experts to tackle the non-i.i.d data problem in federated learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "reisser|federated_mixture_of_experts", "pdf": "/pdf/ea76c45ea010f0bbf4b97fb5a54dd42d495f3bad.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OoLI4R7-lL", "_bibtex": "@misc{\nreisser2021federated,\ntitle={Federated Mixture of Experts},\nauthor={Matthias Reisser and Christos Louizos and Efstratios Gavves and Max Welling},\nyear={2021},\nurl={https://openreview.net/forum?id=YgrdmztE4OY}\n}"}, "tags": [], "invitation": {"reply": {"forum": "YgrdmztE4OY", "replyto": "YgrdmztE4OY", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040397716, "tmdate": 1610473993159, "id": "ICLR.cc/2021/Conference/Paper1092/-/Decision"}}}, {"id": "3RIOV9TK9p", "original": null, "number": 14, "cdate": 1606238932753, "ddate": null, "tcdate": 1606238932753, "tmdate": 1606238932753, "tddate": null, "forum": "YgrdmztE4OY", "replyto": "uoKEBIdg7C1", "invitation": "ICLR.cc/2021/Conference/Paper1092/-/Official_Comment", "content": {"title": "No, it is not", "comment": "Thank you for pointing this out - indeed, we found a few cases where with the additional constraint that all per-data-points need to add up to the total number of data-points as well as that each individual p(y=c|s) needs to be positive, some systems of equations are solvable, whereas others are not. \nIn the example above with $q(z=1|y=1) = 0.8, q(z=1|y=2) =0.6, q(z=1|y=3)=0.4$, there are multiple solutions $(x, y, z) \\in [(3, 7, 0), (4, 5, 1), (5, 3, 2), (6, 1, 3)]$. \nIt goes beyond this work (and this rebuttal) to fully characterise under which conditions the marginal can be reconstructed and which not. We updated the corresponding section in our paper. \n\nWe thank you for sticking with us - our paper became much more precise about its privacy implications. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1092/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1092/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Federated Mixture of Experts", "authorids": ["~Matthias_Reisser1", "~Christos_Louizos1", "~Efstratios_Gavves1", "~Max_Welling1"], "authors": ["Matthias Reisser", "Christos Louizos", "Efstratios Gavves", "Max Welling"], "keywords": ["Federated Learning", "personalized models", "non-i.i.d data"], "abstract": "Federated learning (FL) has emerged as the predominant approach for collaborative training of neural network models across multiple users, without the need to gather the data at a central location. One of the important challenges in this setting is data heterogeneity; different users have different data characteristics. For this reason, training and using a single global model might be suboptimal when considering the performance of each of the individual user\u2019s data. In this work, we tackle this problem via Federated Mixture of Experts, FedMix, a framework that allows us to train an ensemble of specialized models. FedMix adaptively selects and trains a user-specific selection of the ensemble members. We show that users with similar data characteristics select the same members and therefore share statistical strength while mitigating the effect of non-i.i.d data. Empirically, we show through an extensive experimental evaluation that FedMix improves performance compared to using a single global model while requiring similar or less communication costs.", "one-sentence_summary": "We propose Federated Mixture of Experts to tackle the non-i.i.d data problem in federated learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "reisser|federated_mixture_of_experts", "pdf": "/pdf/ea76c45ea010f0bbf4b97fb5a54dd42d495f3bad.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OoLI4R7-lL", "_bibtex": "@misc{\nreisser2021federated,\ntitle={Federated Mixture of Experts},\nauthor={Matthias Reisser and Christos Louizos and Efstratios Gavves and Max Welling},\nyear={2021},\nurl={https://openreview.net/forum?id=YgrdmztE4OY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "YgrdmztE4OY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1092/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1092/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1092/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1092/Authors|ICLR.cc/2021/Conference/Paper1092/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1092/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863783, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1092/-/Official_Comment"}}}, {"id": "uoKEBIdg7C1", "original": null, "number": 13, "cdate": 1606233521146, "ddate": null, "tcdate": 1606233521146, "tmdate": 1606233521146, "tddate": null, "forum": "YgrdmztE4OY", "replyto": "zBcOKL1iE0V", "invitation": "ICLR.cc/2021/Conference/Paper1092/-/Official_Comment", "content": {"title": "Is this general?", "comment": "Is this not a special case due to $q(z=1|y=2) = q(z=1|y=3)$? If you changed is so those values were all unique (which the would be in general I think) would that not provide unique (integer) solutions. For example a slight change to $q(z=1|y=1) = 0.7, q(z=1|y=2)=0.2$ would make the integer solutions unique again. \n\nAnd even in this example we have already exposed $p(y=1|S)$ correctly and uniquely to the server. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1092/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1092/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Federated Mixture of Experts", "authorids": ["~Matthias_Reisser1", "~Christos_Louizos1", "~Efstratios_Gavves1", "~Max_Welling1"], "authors": ["Matthias Reisser", "Christos Louizos", "Efstratios Gavves", "Max Welling"], "keywords": ["Federated Learning", "personalized models", "non-i.i.d data"], "abstract": "Federated learning (FL) has emerged as the predominant approach for collaborative training of neural network models across multiple users, without the need to gather the data at a central location. One of the important challenges in this setting is data heterogeneity; different users have different data characteristics. For this reason, training and using a single global model might be suboptimal when considering the performance of each of the individual user\u2019s data. In this work, we tackle this problem via Federated Mixture of Experts, FedMix, a framework that allows us to train an ensemble of specialized models. FedMix adaptively selects and trains a user-specific selection of the ensemble members. We show that users with similar data characteristics select the same members and therefore share statistical strength while mitigating the effect of non-i.i.d data. Empirically, we show through an extensive experimental evaluation that FedMix improves performance compared to using a single global model while requiring similar or less communication costs.", "one-sentence_summary": "We propose Federated Mixture of Experts to tackle the non-i.i.d data problem in federated learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "reisser|federated_mixture_of_experts", "pdf": "/pdf/ea76c45ea010f0bbf4b97fb5a54dd42d495f3bad.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OoLI4R7-lL", "_bibtex": "@misc{\nreisser2021federated,\ntitle={Federated Mixture of Experts},\nauthor={Matthias Reisser and Christos Louizos and Efstratios Gavves and Max Welling},\nyear={2021},\nurl={https://openreview.net/forum?id=YgrdmztE4OY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "YgrdmztE4OY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1092/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1092/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1092/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1092/Authors|ICLR.cc/2021/Conference/Paper1092/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1092/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863783, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1092/-/Official_Comment"}}}, {"id": "zBcOKL1iE0V", "original": null, "number": 12, "cdate": 1606228054689, "ddate": null, "tcdate": 1606228054689, "tmdate": 1606228054689, "tddate": null, "forum": "YgrdmztE4OY", "replyto": "KqpTswGgMfX", "invitation": "ICLR.cc/2021/Conference/Paper1092/-/Official_Comment", "content": {"title": "Brute Force still underdetermined", "comment": "A brute-force enumeration of all $p(y|s)$ is certainly possible under the assumption of discrete number of data-points per class. However since the system is still underdetermined, it will generally not be possible to identify which of those $p(y|s)$ is the *true* $p(y|s)$.\n\nConsider this concrete example (with arbitrary values).\nFor a problem with $3$ classes, assume a client has the following number of data-points per class:\n\n$N_{y1} = 5$, $N_{y2} = 3$, $N_{y3}=2$, with a total of $10$ data-points.\nWe assume $K=2$ experts, i.e $z \\in${1,2} and a $q(z=1|y)$ as follows:\n\n$q(z = 1 | y=1) = 8/10$\n\n$q(z=1 | y=2) = 1/10$\n\n$q(z=1 | y=3) = 1/10$\n\n$q(z=2|y=c)$ is then equal to $1-q(z=1|y=c)$\n\nFor this client, $q(z=1|s) = 45/100$ and $q(z=2|s) = 55/100$\n\nIf you attempt solve the system of equations that says \n\n$q(z=1|s) = 0.8x + 0.1y + 0.1z$\n\n$q(z=2|s) = 0.2x + 0.9y + 0.9z$\n\nwhere $x = p(y=1|s), y = p(y=2|s),z = p(y=3|s)$, \n\nyou end up at $x=5$, and $y + z = 5$, so any combination of $y + z$ (even if they are positive integers only) satisfies the equality.  \n\nThis example serves to illustrate the more general case where there are more number of classes than number of experts. \n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1092/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1092/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Federated Mixture of Experts", "authorids": ["~Matthias_Reisser1", "~Christos_Louizos1", "~Efstratios_Gavves1", "~Max_Welling1"], "authors": ["Matthias Reisser", "Christos Louizos", "Efstratios Gavves", "Max Welling"], "keywords": ["Federated Learning", "personalized models", "non-i.i.d data"], "abstract": "Federated learning (FL) has emerged as the predominant approach for collaborative training of neural network models across multiple users, without the need to gather the data at a central location. One of the important challenges in this setting is data heterogeneity; different users have different data characteristics. For this reason, training and using a single global model might be suboptimal when considering the performance of each of the individual user\u2019s data. In this work, we tackle this problem via Federated Mixture of Experts, FedMix, a framework that allows us to train an ensemble of specialized models. FedMix adaptively selects and trains a user-specific selection of the ensemble members. We show that users with similar data characteristics select the same members and therefore share statistical strength while mitigating the effect of non-i.i.d data. Empirically, we show through an extensive experimental evaluation that FedMix improves performance compared to using a single global model while requiring similar or less communication costs.", "one-sentence_summary": "We propose Federated Mixture of Experts to tackle the non-i.i.d data problem in federated learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "reisser|federated_mixture_of_experts", "pdf": "/pdf/ea76c45ea010f0bbf4b97fb5a54dd42d495f3bad.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OoLI4R7-lL", "_bibtex": "@misc{\nreisser2021federated,\ntitle={Federated Mixture of Experts},\nauthor={Matthias Reisser and Christos Louizos and Efstratios Gavves and Max Welling},\nyear={2021},\nurl={https://openreview.net/forum?id=YgrdmztE4OY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "YgrdmztE4OY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1092/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1092/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1092/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1092/Authors|ICLR.cc/2021/Conference/Paper1092/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1092/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863783, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1092/-/Official_Comment"}}}, {"id": "KqpTswGgMfX", "original": null, "number": 11, "cdate": 1606224275892, "ddate": null, "tcdate": 1606224275892, "tmdate": 1606224275892, "tddate": null, "forum": "YgrdmztE4OY", "replyto": "aZTN58NtjLY", "invitation": "ICLR.cc/2021/Conference/Paper1092/-/Official_Comment", "content": {"title": "Brute force", "comment": "Unless both the number of points per client and the number of classes were large, would you not be able to brute force this? The number of possible solutions is $n^{k-1}$ and each check is a small amount of linear algebra. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1092/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1092/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Federated Mixture of Experts", "authorids": ["~Matthias_Reisser1", "~Christos_Louizos1", "~Efstratios_Gavves1", "~Max_Welling1"], "authors": ["Matthias Reisser", "Christos Louizos", "Efstratios Gavves", "Max Welling"], "keywords": ["Federated Learning", "personalized models", "non-i.i.d data"], "abstract": "Federated learning (FL) has emerged as the predominant approach for collaborative training of neural network models across multiple users, without the need to gather the data at a central location. One of the important challenges in this setting is data heterogeneity; different users have different data characteristics. For this reason, training and using a single global model might be suboptimal when considering the performance of each of the individual user\u2019s data. In this work, we tackle this problem via Federated Mixture of Experts, FedMix, a framework that allows us to train an ensemble of specialized models. FedMix adaptively selects and trains a user-specific selection of the ensemble members. We show that users with similar data characteristics select the same members and therefore share statistical strength while mitigating the effect of non-i.i.d data. Empirically, we show through an extensive experimental evaluation that FedMix improves performance compared to using a single global model while requiring similar or less communication costs.", "one-sentence_summary": "We propose Federated Mixture of Experts to tackle the non-i.i.d data problem in federated learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "reisser|federated_mixture_of_experts", "pdf": "/pdf/ea76c45ea010f0bbf4b97fb5a54dd42d495f3bad.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OoLI4R7-lL", "_bibtex": "@misc{\nreisser2021federated,\ntitle={Federated Mixture of Experts},\nauthor={Matthias Reisser and Christos Louizos and Efstratios Gavves and Max Welling},\nyear={2021},\nurl={https://openreview.net/forum?id=YgrdmztE4OY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "YgrdmztE4OY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1092/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1092/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1092/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1092/Authors|ICLR.cc/2021/Conference/Paper1092/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1092/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863783, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1092/-/Official_Comment"}}}, {"id": "6B00ygWxW6I", "original": null, "number": 10, "cdate": 1606213009226, "ddate": null, "tcdate": 1606213009226, "tmdate": 1606213009226, "tddate": null, "forum": "YgrdmztE4OY", "replyto": "YgrdmztE4OY", "invitation": "ICLR.cc/2021/Conference/Paper1092/-/Official_Comment", "content": {"title": "Towards the end of the discussion period", "comment": "Given that there are approximately 24h left until the end of the discussion period, we hope that the reviewers had the chance to read our rebuttal and revise their opinions. We are thankful for your considerations and hope that we could address your concerns. If so, we would appreciate a comment and reevaluation of your score to better reflect the current state of our submission."}, "signatures": ["ICLR.cc/2021/Conference/Paper1092/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1092/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Federated Mixture of Experts", "authorids": ["~Matthias_Reisser1", "~Christos_Louizos1", "~Efstratios_Gavves1", "~Max_Welling1"], "authors": ["Matthias Reisser", "Christos Louizos", "Efstratios Gavves", "Max Welling"], "keywords": ["Federated Learning", "personalized models", "non-i.i.d data"], "abstract": "Federated learning (FL) has emerged as the predominant approach for collaborative training of neural network models across multiple users, without the need to gather the data at a central location. One of the important challenges in this setting is data heterogeneity; different users have different data characteristics. For this reason, training and using a single global model might be suboptimal when considering the performance of each of the individual user\u2019s data. In this work, we tackle this problem via Federated Mixture of Experts, FedMix, a framework that allows us to train an ensemble of specialized models. FedMix adaptively selects and trains a user-specific selection of the ensemble members. We show that users with similar data characteristics select the same members and therefore share statistical strength while mitigating the effect of non-i.i.d data. Empirically, we show through an extensive experimental evaluation that FedMix improves performance compared to using a single global model while requiring similar or less communication costs.", "one-sentence_summary": "We propose Federated Mixture of Experts to tackle the non-i.i.d data problem in federated learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "reisser|federated_mixture_of_experts", "pdf": "/pdf/ea76c45ea010f0bbf4b97fb5a54dd42d495f3bad.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OoLI4R7-lL", "_bibtex": "@misc{\nreisser2021federated,\ntitle={Federated Mixture of Experts},\nauthor={Matthias Reisser and Christos Louizos and Efstratios Gavves and Max Welling},\nyear={2021},\nurl={https://openreview.net/forum?id=YgrdmztE4OY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "YgrdmztE4OY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1092/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1092/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1092/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1092/Authors|ICLR.cc/2021/Conference/Paper1092/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1092/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863783, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1092/-/Official_Comment"}}}, {"id": "aZTN58NtjLY", "original": null, "number": 9, "cdate": 1606212325758, "ddate": null, "tcdate": 1606212325758, "tmdate": 1606212325758, "tddate": null, "forum": "YgrdmztE4OY", "replyto": "uYLMpp4dlTM", "invitation": "ICLR.cc/2021/Conference/Paper1092/-/Official_Comment", "content": {"title": "Responds to your comment", "comment": "Thank you for engaging with us once more! We agree that the integer constraints are additional prior knowledge that we hadn\u2019t considered yet \u2013 however we fail to see how that solves the fundamental issue with invertibility. Maybe the reviewer has some additional pointers. \nFor the privacy implications of FedAvg, we would assume a similar insight into the bias vector even if the client does not compute for a full epoch \u2013 that is assuming the client performs random sub-sampling of its dataset. On a more fundamental comparison with FedMix, we note that the same type of analysis is applicable to any type of FL algorithm that updates its bias vector with (S)GD locally and communicates it to the server. Our exposition was intended to point out that FedMix does not offer up additional information compared to what can be inferred with FedAvg in the first place. \n\nWe would like to point out to the reviewer that we have a personalization-specific method, Local/Global [Liang et. al. 2020] in the paper. Additionally, we have added in our rebuttal the \u2018personalization by fine-tuning\u2019 results for all of our methods. We hope that those give you enough insight into the personalization angle of FedMix. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1092/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1092/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Federated Mixture of Experts", "authorids": ["~Matthias_Reisser1", "~Christos_Louizos1", "~Efstratios_Gavves1", "~Max_Welling1"], "authors": ["Matthias Reisser", "Christos Louizos", "Efstratios Gavves", "Max Welling"], "keywords": ["Federated Learning", "personalized models", "non-i.i.d data"], "abstract": "Federated learning (FL) has emerged as the predominant approach for collaborative training of neural network models across multiple users, without the need to gather the data at a central location. One of the important challenges in this setting is data heterogeneity; different users have different data characteristics. For this reason, training and using a single global model might be suboptimal when considering the performance of each of the individual user\u2019s data. In this work, we tackle this problem via Federated Mixture of Experts, FedMix, a framework that allows us to train an ensemble of specialized models. FedMix adaptively selects and trains a user-specific selection of the ensemble members. We show that users with similar data characteristics select the same members and therefore share statistical strength while mitigating the effect of non-i.i.d data. Empirically, we show through an extensive experimental evaluation that FedMix improves performance compared to using a single global model while requiring similar or less communication costs.", "one-sentence_summary": "We propose Federated Mixture of Experts to tackle the non-i.i.d data problem in federated learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "reisser|federated_mixture_of_experts", "pdf": "/pdf/ea76c45ea010f0bbf4b97fb5a54dd42d495f3bad.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OoLI4R7-lL", "_bibtex": "@misc{\nreisser2021federated,\ntitle={Federated Mixture of Experts},\nauthor={Matthias Reisser and Christos Louizos and Efstratios Gavves and Max Welling},\nyear={2021},\nurl={https://openreview.net/forum?id=YgrdmztE4OY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "YgrdmztE4OY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1092/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1092/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1092/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1092/Authors|ICLR.cc/2021/Conference/Paper1092/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1092/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863783, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1092/-/Official_Comment"}}}, {"id": "uYLMpp4dlTM", "original": null, "number": 8, "cdate": 1606161235402, "ddate": null, "tcdate": 1606161235402, "tmdate": 1606161235402, "tddate": null, "forum": "YgrdmztE4OY", "replyto": "owawxL6nFVu", "invitation": "ICLR.cc/2021/Conference/Paper1092/-/Official_Comment", "content": {"title": "Very interesting privacy exposition", "comment": "Thank you for this additional privacy discussion for FedAvg, as well as for addressing the issue with FedMix. Is it not possible that even when the probability matrix is not invertable that you could still solve for the probabilities by using knowledge of the total number of points on the client, and the integer constraints on all the label distributions (i.e. us the fact that when each label proportion is multiplied by the number of points on the client it must still be an integer)? \n\nAs for the issue with FedAvg, this is an interesting point that FedAvg is also vulnerable to a similar issue (at least during the first update). However it seems like this would be easier to solve (for example by not running full local epochs), whereas for FedMix these proportions are integral to the method. \n\nFor comparison with personalization, there are many methods of personalization, and you need not test all of them but just having one comparison would be sufficient. The concern here is you can get most of the gains using another personalization method which does not have these same communication costs and privacy concerns. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1092/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1092/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Federated Mixture of Experts", "authorids": ["~Matthias_Reisser1", "~Christos_Louizos1", "~Efstratios_Gavves1", "~Max_Welling1"], "authors": ["Matthias Reisser", "Christos Louizos", "Efstratios Gavves", "Max Welling"], "keywords": ["Federated Learning", "personalized models", "non-i.i.d data"], "abstract": "Federated learning (FL) has emerged as the predominant approach for collaborative training of neural network models across multiple users, without the need to gather the data at a central location. One of the important challenges in this setting is data heterogeneity; different users have different data characteristics. For this reason, training and using a single global model might be suboptimal when considering the performance of each of the individual user\u2019s data. In this work, we tackle this problem via Federated Mixture of Experts, FedMix, a framework that allows us to train an ensemble of specialized models. FedMix adaptively selects and trains a user-specific selection of the ensemble members. We show that users with similar data characteristics select the same members and therefore share statistical strength while mitigating the effect of non-i.i.d data. Empirically, we show through an extensive experimental evaluation that FedMix improves performance compared to using a single global model while requiring similar or less communication costs.", "one-sentence_summary": "We propose Federated Mixture of Experts to tackle the non-i.i.d data problem in federated learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "reisser|federated_mixture_of_experts", "pdf": "/pdf/ea76c45ea010f0bbf4b97fb5a54dd42d495f3bad.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OoLI4R7-lL", "_bibtex": "@misc{\nreisser2021federated,\ntitle={Federated Mixture of Experts},\nauthor={Matthias Reisser and Christos Louizos and Efstratios Gavves and Max Welling},\nyear={2021},\nurl={https://openreview.net/forum?id=YgrdmztE4OY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "YgrdmztE4OY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1092/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1092/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1092/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1092/Authors|ICLR.cc/2021/Conference/Paper1092/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1092/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863783, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1092/-/Official_Comment"}}}, {"id": "owawxL6nFVu", "original": null, "number": 5, "cdate": 1605783112889, "ddate": null, "tcdate": 1605783112889, "tmdate": 1606124834410, "tddate": null, "forum": "YgrdmztE4OY", "replyto": "oCNPe4bmfVK", "invitation": "ICLR.cc/2021/Conference/Paper1092/-/Official_Comment", "content": {"title": "Response to AnonReviewer1", "comment": "We thank the reviewer for their feedback. We are glad that you find it interesting and would like to address your concerns.\n\nYou mention significant communication costs. We agree that FedMix in its na\u00efve form requires K times the communication budget. We propose FedMix as a way to combat non-iid-ness subject to this additional cost. The additional costs of communicating $q(z|s)$ and $\\phi$, however, are negligible: For Cifar10, for example, $\\phi$ is of size $(10\\times K)$ and $q(z|s)$ is just $1\\times K$.\n\nWe thank you for pointing out the potential privacy implications on inferring the label marginal $p(y|s)$ at the server-side. We have included, in the Appendix, a discussion on when a reconstruction of the marginal with FedMix is possible. Roughly, this is possible when the probability matrix q(z|y) is invertible, and that is generally only possible if $K \\geq$ # classes. It should be noted that FedAvg can also suffer from a similar attack, as we can infer the marginal label distribution via the gradient of the bias of the output layer. We have included a discussion about this and empirical results in the Appendix as well. \n\nYou mention the different design of the gating function across experiments. We have revised our Femnist experiments such that the architectural design of FedMix is now consistent across all our experiments.\n\nYour main critique of our experimental evaluation is that FedMix is being outperformed by other personalization approaches. We compare against the (limited) biased FedAvg and the Local/Global method. If you have further methods against which a comparison would convince you, we are glad to evaluate them. In our experience, the devil is often in the detail of the experimental setup (number of users, model, source of non-iid-ness, sampling rate of clients, number of local epochs etc.), so it is in general hard to make a paper-to-paper comparison without actually implementing the competing method in the same environment. If at all feasible within the rebuttal period, we will aim to compare if you provide us with concrete references. \n\nAs we have mentioned in our comment to Reviewer2, the federated mixture of experts approach is generally applicable for many sources of non-iid-ness without being restricted to specific data-sets. \nWe have added experiments on the setting where non-i.i.d-ness stems from $p(y|x)$, a setting where (biased) FedAvg fails and FedMix does not. Although FedMix might be out-performed by specific methods targeting a specific non-i.i.d.ness source, it presents a compelling option \u2018across all data-sets\u2019, so to say. \n\nWe hope that with our argumentation about the privacy of FedMix, our additional experiments and clarifications, you will re-evaluate our contribution and consider to raise your score.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1092/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1092/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Federated Mixture of Experts", "authorids": ["~Matthias_Reisser1", "~Christos_Louizos1", "~Efstratios_Gavves1", "~Max_Welling1"], "authors": ["Matthias Reisser", "Christos Louizos", "Efstratios Gavves", "Max Welling"], "keywords": ["Federated Learning", "personalized models", "non-i.i.d data"], "abstract": "Federated learning (FL) has emerged as the predominant approach for collaborative training of neural network models across multiple users, without the need to gather the data at a central location. One of the important challenges in this setting is data heterogeneity; different users have different data characteristics. For this reason, training and using a single global model might be suboptimal when considering the performance of each of the individual user\u2019s data. In this work, we tackle this problem via Federated Mixture of Experts, FedMix, a framework that allows us to train an ensemble of specialized models. FedMix adaptively selects and trains a user-specific selection of the ensemble members. We show that users with similar data characteristics select the same members and therefore share statistical strength while mitigating the effect of non-i.i.d data. Empirically, we show through an extensive experimental evaluation that FedMix improves performance compared to using a single global model while requiring similar or less communication costs.", "one-sentence_summary": "We propose Federated Mixture of Experts to tackle the non-i.i.d data problem in federated learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "reisser|federated_mixture_of_experts", "pdf": "/pdf/ea76c45ea010f0bbf4b97fb5a54dd42d495f3bad.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OoLI4R7-lL", "_bibtex": "@misc{\nreisser2021federated,\ntitle={Federated Mixture of Experts},\nauthor={Matthias Reisser and Christos Louizos and Efstratios Gavves and Max Welling},\nyear={2021},\nurl={https://openreview.net/forum?id=YgrdmztE4OY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "YgrdmztE4OY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1092/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1092/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1092/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1092/Authors|ICLR.cc/2021/Conference/Paper1092/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1092/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863783, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1092/-/Official_Comment"}}}, {"id": "QPVU8Xov7qX", "original": null, "number": 7, "cdate": 1605783344410, "ddate": null, "tcdate": 1605783344410, "tmdate": 1605783344410, "tddate": null, "forum": "YgrdmztE4OY", "replyto": "7oDIoYkhxl", "invitation": "ICLR.cc/2021/Conference/Paper1092/-/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "Thank you for your assessment of our paper. We are glad you found the paper easy to follow and its scope as relevant. \n\nThe reviewer mentions our experimental evaluation as a major drawback of our paper. Similarly to the other reviewers\u2019 comments, we would like to point out that our proposed biased FedAvg baseline is specialized to the label-skew setting, whereas FedMix generalizes to other sources of non-iid-ness. We showcase that in the case of rotated MNIST as well as additional experiments based on your suggestion of CFL [Sattler et. al 2019]. You are right that FedMix is clearly not superior on the label-skew experiments \u2013 however we would expect any specialized method to outperform a more general one.  \n\nPlease note that the STC paper experiments with a modified VGG architecture, whereas we experiment with LeNet-5 on Cifar10. The results are thus not comparable.  \nThe method of Clustered Federated Learning (CFL) is indeed similar to FedMix but crucially clusters users, whereas FedMix assigns portions of the input-space to different experts. We have taken your critique as an opportunity to show empirically that FedMix can recover the correct permutation in the setting that is discussed in the CFL paper. \n\nWe hope that the additional empirical observations will convince you about the usefulness of FedMix and you consider raising your score."}, "signatures": ["ICLR.cc/2021/Conference/Paper1092/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1092/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Federated Mixture of Experts", "authorids": ["~Matthias_Reisser1", "~Christos_Louizos1", "~Efstratios_Gavves1", "~Max_Welling1"], "authors": ["Matthias Reisser", "Christos Louizos", "Efstratios Gavves", "Max Welling"], "keywords": ["Federated Learning", "personalized models", "non-i.i.d data"], "abstract": "Federated learning (FL) has emerged as the predominant approach for collaborative training of neural network models across multiple users, without the need to gather the data at a central location. One of the important challenges in this setting is data heterogeneity; different users have different data characteristics. For this reason, training and using a single global model might be suboptimal when considering the performance of each of the individual user\u2019s data. In this work, we tackle this problem via Federated Mixture of Experts, FedMix, a framework that allows us to train an ensemble of specialized models. FedMix adaptively selects and trains a user-specific selection of the ensemble members. We show that users with similar data characteristics select the same members and therefore share statistical strength while mitigating the effect of non-i.i.d data. Empirically, we show through an extensive experimental evaluation that FedMix improves performance compared to using a single global model while requiring similar or less communication costs.", "one-sentence_summary": "We propose Federated Mixture of Experts to tackle the non-i.i.d data problem in federated learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "reisser|federated_mixture_of_experts", "pdf": "/pdf/ea76c45ea010f0bbf4b97fb5a54dd42d495f3bad.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OoLI4R7-lL", "_bibtex": "@misc{\nreisser2021federated,\ntitle={Federated Mixture of Experts},\nauthor={Matthias Reisser and Christos Louizos and Efstratios Gavves and Max Welling},\nyear={2021},\nurl={https://openreview.net/forum?id=YgrdmztE4OY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "YgrdmztE4OY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1092/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1092/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1092/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1092/Authors|ICLR.cc/2021/Conference/Paper1092/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1092/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863783, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1092/-/Official_Comment"}}}, {"id": "upZnAGa8Uph", "original": null, "number": 6, "cdate": 1605783170210, "ddate": null, "tcdate": 1605783170210, "tmdate": 1605783170210, "tddate": null, "forum": "YgrdmztE4OY", "replyto": "VMYQO5L2wZo", "invitation": "ICLR.cc/2021/Conference/Paper1092/-/Official_Comment", "content": {"title": "Response to AnonReviewer4", "comment": "We thank the reviewer for their assessment. \nPlease let us know which references for mixture models in FL we are missing \u2013 we will gladly improve our paper by including further references. If possible, we will compare against those methods, given that the devil is in the details of the experimental setup.\n\nLanguage models require some considerations for how to construct the gating mechanism and we leave such models to future work. Given proper construction, we expect FedMix to work for recurrent or transformer-based models. \n\nWe agree that FedMix in its na\u00efve version requires K times more bandwidth compared to some specialized approaches. FedMix does not require label-skew, as does, for example, our biased FedAvg baseline. We therefore believe that the federated mixture of experts approach can serve as a more general approach to non-iid-ness. To that extent, we have included a new discussion about the $p(y|x)$ non-i.i.d-ness setting and show that FedMix can recover it perfectly.\n\nPlease let us know where we can improve upon the clarity of our work.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1092/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1092/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Federated Mixture of Experts", "authorids": ["~Matthias_Reisser1", "~Christos_Louizos1", "~Efstratios_Gavves1", "~Max_Welling1"], "authors": ["Matthias Reisser", "Christos Louizos", "Efstratios Gavves", "Max Welling"], "keywords": ["Federated Learning", "personalized models", "non-i.i.d data"], "abstract": "Federated learning (FL) has emerged as the predominant approach for collaborative training of neural network models across multiple users, without the need to gather the data at a central location. One of the important challenges in this setting is data heterogeneity; different users have different data characteristics. For this reason, training and using a single global model might be suboptimal when considering the performance of each of the individual user\u2019s data. In this work, we tackle this problem via Federated Mixture of Experts, FedMix, a framework that allows us to train an ensemble of specialized models. FedMix adaptively selects and trains a user-specific selection of the ensemble members. We show that users with similar data characteristics select the same members and therefore share statistical strength while mitigating the effect of non-i.i.d data. Empirically, we show through an extensive experimental evaluation that FedMix improves performance compared to using a single global model while requiring similar or less communication costs.", "one-sentence_summary": "We propose Federated Mixture of Experts to tackle the non-i.i.d data problem in federated learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "reisser|federated_mixture_of_experts", "pdf": "/pdf/ea76c45ea010f0bbf4b97fb5a54dd42d495f3bad.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OoLI4R7-lL", "_bibtex": "@misc{\nreisser2021federated,\ntitle={Federated Mixture of Experts},\nauthor={Matthias Reisser and Christos Louizos and Efstratios Gavves and Max Welling},\nyear={2021},\nurl={https://openreview.net/forum?id=YgrdmztE4OY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "YgrdmztE4OY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1092/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1092/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1092/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1092/Authors|ICLR.cc/2021/Conference/Paper1092/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1092/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863783, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1092/-/Official_Comment"}}}, {"id": "SmP4gJxvOgr", "original": null, "number": 4, "cdate": 1605783058174, "ddate": null, "tcdate": 1605783058174, "tmdate": 1605783058174, "tddate": null, "forum": "YgrdmztE4OY", "replyto": "u6K_L3Z-e0D", "invitation": "ICLR.cc/2021/Conference/Paper1092/-/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "We thank the reviewer for their thorough review. We are glad to see that you find our approach and its specialization capabilities extremely appealing. We would like to address your concerns.\n\nIndeed, optimizing the MoE likelihood directly is challenging, which we first observed in the centralized training case. Before deciding upon localized or shared gating networks, performing gradient ascent on the likelihood directly is challenging to get to work in practice. Our motivation of lower-bounding the likelihood is therefore related to the MoE model, and not necessarily to the federated scenario. We find that the gate immediately collapses to a single expert and does not recover. Lower-bounding proved to be a robust solution to this problem in addition to the benefits we gain in the federated setting. We have added a visualization of the MoE behavior to the Appendix H.\n\nYou mention our changes to the lower-bound in the form of removing the entropy term at the client level. Indeed, our motivation for this is that we would like to encourage specialization of experts. As FedAvg shows, a single expert is quite capable of modelling the non-iid dataset to some degree. In order to not train \u2018K almost identical experts\u2019 and to manifest our intuition about the role of experts, we target specialization by encouraging low-entropy for the gates. Empirically we show that this intuition leads to better models (Figure 3). We believe that it is better to make such a decision \u2018informed\u2019 by first discussing the graphical model in its entirety. Similarly, the additional entropy term at the server is used since without an entropy term at the client, we empirically observed that some experts were not used at any shard. We understand that these modifications stem from intuition and empirical observations, which is why we either experimentally show the consequences of each modification (e.g. Figure 3) or mention what we observed directly. \n\nK<#classes is not a requirement, Figure 7 a) and c) in the Appendix show K=10 for Cifar10. We have updated the plots to include experiments with K=12 and K=14 for completeness. \n\nWith respect to the discussion of our experiments. It is true that FedMix adds a communication overhead to FedAvg as well as being out-performed by the specialized biased FedAvg approach that targets label skew. We argue that FedMix offers a general method, and, by marginalizing across non-iid dataset configurations, so to say, offers a robust choice. \nAll learning curves are created by evaluating the local dataset on the most recent server model (i.e. without fine-tuning). We observe significant overfitting in all considered algorithms when fine-tuning locally. Based on your suggestion, we have added a discussion on this point in the Appendix.\n\nWith respect to your minor points:\nYes, our Figures display the average of a sliding window. We have updated the main text accordingly.\nFigure 3 contains result on cifar10, we updated our caption.\nWe are using the \u2018Femnist\u2019 dataset as provided here https://github.com/TalwalkarLab/leaf, consisting of 3.5k users.\nWe have fixed notation\n\nThank you for pointing out ways to improve our paper. We hope that with the additional clarifications and modifications you will consider raising your score. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1092/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1092/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Federated Mixture of Experts", "authorids": ["~Matthias_Reisser1", "~Christos_Louizos1", "~Efstratios_Gavves1", "~Max_Welling1"], "authors": ["Matthias Reisser", "Christos Louizos", "Efstratios Gavves", "Max Welling"], "keywords": ["Federated Learning", "personalized models", "non-i.i.d data"], "abstract": "Federated learning (FL) has emerged as the predominant approach for collaborative training of neural network models across multiple users, without the need to gather the data at a central location. One of the important challenges in this setting is data heterogeneity; different users have different data characteristics. For this reason, training and using a single global model might be suboptimal when considering the performance of each of the individual user\u2019s data. In this work, we tackle this problem via Federated Mixture of Experts, FedMix, a framework that allows us to train an ensemble of specialized models. FedMix adaptively selects and trains a user-specific selection of the ensemble members. We show that users with similar data characteristics select the same members and therefore share statistical strength while mitigating the effect of non-i.i.d data. Empirically, we show through an extensive experimental evaluation that FedMix improves performance compared to using a single global model while requiring similar or less communication costs.", "one-sentence_summary": "We propose Federated Mixture of Experts to tackle the non-i.i.d data problem in federated learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "reisser|federated_mixture_of_experts", "pdf": "/pdf/ea76c45ea010f0bbf4b97fb5a54dd42d495f3bad.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OoLI4R7-lL", "_bibtex": "@misc{\nreisser2021federated,\ntitle={Federated Mixture of Experts},\nauthor={Matthias Reisser and Christos Louizos and Efstratios Gavves and Max Welling},\nyear={2021},\nurl={https://openreview.net/forum?id=YgrdmztE4OY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "YgrdmztE4OY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1092/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1092/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1092/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1092/Authors|ICLR.cc/2021/Conference/Paper1092/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1092/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863783, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1092/-/Official_Comment"}}}, {"id": "vB1VgXhlC1_", "original": null, "number": 3, "cdate": 1605782708825, "ddate": null, "tcdate": 1605782708825, "tmdate": 1605782708825, "tddate": null, "forum": "YgrdmztE4OY", "replyto": "YgrdmztE4OY", "invitation": "ICLR.cc/2021/Conference/Paper1092/-/Official_Comment", "content": {"title": "To all reviewers", "comment": "We thank the reviewers for their thorough evaluation and their feedback. We have updated the submission and made the following changes:\n1. In addition to experiments on non-i.i.d-ness on $p(y)$ and $p(x)$, we present a discussion on the experimental setup of Clustered Federated Learning [Sattler et. al,2019] and show that FedMix is able to perfectly recover the label permutation cluster assignment across the different clients. This corresponds to non-i.i.d-ness in $p(y|x)$.\n2. We have changed our parameterization of the gating mechanism $p(z|x,s)$ for the Femnist experiments to be consistent across all experiments. We have updated tables, plots and the discussion of those results.\n3. We added, in the Appendix, a discussion on the potential danger of inferring the label marginal $p(y|s)$ in FedMix and FedAvg.\n4. We discuss overfitting when locally fine-tuning models in the Appendix.\n\nWe believe your suggestions and critique allowed us to improve our submission and hope that you consider increasing your score."}, "signatures": ["ICLR.cc/2021/Conference/Paper1092/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1092/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Federated Mixture of Experts", "authorids": ["~Matthias_Reisser1", "~Christos_Louizos1", "~Efstratios_Gavves1", "~Max_Welling1"], "authors": ["Matthias Reisser", "Christos Louizos", "Efstratios Gavves", "Max Welling"], "keywords": ["Federated Learning", "personalized models", "non-i.i.d data"], "abstract": "Federated learning (FL) has emerged as the predominant approach for collaborative training of neural network models across multiple users, without the need to gather the data at a central location. One of the important challenges in this setting is data heterogeneity; different users have different data characteristics. For this reason, training and using a single global model might be suboptimal when considering the performance of each of the individual user\u2019s data. In this work, we tackle this problem via Federated Mixture of Experts, FedMix, a framework that allows us to train an ensemble of specialized models. FedMix adaptively selects and trains a user-specific selection of the ensemble members. We show that users with similar data characteristics select the same members and therefore share statistical strength while mitigating the effect of non-i.i.d data. Empirically, we show through an extensive experimental evaluation that FedMix improves performance compared to using a single global model while requiring similar or less communication costs.", "one-sentence_summary": "We propose Federated Mixture of Experts to tackle the non-i.i.d data problem in federated learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "reisser|federated_mixture_of_experts", "pdf": "/pdf/ea76c45ea010f0bbf4b97fb5a54dd42d495f3bad.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OoLI4R7-lL", "_bibtex": "@misc{\nreisser2021federated,\ntitle={Federated Mixture of Experts},\nauthor={Matthias Reisser and Christos Louizos and Efstratios Gavves and Max Welling},\nyear={2021},\nurl={https://openreview.net/forum?id=YgrdmztE4OY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "YgrdmztE4OY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1092/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1092/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1092/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1092/Authors|ICLR.cc/2021/Conference/Paper1092/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1092/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863783, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1092/-/Official_Comment"}}}, {"id": "7oDIoYkhxl", "original": null, "number": 1, "cdate": 1603989531031, "ddate": null, "tcdate": 1603989531031, "tmdate": 1605024533064, "tddate": null, "forum": "YgrdmztE4OY", "replyto": "YgrdmztE4OY", "invitation": "ICLR.cc/2021/Conference/Paper1092/-/Official_Review", "content": {"title": "Interesting approach, more in-depth experimental evaluation and theoretical insights needed", "review": "The paper proposes a novel algorithm, which is a federated form on mixture of experts, called Federated Mixture of Experts (FedMix). In FedMix, an ensemble of specialized models is trained instead of a single global model. This strikes a compromise between training a single global model and one model per client. A gating mechanism is employed, to choose an expert model that is responsible for a given data point, thus aligning the gradient updates across experts and alleviating the consequences of non-i.i.d. data.\n\nPros:\n- the paper is well written and easy to follow\n- it tackles an important problem, namely how to cope with non-iid data in FL\n- experiments are performed on different datasets\n\nCons:\n- A major drawback of the paper is the lack of  in-depth experimental evaluation and theoretical insights. On Cifar-10 and Cifar-100 the proposed method performs worse than related state-of-the-art methods. On the rotated MNIST experiments in 4.2 it performs better, however, this section is very short and the insights from the presented results are limited.\n\n- The STC approach presented in \"Robust and Communication-Efficient Federated Learning from Non-IID Data\" performs even better than biased FedAve on Cifar-10. It may well be that it also performs well in the rotated MNIST experiments. Anyways I am not convinced that the proposed method is superior to state-of-the-art techniques in non-iid settings (on Cifar-10 and -100 it is clearly not).\n\n- The paper does not provide any new theoretical insights, it only reports empirical results. For instance, can you give guarantees that the proposed approach will result in K specialised models if the client data was generated from K different distributions (see  proof in Sattler et al. 2020).\n\n- The authors mention the conceptual similarity of the proposed approach to Clustered Federated Learning (Sattler et al. 2020, Briggs et al., 2020), that allow to jointly train specialised models. Why do you not compare the proposed approach to these methods? I assume that Clustered FL will solve the rotated MNIST problem (although it may be worse in terms of communication-efficiency). More insightful experiments would be great.\n\nOverall, I am not convinced by the proposed method and be the reported results.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1092/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1092/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Federated Mixture of Experts", "authorids": ["~Matthias_Reisser1", "~Christos_Louizos1", "~Efstratios_Gavves1", "~Max_Welling1"], "authors": ["Matthias Reisser", "Christos Louizos", "Efstratios Gavves", "Max Welling"], "keywords": ["Federated Learning", "personalized models", "non-i.i.d data"], "abstract": "Federated learning (FL) has emerged as the predominant approach for collaborative training of neural network models across multiple users, without the need to gather the data at a central location. One of the important challenges in this setting is data heterogeneity; different users have different data characteristics. For this reason, training and using a single global model might be suboptimal when considering the performance of each of the individual user\u2019s data. In this work, we tackle this problem via Federated Mixture of Experts, FedMix, a framework that allows us to train an ensemble of specialized models. FedMix adaptively selects and trains a user-specific selection of the ensemble members. We show that users with similar data characteristics select the same members and therefore share statistical strength while mitigating the effect of non-i.i.d data. Empirically, we show through an extensive experimental evaluation that FedMix improves performance compared to using a single global model while requiring similar or less communication costs.", "one-sentence_summary": "We propose Federated Mixture of Experts to tackle the non-i.i.d data problem in federated learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "reisser|federated_mixture_of_experts", "pdf": "/pdf/ea76c45ea010f0bbf4b97fb5a54dd42d495f3bad.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OoLI4R7-lL", "_bibtex": "@misc{\nreisser2021federated,\ntitle={Federated Mixture of Experts},\nauthor={Matthias Reisser and Christos Louizos and Efstratios Gavves and Max Welling},\nyear={2021},\nurl={https://openreview.net/forum?id=YgrdmztE4OY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "YgrdmztE4OY", "replyto": "YgrdmztE4OY", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1092/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538127186, "tmdate": 1606915771941, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1092/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1092/-/Official_Review"}}}, {"id": "VMYQO5L2wZo", "original": null, "number": 2, "cdate": 1604038699471, "ddate": null, "tcdate": 1604038699471, "tmdate": 1605024533006, "tddate": null, "forum": "YgrdmztE4OY", "replyto": "YgrdmztE4OY", "invitation": "ICLR.cc/2021/Conference/Paper1092/-/Official_Review", "content": {"title": "The idea is interesting but improvements are expected", "review": "Strength:\n* The idea of training an ensemble of specialized models in FL is interesting, although not novel.\n*  The evaluation is solid. I would be more impressed if the authors could also evaluate some SOTA language models.\n\nWeakness:\n*  The technical contribution is limited and the novelty is not sufficient. I am not against \"A+B\" research if it does bring drastic practical improvement. However, the proposed method is worse than some prior works on CIFAR 10 even using ten-time bandwidth (correct me if I interpret the results wrongly).\n*  There are many other works with similar technical consideration although their intention might be different. Please check the line of works about personalized FL and compare with them if possible.\n*  The writing is not clear given that the idea is actually simple.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1092/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1092/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Federated Mixture of Experts", "authorids": ["~Matthias_Reisser1", "~Christos_Louizos1", "~Efstratios_Gavves1", "~Max_Welling1"], "authors": ["Matthias Reisser", "Christos Louizos", "Efstratios Gavves", "Max Welling"], "keywords": ["Federated Learning", "personalized models", "non-i.i.d data"], "abstract": "Federated learning (FL) has emerged as the predominant approach for collaborative training of neural network models across multiple users, without the need to gather the data at a central location. One of the important challenges in this setting is data heterogeneity; different users have different data characteristics. For this reason, training and using a single global model might be suboptimal when considering the performance of each of the individual user\u2019s data. In this work, we tackle this problem via Federated Mixture of Experts, FedMix, a framework that allows us to train an ensemble of specialized models. FedMix adaptively selects and trains a user-specific selection of the ensemble members. We show that users with similar data characteristics select the same members and therefore share statistical strength while mitigating the effect of non-i.i.d data. Empirically, we show through an extensive experimental evaluation that FedMix improves performance compared to using a single global model while requiring similar or less communication costs.", "one-sentence_summary": "We propose Federated Mixture of Experts to tackle the non-i.i.d data problem in federated learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "reisser|federated_mixture_of_experts", "pdf": "/pdf/ea76c45ea010f0bbf4b97fb5a54dd42d495f3bad.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OoLI4R7-lL", "_bibtex": "@misc{\nreisser2021federated,\ntitle={Federated Mixture of Experts},\nauthor={Matthias Reisser and Christos Louizos and Efstratios Gavves and Max Welling},\nyear={2021},\nurl={https://openreview.net/forum?id=YgrdmztE4OY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "YgrdmztE4OY", "replyto": "YgrdmztE4OY", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1092/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538127186, "tmdate": 1606915771941, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1092/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1092/-/Official_Review"}}}, {"id": "oCNPe4bmfVK", "original": null, "number": 3, "cdate": 1604594041902, "ddate": null, "tcdate": 1604594041902, "tmdate": 1605024532942, "tddate": null, "forum": "YgrdmztE4OY", "replyto": "YgrdmztE4OY", "invitation": "ICLR.cc/2021/Conference/Paper1092/-/Official_Review", "content": {"title": "A (personalized) mixture of experts, but with heavy communication and privacy cost", "review": "The authors propose using an ensemble of experts, but where the mixing proportions depend on both the data and the client (providing the personalization in the method). The proposed method is very interesting, but there are several issues which need to be addressed.\n\nThe method requires significantly higher communication cost to train, since multiple versions of the model parameters need to be communicated, as well as parameters for other models used for the mixing. The authors propose suggestions for reducing the number of models that need to be communicated, but they rely on very special behaviour of the different models (namely that they separate well so that clients do not actually need all of them). \n\nThe clients need to transmit mixing parameters $q(z|s)$ as well as the $\\phi$ which generate them. Given access to $N_s$ and the discrete nature of $y$, this would allow the server to reconstruct the label distribution of each user's data, which is a huge reduction in the privacy achieved by federated learning. \n\nThe algorithm is quite complicated, and appears fairly fragile, as is demonstrated by the different functions for $h_k(x)$ required for just 3 data sets. Selection of tuning parameters in federated learning is a challenging and task, and having one which is a function makes the method quite challenging to use in practice.\n\nExperimental results are not very compelling, with a simple personalized bias term outperforming the proposed model in terms of both number of communication rounds and total volume of data transmitted, and does not produce the privacy violation the proposed method produces. Although the simple personalized bias does not generalize beyond label distribution, there are many many similar personalization proposals for federated learning which do, and it is not clear whether the proposed method is competitive with those. Of course comparison with all existing methods is not feasible, but given the method must be entirely judged by empirical results, comparison with at least some should be done.\n\nOverall it is not clear that the extra costs induced by the method are justified by the improvement in convergence speed, especially compared to more simple personalization methods which do not incur any of these extra costs (either in communication or privacy). ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1092/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1092/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Federated Mixture of Experts", "authorids": ["~Matthias_Reisser1", "~Christos_Louizos1", "~Efstratios_Gavves1", "~Max_Welling1"], "authors": ["Matthias Reisser", "Christos Louizos", "Efstratios Gavves", "Max Welling"], "keywords": ["Federated Learning", "personalized models", "non-i.i.d data"], "abstract": "Federated learning (FL) has emerged as the predominant approach for collaborative training of neural network models across multiple users, without the need to gather the data at a central location. One of the important challenges in this setting is data heterogeneity; different users have different data characteristics. For this reason, training and using a single global model might be suboptimal when considering the performance of each of the individual user\u2019s data. In this work, we tackle this problem via Federated Mixture of Experts, FedMix, a framework that allows us to train an ensemble of specialized models. FedMix adaptively selects and trains a user-specific selection of the ensemble members. We show that users with similar data characteristics select the same members and therefore share statistical strength while mitigating the effect of non-i.i.d data. Empirically, we show through an extensive experimental evaluation that FedMix improves performance compared to using a single global model while requiring similar or less communication costs.", "one-sentence_summary": "We propose Federated Mixture of Experts to tackle the non-i.i.d data problem in federated learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "reisser|federated_mixture_of_experts", "pdf": "/pdf/ea76c45ea010f0bbf4b97fb5a54dd42d495f3bad.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OoLI4R7-lL", "_bibtex": "@misc{\nreisser2021federated,\ntitle={Federated Mixture of Experts},\nauthor={Matthias Reisser and Christos Louizos and Efstratios Gavves and Max Welling},\nyear={2021},\nurl={https://openreview.net/forum?id=YgrdmztE4OY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "YgrdmztE4OY", "replyto": "YgrdmztE4OY", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1092/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538127186, "tmdate": 1606915771941, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1092/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1092/-/Official_Review"}}}, {"id": "u6K_L3Z-e0D", "original": null, "number": 4, "cdate": 1604614329966, "ddate": null, "tcdate": 1604614329966, "tmdate": 1605024532878, "tddate": null, "forum": "YgrdmztE4OY", "replyto": "YgrdmztE4OY", "invitation": "ICLR.cc/2021/Conference/Paper1092/-/Official_Review", "content": {"title": "Interesting approach, but subpar results and needs more clarification", "review": "-- Summary --\nThe paper proposes a method for federated learning of a mixture of experts model (FedMix). The approach allows training an ensemble of models each of which specializes to a subset of clients with similar data characteristics. The authors argue that this way of training an ensemble reduces the gradient divergence/interference, improves the overall performance, and sometimes reduces the communication overhead. The new method is evaluated a few federated image classification datasets.\n\n\n-- Overall evaluation --\nI find the idea of training a model ensemble instead of a single global model extremely appealing in the context of federated learning. The fact that the proposed approach allows models in the mixture (i.e., experts) to automatically specialize to different subsets of clients throughout training without having to run any clustering on the server makes the approach particularly appealing. Having said that, I also believe that the paper in the current form has many weaknesses, including:\n- a significant lack of clarity throughout section 2 (see comments and questions below),\n- in terms of methods, fairly ad-hoc changes introduced into the evidence lower-bound objective given in eq. 5 (removal of the entropy term and addition of a different regularize, which is justified in a very handwavy way),\n- weak experimental results, which indicate that FedMix is, in fact, worse than the baselines both in terms of performance and the communication cost (unless I'm misreading Table 1 and Figure 4).\n\nI believe that the paper has interesting and novel ideas, but falls short on both presenting them as well as getting them to work empirically. I would not recommend accepting the paper in the current form.\n\n\n-- Comments and questions --\n\n- Using MoE in federated learning makes a lot of sense intuitively. The authors further specialize MoE to FL by introducing a local gating network and then tying everything together through a global q distribution that approximates the posterior. These design choices are justified by stating that running federated optimization on the objective given in eq. 3 does not work well. I wonder, however, why not use a shared gating network instead of having to approximate the objective?\n\n- I find the way lower bound in eqs. 5-6 is introduced and then the entropy term is dropped and substituted with some regularize (see 2nd paragraph on page 4) very ad-hoc. Typically, one would use a graphical model such as the one given in Figure 2 to derive a loss function in a principled way, without additional \"hacks.\" Why use the graphical model formalism in the first place, if the final loss is being designed using some additional intuitive heuristics? Relatedly, perhaps there's a way to change the graphical model somehow and show that the final loss corresponds to the evidence lower bound in a slightly different probabilistic model? (e.g., with a different prior)\n\n- Since $q_\\phi(z \\mid y)$ is a distribution over the experts in the mixture conditional on the class of the data point, what happens if K > # classes? It seems like q will select a separate model for each output class, so each expert will specialize to a single class only. How would that affect performance? Is K < # classes a requirement?\n\n\n-- Experiments --\n\n- If I'm not misreading the results in Table 1, FedMix adds a drastic communication overhead (due to multiple experts in the mixture) and does not perform as well as biased FedAvg on the CIFAR 10/100 datasets. Results for two baselines for EMNIST are missing.\n\n- Similarly, results presented on Figure 4 for CIFAR datasets demonstrate that the baselines are better.\n\n- For FedAvg, was the accuracy computed for the global model or for the global model after fine-tuning locally? Since FedMix targets to improve personalized FL, it should be compared with FedAvg with a simple fine-tuning-based personalization.\n\n\n-- Minor points --\n\n- Figure 1: are the bold curves some kind of running averages? Also, if the figure is given in the main text, it's strange that the definition of gradient divergence or alignment is provided in the appendix. I would recommend giving it somewhere in the main text.\n- Figure 3: which dataset and task are these results for?\n- Section 4, paragraph 1: I believe the EMNIST dataset has 3400 users if the version from tensorflow federated is used.\n- Very minor: In Eq. 3, the notation for the sum over S is strange; maybe change to $\\sum_{s=1}^S$?", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1092/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1092/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Federated Mixture of Experts", "authorids": ["~Matthias_Reisser1", "~Christos_Louizos1", "~Efstratios_Gavves1", "~Max_Welling1"], "authors": ["Matthias Reisser", "Christos Louizos", "Efstratios Gavves", "Max Welling"], "keywords": ["Federated Learning", "personalized models", "non-i.i.d data"], "abstract": "Federated learning (FL) has emerged as the predominant approach for collaborative training of neural network models across multiple users, without the need to gather the data at a central location. One of the important challenges in this setting is data heterogeneity; different users have different data characteristics. For this reason, training and using a single global model might be suboptimal when considering the performance of each of the individual user\u2019s data. In this work, we tackle this problem via Federated Mixture of Experts, FedMix, a framework that allows us to train an ensemble of specialized models. FedMix adaptively selects and trains a user-specific selection of the ensemble members. We show that users with similar data characteristics select the same members and therefore share statistical strength while mitigating the effect of non-i.i.d data. Empirically, we show through an extensive experimental evaluation that FedMix improves performance compared to using a single global model while requiring similar or less communication costs.", "one-sentence_summary": "We propose Federated Mixture of Experts to tackle the non-i.i.d data problem in federated learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "reisser|federated_mixture_of_experts", "pdf": "/pdf/ea76c45ea010f0bbf4b97fb5a54dd42d495f3bad.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=OoLI4R7-lL", "_bibtex": "@misc{\nreisser2021federated,\ntitle={Federated Mixture of Experts},\nauthor={Matthias Reisser and Christos Louizos and Efstratios Gavves and Max Welling},\nyear={2021},\nurl={https://openreview.net/forum?id=YgrdmztE4OY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "YgrdmztE4OY", "replyto": "YgrdmztE4OY", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1092/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538127186, "tmdate": 1606915771941, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1092/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1092/-/Official_Review"}}}], "count": 18}