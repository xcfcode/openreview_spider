{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1487628365528, "tcdate": 1478286198439, "number": 319, "id": "BJC_jUqxe", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "BJC_jUqxe", "signatures": ["~Zhouhan_Lin1"], "readers": ["everyone"], "content": {"title": "A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING", "abstract": "This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.", "pdf": "/pdf/04457e6e3ea211450caf6a06cf0981744ba33849.pdf", "TL;DR": "a new model for extracting an interpretable sentence embedding by introducing self-attention and matrix representation.", "paperhash": "lin|a_structured_selfattentive_sentence_embedding", "authors": ["Zhouhan Lin", "Minwei Feng", "Cicero Nogueira dos Santos", "Mo Yu", "Bing Xiang", "Bowen Zhou", "Yoshua Bengio"], "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"], "conflicts": ["us.ibm.com", "iro.umontreal.ca", "umontreal.ca"], "authorids": ["lin.zhouhan@gmail.com", "mfeng@us.ibm.com", "cicerons@us.ibm.com", "yum@us.ibm.com", "bingxia@us.ibm.com", "zhou@us.ibm.com", "yoshua.bengio@umontreal.ca"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 15, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396502733, "tcdate": 1486396502733, "number": 1, "id": "HJkw3fLdl", "invitation": "ICLR.cc/2017/conference/-/paper319/acceptance", "forum": "BJC_jUqxe", "replyto": "BJC_jUqxe", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "A solid paper about sentence representation learning using \"internal attention\" (representing sentences as a self-attention matrix rather than a vector). The idea is novel, well presented, and has potential applications to a variety of sequence-based problems. The main criticism in reviews was about minor points needing clarification, but I am suitably satisfied that these have been acknowledged and incorporated into the current revision. Reviewer 2 is concerned that the experiments do not fully support the claim that this model will improve end task performance over more standard attention mechanisms. Glancing over the paper, and in light of other reviews and discussion, I am not convinced that this objection should halt acceptance, but invite the authors to take this suggestion onboard for future work (or in the final version of their paper). This paper should be accepted to the conference.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING", "abstract": "This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.", "pdf": "/pdf/04457e6e3ea211450caf6a06cf0981744ba33849.pdf", "TL;DR": "a new model for extracting an interpretable sentence embedding by introducing self-attention and matrix representation.", "paperhash": "lin|a_structured_selfattentive_sentence_embedding", "authors": ["Zhouhan Lin", "Minwei Feng", "Cicero Nogueira dos Santos", "Mo Yu", "Bing Xiang", "Bowen Zhou", "Yoshua Bengio"], "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"], "conflicts": ["us.ibm.com", "iro.umontreal.ca", "umontreal.ca"], "authorids": ["lin.zhouhan@gmail.com", "mfeng@us.ibm.com", "cicerons@us.ibm.com", "yum@us.ibm.com", "bingxia@us.ibm.com", "zhou@us.ibm.com", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396503468, "id": "ICLR.cc/2017/conference/-/paper319/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "BJC_jUqxe", "replyto": "BJC_jUqxe", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396503468}}}, {"tddate": null, "tmdate": 1484371564571, "tcdate": 1484370254026, "number": 9, "id": "rJIUZEPIx", "invitation": "ICLR.cc/2017/conference/-/paper319/public/comment", "forum": "BJC_jUqxe", "replyto": "ByHCv9b4e", "signatures": ["~Zhouhan_Lin1"], "readers": ["everyone"], "writers": ["~Zhouhan_Lin1"], "content": {"title": "Rebuttal to Reviewer1", "comment": "We thank the reviewer for the detailed review and suggestion. We have made 3 major changes and some other minor changes in our paper to address the former two concerns you raised. \n\n1. We rewrite the \"related works\" section in the revised paper. We cited those works mentioned in the other comment, discussed and compared them with our proposed approach. \n\n2. Also, we now have added new experiments in Section 4.4.2 that shows the influence of attention vector numbers ($r$). \n\n3. To address the concern about parameters, we add a supplementary material which introduced weight pruning by taking advantage of the matrix structure. This weight pruning mechanism allows us to drastically reduce the number of parameters, by sacrificing a bit of performance. \n\nOn the other hand, we want to mention that all of the hyperparameters for baseline models are either independently optimized or cited from other papers. Thus those baseline results are almost the best these models can achieve, even if you add more parameters to it. So in terms of performance, we think having more parameters doesn\u2019t necessarily give our model advantage.\n\nQuote:\"Specifically, the statement that \"the best way to evaluate the diversity is definitely the Kullback-Leibler divergence between any 2 of the summation weight vectors\" runs somewhat counter to the authors' comments on this topic below.\n\"\nThat statement was not precise and detailed enough. Now we have updated the statement about KL divergence in the paper, integrating some of the contents in the discussion made on this webpage. \n\nAbout concerns on Figure 2: We agree that it is very hard to find evidence to support the idea of using multiple vectors just by looking at the heatmap plots. Now we have a new subsection in the explorative experiments part (Section 4.4.2) to provide quantitative evidences to support the idea of using multiple vectors. Figure 2 by itself is trying to show that having the penalty term forces the model to learn a set of attentions that is more variant. Figure 2 left every hops of attention appears very similar, which in the right the model's attention exhibits more variance.\n\nNow in the new 4.4.2 section, we can tell that the model performance is indeed influenced by $r$ values. Having a matrix embedding brings a significant advantage over using vector embeddings (i.e., when $r=1$). We can figure that $r$ is not the larger the better (and so do the parameters), a value between 5~20 turned out to be the most suitable. We also show in table 1 that matrix sentence outperforms LSTM and CNN baselines. In Table 2, we show that matrix embedding outperforms a bunch of other sentence embedding models by a significant margin, only slightly worse than NSE. \n\nThanks again for your contributive comments and suggestions, and we sincerely hope our revision addresses your concerns satisfyingly.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING", "abstract": "This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.", "pdf": "/pdf/04457e6e3ea211450caf6a06cf0981744ba33849.pdf", "TL;DR": "a new model for extracting an interpretable sentence embedding by introducing self-attention and matrix representation.", "paperhash": "lin|a_structured_selfattentive_sentence_embedding", "authors": ["Zhouhan Lin", "Minwei Feng", "Cicero Nogueira dos Santos", "Mo Yu", "Bing Xiang", "Bowen Zhou", "Yoshua Bengio"], "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"], "conflicts": ["us.ibm.com", "iro.umontreal.ca", "umontreal.ca"], "authorids": ["lin.zhouhan@gmail.com", "mfeng@us.ibm.com", "cicerons@us.ibm.com", "yum@us.ibm.com", "bingxia@us.ibm.com", "zhou@us.ibm.com", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287622953, "id": "ICLR.cc/2017/conference/-/paper319/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJC_jUqxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper319/reviewers", "ICLR.cc/2017/conference/paper319/areachairs"], "cdate": 1485287622953}}}, {"tddate": null, "tmdate": 1484371061800, "tcdate": 1484370898811, "number": 10, "id": "rJjCX4PIg", "invitation": "ICLR.cc/2017/conference/-/paper319/public/comment", "forum": "BJC_jUqxe", "replyto": "H18MIfimg", "signatures": ["~Zhouhan_Lin1"], "readers": ["everyone"], "writers": ["~Zhouhan_Lin1"], "content": {"title": "Rebuttal to Reviewer3", "comment": "Thanks for your review and comments. We made 4 changes according to your suggestion:\n\n1. Now we have made the related works more complete by adding several new citations to supervised learning. \n\n2. As we responded in the former comment, there are two ways to use the matrix embeddings. We have made it clear in the paper and added a whole appendix about the other parameter-saving method. One gives you better performance but with a larger number of parameters, the other reduces the parameters a lot but at the cost of reducing performance a little bit. \n\n3. We also added graphical representations of the models both in the paper and in the supplementary material. \n\n4. We discussed LSTMN in the related work section, which we also think is a very important related work.\n\nThe visualization introduced in Jiwei et. al. 2015 is an interesting and related work. It focuses on understanding how the LSTM states function in NLP applications. The closest visualization among all the visualization methods proposed in that paper is the Figure 9 in their paper. However we still haven't found a good enough way to compare this: it seems more similar to LSTMN's type of attention. So in this revision, we haven't included it.\n\nThanks again for your contributive comments and suggestions, and we sincerely hope our revision addresses your concerns satisfyingly.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING", "abstract": "This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.", "pdf": "/pdf/04457e6e3ea211450caf6a06cf0981744ba33849.pdf", "TL;DR": "a new model for extracting an interpretable sentence embedding by introducing self-attention and matrix representation.", "paperhash": "lin|a_structured_selfattentive_sentence_embedding", "authors": ["Zhouhan Lin", "Minwei Feng", "Cicero Nogueira dos Santos", "Mo Yu", "Bing Xiang", "Bowen Zhou", "Yoshua Bengio"], "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"], "conflicts": ["us.ibm.com", "iro.umontreal.ca", "umontreal.ca"], "authorids": ["lin.zhouhan@gmail.com", "mfeng@us.ibm.com", "cicerons@us.ibm.com", "yum@us.ibm.com", "bingxia@us.ibm.com", "zhou@us.ibm.com", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287622953, "id": "ICLR.cc/2017/conference/-/paper319/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJC_jUqxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper319/reviewers", "ICLR.cc/2017/conference/paper319/areachairs"], "cdate": 1485287622953}}}, {"tddate": null, "tmdate": 1484369757299, "tcdate": 1484369757299, "number": 8, "id": "SJBDy4v8x", "invitation": "ICLR.cc/2017/conference/-/paper319/public/comment", "forum": "BJC_jUqxe", "replyto": "H1zxxEXEg", "signatures": ["~Zhouhan_Lin1"], "readers": ["everyone"], "writers": ["~Zhouhan_Lin1"], "content": {"title": "Rebuttal to Reviewer2", "comment": "We thank the reviewer for the detailed review, and for your interest in our multiple-vector representation. \n\nWe do agree that we are lacking a set of experiments about how the $r$, i.e. number of rows in the matrix representation, would affect the model\u2019s performance. In the reviewed version of the text, we added 2 sets of experiments in Section 4.4.2, one for yelp and the other for SNLI, that shows how the model\u2019s performance is related to $r$. In both experiments, it turned out that using multiple vector representation does have a significant impact on the performance on each task. Additionally, although we were using $r=30$ in previous experiments, we also show that a smaller $r$ would also yield a similar result. \n\nAs for the model size, we do agree that our model ends up with a much larger number of parameters. Like in convnets, 90% of the weights are in the output MLP (the MLP connecting the matrix representation to softmax). We do have a weight pruning mechanism that significantly reduces the number of parameters in the big output MLP by taking advantage of the matrix representation. We didn\u2019t include that in the first version of the paper because we wanted to use the limited space to focus on the sentence embedding and penalization term. Now we added a full supplementary material on how these weight sharing can significantly reduce the parameters. Compared to our original model, we can still get similar results after structurally pruning most of the weights. \n\nOn the other hand, the improvement one can reach by naively adding more and more parameters to a certain model is really marginal. Hyperparameters in all baseline models in our paper are either optimized independently or directly cited from other papers, which means the reported baseline results are almost the best these models can do, even if you add more parameters to it. So in terms of performance, we think having more parameters doesn\u2019t necessarily give our model advantage. But we do agree that having more parameters means taking more memory space, more inference time, etc. In addition, the new Appendix A we attached to the end of the paper provides a satisfying method to overcome this problem.\n\nQuote:\"In the conclusion, the authors remark that \"attention mechanism reliefs the burden of LSTM\". If the 2D representations are effective in that aspect, I'd expect that the authors might be able to train with a smaller LSTM. Testing the effect of LSTM dimension vs $r$ will be helpful.\n\"\nThanks for pointing that out. It seems that we didn\u2019t state it in a clear enough way. The amount of information for each LSTM hidden state to carry is not changed, since each hidden state should still roughly carry the amount of information contained in a word embedding. What is \u201crelieved\u201d is that the LSTM is not expected to carry all information through all of its hidden states, which is the case if one picks the last LSTM state as sentence embedding. Just providing some correlational information around each hidden state\u2019s peripheral time steps would be sufficient for the self-attention model. Of course the latter requires the hidden states to cover more content and be more abstractive, thus is much harder to learn. So the \u201crelief\u201d here is more about relieving the long term dependencies, which cannot be solved by changing hidden state numbers. Now we have rephrased our reasoning in the corresponding part of the paper. \n\nWe thought that since the sentence embedding is for supervised tasks, the matrix sentence embedding model can be combined with any downstream applications. So the objective is thus dependent on what tasks the downstream application belongs to. To make the architecture presentation clearer, we added graphical representations of the structure of the model. \n\nFigure 3 is intended to show that with the penalty term, the \"overall attention\" becomes more focused. Both models trained with and without the penalty term will learn to focus on keywords that contribute to the sentiment of the sentence, but the difference we want to show is that the one trained with penalty will pay even less attention to less related parts of the sentence. This is reflected in the plot that many of the shallow red parts are even shallower when using penalty term.\n\nThanks again for your contributive comments and suggestions, and we sincerely hope our revision addresses your concerns satisfyingly.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING", "abstract": "This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.", "pdf": "/pdf/04457e6e3ea211450caf6a06cf0981744ba33849.pdf", "TL;DR": "a new model for extracting an interpretable sentence embedding by introducing self-attention and matrix representation.", "paperhash": "lin|a_structured_selfattentive_sentence_embedding", "authors": ["Zhouhan Lin", "Minwei Feng", "Cicero Nogueira dos Santos", "Mo Yu", "Bing Xiang", "Bowen Zhou", "Yoshua Bengio"], "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"], "conflicts": ["us.ibm.com", "iro.umontreal.ca", "umontreal.ca"], "authorids": ["lin.zhouhan@gmail.com", "mfeng@us.ibm.com", "cicerons@us.ibm.com", "yum@us.ibm.com", "bingxia@us.ibm.com", "zhou@us.ibm.com", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287622953, "id": "ICLR.cc/2017/conference/-/paper319/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJC_jUqxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper319/reviewers", "ICLR.cc/2017/conference/paper319/areachairs"], "cdate": 1485287622953}}}, {"tddate": null, "tmdate": 1484312216654, "tcdate": 1484311985939, "number": 7, "id": "Hkch6rL8g", "invitation": "ICLR.cc/2017/conference/-/paper319/public/comment", "forum": "BJC_jUqxe", "replyto": "Sypa05lNx", "signatures": ["~Zhouhan_Lin1"], "readers": ["everyone"], "writers": ["~Zhouhan_Lin1"], "content": {"title": "related work", "comment": "Thanks for mentioning these works to us. We agree that the \"related works\" part was skewed towards unsupervised methods.  Now we added these 5 papers to the related works part in the new revision of the paper. We think that we should really cite the paper (d) and (e)(Li et al. 2016) since they are closely related. \n\nThe Section 3.3 in (e) is indeed almost the same idea about self-attention (with tiny differences, like adjustable hidden layer sizes and activations). We had this idea during mid-June, and when we googled around at that time we didn\u2019t find anything similar\u2026 Now we cited, highlighted this paper in the updated version, ensuring readers to know that there exists another group coming up with a similar idea concurrently.\n\nThere are still several differences between (d) and our work: they were using a mean pooling over LSTM states to get the attention source, and we were using an MLP. They have to augment their features as well as input sequences to get the 84.6% accuracy in SNLI dataset, however we were not using those augmentations. \n\n(a) proposed an attention based word embedding model, which uses a huge matrix to store the attention weights for each relative position of each word. This is indeed a self-attention mechanism, but it cannot be properly extended to sentences. -- It is possible to list all words and compute a set of attention weights for each of the word, but you can't exhaustively list all possible sentences. So we don't think this work is closely related. \n\nThe attention mechanism (LSTMN) proposed in (b) is very successful and popular, and it has been incorporated into the \u201cintra-sentence\u201d level attention in (c). LSTMN uses a sort of \"online updating\" attention, which computes an attention vector for each _word_ in the sentence. However, our attention mechanism is only conducted once after the LSTM iterates through the whole _sentence_. So (b) is a more fine-grained attention that targets at discovering lexical correlations between a certain word and its previous words.  We can say that they are different levels of attention. As a result, the two methods are also different in terms of computational complexity. You can refer to the response we wrote to the first question proposed by Reviewer1 at the bottom of this page for more details.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING", "abstract": "This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.", "pdf": "/pdf/04457e6e3ea211450caf6a06cf0981744ba33849.pdf", "TL;DR": "a new model for extracting an interpretable sentence embedding by introducing self-attention and matrix representation.", "paperhash": "lin|a_structured_selfattentive_sentence_embedding", "authors": ["Zhouhan Lin", "Minwei Feng", "Cicero Nogueira dos Santos", "Mo Yu", "Bing Xiang", "Bowen Zhou", "Yoshua Bengio"], "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"], "conflicts": ["us.ibm.com", "iro.umontreal.ca", "umontreal.ca"], "authorids": ["lin.zhouhan@gmail.com", "mfeng@us.ibm.com", "cicerons@us.ibm.com", "yum@us.ibm.com", "bingxia@us.ibm.com", "zhou@us.ibm.com", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287622953, "id": "ICLR.cc/2017/conference/-/paper319/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJC_jUqxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper319/reviewers", "ICLR.cc/2017/conference/paper319/areachairs"], "cdate": 1485287622953}}}, {"tddate": null, "tmdate": 1482010638892, "tcdate": 1482010602371, "number": 3, "id": "H1zxxEXEg", "invitation": "ICLR.cc/2017/conference/-/paper319/official/review", "forum": "BJC_jUqxe", "replyto": "BJC_jUqxe", "signatures": ["ICLR.cc/2017/conference/paper319/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper319/AnonReviewer2"], "content": {"title": "Interesting idea but need additional work to be convincing", "rating": "6: Marginally above acceptance threshold", "review": "I like the idea in this paper that use not just one but multiple attentional vectors to extract multiple representations for a sentence. The authors have demonstrated consistent gains across three different tasks Age, Yelp, & SNLI. However, I'd like to see more analysis on the 2D representations (as concerned by another reviewer) to be convinced. Specifically, r=30 seems to be a pretty large value when applying to short sentences like tweets or those in the SNLI dataset. I'd like to see the effect of varying r from small to large value. With large r value, I suspect your models might have an advantage in having a much larger number of parameters (specifically in the supervised components) compare to other models. To make it transparent, the model sizes should be reported. I'd also like to see performances on the dev sets or learning curves.\n\nIn the conclusion, the authors remark that \"attention mechanism reliefs the burden of LSTM\". If the 2D representations are effective in that aspect, I'd expect that the authors might be able to train with a smaller LSTM. Testing the effect of LSTM dimension vs $r$ will be helpful.\n\nLastly, there is a problem in the presentation of the paper in which there is no training objective defined. Readers have to read until the experimental sections to guess that the authors perform supervised learning and back-prop through the self-attention mechanism as well as the LSTM.\n\n* Minor comments:\nTypos: netowkrs, toghter, performd\nMissing year for the citation of (Margarit & Subramaniam)\nIn figure 3, attention plotswith and without penalization look similar.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING", "abstract": "This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.", "pdf": "/pdf/04457e6e3ea211450caf6a06cf0981744ba33849.pdf", "TL;DR": "a new model for extracting an interpretable sentence embedding by introducing self-attention and matrix representation.", "paperhash": "lin|a_structured_selfattentive_sentence_embedding", "authors": ["Zhouhan Lin", "Minwei Feng", "Cicero Nogueira dos Santos", "Mo Yu", "Bing Xiang", "Bowen Zhou", "Yoshua Bengio"], "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"], "conflicts": ["us.ibm.com", "iro.umontreal.ca", "umontreal.ca"], "authorids": ["lin.zhouhan@gmail.com", "mfeng@us.ibm.com", "cicerons@us.ibm.com", "yum@us.ibm.com", "bingxia@us.ibm.com", "zhou@us.ibm.com", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512624558, "id": "ICLR.cc/2017/conference/-/paper319/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper319/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper319/AnonReviewer3", "ICLR.cc/2017/conference/paper319/AnonReviewer1", "ICLR.cc/2017/conference/paper319/AnonReviewer2"], "reply": {"forum": "BJC_jUqxe", "replyto": "BJC_jUqxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper319/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper319/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512624558}}}, {"tddate": null, "tmdate": 1481906125117, "tcdate": 1481906125117, "number": 2, "id": "ByHCv9b4e", "invitation": "ICLR.cc/2017/conference/-/paper319/official/review", "forum": "BJC_jUqxe", "replyto": "BJC_jUqxe", "signatures": ["ICLR.cc/2017/conference/paper319/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper319/AnonReviewer1"], "content": {"title": "Interesting embedding method, lacking in analysis of 2d structure", "rating": "5: Marginally below acceptance threshold", "review": "This paper proposes a method for representing sentences as a 2d matrix by utilizing a self-attentive mechanism on the hidden states of a bi-directional LSTM encoder. This work differs from prior work mainly in the 2d structure of embedding, which the authors use to produce heat-map visualizations of input sentences and to generate good performance on several downstream tasks.\n\nThere is a substantial amount of prior work which the authors do not appropriately address, some of which is listed in previous comments. The main novelty of this work is in the 2d structure of embeddings, and as such, I would have liked to see this structure investigated in much more depth. Specifically, a couple important relevant experiments would have been:\n\n* How do the performance and visualizations change as the number of attention vectors (r) varies?\n* For a fixed parameter budget, how important is using multiple attention vectors versus, say, using a larger hidden state or embedding size?\n\nI would recommend changing some of the presentation in the penalization term section. Specifically, the statement that \"the best way to evaluate the diversity is definitely the Kullback Leibler divergence between any 2 of the summation weight vectors\" runs somewhat counter to the authors' comments about this topic below.\n\nIn Fig. (2), I did not find the visualizations to provide particularly compelling evidence that the multiple attention vectors were doing much of interest beyond a single attention vector, even with penalization. To me this seems like a necessary component to support the main claims of this paper.\n\nOverall, while I found the architecture interesting, I am not convinced that the model's main innovation -- the 2d structure of the embedding matrix -- is actually doing anything important or meaningful beyond what is being accomplished by similar attentive embedding models already present in the literature. Further experiments demonstrating this effect would be necessary for me to give this paper my full endorsement.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING", "abstract": "This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.", "pdf": "/pdf/04457e6e3ea211450caf6a06cf0981744ba33849.pdf", "TL;DR": "a new model for extracting an interpretable sentence embedding by introducing self-attention and matrix representation.", "paperhash": "lin|a_structured_selfattentive_sentence_embedding", "authors": ["Zhouhan Lin", "Minwei Feng", "Cicero Nogueira dos Santos", "Mo Yu", "Bing Xiang", "Bowen Zhou", "Yoshua Bengio"], "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"], "conflicts": ["us.ibm.com", "iro.umontreal.ca", "umontreal.ca"], "authorids": ["lin.zhouhan@gmail.com", "mfeng@us.ibm.com", "cicerons@us.ibm.com", "yum@us.ibm.com", "bingxia@us.ibm.com", "zhou@us.ibm.com", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512624558, "id": "ICLR.cc/2017/conference/-/paper319/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper319/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper319/AnonReviewer3", "ICLR.cc/2017/conference/paper319/AnonReviewer1", "ICLR.cc/2017/conference/paper319/AnonReviewer2"], "reply": {"forum": "BJC_jUqxe", "replyto": "BJC_jUqxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper319/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper319/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512624558}}}, {"tddate": null, "tmdate": 1481842498855, "tcdate": 1481842372711, "number": 5, "id": "Sypa05lNx", "invitation": "ICLR.cc/2017/conference/-/paper319/public/comment", "forum": "BJC_jUqxe", "replyto": "BJC_jUqxe", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "A couple comments about framing and related work.", "comment": "The paper essentially constructs a \"matrix\" sentence encoding where each column is a weighted sum of biLSTM states and the weights are determined using attention.\n\nPros:\n-The use of a regularizer to force different sentence vectors to be different is interesting and not present in previous literature.\n\n\nCons:\n\n1. Lack of discussion of related work. The idea of using \"self\"-attention i.e. within a sentence instead of across sentences is not novel, but the authors do not discuss any related work. For example,\n\na. Ling et al. 2015, Not All Contexts Are Created Equal: Better Word Representations with Variable Attention\nb. Cheng et al. 2016, Long Short-Term Memory-Networks for Machine Reading.\nc. Parikh et al. 2016, A Decomposable Attention Model for Natural Language Inference.\nd. Liu et al. 2016, Learning natural language inference using bidirectional lstm model and inner-attention\n\nFrom my understanding, (d) is essentially just a special case of what the authors do (when you run attention only once and produce a vector instead of a matrix).\n\nSee also Section 3.3 in the following paper (its on arxiv so it probably was developed concurrently). Again the authors here basically just run attention once (instead of multiple times) as the authors do.\n\nLi et al. 2016, Dataset and neural recurrent sequence labeling model for open-domain factoid question answering.\n\n\n(2) Eliminating discussion of \"unsupervised learning\". As the reviewer below mentioned, it is not clear to me why the authors talk so much about unsupervised learning especially in the related work section. The authors' approach is not unsupervised in any way as far as I can tell?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING", "abstract": "This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.", "pdf": "/pdf/04457e6e3ea211450caf6a06cf0981744ba33849.pdf", "TL;DR": "a new model for extracting an interpretable sentence embedding by introducing self-attention and matrix representation.", "paperhash": "lin|a_structured_selfattentive_sentence_embedding", "authors": ["Zhouhan Lin", "Minwei Feng", "Cicero Nogueira dos Santos", "Mo Yu", "Bing Xiang", "Bowen Zhou", "Yoshua Bengio"], "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"], "conflicts": ["us.ibm.com", "iro.umontreal.ca", "umontreal.ca"], "authorids": ["lin.zhouhan@gmail.com", "mfeng@us.ibm.com", "cicerons@us.ibm.com", "yum@us.ibm.com", "bingxia@us.ibm.com", "zhou@us.ibm.com", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287622953, "id": "ICLR.cc/2017/conference/-/paper319/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJC_jUqxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper319/reviewers", "ICLR.cc/2017/conference/paper319/areachairs"], "cdate": 1485287622953}}}, {"tddate": null, "tmdate": 1481479694500, "tcdate": 1481479694489, "number": 1, "id": "H18MIfimg", "invitation": "ICLR.cc/2017/conference/-/paper319/official/review", "forum": "BJC_jUqxe", "replyto": "BJC_jUqxe", "signatures": ["ICLR.cc/2017/conference/paper319/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper319/AnonReviewer3"], "content": {"title": "Strong, but some framing issues", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper introduces a sentence encoding model (for use within larger text understanding models) that can extract a matrix-valued sentence representation by way of within-sentence attention. The new model lends itself to (slightly) more informative visualizations than could be gotten otherwise, and beats reasonable baselines on three datasets.\n\nThe paper is reasonably clear, I see no major technical issues, and the proposed model is novel and effective. It could plausibly be relevant to sequence modeling tasks beyond NLP. I recommend acceptance.\n\nThere is one fairly serious writing issue that I'd like to see fixed, though: The abstract, introduction, and related work sections are all heavily skewed towards unsupervised learning. The paper doesn't appear to be doing unsupervised learning, and the ideas are no more nor less suited to unsupervised learning than any other mainstream ideas in the sentence encoding literature.\n\nDetails:\n- You should be clearer about how you expect these embeddings to be used, since that will be of certain interest to anyone attempting to use the results of this work. In particular, how you should convert the matrix representation into a vector for downstream tasks that require one. Some of the content of your reply to my comment could be reasonably added to the paper.\n- A graphical representation of the structure of the model would be helpful.\n- The LSTMN (Cheng et al., EMNLP '16) is similar enough to this work that an explicit comparison would be helpful. Again, incorporating your reply to my comment into the paper would be more than adequate. \n- Jiwei Li et al. (Visualizing and Understanding Neural Models in NLP, NAACL '15) present an alternative way of visualizing the influence of words on sentence encodings without using cross-sentence attention. A brief explicit comparison would be nice here.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING", "abstract": "This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.", "pdf": "/pdf/04457e6e3ea211450caf6a06cf0981744ba33849.pdf", "TL;DR": "a new model for extracting an interpretable sentence embedding by introducing self-attention and matrix representation.", "paperhash": "lin|a_structured_selfattentive_sentence_embedding", "authors": ["Zhouhan Lin", "Minwei Feng", "Cicero Nogueira dos Santos", "Mo Yu", "Bing Xiang", "Bowen Zhou", "Yoshua Bengio"], "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"], "conflicts": ["us.ibm.com", "iro.umontreal.ca", "umontreal.ca"], "authorids": ["lin.zhouhan@gmail.com", "mfeng@us.ibm.com", "cicerons@us.ibm.com", "yum@us.ibm.com", "bingxia@us.ibm.com", "zhou@us.ibm.com", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512624558, "id": "ICLR.cc/2017/conference/-/paper319/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper319/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper319/AnonReviewer3", "ICLR.cc/2017/conference/paper319/AnonReviewer1", "ICLR.cc/2017/conference/paper319/AnonReviewer2"], "reply": {"forum": "BJC_jUqxe", "replyto": "BJC_jUqxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper319/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper319/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512624558}}}, {"tddate": null, "tmdate": 1481194190332, "tcdate": 1481193716738, "number": 3, "id": "S1agFhIQe", "invitation": "ICLR.cc/2017/conference/-/paper319/public/comment", "forum": "BJC_jUqxe", "replyto": "BJBFQLJ7g", "signatures": ["~Zhouhan_Lin1"], "readers": ["everyone"], "writers": ["~Zhouhan_Lin1"], "content": {"title": "Response regarding the KL divergence", "comment": "We tried but we haven't got it work correctly. There includes a p(i) / q(i) operation in the KL divergence expression, and since we are maximizing (instead of minimizing) the KL divergence, We are optimizing the annotation matrix A to have a lot of sufficiently small or even zero values at different softmax output units (which maximizes the \"mismatch\" between two distributions). Thus it makes the training not very stable. Maybe there is something more to play with to make the KL term stable. So that's why we moved on to devise another more stable and computationally cheaper penalty term instead.\n\nOn another note, if we use the KL divergence, then a KL divergence should be computed between every pair of distributions, i.e., all possible pairs of rows in the A (annotation) matrix.  For each pair we need to compute \\Sigma{i}{p(i)log{\\frac{p(i)}{q(i)}}}, which takes 3n multiplications. (Inheriting notations, n here means the number of tokens in the sentence.) To compare KL for all possible pairs, we thus need 3n*r^2 multiplications. Plus the Frobenius norm you will still need, it sums up to (3n+1)*r^2 multiplications. However, it only takes (n+1)*r^2 multiplications for the penalty we currently use. It\u2019s like one-third of the computation, no matter how large the \u201cr\u201d is. Since there are specific GEMM for matrix multiplications in GPU, the run time difference will be much more than 3 times. \n\nThere is another feature that KL doesn\u2019t provide but we want, which is, we want each individual row to focus on a single aspect of semantics, so we want the attention annotation to be more focused. That is enforced by forcing the diagonal of AA^T to be close to 1, but with KL penalty we can\u2019t encourage that."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING", "abstract": "This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.", "pdf": "/pdf/04457e6e3ea211450caf6a06cf0981744ba33849.pdf", "TL;DR": "a new model for extracting an interpretable sentence embedding by introducing self-attention and matrix representation.", "paperhash": "lin|a_structured_selfattentive_sentence_embedding", "authors": ["Zhouhan Lin", "Minwei Feng", "Cicero Nogueira dos Santos", "Mo Yu", "Bing Xiang", "Bowen Zhou", "Yoshua Bengio"], "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"], "conflicts": ["us.ibm.com", "iro.umontreal.ca", "umontreal.ca"], "authorids": ["lin.zhouhan@gmail.com", "mfeng@us.ibm.com", "cicerons@us.ibm.com", "yum@us.ibm.com", "bingxia@us.ibm.com", "zhou@us.ibm.com", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287622953, "id": "ICLR.cc/2017/conference/-/paper319/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJC_jUqxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper319/reviewers", "ICLR.cc/2017/conference/paper319/areachairs"], "cdate": 1485287622953}}}, {"tddate": null, "tmdate": 1481193893892, "tcdate": 1481193848836, "number": 4, "id": "BkZYYnUXx", "invitation": "ICLR.cc/2017/conference/-/paper319/public/comment", "forum": "BJC_jUqxe", "replyto": "Hkep_aGml", "signatures": ["~Zhouhan_Lin1"], "readers": ["everyone"], "writers": ["~Zhouhan_Lin1"], "content": {"title": "Response regarding Parikh\u2019s work", "comment": "The \u201cintra-sentence\u201d level attention that was used in Parikh\u2019s paper is the same to Cheng et al. 2016 (arXiv:1601.06733). We do receive questions about the relation between our work and Cheng\u2019s, and we wrote a response here in this page, which you may be interested to have a look."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING", "abstract": "This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.", "pdf": "/pdf/04457e6e3ea211450caf6a06cf0981744ba33849.pdf", "TL;DR": "a new model for extracting an interpretable sentence embedding by introducing self-attention and matrix representation.", "paperhash": "lin|a_structured_selfattentive_sentence_embedding", "authors": ["Zhouhan Lin", "Minwei Feng", "Cicero Nogueira dos Santos", "Mo Yu", "Bing Xiang", "Bowen Zhou", "Yoshua Bengio"], "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"], "conflicts": ["us.ibm.com", "iro.umontreal.ca", "umontreal.ca"], "authorids": ["lin.zhouhan@gmail.com", "mfeng@us.ibm.com", "cicerons@us.ibm.com", "yum@us.ibm.com", "bingxia@us.ibm.com", "zhou@us.ibm.com", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287622953, "id": "ICLR.cc/2017/conference/-/paper319/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJC_jUqxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper319/reviewers", "ICLR.cc/2017/conference/paper319/areachairs"], "cdate": 1485287622953}}}, {"tddate": null, "tmdate": 1480935608106, "tcdate": 1480935608099, "number": 2, "id": "Hkep_aGml", "invitation": "ICLR.cc/2017/conference/-/paper319/public/comment", "forum": "BJC_jUqxe", "replyto": "BJC_jUqxe", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Relation to Parikh et al. (2016)", "comment": "I'm curious how the idea of \"self-attention\" compares to the idea of \"intra-sentence attention\" introduced by Parikh et al. (2016). Does \"self-attention\" as implemented here simpy mean softmax-pooling to aggregate over LSTM hidden states?\n\nReference:\nA. Parikh, O. Tackstrom, D. Das J. Uszkoreit. A Decomposable Attention Model for Natural Language Inference. EMNLP 2016. http://aclweb.org/anthology/D/D16/D16-1244.pdf"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING", "abstract": "This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.", "pdf": "/pdf/04457e6e3ea211450caf6a06cf0981744ba33849.pdf", "TL;DR": "a new model for extracting an interpretable sentence embedding by introducing self-attention and matrix representation.", "paperhash": "lin|a_structured_selfattentive_sentence_embedding", "authors": ["Zhouhan Lin", "Minwei Feng", "Cicero Nogueira dos Santos", "Mo Yu", "Bing Xiang", "Bowen Zhou", "Yoshua Bengio"], "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"], "conflicts": ["us.ibm.com", "iro.umontreal.ca", "umontreal.ca"], "authorids": ["lin.zhouhan@gmail.com", "mfeng@us.ibm.com", "cicerons@us.ibm.com", "yum@us.ibm.com", "bingxia@us.ibm.com", "zhou@us.ibm.com", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287622953, "id": "ICLR.cc/2017/conference/-/paper319/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJC_jUqxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper319/reviewers", "ICLR.cc/2017/conference/paper319/areachairs"], "cdate": 1485287622953}}}, {"tddate": null, "tmdate": 1480708988911, "tcdate": 1480708988907, "number": 2, "id": "BJBFQLJ7g", "invitation": "ICLR.cc/2017/conference/-/paper319/pre-review/question", "forum": "BJC_jUqxe", "replyto": "BJC_jUqxe", "signatures": ["ICLR.cc/2017/conference/paper319/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper319/AnonReviewer1"], "content": {"title": "Penalization term", "question": "You mention that using KL divergence as a diversity regularizer is too computationally expensive, but presumably this is not true if r (the number of rows) is sufficiently small. Have you tried decreasing r to a small enough value so that you can utilize the KL penalty? If so, does it out perform the Frobenius norm penalty with the same (or larger) value of r?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING", "abstract": "This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.", "pdf": "/pdf/04457e6e3ea211450caf6a06cf0981744ba33849.pdf", "TL;DR": "a new model for extracting an interpretable sentence embedding by introducing self-attention and matrix representation.", "paperhash": "lin|a_structured_selfattentive_sentence_embedding", "authors": ["Zhouhan Lin", "Minwei Feng", "Cicero Nogueira dos Santos", "Mo Yu", "Bing Xiang", "Bowen Zhou", "Yoshua Bengio"], "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"], "conflicts": ["us.ibm.com", "iro.umontreal.ca", "umontreal.ca"], "authorids": ["lin.zhouhan@gmail.com", "mfeng@us.ibm.com", "cicerons@us.ibm.com", "yum@us.ibm.com", "bingxia@us.ibm.com", "zhou@us.ibm.com", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959342331, "id": "ICLR.cc/2017/conference/-/paper319/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper319/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper319/AnonReviewer3", "ICLR.cc/2017/conference/paper319/AnonReviewer1"], "reply": {"forum": "BJC_jUqxe", "replyto": "BJC_jUqxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper319/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper319/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959342331}}}, {"tddate": null, "tmdate": 1479863316879, "tcdate": 1479863220211, "number": 1, "id": "Hy33iPMMe", "invitation": "ICLR.cc/2017/conference/-/paper319/public/comment", "forum": "BJC_jUqxe", "replyto": "Hko9vSRZl", "signatures": ["~Zhouhan_Lin1"], "readers": ["everyone"], "writers": ["~Zhouhan_Lin1"], "content": {"title": "response to the two questions", "comment": "How do you use the sentence representation matrix M as the input to a classifier? Do you simply concatenate the rows into one long vector, and feed that into a softmax, or is there any kind of weight-sharing between the classifier parameters for different rows?\n\n- There can be weight sharing or not. The simplist way is to concatenate all the rows together and then feed them into an MLP as a regular vector, which is what we did in those results shown in the paper. Otherwise, we can also take advantage of the 2D structure, multiplying a same weight matrix for each row, and producing a 2D hidden representation. We actually tried that, and it turns out to have yielded similar performance (but with much fewer parameters of course). We didn't mention that in the paper majorly because we want this paper to foucs on the sentence embedding itself.\n\nOn another note, how do you see this work relating to the LSTMN? Your work has clear differences with theirs, but they share the idea of attending over a sequence without any outside input to the attention process, and they both work well on similar tasks.\n\n- The attention of LSTMN is sort of an \"online updating\" attention, which is a more fine-grained attention that targets at discovering lexical correlations between a certain word and its previous words. It achieves extra language understanding by discovering lexical correlations. On the contrary, the attention we are using happens after the LSTM processes the whole sentence, so it is less fine-grained, focuses directly on the semantics that makes sense for discriminating the targets. It is less focused on relations between words, but more on the semantics of the whole sentence that each word contributes to.\n\nBecause of the aforementioned reason, we can also say that they are different levels of attention. Moreover, LSTMN still needs a maxpooling step to compress all the contents in a tape into a fixed length vector afterward. Thus they can be used together: LSTMN but with our attention substituting the maxpooling part. \n\nAnother difference that resulted from this granularity is the scalability. LSTMN computes an annotation vector over all of its previous words each time when the LSTMN computes its next step, and that is O(n^2) complexity. So when the sequence to be processed becomes very long (like the long reviews in the author profiling dataset), it becomes more and more expensive to compute its next time step. It makes it hard for this model to scale up to longer contents, like long reviews, paragraphs, etc. In that respect, our model is only O(n) complexity, which scales up better. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING", "abstract": "This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.", "pdf": "/pdf/04457e6e3ea211450caf6a06cf0981744ba33849.pdf", "TL;DR": "a new model for extracting an interpretable sentence embedding by introducing self-attention and matrix representation.", "paperhash": "lin|a_structured_selfattentive_sentence_embedding", "authors": ["Zhouhan Lin", "Minwei Feng", "Cicero Nogueira dos Santos", "Mo Yu", "Bing Xiang", "Bowen Zhou", "Yoshua Bengio"], "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"], "conflicts": ["us.ibm.com", "iro.umontreal.ca", "umontreal.ca"], "authorids": ["lin.zhouhan@gmail.com", "mfeng@us.ibm.com", "cicerons@us.ibm.com", "yum@us.ibm.com", "bingxia@us.ibm.com", "zhou@us.ibm.com", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287622953, "id": "ICLR.cc/2017/conference/-/paper319/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJC_jUqxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper319/reviewers", "ICLR.cc/2017/conference/paper319/areachairs"], "cdate": 1485287622953}}}, {"tddate": null, "tmdate": 1479591826664, "tcdate": 1479591826658, "number": 1, "id": "Hko9vSRZl", "invitation": "ICLR.cc/2017/conference/-/paper319/pre-review/question", "forum": "BJC_jUqxe", "replyto": "BJC_jUqxe", "signatures": ["ICLR.cc/2017/conference/paper319/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper319/AnonReviewer3"], "content": {"title": "Two questions", "question": "How do you use the sentence representation matrix M as the input to a classifier? Do you simply concatenate the rows into one long vector, and feed that into a softmax, or is there any kind of weight-sharing between the classifier parameters for different rows?\n\nOn another note, how do you see this work relating to the LSTMN? Your work has clear differences with theirs, but they share the idea of attending over a sequence without any outside input to the attention process, and they both work well on similar tasks.\n\nPaper (EMNLP '16): https://arxiv.org/pdf/1601.06733v7.pdf"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING", "abstract": "This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.", "pdf": "/pdf/04457e6e3ea211450caf6a06cf0981744ba33849.pdf", "TL;DR": "a new model for extracting an interpretable sentence embedding by introducing self-attention and matrix representation.", "paperhash": "lin|a_structured_selfattentive_sentence_embedding", "authors": ["Zhouhan Lin", "Minwei Feng", "Cicero Nogueira dos Santos", "Mo Yu", "Bing Xiang", "Bowen Zhou", "Yoshua Bengio"], "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"], "conflicts": ["us.ibm.com", "iro.umontreal.ca", "umontreal.ca"], "authorids": ["lin.zhouhan@gmail.com", "mfeng@us.ibm.com", "cicerons@us.ibm.com", "yum@us.ibm.com", "bingxia@us.ibm.com", "zhou@us.ibm.com", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959342331, "id": "ICLR.cc/2017/conference/-/paper319/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper319/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper319/AnonReviewer3", "ICLR.cc/2017/conference/paper319/AnonReviewer1"], "reply": {"forum": "BJC_jUqxe", "replyto": "BJC_jUqxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper319/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper319/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959342331}}}], "count": 16}