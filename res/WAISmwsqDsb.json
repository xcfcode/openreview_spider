{"notes": [{"id": "WAISmwsqDsb", "original": "PpU5Mhlj73H", "number": 509, "cdate": 1601308063528, "ddate": null, "tcdate": 1601308063528, "tmdate": 1615923372297, "tddate": null, "forum": "WAISmwsqDsb", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "DINO: A Conditional Energy-Based GAN for Domain Translation", "authorids": ["~Konstantinos_Vougioukas1", "~Stavros_Petridis1", "~Maja_Pantic1"], "authors": ["Konstantinos Vougioukas", "Stavros Petridis", "Maja Pantic"], "keywords": ["Generative Modelling", "Domain Translation", "Conditional GANs", "Energy-Based GANs"], "abstract": "Domain translation is the process of transforming data from one domain to another while preserving the common semantics. Some of the most popular domain translation systems are based on conditional generative adversarial networks, which use source domain data to drive the generator and as an input to the discriminator. However, this approach does not enforce the preservation of shared semantics since the conditional input can often be ignored by the discriminator. We propose an alternative method for conditioning and present a new framework, where two networks are simultaneously trained, in a supervised manner, to perform domain translation in opposite directions. Our method is not only better at capturing the shared information between two domains but is more generic and can be applied to a broader range of problems. The proposed framework performs well even in challenging cross-modal translations, such as video-driven speech reconstruction, for which other systems struggle to maintain correspondence.", "one-sentence_summary": "A framework  for domain translation which uses a novel mechanism for conditioning energy-based GANs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vougioukas|dino_a_conditional_energybased_gan_for_domain_translation", "supplementary_material": "/attachment/edea68cce0d28da12184e7f35b1cf7eed6e014a2.zip", "pdf": "/pdf/1770fc1a0716d2fde0cefb49d59d540311331789.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvougioukas2021dino,\ntitle={{\\{}DINO{\\}}: A Conditional Energy-Based {\\{}GAN{\\}} for Domain Translation},\nauthor={Konstantinos Vougioukas and Stavros Petridis and Maja Pantic},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=WAISmwsqDsb}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "6ScLFjYPABw", "original": null, "number": 1, "cdate": 1610040401074, "ddate": null, "tcdate": 1610040401074, "tmdate": 1610473997031, "tddate": null, "forum": "WAISmwsqDsb", "replyto": "WAISmwsqDsb", "invitation": "ICLR.cc/2021/Conference/Paper509/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "This paper presents a novel method for general-purpose supervised domain transfer that trains both generator and discriminator to compete in a minimax game in order to reconstruct data. This setup is meant to address a common issue in conditional GAN setups: they often ignore conditioning information. Results are positive and span two very different tasks: image-to-image translation and silent-video-to-speech reconstruction. Overall reviewers were quite positive about this paper: they found the method to be novel and well-motivated, and after rebuttal, found experimental results to be sufficiently convincing. Several concerns were brought up: (a) lack of emphasis that the approach is in fact supervised, (b) need for comparisons with stronger or task-specific baselines, (c) lack of description of experimental details for reproducibility, and (d) lack of discussion of ethical implications. All of these concerns were satisfactorily addressed by authors in rebuttal and reviewers unanimously vote for acceptance. I agree, and recommend this paper be accepted. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DINO: A Conditional Energy-Based GAN for Domain Translation", "authorids": ["~Konstantinos_Vougioukas1", "~Stavros_Petridis1", "~Maja_Pantic1"], "authors": ["Konstantinos Vougioukas", "Stavros Petridis", "Maja Pantic"], "keywords": ["Generative Modelling", "Domain Translation", "Conditional GANs", "Energy-Based GANs"], "abstract": "Domain translation is the process of transforming data from one domain to another while preserving the common semantics. Some of the most popular domain translation systems are based on conditional generative adversarial networks, which use source domain data to drive the generator and as an input to the discriminator. However, this approach does not enforce the preservation of shared semantics since the conditional input can often be ignored by the discriminator. We propose an alternative method for conditioning and present a new framework, where two networks are simultaneously trained, in a supervised manner, to perform domain translation in opposite directions. Our method is not only better at capturing the shared information between two domains but is more generic and can be applied to a broader range of problems. The proposed framework performs well even in challenging cross-modal translations, such as video-driven speech reconstruction, for which other systems struggle to maintain correspondence.", "one-sentence_summary": "A framework  for domain translation which uses a novel mechanism for conditioning energy-based GANs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vougioukas|dino_a_conditional_energybased_gan_for_domain_translation", "supplementary_material": "/attachment/edea68cce0d28da12184e7f35b1cf7eed6e014a2.zip", "pdf": "/pdf/1770fc1a0716d2fde0cefb49d59d540311331789.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvougioukas2021dino,\ntitle={{\\{}DINO{\\}}: A Conditional Energy-Based {\\{}GAN{\\}} for Domain Translation},\nauthor={Konstantinos Vougioukas and Stavros Petridis and Maja Pantic},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=WAISmwsqDsb}\n}"}, "tags": [], "invitation": {"reply": {"forum": "WAISmwsqDsb", "replyto": "WAISmwsqDsb", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040401060, "tmdate": 1610473997012, "id": "ICLR.cc/2021/Conference/Paper509/-/Decision"}}}, {"id": "LG9-ry7s8se", "original": null, "number": 1, "cdate": 1603839512214, "ddate": null, "tcdate": 1603839512214, "tmdate": 1607355207658, "tddate": null, "forum": "WAISmwsqDsb", "replyto": "WAISmwsqDsb", "invitation": "ICLR.cc/2021/Conference/Paper509/-/Official_Review", "content": {"title": "Review", "review": "This paper presents a method for performing cross-domain or cross-modality translation models using a GAN-flavored framework where two models are trained to translate in both directions simultaneously.\n\n[As a caveat: I am not well-versed in this area of the literature]\n\nThe paper is well-written for the most part and the experimental results are promising. My main concern are the ethical implications of some of the lip-reading experiments which go unaddressed.\n\n**Pros**\n- Clear presentation (mostly, see remarks for some exceptions)\n- Good results as far as I can tell (although it is hard to interpret what all the various metrics mean, but it does seem like DINO is consistently better than the alternatives along most metrics)\n- Experiments are not limited to image to image translation but also to \"cross-modality translation\" (image to text)\n\n**Cons**\n- If I understand correctly, the video->speech task is essentially a lip-reading task. State-of-the-art lip reading raises a number of privacy related concerns, and I think the potential impact of this research should at least be acknowledged in the paper.\n\n\n**Remarks**\n- The proposed method bears some conceptual similarity with recent work in unsupervised machine translation (see eg. Artetxe et al. https://arxiv.org/abs/1710.11041, Lample et al. https://arxiv.org/abs/1711.00043, also Lample et al. 2019 https://openreview.net/pdf?id=H1g2NhC5KQ which is particularly relevant as it tackles \"style transfer\" for text). These similarities are worth mentioning in the paper.\n- I found the use of the term \"discriminator\" confusing, especially in the beginning in the paper. It makes sense in the usual GAN setup where the discriminator is an actual discriminative model, but it seems inappropriate in this case where the \"discriminator\" is a generative model.\n- Eq. 6 (the main objective) is incredibly confusing. First, (and this relates to my last remark) the notation D_disc, D_gen, G_disc, G_gen unnecessarily confounding. Consider using \"s -> t\" and \"t->s\" for source to target and vice-versa instead of G and D. Also, perhaps color-coding Eq.6 would make it easier to parse (or maybe just separate the different terms a bit more).", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper509/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper509/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DINO: A Conditional Energy-Based GAN for Domain Translation", "authorids": ["~Konstantinos_Vougioukas1", "~Stavros_Petridis1", "~Maja_Pantic1"], "authors": ["Konstantinos Vougioukas", "Stavros Petridis", "Maja Pantic"], "keywords": ["Generative Modelling", "Domain Translation", "Conditional GANs", "Energy-Based GANs"], "abstract": "Domain translation is the process of transforming data from one domain to another while preserving the common semantics. Some of the most popular domain translation systems are based on conditional generative adversarial networks, which use source domain data to drive the generator and as an input to the discriminator. However, this approach does not enforce the preservation of shared semantics since the conditional input can often be ignored by the discriminator. We propose an alternative method for conditioning and present a new framework, where two networks are simultaneously trained, in a supervised manner, to perform domain translation in opposite directions. Our method is not only better at capturing the shared information between two domains but is more generic and can be applied to a broader range of problems. The proposed framework performs well even in challenging cross-modal translations, such as video-driven speech reconstruction, for which other systems struggle to maintain correspondence.", "one-sentence_summary": "A framework  for domain translation which uses a novel mechanism for conditioning energy-based GANs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vougioukas|dino_a_conditional_energybased_gan_for_domain_translation", "supplementary_material": "/attachment/edea68cce0d28da12184e7f35b1cf7eed6e014a2.zip", "pdf": "/pdf/1770fc1a0716d2fde0cefb49d59d540311331789.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvougioukas2021dino,\ntitle={{\\{}DINO{\\}}: A Conditional Energy-Based {\\{}GAN{\\}} for Domain Translation},\nauthor={Konstantinos Vougioukas and Stavros Petridis and Maja Pantic},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=WAISmwsqDsb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "WAISmwsqDsb", "replyto": "WAISmwsqDsb", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper509/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538141536, "tmdate": 1606915772911, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper509/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper509/-/Official_Review"}}}, {"id": "VLFRf59MJvv", "original": null, "number": 12, "cdate": 1606251509056, "ddate": null, "tcdate": 1606251509056, "tmdate": 1606251509056, "tddate": null, "forum": "WAISmwsqDsb", "replyto": "ThTugSRW9n5", "invitation": "ICLR.cc/2021/Conference/Paper509/-/Official_Comment", "content": {"title": "Thank you for the response", "comment": "I thank the author for the response.\n\nI appreciate that the addition of an ethics statement to address my main concern. The additional discussion with R3 regarding relevant baselines is also comforting.\n\nWith the caveat that I am not very familiar with this general area of the literature, I maintain my recommendation that this paper should be accepted"}, "signatures": ["ICLR.cc/2021/Conference/Paper509/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper509/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DINO: A Conditional Energy-Based GAN for Domain Translation", "authorids": ["~Konstantinos_Vougioukas1", "~Stavros_Petridis1", "~Maja_Pantic1"], "authors": ["Konstantinos Vougioukas", "Stavros Petridis", "Maja Pantic"], "keywords": ["Generative Modelling", "Domain Translation", "Conditional GANs", "Energy-Based GANs"], "abstract": "Domain translation is the process of transforming data from one domain to another while preserving the common semantics. Some of the most popular domain translation systems are based on conditional generative adversarial networks, which use source domain data to drive the generator and as an input to the discriminator. However, this approach does not enforce the preservation of shared semantics since the conditional input can often be ignored by the discriminator. We propose an alternative method for conditioning and present a new framework, where two networks are simultaneously trained, in a supervised manner, to perform domain translation in opposite directions. Our method is not only better at capturing the shared information between two domains but is more generic and can be applied to a broader range of problems. The proposed framework performs well even in challenging cross-modal translations, such as video-driven speech reconstruction, for which other systems struggle to maintain correspondence.", "one-sentence_summary": "A framework  for domain translation which uses a novel mechanism for conditioning energy-based GANs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vougioukas|dino_a_conditional_energybased_gan_for_domain_translation", "supplementary_material": "/attachment/edea68cce0d28da12184e7f35b1cf7eed6e014a2.zip", "pdf": "/pdf/1770fc1a0716d2fde0cefb49d59d540311331789.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvougioukas2021dino,\ntitle={{\\{}DINO{\\}}: A Conditional Energy-Based {\\{}GAN{\\}} for Domain Translation},\nauthor={Konstantinos Vougioukas and Stavros Petridis and Maja Pantic},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=WAISmwsqDsb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "WAISmwsqDsb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper509/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper509/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper509/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper509/Authors|ICLR.cc/2021/Conference/Paper509/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper509/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870192, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper509/-/Official_Comment"}}}, {"id": "H_Mlpd34jJ", "original": null, "number": 2, "cdate": 1603889053135, "ddate": null, "tcdate": 1603889053135, "tmdate": 1606235936262, "tddate": null, "forum": "WAISmwsqDsb", "replyto": "WAISmwsqDsb", "invitation": "ICLR.cc/2021/Conference/Paper509/-/Official_Review", "content": {"title": "Seems reasonable, some clarity issues and not clear comparison is extensive (low confidence)", "review": "This paper proposes a conditional energy-based GAN technique for translation between data domains.\n\nFirst, let me preface this review by noting that this paper is far outside of my area of expertise. I have tried to do my best in reviewing it, but I'd appreciate any clarification of mistaken points from the authors.\n\nOverall, the idea itself seems reasonable: in conditional GAN-based models, instead of using a discriminator that explicitly tries to predict whether the generated output is true or fake, the discriminator tries to maximize the reconstruction score of true outputs, and minimize it for fake outputs.\n\nHowever, there were many questions I had based on just reading the paper. I'm not sure whether this is due to my lack of background knowledge in this field, or because the writing itself is unclear (perhaps a bit of both):\n\n1. First big question: from my reading it seems that this is a *supervised* model, in that it needs $(x,y)$ pairs to calculate the objective in Equation (2). Is this correct? It doesn't seem to be explicitly stated anywhere.\n\n2. Given this, I was not sure if the baselines in Table 1 and 2 actually represent the state-of-the-art in this domain. Pix2pix seems to be from 2017, which seems to be quite old given the huge progress this field has made in the past 3 years. BiCycleGAN, according to my understanding, is an unsupervised method, which presumably will do much worse than supervised methods.\n\n3. Are the datasets in 4.1 standard and used in the literature? If so, what are some recent papers that evaluate on these datasets? If not, why use these datasets instead of others? While I'm not very familiar with the field, I do know that image style transfer is a big thing, and surely there are other datasets that people have evaluated on previously.\n\n4. The description of Equation (1) was a bit hard to follow, as the role of the discriminator was not made explicit. The \"margin loss\" was also not explained concretely.\n\n5. I was not able to understand the description of $\\gamma$ in Equation (3), please elaborate if possible. The gain value $\\lambda$ was also not easy to follow.\n\n6. It was mentioned that MirrorGAN is the most similar method. While there was an explanation that DINO is simpler than MirrorGAN, it would be nice to explain the implications of this. Does this just mean that DINO is a bit easier to implement? Or does it mean that it is fundamentally applicable to a wider variety of tasks? Also, why is there no empirical comparison with MirrorGAN?", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper509/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper509/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DINO: A Conditional Energy-Based GAN for Domain Translation", "authorids": ["~Konstantinos_Vougioukas1", "~Stavros_Petridis1", "~Maja_Pantic1"], "authors": ["Konstantinos Vougioukas", "Stavros Petridis", "Maja Pantic"], "keywords": ["Generative Modelling", "Domain Translation", "Conditional GANs", "Energy-Based GANs"], "abstract": "Domain translation is the process of transforming data from one domain to another while preserving the common semantics. Some of the most popular domain translation systems are based on conditional generative adversarial networks, which use source domain data to drive the generator and as an input to the discriminator. However, this approach does not enforce the preservation of shared semantics since the conditional input can often be ignored by the discriminator. We propose an alternative method for conditioning and present a new framework, where two networks are simultaneously trained, in a supervised manner, to perform domain translation in opposite directions. Our method is not only better at capturing the shared information between two domains but is more generic and can be applied to a broader range of problems. The proposed framework performs well even in challenging cross-modal translations, such as video-driven speech reconstruction, for which other systems struggle to maintain correspondence.", "one-sentence_summary": "A framework  for domain translation which uses a novel mechanism for conditioning energy-based GANs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vougioukas|dino_a_conditional_energybased_gan_for_domain_translation", "supplementary_material": "/attachment/edea68cce0d28da12184e7f35b1cf7eed6e014a2.zip", "pdf": "/pdf/1770fc1a0716d2fde0cefb49d59d540311331789.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvougioukas2021dino,\ntitle={{\\{}DINO{\\}}: A Conditional Energy-Based {\\{}GAN{\\}} for Domain Translation},\nauthor={Konstantinos Vougioukas and Stavros Petridis and Maja Pantic},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=WAISmwsqDsb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "WAISmwsqDsb", "replyto": "WAISmwsqDsb", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper509/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538141536, "tmdate": 1606915772911, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper509/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper509/-/Official_Review"}}}, {"id": "K9o71u7cOTz", "original": null, "number": 11, "cdate": 1606235911036, "ddate": null, "tcdate": 1606235911036, "tmdate": 1606235911036, "tddate": null, "forum": "WAISmwsqDsb", "replyto": "Ij29Tg-7Gy4", "invitation": "ICLR.cc/2021/Conference/Paper509/-/Official_Comment", "content": {"title": "Thank you for the clarification and updated results", "comment": "Thank you, this addresses my major concern.\n\nI'd like to reiterate that I'm still not very confident in my assessment here, but based on the revision I think that the paper probably warrants acceptance and will raise my score accordingly."}, "signatures": ["ICLR.cc/2021/Conference/Paper509/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper509/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DINO: A Conditional Energy-Based GAN for Domain Translation", "authorids": ["~Konstantinos_Vougioukas1", "~Stavros_Petridis1", "~Maja_Pantic1"], "authors": ["Konstantinos Vougioukas", "Stavros Petridis", "Maja Pantic"], "keywords": ["Generative Modelling", "Domain Translation", "Conditional GANs", "Energy-Based GANs"], "abstract": "Domain translation is the process of transforming data from one domain to another while preserving the common semantics. Some of the most popular domain translation systems are based on conditional generative adversarial networks, which use source domain data to drive the generator and as an input to the discriminator. However, this approach does not enforce the preservation of shared semantics since the conditional input can often be ignored by the discriminator. We propose an alternative method for conditioning and present a new framework, where two networks are simultaneously trained, in a supervised manner, to perform domain translation in opposite directions. Our method is not only better at capturing the shared information between two domains but is more generic and can be applied to a broader range of problems. The proposed framework performs well even in challenging cross-modal translations, such as video-driven speech reconstruction, for which other systems struggle to maintain correspondence.", "one-sentence_summary": "A framework  for domain translation which uses a novel mechanism for conditioning energy-based GANs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vougioukas|dino_a_conditional_energybased_gan_for_domain_translation", "supplementary_material": "/attachment/edea68cce0d28da12184e7f35b1cf7eed6e014a2.zip", "pdf": "/pdf/1770fc1a0716d2fde0cefb49d59d540311331789.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvougioukas2021dino,\ntitle={{\\{}DINO{\\}}: A Conditional Energy-Based {\\{}GAN{\\}} for Domain Translation},\nauthor={Konstantinos Vougioukas and Stavros Petridis and Maja Pantic},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=WAISmwsqDsb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "WAISmwsqDsb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper509/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper509/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper509/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper509/Authors|ICLR.cc/2021/Conference/Paper509/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper509/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870192, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper509/-/Official_Comment"}}}, {"id": "Z_VytUEiy-L", "original": null, "number": 10, "cdate": 1606233274695, "ddate": null, "tcdate": 1606233274695, "tmdate": 1606233274695, "tddate": null, "forum": "WAISmwsqDsb", "replyto": "WAISmwsqDsb", "invitation": "ICLR.cc/2021/Conference/Paper509/-/Official_Comment", "content": {"title": "Summary of Changes", "comment": "We thank the reviewers for their feedback. We have compiled a comprehensive list of changes that we have made to the rebuttal revision (compared to the original submission) following the discussions:\n\n1. We explicitly state that the DINO method is supervised (i.e. requires paired data) in the abstract to avoid confusion.\n2. We have changed the names of our networks from generator and discriminator to forward and reverse translation networks as per R1's request and updated the notation in equations (Eq.3-7) to match.\n3. We have included the related works suggested by R1 in section 3.2 (paragraph 3).\n4. We have added a discussion about the SPADE method in Section 2.1 (paragraph 2) and also added SPADE to the list of methods that we compare against in the Cityscapes dataset (Table 2, Figure 12 in the appendix).\n5. We have added descriptions for the margin loss and added an additional equation (Eq.2) for clarification.\n6. We have added more details regarding hyperparameters used for training our model in Sections 4.1 (paragraph 4) and 4.2 (paragraph 3).\n7. We have included the source images in Fig. 10 of the Appendix so that the readers can see the type of data used to drive the translation."}, "signatures": ["ICLR.cc/2021/Conference/Paper509/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper509/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DINO: A Conditional Energy-Based GAN for Domain Translation", "authorids": ["~Konstantinos_Vougioukas1", "~Stavros_Petridis1", "~Maja_Pantic1"], "authors": ["Konstantinos Vougioukas", "Stavros Petridis", "Maja Pantic"], "keywords": ["Generative Modelling", "Domain Translation", "Conditional GANs", "Energy-Based GANs"], "abstract": "Domain translation is the process of transforming data from one domain to another while preserving the common semantics. Some of the most popular domain translation systems are based on conditional generative adversarial networks, which use source domain data to drive the generator and as an input to the discriminator. However, this approach does not enforce the preservation of shared semantics since the conditional input can often be ignored by the discriminator. We propose an alternative method for conditioning and present a new framework, where two networks are simultaneously trained, in a supervised manner, to perform domain translation in opposite directions. Our method is not only better at capturing the shared information between two domains but is more generic and can be applied to a broader range of problems. The proposed framework performs well even in challenging cross-modal translations, such as video-driven speech reconstruction, for which other systems struggle to maintain correspondence.", "one-sentence_summary": "A framework  for domain translation which uses a novel mechanism for conditioning energy-based GANs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vougioukas|dino_a_conditional_energybased_gan_for_domain_translation", "supplementary_material": "/attachment/edea68cce0d28da12184e7f35b1cf7eed6e014a2.zip", "pdf": "/pdf/1770fc1a0716d2fde0cefb49d59d540311331789.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvougioukas2021dino,\ntitle={{\\{}DINO{\\}}: A Conditional Energy-Based {\\{}GAN{\\}} for Domain Translation},\nauthor={Konstantinos Vougioukas and Stavros Petridis and Maja Pantic},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=WAISmwsqDsb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "WAISmwsqDsb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper509/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper509/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper509/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper509/Authors|ICLR.cc/2021/Conference/Paper509/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper509/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870192, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper509/-/Official_Comment"}}}, {"id": "Ij29Tg-7Gy4", "original": null, "number": 9, "cdate": 1606222742428, "ddate": null, "tcdate": 1606222742428, "tmdate": 1606222742428, "tddate": null, "forum": "WAISmwsqDsb", "replyto": "jFgyq-zTB3h", "invitation": "ICLR.cc/2021/Conference/Paper509/-/Official_Comment", "content": {"title": "Update: Comparison with Task-Specific SoTA Pretrained Gaugan (SPADE)", "comment": "To further alleviate the reviewer's concerns we have taken the official source code and pre-trained model for SPADE (CVPR 2019) for the Cityscapes dataset and calculated all our metrics for the generated images. We have added these in the paper so that there is now a direct comparison with a state-of-the-art task-specific model.  The results are shown in the following Table which we have also added to the main paper (Tabe 2).\n\n| Model  | PSNR   | SSIM   | CPBD   |Pixel. Acc   | mIoU |\n|---|---|---|---|---|---|\n|  SPADE | 15.97  |  0.38 | **0.72**  |**92.8 %** | **38.3 %** |\n|  DINO | 16.44 |  0.41 | 0.71  | 91.4  %|35.9 %|\n|  DINO (2-way)| **16.78** |  **0.42** |**0.72**  | 91.3  %|35.2 %|\n||Table:|Comparison with SPADE on Cityscapes||||\n\nAs is evident from the results, our model is comparable to the SPADE model. As noted in the previous discussion the SPADE model is specifically designed for translation from segmentation maps whereas our model is generic (not limited to one type of translation) and can perform the translation in both directions. \n\nAlso, we would like to point out that for the more challenging video-to-waveform translation we are outperforming the task-specific state-of-the-art model (Perceptual GAN) as shown in Table 3 the main paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper509/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper509/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DINO: A Conditional Energy-Based GAN for Domain Translation", "authorids": ["~Konstantinos_Vougioukas1", "~Stavros_Petridis1", "~Maja_Pantic1"], "authors": ["Konstantinos Vougioukas", "Stavros Petridis", "Maja Pantic"], "keywords": ["Generative Modelling", "Domain Translation", "Conditional GANs", "Energy-Based GANs"], "abstract": "Domain translation is the process of transforming data from one domain to another while preserving the common semantics. Some of the most popular domain translation systems are based on conditional generative adversarial networks, which use source domain data to drive the generator and as an input to the discriminator. However, this approach does not enforce the preservation of shared semantics since the conditional input can often be ignored by the discriminator. We propose an alternative method for conditioning and present a new framework, where two networks are simultaneously trained, in a supervised manner, to perform domain translation in opposite directions. Our method is not only better at capturing the shared information between two domains but is more generic and can be applied to a broader range of problems. The proposed framework performs well even in challenging cross-modal translations, such as video-driven speech reconstruction, for which other systems struggle to maintain correspondence.", "one-sentence_summary": "A framework  for domain translation which uses a novel mechanism for conditioning energy-based GANs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vougioukas|dino_a_conditional_energybased_gan_for_domain_translation", "supplementary_material": "/attachment/edea68cce0d28da12184e7f35b1cf7eed6e014a2.zip", "pdf": "/pdf/1770fc1a0716d2fde0cefb49d59d540311331789.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvougioukas2021dino,\ntitle={{\\{}DINO{\\}}: A Conditional Energy-Based {\\{}GAN{\\}} for Domain Translation},\nauthor={Konstantinos Vougioukas and Stavros Petridis and Maja Pantic},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=WAISmwsqDsb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "WAISmwsqDsb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper509/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper509/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper509/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper509/Authors|ICLR.cc/2021/Conference/Paper509/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper509/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870192, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper509/-/Official_Comment"}}}, {"id": "jFgyq-zTB3h", "original": null, "number": 7, "cdate": 1606170451105, "ddate": null, "tcdate": 1606170451105, "tmdate": 1606171210734, "tddate": null, "forum": "WAISmwsqDsb", "replyto": "XSo_VoKEMEl", "invitation": "ICLR.cc/2021/Conference/Paper509/-/Official_Comment", "content": {"title": "Addressing remaining concerns", "comment": "We thank the reviewer for the points raised in the discussion. We hope that the following sections will sufficiently address his concerns.\n\n1) We avoid the comparison with certain models not because our approach is inferior in performance but because we believe that a comparison between methods should be done under similar conditions (e.g. architecture, resolution, required inputs). In fact when we compare our model's performance to the results reported for GauGAN and Pix2PixHD (100M parameters in the generator) in Park et.al 2019 we see that our method (37M parameters in generator) has higher segmentation accuracy (91.4% vs 81.9%) on the Cityscapes dataset and slightly worse FID score (51.5 vs 42.3) on the CelebAMask-HQ dataset. However, this is not a fair comparison because other methods usually make use of additional information (i.e. segmentation labels) or use losses that are tailored to a particular problem (whereas our approach is task-independent). Moreover, for some methods, a comparison is not possible since they are designed for different tasks. For example, MaskGAN deals with face editing, which takes as input a face, its segmentation map, and a user-modified segmentation map to produce an altered image. More importantly, all of these methods still use the same conditioning mechanism in the discriminator as proposed in cGAN. Since our goal is to evaluate the impact of the proposed predictive conditioning it is not useful to introduce more factors of variation in the comparison. The advantages of predictive conditioning are evident when we compare models with similar architectures (Table 1 and 2 of the paper). \n\n We have also performed additional experiments to compare to the recently proposed NICE-GAN (CVPR 2020) using the official publicly available source code. The results are shown in the following table for the CelebAMask-HQ and Cityscapes dataset. Although this approach uses unpaired data and is expected to have a worse performance with regards to maintaining the semantics (i.e. Pixel. Acc. and mIoU) we note that it also has worse performance across all other metrics including perceptual metrics that should not be affected by the absence of supervision (i.e. FID and CPBD). This reveals that although NICE-GAN has seen the same training images from each domain (independently) as our method it still produces blurrier, less realistic results while using a far larger network and requiring longer training times (15 days vs 3 days on the same machine).\n\n We would also like to point out that in the case of the speech reconstruction task we do compare to the state-of-the-art approach (Vougioukas et.al 2019) for video-to-waveform conversion and beat it despite its use of a pre-trained model for preserving content. This is shown in Table 3 of the paper. \n\n We would like to stress that DINO is a generic domain translation method hence it is possible to apply it to a wide range of different tasks. We have chosen to showcase the performance for two very different translation tasks and we have shown that our model can perform well in both cases while remaining agnostic to the domains (i.e. does not use domain-specific knowledge). To the best of our knowledge, DINO is the first model to perform well for varied translation tasks whereas other generic translation methods such as CycleGAN and cGAN fail (as evident by the high WER in the speech-reconstruction experiment).\n\n|   Dataset|   PSNR| SSIM  | CPBD  |  FID |   Pixel Acc. |  mIoU |\n|---|---|---|---|---|---|---|\n|  CelebA |10.24   |  0.31 |  0.4 | 53.20 | 93.4%| 62.9%  |\n| Cityscapes | 13.89 |  0.30  | 0.59    | N/A   | 65.7% | 17.9%  |\n| |Table: |NICE-GAN | evaluation | | |\n\n2. Although we agree that the need for paired examples does indeed restrict supervised methods there are still many datasets that have paired data. Indeed many successful approaches in image-to-image translation require supervision (Pix2Pix, Pix2PixHD, GauGAN). This is also especially true for cross-modal translation systems which are still mostly supervised (Qiao et al., 2019, Vougioukas et al. 2019). Finally, for some problems such as speech-driven animation or video-driven speech reconstruction the data from two domains is abundant and comes naturally paired (i.e. the video is recorded with its corresponding audio)."}, "signatures": ["ICLR.cc/2021/Conference/Paper509/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper509/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DINO: A Conditional Energy-Based GAN for Domain Translation", "authorids": ["~Konstantinos_Vougioukas1", "~Stavros_Petridis1", "~Maja_Pantic1"], "authors": ["Konstantinos Vougioukas", "Stavros Petridis", "Maja Pantic"], "keywords": ["Generative Modelling", "Domain Translation", "Conditional GANs", "Energy-Based GANs"], "abstract": "Domain translation is the process of transforming data from one domain to another while preserving the common semantics. Some of the most popular domain translation systems are based on conditional generative adversarial networks, which use source domain data to drive the generator and as an input to the discriminator. However, this approach does not enforce the preservation of shared semantics since the conditional input can often be ignored by the discriminator. We propose an alternative method for conditioning and present a new framework, where two networks are simultaneously trained, in a supervised manner, to perform domain translation in opposite directions. Our method is not only better at capturing the shared information between two domains but is more generic and can be applied to a broader range of problems. The proposed framework performs well even in challenging cross-modal translations, such as video-driven speech reconstruction, for which other systems struggle to maintain correspondence.", "one-sentence_summary": "A framework  for domain translation which uses a novel mechanism for conditioning energy-based GANs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vougioukas|dino_a_conditional_energybased_gan_for_domain_translation", "supplementary_material": "/attachment/edea68cce0d28da12184e7f35b1cf7eed6e014a2.zip", "pdf": "/pdf/1770fc1a0716d2fde0cefb49d59d540311331789.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvougioukas2021dino,\ntitle={{\\{}DINO{\\}}: A Conditional Energy-Based {\\{}GAN{\\}} for Domain Translation},\nauthor={Konstantinos Vougioukas and Stavros Petridis and Maja Pantic},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=WAISmwsqDsb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "WAISmwsqDsb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper509/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper509/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper509/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper509/Authors|ICLR.cc/2021/Conference/Paper509/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper509/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870192, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper509/-/Official_Comment"}}}, {"id": "XSo_VoKEMEl", "original": null, "number": 6, "cdate": 1606143230709, "ddate": null, "tcdate": 1606143230709, "tmdate": 1606143230709, "tddate": null, "forum": "WAISmwsqDsb", "replyto": "fL0VVSvXVOj", "invitation": "ICLR.cc/2021/Conference/Paper509/-/Official_Comment", "content": {"title": "Thank you for the clarification, but still concerns remain", "comment": "Thank you for the clarifications here.\n\nThis answered many of my questions, but in fact it somewhat increased my concerns regarding the empirical evaluation, as apparently there are (somewhat as I expected) many state-of-the-art approaches that have been excluded from the empirical comparison. While I understand the argument that more general approaches are preferable, if more specific approaches exist I feel they should be reported (maybe delineated as being more specific in the results tables). Right now it feels like the reporting is hiding the fact that the proposed models are far from the state of the art.\n\nSimilarly, the fact that the method is supervised somewhat decreases its applicability obviously. Not that this is a big problem, but as the authors state the number of datasets where this supervised data is available (or could even feasibly ever *be* available) is quite limited."}, "signatures": ["ICLR.cc/2021/Conference/Paper509/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper509/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DINO: A Conditional Energy-Based GAN for Domain Translation", "authorids": ["~Konstantinos_Vougioukas1", "~Stavros_Petridis1", "~Maja_Pantic1"], "authors": ["Konstantinos Vougioukas", "Stavros Petridis", "Maja Pantic"], "keywords": ["Generative Modelling", "Domain Translation", "Conditional GANs", "Energy-Based GANs"], "abstract": "Domain translation is the process of transforming data from one domain to another while preserving the common semantics. Some of the most popular domain translation systems are based on conditional generative adversarial networks, which use source domain data to drive the generator and as an input to the discriminator. However, this approach does not enforce the preservation of shared semantics since the conditional input can often be ignored by the discriminator. We propose an alternative method for conditioning and present a new framework, where two networks are simultaneously trained, in a supervised manner, to perform domain translation in opposite directions. Our method is not only better at capturing the shared information between two domains but is more generic and can be applied to a broader range of problems. The proposed framework performs well even in challenging cross-modal translations, such as video-driven speech reconstruction, for which other systems struggle to maintain correspondence.", "one-sentence_summary": "A framework  for domain translation which uses a novel mechanism for conditioning energy-based GANs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vougioukas|dino_a_conditional_energybased_gan_for_domain_translation", "supplementary_material": "/attachment/edea68cce0d28da12184e7f35b1cf7eed6e014a2.zip", "pdf": "/pdf/1770fc1a0716d2fde0cefb49d59d540311331789.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvougioukas2021dino,\ntitle={{\\{}DINO{\\}}: A Conditional Energy-Based {\\{}GAN{\\}} for Domain Translation},\nauthor={Konstantinos Vougioukas and Stavros Petridis and Maja Pantic},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=WAISmwsqDsb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "WAISmwsqDsb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper509/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper509/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper509/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper509/Authors|ICLR.cc/2021/Conference/Paper509/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper509/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870192, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper509/-/Official_Comment"}}}, {"id": "ThTugSRW9n5", "original": null, "number": 5, "cdate": 1605314284014, "ddate": null, "tcdate": 1605314284014, "tmdate": 1606129375264, "tddate": null, "forum": "WAISmwsqDsb", "replyto": "LG9-ry7s8se", "invitation": "ICLR.cc/2021/Conference/Paper509/-/Official_Comment", "content": {"title": "Response to R1:", "comment": "We thank R1 for his comments and for bringing our attention to the privacy concerns regarding the lip-reading application. We acknowledge this in an ethics statement which we have added to Section 4.2 par.3 of the paper. Please find our answers below:\n\n1. We thank the reviewer for bringing these papers to our attention. They are indeed certain aspects that are similar in these approaches (i.e. back-translation, and bidirectional translation) hence we have mentioned and reference them in section 3.2.\n\n2. We understand that it may be confusing to the reader to refer to the network as discriminator, especially in the bidirectional case where both networks essentially have both roles. We have changed the text by naming the translation networks Forward (source-to-target) and Reverse (target-to-source).\n\n3. We updated the notation in the equation (now equation 7) to reflect the new naming of the networks and added underbraces to show which parts of the objective correspond to each network's role as a generator and which parts correspond to its role as a discriminator."}, "signatures": ["ICLR.cc/2021/Conference/Paper509/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper509/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DINO: A Conditional Energy-Based GAN for Domain Translation", "authorids": ["~Konstantinos_Vougioukas1", "~Stavros_Petridis1", "~Maja_Pantic1"], "authors": ["Konstantinos Vougioukas", "Stavros Petridis", "Maja Pantic"], "keywords": ["Generative Modelling", "Domain Translation", "Conditional GANs", "Energy-Based GANs"], "abstract": "Domain translation is the process of transforming data from one domain to another while preserving the common semantics. Some of the most popular domain translation systems are based on conditional generative adversarial networks, which use source domain data to drive the generator and as an input to the discriminator. However, this approach does not enforce the preservation of shared semantics since the conditional input can often be ignored by the discriminator. We propose an alternative method for conditioning and present a new framework, where two networks are simultaneously trained, in a supervised manner, to perform domain translation in opposite directions. Our method is not only better at capturing the shared information between two domains but is more generic and can be applied to a broader range of problems. The proposed framework performs well even in challenging cross-modal translations, such as video-driven speech reconstruction, for which other systems struggle to maintain correspondence.", "one-sentence_summary": "A framework  for domain translation which uses a novel mechanism for conditioning energy-based GANs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vougioukas|dino_a_conditional_energybased_gan_for_domain_translation", "supplementary_material": "/attachment/edea68cce0d28da12184e7f35b1cf7eed6e014a2.zip", "pdf": "/pdf/1770fc1a0716d2fde0cefb49d59d540311331789.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvougioukas2021dino,\ntitle={{\\{}DINO{\\}}: A Conditional Energy-Based {\\{}GAN{\\}} for Domain Translation},\nauthor={Konstantinos Vougioukas and Stavros Petridis and Maja Pantic},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=WAISmwsqDsb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "WAISmwsqDsb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper509/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper509/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper509/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper509/Authors|ICLR.cc/2021/Conference/Paper509/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper509/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870192, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper509/-/Official_Comment"}}}, {"id": "fL0VVSvXVOj", "original": null, "number": 4, "cdate": 1605314174152, "ddate": null, "tcdate": 1605314174152, "tmdate": 1605314174152, "tddate": null, "forum": "WAISmwsqDsb", "replyto": "H_Mlpd34jJ", "invitation": "ICLR.cc/2021/Conference/Paper509/-/Official_Comment", "content": {"title": "Response to R3:", "comment": "We thank R3 for his feedback and for taking the time to carefully read the paper. Here are the answers to the questions:\n\n1: The reviewer is correct to say that this is a supervised method. We have amended the paper to explicitly mention this both in the abstract and when introducing the method.\n\n2: Pix2Pix and BicycleGAN (this is stated in Section 3 of Zhu et.al. which mentions the need for paired data) are both supervised methods for image-to-image translation. We have chosen these methods because they are representative of supervised methods that use cGANs and VAE-GANs respectively. Although improved versions for Pix2Pix exist such as Pix2PixHD they rely on different architectures and training schemes (progressive growing, multiscale discrimination) to improve the resolution of images. Other state-of-the-art approaches like GauGAN (SPADE) (works only with segmentation maps), MaskGAN (works only on facial images) or APDrawing GAN (only performs sketch generation from photos) achieve impressive results but are tied to very specific types of translations. In this paper, we propose a generic domain translation method and a novel way of conditioning and wanted to compare it to standard conditioning methods for GANs. By keeping the network architecture simple and consistent for all models we can measure the effectiveness of predictive conditioning. We avoid comparisons with methods that are tailored to specific tasks since they often use additional information (pretrained models, accurate segmentation maps) to achieve their results. We have clarified this point in Section 4.1 of the paper.\n\n3: Our method requires paired data and hence the selection of datasets is limited compared to unsupervised methods. We have chosen the Cityscapes and  CelebA-MaskHQ datasets because they are large, have high-resolution images and have previously been used for translation tasks. The cityscapes dataset has 5000 images with annotations and is used in many image translation papers including Pix2Pix, Pix2PixHD, and SPADE. The CelebA-MaskHQ contains 30,000 high-resolution face images selected from the CelebA with their corresponding annotations and is introduced and used in MaskGAN. We have chosen this because it is an annotated subset of CelebA, which is one of the most widely used datasets for faces. Finally, we use the APDrawings dataset in the Appendix (Fig.10) to show an example of why the balancing used in DINO is better suited for predictive conditioning compared to the balancing proposed in BEGAN. This is a small dataset of 70 images (when not counting augmentations by affine transforms) introduced and used in APDrawingGAN for sketch generation.\n\n4: The margin loss refers to a loss which tries to maximize the energy of fake samples and minimize the energy for real samples. This is described in LeCun et.al. 2006, which mentions that a generalized margin loss is one that \"uses some form of margin to create an energy gap between the correct answer and the incorrect answers\". In the margin loss the fake samples are pushed to a high energy which is dictated by the margin parameter (highest allowable energy). This is a similar loss to that used in EBGAN and we have revised Eq.1 and added a new equation showing the exact formula for the loss (Eq. 2) to make this clear. The margin parameter in this loss (highest energy for fake samples) is a fixed number. In DINO we do not use a fixed margin with decay but use adaptive balancing instead (see next bullet-point)\n \n5. As shown in EBGAN better results are achieved when the margin (as explained above) decays during training using some decay schedule. In order to avoid the need for a decay schedule, we use adaptive balancing to request that the highest energy of fake samples must always be a multiple of the energy of real samples E(Real)/\u03b3 =  E(Fake) (0<\u03b3<1). The parameter \u03b3 is just dictating how large that multiple will be. For example, if gamma is 0.5 then we require that fake energies always be pushed so that they are 2 times the energies of real samples. To maintain this balance during training we use a proportional controller as is done in BEGAN. The controller affects how much emphasis the discriminator places on pushing fake samples to high energy. As fake samples get close to the high energy (E(Real) / \u03b3 ) the controller will slowly stop placing emphasis on pushing them to higher energy. The rate at which the controller changes the emphasis is the control gain \u03bb.\n\n6: MirrorGAN is indeed similar to our approach in that it enforces the semantics in generated image from text through redescription (image-to-text). However, unlike our approach, the network used for re-description is pretrained and used in addition to the discriminator of the GAN. In our method We have not compared to MirrorGAN because it is a GAN that has been designed for text-to-image synthesis and it is not straightforward to apply it for different translation tasks."}, "signatures": ["ICLR.cc/2021/Conference/Paper509/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper509/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DINO: A Conditional Energy-Based GAN for Domain Translation", "authorids": ["~Konstantinos_Vougioukas1", "~Stavros_Petridis1", "~Maja_Pantic1"], "authors": ["Konstantinos Vougioukas", "Stavros Petridis", "Maja Pantic"], "keywords": ["Generative Modelling", "Domain Translation", "Conditional GANs", "Energy-Based GANs"], "abstract": "Domain translation is the process of transforming data from one domain to another while preserving the common semantics. Some of the most popular domain translation systems are based on conditional generative adversarial networks, which use source domain data to drive the generator and as an input to the discriminator. However, this approach does not enforce the preservation of shared semantics since the conditional input can often be ignored by the discriminator. We propose an alternative method for conditioning and present a new framework, where two networks are simultaneously trained, in a supervised manner, to perform domain translation in opposite directions. Our method is not only better at capturing the shared information between two domains but is more generic and can be applied to a broader range of problems. The proposed framework performs well even in challenging cross-modal translations, such as video-driven speech reconstruction, for which other systems struggle to maintain correspondence.", "one-sentence_summary": "A framework  for domain translation which uses a novel mechanism for conditioning energy-based GANs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vougioukas|dino_a_conditional_energybased_gan_for_domain_translation", "supplementary_material": "/attachment/edea68cce0d28da12184e7f35b1cf7eed6e014a2.zip", "pdf": "/pdf/1770fc1a0716d2fde0cefb49d59d540311331789.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvougioukas2021dino,\ntitle={{\\{}DINO{\\}}: A Conditional Energy-Based {\\{}GAN{\\}} for Domain Translation},\nauthor={Konstantinos Vougioukas and Stavros Petridis and Maja Pantic},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=WAISmwsqDsb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "WAISmwsqDsb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper509/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper509/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper509/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper509/Authors|ICLR.cc/2021/Conference/Paper509/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper509/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870192, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper509/-/Official_Comment"}}}, {"id": "FAqI8UB-P_1", "original": null, "number": 3, "cdate": 1605313966229, "ddate": null, "tcdate": 1605313966229, "tmdate": 1605314002026, "tddate": null, "forum": "WAISmwsqDsb", "replyto": "EnmRyVteYmm", "invitation": "ICLR.cc/2021/Conference/Paper509/-/Official_Comment", "content": {"title": "Response to R2:", "comment": "We thank R2 for his feedback and comments. Please find our answers below:\n\n1: We realize that some details such as the optimizer and learning rate used for the experiments are missing from the paper. We thank the reviewer for pointing this out. In the image-to-image experiments, we used an Adam optimizer with a learning rate of 0.0002 and a batch size of 8. In the audio-visual experiments, we used an Adam optimizer with a learning rate of 0.0001 for the parameters of the Video-to-Audio network and a learning rate of 0.001 for the parameters Audio-to-Video network. We used a batch size of 8 for the Video-to-Audio experiments. We have added this information in the paper in Sections 4.1 and 4.2. The details of the architecture for each network are shown in the Appendix in Section A.1. We will also make our source code and pretrained models publicly available so that others can reproduce the results.\n\n2: The reviewer is correct to say that there is not a clear winner in terms of performance between the bidirectional and unidirectional version of DINO. In the unidirectional version, the network acting as the discriminator will not benefit from the adversarial loss and will therefore produce less realistic results. To obtain realistic samples in both directions using the unidirectional DINO method training would have to be performed separately for each direction and the discriminator network would be discarded each time. This problem can be solved by using the bidirectional version where both networks benefit from an adversarial loss and both produce realistic samples (and no network is discarded after training). Therefore, the bidirectional DINO method can train for bidirectional translation in half the time of the unidirectional version while achieving comparable results. The motivation for bidirectional translation is to reduce the number of parameters required for bidirectional translation without sacrificing the quality of the results. This is reflected in section 3.1 where we introduce the bidirectional model and in the conclusion. \n\n3: DINO can work without the adaptive balancing by using a fixed margin for the margin loss like in EBGAN. However, we have found that using adaptive balancing improves the stability and quality of the results. The performance with and without adaptive balancing is shown in our ablation study in the Appendix Section A.3 and Table 4 (rows 2 and 3). The results show that adaptive balancing results in a significant improvement across all performance metrics."}, "signatures": ["ICLR.cc/2021/Conference/Paper509/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper509/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DINO: A Conditional Energy-Based GAN for Domain Translation", "authorids": ["~Konstantinos_Vougioukas1", "~Stavros_Petridis1", "~Maja_Pantic1"], "authors": ["Konstantinos Vougioukas", "Stavros Petridis", "Maja Pantic"], "keywords": ["Generative Modelling", "Domain Translation", "Conditional GANs", "Energy-Based GANs"], "abstract": "Domain translation is the process of transforming data from one domain to another while preserving the common semantics. Some of the most popular domain translation systems are based on conditional generative adversarial networks, which use source domain data to drive the generator and as an input to the discriminator. However, this approach does not enforce the preservation of shared semantics since the conditional input can often be ignored by the discriminator. We propose an alternative method for conditioning and present a new framework, where two networks are simultaneously trained, in a supervised manner, to perform domain translation in opposite directions. Our method is not only better at capturing the shared information between two domains but is more generic and can be applied to a broader range of problems. The proposed framework performs well even in challenging cross-modal translations, such as video-driven speech reconstruction, for which other systems struggle to maintain correspondence.", "one-sentence_summary": "A framework  for domain translation which uses a novel mechanism for conditioning energy-based GANs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vougioukas|dino_a_conditional_energybased_gan_for_domain_translation", "supplementary_material": "/attachment/edea68cce0d28da12184e7f35b1cf7eed6e014a2.zip", "pdf": "/pdf/1770fc1a0716d2fde0cefb49d59d540311331789.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvougioukas2021dino,\ntitle={{\\{}DINO{\\}}: A Conditional Energy-Based {\\{}GAN{\\}} for Domain Translation},\nauthor={Konstantinos Vougioukas and Stavros Petridis and Maja Pantic},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=WAISmwsqDsb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "WAISmwsqDsb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper509/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper509/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper509/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper509/Authors|ICLR.cc/2021/Conference/Paper509/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper509/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870192, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper509/-/Official_Comment"}}}, {"id": "EnmRyVteYmm", "original": null, "number": 3, "cdate": 1603991576078, "ddate": null, "tcdate": 1603991576078, "tmdate": 1605024672449, "tddate": null, "forum": "WAISmwsqDsb", "replyto": "WAISmwsqDsb", "invitation": "ICLR.cc/2021/Conference/Paper509/-/Official_Review", "content": {"title": "Energy based GAN with symmetric generators ", "review": "The paper proposes an adversarial framework DINO to train translation models from source to target and target to source. The basic idea is to replace generator and discriminator in the energy based GAN with two source-to-target generation models. The discriminator(reverse generator) and the generator competes in a minimax game to reconstruct the data. The framework is further extended with duplicate output heads for both discriminator and generator to enhance the training robustness. \nThe authors evaluated their framework on two tasks: image to image translation and silent-video to speech reconstruction. The DINO method impressive improvement in both tasks. \n\nStrong points:\n1. The proposed DINO framework is well motivated. The objectives in DINO are reasonable and novel.\n2. Experiments on both image to image translation and video to speech reconstruction verify the DINO method achieves significant improvement comparing with other translation methods. \n\nWeak points:\n1. Important details are omitted in image-to-image translation and video to speech reconstruction. It is unclear about the backbone network as well as the parameter setup. Therefore, it is impossible to reproduce the method. \n2. DINO and DINO(bidirectional) are not consistent winners. It is not explained or analyzed why DINO sometimes wins while DINO(bidirectional) wins otherwise. There is no recommendation for practical use either. \n3. The adaptive balancing seems reasonable. But it is not studied in the experiment whether it improves the training. ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper509/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper509/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DINO: A Conditional Energy-Based GAN for Domain Translation", "authorids": ["~Konstantinos_Vougioukas1", "~Stavros_Petridis1", "~Maja_Pantic1"], "authors": ["Konstantinos Vougioukas", "Stavros Petridis", "Maja Pantic"], "keywords": ["Generative Modelling", "Domain Translation", "Conditional GANs", "Energy-Based GANs"], "abstract": "Domain translation is the process of transforming data from one domain to another while preserving the common semantics. Some of the most popular domain translation systems are based on conditional generative adversarial networks, which use source domain data to drive the generator and as an input to the discriminator. However, this approach does not enforce the preservation of shared semantics since the conditional input can often be ignored by the discriminator. We propose an alternative method for conditioning and present a new framework, where two networks are simultaneously trained, in a supervised manner, to perform domain translation in opposite directions. Our method is not only better at capturing the shared information between two domains but is more generic and can be applied to a broader range of problems. The proposed framework performs well even in challenging cross-modal translations, such as video-driven speech reconstruction, for which other systems struggle to maintain correspondence.", "one-sentence_summary": "A framework  for domain translation which uses a novel mechanism for conditioning energy-based GANs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vougioukas|dino_a_conditional_energybased_gan_for_domain_translation", "supplementary_material": "/attachment/edea68cce0d28da12184e7f35b1cf7eed6e014a2.zip", "pdf": "/pdf/1770fc1a0716d2fde0cefb49d59d540311331789.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvougioukas2021dino,\ntitle={{\\{}DINO{\\}}: A Conditional Energy-Based {\\{}GAN{\\}} for Domain Translation},\nauthor={Konstantinos Vougioukas and Stavros Petridis and Maja Pantic},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=WAISmwsqDsb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "WAISmwsqDsb", "replyto": "WAISmwsqDsb", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper509/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538141536, "tmdate": 1606915772911, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper509/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper509/-/Official_Review"}}}], "count": 14}