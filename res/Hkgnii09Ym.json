{"notes": [{"id": "Hkgnii09Ym", "original": "H1lOZVN5KX", "number": 656, "cdate": 1538087843642, "ddate": null, "tcdate": 1538087843642, "tmdate": 1545355396711, "tddate": null, "forum": "Hkgnii09Ym", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Set Transformer", "abstract": "Many machine learning tasks such as multiple instance learning, 3D shape recognition and few-shot image classification are defined on sets of instances. Since solutions to such problems do not depend on the permutation of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating increased performance compared to recent methods for set-structured data.", "keywords": ["attention", "meta-learning", "set-input neural networks", "permutation invariant modeling"], "authorids": ["juho.lee@stats.ox.ac.uk", "einet89@gmail.com", "jtkim@postech.ac.kr", "adamk@robots.ox.ac.uk", "seungjin@postech.ac.kr", "y.w.teh@stats.ox.ac.uk"], "authors": ["Juho Lee", "Yoonho Lee", "Jungtaek Kim", "Adam R. Kosiorek", "Seungjin Choi", "Yee Whye Teh"], "TL;DR": "Attention-based neural network to process set-structured data", "pdf": "/pdf/87de8c8193bf9a20d1a2183e4d5c46b0d14feb77.pdf", "paperhash": "lee|set_transformer", "_bibtex": "@misc{\nlee2019set,\ntitle={Set Transformer},\nauthor={Juho Lee and Yoonho Lee and Jungtaek Kim and Adam R. Kosiorek and Seungjin Choi and Yee Whye Teh},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkgnii09Ym},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rJgXZuvxl4", "original": null, "number": 1, "cdate": 1544742907378, "ddate": null, "tcdate": 1544742907378, "tmdate": 1545354515102, "tddate": null, "forum": "Hkgnii09Ym", "replyto": "Hkgnii09Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper656/Meta_Review", "content": {"metareview": "This paper introduces set transformer for set inputs. The idea is built upon the transformer and introduces the attention mechanism. Major concerns on novelty were raised by the reviewers. ", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Novelty is limited."}, "signatures": ["ICLR.cc/2019/Conference/Paper656/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper656/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Set Transformer", "abstract": "Many machine learning tasks such as multiple instance learning, 3D shape recognition and few-shot image classification are defined on sets of instances. Since solutions to such problems do not depend on the permutation of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating increased performance compared to recent methods for set-structured data.", "keywords": ["attention", "meta-learning", "set-input neural networks", "permutation invariant modeling"], "authorids": ["juho.lee@stats.ox.ac.uk", "einet89@gmail.com", "jtkim@postech.ac.kr", "adamk@robots.ox.ac.uk", "seungjin@postech.ac.kr", "y.w.teh@stats.ox.ac.uk"], "authors": ["Juho Lee", "Yoonho Lee", "Jungtaek Kim", "Adam R. Kosiorek", "Seungjin Choi", "Yee Whye Teh"], "TL;DR": "Attention-based neural network to process set-structured data", "pdf": "/pdf/87de8c8193bf9a20d1a2183e4d5c46b0d14feb77.pdf", "paperhash": "lee|set_transformer", "_bibtex": "@misc{\nlee2019set,\ntitle={Set Transformer},\nauthor={Juho Lee and Yoonho Lee and Jungtaek Kim and Adam R. Kosiorek and Seungjin Choi and Yee Whye Teh},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkgnii09Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper656/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353136452, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hkgnii09Ym", "replyto": "Hkgnii09Ym", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper656/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper656/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper656/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353136452}}}, {"id": "HkepSRl1JN", "original": null, "number": 5, "cdate": 1543601732670, "ddate": null, "tcdate": 1543601732670, "tmdate": 1543601732670, "tddate": null, "forum": "Hkgnii09Ym", "replyto": "rkx3ROgchX", "invitation": "ICLR.cc/2019/Conference/-/Paper656/Official_Comment", "content": {"title": "Response after edits", "comment": "Thank you very much for raising the score. Sorry for not updating Table 1, we were aware of it but forgot to update it when we uploaded our revision. We will correct it upon our acceptance. We will also try to discuss more about permutation equivariant layers as you suggested."}, "signatures": ["ICLR.cc/2019/Conference/Paper656/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper656/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper656/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Set Transformer", "abstract": "Many machine learning tasks such as multiple instance learning, 3D shape recognition and few-shot image classification are defined on sets of instances. Since solutions to such problems do not depend on the permutation of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating increased performance compared to recent methods for set-structured data.", "keywords": ["attention", "meta-learning", "set-input neural networks", "permutation invariant modeling"], "authorids": ["juho.lee@stats.ox.ac.uk", "einet89@gmail.com", "jtkim@postech.ac.kr", "adamk@robots.ox.ac.uk", "seungjin@postech.ac.kr", "y.w.teh@stats.ox.ac.uk"], "authors": ["Juho Lee", "Yoonho Lee", "Jungtaek Kim", "Adam R. Kosiorek", "Seungjin Choi", "Yee Whye Teh"], "TL;DR": "Attention-based neural network to process set-structured data", "pdf": "/pdf/87de8c8193bf9a20d1a2183e4d5c46b0d14feb77.pdf", "paperhash": "lee|set_transformer", "_bibtex": "@misc{\nlee2019set,\ntitle={Set Transformer},\nauthor={Juho Lee and Yoonho Lee and Jungtaek Kim and Adam R. Kosiorek and Seungjin Choi and Yee Whye Teh},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkgnii09Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper656/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617272, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hkgnii09Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper656/Authors", "ICLR.cc/2019/Conference/Paper656/Reviewers", "ICLR.cc/2019/Conference/Paper656/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper656/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper656/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper656/Authors|ICLR.cc/2019/Conference/Paper656/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper656/Reviewers", "ICLR.cc/2019/Conference/Paper656/Authors", "ICLR.cc/2019/Conference/Paper656/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617272}}}, {"id": "rkx3ROgchX", "original": null, "number": 1, "cdate": 1541175508469, "ddate": null, "tcdate": 1541175508469, "tmdate": 1543586887252, "tddate": null, "forum": "Hkgnii09Ym", "replyto": "Hkgnii09Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper656/Official_Review", "content": {"title": "Missing comparisons to permutation equivariant DeepSets", "review": "This paper looks at stacking attention mechanism for learning over sets.\n\nI think that the paper is well written overall. The architecture put forth is a fairly straightforward implementation of attention. Thus the methodological contribution is incremental. Still, it is nice to see some implementation of an attention model be considered for permutation invariant set embeddings.\n\nHowever, there are some core misrepresentations and omissions that make publication difficult. The main problem is that the paper completely ignores the permutation equivariant mappings discussed in DeepSets (Zaheer 2017). See (4) and (23) of https://arxiv.org/pdf/1703.06114.pdf: \"Since composition of permutation equivariant functions is also permutation equivariant, we can build deep models by stacking layers.\"\nIn practice this is often done by mapping points x_i in a set as x_i -> \\phi(x_i) - max_j \\phi(x_j). Stacking this layer works surprisingly well, typically better than just with a single pool. Thus, the permutation equivalent mappings of Zaheer 2017, which do have higher-order interactions and are linear in the number of points, are a glaring omission of table 1 and all of the experiments. Furthermore, the omission leads to a misrepresentation of the work.\n\nAnother unfortunate omission is previous work that considers set and distribution data through kernels and other nonparametric methods such as: \nMuandet, Krikamol, et al. \"Learning from distributions via support measure machines.\" Advances in neural information processing systems. 2012.\nOliva, Junier, Barnab\u00e1s P\u00f3czos, and Jeff Schneider. \"Distribution to distribution regression.\" International Conference on Machine Learning. 2013.\n\nIt is also odd that the paper compared to DeepSets on modelnet with 100 and 1000 points but not with 5000 points. Will there be code available?\n\nWithout a better description of and comparison to permutation equivariant mappings I would feel hesitant to recommend publication.\n\nedit:\nIn light of the revised experiments and inclusion of permutation equivariant deepset layers, I'm inclined to recommend publication. However, if I could nitpick further, I think it would be nice to make some edit (or addition) to Table 1 to include permutation equivariant deepsets. Moreover, it would be nice to have some additional description of permutation equivariant layers in Section 2.1.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper656/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Set Transformer", "abstract": "Many machine learning tasks such as multiple instance learning, 3D shape recognition and few-shot image classification are defined on sets of instances. Since solutions to such problems do not depend on the permutation of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating increased performance compared to recent methods for set-structured data.", "keywords": ["attention", "meta-learning", "set-input neural networks", "permutation invariant modeling"], "authorids": ["juho.lee@stats.ox.ac.uk", "einet89@gmail.com", "jtkim@postech.ac.kr", "adamk@robots.ox.ac.uk", "seungjin@postech.ac.kr", "y.w.teh@stats.ox.ac.uk"], "authors": ["Juho Lee", "Yoonho Lee", "Jungtaek Kim", "Adam R. Kosiorek", "Seungjin Choi", "Yee Whye Teh"], "TL;DR": "Attention-based neural network to process set-structured data", "pdf": "/pdf/87de8c8193bf9a20d1a2183e4d5c46b0d14feb77.pdf", "paperhash": "lee|set_transformer", "_bibtex": "@misc{\nlee2019set,\ntitle={Set Transformer},\nauthor={Juho Lee and Yoonho Lee and Jungtaek Kim and Adam R. Kosiorek and Seungjin Choi and Yee Whye Teh},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkgnii09Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper656/Official_Review", "cdate": 1542234409912, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hkgnii09Ym", "replyto": "Hkgnii09Ym", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper656/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335773608, "tmdate": 1552335773608, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper656/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "ryl1nBJ0pQ", "original": null, "number": 4, "cdate": 1542481318931, "ddate": null, "tcdate": 1542481318931, "tmdate": 1542481318931, "tddate": null, "forum": "Hkgnii09Ym", "replyto": "Hkgnii09Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper656/Official_Comment", "content": {"title": "Revision updated", "comment": "Dear reviewers, \n\nThanks for your comments. According to your opinion, we added three baselines to all experiments (mean pooling based permutation equivariant deep set , max pooling based permutation equivariant deep set (Zaheer et al, 2017), dot product attention based pooling (Yang et al., 2018, Ilse et al., 2018)). We've also added some extra experiments to see the scalability of the set transformer on large scale clustering experiments. Right now we are running the point cloud experiments with 5,000 pts, and the results will be updated as soon as it is completed. \n\nThere has been common concern about the novelty of our work. We want to emphasize again that our architecture is not a simple combination of existing works or naive adaptation of attention mechanism. Please refer to our comment to Reviewer 3 regarding the originality. Thanks."}, "signatures": ["ICLR.cc/2019/Conference/Paper656/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper656/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper656/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Set Transformer", "abstract": "Many machine learning tasks such as multiple instance learning, 3D shape recognition and few-shot image classification are defined on sets of instances. Since solutions to such problems do not depend on the permutation of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating increased performance compared to recent methods for set-structured data.", "keywords": ["attention", "meta-learning", "set-input neural networks", "permutation invariant modeling"], "authorids": ["juho.lee@stats.ox.ac.uk", "einet89@gmail.com", "jtkim@postech.ac.kr", "adamk@robots.ox.ac.uk", "seungjin@postech.ac.kr", "y.w.teh@stats.ox.ac.uk"], "authors": ["Juho Lee", "Yoonho Lee", "Jungtaek Kim", "Adam R. Kosiorek", "Seungjin Choi", "Yee Whye Teh"], "TL;DR": "Attention-based neural network to process set-structured data", "pdf": "/pdf/87de8c8193bf9a20d1a2183e4d5c46b0d14feb77.pdf", "paperhash": "lee|set_transformer", "_bibtex": "@misc{\nlee2019set,\ntitle={Set Transformer},\nauthor={Juho Lee and Yoonho Lee and Jungtaek Kim and Adam R. Kosiorek and Seungjin Choi and Yee Whye Teh},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkgnii09Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper656/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617272, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hkgnii09Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper656/Authors", "ICLR.cc/2019/Conference/Paper656/Reviewers", "ICLR.cc/2019/Conference/Paper656/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper656/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper656/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper656/Authors|ICLR.cc/2019/Conference/Paper656/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper656/Reviewers", "ICLR.cc/2019/Conference/Paper656/Authors", "ICLR.cc/2019/Conference/Paper656/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617272}}}, {"id": "SklNCMkRp7", "original": null, "number": 3, "cdate": 1542480587639, "ddate": null, "tcdate": 1542480587639, "tmdate": 1542480587639, "tddate": null, "forum": "Hkgnii09Ym", "replyto": "rkx3ROgchX", "invitation": "ICLR.cc/2019/Conference/-/Paper656/Official_Comment", "content": {"title": "Added permutation equivariant baselines", "comment": "Thanks for your constructive comments.\ni) Consider permutation equivariant mappings (Zaheer et al).\nThanks for pointing this out. We added permutation equivariant architectures with both mean pooling and max pooling (rFFp-mean and rFFp-max) as baselines for all experiments, and have updated the paper. Our overall observation is that these permutation equivariant baselines do help, but the performance gain was not as significant as the gains achieved by SAB, ISAB and PMA.\n\nii) Cite and consider Muandet et al. and Oliva et al.\nThanks for mentioning the related works. Muandet et al. was cited and mentioned in the introduction in the submitted version of our paper. We have revised to include Oliva et al.\n\niii) Add modelnet w/5000; will code be available?\nWe had no time to conduct experiments with 5,000 pts during our first submission. \nRight now we are running experiments with 5,000 pts and they are going to be added to the appendix as soon as it is completed. The code will definitely be available open source.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper656/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper656/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper656/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Set Transformer", "abstract": "Many machine learning tasks such as multiple instance learning, 3D shape recognition and few-shot image classification are defined on sets of instances. Since solutions to such problems do not depend on the permutation of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating increased performance compared to recent methods for set-structured data.", "keywords": ["attention", "meta-learning", "set-input neural networks", "permutation invariant modeling"], "authorids": ["juho.lee@stats.ox.ac.uk", "einet89@gmail.com", "jtkim@postech.ac.kr", "adamk@robots.ox.ac.uk", "seungjin@postech.ac.kr", "y.w.teh@stats.ox.ac.uk"], "authors": ["Juho Lee", "Yoonho Lee", "Jungtaek Kim", "Adam R. Kosiorek", "Seungjin Choi", "Yee Whye Teh"], "TL;DR": "Attention-based neural network to process set-structured data", "pdf": "/pdf/87de8c8193bf9a20d1a2183e4d5c46b0d14feb77.pdf", "paperhash": "lee|set_transformer", "_bibtex": "@misc{\nlee2019set,\ntitle={Set Transformer},\nauthor={Juho Lee and Yoonho Lee and Jungtaek Kim and Adam R. Kosiorek and Seungjin Choi and Yee Whye Teh},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkgnii09Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper656/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617272, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hkgnii09Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper656/Authors", "ICLR.cc/2019/Conference/Paper656/Reviewers", "ICLR.cc/2019/Conference/Paper656/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper656/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper656/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper656/Authors|ICLR.cc/2019/Conference/Paper656/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper656/Reviewers", "ICLR.cc/2019/Conference/Paper656/Authors", "ICLR.cc/2019/Conference/Paper656/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617272}}}, {"id": "rkxjwGJRTQ", "original": null, "number": 2, "cdate": 1542480482934, "ddate": null, "tcdate": 1542480482934, "tmdate": 1542480482934, "tddate": null, "forum": "Hkgnii09Ym", "replyto": "B1eH_an927", "invitation": "ICLR.cc/2019/Conference/-/Paper656/Official_Comment", "content": {"title": "About ablation studies", "comment": "Thanks for your constructive comments.\nIn our experiments, we compare (rFF+Pooling, SAB+Pooling, ISAB+Pooling, rFF+PMA, SAB+PMA, ISAB+PMA).\nEach of those variants are the Set Transformer with some (or no) components removed, so the experiments do report ablation results. We also added extra baselines (rFFp_mean + Pooling, rFFp_max + Pooling, rFF + Dotprod), and comparison to these methods supports our claim on the importance of having self-attention mechanism.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper656/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper656/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper656/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Set Transformer", "abstract": "Many machine learning tasks such as multiple instance learning, 3D shape recognition and few-shot image classification are defined on sets of instances. Since solutions to such problems do not depend on the permutation of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating increased performance compared to recent methods for set-structured data.", "keywords": ["attention", "meta-learning", "set-input neural networks", "permutation invariant modeling"], "authorids": ["juho.lee@stats.ox.ac.uk", "einet89@gmail.com", "jtkim@postech.ac.kr", "adamk@robots.ox.ac.uk", "seungjin@postech.ac.kr", "y.w.teh@stats.ox.ac.uk"], "authors": ["Juho Lee", "Yoonho Lee", "Jungtaek Kim", "Adam R. Kosiorek", "Seungjin Choi", "Yee Whye Teh"], "TL;DR": "Attention-based neural network to process set-structured data", "pdf": "/pdf/87de8c8193bf9a20d1a2183e4d5c46b0d14feb77.pdf", "paperhash": "lee|set_transformer", "_bibtex": "@misc{\nlee2019set,\ntitle={Set Transformer},\nauthor={Juho Lee and Yoonho Lee and Jungtaek Kim and Adam R. Kosiorek and Seungjin Choi and Yee Whye Teh},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkgnii09Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper656/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617272, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hkgnii09Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper656/Authors", "ICLR.cc/2019/Conference/Paper656/Reviewers", "ICLR.cc/2019/Conference/Paper656/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper656/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper656/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper656/Authors|ICLR.cc/2019/Conference/Paper656/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper656/Reviewers", "ICLR.cc/2019/Conference/Paper656/Authors", "ICLR.cc/2019/Conference/Paper656/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617272}}}, {"id": "B1lmlfyRa7", "original": null, "number": 1, "cdate": 1542480363373, "ddate": null, "tcdate": 1542480363373, "tmdate": 1542480363373, "tddate": null, "forum": "Hkgnii09Ym", "replyto": "BJec0UPRhm", "invitation": "ICLR.cc/2019/Conference/-/Paper656/Official_Comment", "content": {"title": "Clarification for the novelty and additional experiments", "comment": "Thanks for your constructive comments. \n\ni) Clarify originality\nOur method is not a simple combination of [1,2,3]. [1,3] uses dot product attention, where the transformed features are fed into a FF layer to produce softmax weights to be used to pool the features via weighted average. Hence, these methods do not take into account pairwise/higher-order interactions between elements in sets. We added dot-product attention based pooling as another baseline for all experiments. As we reviewed in the related works section, there are works using transformer-type self attention mechanism in encoder part of the model [2,4], but none of them were presented in context of permutation invariant set-taking neural nets. We summarize the novelty of our model below.\n\n- We adapted transformer based self-attention mechanism for *both* encoder and decoder part of permutation invariant set networks. \n\n- We introduce ISAB, which allows us to implement self-attention mechanism with reduced runtime complexity. This is an original contribution that was not present in previous works.\n\n- We introduce PMA, which differs from the dot-product attention-based pooling schemes presented in previous works. Especially, having multiple seed vectors and applying self-attention among them is a novel idea that we found to be very effective, especially for clustering-like problems, where modeling of output interactions (such as explaining away) is important.\n\n[1] Yang et al. 2018, Attentional aggregation of deep feature sets for multi-view 3d reconstruction.\n[2] Mishra et al. 2018, A simple neural attentive meta-learner.\n[3] Ilse et al. 2018, Attention-based deep multiple instance learning.\n[4] Ma et al. 2018, Attend and interact: higher-order object interactions for video understanding.\n\nii) Runtime concerns; can Set Transformer scale up?\nISABs should be able to scale up since they require O(n) memory and time, where n is the number of points in a set. In fact this is precisely why we introduced ISAB. We have added additional experiments to demonstrate actual running time of ISAB and SAB, and the tradeoff between accuracy and running time with respect to the different number of inducing points: see Appendix C.1 and Figure 5 in the revised paper.\n\niii) Is attention useful when the set size is large and the embedding is expressive?\nFirst of all, please note that ISAB + Pooling is also our contribution, which performed the best in Table 6. We presume that the reason why the set transformer was not as effective as ISAB + Pooling in Table 6 was due to the nature of the problem. In point-cloud classification, once we encode interactions between elements via the self-attention mechanism, decoding them into label vectors does not require complex architectures like PMA. To verify this, we conducted extra experiments on clustering, where we used up to 5,000 data points per set. See Appendix B.3.2 and Table 12. In this experiment, where the PMA plays an important role, set transformer works extremely well with as few as 32 inducing points.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper656/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper656/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper656/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Set Transformer", "abstract": "Many machine learning tasks such as multiple instance learning, 3D shape recognition and few-shot image classification are defined on sets of instances. Since solutions to such problems do not depend on the permutation of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating increased performance compared to recent methods for set-structured data.", "keywords": ["attention", "meta-learning", "set-input neural networks", "permutation invariant modeling"], "authorids": ["juho.lee@stats.ox.ac.uk", "einet89@gmail.com", "jtkim@postech.ac.kr", "adamk@robots.ox.ac.uk", "seungjin@postech.ac.kr", "y.w.teh@stats.ox.ac.uk"], "authors": ["Juho Lee", "Yoonho Lee", "Jungtaek Kim", "Adam R. Kosiorek", "Seungjin Choi", "Yee Whye Teh"], "TL;DR": "Attention-based neural network to process set-structured data", "pdf": "/pdf/87de8c8193bf9a20d1a2183e4d5c46b0d14feb77.pdf", "paperhash": "lee|set_transformer", "_bibtex": "@misc{\nlee2019set,\ntitle={Set Transformer},\nauthor={Juho Lee and Yoonho Lee and Jungtaek Kim and Adam R. Kosiorek and Seungjin Choi and Yee Whye Teh},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkgnii09Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper656/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617272, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hkgnii09Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper656/Authors", "ICLR.cc/2019/Conference/Paper656/Reviewers", "ICLR.cc/2019/Conference/Paper656/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper656/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper656/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper656/Authors|ICLR.cc/2019/Conference/Paper656/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper656/Reviewers", "ICLR.cc/2019/Conference/Paper656/Authors", "ICLR.cc/2019/Conference/Paper656/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617272}}}, {"id": "BJec0UPRhm", "original": null, "number": 3, "cdate": 1541465809754, "ddate": null, "tcdate": 1541465809754, "tmdate": 1541533801803, "tddate": null, "forum": "Hkgnii09Ym", "replyto": "Hkgnii09Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper656/Official_Review", "content": {"title": "A good paper but need some clarifications and improvements", "review": "This paper presented an attention-based neural network, namely set transformer, a new neural model \nbased on original transformer designed for set inputs. The basic idea is to introduce the attention\nmechanism in both learning the feature embeddings of the set inputs during \u201cencoding\u201d and aggregating \nthese embeddings during \u201cdecoding\u201d. The paper is written clearly and well motivated. The extensive \nset of experiments were conducted to demonstrate the effectiveness of the proposed method. In general, \nI like reading this paper but there are some limitations or unclear parts I need authors to clarify\nand explain. \n\ni) The proposed architecture is mainly adopted from the original transformer but it is highly related\nto the baselines used in the experiments. For instance, it seems like that the current set \ntransformer is a simple combination of Yang et al.(2018) and Mishra et al.(2018) (using Stack of\nSABs) in encoder side and of Ilse et al.(2018) (using PMA and stack of SABs) in the decoder side. \nThis simple combination makes the novelty of this paper unclear. I would like authors to clarify \nmore on the originality w.r.t. these previous works. \n\nii) Although authors proposed a variant of SABs - ISABs using landmark points to accelerate the \ncomputation, there are no any runtime comparisons between SABs and ISABs by fixing other components. \nIt would be interesting to see that ISABs can approach the performance SABs and how it approaches it. \nFor instance, shall we expect that ISABs approach the performance of SABs when increasing the number\nof landmark points (inducing points)? Since in practice most of datasets are relatedly large, I think\nunderstanding the behavior of ISABs is a more interesting problem. \n\niii) After seeing the results in table 6, I have quite concerned about the practical performance of\nset transformer on relatively large datasets (like 1000 points each class in the settings.) It looks\nto me that not only set transformer may have computational issues to scale up, but more importantly\nthat when encoder learned really expressive embeddings with a relatively large number of the set \ninputs it might be little need to leverage attention in pooling anymore. I would like authors to \nconduct some other experiments on relatively large datasets to verify this hypothesis, which is \nimportant for the practical applications of the proposed model. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper656/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Set Transformer", "abstract": "Many machine learning tasks such as multiple instance learning, 3D shape recognition and few-shot image classification are defined on sets of instances. Since solutions to such problems do not depend on the permutation of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating increased performance compared to recent methods for set-structured data.", "keywords": ["attention", "meta-learning", "set-input neural networks", "permutation invariant modeling"], "authorids": ["juho.lee@stats.ox.ac.uk", "einet89@gmail.com", "jtkim@postech.ac.kr", "adamk@robots.ox.ac.uk", "seungjin@postech.ac.kr", "y.w.teh@stats.ox.ac.uk"], "authors": ["Juho Lee", "Yoonho Lee", "Jungtaek Kim", "Adam R. Kosiorek", "Seungjin Choi", "Yee Whye Teh"], "TL;DR": "Attention-based neural network to process set-structured data", "pdf": "/pdf/87de8c8193bf9a20d1a2183e4d5c46b0d14feb77.pdf", "paperhash": "lee|set_transformer", "_bibtex": "@misc{\nlee2019set,\ntitle={Set Transformer},\nauthor={Juho Lee and Yoonho Lee and Jungtaek Kim and Adam R. Kosiorek and Seungjin Choi and Yee Whye Teh},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkgnii09Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper656/Official_Review", "cdate": 1542234409912, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hkgnii09Ym", "replyto": "Hkgnii09Ym", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper656/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335773608, "tmdate": 1552335773608, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper656/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1eH_an927", "original": null, "number": 2, "cdate": 1541225836691, "ddate": null, "tcdate": 1541225836691, "tmdate": 1541533801599, "tddate": null, "forum": "Hkgnii09Ym", "replyto": "Hkgnii09Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper656/Official_Review", "content": {"title": "Interesting paper that uses attention for set inputs but needs more ablation study", "review": "The paper proposes several variants of attention-based algorithms for set inputs. Compared with previous approach that processes each instance separately and then pooling, the proposed algorithm models the interactions among the instances within the set and performs better on tasks where such properties are important.\n\nThe experiments seem promising. The paper compares SAB and ISAB to rFF + pooling over multiple different tasks and SAB and ISAB outperform rFF + pooling in many tasks.\n\nOne drawback of the paper which limits its significance is that there are seemingly too many components and it is not clear which components are most important and which are not unnecessary. The authors can conduct some ablation study by removing some components and compare the performance to understand which parts are essential to the improvements.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper656/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Set Transformer", "abstract": "Many machine learning tasks such as multiple instance learning, 3D shape recognition and few-shot image classification are defined on sets of instances. Since solutions to such problems do not depend on the permutation of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating increased performance compared to recent methods for set-structured data.", "keywords": ["attention", "meta-learning", "set-input neural networks", "permutation invariant modeling"], "authorids": ["juho.lee@stats.ox.ac.uk", "einet89@gmail.com", "jtkim@postech.ac.kr", "adamk@robots.ox.ac.uk", "seungjin@postech.ac.kr", "y.w.teh@stats.ox.ac.uk"], "authors": ["Juho Lee", "Yoonho Lee", "Jungtaek Kim", "Adam R. Kosiorek", "Seungjin Choi", "Yee Whye Teh"], "TL;DR": "Attention-based neural network to process set-structured data", "pdf": "/pdf/87de8c8193bf9a20d1a2183e4d5c46b0d14feb77.pdf", "paperhash": "lee|set_transformer", "_bibtex": "@misc{\nlee2019set,\ntitle={Set Transformer},\nauthor={Juho Lee and Yoonho Lee and Jungtaek Kim and Adam R. Kosiorek and Seungjin Choi and Yee Whye Teh},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkgnii09Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper656/Official_Review", "cdate": 1542234409912, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hkgnii09Ym", "replyto": "Hkgnii09Ym", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper656/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335773608, "tmdate": 1552335773608, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper656/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 10}