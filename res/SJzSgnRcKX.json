{"notes": [{"id": "SJzSgnRcKX", "original": "S1g4wRncYX", "number": 1073, "cdate": 1538087917134, "ddate": null, "tcdate": 1538087917134, "tmdate": 1551054046946, "tddate": null, "forum": "SJzSgnRcKX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "What do you learn from context? Probing for sentence structure in contextualized word representations", "abstract": "Contextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline.", "keywords": ["natural language processing", "word embeddings", "transfer learning", "interpretability"], "authorids": ["iftenney@google.com", "paxia@cs.jhu.edu", "bchen6@swarthmore.edu", "alexwang@nyu.edu", "azpoliak@cs.jhu.edu", "tom.mccoy@jhu.edu", "n.kim@jhu.edu", "vandurme@cs.jhu.edu", "bowman@nyu.edu", "dipanjand@google.com", "ellie_pavlick@brown.edu"], "authors": ["Ian Tenney", "Patrick Xia", "Berlin Chen", "Alex Wang", "Adam Poliak", "R Thomas McCoy", "Najoung Kim", "Benjamin Van Durme", "Samuel R. Bowman", "Dipanjan Das", "Ellie Pavlick"], "TL;DR": "We probe for sentence structure in ELMo and related contextual embedding models. We find existing models efficiently encode syntax and show evidence of long-range dependencies, but only offer small improvements on semantic tasks.", "pdf": "/pdf/212aeed26e7ac38c925bad565473529502ee88e2.pdf", "paperhash": "tenney|what_do_you_learn_from_context_probing_for_sentence_structure_in_contextualized_word_representations", "_bibtex": "@inproceedings{\ntenney2018what,\ntitle={What do you learn from context? Probing for sentence structure in contextualized word representations},\nauthor={Ian Tenney and Patrick Xia and Berlin Chen and Alex Wang and Adam Poliak and R Thomas McCoy and Najoung Kim and Benjamin Van Durme and Sam Bowman and Dipanjan Das and Ellie Pavlick},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJzSgnRcKX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "Syes7Emgl4", "original": null, "number": 1, "cdate": 1544725538842, "ddate": null, "tcdate": 1544725538842, "tmdate": 1545354510818, "tddate": null, "forum": "SJzSgnRcKX", "replyto": "SJzSgnRcKX", "invitation": "ICLR.cc/2019/Conference/-/Paper1073/Meta_Review", "content": {"metareview": "Pros\n\n- Thorough analysis on a large number of diverse tasks\n- Extending the probing technique typically applied to individual encoder states to testing for presence of certain (linguistic) information based on pairs of encoders states (corresponding to pairs of words)\n- The comparison can be useful when deciding which representations to use for a given task\n\nCons\n\n- Nothing serious, it is solid and important empirical study\n\nThe reviewers are in consensus.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Accept (Poster)", "title": "A thorough study of contextualized word representations"}, "signatures": ["ICLR.cc/2019/Conference/Paper1073/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1073/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What do you learn from context? Probing for sentence structure in contextualized word representations", "abstract": "Contextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline.", "keywords": ["natural language processing", "word embeddings", "transfer learning", "interpretability"], "authorids": ["iftenney@google.com", "paxia@cs.jhu.edu", "bchen6@swarthmore.edu", "alexwang@nyu.edu", "azpoliak@cs.jhu.edu", "tom.mccoy@jhu.edu", "n.kim@jhu.edu", "vandurme@cs.jhu.edu", "bowman@nyu.edu", "dipanjand@google.com", "ellie_pavlick@brown.edu"], "authors": ["Ian Tenney", "Patrick Xia", "Berlin Chen", "Alex Wang", "Adam Poliak", "R Thomas McCoy", "Najoung Kim", "Benjamin Van Durme", "Samuel R. Bowman", "Dipanjan Das", "Ellie Pavlick"], "TL;DR": "We probe for sentence structure in ELMo and related contextual embedding models. We find existing models efficiently encode syntax and show evidence of long-range dependencies, but only offer small improvements on semantic tasks.", "pdf": "/pdf/212aeed26e7ac38c925bad565473529502ee88e2.pdf", "paperhash": "tenney|what_do_you_learn_from_context_probing_for_sentence_structure_in_contextualized_word_representations", "_bibtex": "@inproceedings{\ntenney2018what,\ntitle={What do you learn from context? Probing for sentence structure in contextualized word representations},\nauthor={Ian Tenney and Patrick Xia and Berlin Chen and Alex Wang and Adam Poliak and R Thomas McCoy and Najoung Kim and Benjamin Van Durme and Sam Bowman and Dipanjan Das and Ellie Pavlick},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJzSgnRcKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1073/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352975307, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJzSgnRcKX", "replyto": "SJzSgnRcKX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1073/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1073/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1073/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352975307}}}, {"id": "SkxaqkptCX", "original": null, "number": 4, "cdate": 1543258004829, "ddate": null, "tcdate": 1543258004829, "tmdate": 1543258004829, "tddate": null, "forum": "SJzSgnRcKX", "replyto": "Skey6mqlA7", "invitation": "ICLR.cc/2019/Conference/-/Paper1073/Official_Comment", "content": {"title": "Thanks for the reference", "comment": "This is definitely related; we'll be sure to add a citation!"}, "signatures": ["ICLR.cc/2019/Conference/Paper1073/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1073/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1073/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What do you learn from context? Probing for sentence structure in contextualized word representations", "abstract": "Contextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline.", "keywords": ["natural language processing", "word embeddings", "transfer learning", "interpretability"], "authorids": ["iftenney@google.com", "paxia@cs.jhu.edu", "bchen6@swarthmore.edu", "alexwang@nyu.edu", "azpoliak@cs.jhu.edu", "tom.mccoy@jhu.edu", "n.kim@jhu.edu", "vandurme@cs.jhu.edu", "bowman@nyu.edu", "dipanjand@google.com", "ellie_pavlick@brown.edu"], "authors": ["Ian Tenney", "Patrick Xia", "Berlin Chen", "Alex Wang", "Adam Poliak", "R Thomas McCoy", "Najoung Kim", "Benjamin Van Durme", "Samuel R. Bowman", "Dipanjan Das", "Ellie Pavlick"], "TL;DR": "We probe for sentence structure in ELMo and related contextual embedding models. We find existing models efficiently encode syntax and show evidence of long-range dependencies, but only offer small improvements on semantic tasks.", "pdf": "/pdf/212aeed26e7ac38c925bad565473529502ee88e2.pdf", "paperhash": "tenney|what_do_you_learn_from_context_probing_for_sentence_structure_in_contextualized_word_representations", "_bibtex": "@inproceedings{\ntenney2018what,\ntitle={What do you learn from context? Probing for sentence structure in contextualized word representations},\nauthor={Ian Tenney and Patrick Xia and Berlin Chen and Alex Wang and Adam Poliak and R Thomas McCoy and Najoung Kim and Benjamin Van Durme and Sam Bowman and Dipanjan Das and Ellie Pavlick},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJzSgnRcKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1073/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609698, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJzSgnRcKX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1073/Authors", "ICLR.cc/2019/Conference/Paper1073/Reviewers", "ICLR.cc/2019/Conference/Paper1073/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1073/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1073/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1073/Authors|ICLR.cc/2019/Conference/Paper1073/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1073/Reviewers", "ICLR.cc/2019/Conference/Paper1073/Authors", "ICLR.cc/2019/Conference/Paper1073/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609698}}}, {"id": "Skey6mqlA7", "original": null, "number": 1, "cdate": 1542656951092, "ddate": null, "tcdate": 1542656951092, "tmdate": 1542657000326, "tddate": null, "forum": "SJzSgnRcKX", "replyto": "SJzSgnRcKX", "invitation": "ICLR.cc/2019/Conference/-/Paper1073/Public_Comment", "content": {"comment": "Just wanted to mention a related work: Yonatan Belinkov's thesis ( http://people.csail.mit.edu/belinkov/assets/pdf/thesis2018.pdf ) has some prior experiments with the edge probing task design outlined in this paper. See Chapter 4, \"Sentence Structure and Neural Machine Translation: Word Relations\".", "title": "Some previous work on edge probing"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What do you learn from context? Probing for sentence structure in contextualized word representations", "abstract": "Contextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline.", "keywords": ["natural language processing", "word embeddings", "transfer learning", "interpretability"], "authorids": ["iftenney@google.com", "paxia@cs.jhu.edu", "bchen6@swarthmore.edu", "alexwang@nyu.edu", "azpoliak@cs.jhu.edu", "tom.mccoy@jhu.edu", "n.kim@jhu.edu", "vandurme@cs.jhu.edu", "bowman@nyu.edu", "dipanjand@google.com", "ellie_pavlick@brown.edu"], "authors": ["Ian Tenney", "Patrick Xia", "Berlin Chen", "Alex Wang", "Adam Poliak", "R Thomas McCoy", "Najoung Kim", "Benjamin Van Durme", "Samuel R. Bowman", "Dipanjan Das", "Ellie Pavlick"], "TL;DR": "We probe for sentence structure in ELMo and related contextual embedding models. We find existing models efficiently encode syntax and show evidence of long-range dependencies, but only offer small improvements on semantic tasks.", "pdf": "/pdf/212aeed26e7ac38c925bad565473529502ee88e2.pdf", "paperhash": "tenney|what_do_you_learn_from_context_probing_for_sentence_structure_in_contextualized_word_representations", "_bibtex": "@inproceedings{\ntenney2018what,\ntitle={What do you learn from context? Probing for sentence structure in contextualized word representations},\nauthor={Ian Tenney and Patrick Xia and Berlin Chen and Alex Wang and Adam Poliak and R Thomas McCoy and Najoung Kim and Benjamin Van Durme and Sam Bowman and Dipanjan Das and Ellie Pavlick},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJzSgnRcKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1073/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311685239, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "SJzSgnRcKX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1073/Authors", "ICLR.cc/2019/Conference/Paper1073/Reviewers", "ICLR.cc/2019/Conference/Paper1073/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1073/Authors", "ICLR.cc/2019/Conference/Paper1073/Reviewers", "ICLR.cc/2019/Conference/Paper1073/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311685239}}}, {"id": "SJlErExtT7", "original": null, "number": 3, "cdate": 1542157371793, "ddate": null, "tcdate": 1542157371793, "tmdate": 1542157371793, "tddate": null, "forum": "SJzSgnRcKX", "replyto": "rJx0hgbKnX", "invitation": "ICLR.cc/2019/Conference/-/Paper1073/Official_Comment", "content": {"title": "Author response", "comment": "Thank you for the review!\n\nWe agree that it would be interesting to explore more specific tail phenomena. Attachment phenomena in particular can be studied on many of the same datasets if we fix labels and instead predict one of the two spans; this would be an interesting direction for future study.\n\nIt would also be very interesting to explore other languages! While we are limited by available data and encoder models, there\u2019s nothing in the edge probing technique that makes English-specific assumptions. Probing for phenomena that require long contexts could be a good test of advanced encoders, and can be easily quantified in our framework (for example, see Figure 3)."}, "signatures": ["ICLR.cc/2019/Conference/Paper1073/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1073/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1073/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What do you learn from context? Probing for sentence structure in contextualized word representations", "abstract": "Contextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline.", "keywords": ["natural language processing", "word embeddings", "transfer learning", "interpretability"], "authorids": ["iftenney@google.com", "paxia@cs.jhu.edu", "bchen6@swarthmore.edu", "alexwang@nyu.edu", "azpoliak@cs.jhu.edu", "tom.mccoy@jhu.edu", "n.kim@jhu.edu", "vandurme@cs.jhu.edu", "bowman@nyu.edu", "dipanjand@google.com", "ellie_pavlick@brown.edu"], "authors": ["Ian Tenney", "Patrick Xia", "Berlin Chen", "Alex Wang", "Adam Poliak", "R Thomas McCoy", "Najoung Kim", "Benjamin Van Durme", "Samuel R. Bowman", "Dipanjan Das", "Ellie Pavlick"], "TL;DR": "We probe for sentence structure in ELMo and related contextual embedding models. We find existing models efficiently encode syntax and show evidence of long-range dependencies, but only offer small improvements on semantic tasks.", "pdf": "/pdf/212aeed26e7ac38c925bad565473529502ee88e2.pdf", "paperhash": "tenney|what_do_you_learn_from_context_probing_for_sentence_structure_in_contextualized_word_representations", "_bibtex": "@inproceedings{\ntenney2018what,\ntitle={What do you learn from context? Probing for sentence structure in contextualized word representations},\nauthor={Ian Tenney and Patrick Xia and Berlin Chen and Alex Wang and Adam Poliak and R Thomas McCoy and Najoung Kim and Benjamin Van Durme and Sam Bowman and Dipanjan Das and Ellie Pavlick},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJzSgnRcKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1073/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609698, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJzSgnRcKX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1073/Authors", "ICLR.cc/2019/Conference/Paper1073/Reviewers", "ICLR.cc/2019/Conference/Paper1073/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1073/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1073/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1073/Authors|ICLR.cc/2019/Conference/Paper1073/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1073/Reviewers", "ICLR.cc/2019/Conference/Paper1073/Authors", "ICLR.cc/2019/Conference/Paper1073/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609698}}}, {"id": "HklxVExYpQ", "original": null, "number": 2, "cdate": 1542157352181, "ddate": null, "tcdate": 1542157352181, "tmdate": 1542157352181, "tddate": null, "forum": "SJzSgnRcKX", "replyto": "SJgb3cQF2Q", "invitation": "ICLR.cc/2019/Conference/-/Paper1073/Official_Comment", "content": {"title": "Author response", "comment": "Thank you for the review! \n\nWe\u2019re very interested in probing for other linguistic attributes - while we present a broad analysis in this paper, there\u2019s certainly room to use edge probing to study more focused phenomena like PP attachment or ambiguities between specific semantic roles. We use a standardized data format that makes it easy to add new tasks, and we hope that our code release will be a useful platform for this kind of analysis.\n\nWe\u2019ll be sure to update the text to more clearly describe the tables.\n\nWhoops! In Figure 2 and 3, the bars/bands are 95% confidence intervals calculated using the Normal approximation. We wanted to emphasize that the SPR and Winograd datasets are quite small and that the differences between models are often not significant. We\u2019ll add this to the caption in the final version."}, "signatures": ["ICLR.cc/2019/Conference/Paper1073/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1073/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1073/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What do you learn from context? Probing for sentence structure in contextualized word representations", "abstract": "Contextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline.", "keywords": ["natural language processing", "word embeddings", "transfer learning", "interpretability"], "authorids": ["iftenney@google.com", "paxia@cs.jhu.edu", "bchen6@swarthmore.edu", "alexwang@nyu.edu", "azpoliak@cs.jhu.edu", "tom.mccoy@jhu.edu", "n.kim@jhu.edu", "vandurme@cs.jhu.edu", "bowman@nyu.edu", "dipanjand@google.com", "ellie_pavlick@brown.edu"], "authors": ["Ian Tenney", "Patrick Xia", "Berlin Chen", "Alex Wang", "Adam Poliak", "R Thomas McCoy", "Najoung Kim", "Benjamin Van Durme", "Samuel R. Bowman", "Dipanjan Das", "Ellie Pavlick"], "TL;DR": "We probe for sentence structure in ELMo and related contextual embedding models. We find existing models efficiently encode syntax and show evidence of long-range dependencies, but only offer small improvements on semantic tasks.", "pdf": "/pdf/212aeed26e7ac38c925bad565473529502ee88e2.pdf", "paperhash": "tenney|what_do_you_learn_from_context_probing_for_sentence_structure_in_contextualized_word_representations", "_bibtex": "@inproceedings{\ntenney2018what,\ntitle={What do you learn from context? Probing for sentence structure in contextualized word representations},\nauthor={Ian Tenney and Patrick Xia and Berlin Chen and Alex Wang and Adam Poliak and R Thomas McCoy and Najoung Kim and Benjamin Van Durme and Sam Bowman and Dipanjan Das and Ellie Pavlick},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJzSgnRcKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1073/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609698, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJzSgnRcKX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1073/Authors", "ICLR.cc/2019/Conference/Paper1073/Reviewers", "ICLR.cc/2019/Conference/Paper1073/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1073/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1073/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1073/Authors|ICLR.cc/2019/Conference/Paper1073/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1073/Reviewers", "ICLR.cc/2019/Conference/Paper1073/Authors", "ICLR.cc/2019/Conference/Paper1073/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609698}}}, {"id": "SJekzNxtaX", "original": null, "number": 1, "cdate": 1542157318588, "ddate": null, "tcdate": 1542157318588, "tmdate": 1542157318588, "tddate": null, "forum": "SJzSgnRcKX", "replyto": "rylJ-ovR2X", "invitation": "ICLR.cc/2019/Conference/-/Paper1073/Official_Comment", "content": {"title": "Author response", "comment": "Thank you for the review! We do hope that this will be of broad interest given recent progress in sentence representations, and hope that our code release will allow continued evaluation of new and better representation models (like BERT).\n\nWe\u2019ll certainly include examples of specific win / loss cases in the final version. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1073/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1073/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1073/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What do you learn from context? Probing for sentence structure in contextualized word representations", "abstract": "Contextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline.", "keywords": ["natural language processing", "word embeddings", "transfer learning", "interpretability"], "authorids": ["iftenney@google.com", "paxia@cs.jhu.edu", "bchen6@swarthmore.edu", "alexwang@nyu.edu", "azpoliak@cs.jhu.edu", "tom.mccoy@jhu.edu", "n.kim@jhu.edu", "vandurme@cs.jhu.edu", "bowman@nyu.edu", "dipanjand@google.com", "ellie_pavlick@brown.edu"], "authors": ["Ian Tenney", "Patrick Xia", "Berlin Chen", "Alex Wang", "Adam Poliak", "R Thomas McCoy", "Najoung Kim", "Benjamin Van Durme", "Samuel R. Bowman", "Dipanjan Das", "Ellie Pavlick"], "TL;DR": "We probe for sentence structure in ELMo and related contextual embedding models. We find existing models efficiently encode syntax and show evidence of long-range dependencies, but only offer small improvements on semantic tasks.", "pdf": "/pdf/212aeed26e7ac38c925bad565473529502ee88e2.pdf", "paperhash": "tenney|what_do_you_learn_from_context_probing_for_sentence_structure_in_contextualized_word_representations", "_bibtex": "@inproceedings{\ntenney2018what,\ntitle={What do you learn from context? Probing for sentence structure in contextualized word representations},\nauthor={Ian Tenney and Patrick Xia and Berlin Chen and Alex Wang and Adam Poliak and R Thomas McCoy and Najoung Kim and Benjamin Van Durme and Sam Bowman and Dipanjan Das and Ellie Pavlick},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJzSgnRcKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1073/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609698, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJzSgnRcKX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1073/Authors", "ICLR.cc/2019/Conference/Paper1073/Reviewers", "ICLR.cc/2019/Conference/Paper1073/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1073/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1073/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1073/Authors|ICLR.cc/2019/Conference/Paper1073/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1073/Reviewers", "ICLR.cc/2019/Conference/Paper1073/Authors", "ICLR.cc/2019/Conference/Paper1073/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609698}}}, {"id": "rylJ-ovR2X", "original": null, "number": 3, "cdate": 1541466871437, "ddate": null, "tcdate": 1541466871437, "tmdate": 1541533446320, "tddate": null, "forum": "SJzSgnRcKX", "replyto": "SJzSgnRcKX", "invitation": "ICLR.cc/2019/Conference/-/Paper1073/Official_Review", "content": {"title": "Nice empirical paper", "review": "\nThis is a nice paper that attempts to tease apart some questions about the effectiveness of contextual word embeddings (ELMo, CoVe, and the Transformer LM). The main question is about the value of context in these representations, and in particular how their ability to encode context allows them to also (implicitly) represent linguistic properties of words. What I really like about the paper is the \u201cEdge probing\u201d method it introduces. The idea is to probe the representations using diagnostic classifiers\u2014something that\u2019s already widespread practice\u2014but to focus on the relationship between spans rather than individual words. This is really nice because it enables them to look at more than just tagging problems: the paper looks at syntactic constituency, dependencies, entity labels, and semantic role labeling. I think the combination of an interesting research question and a new method (which will probably be picked up by others working in this area) make this a strong candidate for ICLR. The paper is well-written and experimentally thorough.\n\nNitpick: It would be nice to see some examples of cases where the edge probe is correct, and where it isn\u2019t.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1073/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What do you learn from context? Probing for sentence structure in contextualized word representations", "abstract": "Contextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline.", "keywords": ["natural language processing", "word embeddings", "transfer learning", "interpretability"], "authorids": ["iftenney@google.com", "paxia@cs.jhu.edu", "bchen6@swarthmore.edu", "alexwang@nyu.edu", "azpoliak@cs.jhu.edu", "tom.mccoy@jhu.edu", "n.kim@jhu.edu", "vandurme@cs.jhu.edu", "bowman@nyu.edu", "dipanjand@google.com", "ellie_pavlick@brown.edu"], "authors": ["Ian Tenney", "Patrick Xia", "Berlin Chen", "Alex Wang", "Adam Poliak", "R Thomas McCoy", "Najoung Kim", "Benjamin Van Durme", "Samuel R. Bowman", "Dipanjan Das", "Ellie Pavlick"], "TL;DR": "We probe for sentence structure in ELMo and related contextual embedding models. We find existing models efficiently encode syntax and show evidence of long-range dependencies, but only offer small improvements on semantic tasks.", "pdf": "/pdf/212aeed26e7ac38c925bad565473529502ee88e2.pdf", "paperhash": "tenney|what_do_you_learn_from_context_probing_for_sentence_structure_in_contextualized_word_representations", "_bibtex": "@inproceedings{\ntenney2018what,\ntitle={What do you learn from context? Probing for sentence structure in contextualized word representations},\nauthor={Ian Tenney and Patrick Xia and Berlin Chen and Alex Wang and Adam Poliak and R Thomas McCoy and Najoung Kim and Benjamin Van Durme and Sam Bowman and Dipanjan Das and Ellie Pavlick},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJzSgnRcKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1073/Official_Review", "cdate": 1542234312202, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SJzSgnRcKX", "replyto": "SJzSgnRcKX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1073/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335866873, "tmdate": 1552335866873, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1073/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SJgb3cQF2Q", "original": null, "number": 2, "cdate": 1541122728715, "ddate": null, "tcdate": 1541122728715, "tmdate": 1541533446110, "tddate": null, "forum": "SJzSgnRcKX", "replyto": "SJzSgnRcKX", "invitation": "ICLR.cc/2019/Conference/-/Paper1073/Official_Review", "content": {"title": "Nice discussion of what type of information is actually encoded by contextualized word embeddings", "review": "This paper provides new insights on what is captured contextualized word embeddings by compiling a set of \u201cedge probing\u201d tasks.  This is not the first paper to attempt this type of analysis, but the results seem pretty thorough and cover a wider range of tasks than some similar previous works.  The findings in this paper are very timely and relevant given the increasing usage of these types of embeddings.  I imagine that the edge probing tasks could be extended towards looking for other linguistic attributes getting encoded in these embeddings.\n\nQuestions & other remarks:\n-The discussion of the tables and graphs in the running text feels a bit condensed and at times unclear about which rows are being referred to.\n-In figures 2 & 3: what are the tinted areas around the lines signifying here? Standard deviation?  Standard error?  Confidence intervals?\n-It seems the orthonormal encoder actually outperforms the full elmo model with the learned weights on the Winograd Schema.  Can the authors comment on this a bit more?\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1073/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What do you learn from context? Probing for sentence structure in contextualized word representations", "abstract": "Contextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline.", "keywords": ["natural language processing", "word embeddings", "transfer learning", "interpretability"], "authorids": ["iftenney@google.com", "paxia@cs.jhu.edu", "bchen6@swarthmore.edu", "alexwang@nyu.edu", "azpoliak@cs.jhu.edu", "tom.mccoy@jhu.edu", "n.kim@jhu.edu", "vandurme@cs.jhu.edu", "bowman@nyu.edu", "dipanjand@google.com", "ellie_pavlick@brown.edu"], "authors": ["Ian Tenney", "Patrick Xia", "Berlin Chen", "Alex Wang", "Adam Poliak", "R Thomas McCoy", "Najoung Kim", "Benjamin Van Durme", "Samuel R. Bowman", "Dipanjan Das", "Ellie Pavlick"], "TL;DR": "We probe for sentence structure in ELMo and related contextual embedding models. We find existing models efficiently encode syntax and show evidence of long-range dependencies, but only offer small improvements on semantic tasks.", "pdf": "/pdf/212aeed26e7ac38c925bad565473529502ee88e2.pdf", "paperhash": "tenney|what_do_you_learn_from_context_probing_for_sentence_structure_in_contextualized_word_representations", "_bibtex": "@inproceedings{\ntenney2018what,\ntitle={What do you learn from context? Probing for sentence structure in contextualized word representations},\nauthor={Ian Tenney and Patrick Xia and Berlin Chen and Alex Wang and Adam Poliak and R Thomas McCoy and Najoung Kim and Benjamin Van Durme and Sam Bowman and Dipanjan Das and Ellie Pavlick},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJzSgnRcKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1073/Official_Review", "cdate": 1542234312202, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SJzSgnRcKX", "replyto": "SJzSgnRcKX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1073/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335866873, "tmdate": 1552335866873, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1073/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJx0hgbKnX", "original": null, "number": 1, "cdate": 1541111989687, "ddate": null, "tcdate": 1541111989687, "tmdate": 1541533445896, "tddate": null, "forum": "SJzSgnRcKX", "replyto": "SJzSgnRcKX", "invitation": "ICLR.cc/2019/Conference/-/Paper1073/Official_Review", "content": {"title": "Current work reps capture a surprising amount of structure", "review": "I have no major complaints with this work.  It is well presented and easily understandable. I agree with the claim that the largest gains are largely syntactic, but this leads me to wonder about more tail phenomena.   PP attachment is a classic example of a syntactic decision requiring semantics, but one could also imagine doing a CCG supertagging analysis to see how well the model captures specific long-tail phenomena.  Though a very different task Vaswani et al 16, for example, showed how bi-LSTMs were necessary for certain constructions (presumably current models would perform much better and may capture this information already).\n\nAn important caveat of these results is that the evaluation (by necessity) is occurring in English.  Discourse in a pro-drop language would presumably require longer contexts than many of these approaches currently handle.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1073/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What do you learn from context? Probing for sentence structure in contextualized word representations", "abstract": "Contextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline.", "keywords": ["natural language processing", "word embeddings", "transfer learning", "interpretability"], "authorids": ["iftenney@google.com", "paxia@cs.jhu.edu", "bchen6@swarthmore.edu", "alexwang@nyu.edu", "azpoliak@cs.jhu.edu", "tom.mccoy@jhu.edu", "n.kim@jhu.edu", "vandurme@cs.jhu.edu", "bowman@nyu.edu", "dipanjand@google.com", "ellie_pavlick@brown.edu"], "authors": ["Ian Tenney", "Patrick Xia", "Berlin Chen", "Alex Wang", "Adam Poliak", "R Thomas McCoy", "Najoung Kim", "Benjamin Van Durme", "Samuel R. Bowman", "Dipanjan Das", "Ellie Pavlick"], "TL;DR": "We probe for sentence structure in ELMo and related contextual embedding models. We find existing models efficiently encode syntax and show evidence of long-range dependencies, but only offer small improvements on semantic tasks.", "pdf": "/pdf/212aeed26e7ac38c925bad565473529502ee88e2.pdf", "paperhash": "tenney|what_do_you_learn_from_context_probing_for_sentence_structure_in_contextualized_word_representations", "_bibtex": "@inproceedings{\ntenney2018what,\ntitle={What do you learn from context? Probing for sentence structure in contextualized word representations},\nauthor={Ian Tenney and Patrick Xia and Berlin Chen and Alex Wang and Adam Poliak and R Thomas McCoy and Najoung Kim and Benjamin Van Durme and Sam Bowman and Dipanjan Das and Ellie Pavlick},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJzSgnRcKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1073/Official_Review", "cdate": 1542234312202, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SJzSgnRcKX", "replyto": "SJzSgnRcKX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1073/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335866873, "tmdate": 1552335866873, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1073/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 10}