{"notes": [{"id": "SkgsQ8LK_E", "original": "SkgvZcNBv4", "number": 3, "cdate": 1553716771480, "ddate": null, "tcdate": 1553716771480, "tmdate": 1562083041560, "tddate": null, "forum": "SkgsQ8LK_E", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Blind_Submission", "content": {"title": "Disentangling Content and Style via Unsupervised Geometry Distillation", "authors": ["Wayne Wu", "Kaidi Cao", "Cheng Li", "Chen Qian", "Chen Change Loy"], "authorids": ["wuwenyan@sensetime.com", "kaidicao@cs.stanford.edu", "chengli@sensetime.com", "qianchen@sensetime.com", "ccloy225@gmail.com"], "keywords": ["generative models", "unsupervised learning"], "TL;DR": "We present a novel framework to learn the disentangled representation of content and style in a completely unsupervised manner. ", "abstract": "It is challenging to disentangle an object into two orthogonal spaces of content and style since each can influence the visual observation in a different and unpredictable way. It is rare for one to have access to a large number of data to help separate the influences. In this paper, we present a novel framework to learn this disentangled representation in a completely unsupervised manner. We address this problem in a two-branch Autoencoder framework. For the structural content branch, we project the latent factor into a soft structured point tensor and constrain it with losses derived from prior knowledge. This encourages the branch to distill geometry information. Another branch learns the complementary style information. The two branches form an effective framework that can disentangle object's content-style representation without any human annotation. We evaluate our approach on four image datasets, on which we demonstrate the superior disentanglement and visual analogy quality both in synthesized and real-world data. We are able to generate photo-realistic images with 256x256 resolution that are clearly disentangled in content and style.", "pdf": "/pdf/5923b237e3b1e4972dc910f5c564fcf42c5ffeb4.pdf", "paperhash": "wu|disentangling_content_and_style_via_unsupervised_geometry_distillation"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Blind_Submission", "cdate": 1547567085825, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": [".*"]}, "writers": {"values": ["ICLR.cc/2019/Workshop/DeepGenStruct"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/DeepGenStruct"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1547567085825, "tmdate": 1555704438520, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"]}}, "tauthor": "OpenReview.net"}, {"id": "SyghDeIM94", "original": null, "number": 2, "cdate": 1555353700183, "ddate": null, "tcdate": 1555353700183, "tmdate": 1556906116872, "tddate": null, "forum": "SkgsQ8LK_E", "replyto": "SkgsQ8LK_E", "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper3/Official_Review", "content": {"title": "interesting model, sloppy presentation with many information gaps", "review": "This paper proposes an unsupervised method for disentangling the content from \nthe style of images. I proposes a 2-branch VAE. The structure branch is \nexplicitly designed to capture geometric properties, and the style branch should \ncapture 'everything else' as style. Extensive  evaluation shows that the model \noutperforms competitors qualitatively and quantitatively on real and artificial \ndata sets.\n\nThe paper proposes an interesting model and presents an impressive diversity\nof evaluation. Unfortunately, the writing is often unclear (and the paper has a\nlarge number of grammar errors) which makes the model description, and especially\nthe evaluation, very hard to follow.\n\nPros\n------\n- interesting and intuitive prior which drives content capturing\n- exhaustive evaluation; good effort to capture results quantitatively\n- ablation study showing benefit of different model components\n\nCons\n------\n- quality of writing; many grammar mistakes (specifically 2.3 and 2.4); \nincomplete sentences (e.g., beginning of 2.3; last paragraph on page 2)\n- many details left out (e.g., in model description and evaluation), model description\n is hard to follow at places\n- evaluation insufficiently described; many tasks are not comprehensible form \nthe provided information.\n\nDetailed Comments\n------------------\n- a motivation of why the task of content and style disentanglement is useful \n(or interesting) would improve the paper. Relatedly, Figure 1 is very much left \nout of context until the experiment section; it could be used to provide the \nmotivation, and should be explained in more detail in the intro, or moved to a \nlater spot.\n- The prior loss description is very dense. The hour glass network and the \nlandmark mapping process should be explained in detail. It does not become clear \nif this is standard methodology or a contribution of this work. \n- an Explanation of hourglass networks should be added. \n- the description of the KL Loss penalty formulation (second part of 2.3) is \nvery dense and hard to follow\n- many details for the evaluation are missing, e.g., regarding SSIM and IS \nscores\n- The figures and tables should be rearranged so that they appear close to where \nthey are discussed in text. A single table should not include results from \ndifferent experiments (as done in Table 2)\n- Retrieval experiments should include comparison against a baseline model and \ncompetitive comparison models to put the proposed models' scores into context\n- The Comparison evaluation (in 4.2) is insufficiently described. What is the \nazimuth factor? From Figure 5 the difference in performance between the systems \nis not obvious. What do the models predict? What changes along the rows?\n- Last paragraph on p. 6 refers to Figure 7, you probably mean Figure 1.\n- good intuitive explanation of geometry prior in last paragraph page 6 (\"The \nlearnt structural heatmaps...\") This explanation would be useful to have earlier \nin the paper.\n- Will the 18-point landmark annotated data be made available?\n- last paragraph of page 7 is very hard to follow", "rating": "2: Marginally below acceptance threshold", "confidence": "1: The reviewer's evaluation is an educated guess"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper3/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper3/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangling Content and Style via Unsupervised Geometry Distillation", "authors": ["Wayne Wu", "Kaidi Cao", "Cheng Li", "Chen Qian", "Chen Change Loy"], "authorids": ["wuwenyan@sensetime.com", "kaidicao@cs.stanford.edu", "chengli@sensetime.com", "qianchen@sensetime.com", "ccloy225@gmail.com"], "keywords": ["generative models", "unsupervised learning"], "TL;DR": "We present a novel framework to learn the disentangled representation of content and style in a completely unsupervised manner. ", "abstract": "It is challenging to disentangle an object into two orthogonal spaces of content and style since each can influence the visual observation in a different and unpredictable way. It is rare for one to have access to a large number of data to help separate the influences. In this paper, we present a novel framework to learn this disentangled representation in a completely unsupervised manner. We address this problem in a two-branch Autoencoder framework. For the structural content branch, we project the latent factor into a soft structured point tensor and constrain it with losses derived from prior knowledge. This encourages the branch to distill geometry information. Another branch learns the complementary style information. The two branches form an effective framework that can disentangle object's content-style representation without any human annotation. We evaluate our approach on four image datasets, on which we demonstrate the superior disentanglement and visual analogy quality both in synthesized and real-world data. We are able to generate photo-realistic images with 256x256 resolution that are clearly disentangled in content and style.", "pdf": "/pdf/5923b237e3b1e4972dc910f5c564fcf42c5ffeb4.pdf", "paperhash": "wu|disentangling_content_and_style_via_unsupervised_geometry_distillation"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper3/Official_Review", "cdate": 1554234181608, "reply": {"forum": "SkgsQ8LK_E", "replyto": "SkgsQ8LK_E", "readers": [".*"], "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper3/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper3/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1554234181608, "tmdate": 1556906083499, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper3/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"writable": true}}}}, {"id": "r1xb8L_b5E", "original": null, "number": 1, "cdate": 1555297864536, "ddate": null, "tcdate": 1555297864536, "tmdate": 1556906116619, "tddate": null, "forum": "SkgsQ8LK_E", "replyto": "SkgsQ8LK_E", "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper3/Official_Review", "content": {"title": "Two-Branch autoencoder model for simultaneous content and style unsupervised learning", "review": "In this paper, a model based on autoencoder framework is proposed to disentangle object\u2019s representation by content and style in an unsupervised manner. The model contains two branches, one for content and the other for style. The structural content branch looks at the structural points to capture the object geometry, while the style branch learns the style representation. The objective function contains the prior loss, reconstruction loss and the KL loss, which is an extension of the traditional VAE framework. \n\nExperiments show that the proposed model can to some extent produce representations capturing both content and style. In particular, a content representation of the query image and the style representation of the reference one can output an image maintaining the geometric information of the query while having the style of the reference. In terms of quantitative results, the proposed method also outperforms existing methods wrt SSIM and IS score.  ", "rating": "3: Marginally above acceptance threshold", "confidence": "1: The reviewer's evaluation is an educated guess"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper3/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper3/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangling Content and Style via Unsupervised Geometry Distillation", "authors": ["Wayne Wu", "Kaidi Cao", "Cheng Li", "Chen Qian", "Chen Change Loy"], "authorids": ["wuwenyan@sensetime.com", "kaidicao@cs.stanford.edu", "chengli@sensetime.com", "qianchen@sensetime.com", "ccloy225@gmail.com"], "keywords": ["generative models", "unsupervised learning"], "TL;DR": "We present a novel framework to learn the disentangled representation of content and style in a completely unsupervised manner. ", "abstract": "It is challenging to disentangle an object into two orthogonal spaces of content and style since each can influence the visual observation in a different and unpredictable way. It is rare for one to have access to a large number of data to help separate the influences. In this paper, we present a novel framework to learn this disentangled representation in a completely unsupervised manner. We address this problem in a two-branch Autoencoder framework. For the structural content branch, we project the latent factor into a soft structured point tensor and constrain it with losses derived from prior knowledge. This encourages the branch to distill geometry information. Another branch learns the complementary style information. The two branches form an effective framework that can disentangle object's content-style representation without any human annotation. We evaluate our approach on four image datasets, on which we demonstrate the superior disentanglement and visual analogy quality both in synthesized and real-world data. We are able to generate photo-realistic images with 256x256 resolution that are clearly disentangled in content and style.", "pdf": "/pdf/5923b237e3b1e4972dc910f5c564fcf42c5ffeb4.pdf", "paperhash": "wu|disentangling_content_and_style_via_unsupervised_geometry_distillation"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper3/Official_Review", "cdate": 1554234181608, "reply": {"forum": "SkgsQ8LK_E", "replyto": "SkgsQ8LK_E", "readers": [".*"], "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper3/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper3/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1554234181608, "tmdate": 1556906083499, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper3/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"writable": true}}}}, {"id": "ryeIUBYD94", "original": null, "number": 1, "cdate": 1555694925851, "ddate": null, "tcdate": 1555694925851, "tmdate": 1556906116402, "tddate": null, "forum": "SkgsQ8LK_E", "replyto": "SkgsQ8LK_E", "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper3/Decision", "content": {"title": "Acceptance Decision", "decision": "Accept"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangling Content and Style via Unsupervised Geometry Distillation", "authors": ["Wayne Wu", "Kaidi Cao", "Cheng Li", "Chen Qian", "Chen Change Loy"], "authorids": ["wuwenyan@sensetime.com", "kaidicao@cs.stanford.edu", "chengli@sensetime.com", "qianchen@sensetime.com", "ccloy225@gmail.com"], "keywords": ["generative models", "unsupervised learning"], "TL;DR": "We present a novel framework to learn the disentangled representation of content and style in a completely unsupervised manner. ", "abstract": "It is challenging to disentangle an object into two orthogonal spaces of content and style since each can influence the visual observation in a different and unpredictable way. It is rare for one to have access to a large number of data to help separate the influences. In this paper, we present a novel framework to learn this disentangled representation in a completely unsupervised manner. We address this problem in a two-branch Autoencoder framework. For the structural content branch, we project the latent factor into a soft structured point tensor and constrain it with losses derived from prior knowledge. This encourages the branch to distill geometry information. Another branch learns the complementary style information. The two branches form an effective framework that can disentangle object's content-style representation without any human annotation. We evaluate our approach on four image datasets, on which we demonstrate the superior disentanglement and visual analogy quality both in synthesized and real-world data. We are able to generate photo-realistic images with 256x256 resolution that are clearly disentangled in content and style.", "pdf": "/pdf/5923b237e3b1e4972dc910f5c564fcf42c5ffeb4.pdf", "paperhash": "wu|disentangling_content_and_style_via_unsupervised_geometry_distillation"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper3/Decision", "cdate": 1555612282058, "reply": {"forum": "SkgsQ8LK_E", "replyto": "SkgsQ8LK_E", "readers": [".*"], "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1555612282058, "tmdate": 1556906094615, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"writable": true}}}}], "count": 4}