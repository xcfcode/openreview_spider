{"notes": [{"id": "O1w48WvlmiC", "original": "xzltRHhTDKD", "number": 32, "cdate": 1615310254834, "ddate": null, "tcdate": 1615310254834, "tmdate": 1615313024536, "tddate": null, "forum": "O1w48WvlmiC", "replyto": null, "invitation": "ICLR.cc/2021/Workshop/SSL-RL/-/Blind_Submission", "content": {"title": "Model-Invariant State Abstractions for Model-Based Reinforcement Learning", "authorids": ["ICLR.cc/2021/Workshop/SSL-RL/Paper32/Authors"], "authors": ["Anonymous"], "keywords": ["Reinforcement Learning", "Model-based R", "State Abstractions"], "abstract": "   Accuracy and generalization of dynamics models is key to the success of model-based reinforcement learning (MBRL). As the complexity of tasks increases, learning dynamics models becomes increasingly sample inefficient for MBRL methods. However, many tasks also exhibit sparsity in the dynamics, i.e., actions have only a local effect on the system dynamics. In this paper, we exploit this property with a causal invariance perspective in the single-task setting, introducing a new type of state abstraction called \\textit{model-invariance}. Unlike previous forms of state abstractions, a model-invariance state abstraction  leverages causal sparsity over state variables. This allows for generalization to novel combinations of unseen values of state variables, something that non-factored forms of state abstractions cannot do. We prove that an optimal policy can be learned over this model-invariance state abstraction. Next, we propose a practical method to approximately learn a model-invariant representation for complex domains. We validate our approach by showing improved modeling performance over standard maximum likelihood approaches on challenging tasks, such as the MuJoCo-based Humanoid. Furthermore, within the MBRL setting we show strong performance gains w.r.t. sample efficiency across a host of other continuous control tasks. ", "pdf": "/pdf/16f4e26c60e06a761fb4e4e2739ed6d31c4474a2.pdf", "paperhash": "anonymous|modelinvariant_state_abstractions_for_modelbased_reinforcement_learning", "_bibtex": "@inproceedings{\nanonymous2021modelinvariant,\ntitle={Model-Invariant State Abstractions for Model-Based Reinforcement Learning},\nauthor={Anonymous},\nbooktitle={Submitted to Self-Supervision for Reinforcement Learning Workshop - ICLR 2021},\nyear={2021},\nurl={https://openreview.net/forum?id=O1w48WvlmiC},\nnote={under review}\n}"}, "signatures": ["ICLR.cc/2021/Workshop/SSL-RL"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Workshop/SSL-RL"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Workshop/SSL-RL"]}, "signatures": {"values": ["ICLR.cc/2021/Workshop/SSL-RL"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Workshop/SSL-RL"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Workshop/SSL-RL"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1615310247528, "tmdate": 1615313016556, "id": "ICLR.cc/2021/Workshop/SSL-RL/-/Blind_Submission"}}, "tauthor": "~Super_User1"}], "count": 1}