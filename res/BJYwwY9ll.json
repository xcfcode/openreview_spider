{"notes": [{"tddate": null, "ddate": null, "tmdate": 1511293276200, "tcdate": 1511293276200, "number": 3, "cdate": 1511293276200, "content": {"title": ".", "question": "."}, "id": "ryEwZ-MxG", "invitation": "ICLR.cc/2017/conference/-/paper501/pre-review/question", "forum": "BJYwwY9ll", "replyto": "BJYwwY9ll", "signatures": ["ICLR.cc/2017/conference/paper501/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper501/AnonReviewer3"], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Snapshot Ensembles: Train 1, Get M for Free", "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters.  To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective.  We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields  lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.", "pdf": "/pdf/6012b9cd09762e2a9c7ad5b90014b8e6ac4a07b8.pdf", "paperhash": "huang|snapshot_ensembles_train_1_get_m_for_free", "keywords": ["Deep learning", "Computer vision"], "conflicts": ["cornell.edu", "tsinghua.edu.cn"], "authors": ["Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger"], "authorids": ["gh349@cornell.edu", "yl2363@cornell.edu", "geoff@cs.cornell.edu", "liuzhuangthu@gmail.com", "jeh@cs.cornell.edu", "kqw4@cornell.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1511293277005, "id": "ICLR.cc/2017/conference/-/paper501/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper501/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper501/AnonReviewer2", "ICLR.cc/2017/conference/paper501/AnonReviewer1", "ICLR.cc/2017/conference/paper501/AnonReviewer3"], "reply": {"forum": "BJYwwY9ll", "replyto": "BJYwwY9ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper501/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper501/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1511293277005}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488552235620, "tcdate": 1478297441439, "number": 501, "id": "BJYwwY9ll", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "BJYwwY9ll", "signatures": ["~Gao_Huang1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Snapshot Ensembles: Train 1, Get M for Free", "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters.  To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective.  We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields  lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.", "pdf": "/pdf/6012b9cd09762e2a9c7ad5b90014b8e6ac4a07b8.pdf", "paperhash": "huang|snapshot_ensembles_train_1_get_m_for_free", "keywords": ["Deep learning", "Computer vision"], "conflicts": ["cornell.edu", "tsinghua.edu.cn"], "authors": ["Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger"], "authorids": ["gh349@cornell.edu", "yl2363@cornell.edu", "geoff@cs.cornell.edu", "liuzhuangthu@gmail.com", "jeh@cs.cornell.edu", "kqw4@cornell.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 36, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396622918, "tcdate": 1486396622918, "number": 1, "id": "HkPAhz8ue", "invitation": "ICLR.cc/2017/conference/-/paper501/acceptance", "forum": "BJYwwY9ll", "replyto": "BJYwwY9ll", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "Significant problem, interesting and simple solution, broad evaluation, authors highly responsive in incorporating feedback, all reviewers recommend acceptance. I agree.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Snapshot Ensembles: Train 1, Get M for Free", "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters.  To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective.  We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields  lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.", "pdf": "/pdf/6012b9cd09762e2a9c7ad5b90014b8e6ac4a07b8.pdf", "paperhash": "huang|snapshot_ensembles_train_1_get_m_for_free", "keywords": ["Deep learning", "Computer vision"], "conflicts": ["cornell.edu", "tsinghua.edu.cn"], "authors": ["Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger"], "authorids": ["gh349@cornell.edu", "yl2363@cornell.edu", "geoff@cs.cornell.edu", "liuzhuangthu@gmail.com", "jeh@cs.cornell.edu", "kqw4@cornell.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396623705, "id": "ICLR.cc/2017/conference/-/paper501/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "BJYwwY9ll", "replyto": "BJYwwY9ll", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396623705}}}, {"tddate": null, "tmdate": 1486081364147, "tcdate": 1486081364147, "number": 4, "id": "Hy28aSbde", "invitation": "ICLR.cc/2017/conference/-/paper501/official/comment", "forum": "BJYwwY9ll", "replyto": "B1jMfD0wg", "signatures": ["ICLR.cc/2017/conference/paper501/areachair1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper501/areachair1"], "content": {"title": "reply to Kilian", "comment": "Hi Kilian,\n\nOpenreview does not allow me to respond to authors alone (without making the response public). I will just respond by saying that all reviews, concerns, discussions have been incorporated and AC recommendations have been submitted to the PCs (to make a final decision). "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Snapshot Ensembles: Train 1, Get M for Free", "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters.  To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective.  We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields  lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.", "pdf": "/pdf/6012b9cd09762e2a9c7ad5b90014b8e6ac4a07b8.pdf", "paperhash": "huang|snapshot_ensembles_train_1_get_m_for_free", "keywords": ["Deep learning", "Computer vision"], "conflicts": ["cornell.edu", "tsinghua.edu.cn"], "authors": ["Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger"], "authorids": ["gh349@cornell.edu", "yl2363@cornell.edu", "geoff@cs.cornell.edu", "liuzhuangthu@gmail.com", "jeh@cs.cornell.edu", "kqw4@cornell.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287549821, "id": "ICLR.cc/2017/conference/-/paper501/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "BJYwwY9ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper501/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper501/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper501/reviewers", "ICLR.cc/2017/conference/paper501/areachairs"], "cdate": 1485287549821}}}, {"tddate": null, "tmdate": 1485816680686, "tcdate": 1485816680686, "number": 25, "id": "ry-dQr6wl", "invitation": "ICLR.cc/2017/conference/-/paper501/public/comment", "forum": "BJYwwY9ll", "replyto": "Sy_wWdqwg", "signatures": ["~George_Philipp1"], "readers": ["everyone"], "writers": ["~George_Philipp1"], "content": {"title": "further comments", "comment": "Dear Author,\n\nThank you for your response.\n\n**Strategy C**\n\nYou say that the \u201cSingle Cycle Ensemble\u201d that is freshly included in the paper (table 1) was trained with 1/M epochs in the standard learning rate schedule. Am I correct in understanding that by standard learning rate schedule you do NOT mean cosine learning rate schedule? If so, then you are not using strategy C. Strategy C requires the use of the cosine schedule, as pointed out by the anonymous commentator above. The reason for this is clear when looking at Figure 2. After 50 epochs, the cosine schedule has a training error of between 0.01 and 0.001, whereas the standard learning rate (what I called traditional learning rate in my earlier comments) has more than 0.1. Hence, ensembling models that have more than 10 times the training error, unless there is severe overfitting (which is unlikely given that models have been trained for only 50 epochs), is a much worse strategy. Hence, I expect strategy C to perform significantly better than \u201cSingle Cycle Ensemble\u201d in the paper.\n\nLooking at Figure 1, I note that \u201cSingle Cycle Ensemble\u201d outperforms single model in 8 out of 14 cases shown. This makes me more curious than ever to see strategy C.\n\n**Strategy B as a baseline**\n\nLet me point out that of course, I don't know for sure what the impact of shortening the standard schedule would be. My point was that according to my subjective judgment, the reduced accuracy incurred by shortening the standard schedule could be more than compensated for by ensembling. While again, I cannot prove this to be true, your reply also did not provide any counterevidence.\n\n\"put another way - if they had been able to only train for 100 epochs they likely would have done it\"\n\nI find this relatively unlikely. Most DNN papers report only accuracy for most of their experiments and not runtime, and competition for having low error rate on CIFAR is fierce. If I can train a net for 300 epochs instead of 100 epochs to gain an additional half a per cent, I would likely do it. Also, even if we assume that shortening the schedule to 100 epochs would lead to significant error increase, ensembling such networks could still be competitive.\n\n\"Moreover, our paper also concerns both runtime and accuracy (it aims at achieving high accuracy at a given training time budget), instead of \u201cchiefly concerned with runtime\u201d as you mentioned.\" \n\nI suppose whether the paper is \"chiefly concerned with runtime\" is a subjective judgment. I did not mean to mis-characterize the paper. I made this comment because snapshot ensembles actively sacrifices accuracy in order to shorten runtime (compared to full ensembles). Therefore it addresses cases where a user is interested in sacrificing accuracy for runtime, therefore it is likely that this user is at least more concerned with runtime than the average user, therefore one might say that that user is \"chiefly concerned with runtime\", therefore I said the paper was \"chiefly concerned with runtime\". But as I mentioned, this label is subjective. Independently of whether the label is applied or not, if the method presented in a paper sacrifices accuracy for runtime, there is a higher burden on that paper to compare with other methods that do the same thing, especially when they are as simple and obvious as shortening the learning rate schedule.\n\n** Final test error **\n\nI am glad to be of service here. I would certainly be very happy to be able to take into account those numbers in any final judgment of the paper.\n\n**No additional cost**\n\n\"We recommend to train M models with T/M cycles instead to obtain snapshot ensembles. The computation time is exactly the same but the accuracy will likely improve.\"\n\nIn your previous paragraph you stated that in some cases, the cosine schedule will make the error worse, now you claim that \"accuracy will likely improve\". This makes me even more curious to actually see in how many cases it gets worse, in how many cases it gets better, and by how much. \n\nAlso, it is noteworthy that in your ImageNet results, you choose to ensemble not 6, but 2 or 3 models. Does this mean that ensembling 6 models underperforms the \"single model\" case? If so, then the claim that \"accuracy will likely improve\" is actually contingent on knowing a good value for M, which is an additional cost (the cost of knowing M, whatever that translates to in practice).\n\n**Summary**\n\nI think it is necessary to include the following:\n\n - actual results for strategy C in Table 1, and ideally table 2\n - the final test error for the cosine schedule in table 1 and table 2\n\nI would like to see the equivalent of Figure 2 for test classification error.\n\nFinally, instead of the central claim being \"ensembling multiple neural network at no additional training cost\" I would say something like \"ensembling multiple neural network without training several networks from scratch\". \"at no cost\", to me, implies that there are no potential downsides to the cosine schedule, and I think it has become clear that there are."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Snapshot Ensembles: Train 1, Get M for Free", "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters.  To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective.  We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields  lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.", "pdf": "/pdf/6012b9cd09762e2a9c7ad5b90014b8e6ac4a07b8.pdf", "paperhash": "huang|snapshot_ensembles_train_1_get_m_for_free", "keywords": ["Deep learning", "Computer vision"], "conflicts": ["cornell.edu", "tsinghua.edu.cn"], "authors": ["Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger"], "authorids": ["gh349@cornell.edu", "yl2363@cornell.edu", "geoff@cs.cornell.edu", "liuzhuangthu@gmail.com", "jeh@cs.cornell.edu", "kqw4@cornell.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287549952, "id": "ICLR.cc/2017/conference/-/paper501/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJYwwY9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper501/reviewers", "ICLR.cc/2017/conference/paper501/areachairs"], "cdate": 1485287549952}}}, {"tddate": null, "tmdate": 1485644159845, "tcdate": 1485631840290, "number": 24, "id": "Sy_wWdqwg", "invitation": "ICLR.cc/2017/conference/-/paper501/public/comment", "forum": "BJYwwY9ll", "replyto": "BJaN9lKPl", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Further question", "comment": "If I understood correctly, your additional baseline uses standard schedule while the strategy C mentioned by George uses **cosine** schedule instead of **standard** schedule. To clearly understand the advantage of \"Snapshot Ensemble\" compared with \"Independent Ensemble\", I believe it is necessary to use the same schedule (i.e. cosine schedule) for the independent baseline."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Snapshot Ensembles: Train 1, Get M for Free", "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters.  To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective.  We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields  lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.", "pdf": "/pdf/6012b9cd09762e2a9c7ad5b90014b8e6ac4a07b8.pdf", "paperhash": "huang|snapshot_ensembles_train_1_get_m_for_free", "keywords": ["Deep learning", "Computer vision"], "conflicts": ["cornell.edu", "tsinghua.edu.cn"], "authors": ["Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger"], "authorids": ["gh349@cornell.edu", "yl2363@cornell.edu", "geoff@cs.cornell.edu", "liuzhuangthu@gmail.com", "jeh@cs.cornell.edu", "kqw4@cornell.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287549952, "id": "ICLR.cc/2017/conference/-/paper501/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJYwwY9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper501/reviewers", "ICLR.cc/2017/conference/paper501/areachairs"], "cdate": 1485287549952}}}, {"tddate": null, "tmdate": 1485535796853, "tcdate": 1485535796853, "number": 23, "id": "BJaN9lKPl", "invitation": "ICLR.cc/2017/conference/-/paper501/public/comment", "forum": "BJYwwY9ll", "replyto": "SJI2WPdwx", "signatures": ["~Gao_Huang1"], "readers": ["everyone"], "writers": ["~Gao_Huang1"], "content": {"title": "response", "comment": "**Strategy C**\nWe have updated Table 1 with an additional baseline \u201cSingle Cycle Ensemble\u201d, which is basically the \u201cstrategy C\u201d you mentioned: it ensembles M independently trained models, each of which is trained for 1/M the total number of epochs with standard learning rate schedule.\nSnapshot Ensemble outperforms this baseline by a large margin for most of the models and datasets.\n\n**Strategy B as a baseline**\nWe disagree that Strategy B is a weak baseline. The training budgets (total number of training epochs) for all the models on all the datasets were not picked in favor of our algorithm, but were proposed in the original papers. Although accuracy was likely their main pursuit, in deep learning most researchers are bound by computational budgets (especially during hyper-parameter search) and it is unlikely this is not reflected in their parameter choice (put another way - if they had been able to only train for 100 epochs they likely would have done it). Moreover, our paper also concerns both runtime and accuracy (it aims at achieving high accuracy at a given training time budget), instead of \u201cchiefly concerned with runtime\u201d as you mentioned. \n \n**Final test error**\nWe will report the final test error of the cyclical cosine shape learning rate. Indeed, in some cases it is higher than standard schedule after the same number of epochs. Thanks for pointing this out.\n\n**No additional cost**\nWith \u201cno additional cost\u201d at training time we mean the following: Assume a researcher is planning to train a deep net for T iterations (under some standard learning rate schedule) -  this is arguably the most common setup in deep learning. We recommend to train M models with T/M cycles instead to obtain snapshot ensembles. The computation time is exactly the same but the accuracy will likely improve. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Snapshot Ensembles: Train 1, Get M for Free", "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters.  To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective.  We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields  lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.", "pdf": "/pdf/6012b9cd09762e2a9c7ad5b90014b8e6ac4a07b8.pdf", "paperhash": "huang|snapshot_ensembles_train_1_get_m_for_free", "keywords": ["Deep learning", "Computer vision"], "conflicts": ["cornell.edu", "tsinghua.edu.cn"], "authors": ["Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger"], "authorids": ["gh349@cornell.edu", "yl2363@cornell.edu", "geoff@cs.cornell.edu", "liuzhuangthu@gmail.com", "jeh@cs.cornell.edu", "kqw4@cornell.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287549952, "id": "ICLR.cc/2017/conference/-/paper501/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJYwwY9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper501/reviewers", "ICLR.cc/2017/conference/paper501/areachairs"], "cdate": 1485287549952}}}, {"tddate": null, "tmdate": 1485496749723, "tcdate": 1485496749723, "number": 3, "id": "SJI2WPdwx", "invitation": "ICLR.cc/2017/conference/-/paper501/official/comment", "forum": "BJYwwY9ll", "replyto": "SJZ9OwZIe", "signatures": ["ICLR.cc/2017/conference/paper501/areachair1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper501/areachair1"], "content": {"title": "finalizing this thread", "comment": "Dear authors,\n\nA response from you on this thread will be helpful. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Snapshot Ensembles: Train 1, Get M for Free", "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters.  To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective.  We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields  lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.", "pdf": "/pdf/6012b9cd09762e2a9c7ad5b90014b8e6ac4a07b8.pdf", "paperhash": "huang|snapshot_ensembles_train_1_get_m_for_free", "keywords": ["Deep learning", "Computer vision"], "conflicts": ["cornell.edu", "tsinghua.edu.cn"], "authors": ["Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger"], "authorids": ["gh349@cornell.edu", "yl2363@cornell.edu", "geoff@cs.cornell.edu", "liuzhuangthu@gmail.com", "jeh@cs.cornell.edu", "kqw4@cornell.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287549821, "id": "ICLR.cc/2017/conference/-/paper501/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "BJYwwY9ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper501/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper501/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper501/reviewers", "ICLR.cc/2017/conference/paper501/areachairs"], "cdate": 1485287549821}}}, {"tddate": null, "tmdate": 1485496685342, "tcdate": 1483672027885, "number": 1, "id": "Hk4kqFhBg", "invitation": "ICLR.cc/2017/conference/-/paper501/official/comment", "forum": "BJYwwY9ll", "replyto": "rJfMDFhSl", "signatures": ["ICLR.cc/2017/conference/paper501/areachair1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper501/areachair1"], "content": {"title": "AC response to public comment", "comment": "Dear George Philipp,\n\nThank you for your public comment. I am the Area Chair assigned on this paper. \n\nCan you please rephrase/edit your comment to remove unnecessarily confrontational language (\"What on earth is going on?\", etc)? \n\nSuch phrasing is unhelpful, unwelcome, disrespectful to colleagues, and likely to distract from a legitimate discussion. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Snapshot Ensembles: Train 1, Get M for Free", "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters.  To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective.  We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields  lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.", "pdf": "/pdf/6012b9cd09762e2a9c7ad5b90014b8e6ac4a07b8.pdf", "paperhash": "huang|snapshot_ensembles_train_1_get_m_for_free", "keywords": ["Deep learning", "Computer vision"], "conflicts": ["cornell.edu", "tsinghua.edu.cn"], "authors": ["Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger"], "authorids": ["gh349@cornell.edu", "yl2363@cornell.edu", "geoff@cs.cornell.edu", "liuzhuangthu@gmail.com", "jeh@cs.cornell.edu", "kqw4@cornell.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287549821, "id": "ICLR.cc/2017/conference/-/paper501/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "BJYwwY9ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper501/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper501/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper501/reviewers", "ICLR.cc/2017/conference/paper501/areachairs"], "cdate": 1485287549821}}}, {"tddate": null, "tmdate": 1484689595344, "tcdate": 1484689595344, "number": 22, "id": "HkmpgfnLg", "invitation": "ICLR.cc/2017/conference/-/paper501/public/comment", "forum": "BJYwwY9ll", "replyto": "BJs7CXuUl", "signatures": ["~Kilian_Q_Weinberger1"], "readers": ["everyone"], "writers": ["~Kilian_Q_Weinberger1"], "content": {"title": "Maybe I can ...", "comment": "(Responding in place of the lead, authors who are traveling.)\n\nThe focus of this paper is primarily a method to obtain an (approximate) ensemble of neural networks. The size of the images is unlikely to impact this procedure. It is also not clear how larger data sets (e.g. ImageNet) would make our contribution more or less novel.\n\nThere is the temptation in machine learning to propose complicated \"highly novel\" approaches - many of which are never adopted and often similar results can be achieved by much simpler (possibly harder to publish) alternatives. In contrast, we explicitly tried to identify the simplest approach to make Snapshot ensembles work, something that several researchers have tried before us (with limited success). We are fully aware that there is the danger that readers may expect to see a big complicated solution, but we hope that by keeping things simple we can get more people to understand the paper and ultimately maybe implement it. The unusually large number of comments may suggest we are on the right track. \u00a0:-)\n\n-Kilian"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Snapshot Ensembles: Train 1, Get M for Free", "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters.  To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective.  We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields  lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.", "pdf": "/pdf/6012b9cd09762e2a9c7ad5b90014b8e6ac4a07b8.pdf", "paperhash": "huang|snapshot_ensembles_train_1_get_m_for_free", "keywords": ["Deep learning", "Computer vision"], "conflicts": ["cornell.edu", "tsinghua.edu.cn"], "authors": ["Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger"], "authorids": ["gh349@cornell.edu", "yl2363@cornell.edu", "geoff@cs.cornell.edu", "liuzhuangthu@gmail.com", "jeh@cs.cornell.edu", "kqw4@cornell.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287549952, "id": "ICLR.cc/2017/conference/-/paper501/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJYwwY9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper501/reviewers", "ICLR.cc/2017/conference/paper501/areachairs"], "cdate": 1485287549952}}}, {"tddate": null, "tmdate": 1484434979361, "tcdate": 1484434979361, "number": 21, "id": "BJs7CXuUl", "invitation": "ICLR.cc/2017/conference/-/paper501/public/comment", "forum": "BJYwwY9ll", "replyto": "S1U7EXIVx", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Sorry, but I can hardly understand the rating scores.", "comment": "Dear Authors,\n\nThis paper seems to have the most comments and very high rating scores, so I read it through, I think I have comments on this :\n\n1. The results are good on the datasets you report e.g. CIFAR-10, SVHN, Tiny ImageNet. However, from a vision perspective, I don't see any dataset with images larger than 200 pixels.\n\n2. Like the other comments, I can hardly see any novelty from this ensemble method, and from your previous response, my understanding is you used the cyclic learning rate which seems to be very important. However, this cyclic learning rate schedule was proposed by another paper.\n\nBased on all these, I do think you need to provide more evidence to show the novelty, for instance, ImageNet results ?\n\nThank you !\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Snapshot Ensembles: Train 1, Get M for Free", "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters.  To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective.  We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields  lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.", "pdf": "/pdf/6012b9cd09762e2a9c7ad5b90014b8e6ac4a07b8.pdf", "paperhash": "huang|snapshot_ensembles_train_1_get_m_for_free", "keywords": ["Deep learning", "Computer vision"], "conflicts": ["cornell.edu", "tsinghua.edu.cn"], "authors": ["Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger"], "authorids": ["gh349@cornell.edu", "yl2363@cornell.edu", "geoff@cs.cornell.edu", "liuzhuangthu@gmail.com", "jeh@cs.cornell.edu", "kqw4@cornell.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287549952, "id": "ICLR.cc/2017/conference/-/paper501/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJYwwY9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper501/reviewers", "ICLR.cc/2017/conference/paper501/areachairs"], "cdate": 1485287549952}}}, {"tddate": null, "tmdate": 1483991177362, "tcdate": 1483991177362, "number": 20, "id": "SJZ9OwZIe", "invitation": "ICLR.cc/2017/conference/-/paper501/public/comment", "forum": "BJYwwY9ll", "replyto": "SJLcZITre", "signatures": ["~George_Philipp1"], "readers": ["everyone"], "writers": ["~George_Philipp1"], "content": {"title": "Further comments", "comment": "Dear Dr Weinberger,\n\nThank you for your response.\n\n**Strategy C**\n\nThe fact that ensembling 50-epoch models does not work is surprising to me, because of Figure 2. Figure 2 indicates to me that the model trained with the cosine schedule is already pretty good after 50 epochs. Hmmm ... do I fail to read Figure 2 correctly? Figure 2 looks to me that the benefit of running multiple 50-epoch cosine cycles is not even that dramatic. I would be very interested in seeing the curve of test error over time, or even just a list of the form \"test error after 50 epochs\", \"test error after 100 epochs\", etc. for both the cosine and traditional schedules.\n\nOf course, I still think that including the actual numbers for strategy C is absolutely necessary.\n\n**Strategy B as a baseline**\n\nLet me clarify my position on this. I agree that comparing the snapshot ensemble with a full ensemble of extensively trained models (strategy B) is relevant and interesting and Figure 4 is indeed valuable. (Contrary to what I said in my original comment.)\n\nHowever, I still believe that it is a weak baseline, because your paper is chiefly concerned with runtime. It appears to me that strategy B is concerned chiefly with test error of the final model, not with runtime. Therefore, it is a weak baseline to use strategy B in any kind of runtime comparison. \n\nRight now, the snapshot ensemble achieves performance comparable to that of 2.5 models fully trained with strategy B. This is a speedup of factor 2.5 (300 epochs for the snapshot ensemble, 750 epochs for the full ensemble), which is essentially your flagship result.\n\nBut what if I shortened strategy B to 150 epochs. Say this reduces the final test error by 1% and say this means I now need 3 full models to achieve the performance of the snapshot ensemble. Then my speedup would only be factor 1.5 (300 compared to 450). Now of course these numbers are made up. My point is only that it looks like one could easily train the network in a lot less than 300 epochs while only suffering a minor loss in accuracy which would be more than compensated for via ensembling. And if that is the case, then strategy B would be, by definition, a weak baseline, as there would exist another baseline of equal conceptual complexity that performs much better.\n\nDon't get me wrong, if strategy C indeed doesn't work, then I think comparing to strategy B is ok (not a fatal flaw). However, I think it would be a significant weakness.\n\n**\"at no additional cost\"**\n\nYou state:\n\n\"Our statement that we don't incur any additional cost was made in the context that we focus on the (common) setting where you stop after a fixed budget of T epochs\"\n\nI don't understand this statement, because to me cost and budget are synonymous. I don't understand how one can incur no additional cost in the setting where there is a fixed cost. What cost, if not training time, are you referring to? \n\nI think the main claim of the paper could be worded more clearly. (At this moment, I don't fully understand what it is.)\n\nFurther you state:\n\n\"The training loss of the cosine schedule is indeed higher (see Figure 2), whereas the test error is lower (very left point of Figure 4).\"\n\nIn my original comment, I indeed got mixed up between training and test error. Thank you for pointing this out.\n\nWhat about Figure 3 though? The leftmost blue bar is above the red dashed line. Does that not mean that the test error for the cosine schedule is higher? If so, you can only claim that the test error is **sometimes** lower for the cosine schedule (DenseNet-40) but sometimes higher (DenseNet-100). I think this point needs to be expanded on in the paper, as it is important to your claim that there are no downsides to using the cosine schedule.\n\n\nSUMMARY:\n\nI think the following needs to be included in the paper:\n\n - comparison with strategy C (see my original comment)\n - comparison of how the final test error of the cosine schedule compares with the final test error of the traditional schedule after 300 epochs. Some information can be gleaned from Figures 3 and 4, but I would include the information as en extra line in Table 1 and discuss the difference in performance in the main text. After all, this difference in performance is incurred by whoever chooses to train a snapshot ensemble, and therefore it is significant. \n\nIn addition, I would like to see at least one training curve of test error for both schedules in the style of Figure 2 and I would like some clarification on what \"no additional cost\" means when it doesn't refer to training time.\n\n\n\nAgain, thank you so much for your reply. I hope I am adding to (rather than detracting from) clarity for everyone involved.\n\nGeorge"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Snapshot Ensembles: Train 1, Get M for Free", "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters.  To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective.  We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields  lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.", "pdf": "/pdf/6012b9cd09762e2a9c7ad5b90014b8e6ac4a07b8.pdf", "paperhash": "huang|snapshot_ensembles_train_1_get_m_for_free", "keywords": ["Deep learning", "Computer vision"], "conflicts": ["cornell.edu", "tsinghua.edu.cn"], "authors": ["Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger"], "authorids": ["gh349@cornell.edu", "yl2363@cornell.edu", "geoff@cs.cornell.edu", "liuzhuangthu@gmail.com", "jeh@cs.cornell.edu", "kqw4@cornell.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287549952, "id": "ICLR.cc/2017/conference/-/paper501/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJYwwY9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper501/reviewers", "ICLR.cc/2017/conference/paper501/areachairs"], "cdate": 1485287549952}}}, {"tddate": null, "tmdate": 1483723149944, "tcdate": 1483723149944, "number": 19, "id": "SJLcZITre", "invitation": "ICLR.cc/2017/conference/-/paper501/public/comment", "forum": "BJYwwY9ll", "replyto": "HkGxU22rx", "signatures": ["~Kilian_Q_Weinberger1"], "readers": ["everyone"], "writers": ["~Kilian_Q_Weinberger1"], "content": {"title": "Response to questions from George Philipp", "comment": "Hi George,\n\nNo worries, we are not that easily offended. :-)\n\nWe did indeed try out your strategy C, and found it to be not competitive. There is a simple and intuitive explanation. \nEnsembles work if the individual models are \na) diverse\nb) highly accurate.\nStrategy C is essentially a true ensemble (with cosine schedule), where each model is trained for only 50 epochs. They are diverse but in any interesting setting they are not accurate because 50 epochs is insufficient. \nAdmittedly on some data sets 50 epochs might be sufficient (e.g. toy data like MNIST). Then, you could absolutely train six models all the way to completion from random initialization and ensemble them and it would do great.\nHowever, this unconstrained setting is not the case we are focusing on. For these data sets we would consider the case where you have a budget of T=50 epochs. Then you can run Snapshot ensembles with T=50 and M=5, i.e. train five models each with T/M=10 epochs. You will observe that your strategy C with 10-epochs models will no longer work.  \n\nConcerning your other comments: \n(1) I agree that \"comparable\" is too strong and we will make this more precise. \nI disagree that the black line is a \"very weak baseline\". It is the true ensemble which we are trying to approximate.  \n\n(2) We are more than happy to tone down that sentence and make it more precise (we got a little carried away there). Figure 4 does show however that the (free) snapshot ensemble outperforms the significantly more expensive true ensemble with two independent models. Once you go to six independent models there is a 1% absolute gap, but now things are starting to get very expensive. \n\n(3) The training loss of the cosine schedule is indeed higher (see Figure 2), whereas the test error is lower (very left point of Figure 4). This is not a contradiction and also not a bad thing. One explanation could be that the repeated restarts prevent the model from getting stuck in narrow but deep local minima, which are suspected to generalize badly. However, this is really the contribution of Loshchilov and Hutter\n https://openreview.net/forum?id=Skq89Scxx and not ours. \nOur statement that we don't incur any additional cost was made in the context that we focus on the (common) setting where you stop after a fixed budget of T epochs and not the (admittedly rarer) setting where you stop the moment your training loss hits a certain mark. \n\nIf the area chair wants to see the additional baseline of strategy C we are happy to include it. Obviously we all disagree that our paper makes \"disingenuous\" claims and that we \"obscure\" things with a \"very weak baseline\". You should try Snapshot ensembling, it works great. :-)\n\n-Kilian"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Snapshot Ensembles: Train 1, Get M for Free", "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters.  To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective.  We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields  lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.", "pdf": "/pdf/6012b9cd09762e2a9c7ad5b90014b8e6ac4a07b8.pdf", "paperhash": "huang|snapshot_ensembles_train_1_get_m_for_free", "keywords": ["Deep learning", "Computer vision"], "conflicts": ["cornell.edu", "tsinghua.edu.cn"], "authors": ["Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger"], "authorids": ["gh349@cornell.edu", "yl2363@cornell.edu", "geoff@cs.cornell.edu", "liuzhuangthu@gmail.com", "jeh@cs.cornell.edu", "kqw4@cornell.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287549952, "id": "ICLR.cc/2017/conference/-/paper501/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJYwwY9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper501/reviewers", "ICLR.cc/2017/conference/paper501/areachairs"], "cdate": 1485287549952}}}, {"tddate": null, "tmdate": 1483683305692, "tcdate": 1483683305692, "number": 2, "id": "HkGxU22rx", "invitation": "ICLR.cc/2017/conference/-/paper501/official/comment", "forum": "BJYwwY9ll", "replyto": "SkqH-5nrg", "signatures": ["ICLR.cc/2017/conference/paper501/areachair1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper501/areachair1"], "content": {"title": "Thanks!", "comment": "Thanks George. Much appreciated. \n\nI look forward to hearing the response to your questions from the authors. \n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Snapshot Ensembles: Train 1, Get M for Free", "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters.  To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective.  We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields  lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.", "pdf": "/pdf/6012b9cd09762e2a9c7ad5b90014b8e6ac4a07b8.pdf", "paperhash": "huang|snapshot_ensembles_train_1_get_m_for_free", "keywords": ["Deep learning", "Computer vision"], "conflicts": ["cornell.edu", "tsinghua.edu.cn"], "authors": ["Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger"], "authorids": ["gh349@cornell.edu", "yl2363@cornell.edu", "geoff@cs.cornell.edu", "liuzhuangthu@gmail.com", "jeh@cs.cornell.edu", "kqw4@cornell.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287549821, "id": "ICLR.cc/2017/conference/-/paper501/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "BJYwwY9ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper501/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper501/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper501/reviewers", "ICLR.cc/2017/conference/paper501/areachairs"], "cdate": 1485287549821}}}, {"tddate": null, "tmdate": 1483674012715, "tcdate": 1483673921831, "number": 18, "id": "SkqH-5nrg", "invitation": "ICLR.cc/2017/conference/-/paper501/public/comment", "forum": "BJYwwY9ll", "replyto": "rJfMDFhSl", "signatures": ["~George_Philipp1"], "readers": ["everyone"], "writers": ["~George_Philipp1"], "content": {"title": "Apologies", "comment": "Dear Authors, \n\nI would like to apologize for the wording I used in writing the original version of the above comment (which you would have received via email). I agree with the Area Chair that it was unnecessarily strongly worded. I hope I have not offended anyone. I have reworded the comment.\n\nAnyway, I hope I am wrong with what I am saying and the paper is as good as the other reviewers suggest :)\n\nAll the best!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Snapshot Ensembles: Train 1, Get M for Free", "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters.  To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective.  We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields  lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.", "pdf": "/pdf/6012b9cd09762e2a9c7ad5b90014b8e6ac4a07b8.pdf", "paperhash": "huang|snapshot_ensembles_train_1_get_m_for_free", "keywords": ["Deep learning", "Computer vision"], "conflicts": ["cornell.edu", "tsinghua.edu.cn"], "authors": ["Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger"], "authorids": ["gh349@cornell.edu", "yl2363@cornell.edu", "geoff@cs.cornell.edu", "liuzhuangthu@gmail.com", "jeh@cs.cornell.edu", "kqw4@cornell.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287549952, "id": "ICLR.cc/2017/conference/-/paper501/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJYwwY9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper501/reviewers", "ICLR.cc/2017/conference/paper501/areachairs"], "cdate": 1485287549952}}}, {"tddate": null, "tmdate": 1483673765728, "tcdate": 1483671306177, "number": 17, "id": "rJfMDFhSl", "invitation": "ICLR.cc/2017/conference/-/paper501/public/comment", "forum": "BJYwwY9ll", "replyto": "BJYwwY9ll", "signatures": ["~George_Philipp1"], "readers": ["everyone"], "writers": ["~George_Philipp1"], "content": {"title": "Questions", "comment": "After reading the paper, it appears to have a fatal flaw. In addition, it seems to me to make very disingenuous claims about its own experimental results.\n\nAs far as I can tell, this is how the authors train their snapshot ensemble:\n\n(1) initialize the model\n(2) train for a short period (e.g. 50 epochs for DenseNet) with a cosine schedule\n(3) take a snapshot\n(4) continue training with a \"new\" cosine schedule for another short period\n(5) take a snapshot\n(6) repeat steps 4 and 5 a few times\n(7) ensemble the snapshots\n\nCall this strategy A\n\nThen the authors also discuss another strategy they call \"full ensembling\", which they compare to in Figure 4:\n\n(1) initialize the model\n(2) train for a long period (e.g. 300 epochs) with a \"traditional\" schedule\n(3) take a snapshot\n(4) re-initialize the model randomly\n(5) train for a long period (e.g. 300 epochs) with a \"traditional\" schedule\n(6) take a snapshot\n(7) repeat steps (4) through (6) a few times\n(8) ensemble the snapshots\n\nCall this strategy B\n\nHowever, consider the following strategy C:\n\n(1) initialize the model\n(2) train for a short period (e.g. 50 epochs in Figure 2) with a cosine schedule\n(3) take a snapshot\n(4) re-initialize the model randomly\n(5) train for a short period (e.g. 50 epochs in Figure 2) with a cosine schedule\n(6) take a snapshot\n(7) repeat steps (4) through (6) a few times\n(8) ensemble the snapshots\n\nStrategy C is likely superior to strategy A, and this seems to invalidates the entire claim of the paper that it we should build ensembles by taking snapshots along the training path. It is likely better to train many independent models each with the cosine schedule for a short period instead of taking snapshots. Therefore, the paper appears to be a de-novation compared to full ensembling, not an innovation. This is obscured by the fact that the authors only compare to strategy B, which is a very weak baseline. Of course, if I want to train an ensemble quickly, it seems suboptimal to train any individual model for a very long time, which strategy B does.\n\nThe reason I believe that strategy C is likely superior to strategy A is two-fold. First, in Figure 3 you show that beginning the cosine schedule with a higher step size (and thus more re-randomization) leads to better performance as the ensemble grows. Therefore, total re-randomization likely does even better. Further, in Figure 4, you show that full ensembling enjoys a better slope of improvement as the ensemble grows. Strategy C also likely enjoys this slope, whereas strategy A does not.\n\nHowever, even if it turns out that Strategy A indeed outperforms strategy C, not showing this in the paper I would still consider a fatal flaw unless such results were to be included. To be precise, what I would like to see is a comparison between an ensemble of the FIRST k snapshots following your strategy (strategy A), and k independently trained models that were each trained under the the cosine schedule (strategy C), for various values of k.\n\nThere is one use case where I could possibly imagine using strategy A: If I have time to train roughly for the length of one traditional schedule and want an ensemble that contains one model that has, by itself, a good performance. In that case, strategy A would outperform strategy B on the ensemble and strategy C on the single model, though A would lose to B on the single model and to C on the ensemble. In that specific case, we cannot say that there is a strategy that dominates strategy A. However, this claim does not seem strong enough as the main contribution of a top conference paper.\n\nLooking at the high scores from the three reviewers makes me think I somehow missed something in my analysis. I am willing to change my opinion on this. Please enlighten me.\n\n.\n.\n.\n\nAnother thing I want to point out is that the authors seem to make very disingenuous claims about their experimental results in at least 3 instances:\n\n(1) At the end of section 4.3 the authors state \"Our method achieves with comparable performance of ensembling 3 independant models, but with the training cost of one model.\" However, in Figure 4 we find that the snapshot ensemble is halfway between ensembling 2 and 3 independent models. This is not the same thing as \"comparable to 3 independent models\". Further, the difference in test error between the snapshot ensemble and 3 independent models is almost 1 per cent. I would not call such a difference comparable. Of course, since the baseline is extremely weak as I mentioned above, the graph has little value to begin with.\n\n(2)  In the abstract, the authors state \"It [snapshot ensembling] consistently yields significantly lower error rates than state-of-the-art single models at no additional training cost, and almost matches the results of (far more expensive) independently trained network ensembles.\" Looking at Figure 4, you don't come close!\n\n(3) The authors repeatedly claim to get \"multiple neural network at no additional training cost\". However then they point out \"It is also fair to point out that the training loss under the standard schedule is eventually lower\". So they admit that if the stagewise cosine schedule takes M epochs to train (say 300), then the standard schedule achieves a lower error in those same M epochs. Therefore, the standard schedule attains the same error as the stagewise cosine schedule at some earlier epoch, say N. Therefore, the authors DO incur an additional cost in obtaining their ensemble, and it is M - N epochs.\n\n.\n.\n.\n\nOne final question: The authors state that \"It is also fair to point out that the training loss under the standard schedule is eventually lower\". However in Figure 4 it appears that a snapshot ensemble of size 1 (i.e. the final error under the stagewise cosine schedule) has lower error than a full ensemble of size 1, i.e. the final error under the traditional schedule. This seems to be a contradiction.\n\n.\n.\n.\n\nI do agree that the general motivation of the paper is sound and the paper is, apart from the issues discussed above, well written. Again, I am inviting the authors and everybody else to prove me wrong on all of my points."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Snapshot Ensembles: Train 1, Get M for Free", "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters.  To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective.  We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields  lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.", "pdf": "/pdf/6012b9cd09762e2a9c7ad5b90014b8e6ac4a07b8.pdf", "paperhash": "huang|snapshot_ensembles_train_1_get_m_for_free", "keywords": ["Deep learning", "Computer vision"], "conflicts": ["cornell.edu", "tsinghua.edu.cn"], "authors": ["Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger"], "authorids": ["gh349@cornell.edu", "yl2363@cornell.edu", "geoff@cs.cornell.edu", "liuzhuangthu@gmail.com", "jeh@cs.cornell.edu", "kqw4@cornell.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287549952, "id": "ICLR.cc/2017/conference/-/paper501/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJYwwY9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper501/reviewers", "ICLR.cc/2017/conference/paper501/areachairs"], "cdate": 1485287549952}}}, {"tddate": null, "tmdate": 1483447085576, "tcdate": 1483447085576, "number": 16, "id": "SkUEoftHx", "invitation": "ICLR.cc/2017/conference/-/paper501/public/comment", "forum": "BJYwwY9ll", "replyto": "rkeJ_bgBx", "signatures": ["~Gao_Huang1"], "readers": ["everyone"], "writers": ["~Gao_Huang1"], "content": {"title": "Related work", "comment": "Thanks for pointing us towards this paper. It looks like this is yet another publication that invented the baseline we compare against (Table 1, also see comment on Horizontal Ensemble below). This may now be the oldest citation for that baseline that we are aware of. We will definitely cite it in the final version. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Snapshot Ensembles: Train 1, Get M for Free", "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters.  To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective.  We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields  lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.", "pdf": "/pdf/6012b9cd09762e2a9c7ad5b90014b8e6ac4a07b8.pdf", "paperhash": "huang|snapshot_ensembles_train_1_get_m_for_free", "keywords": ["Deep learning", "Computer vision"], "conflicts": ["cornell.edu", "tsinghua.edu.cn"], "authors": ["Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger"], "authorids": ["gh349@cornell.edu", "yl2363@cornell.edu", "geoff@cs.cornell.edu", "liuzhuangthu@gmail.com", "jeh@cs.cornell.edu", "kqw4@cornell.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287549952, "id": "ICLR.cc/2017/conference/-/paper501/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJYwwY9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper501/reviewers", "ICLR.cc/2017/conference/paper501/areachairs"], "cdate": 1485287549952}}}, {"tddate": null, "tmdate": 1482852312276, "tcdate": 1482852312276, "number": 15, "id": "rkeJ_bgBx", "invitation": "ICLR.cc/2017/conference/-/paper501/public/comment", "forum": "BJYwwY9ll", "replyto": "BJYwwY9ll", "signatures": ["~Gavin_Brown1"], "readers": ["everyone"], "writers": ["~Gavin_Brown1"], "content": {"title": "Related work", "comment": "There is a related paper to this work - \"Fast Committee Learning\", by Swann and Allinson, 1998.\n\nhttp://digital-library.theiet.org/content/journals/10.1049/el_19981000\n\nI believe the principles behind it are identical to this work - snapshotting slices of the learning trajectory."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Snapshot Ensembles: Train 1, Get M for Free", "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters.  To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective.  We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields  lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.", "pdf": "/pdf/6012b9cd09762e2a9c7ad5b90014b8e6ac4a07b8.pdf", "paperhash": "huang|snapshot_ensembles_train_1_get_m_for_free", "keywords": ["Deep learning", "Computer vision"], "conflicts": ["cornell.edu", "tsinghua.edu.cn"], "authors": ["Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger"], "authorids": ["gh349@cornell.edu", "yl2363@cornell.edu", "geoff@cs.cornell.edu", "liuzhuangthu@gmail.com", "jeh@cs.cornell.edu", "kqw4@cornell.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287549952, "id": "ICLR.cc/2017/conference/-/paper501/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJYwwY9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper501/reviewers", "ICLR.cc/2017/conference/paper501/areachairs"], "cdate": 1485287549952}}}, {"tddate": null, "tmdate": 1482205558282, "tcdate": 1482205558282, "number": 14, "id": "HJAuF7LNg", "invitation": "ICLR.cc/2017/conference/-/paper501/public/comment", "forum": "BJYwwY9ll", "replyto": "rylUZPZVl", "signatures": ["~Gao_Huang1"], "readers": ["everyone"], "writers": ["~Gao_Huang1"], "content": {"title": "reply", "comment": "Thanks for the detailed suggestions. \n\n1. We agree that \u201ctrue ensemble\u201d results are important as they can be viewed as a lower bound of the test error given by Snapshot Ensemble. We\u2019ll include more of those in the final version. \n\n2. Thanks for pointing out this. We will fix it in the revision.\n\n3. Thanks, we will add those comparisons. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Snapshot Ensembles: Train 1, Get M for Free", "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters.  To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective.  We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields  lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.", "pdf": "/pdf/6012b9cd09762e2a9c7ad5b90014b8e6ac4a07b8.pdf", "paperhash": "huang|snapshot_ensembles_train_1_get_m_for_free", "keywords": ["Deep learning", "Computer vision"], "conflicts": ["cornell.edu", "tsinghua.edu.cn"], "authors": ["Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger"], "authorids": ["gh349@cornell.edu", "yl2363@cornell.edu", "geoff@cs.cornell.edu", "liuzhuangthu@gmail.com", "jeh@cs.cornell.edu", "kqw4@cornell.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287549952, "id": "ICLR.cc/2017/conference/-/paper501/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJYwwY9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper501/reviewers", "ICLR.cc/2017/conference/paper501/areachairs"], "cdate": 1485287549952}}}, {"tddate": null, "tmdate": 1482205425696, "tcdate": 1482205425696, "number": 13, "id": "ryqlYXL4e", "invitation": "ICLR.cc/2017/conference/-/paper501/public/comment", "forum": "BJYwwY9ll", "replyto": "ryp6OObVe", "signatures": ["~Gao_Huang1"], "readers": ["everyone"], "writers": ["~Gao_Huang1"], "content": {"title": "reply", "comment": "Thanks for the suggestions. We will include statistics about the individual ensemble members, and evaluate our method on a larger data set, e.g., ImageNet."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Snapshot Ensembles: Train 1, Get M for Free", "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters.  To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective.  We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields  lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.", "pdf": "/pdf/6012b9cd09762e2a9c7ad5b90014b8e6ac4a07b8.pdf", "paperhash": "huang|snapshot_ensembles_train_1_get_m_for_free", "keywords": ["Deep learning", "Computer vision"], "conflicts": ["cornell.edu", "tsinghua.edu.cn"], "authors": ["Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger"], "authorids": ["gh349@cornell.edu", "yl2363@cornell.edu", "geoff@cs.cornell.edu", "liuzhuangthu@gmail.com", "jeh@cs.cornell.edu", "kqw4@cornell.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287549952, "id": "ICLR.cc/2017/conference/-/paper501/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJYwwY9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper501/reviewers", "ICLR.cc/2017/conference/paper501/areachairs"], "cdate": 1485287549952}}}, {"tddate": null, "tmdate": 1482205342429, "tcdate": 1482205342429, "number": 12, "id": "r1LjOXIVg", "invitation": "ICLR.cc/2017/conference/-/paper501/public/comment", "forum": "BJYwwY9ll", "replyto": "B1GutgfVe", "signatures": ["~Gao_Huang1"], "readers": ["everyone"], "writers": ["~Gao_Huang1"], "content": {"title": "reply", "comment": "Thanks for the positive comments. We will provide more detailed analysis about the individual model snapshots, and more comparisons with true ensembles in the final version.\n\nWe agree that for lambda the most interesting values are between 0 and 1. We will update the figures in the final version."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Snapshot Ensembles: Train 1, Get M for Free", "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters.  To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective.  We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields  lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.", "pdf": "/pdf/6012b9cd09762e2a9c7ad5b90014b8e6ac4a07b8.pdf", "paperhash": "huang|snapshot_ensembles_train_1_get_m_for_free", "keywords": ["Deep learning", "Computer vision"], "conflicts": ["cornell.edu", "tsinghua.edu.cn"], "authors": ["Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger"], "authorids": ["gh349@cornell.edu", "yl2363@cornell.edu", "geoff@cs.cornell.edu", "liuzhuangthu@gmail.com", "jeh@cs.cornell.edu", "kqw4@cornell.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287549952, "id": "ICLR.cc/2017/conference/-/paper501/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJYwwY9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper501/reviewers", "ICLR.cc/2017/conference/paper501/areachairs"], "cdate": 1485287549952}}}, {"tddate": null, "tmdate": 1482204189964, "tcdate": 1482204189964, "number": 11, "id": "S1U7EXIVx", "invitation": "ICLR.cc/2017/conference/-/paper501/public/comment", "forum": "BJYwwY9ll", "replyto": "Bkv9XRQEg", "signatures": ["~Gao_Huang1"], "readers": ["everyone"], "writers": ["~Gao_Huang1"], "content": {"title": "Difference to Horizontal Ensemble", "comment": "The Horizontal ensemble, similar to Cho et al. (see other comments), essentially proposes our baseline \u201cNoCycle Snapshot Ensemble\u201d in Table 1.  One of our key insights is that the cyclical learning rate is crucial for driving the model towards and then escaping from multiple local minimum. This introduces more diversity into ensemble members and leads to significantly better results. We will definitely cite your report in the final version and clarify this difference. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Snapshot Ensembles: Train 1, Get M for Free", "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters.  To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective.  We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields  lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.", "pdf": "/pdf/6012b9cd09762e2a9c7ad5b90014b8e6ac4a07b8.pdf", "paperhash": "huang|snapshot_ensembles_train_1_get_m_for_free", "keywords": ["Deep learning", "Computer vision"], "conflicts": ["cornell.edu", "tsinghua.edu.cn"], "authors": ["Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger"], "authorids": ["gh349@cornell.edu", "yl2363@cornell.edu", "geoff@cs.cornell.edu", "liuzhuangthu@gmail.com", "jeh@cs.cornell.edu", "kqw4@cornell.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287549952, "id": "ICLR.cc/2017/conference/-/paper501/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJYwwY9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper501/reviewers", "ICLR.cc/2017/conference/paper501/areachairs"], "cdate": 1485287549952}}}, {"tddate": null, "tmdate": 1482066243285, "tcdate": 1482066243285, "number": 10, "id": "S1oHK-ENx", "invitation": "ICLR.cc/2017/conference/-/paper501/public/comment", "forum": "BJYwwY9ll", "replyto": "Bkv9XRQEg", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "no title", "comment": "Good comment!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Snapshot Ensembles: Train 1, Get M for Free", "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters.  To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective.  We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields  lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.", "pdf": "/pdf/6012b9cd09762e2a9c7ad5b90014b8e6ac4a07b8.pdf", "paperhash": "huang|snapshot_ensembles_train_1_get_m_for_free", "keywords": ["Deep learning", "Computer vision"], "conflicts": ["cornell.edu", "tsinghua.edu.cn"], "authors": ["Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger"], "authorids": ["gh349@cornell.edu", "yl2363@cornell.edu", "geoff@cs.cornell.edu", "liuzhuangthu@gmail.com", "jeh@cs.cornell.edu", "kqw4@cornell.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287549952, "id": "ICLR.cc/2017/conference/-/paper501/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJYwwY9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper501/reviewers", "ICLR.cc/2017/conference/paper501/areachairs"], "cdate": 1485287549952}}}, {"tddate": null, "tmdate": 1482052789634, "tcdate": 1482052495347, "number": 9, "id": "Bkv9XRQEg", "invitation": "ICLR.cc/2017/conference/-/paper501/public/comment", "forum": "BJYwwY9ll", "replyto": "BJYwwY9ll", "signatures": ["~Bing_Xu1"], "readers": ["everyone"], "writers": ["~Bing_Xu1"], "content": {"title": "Difference to Horizontal Ensemble?", "comment": "Congratulation to the authors for getting very high review score. However I want to know the difference between Snapshot Ensembles and Horizontal Ensemble (Thank Dmytro Mishkin for reading my undergrad paper).\n\nI take a quick look of the paper, seems Snapshot Ensembles is very similar to Horizontal Ensemble, maybe with a different LR schedule? \n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Snapshot Ensembles: Train 1, Get M for Free", "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters.  To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective.  We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields  lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.", "pdf": "/pdf/6012b9cd09762e2a9c7ad5b90014b8e6ac4a07b8.pdf", "paperhash": "huang|snapshot_ensembles_train_1_get_m_for_free", "keywords": ["Deep learning", "Computer vision"], "conflicts": ["cornell.edu", "tsinghua.edu.cn"], "authors": ["Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger"], "authorids": ["gh349@cornell.edu", "yl2363@cornell.edu", "geoff@cs.cornell.edu", "liuzhuangthu@gmail.com", "jeh@cs.cornell.edu", "kqw4@cornell.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287549952, "id": "ICLR.cc/2017/conference/-/paper501/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJYwwY9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper501/reviewers", "ICLR.cc/2017/conference/paper501/areachairs"], "cdate": 1485287549952}}}, {"tddate": null, "tmdate": 1481931113896, "tcdate": 1481931113896, "number": 3, "id": "B1GutgfVe", "invitation": "ICLR.cc/2017/conference/-/paper501/official/review", "forum": "BJYwwY9ll", "replyto": "BJYwwY9ll", "signatures": ["ICLR.cc/2017/conference/paper501/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper501/AnonReviewer3"], "content": {"title": "Review", "rating": "9: Top 15% of accepted papers, strong accept", "review": "This work develops a method to quickly produce an ensemble of deep networks that outperform a single network trained for an equivalent amount of time. The basis of this approach is to use a cyclic learning rate to quickly settle the model into a local minima and saving a model snapshot at this time before quickly raising the learning rate to escape towards a different minima's well of attraction. The resulting snapshots can be collected throughout a single training run and achieve reasonable performance compared to baselines and have some of the gains of traditional ensembles (at a much lower cost). \n\nThis paper is well written, has clear and informative figures/tables, and provides convincing results across a broad range of models and datasets. I especially liked the analysis in Section 4.4.  The publicly available code to ensure reproducibility is also greatly appreciated.\n\nI would like to see more discussion of the accuracy and variability of each snapshot and further comparison with true ensembles.\n\nPreliminary rating:\nThis is an interesting work with convincing experiments and clear writing. \n\nMinor note:\nWhy is the axis for lambda from -1 to 2 in Figure 5 where lambda is naturally between 0 and 1.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Snapshot Ensembles: Train 1, Get M for Free", "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters.  To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective.  We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields  lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.", "pdf": "/pdf/6012b9cd09762e2a9c7ad5b90014b8e6ac4a07b8.pdf", "paperhash": "huang|snapshot_ensembles_train_1_get_m_for_free", "keywords": ["Deep learning", "Computer vision"], "conflicts": ["cornell.edu", "tsinghua.edu.cn"], "authors": ["Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger"], "authorids": ["gh349@cornell.edu", "yl2363@cornell.edu", "geoff@cs.cornell.edu", "liuzhuangthu@gmail.com", "jeh@cs.cornell.edu", "kqw4@cornell.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512563918, "id": "ICLR.cc/2017/conference/-/paper501/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper501/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper501/AnonReviewer1", "ICLR.cc/2017/conference/paper501/AnonReviewer2", "ICLR.cc/2017/conference/paper501/AnonReviewer3"], "reply": {"forum": "BJYwwY9ll", "replyto": "BJYwwY9ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper501/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper501/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512563918}}}, {"tddate": null, "tmdate": 1481898181405, "tcdate": 1481898181405, "number": 2, "id": "ryp6OObVe", "invitation": "ICLR.cc/2017/conference/-/paper501/official/review", "forum": "BJYwwY9ll", "replyto": "BJYwwY9ll", "signatures": ["ICLR.cc/2017/conference/paper501/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper501/AnonReviewer2"], "content": {"title": "A good paper.", "rating": "7: Good paper, accept", "review": "I don't have much to add to my pre-review questions. The main thing I'd like to see that would strengthen my review further is a larger scale evaluation, more discussion of the hyperparameters, etc. Where test error are reported for snapshot ensembles it would be useful to report statistics about the performance of individual ensemble members for comparison (mean and standard deviation, maybe best single member's error rate).", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Snapshot Ensembles: Train 1, Get M for Free", "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters.  To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective.  We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields  lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.", "pdf": "/pdf/6012b9cd09762e2a9c7ad5b90014b8e6ac4a07b8.pdf", "paperhash": "huang|snapshot_ensembles_train_1_get_m_for_free", "keywords": ["Deep learning", "Computer vision"], "conflicts": ["cornell.edu", "tsinghua.edu.cn"], "authors": ["Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger"], "authorids": ["gh349@cornell.edu", "yl2363@cornell.edu", "geoff@cs.cornell.edu", "liuzhuangthu@gmail.com", "jeh@cs.cornell.edu", "kqw4@cornell.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512563918, "id": "ICLR.cc/2017/conference/-/paper501/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper501/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper501/AnonReviewer1", "ICLR.cc/2017/conference/paper501/AnonReviewer2", "ICLR.cc/2017/conference/paper501/AnonReviewer3"], "reply": {"forum": "BJYwwY9ll", "replyto": "BJYwwY9ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper501/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper501/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512563918}}}, {"tddate": null, "tmdate": 1481892991096, "tcdate": 1481892168258, "number": 1, "id": "rylUZPZVl", "invitation": "ICLR.cc/2017/conference/-/paper501/official/review", "forum": "BJYwwY9ll", "replyto": "BJYwwY9ll", "signatures": ["ICLR.cc/2017/conference/paper501/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper501/AnonReviewer1"], "content": {"title": "Review", "rating": "8: Top 50% of accepted papers, clear accept", "review": "The work presented in this paper proposes a method to get an ensemble of neural networks at no extra training cost (i.e., at the cost of training a single network), by saving snapshots of the network during training. Network is trained using a cyclic (cosine) learning rate schedule; the snapshots are obtained when the learning rate is at the lowest points of the cycles. Using these snapshot ensembles, they show gains in performance over a single network on the image classification task on a variety of datasets.\n\n\nPositives:\n\n1. The work should be easy to adopt and re-produce, given the simple techinque and the experimental details in the paper.\n2. Well written paper, with clear description of the method and thorough experiments.\n\n\nSuggestions for improvement / other comments:\n\n1. While it is fair to compare against other techniques assuming a fixed computational budget, for a clear perspective, thorough comaprisons with \"true ensembles\" (i.e., ensembles of networks trained independently) should be provided.\nSpecificially, Table 4 should be augmented with results from \"true ensembles\".\n\n2. Comparison with true ensembles is only provided for DenseNet-40 on CIFAR100 in Figure 4. The proposed snapshot ensemble achieves approximately 66% of the improvement of \"true ensemble\" over the single baseline model. This is not reflected accurately in the authors' claim in the abstract: \"[snapshot ensembles] **almost match[es]** the results of far more expensive independently trained [true ensembles].\"\n\n3. As mentioned before: to understand the diversity of snapshot ensembles, it would help to the diversity against different ensembling technique, e.g. (1) \"true ensembles\", (2) ensembles from dropout as described by Gal et. al, 2016 (Dropout as a Bayesian Approximation).", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Snapshot Ensembles: Train 1, Get M for Free", "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters.  To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective.  We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields  lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.", "pdf": "/pdf/6012b9cd09762e2a9c7ad5b90014b8e6ac4a07b8.pdf", "paperhash": "huang|snapshot_ensembles_train_1_get_m_for_free", "keywords": ["Deep learning", "Computer vision"], "conflicts": ["cornell.edu", "tsinghua.edu.cn"], "authors": ["Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger"], "authorids": ["gh349@cornell.edu", "yl2363@cornell.edu", "geoff@cs.cornell.edu", "liuzhuangthu@gmail.com", "jeh@cs.cornell.edu", "kqw4@cornell.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512563918, "id": "ICLR.cc/2017/conference/-/paper501/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper501/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper501/AnonReviewer1", "ICLR.cc/2017/conference/paper501/AnonReviewer2", "ICLR.cc/2017/conference/paper501/AnonReviewer3"], "reply": {"forum": "BJYwwY9ll", "replyto": "BJYwwY9ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper501/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper501/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512563918}}}, {"tddate": null, "tmdate": 1481115291872, "tcdate": 1481115291866, "number": 8, "id": "HJVsIFBQl", "invitation": "ICLR.cc/2017/conference/-/paper501/public/comment", "forum": "BJYwwY9ll", "replyto": "Sy65VX0zx", "signatures": ["~Gao_Huang1"], "readers": ["everyone"], "writers": ["~Gao_Huang1"], "content": {"title": "no title", "comment": "1. Such equivalent local minima will ultimately lead to identical softmax predictions, although they are difficult to be observed from the hidden layers. According to our Snapshot Ensemble results, the models obtained from different learning rate cycles should have give very different softmax predictions. Hence we can safely conclude that the trivial solutions did not occur in our experiments. Probably they are automatically discouraged by the randomness of the SGD algorithm and the high dimensionality of the parameter space.\n\n2. This is an interesting point. We did not emphasize the hyperparameter selection method in our manuscript because we simply re-used the settings of training a single model. One would expect a set of particularly tuned hyperparameters will further improve the ensemble performance. In that case, we may use your suggest idea to perform hyperparameter selection.\n\n3. We tried this in an earlier experiment, but it did not perform as well as retraining the entire model. Moreover, it is less straightforward to implement and does not necessarily save much training time when data augmentation is used. So we opted out this choice.\n\n "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Snapshot Ensembles: Train 1, Get M for Free", "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters.  To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective.  We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields  lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.", "pdf": "/pdf/6012b9cd09762e2a9c7ad5b90014b8e6ac4a07b8.pdf", "paperhash": "huang|snapshot_ensembles_train_1_get_m_for_free", "keywords": ["Deep learning", "Computer vision"], "conflicts": ["cornell.edu", "tsinghua.edu.cn"], "authors": ["Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger"], "authorids": ["gh349@cornell.edu", "yl2363@cornell.edu", "geoff@cs.cornell.edu", "liuzhuangthu@gmail.com", "jeh@cs.cornell.edu", "kqw4@cornell.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287549952, "id": "ICLR.cc/2017/conference/-/paper501/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJYwwY9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper501/reviewers", "ICLR.cc/2017/conference/paper501/areachairs"], "cdate": 1485287549952}}}, {"tddate": null, "tmdate": 1481114991112, "tcdate": 1481114991104, "number": 7, "id": "rJDuSFHml", "invitation": "ICLR.cc/2017/conference/-/paper501/public/comment", "forum": "BJYwwY9ll", "replyto": "ryots9J7x", "signatures": ["~Gao_Huang1"], "readers": ["everyone"], "writers": ["~Gao_Huang1"], "content": {"title": "no title", "comment": "1. Thanks for the suggestion. We will include the result for \u201ctrue ensembles\u201d, and try to compare with ensembles from dropout.\n\n2. It is the ensemble result. The ensemble size is shown in the first column. We will make this clearer."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Snapshot Ensembles: Train 1, Get M for Free", "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters.  To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective.  We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields  lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.", "pdf": "/pdf/6012b9cd09762e2a9c7ad5b90014b8e6ac4a07b8.pdf", "paperhash": "huang|snapshot_ensembles_train_1_get_m_for_free", "keywords": ["Deep learning", "Computer vision"], "conflicts": ["cornell.edu", "tsinghua.edu.cn"], "authors": ["Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger"], "authorids": ["gh349@cornell.edu", "yl2363@cornell.edu", "geoff@cs.cornell.edu", "liuzhuangthu@gmail.com", "jeh@cs.cornell.edu", "kqw4@cornell.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287549952, "id": "ICLR.cc/2017/conference/-/paper501/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJYwwY9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper501/reviewers", "ICLR.cc/2017/conference/paper501/areachairs"], "cdate": 1485287549952}}}, {"tddate": null, "tmdate": 1480727427291, "tcdate": 1480727427286, "number": 2, "id": "ryots9J7x", "invitation": "ICLR.cc/2017/conference/-/paper501/pre-review/question", "forum": "BJYwwY9ll", "replyto": "BJYwwY9ll", "signatures": ["ICLR.cc/2017/conference/paper501/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper501/AnonReviewer1"], "content": {"title": "Activation Diversity and a clarification", "question": "1. In Figure 6, the diversity of activations is studied with respect to different learning rate schedules. It would be interesting to see how this diversity, which is key for ensembles, compares with a different ensembling technique, e.g. (1) \"true ensembles\", (2) ensembles from dropout as described by Gal et. al, 2016 (Dropout as a Bayesian Approximation).\n\n2. In Table 2 (pg. 7), is the evaluation done using just the final snapshot or an ensemble? If using an ensemble, please specify the size of the ensemble (m) for clarity.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Snapshot Ensembles: Train 1, Get M for Free", "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters.  To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective.  We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields  lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.", "pdf": "/pdf/6012b9cd09762e2a9c7ad5b90014b8e6ac4a07b8.pdf", "paperhash": "huang|snapshot_ensembles_train_1_get_m_for_free", "keywords": ["Deep learning", "Computer vision"], "conflicts": ["cornell.edu", "tsinghua.edu.cn"], "authors": ["Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger"], "authorids": ["gh349@cornell.edu", "yl2363@cornell.edu", "geoff@cs.cornell.edu", "liuzhuangthu@gmail.com", "jeh@cs.cornell.edu", "kqw4@cornell.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1511293277005, "id": "ICLR.cc/2017/conference/-/paper501/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper501/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper501/AnonReviewer2", "ICLR.cc/2017/conference/paper501/AnonReviewer1", "ICLR.cc/2017/conference/paper501/AnonReviewer3"], "reply": {"forum": "BJYwwY9ll", "replyto": "BJYwwY9ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper501/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper501/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1511293277005}}}, {"tddate": null, "tmdate": 1480631444734, "tcdate": 1480631444547, "number": 1, "id": "Sy65VX0zx", "invitation": "ICLR.cc/2017/conference/-/paper501/pre-review/question", "forum": "BJYwwY9ll", "replyto": "BJYwwY9ll", "signatures": ["ICLR.cc/2017/conference/paper501/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper501/AnonReviewer2"], "content": {"title": "Symmetry & hyperparameter selection", "question": "- A trivial way to obtain equivalent local minima is to permute the hidden units. Have you thought about ways to discourage this? (I'd be interested to what degree this happens, which might be measured by comparing activations in sorted order or something like that.)\n- It seems on first glance difficult to evaluate hyperparameter choices when you're intentionally allowing the objective function to grow. Can you comment on the feasibility of hyperparameter search in this model class? One particular (perhaps obvious) improvement that as far as I can tell isn't mentioned is that, at each convergence, the new member's validation set predictions can be saved and a running sum/average maintained, enabling cheap computation and monitoring of the *ensemble* validation error.\n- Is it necessary to enable re-training of the entire model with the cycled learning rates or would a few of the top layers suffice?\n\nA small comment: please consider that your figures may be read in grayscale (or by the colorblind) and try to make them legible without relying on color.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Snapshot Ensembles: Train 1, Get M for Free", "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters.  To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective.  We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields  lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.", "pdf": "/pdf/6012b9cd09762e2a9c7ad5b90014b8e6ac4a07b8.pdf", "paperhash": "huang|snapshot_ensembles_train_1_get_m_for_free", "keywords": ["Deep learning", "Computer vision"], "conflicts": ["cornell.edu", "tsinghua.edu.cn"], "authors": ["Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger"], "authorids": ["gh349@cornell.edu", "yl2363@cornell.edu", "geoff@cs.cornell.edu", "liuzhuangthu@gmail.com", "jeh@cs.cornell.edu", "kqw4@cornell.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1511293277005, "id": "ICLR.cc/2017/conference/-/paper501/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper501/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper501/AnonReviewer2", "ICLR.cc/2017/conference/paper501/AnonReviewer1", "ICLR.cc/2017/conference/paper501/AnonReviewer3"], "reply": {"forum": "BJYwwY9ll", "replyto": "BJYwwY9ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper501/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper501/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1511293277005}}}, {"tddate": null, "tmdate": 1479430593349, "tcdate": 1479430593345, "number": 6, "id": "SyKpbRiZl", "invitation": "ICLR.cc/2017/conference/-/paper501/public/comment", "forum": "BJYwwY9ll", "replyto": "BksEAsN-l", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "thanks", "comment": "Thank you for explaining this. I look forward to seeing the additional comparisons."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Snapshot Ensembles: Train 1, Get M for Free", "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters.  To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective.  We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields  lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.", "pdf": "/pdf/6012b9cd09762e2a9c7ad5b90014b8e6ac4a07b8.pdf", "paperhash": "huang|snapshot_ensembles_train_1_get_m_for_free", "keywords": ["Deep learning", "Computer vision"], "conflicts": ["cornell.edu", "tsinghua.edu.cn"], "authors": ["Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger"], "authorids": ["gh349@cornell.edu", "yl2363@cornell.edu", "geoff@cs.cornell.edu", "liuzhuangthu@gmail.com", "jeh@cs.cornell.edu", "kqw4@cornell.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287549952, "id": "ICLR.cc/2017/conference/-/paper501/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJYwwY9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper501/reviewers", "ICLR.cc/2017/conference/paper501/areachairs"], "cdate": 1485287549952}}}, {"tddate": null, "tmdate": 1478962769059, "tcdate": 1478916700381, "number": 4, "id": "S14v9eEbx", "invitation": "ICLR.cc/2017/conference/-/paper501/public/comment", "forum": "BJYwwY9ll", "replyto": "Hy3BcY7be", "signatures": ["~Gao_Huang1"], "readers": ["everyone"], "writers": ["~Gao_Huang1"], "content": {"title": "Thanks for pointing us to these relevant works", "comment": "Thanks for pointing us to these papers. They are indeed very relevant (it is hard to keep track these days.) We'll definitely reference those in our updated version. \n\nEssentially they are using what we compare against as our baseline - ensembling intermediate networks along the optimization path with a standard learning rate schedule (see \"NoCycle Snapshot Ensemble\" in Table 1). This works, but the cyclic learning rate schedule makes quite a difference, as shown by our results.\n\nIt might also be interesting to see if these papers would get better results if they switched to the SGDR cosine annealing learning rate schedule. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Snapshot Ensembles: Train 1, Get M for Free", "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters.  To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective.  We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields  lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.", "pdf": "/pdf/6012b9cd09762e2a9c7ad5b90014b8e6ac4a07b8.pdf", "paperhash": "huang|snapshot_ensembles_train_1_get_m_for_free", "keywords": ["Deep learning", "Computer vision"], "conflicts": ["cornell.edu", "tsinghua.edu.cn"], "authors": ["Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger"], "authorids": ["gh349@cornell.edu", "yl2363@cornell.edu", "geoff@cs.cornell.edu", "liuzhuangthu@gmail.com", "jeh@cs.cornell.edu", "kqw4@cornell.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287549952, "id": "ICLR.cc/2017/conference/-/paper501/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJYwwY9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper501/reviewers", "ICLR.cc/2017/conference/paper501/areachairs"], "cdate": 1485287549952}}}, {"tddate": null, "tmdate": 1478962738920, "tcdate": 1478962738914, "number": 5, "id": "BksEAsN-l", "invitation": "ICLR.cc/2017/conference/-/paper501/public/comment", "forum": "BJYwwY9ll", "replyto": "Hy4gEPRxx", "signatures": ["~Gao_Huang1"], "readers": ["everyone"], "writers": ["~Gao_Huang1"], "content": {"title": "Ensemble method", "comment": "Thanks for the comments. For the actual ensemble method in section 4.3, we use the average of the softmax probabilities of independently trained networks for prediction. This can be viewed as Bagging, but without subsampling the training data. It is a widely used and actually very strong approach for ensembling neural networks (e.g., it is adopted by many teams in the ILSVRC competition). We did have tried Bagging with data subsampling on CIFAR-10, but it didn\u2019t improve the ensemble results. \n\nWe agree that comparing with more traditional ensemble methods is interesting, and we will provide more baseline results in our updated version.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Snapshot Ensembles: Train 1, Get M for Free", "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters.  To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective.  We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields  lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.", "pdf": "/pdf/6012b9cd09762e2a9c7ad5b90014b8e6ac4a07b8.pdf", "paperhash": "huang|snapshot_ensembles_train_1_get_m_for_free", "keywords": ["Deep learning", "Computer vision"], "conflicts": ["cornell.edu", "tsinghua.edu.cn"], "authors": ["Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger"], "authorids": ["gh349@cornell.edu", "yl2363@cornell.edu", "geoff@cs.cornell.edu", "liuzhuangthu@gmail.com", "jeh@cs.cornell.edu", "kqw4@cornell.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287549952, "id": "ICLR.cc/2017/conference/-/paper501/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJYwwY9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper501/reviewers", "ICLR.cc/2017/conference/paper501/areachairs"], "cdate": 1485287549952}}}, {"tddate": null, "tmdate": 1478888037693, "tcdate": 1478888004367, "number": 3, "id": "Hy3BcY7be", "invitation": "ICLR.cc/2017/conference/-/paper501/public/comment", "forum": "BJYwwY9ll", "replyto": "BkanUOClx", "signatures": ["~Kyunghyun_Cho1"], "readers": ["everyone"], "writers": ["~Kyunghyun_Cho1"], "content": {"title": "Some more missing references", "comment": "I want to point out that the idea of snapshot ensemble has been used quite widely in the field of neural machine translation recently. Some of the works that report the improvement by building an ensemble of multiple snapshots include (but are not exclusive to)\n\nJean, Cho, Memisevic and Bengio. On Using Very Large Target Vocabulary for Neural Machine Translation. ACL 2015 https://arxiv.org/abs/1412.2007\nSennrich, Haddow and Birch. Edinburgh Neural Machine Translation Systems for WMT 16. WMT 2016 https://arxiv.org/abs/1606.02891\n\nThough, I must say I'm a fan of the proposed cyclic learning rate scheduling.\n\n(This was discussed on Twitter earlier: https://twitter.com/fchollet/status/795342292596891649)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Snapshot Ensembles: Train 1, Get M for Free", "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters.  To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective.  We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields  lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.", "pdf": "/pdf/6012b9cd09762e2a9c7ad5b90014b8e6ac4a07b8.pdf", "paperhash": "huang|snapshot_ensembles_train_1_get_m_for_free", "keywords": ["Deep learning", "Computer vision"], "conflicts": ["cornell.edu", "tsinghua.edu.cn"], "authors": ["Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger"], "authorids": ["gh349@cornell.edu", "yl2363@cornell.edu", "geoff@cs.cornell.edu", "liuzhuangthu@gmail.com", "jeh@cs.cornell.edu", "kqw4@cornell.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287549952, "id": "ICLR.cc/2017/conference/-/paper501/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJYwwY9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper501/reviewers", "ICLR.cc/2017/conference/paper501/areachairs"], "cdate": 1485287549952}}}, {"tddate": null, "tmdate": 1478555317184, "tcdate": 1478555317179, "number": 2, "id": "BkanUOClx", "invitation": "ICLR.cc/2017/conference/-/paper501/public/comment", "forum": "BJYwwY9ll", "replyto": "BJYwwY9ll", "signatures": ["~Dmytro_Mishkin1"], "readers": ["everyone"], "writers": ["~Dmytro_Mishkin1"], "content": {"title": "Missing reference, which is extremely relevant", "comment": "Horizontal and Vertical Ensemble with Deep Representation for Classification\nJingjing Xie, Bing Xu, Zhang Chuang\n\npage 3: \"Similar to the horizontal voting method in section 3.2, it takes the output of networks within a continuous range of epoch\"\n\nhttps://arxiv.org/abs/1306.2759"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Snapshot Ensembles: Train 1, Get M for Free", "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters.  To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective.  We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields  lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.", "pdf": "/pdf/6012b9cd09762e2a9c7ad5b90014b8e6ac4a07b8.pdf", "paperhash": "huang|snapshot_ensembles_train_1_get_m_for_free", "keywords": ["Deep learning", "Computer vision"], "conflicts": ["cornell.edu", "tsinghua.edu.cn"], "authors": ["Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger"], "authorids": ["gh349@cornell.edu", "yl2363@cornell.edu", "geoff@cs.cornell.edu", "liuzhuangthu@gmail.com", "jeh@cs.cornell.edu", "kqw4@cornell.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287549952, "id": "ICLR.cc/2017/conference/-/paper501/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJYwwY9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper501/reviewers", "ICLR.cc/2017/conference/paper501/areachairs"], "cdate": 1485287549952}}}, {"tddate": null, "tmdate": 1478550508210, "tcdate": 1478550508204, "number": 1, "id": "Hy4gEPRxx", "invitation": "ICLR.cc/2017/conference/-/paper501/public/comment", "forum": "BJYwwY9ll", "replyto": "BJYwwY9ll", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Comparison with Ensemble method", "comment": "This paper presents an interesting take on getting an ensemble for free whilst training a single network. However, your main accuracy comparison seems to exclude traditional ensemble methods, save for the very end of section 4.3, where the actual ensemble method you used is not mentioned. I would advise expanding this paragraph to explain what ensemble technique you compared against.\n\nDid you try comparing your results with other traditional ensembles like Bagging or AdaBoost? What were your results there? I understand that of course these are more expensive to train, but it would provide a baseline for a fair comparison of your results. It would be very interesting if you got close (or even beat!) these traditional expensive methods. At which point a comparison of training times would also be useful."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Snapshot Ensembles: Train 1, Get M for Free", "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters.  To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective.  We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields  lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.", "pdf": "/pdf/6012b9cd09762e2a9c7ad5b90014b8e6ac4a07b8.pdf", "paperhash": "huang|snapshot_ensembles_train_1_get_m_for_free", "keywords": ["Deep learning", "Computer vision"], "conflicts": ["cornell.edu", "tsinghua.edu.cn"], "authors": ["Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger"], "authorids": ["gh349@cornell.edu", "yl2363@cornell.edu", "geoff@cs.cornell.edu", "liuzhuangthu@gmail.com", "jeh@cs.cornell.edu", "kqw4@cornell.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287549952, "id": "ICLR.cc/2017/conference/-/paper501/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJYwwY9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper501/reviewers", "ICLR.cc/2017/conference/paper501/areachairs"], "cdate": 1485287549952}}}], "count": 37}