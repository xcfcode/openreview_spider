{"notes": [{"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457706478210, "tcdate": 1457706478210, "id": "NL6V98ox6F0VOPA8ix0l", "invitation": "ICLR.cc/2016/workshop/-/paper/193/review/11", "forum": "ANYzpXg3LcNrwlgXCq9G", "replyto": "ANYzpXg3LcNrwlgXCq9G", "signatures": ["ICLR.cc/2016/workshop/paper/193/reviewer/11"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/193/reviewer/11"], "content": {"title": "The paper introduces new algorithm for improving approximate posterior in variational inference and obtains very good results on images, both without and with this technique.", "rating": "9: Top 15% of accepted papers, strong accept", "review": "Pros: Authors extend normalizing flows to a more powerful family of function - autoregressive functions. Importantly despite autoregressive structure the computations are all parallel which is obtained by running the autoregressive transformation in the opposite direction. Second authors did a large amount experimentation with variational auto encoder architectures and obtain a very good performance in cifar dataset both with and without the inverse auto-regressive flow. \n\nCons: While they did explain the network details in the text, it is hard to parse it accurately enough to be able to reproduce. It would be good if they wrote a table or diagram with all the transformations and all the details. They can also publish the code. \n\nAdditional Comments: \nIt would be good to emphasize which z is fed into the decoder z_K or z_0 (I assume z_K). \n\nAbout the equivalence in section 4.4: The autoregressive prior model is a different model then the model of this paper. One can indeed change coordinates of the model of this paper so that the prior is autoregressive and approximate posterior is diagonal, but then one also has to change the model so that it undoes the autoregressive prior - defeating the purpose of autoregressive prior - making cheap deep generative model (deep \u201calong the layer\u201d)", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Improving Variational Inference with Inverse Autoregressive Flow", "abstract": "We propose a simple and practical method for improving the flexibility of the approximate posterior in variational auto-encoders (VAEs) through a transformation with autoregressive networks.\n\nAutoregressive networks, such as RNNs and RNADE networks, are very powerful models. However, their sequential nature makes them impractical for direct use with VAEs, as sequentially sampling the latent variables is slow when implemented on a GPU. Fortunately, we find that by inverting autoregressive networks we can obtain equally powerful data transformations that can be computed in parallel. We call these data transformations inverse autoregressive flows (IAF), and we show that they can be used to transform a simple distribution over the latent variables into a much more flexible distribution, while still allowing us to compute the resulting variables' probability density function. The method is computationally cheap, can be made arbitrarily flexible, and (in contrast with previous work) is naturally applicable to latent variables that are organized in multidimensional tensors, such as 2D grids or time series.\n\nThe method is applied to a novel deep architecture of variational auto-encoders. In experiments we demonstrate that autoregressive flow leads to significant performance gains when applied to variational autoencoders for natural images.", "pdf": "/pdf/ANYzpXg3LcNrwlgXCq9G.pdf", "paperhash": "kingma|improving_variational_inference_with_inverse_autoregressive_flow", "conflicts": ["uva.nl"], "authors": ["Diederik P. Kingma", "Tim Salimans", "Max Welling"], "authorids": ["D.P.Kingma@uva.nl", "salimans.tim@gmail.com", "M.Welling@uva.nl"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580183484, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580183484, "id": "ICLR.cc/2016/workshop/-/paper/193/review/11", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "ANYzpXg3LcNrwlgXCq9G", "replyto": "ANYzpXg3LcNrwlgXCq9G", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/193/reviewer/11", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457647602245, "tcdate": 1457647602245, "id": "WL9xVR1Zlc5zMX2Kf2mW", "invitation": "ICLR.cc/2016/workshop/-/paper/193/review/12", "forum": "ANYzpXg3LcNrwlgXCq9G", "replyto": "ANYzpXg3LcNrwlgXCq9G", "signatures": ["ICLR.cc/2016/workshop/paper/193/reviewer/12"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/193/reviewer/12"], "content": {"title": "", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper proposes to use the Gaussianization transform of an autoregressive model as an efficient way to implement the encoder of a variational autoencoder.\n\nThe proposed approach makes a lot of sense to me. Both the inverse / Gaussianization transform of a well-working autoregressive model and the posterior of a good variational autoencoder with Gaussian prior should on average map to an approximately Gaussian random variable, so it is reasonable to assume that the former is a useful building block for the latter.\n\nThe comparisons seem a bit limited but appropriate for a workshop paper.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Improving Variational Inference with Inverse Autoregressive Flow", "abstract": "We propose a simple and practical method for improving the flexibility of the approximate posterior in variational auto-encoders (VAEs) through a transformation with autoregressive networks.\n\nAutoregressive networks, such as RNNs and RNADE networks, are very powerful models. However, their sequential nature makes them impractical for direct use with VAEs, as sequentially sampling the latent variables is slow when implemented on a GPU. Fortunately, we find that by inverting autoregressive networks we can obtain equally powerful data transformations that can be computed in parallel. We call these data transformations inverse autoregressive flows (IAF), and we show that they can be used to transform a simple distribution over the latent variables into a much more flexible distribution, while still allowing us to compute the resulting variables' probability density function. The method is computationally cheap, can be made arbitrarily flexible, and (in contrast with previous work) is naturally applicable to latent variables that are organized in multidimensional tensors, such as 2D grids or time series.\n\nThe method is applied to a novel deep architecture of variational auto-encoders. In experiments we demonstrate that autoregressive flow leads to significant performance gains when applied to variational autoencoders for natural images.", "pdf": "/pdf/ANYzpXg3LcNrwlgXCq9G.pdf", "paperhash": "kingma|improving_variational_inference_with_inverse_autoregressive_flow", "conflicts": ["uva.nl"], "authors": ["Diederik P. Kingma", "Tim Salimans", "Max Welling"], "authorids": ["D.P.Kingma@uva.nl", "salimans.tim@gmail.com", "M.Welling@uva.nl"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580182628, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580182628, "id": "ICLR.cc/2016/workshop/-/paper/193/review/12", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "ANYzpXg3LcNrwlgXCq9G", "replyto": "ANYzpXg3LcNrwlgXCq9G", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/193/reviewer/12", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457635468275, "tcdate": 1457635468275, "id": "k80Yn6JjgtOYKX7ji49K", "invitation": "ICLR.cc/2016/workshop/-/paper/193/review/10", "forum": "ANYzpXg3LcNrwlgXCq9G", "replyto": "ANYzpXg3LcNrwlgXCq9G", "signatures": ["~Danilo_Jimenez_Rezende1"], "readers": ["everyone"], "writers": ["~Danilo_Jimenez_Rezende1"], "content": {"title": "The paper is well written and clearly explained. It introduces an original type of normalizing flow for inference networks. The proposed model significantly improves the scalability and general usability of previous related work.", "rating": "9: Top 15% of accepted papers, strong accept", "review": "This paper introduces a new form of Normalizing Flows, the Inverse Autoregressive Flow,  that is constructed by inverting an autoregressive map.\nThe resulting maps are easier to scale compared to previously proposed normalizing flows and autoregressive posterior networks. These flexible maps are used to equip VAEs with better inference networks.\n\nThe originality of this paper lies in the key observation that, while being as flexible as Autoregressive maps, Inverse Autoregressive maps do not require sequential computation and can be easily parallelized.\nFurthermore, as for Normalizing Flows, it is cheap to compute the log-det-Jacobian terms for Inverse Autoregressive Flows. These terms are necessary to compute the likelihood of transformed samples.\n\nThe paper is well written and clearly explained. Perhaps the order of its narrative could be improved a bit by first introducing the challenges of  flow-based inference models (e.g. sequentiality in autoregressive models and the usual O(d^3) cost in computing log-det-Jacobian terms). Once these challenges are explained, the authors could then explain how the Inverse Autoregressive Flows address them and only then show that they can be interpreted as the inverse of an autoregressive map.\n\nThe authors show that training convolutional VAEs with the Inverse Autoregressive Flows as inference network results in model with better log-likelihoods. \nInstead of reporting the bound and the estimated likelihoods in Table 1, it would be good to show the estimated likelihoods and the KL-divergence between the true posterior and the variational posterior (difference between the variational bound and the estimated log-likelihoods). In this way, the readers could see more easily that the KLD between the true posterior and the variational posterior is actually smaller.\n\nThe idea introduced in this paper lives in the intersection of previous work such as Normalizing Flows, NICE and MADE. But the particular instantiation of the model introduced in this paper constitutes a significant improvement of previous work in terms of scalability and general applicability to amortized inference.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Improving Variational Inference with Inverse Autoregressive Flow", "abstract": "We propose a simple and practical method for improving the flexibility of the approximate posterior in variational auto-encoders (VAEs) through a transformation with autoregressive networks.\n\nAutoregressive networks, such as RNNs and RNADE networks, are very powerful models. However, their sequential nature makes them impractical for direct use with VAEs, as sequentially sampling the latent variables is slow when implemented on a GPU. Fortunately, we find that by inverting autoregressive networks we can obtain equally powerful data transformations that can be computed in parallel. We call these data transformations inverse autoregressive flows (IAF), and we show that they can be used to transform a simple distribution over the latent variables into a much more flexible distribution, while still allowing us to compute the resulting variables' probability density function. The method is computationally cheap, can be made arbitrarily flexible, and (in contrast with previous work) is naturally applicable to latent variables that are organized in multidimensional tensors, such as 2D grids or time series.\n\nThe method is applied to a novel deep architecture of variational auto-encoders. In experiments we demonstrate that autoregressive flow leads to significant performance gains when applied to variational autoencoders for natural images.", "pdf": "/pdf/ANYzpXg3LcNrwlgXCq9G.pdf", "paperhash": "kingma|improving_variational_inference_with_inverse_autoregressive_flow", "conflicts": ["uva.nl"], "authors": ["Diederik P. Kingma", "Tim Salimans", "Max Welling"], "authorids": ["D.P.Kingma@uva.nl", "salimans.tim@gmail.com", "M.Welling@uva.nl"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580183767, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580183767, "id": "ICLR.cc/2016/workshop/-/paper/193/review/10", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "ANYzpXg3LcNrwlgXCq9G", "replyto": "ANYzpXg3LcNrwlgXCq9G", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/193/reviewer/10", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457113445993, "tcdate": 1457113445993, "id": "91E0yx45wfkRlNvXUV3J", "invitation": "ICLR.cc/2016/workshop/-/paper/193/comment", "forum": "ANYzpXg3LcNrwlgXCq9G", "replyto": "ANYzpXg3LcNrwlgXCq9G", "signatures": ["~Tim_Salimans1"], "readers": ["everyone"], "writers": ["~Tim_Salimans1"], "content": {"title": "Updated paper", "comment": "Here's a slightly updated version of our workshop submission:\nhttps://drive.google.com/file/d/0B3OM09ncycBoZF93UWN6a1hFWDQ/view?usp=sharing"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Improving Variational Inference with Inverse Autoregressive Flow", "abstract": "We propose a simple and practical method for improving the flexibility of the approximate posterior in variational auto-encoders (VAEs) through a transformation with autoregressive networks.\n\nAutoregressive networks, such as RNNs and RNADE networks, are very powerful models. However, their sequential nature makes them impractical for direct use with VAEs, as sequentially sampling the latent variables is slow when implemented on a GPU. Fortunately, we find that by inverting autoregressive networks we can obtain equally powerful data transformations that can be computed in parallel. We call these data transformations inverse autoregressive flows (IAF), and we show that they can be used to transform a simple distribution over the latent variables into a much more flexible distribution, while still allowing us to compute the resulting variables' probability density function. The method is computationally cheap, can be made arbitrarily flexible, and (in contrast with previous work) is naturally applicable to latent variables that are organized in multidimensional tensors, such as 2D grids or time series.\n\nThe method is applied to a novel deep architecture of variational auto-encoders. In experiments we demonstrate that autoregressive flow leads to significant performance gains when applied to variational autoencoders for natural images.", "pdf": "/pdf/ANYzpXg3LcNrwlgXCq9G.pdf", "paperhash": "kingma|improving_variational_inference_with_inverse_autoregressive_flow", "conflicts": ["uva.nl"], "authors": ["Diederik P. Kingma", "Tim Salimans", "Max Welling"], "authorids": ["D.P.Kingma@uva.nl", "salimans.tim@gmail.com", "M.Welling@uva.nl"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455833113384, "ddate": null, "super": null, "final": null, "tcdate": 1455833113384, "id": "ICLR.cc/2016/workshop/-/paper/193/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "ANYzpXg3LcNrwlgXCq9G", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/193/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1455833107335, "tcdate": 1455833107335, "id": "ANYzpXg3LcNrwlgXCq9G", "invitation": "ICLR.cc/2016/workshop/-/submission", "forum": "ANYzpXg3LcNrwlgXCq9G", "signatures": ["~Tim_Salimans1"], "readers": ["everyone"], "writers": ["~Tim_Salimans1"], "content": {"CMT_id": "", "title": "Improving Variational Inference with Inverse Autoregressive Flow", "abstract": "We propose a simple and practical method for improving the flexibility of the approximate posterior in variational auto-encoders (VAEs) through a transformation with autoregressive networks.\n\nAutoregressive networks, such as RNNs and RNADE networks, are very powerful models. However, their sequential nature makes them impractical for direct use with VAEs, as sequentially sampling the latent variables is slow when implemented on a GPU. Fortunately, we find that by inverting autoregressive networks we can obtain equally powerful data transformations that can be computed in parallel. We call these data transformations inverse autoregressive flows (IAF), and we show that they can be used to transform a simple distribution over the latent variables into a much more flexible distribution, while still allowing us to compute the resulting variables' probability density function. The method is computationally cheap, can be made arbitrarily flexible, and (in contrast with previous work) is naturally applicable to latent variables that are organized in multidimensional tensors, such as 2D grids or time series.\n\nThe method is applied to a novel deep architecture of variational auto-encoders. In experiments we demonstrate that autoregressive flow leads to significant performance gains when applied to variational autoencoders for natural images.", "pdf": "/pdf/ANYzpXg3LcNrwlgXCq9G.pdf", "paperhash": "kingma|improving_variational_inference_with_inverse_autoregressive_flow", "conflicts": ["uva.nl"], "authors": ["Diederik P. Kingma", "Tim Salimans", "Max Welling"], "authorids": ["D.P.Kingma@uva.nl", "salimans.tim@gmail.com", "M.Welling@uva.nl"]}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1454464564200, "ddate": null, "super": null, "final": null, "duedate": 1455833700000, "tcdate": 1454464564200, "id": "ICLR.cc/2016/workshop/-/submission", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"order": 4, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv.", "value-regex": "upload|http://arxiv.org/pdf/.+"}, "title": {"order": 3, "description": "Title of paper.", "value-regex": ".{0,500}"}, "abstract": {"order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"order": 1, "description": "Comma separated list of author names, as they appear in the paper.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "author_emails": {"order": 2, "description": "Comma separated list of author email addresses, in the same order as above.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "conflicts": {"order": 100, "description": "Semi-colon separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.).", "value-regex": "^([a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))+(;[a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))*$"}, "CMT_id": {"order": 5, "value-regex": ".*", "description": "If the paper is a resubmission from the ICLR 2016 Conference Track, enter its CMT ID; otherwise, leave blank."}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "expdate": 1463609700000}}}], "count": 5}