{"notes": [{"id": "S1l66nNFvB", "original": "SygFTI-VDS", "number": 243, "cdate": 1569438916637, "ddate": null, "tcdate": 1569438916637, "tmdate": 1577168251051, "tddate": null, "forum": "S1l66nNFvB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Graph Warp Module: an Auxiliary Module for Boosting the Power of Graph Neural Networks in Molecular Graph Analysis", "authors": ["Katsuhiko Ishiguro", "Shin-ichi Maeda", "Masanori Koyama"], "authorids": ["k.ishiguro.jp@ieee.org", "ichi@preferred.jp", "masomatics@preferred.jp"], "keywords": ["Graph Neural Networks", "molecular graph analysis", "supernode", "auxiliary module"], "TL;DR": "Proposing an auxiliary  module with its own I/O that can be attached to a generic GNN of message passing type in order to improve its representation power/ generalization performance on small-graph datasets.", "abstract": "Graph Neural Network (GNN) is a popular architecture for the analysis of chemical molecules, and it has numerous applications in material and medicinal science.\nCurrent lines of GNNs developed for molecular analysis, however, do not fit well on the training set, and their performance does not scale well with the complexity of the network. \nIn this paper, we propose an auxiliary module to be attached to a GNN that can boost the representation power of the model without hindering the original GNN architecture. \nOur auxiliary module can improve the representation power and the generalization ability of a wide variety of GNNs, including those that are used commonly in biochemical applications. ", "pdf": "/pdf/a5f1e3d7b24fa071758e07f660b65951f05ba777.pdf", "paperhash": "ishiguro|graph_warp_module_an_auxiliary_module_for_boosting_the_power_of_graph_neural_networks_in_molecular_graph_analysis", "original_pdf": "/attachment/d48e31022af8d37e19042f04fc9333de199af802.pdf", "_bibtex": "@misc{\nishiguro2020graph,\ntitle={Graph Warp Module: an Auxiliary Module for Boosting the Power of Graph Neural Networks in Molecular Graph Analysis},\nauthor={Katsuhiko Ishiguro and Shin-ichi Maeda and Masanori Koyama},\nyear={2020},\nurl={https://openreview.net/forum?id=S1l66nNFvB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "MPn_KALmoh", "original": null, "number": 1, "cdate": 1576798691186, "ddate": null, "tcdate": 1576798691186, "tmdate": 1576800944058, "tddate": null, "forum": "S1l66nNFvB", "replyto": "S1l66nNFvB", "invitation": "ICLR.cc/2020/Conference/Paper243/-/Decision", "content": {"decision": "Reject", "comment": "This paper presents an auxiliary module to boost the representation power of GNNs. The new module consists of virtual supernode, attention unit, and warp gate unit. The usefulness of each component is shown in well-organized experiments.\nThis is the very borderline paper with split scores. While all reviewers basically agree that the empirical findings in the paper are interesting and could be valuable to the community, one reviewer raised concern regarding the incremental novelty of the method, which is also understood by other reviewers. The impression was not changed through authors\u2019 response and reviewer discussion, and there is no strong opinion to champion the paper. Therefore, I\u2019d like to recommend rejection this time. \n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Warp Module: an Auxiliary Module for Boosting the Power of Graph Neural Networks in Molecular Graph Analysis", "authors": ["Katsuhiko Ishiguro", "Shin-ichi Maeda", "Masanori Koyama"], "authorids": ["k.ishiguro.jp@ieee.org", "ichi@preferred.jp", "masomatics@preferred.jp"], "keywords": ["Graph Neural Networks", "molecular graph analysis", "supernode", "auxiliary module"], "TL;DR": "Proposing an auxiliary  module with its own I/O that can be attached to a generic GNN of message passing type in order to improve its representation power/ generalization performance on small-graph datasets.", "abstract": "Graph Neural Network (GNN) is a popular architecture for the analysis of chemical molecules, and it has numerous applications in material and medicinal science.\nCurrent lines of GNNs developed for molecular analysis, however, do not fit well on the training set, and their performance does not scale well with the complexity of the network. \nIn this paper, we propose an auxiliary module to be attached to a GNN that can boost the representation power of the model without hindering the original GNN architecture. \nOur auxiliary module can improve the representation power and the generalization ability of a wide variety of GNNs, including those that are used commonly in biochemical applications. ", "pdf": "/pdf/a5f1e3d7b24fa071758e07f660b65951f05ba777.pdf", "paperhash": "ishiguro|graph_warp_module_an_auxiliary_module_for_boosting_the_power_of_graph_neural_networks_in_molecular_graph_analysis", "original_pdf": "/attachment/d48e31022af8d37e19042f04fc9333de199af802.pdf", "_bibtex": "@misc{\nishiguro2020graph,\ntitle={Graph Warp Module: an Auxiliary Module for Boosting the Power of Graph Neural Networks in Molecular Graph Analysis},\nauthor={Katsuhiko Ishiguro and Shin-ichi Maeda and Masanori Koyama},\nyear={2020},\nurl={https://openreview.net/forum?id=S1l66nNFvB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "S1l66nNFvB", "replyto": "S1l66nNFvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795722060, "tmdate": 1576800273277, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper243/-/Decision"}}}, {"id": "S1gLZR0Ljr", "original": null, "number": 8, "cdate": 1573477885536, "ddate": null, "tcdate": 1573477885536, "tmdate": 1573477885536, "tddate": null, "forum": "S1l66nNFvB", "replyto": "BkesSFrUjB", "invitation": "ICLR.cc/2020/Conference/Paper243/-/Official_Comment", "content": {"title": "Thanks", "comment": "Thank you for the clarification and for updating the paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper243/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper243/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Warp Module: an Auxiliary Module for Boosting the Power of Graph Neural Networks in Molecular Graph Analysis", "authors": ["Katsuhiko Ishiguro", "Shin-ichi Maeda", "Masanori Koyama"], "authorids": ["k.ishiguro.jp@ieee.org", "ichi@preferred.jp", "masomatics@preferred.jp"], "keywords": ["Graph Neural Networks", "molecular graph analysis", "supernode", "auxiliary module"], "TL;DR": "Proposing an auxiliary  module with its own I/O that can be attached to a generic GNN of message passing type in order to improve its representation power/ generalization performance on small-graph datasets.", "abstract": "Graph Neural Network (GNN) is a popular architecture for the analysis of chemical molecules, and it has numerous applications in material and medicinal science.\nCurrent lines of GNNs developed for molecular analysis, however, do not fit well on the training set, and their performance does not scale well with the complexity of the network. \nIn this paper, we propose an auxiliary module to be attached to a GNN that can boost the representation power of the model without hindering the original GNN architecture. \nOur auxiliary module can improve the representation power and the generalization ability of a wide variety of GNNs, including those that are used commonly in biochemical applications. ", "pdf": "/pdf/a5f1e3d7b24fa071758e07f660b65951f05ba777.pdf", "paperhash": "ishiguro|graph_warp_module_an_auxiliary_module_for_boosting_the_power_of_graph_neural_networks_in_molecular_graph_analysis", "original_pdf": "/attachment/d48e31022af8d37e19042f04fc9333de199af802.pdf", "_bibtex": "@misc{\nishiguro2020graph,\ntitle={Graph Warp Module: an Auxiliary Module for Boosting the Power of Graph Neural Networks in Molecular Graph Analysis},\nauthor={Katsuhiko Ishiguro and Shin-ichi Maeda and Masanori Koyama},\nyear={2020},\nurl={https://openreview.net/forum?id=S1l66nNFvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1l66nNFvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper243/Authors", "ICLR.cc/2020/Conference/Paper243/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper243/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper243/Reviewers", "ICLR.cc/2020/Conference/Paper243/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper243/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper243/Authors|ICLR.cc/2020/Conference/Paper243/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504174268, "tmdate": 1576860546920, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper243/Authors", "ICLR.cc/2020/Conference/Paper243/Reviewers", "ICLR.cc/2020/Conference/Paper243/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper243/-/Official_Comment"}}}, {"id": "rJe3Jqr8oH", "original": null, "number": 7, "cdate": 1573439971768, "ddate": null, "tcdate": 1573439971768, "tmdate": 1573439971768, "tddate": null, "forum": "S1l66nNFvB", "replyto": "HJeaT6S-5B", "invitation": "ICLR.cc/2020/Conference/Paper243/-/Official_Comment", "content": {"title": "Reply to the official review #3", "comment": "Thank you very much for the comments! \nWhile we believe that supernode is an essential component of our invention, we also believe that supernode alone is not sufficient. \nFor this claim,  please notice that our method performs much better than the ablation model with no attention/gate mechanism (simple supernode), which is, in essence, the same as an architecture proposed by (Pham et al  2017).  \nIn order to make this point more clear, we added explanations and discussions about the relationship of (Pham et al. 2017) and our ablated model (simple supernode) in the updated manuscript (Table1, Sec. 4.4, and the appendix C.6. )\n\nAlso, as far as we understand,  (Battaglia et al 2018) is a survey paper, and that they do not make a proposal for a new algorithm in their work. \nFinally, thank you very much for pointing out our imprecise description of (Li et al 2017).\nWe fixed the relevant phrases in Table 1 and Sec 4.4."}, "signatures": ["ICLR.cc/2020/Conference/Paper243/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper243/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Warp Module: an Auxiliary Module for Boosting the Power of Graph Neural Networks in Molecular Graph Analysis", "authors": ["Katsuhiko Ishiguro", "Shin-ichi Maeda", "Masanori Koyama"], "authorids": ["k.ishiguro.jp@ieee.org", "ichi@preferred.jp", "masomatics@preferred.jp"], "keywords": ["Graph Neural Networks", "molecular graph analysis", "supernode", "auxiliary module"], "TL;DR": "Proposing an auxiliary  module with its own I/O that can be attached to a generic GNN of message passing type in order to improve its representation power/ generalization performance on small-graph datasets.", "abstract": "Graph Neural Network (GNN) is a popular architecture for the analysis of chemical molecules, and it has numerous applications in material and medicinal science.\nCurrent lines of GNNs developed for molecular analysis, however, do not fit well on the training set, and their performance does not scale well with the complexity of the network. \nIn this paper, we propose an auxiliary module to be attached to a GNN that can boost the representation power of the model without hindering the original GNN architecture. \nOur auxiliary module can improve the representation power and the generalization ability of a wide variety of GNNs, including those that are used commonly in biochemical applications. ", "pdf": "/pdf/a5f1e3d7b24fa071758e07f660b65951f05ba777.pdf", "paperhash": "ishiguro|graph_warp_module_an_auxiliary_module_for_boosting_the_power_of_graph_neural_networks_in_molecular_graph_analysis", "original_pdf": "/attachment/d48e31022af8d37e19042f04fc9333de199af802.pdf", "_bibtex": "@misc{\nishiguro2020graph,\ntitle={Graph Warp Module: an Auxiliary Module for Boosting the Power of Graph Neural Networks in Molecular Graph Analysis},\nauthor={Katsuhiko Ishiguro and Shin-ichi Maeda and Masanori Koyama},\nyear={2020},\nurl={https://openreview.net/forum?id=S1l66nNFvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1l66nNFvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper243/Authors", "ICLR.cc/2020/Conference/Paper243/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper243/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper243/Reviewers", "ICLR.cc/2020/Conference/Paper243/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper243/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper243/Authors|ICLR.cc/2020/Conference/Paper243/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504174268, "tmdate": 1576860546920, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper243/Authors", "ICLR.cc/2020/Conference/Paper243/Reviewers", "ICLR.cc/2020/Conference/Paper243/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper243/-/Official_Comment"}}}, {"id": "rygoiFrLoB", "original": null, "number": 6, "cdate": 1573439907229, "ddate": null, "tcdate": 1573439907229, "tmdate": 1573439907229, "tddate": null, "forum": "S1l66nNFvB", "replyto": "rJlciCPDtS", "invitation": "ICLR.cc/2020/Conference/Paper243/-/Official_Comment", "content": {"title": "Reply to the official blind review #2", "comment": "Thank you very much for positive comments and reviews. \nAs for the word-choices, we might change the naming of the submodules and expression in order to make the writing easier to read. \n\nAlso, thank you very much for the information about  https://openreview.net/forum?id=B1lnbRNtwr. \nWe indeed believe that there is a connection to our work.\nOne notable difference is that our supernode submodule has a network of its own with its own memory,  and that we are giving more freedom to how the global information with the local information is mixed. \nWe added this discussion in the updated manuscript (Sec. 2.2). \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper243/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper243/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Warp Module: an Auxiliary Module for Boosting the Power of Graph Neural Networks in Molecular Graph Analysis", "authors": ["Katsuhiko Ishiguro", "Shin-ichi Maeda", "Masanori Koyama"], "authorids": ["k.ishiguro.jp@ieee.org", "ichi@preferred.jp", "masomatics@preferred.jp"], "keywords": ["Graph Neural Networks", "molecular graph analysis", "supernode", "auxiliary module"], "TL;DR": "Proposing an auxiliary  module with its own I/O that can be attached to a generic GNN of message passing type in order to improve its representation power/ generalization performance on small-graph datasets.", "abstract": "Graph Neural Network (GNN) is a popular architecture for the analysis of chemical molecules, and it has numerous applications in material and medicinal science.\nCurrent lines of GNNs developed for molecular analysis, however, do not fit well on the training set, and their performance does not scale well with the complexity of the network. \nIn this paper, we propose an auxiliary module to be attached to a GNN that can boost the representation power of the model without hindering the original GNN architecture. \nOur auxiliary module can improve the representation power and the generalization ability of a wide variety of GNNs, including those that are used commonly in biochemical applications. ", "pdf": "/pdf/a5f1e3d7b24fa071758e07f660b65951f05ba777.pdf", "paperhash": "ishiguro|graph_warp_module_an_auxiliary_module_for_boosting_the_power_of_graph_neural_networks_in_molecular_graph_analysis", "original_pdf": "/attachment/d48e31022af8d37e19042f04fc9333de199af802.pdf", "_bibtex": "@misc{\nishiguro2020graph,\ntitle={Graph Warp Module: an Auxiliary Module for Boosting the Power of Graph Neural Networks in Molecular Graph Analysis},\nauthor={Katsuhiko Ishiguro and Shin-ichi Maeda and Masanori Koyama},\nyear={2020},\nurl={https://openreview.net/forum?id=S1l66nNFvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1l66nNFvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper243/Authors", "ICLR.cc/2020/Conference/Paper243/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper243/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper243/Reviewers", "ICLR.cc/2020/Conference/Paper243/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper243/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper243/Authors|ICLR.cc/2020/Conference/Paper243/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504174268, "tmdate": 1576860546920, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper243/Authors", "ICLR.cc/2020/Conference/Paper243/Reviewers", "ICLR.cc/2020/Conference/Paper243/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper243/-/Official_Comment"}}}, {"id": "BkesSFrUjB", "original": null, "number": 5, "cdate": 1573439811191, "ddate": null, "tcdate": 1573439811191, "tmdate": 1573439811191, "tddate": null, "forum": "S1l66nNFvB", "replyto": "Hkg526KzqB", "invitation": "ICLR.cc/2020/Conference/Paper243/-/Official_Comment", "content": {"title": "Reply to the official blind review #1", "comment": "Thank you for your positive reviews and comments. \nAs for your concern about the \u201cability of our GWM to overfit\u201d, we do not intend to make as strong a statement as to say that GWM can help assure that one can indefinitely increase the model\u2019s representation power by adding more and more layers.\nHowever, as can be seen in Table 4, unlike the vanilla models,  the performance of the GWM-attached models improve with the number of layers until the model depth reaches quite a large number (~8) (e.g. [QM9, GGNN+Proposed GWM], [HIV, GIN+Proposed GWM]). \nOut of 24 pairs of (dataset, GNN), 11 pairs favor much deeper architecture when deployed with GWM.  \nWe added this discussion in the updated manuscript (Appendix D.4) \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper243/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper243/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Warp Module: an Auxiliary Module for Boosting the Power of Graph Neural Networks in Molecular Graph Analysis", "authors": ["Katsuhiko Ishiguro", "Shin-ichi Maeda", "Masanori Koyama"], "authorids": ["k.ishiguro.jp@ieee.org", "ichi@preferred.jp", "masomatics@preferred.jp"], "keywords": ["Graph Neural Networks", "molecular graph analysis", "supernode", "auxiliary module"], "TL;DR": "Proposing an auxiliary  module with its own I/O that can be attached to a generic GNN of message passing type in order to improve its representation power/ generalization performance on small-graph datasets.", "abstract": "Graph Neural Network (GNN) is a popular architecture for the analysis of chemical molecules, and it has numerous applications in material and medicinal science.\nCurrent lines of GNNs developed for molecular analysis, however, do not fit well on the training set, and their performance does not scale well with the complexity of the network. \nIn this paper, we propose an auxiliary module to be attached to a GNN that can boost the representation power of the model without hindering the original GNN architecture. \nOur auxiliary module can improve the representation power and the generalization ability of a wide variety of GNNs, including those that are used commonly in biochemical applications. ", "pdf": "/pdf/a5f1e3d7b24fa071758e07f660b65951f05ba777.pdf", "paperhash": "ishiguro|graph_warp_module_an_auxiliary_module_for_boosting_the_power_of_graph_neural_networks_in_molecular_graph_analysis", "original_pdf": "/attachment/d48e31022af8d37e19042f04fc9333de199af802.pdf", "_bibtex": "@misc{\nishiguro2020graph,\ntitle={Graph Warp Module: an Auxiliary Module for Boosting the Power of Graph Neural Networks in Molecular Graph Analysis},\nauthor={Katsuhiko Ishiguro and Shin-ichi Maeda and Masanori Koyama},\nyear={2020},\nurl={https://openreview.net/forum?id=S1l66nNFvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1l66nNFvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper243/Authors", "ICLR.cc/2020/Conference/Paper243/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper243/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper243/Reviewers", "ICLR.cc/2020/Conference/Paper243/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper243/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper243/Authors|ICLR.cc/2020/Conference/Paper243/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504174268, "tmdate": 1576860546920, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper243/Authors", "ICLR.cc/2020/Conference/Paper243/Reviewers", "ICLR.cc/2020/Conference/Paper243/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper243/-/Official_Comment"}}}, {"id": "HJeaT6S-5B", "original": null, "number": 2, "cdate": 1572064709236, "ddate": null, "tcdate": 1572064709236, "tmdate": 1572972620307, "tddate": null, "forum": "S1l66nNFvB", "replyto": "S1l66nNFvB", "invitation": "ICLR.cc/2020/Conference/Paper243/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper proposes an auxiliary module for GNNs to boost the representation power. The new module consists of virtual supernode, attention unit, and gating unit, each of which is demonstrated useful in the experiments. The module can be applied to various types of GNNs. \n\nThis work can be seen as an improvement to previous virtual supernode based methods. Adding the attention units and gating units is rational, and the effectiveness is also proved in the ablation studies. However, the claimed contribution of improving the representation power may mainly come from the idea of supernodes (instead of the attention and gating). This largely reduces the novelty of this paper and make it incremental, because using virtual supernodes is not this paper\u2019s original idea.\n\nThe paper is generally well written. However, the comparison with previous supernode based models is not described clearly enough. The authors listed the difference from (Glimer et al. 2017) and (Li et al. 2017) in Table 1, but ignored (Pham et al. 2017) and (Battaglia et al. 2018), which were also cited in the related work. Moreover, (Li et al. 2017)\u2019s method is actually different from the simple supernode baseline, in that it is not a bidirectional message passing between supernode and the main network. Table 1 does not contain this property.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper243/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper243/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Warp Module: an Auxiliary Module for Boosting the Power of Graph Neural Networks in Molecular Graph Analysis", "authors": ["Katsuhiko Ishiguro", "Shin-ichi Maeda", "Masanori Koyama"], "authorids": ["k.ishiguro.jp@ieee.org", "ichi@preferred.jp", "masomatics@preferred.jp"], "keywords": ["Graph Neural Networks", "molecular graph analysis", "supernode", "auxiliary module"], "TL;DR": "Proposing an auxiliary  module with its own I/O that can be attached to a generic GNN of message passing type in order to improve its representation power/ generalization performance on small-graph datasets.", "abstract": "Graph Neural Network (GNN) is a popular architecture for the analysis of chemical molecules, and it has numerous applications in material and medicinal science.\nCurrent lines of GNNs developed for molecular analysis, however, do not fit well on the training set, and their performance does not scale well with the complexity of the network. \nIn this paper, we propose an auxiliary module to be attached to a GNN that can boost the representation power of the model without hindering the original GNN architecture. \nOur auxiliary module can improve the representation power and the generalization ability of a wide variety of GNNs, including those that are used commonly in biochemical applications. ", "pdf": "/pdf/a5f1e3d7b24fa071758e07f660b65951f05ba777.pdf", "paperhash": "ishiguro|graph_warp_module_an_auxiliary_module_for_boosting_the_power_of_graph_neural_networks_in_molecular_graph_analysis", "original_pdf": "/attachment/d48e31022af8d37e19042f04fc9333de199af802.pdf", "_bibtex": "@misc{\nishiguro2020graph,\ntitle={Graph Warp Module: an Auxiliary Module for Boosting the Power of Graph Neural Networks in Molecular Graph Analysis},\nauthor={Katsuhiko Ishiguro and Shin-ichi Maeda and Masanori Koyama},\nyear={2020},\nurl={https://openreview.net/forum?id=S1l66nNFvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1l66nNFvB", "replyto": "S1l66nNFvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper243/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper243/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575697957755, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper243/Reviewers"], "noninvitees": [], "tcdate": 1570237754947, "tmdate": 1575697957776, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper243/-/Official_Review"}}}, {"id": "Hkg526KzqB", "original": null, "number": 3, "cdate": 1572146609839, "ddate": null, "tcdate": 1572146609839, "tmdate": 1572972620261, "tddate": null, "forum": "S1l66nNFvB", "replyto": "S1l66nNFvB", "invitation": "ICLR.cc/2020/Conference/Paper243/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Graph Neural Networks is a popular architecture for the analysis of chemical molecules. The authors propose an auxiliary module that can be attached to a GNN that can boost the representation power of GNNs. The auxiliary module has three building blocks: 1. a supernode, 2. a transmitter unit and 3. a warp gate unit. The authors show through carefully designed experiments that these additions can be attached to any type of GNN and that they are successful in reducing both the training error and test error. A variety of graph regression and graph classification tasks are chosen to show the efficacy of the method.\n\nThe paper is well written and easy to follow. The modification suggested by the authors is novel and useful. Experiments are well designed. An aspect that is not clear from the paper is the ability of these models to overfit the data as you increase the representation power of the network. While the authors claim that to be one of the shortcomings of existing GNNs, it is not clear whether the proposed method solves that problem. For example, in figure 4. the training loss hardly decreases as the number of layers are increased. It would be good if the authors can share any insights on this point. \n\nOverall, I think this is a good paper that the community will benefit from."}, "signatures": ["ICLR.cc/2020/Conference/Paper243/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper243/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Warp Module: an Auxiliary Module for Boosting the Power of Graph Neural Networks in Molecular Graph Analysis", "authors": ["Katsuhiko Ishiguro", "Shin-ichi Maeda", "Masanori Koyama"], "authorids": ["k.ishiguro.jp@ieee.org", "ichi@preferred.jp", "masomatics@preferred.jp"], "keywords": ["Graph Neural Networks", "molecular graph analysis", "supernode", "auxiliary module"], "TL;DR": "Proposing an auxiliary  module with its own I/O that can be attached to a generic GNN of message passing type in order to improve its representation power/ generalization performance on small-graph datasets.", "abstract": "Graph Neural Network (GNN) is a popular architecture for the analysis of chemical molecules, and it has numerous applications in material and medicinal science.\nCurrent lines of GNNs developed for molecular analysis, however, do not fit well on the training set, and their performance does not scale well with the complexity of the network. \nIn this paper, we propose an auxiliary module to be attached to a GNN that can boost the representation power of the model without hindering the original GNN architecture. \nOur auxiliary module can improve the representation power and the generalization ability of a wide variety of GNNs, including those that are used commonly in biochemical applications. ", "pdf": "/pdf/a5f1e3d7b24fa071758e07f660b65951f05ba777.pdf", "paperhash": "ishiguro|graph_warp_module_an_auxiliary_module_for_boosting_the_power_of_graph_neural_networks_in_molecular_graph_analysis", "original_pdf": "/attachment/d48e31022af8d37e19042f04fc9333de199af802.pdf", "_bibtex": "@misc{\nishiguro2020graph,\ntitle={Graph Warp Module: an Auxiliary Module for Boosting the Power of Graph Neural Networks in Molecular Graph Analysis},\nauthor={Katsuhiko Ishiguro and Shin-ichi Maeda and Masanori Koyama},\nyear={2020},\nurl={https://openreview.net/forum?id=S1l66nNFvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1l66nNFvB", "replyto": "S1l66nNFvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper243/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper243/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575697957755, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper243/Reviewers"], "noninvitees": [], "tcdate": 1570237754947, "tmdate": 1575697957776, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper243/-/Official_Review"}}}, {"id": "rJlciCPDtS", "original": null, "number": 1, "cdate": 1571417762091, "ddate": null, "tcdate": 1571417762091, "tmdate": 1572972620217, "tddate": null, "forum": "S1l66nNFvB", "replyto": "S1l66nNFvB", "invitation": "ICLR.cc/2020/Conference/Paper243/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "= Summary\nA Graph Neural Network extension integrating a global supernode explicitly into the message passing process. Concretely, message passing along the graph edges is alternated with message passing to/from a fresh super-node. Experiments show that this improves results of a number of common GNN architectures on four datasets.\n\n= Strong/Weak Points\n+ Simple but useful extension of the existing super-node idea\n+ Experiments on a number of datasets and baseline GNN architectures, providing ample experimental evidence of the usefulness of the method.\n- Writing is overcomplicated and uses a lot of jargon (\"transmitter unit\", \"warp gate\", \"intermodule hyperspace\"). I found the text entirely impenetrable and instead simply focused on Fig. 3 + the actual equations.\n\n= Recommendation\nThis is a nice contribution of minor novelty, with empirical evidence of its usefulness. I believe the paper should be accepted to a large conference such as ICLR.\n\n= Minor Comments\n- Fig. 3: Inconsistent \"intra-module\" (top) vs. \"intra module\" (bottom) \n- Concurrent work in https://openreview.net/forum?id=B1lnbRNtwr discusses a \"sandwich\" model which alternates graph message passing with (essentially) a Transformer layer applied to all nodes. This idea seems related (in that it alternates local and global information exchange)."}, "signatures": ["ICLR.cc/2020/Conference/Paper243/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper243/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Warp Module: an Auxiliary Module for Boosting the Power of Graph Neural Networks in Molecular Graph Analysis", "authors": ["Katsuhiko Ishiguro", "Shin-ichi Maeda", "Masanori Koyama"], "authorids": ["k.ishiguro.jp@ieee.org", "ichi@preferred.jp", "masomatics@preferred.jp"], "keywords": ["Graph Neural Networks", "molecular graph analysis", "supernode", "auxiliary module"], "TL;DR": "Proposing an auxiliary  module with its own I/O that can be attached to a generic GNN of message passing type in order to improve its representation power/ generalization performance on small-graph datasets.", "abstract": "Graph Neural Network (GNN) is a popular architecture for the analysis of chemical molecules, and it has numerous applications in material and medicinal science.\nCurrent lines of GNNs developed for molecular analysis, however, do not fit well on the training set, and their performance does not scale well with the complexity of the network. \nIn this paper, we propose an auxiliary module to be attached to a GNN that can boost the representation power of the model without hindering the original GNN architecture. \nOur auxiliary module can improve the representation power and the generalization ability of a wide variety of GNNs, including those that are used commonly in biochemical applications. ", "pdf": "/pdf/a5f1e3d7b24fa071758e07f660b65951f05ba777.pdf", "paperhash": "ishiguro|graph_warp_module_an_auxiliary_module_for_boosting_the_power_of_graph_neural_networks_in_molecular_graph_analysis", "original_pdf": "/attachment/d48e31022af8d37e19042f04fc9333de199af802.pdf", "_bibtex": "@misc{\nishiguro2020graph,\ntitle={Graph Warp Module: an Auxiliary Module for Boosting the Power of Graph Neural Networks in Molecular Graph Analysis},\nauthor={Katsuhiko Ishiguro and Shin-ichi Maeda and Masanori Koyama},\nyear={2020},\nurl={https://openreview.net/forum?id=S1l66nNFvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1l66nNFvB", "replyto": "S1l66nNFvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper243/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper243/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575697957755, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper243/Reviewers"], "noninvitees": [], "tcdate": 1570237754947, "tmdate": 1575697957776, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper243/-/Official_Review"}}}], "count": 9}