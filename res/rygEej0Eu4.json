{"notes": [{"id": "rygEej0Eu4", "original": "r1gp9enEu4", "number": 55, "cdate": 1553423083642, "ddate": null, "tcdate": 1553423083642, "tmdate": 1562082113516, "tddate": null, "forum": "rygEej0Eu4", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "content": {"title": "Data Interpolating Prediction: Alternative Interpretation of Mixup", "authors": ["Takuya Shimada", "Shoichiro Yamaguchi", "Kohei Hayashi", "Sosuke Kobayashi"], "authorids": ["shima@ms.k.u-tokyo.ac.jp", "guguchi@preferred.jp", "hayashi.kohei@gmail.com", "sosk@preferred.jp"], "keywords": ["data augmentation", "generalization", "image classification"], "abstract": "Data augmentation by mixing samples, such as Mixup, has widely been used typically for classification tasks. However, this strategy is not always effective due to the gap between augmented samples during training and clean samples. This gap may prevent a classifier from learning the optimal decision boundary and increases the generalization error. To overcome this problem, we propose an alternative framework called Data Interpolating Prediction (DIP). Unlike common data augmentations, we encapsulate the sample-mixing process in the hypothesis class of a classifier so that train and test samples are treated equally. We derive the generalization bound and show that DIP reduces the original Rademacher complexity. Also, we empirically demonstrate that DIP can outperform existing Mixup.", "pdf": "/pdf/de7ff77df6d22582ca05a28c95b20179a233bd94.pdf", "paperhash": "shimada|data_interpolating_prediction_alternative_interpretation_of_mixup"}, "signatures": ["ICLR.cc/2019/Workshop/LLD"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD"], "details": {"replyCount": 2, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "cdate": 1548689671889, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "content": {"authors": {"values-regex": ".*"}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1548689671889, "tmdate": 1557933709646, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/LLD"], "details": {"writable": true}}}, "tauthor": "OpenReview.net"}, {"id": "BygktHtDY4", "original": null, "number": 1, "cdate": 1554646390830, "ddate": null, "tcdate": 1554646390830, "tmdate": 1555512018460, "tddate": null, "forum": "rygEej0Eu4", "replyto": "rygEej0Eu4", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper55/Official_Review", "content": {"title": "A somewhat original approach, but insufficient experiments in order to draw conclusions regarding its value", "review": "The authors propose an alternative framework for applying the mixup method of [2]. To the best of my knowledge, this framework has not been propose before, hence the manuscript does offer some original ideas. That being said, the proposed framework is very related to established techniques, in particular to the practice of taking the average of the output of the classifier over a set of augmented version of a testing sample. The main practical contribution of the manuscript is the application of this technique: (a) for mixup and (b) at training time, too. On the downside, the reported results offer inconclusive evidence that this monte carlo sampling in training time can be a beneficial approach for classification. Generally, my main criticism is that I think that the experimental setup should have covered additional configuration combinations, so that the proposed framework is evaluated against suitable alternatives (details follow in the \"Cons\" section). In terms of clarity, I feel that the main ideas could have been presented briefly in a simpler language, before delving into the mathematical analysis. \n\nProns:\n1. In addition to their experiments in CIFAR10 and CIFAR100, the authors briefly discuss the results on a simple synthetic dataset, which IMO illustrates nicely some potential dangers with mixup and it provides a rather clear motivation for this line of work.\n\n2. Derivation of some theoretical results, which are interesting in their own right.\n\nCons:\nIMO, there are quite a few methodological shortcomings:\n\n1. The authors offer no justification of their choice of S=500 at testing time. I would expect a posteriori plot of how the testing accuracy changes for different values of S_testing given at least one training setting. As a related note when it comes to clarity, I assume that this sampling involves 500 cases from the training set, even at testing time. This is not made clear in the manuscript.\n\n2. The main technique used by the paper (averaging the output of the classifier over a set of samples at both training and testing time) is rather orthogonal to mixup, to my understanding at least. In fact, the practice of generating augmented version of a given testing sample and taking the average of the output of the classifier over all the augmented versions is a well-known practice that has been shown to improve the testing performance in many practical scenarios (as an example, [1]). Therefore, I think that this method should have been applied to and compared with other augmentation methods, in addition to mixup. For example, what happens if the hypothesis classes is enriched with augmented versions of the same sample using more standard transformations (rotations, translations, etc) ?\n\n3. It is my understanding that S=1 corresponds more or less to standard mixup, at training time at least. Thus, the difference of the configuration \"Mixup (alpha=1)\" and \"DIP(alpha=1, S=1, label-mixing)\" is only at testing time, with the sampling over the 500 mixup pairs that takes place in the second case. Since the experiments show that S=1 is the most performant configuration in most experiments, the value of the proposed monte carlo sampling at training time is not fully supported by the reported results, IMO. This should be discussed in the \"Conclusion\" section of the manuscript.\n\n4. Dropout should have been used in the no-mixup baseline, as in the mixup paper [2].\n\n5. I have the feeling that this work is only partially relevant to the scope of the workshop. \n\nNitpicking: I would like to mention a specific orthographic error that should be corrected: \"monte carlo\" is spelled \"monte calro\" for most of the manuscript.\n\n[1] Jamaludin, Amir, Timor Kadir, and Andrew Zisserman. \"SpineNet: Automated classification and evidence visualization in spinal MRIs.\" Medical image analysis 41 (2017): 63-73.\n\n[2] Zhang, Hongyi, et al. \"mixup: Beyond empirical risk minimization.\" arXiv preprint arXiv:1710.09412 (2017).", "rating": "3: Marginally above acceptance threshold", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper55/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper55/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Data Interpolating Prediction: Alternative Interpretation of Mixup", "authors": ["Takuya Shimada", "Shoichiro Yamaguchi", "Kohei Hayashi", "Sosuke Kobayashi"], "authorids": ["shima@ms.k.u-tokyo.ac.jp", "guguchi@preferred.jp", "hayashi.kohei@gmail.com", "sosk@preferred.jp"], "keywords": ["data augmentation", "generalization", "image classification"], "abstract": "Data augmentation by mixing samples, such as Mixup, has widely been used typically for classification tasks. However, this strategy is not always effective due to the gap between augmented samples during training and clean samples. This gap may prevent a classifier from learning the optimal decision boundary and increases the generalization error. To overcome this problem, we propose an alternative framework called Data Interpolating Prediction (DIP). Unlike common data augmentations, we encapsulate the sample-mixing process in the hypothesis class of a classifier so that train and test samples are treated equally. We derive the generalization bound and show that DIP reduces the original Rademacher complexity. Also, we empirically demonstrate that DIP can outperform existing Mixup.", "pdf": "/pdf/de7ff77df6d22582ca05a28c95b20179a233bd94.pdf", "paperhash": "shimada|data_interpolating_prediction_alternative_interpretation_of_mixup"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper55/Official_Review", "cdate": 1553713412405, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "rygEej0Eu4", "replyto": "rygEej0Eu4", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper55/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper55/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713412405, "tmdate": 1555511822567, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper55/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "ryxCCRhz9V", "original": null, "number": 1, "cdate": 1555381973935, "ddate": null, "tcdate": 1555381973935, "tmdate": 1555510975958, "tddate": null, "forum": "rygEej0Eu4", "replyto": "rygEej0Eu4", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper55/Decision", "content": {"title": "Acceptance Decision", "decision": "Accept"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Data Interpolating Prediction: Alternative Interpretation of Mixup", "authors": ["Takuya Shimada", "Shoichiro Yamaguchi", "Kohei Hayashi", "Sosuke Kobayashi"], "authorids": ["shima@ms.k.u-tokyo.ac.jp", "guguchi@preferred.jp", "hayashi.kohei@gmail.com", "sosk@preferred.jp"], "keywords": ["data augmentation", "generalization", "image classification"], "abstract": "Data augmentation by mixing samples, such as Mixup, has widely been used typically for classification tasks. However, this strategy is not always effective due to the gap between augmented samples during training and clean samples. This gap may prevent a classifier from learning the optimal decision boundary and increases the generalization error. To overcome this problem, we propose an alternative framework called Data Interpolating Prediction (DIP). Unlike common data augmentations, we encapsulate the sample-mixing process in the hypothesis class of a classifier so that train and test samples are treated equally. We derive the generalization bound and show that DIP reduces the original Rademacher complexity. Also, we empirically demonstrate that DIP can outperform existing Mixup.", "pdf": "/pdf/de7ff77df6d22582ca05a28c95b20179a233bd94.pdf", "paperhash": "shimada|data_interpolating_prediction_alternative_interpretation_of_mixup"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper55/Decision", "cdate": 1554736073884, "reply": {"forum": "rygEej0Eu4", "replyto": "rygEej0Eu4", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1554736073884, "tmdate": 1555510964977, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}], "count": 3}