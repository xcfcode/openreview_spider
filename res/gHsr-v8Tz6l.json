{"notes": [{"id": "gHsr-v8Tz6l", "original": "3S1F7GfxRaG", "number": 1550, "cdate": 1601308171981, "ddate": null, "tcdate": 1601308171981, "tmdate": 1614985692914, "tddate": null, "forum": "gHsr-v8Tz6l", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Variational Invariant Learning for Bayesian Domain Generalization", "authorids": ["~Zehao_Xiao1", "~Jiayi_Shen3", "~Xiantong_Zhen1", "~Ling_Shao1", "~Cees_G._M._Snoek1"], "authors": ["Zehao Xiao", "Jiayi Shen", "Xiantong Zhen", "Ling Shao", "Cees G. M. Snoek"], "keywords": ["domain generalization", "variational invariant learning", "Bayesian inference"], "abstract": "Domain generalization addresses the out-of-distribution problem, which is challenging due to the domain shift and the uncertainty caused by the inaccessibility to data from the target domains. In this paper, we propose variational invariant learning, a probabilistic inference framework that jointly models domain invariance and uncertainty. We introduce variational Bayesian approximation into both the feature representation and classifier layers to facilitate invariant learning for better generalization across domains. In the probabilistic modeling framework, we introduce a domain-invariant principle to explore invariance across domains in a unified way. We incorporate the principle into the variational Bayesian layers in neural networks, achieving domain-invariant representations and classifier. We empirically demonstrate the effectiveness of our proposal on four widely used cross-domain visual recognition benchmarks. Ablation studies demonstrate the benefits of our proposal and on all benchmarks our variational invariant learning consistently delivers state-of-the-art performance. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiao|variational_invariant_learning_for_bayesian_domain_generalization", "one-sentence_summary": "We propose variational invariant learning, a probabilistic inference framework that jointly models domain invariance and uncertainty for Bayesian domain generalization", "pdf": "/pdf/4964403e8dbac758c6a7c684ffdf59b5fef70e7a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=diKKbSEhGe", "_bibtex": "@misc{\nxiao2021variational,\ntitle={Variational Invariant Learning for Bayesian Domain Generalization},\nauthor={Zehao Xiao and Jiayi Shen and Xiantong Zhen and Ling Shao and Cees G. M. Snoek},\nyear={2021},\nurl={https://openreview.net/forum?id=gHsr-v8Tz6l}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "wnRnOAhEP9A", "original": null, "number": 1, "cdate": 1610040455505, "ddate": null, "tcdate": 1610040455505, "tmdate": 1610474058113, "tddate": null, "forum": "gHsr-v8Tz6l", "replyto": "gHsr-v8Tz6l", "invitation": "ICLR.cc/2021/Conference/Paper1550/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "Although the proposed method shows sota results, it is a simple combination of two existing methods, a bit of Bayesian + domain generalization.  It seems that the total improvement by the proposed method is just the sum of improvements by Bayesian and by domain generalization.  No synergy between Bayesian and domain generalization is observed.\n\nI personally doubt that the Bayesian treatment of the domain generalization loss is not essential.\nThe derivation in Sec. 2 is unnecessarily complicated.  In derivation from eq(3) to eq(5), the authors first \"extend\" (3) to (4), which is not appropriate ((4) can hold even if $p(y_{\\zeta}|x_{\\zeta})$ is highly diverse. ).  After that the authors apply Jensen to come back to an appropriate form (5), which is a weighted sum of distances (which is an appropriate criterion).\nIf they start from Eq.(5), the proposed objective is simply the sum of the standard ELBO (2) and a natural domain invariance loss (5).  For non-Bayesian treatment of the domain invariant loss, you could simply replace the KL by Lp norm between $y_s$ and $y_\\zeta$.  I expected that by answering to the question by Reviewer 1 the authors would prove synergy between Bayesian and domain generalization.  But the authors just excused that\n'' The feature distributions  are unknown without Bayesian formalism, leading to an intractable $L_I$. Therefore, we do not conduct the experiment with only the domain-invariant loss on both the classifier and the feature extractor.''\nI don't really understand what the authors mean, but the authors should have explained why you cannot replace the KL with non-Bayesian Lp loss.\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variational Invariant Learning for Bayesian Domain Generalization", "authorids": ["~Zehao_Xiao1", "~Jiayi_Shen3", "~Xiantong_Zhen1", "~Ling_Shao1", "~Cees_G._M._Snoek1"], "authors": ["Zehao Xiao", "Jiayi Shen", "Xiantong Zhen", "Ling Shao", "Cees G. M. Snoek"], "keywords": ["domain generalization", "variational invariant learning", "Bayesian inference"], "abstract": "Domain generalization addresses the out-of-distribution problem, which is challenging due to the domain shift and the uncertainty caused by the inaccessibility to data from the target domains. In this paper, we propose variational invariant learning, a probabilistic inference framework that jointly models domain invariance and uncertainty. We introduce variational Bayesian approximation into both the feature representation and classifier layers to facilitate invariant learning for better generalization across domains. In the probabilistic modeling framework, we introduce a domain-invariant principle to explore invariance across domains in a unified way. We incorporate the principle into the variational Bayesian layers in neural networks, achieving domain-invariant representations and classifier. We empirically demonstrate the effectiveness of our proposal on four widely used cross-domain visual recognition benchmarks. Ablation studies demonstrate the benefits of our proposal and on all benchmarks our variational invariant learning consistently delivers state-of-the-art performance. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiao|variational_invariant_learning_for_bayesian_domain_generalization", "one-sentence_summary": "We propose variational invariant learning, a probabilistic inference framework that jointly models domain invariance and uncertainty for Bayesian domain generalization", "pdf": "/pdf/4964403e8dbac758c6a7c684ffdf59b5fef70e7a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=diKKbSEhGe", "_bibtex": "@misc{\nxiao2021variational,\ntitle={Variational Invariant Learning for Bayesian Domain Generalization},\nauthor={Zehao Xiao and Jiayi Shen and Xiantong Zhen and Ling Shao and Cees G. M. Snoek},\nyear={2021},\nurl={https://openreview.net/forum?id=gHsr-v8Tz6l}\n}"}, "tags": [], "invitation": {"reply": {"forum": "gHsr-v8Tz6l", "replyto": "gHsr-v8Tz6l", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040455492, "tmdate": 1610474058098, "id": "ICLR.cc/2021/Conference/Paper1550/-/Decision"}}}, {"id": "nkJGRo9IV4L", "original": null, "number": 5, "cdate": 1605971859401, "ddate": null, "tcdate": 1605971859401, "tmdate": 1605971859401, "tddate": null, "forum": "gHsr-v8Tz6l", "replyto": "LCahpY941o", "invitation": "ICLR.cc/2021/Conference/Paper1550/-/Official_Comment", "content": {"title": "Response to AnonReviewer 3", "comment": "We thank AnonReviewer3 for acknowledging our methodology, definitions and ablation study are clear and well presented.\n\nWe agree our claim on being the first Bayesian approach to domain generalization is not precise enough. We have softened in Section 1: ''We adopt Bayesian neural networks to domain generalization'', in Section 3 '' the approach based on Bayesian neural networks has not yet been explored in domain generalization'' and in Section 5: ''We introduce Bayesian neural networks into the model''. We have also cited Louizos et al. (2016) in Section 3. Thank you."}, "signatures": ["ICLR.cc/2021/Conference/Paper1550/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1550/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variational Invariant Learning for Bayesian Domain Generalization", "authorids": ["~Zehao_Xiao1", "~Jiayi_Shen3", "~Xiantong_Zhen1", "~Ling_Shao1", "~Cees_G._M._Snoek1"], "authors": ["Zehao Xiao", "Jiayi Shen", "Xiantong Zhen", "Ling Shao", "Cees G. M. Snoek"], "keywords": ["domain generalization", "variational invariant learning", "Bayesian inference"], "abstract": "Domain generalization addresses the out-of-distribution problem, which is challenging due to the domain shift and the uncertainty caused by the inaccessibility to data from the target domains. In this paper, we propose variational invariant learning, a probabilistic inference framework that jointly models domain invariance and uncertainty. We introduce variational Bayesian approximation into both the feature representation and classifier layers to facilitate invariant learning for better generalization across domains. In the probabilistic modeling framework, we introduce a domain-invariant principle to explore invariance across domains in a unified way. We incorporate the principle into the variational Bayesian layers in neural networks, achieving domain-invariant representations and classifier. We empirically demonstrate the effectiveness of our proposal on four widely used cross-domain visual recognition benchmarks. Ablation studies demonstrate the benefits of our proposal and on all benchmarks our variational invariant learning consistently delivers state-of-the-art performance. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiao|variational_invariant_learning_for_bayesian_domain_generalization", "one-sentence_summary": "We propose variational invariant learning, a probabilistic inference framework that jointly models domain invariance and uncertainty for Bayesian domain generalization", "pdf": "/pdf/4964403e8dbac758c6a7c684ffdf59b5fef70e7a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=diKKbSEhGe", "_bibtex": "@misc{\nxiao2021variational,\ntitle={Variational Invariant Learning for Bayesian Domain Generalization},\nauthor={Zehao Xiao and Jiayi Shen and Xiantong Zhen and Ling Shao and Cees G. M. Snoek},\nyear={2021},\nurl={https://openreview.net/forum?id=gHsr-v8Tz6l}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "gHsr-v8Tz6l", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1550/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1550/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1550/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1550/Authors|ICLR.cc/2021/Conference/Paper1550/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1550/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858440, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1550/-/Official_Comment"}}}, {"id": "2jSoMBfnQ8S", "original": null, "number": 4, "cdate": 1605971685153, "ddate": null, "tcdate": 1605971685153, "tmdate": 1605971685153, "tddate": null, "forum": "gHsr-v8Tz6l", "replyto": "bCeFSKfymgw", "invitation": "ICLR.cc/2021/Conference/Paper1550/-/Official_Comment", "content": {"title": "Response to AnonReviewer 2", "comment": "We thank AnonReviewer2 for acknowledging the importance of uncertainty in domain generalization as well as the effectiveness of our proposal.\n\n1. We clarify our novelty w.r.t. Motiian et al. (2017) in Related Work: ''Similar to our proposal, CCSA by  Motiian et al. (2017) also aligns representations across domains in the same class.\nSpecifically, CCSA utilizes an L2 distance between deterministic features while we exploit Bayesian neural networks to learn domain-invariant representations by minimizing the distance between domain distributions. \nTheoretically, minimizing the distance between distributions incorporates larger inter-class variance than minimizing distance of deterministic features.\nMoreover, we apply our variational invariant learning to both the feature extractor and the classifier, while CCSA only considers an alignment loss on the feature representations.'' \nOn Office-Home our proposal results in better domain generalization than CCSA for all domains (Table 4). Thank you.\n\n2. We provide the suggested hyperparameter ablation in Appendix C. The optimal values of $\\lambda_{\\boldsymbol{\\phi}}$, $\\lambda_{\\boldsymbol{\\psi}}$ and $\\pi$ are 1, 100 and 0.5 respectively. The performance is not very sensitive to changes of the values of $\\lambda_{\\boldsymbol{\\phi}}$ and $\\lambda_{\\boldsymbol{\\psi}}$ when they are in a reasonable range. $\\pi {=} 0.5$ is also commonly observed in other works. More analysis is provided in Appendix C.\n\nMinor comments\n \n1. The domain-transform function $g_{\\zeta}(\\cdot)$ is defined as a mapping function that is able to project any sample $\\mathbf{x}_{{s}}$ from domain $\\mathcal{D}_s$ to other different domains $\\mathcal{D}_\\zeta$. Usually the exact form of the function is not necessarily known.\nWe have clarified this in Section 2.2.\n\n2. We tried to enhance dissimilarity of different classes across domains by maximizing the Wasserstein distance between features, but we did not notice any notable performance improvement. We leave further exploration to future work. \n\n3. We clarify in the the updated manuscript that $\\mathbf{z}_s$ and $\\mathbf{z}_t$ in Eq. (10) correspond to samples of the feature distributions not the means.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1550/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1550/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variational Invariant Learning for Bayesian Domain Generalization", "authorids": ["~Zehao_Xiao1", "~Jiayi_Shen3", "~Xiantong_Zhen1", "~Ling_Shao1", "~Cees_G._M._Snoek1"], "authors": ["Zehao Xiao", "Jiayi Shen", "Xiantong Zhen", "Ling Shao", "Cees G. M. Snoek"], "keywords": ["domain generalization", "variational invariant learning", "Bayesian inference"], "abstract": "Domain generalization addresses the out-of-distribution problem, which is challenging due to the domain shift and the uncertainty caused by the inaccessibility to data from the target domains. In this paper, we propose variational invariant learning, a probabilistic inference framework that jointly models domain invariance and uncertainty. We introduce variational Bayesian approximation into both the feature representation and classifier layers to facilitate invariant learning for better generalization across domains. In the probabilistic modeling framework, we introduce a domain-invariant principle to explore invariance across domains in a unified way. We incorporate the principle into the variational Bayesian layers in neural networks, achieving domain-invariant representations and classifier. We empirically demonstrate the effectiveness of our proposal on four widely used cross-domain visual recognition benchmarks. Ablation studies demonstrate the benefits of our proposal and on all benchmarks our variational invariant learning consistently delivers state-of-the-art performance. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiao|variational_invariant_learning_for_bayesian_domain_generalization", "one-sentence_summary": "We propose variational invariant learning, a probabilistic inference framework that jointly models domain invariance and uncertainty for Bayesian domain generalization", "pdf": "/pdf/4964403e8dbac758c6a7c684ffdf59b5fef70e7a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=diKKbSEhGe", "_bibtex": "@misc{\nxiao2021variational,\ntitle={Variational Invariant Learning for Bayesian Domain Generalization},\nauthor={Zehao Xiao and Jiayi Shen and Xiantong Zhen and Ling Shao and Cees G. M. Snoek},\nyear={2021},\nurl={https://openreview.net/forum?id=gHsr-v8Tz6l}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "gHsr-v8Tz6l", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1550/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1550/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1550/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1550/Authors|ICLR.cc/2021/Conference/Paper1550/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1550/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858440, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1550/-/Official_Comment"}}}, {"id": "Mkvk3Cx3E8-", "original": null, "number": 3, "cdate": 1605970298093, "ddate": null, "tcdate": 1605970298093, "tmdate": 1605970298093, "tddate": null, "forum": "gHsr-v8Tz6l", "replyto": "Nf-0ZWZlFN", "invitation": "ICLR.cc/2021/Conference/Paper1550/-/Official_Comment", "content": {"title": "Response to AnonReviewer 4", "comment": "We thank AnonReviewer4 for acknowledging our idea is natural and computationally practical.\n\n1. We clarify in Section 4.1: ''We choose $\\lambda_{\\boldsymbol{\\phi}}$ and $\\lambda_{\\boldsymbol{\\psi}}$ based on the performance on the validation set and their influence is summarized in the new Fig 3 in Appendix C.''\nIndeed, this could be interpreted from a MLE perspective.\n\n2. We clarify in Section 4.3: ''The baselines on PACS (Table 3), Office-Home (Table 4), and rotated MNIST and Fashion-MNIST (Table 5) are all based on the same vanilla deep convolutional ResNet-18 network, without any Bayesian treatment, the same as row (a) in Table 1.''"}, "signatures": ["ICLR.cc/2021/Conference/Paper1550/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1550/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variational Invariant Learning for Bayesian Domain Generalization", "authorids": ["~Zehao_Xiao1", "~Jiayi_Shen3", "~Xiantong_Zhen1", "~Ling_Shao1", "~Cees_G._M._Snoek1"], "authors": ["Zehao Xiao", "Jiayi Shen", "Xiantong Zhen", "Ling Shao", "Cees G. M. Snoek"], "keywords": ["domain generalization", "variational invariant learning", "Bayesian inference"], "abstract": "Domain generalization addresses the out-of-distribution problem, which is challenging due to the domain shift and the uncertainty caused by the inaccessibility to data from the target domains. In this paper, we propose variational invariant learning, a probabilistic inference framework that jointly models domain invariance and uncertainty. We introduce variational Bayesian approximation into both the feature representation and classifier layers to facilitate invariant learning for better generalization across domains. In the probabilistic modeling framework, we introduce a domain-invariant principle to explore invariance across domains in a unified way. We incorporate the principle into the variational Bayesian layers in neural networks, achieving domain-invariant representations and classifier. We empirically demonstrate the effectiveness of our proposal on four widely used cross-domain visual recognition benchmarks. Ablation studies demonstrate the benefits of our proposal and on all benchmarks our variational invariant learning consistently delivers state-of-the-art performance. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiao|variational_invariant_learning_for_bayesian_domain_generalization", "one-sentence_summary": "We propose variational invariant learning, a probabilistic inference framework that jointly models domain invariance and uncertainty for Bayesian domain generalization", "pdf": "/pdf/4964403e8dbac758c6a7c684ffdf59b5fef70e7a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=diKKbSEhGe", "_bibtex": "@misc{\nxiao2021variational,\ntitle={Variational Invariant Learning for Bayesian Domain Generalization},\nauthor={Zehao Xiao and Jiayi Shen and Xiantong Zhen and Ling Shao and Cees G. M. Snoek},\nyear={2021},\nurl={https://openreview.net/forum?id=gHsr-v8Tz6l}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "gHsr-v8Tz6l", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1550/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1550/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1550/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1550/Authors|ICLR.cc/2021/Conference/Paper1550/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1550/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858440, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1550/-/Official_Comment"}}}, {"id": "ii4OdxSPr4", "original": null, "number": 2, "cdate": 1605969779379, "ddate": null, "tcdate": 1605969779379, "tmdate": 1605969779379, "tddate": null, "forum": "gHsr-v8Tz6l", "replyto": "2UG1dHNNbX5", "invitation": "ICLR.cc/2021/Conference/Paper1550/-/Official_Comment", "content": {"title": "Response to AnonReviewer 1", "comment": "We thank AnonReviewer1 for acknowledging our method is novel and promising. \n\n1. We have added the requested ablation study about the number of feature extraction layers on the PACS dataset in Section 4.2, Table 2. \nWhen introducing another Bayesian learning layer $\\boldsymbol{\\phi} '$ without the domain-invariant property into the model, as shown in the second row of Table 2, the average performance improves slightly. \nIf we introduce both the Bayesian learning and domain-invariant learning into $\\boldsymbol{\\phi} '$, as shown in the third row, the overall performance declines a bit. \nOne reason might be the information loss in feature representations caused by the excessive use of domain-invariant learning.\nFor the efficiency of the model, due to the Bayesian inference and Monte-Carlo sampling, more variational-invariant layers leads to higher memory usage and more computations, which is also one reason for us to apply the variational invariant learning only to the last feature extraction layer and the classifier.\n\n2. We clarify in Section 4.2: '' The feature distributions $p(\\mathbf{z}|\\mathbf{x})$ are unknown without Bayesian formalism, leading to an intractable $\\mathcal{L}_{I}(\\boldsymbol{\\phi})$. Therefore, we do not conduct the experiment with only the domain-invariant loss on both the classifier and the feature extractor.''\n\n3. We add to Section 4.3: '' L2A-OT and DSON outperform the proposed model on some domains of PACS and Office-Home. L2A-OT learns a generator to synthesize data from pseudo-novel domains to augment the source domains. \nThe pseudo-novel domains often have similar characteristics with the source data. Thus, when the target data also have similar characteristics with the source domains this pays off as the pseudo domains are more likely to cover the target domain, such as ''Product'' and ''Real World'' in Office-Home and ''Photo'' in PACS. When the test domain is different from all of the training domains the performance suffers,  e.g., ''Clipart'' in Office-Home and ''Sketch'' in PACS. Our method generates domain-invariant representations and classifiers, resulting in competitive results across all domains and overall. \nDSON mixtures batch and instance normalization for domain generalization. This tactic is effective on PACS, but less competitive on Office-Home. We attribute this to the larger number of categories on Office-Home, where instance normalization is known to make features less discriminative with respect to object categories (Seo et al., 2019).\nOur domain-invariant network makes feature distributions and predictive distributions similar across domains, resulting in good performance on both PACS and Office-Home.''\n\n4. Selecting training samples by predictive uncertainty is interesting and we do not set a threshold in our method to do so. Our Bayesian framework captures and models uncertainty to alleviate the overconfidence on out-of-distribution test data."}, "signatures": ["ICLR.cc/2021/Conference/Paper1550/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1550/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variational Invariant Learning for Bayesian Domain Generalization", "authorids": ["~Zehao_Xiao1", "~Jiayi_Shen3", "~Xiantong_Zhen1", "~Ling_Shao1", "~Cees_G._M._Snoek1"], "authors": ["Zehao Xiao", "Jiayi Shen", "Xiantong Zhen", "Ling Shao", "Cees G. M. Snoek"], "keywords": ["domain generalization", "variational invariant learning", "Bayesian inference"], "abstract": "Domain generalization addresses the out-of-distribution problem, which is challenging due to the domain shift and the uncertainty caused by the inaccessibility to data from the target domains. In this paper, we propose variational invariant learning, a probabilistic inference framework that jointly models domain invariance and uncertainty. We introduce variational Bayesian approximation into both the feature representation and classifier layers to facilitate invariant learning for better generalization across domains. In the probabilistic modeling framework, we introduce a domain-invariant principle to explore invariance across domains in a unified way. We incorporate the principle into the variational Bayesian layers in neural networks, achieving domain-invariant representations and classifier. We empirically demonstrate the effectiveness of our proposal on four widely used cross-domain visual recognition benchmarks. Ablation studies demonstrate the benefits of our proposal and on all benchmarks our variational invariant learning consistently delivers state-of-the-art performance. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiao|variational_invariant_learning_for_bayesian_domain_generalization", "one-sentence_summary": "We propose variational invariant learning, a probabilistic inference framework that jointly models domain invariance and uncertainty for Bayesian domain generalization", "pdf": "/pdf/4964403e8dbac758c6a7c684ffdf59b5fef70e7a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=diKKbSEhGe", "_bibtex": "@misc{\nxiao2021variational,\ntitle={Variational Invariant Learning for Bayesian Domain Generalization},\nauthor={Zehao Xiao and Jiayi Shen and Xiantong Zhen and Ling Shao and Cees G. M. Snoek},\nyear={2021},\nurl={https://openreview.net/forum?id=gHsr-v8Tz6l}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "gHsr-v8Tz6l", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1550/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1550/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1550/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1550/Authors|ICLR.cc/2021/Conference/Paper1550/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1550/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858440, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1550/-/Official_Comment"}}}, {"id": "LCahpY941o", "original": null, "number": 1, "cdate": 1603285030670, "ddate": null, "tcdate": 1603285030670, "tmdate": 1605024416864, "tddate": null, "forum": "gHsr-v8Tz6l", "replyto": "gHsr-v8Tz6l", "invitation": "ICLR.cc/2021/Conference/Paper1550/-/Official_Review", "content": {"title": "Recommendation to Accept", "review": "Summary:\nThe paper proposes variational invariant learning (VIL) as a framework for probabilistic inference that jointly models domain invariance and uncertainty. The approach exploits variational Bayesian approximation in both feature encoding and classifier layers in order to facilitate domain generalization and invariance. The paper evaluates VIL on benchmarks for cross-domain visual object classification.\n\nPositives:\nThe paper presents their methodology and definitions very clearly. Comparisons to state-of-the-art methods are well presented. The ablation study is very clear to provide a thorough understanding of the approach. Overall I like this paper.\n\nConcerns:\nThe authors state: \"the Bayesian approach has not yet been explored in domain generalization\" or \"this is the first work to adopt variational Bayes to domain generalization\". I may not strongly agree on these statements in their current form. There has been some previous work on extending variational autoencoding frameworks for domain generalization from several perspectives (e.g, adversarial inference). This type of variational inference based models ideally exploit a Bayesian approach for domain generalization as well. I would expect the authors to rephrase their claims in a more specific manner to make this distinction. Regarding this concern, one example to consider for instance is \"The Variational Fair Autoencoder\" (Louizos et al., ICLR 2016) approach for invariant representation learning with variational autoencoders for domain generalization.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1550/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1550/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variational Invariant Learning for Bayesian Domain Generalization", "authorids": ["~Zehao_Xiao1", "~Jiayi_Shen3", "~Xiantong_Zhen1", "~Ling_Shao1", "~Cees_G._M._Snoek1"], "authors": ["Zehao Xiao", "Jiayi Shen", "Xiantong Zhen", "Ling Shao", "Cees G. M. Snoek"], "keywords": ["domain generalization", "variational invariant learning", "Bayesian inference"], "abstract": "Domain generalization addresses the out-of-distribution problem, which is challenging due to the domain shift and the uncertainty caused by the inaccessibility to data from the target domains. In this paper, we propose variational invariant learning, a probabilistic inference framework that jointly models domain invariance and uncertainty. We introduce variational Bayesian approximation into both the feature representation and classifier layers to facilitate invariant learning for better generalization across domains. In the probabilistic modeling framework, we introduce a domain-invariant principle to explore invariance across domains in a unified way. We incorporate the principle into the variational Bayesian layers in neural networks, achieving domain-invariant representations and classifier. We empirically demonstrate the effectiveness of our proposal on four widely used cross-domain visual recognition benchmarks. Ablation studies demonstrate the benefits of our proposal and on all benchmarks our variational invariant learning consistently delivers state-of-the-art performance. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiao|variational_invariant_learning_for_bayesian_domain_generalization", "one-sentence_summary": "We propose variational invariant learning, a probabilistic inference framework that jointly models domain invariance and uncertainty for Bayesian domain generalization", "pdf": "/pdf/4964403e8dbac758c6a7c684ffdf59b5fef70e7a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=diKKbSEhGe", "_bibtex": "@misc{\nxiao2021variational,\ntitle={Variational Invariant Learning for Bayesian Domain Generalization},\nauthor={Zehao Xiao and Jiayi Shen and Xiantong Zhen and Ling Shao and Cees G. M. Snoek},\nyear={2021},\nurl={https://openreview.net/forum?id=gHsr-v8Tz6l}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "gHsr-v8Tz6l", "replyto": "gHsr-v8Tz6l", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1550/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538116150, "tmdate": 1606915788221, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1550/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1550/-/Official_Review"}}}, {"id": "bCeFSKfymgw", "original": null, "number": 2, "cdate": 1603408107270, "ddate": null, "tcdate": 1603408107270, "tmdate": 1605024416805, "tddate": null, "forum": "gHsr-v8Tz6l", "replyto": "gHsr-v8Tz6l", "invitation": "ICLR.cc/2021/Conference/Paper1550/-/Official_Review", "content": {"title": "Although I like the research direction of this paper, I have some concerns about the technical novelty of this method ", "review": "Summary:\n- The paper proposes a Bayesian inference framework for domain generalization (DG) that deals with both the uncertainty and domain shit. \nThe proposed method treats the uncertainty effectively by applying the variational Bayesian inference to the last two layers of the model (feature representations and classifier layers).\nTo make the model domain-invariant, the proposed method introduces a domain-invariant principle that makes distributions of the model's outputs (representations) in the same class similar across domains. \nThe paper experimentally demonstrated the effectiveness of the method with four visual datasets.\n\nPros:\n- The problem tackled by this paper is very important because accurate prediction is generally difficult in DG due to the lack of target training data and therefore considering the uncertainty is especially important.\n- Experimental results with four widely used datasets show the effectiveness of the method.\n\nCons:\n-  The technical novelty of this method seems a bit low. This is because this method is a relatively simple combination of Bayesian modeling of neural networks and some existing techniques for DG. \nIn previous works for DG, techniques to bring distributions of model's outputs in the same class closer together across domains have been proposed.\nFor example, CCSA [1] learns the domain-invariant representations by matching the representations in the same class across domains.\n\n[1] Motiian et al., Unified deep supervised domain adaptation and generalization, ICCV2017\n\n- More analysis of hyperparameters such as $\\lambda_{\\phi}$, $\\lambda_{\\psi}$, and $\\pi$ would improve the quality of this paper.\n\nReasons for Scores:\n- Although I like the research direction of this paper as described in Pros, I have some concerns about the technical novelty of this method as described in Cons.\n\nMinor comments: \n- definition 2.1 (domain invariance) is a little confusing for me although I can see what the method wants to do by seeing Eq. (7). For example, what is the formal definition of domain-transform function?\n- Although the loss function for domain-invariant learning (7) seems to bring the distributions of the same class across domains closer together, can the method incorporate a\nmechanism to enhance dissimilarity of the different classes across domains? I think that it might improve performance.\n- There are some ambiguous statements. In Eq. (10), are $z_{s}$ and $z_{t}$ the means of distribution of $q(z|x,\\phi)$?\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1550/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1550/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variational Invariant Learning for Bayesian Domain Generalization", "authorids": ["~Zehao_Xiao1", "~Jiayi_Shen3", "~Xiantong_Zhen1", "~Ling_Shao1", "~Cees_G._M._Snoek1"], "authors": ["Zehao Xiao", "Jiayi Shen", "Xiantong Zhen", "Ling Shao", "Cees G. M. Snoek"], "keywords": ["domain generalization", "variational invariant learning", "Bayesian inference"], "abstract": "Domain generalization addresses the out-of-distribution problem, which is challenging due to the domain shift and the uncertainty caused by the inaccessibility to data from the target domains. In this paper, we propose variational invariant learning, a probabilistic inference framework that jointly models domain invariance and uncertainty. We introduce variational Bayesian approximation into both the feature representation and classifier layers to facilitate invariant learning for better generalization across domains. In the probabilistic modeling framework, we introduce a domain-invariant principle to explore invariance across domains in a unified way. We incorporate the principle into the variational Bayesian layers in neural networks, achieving domain-invariant representations and classifier. We empirically demonstrate the effectiveness of our proposal on four widely used cross-domain visual recognition benchmarks. Ablation studies demonstrate the benefits of our proposal and on all benchmarks our variational invariant learning consistently delivers state-of-the-art performance. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiao|variational_invariant_learning_for_bayesian_domain_generalization", "one-sentence_summary": "We propose variational invariant learning, a probabilistic inference framework that jointly models domain invariance and uncertainty for Bayesian domain generalization", "pdf": "/pdf/4964403e8dbac758c6a7c684ffdf59b5fef70e7a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=diKKbSEhGe", "_bibtex": "@misc{\nxiao2021variational,\ntitle={Variational Invariant Learning for Bayesian Domain Generalization},\nauthor={Zehao Xiao and Jiayi Shen and Xiantong Zhen and Ling Shao and Cees G. M. Snoek},\nyear={2021},\nurl={https://openreview.net/forum?id=gHsr-v8Tz6l}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "gHsr-v8Tz6l", "replyto": "gHsr-v8Tz6l", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1550/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538116150, "tmdate": 1606915788221, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1550/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1550/-/Official_Review"}}}, {"id": "Nf-0ZWZlFN", "original": null, "number": 3, "cdate": 1603893640822, "ddate": null, "tcdate": 1603893640822, "tmdate": 1605024416743, "tddate": null, "forum": "gHsr-v8Tz6l", "replyto": "gHsr-v8Tz6l", "invitation": "ICLR.cc/2021/Conference/Paper1550/-/Official_Review", "content": {"title": "Review", "review": "The authors propose a variational Bayesian approach to domain adaptation.  The goal is to achieve flexibility in specifying domain invariance as well as modeling uncertainty. The variational approach could also help generalization.\n\nThe key idea is simple and straightforward \u2014 a distribution over domains is first specified and variational approximation is then used for parameter estimation. In particular, the authors decompose the variational distribution in terms of the domain and classifier parameters, applying the approximation to the last two network layers. The objective function is given as a weighted sum of the KL divergence corresponding the two component distributions.\n\nIn the empirical evaluation, the authors compare the performance on in-distribution and out-of-distribution data. They conclude that the proposed method achieves generally improved accuracy on several image datasets, including PACS, Office-Home, as well as MNIST variants. \n\nStrengths:\n- The presentation is clear. The proposed Bayesian formulation is natural and the resulting learning problem is computationally practical.\n\nWeaknesses: \n- Some parameters such as the weights \\lambda_{\\phi} and \\lambda_{\\phi} do not have clear Bayesian interpretations.\n- In the experimental evaluation, different baseline methods are used for each datasets. \n\nQuestions:\nHow do the parameters \\lambda_{\\phi} and \\lambda_{\\phi} impact the performance? Could we interpret these parameters from the MLE perspective?", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1550/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1550/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variational Invariant Learning for Bayesian Domain Generalization", "authorids": ["~Zehao_Xiao1", "~Jiayi_Shen3", "~Xiantong_Zhen1", "~Ling_Shao1", "~Cees_G._M._Snoek1"], "authors": ["Zehao Xiao", "Jiayi Shen", "Xiantong Zhen", "Ling Shao", "Cees G. M. Snoek"], "keywords": ["domain generalization", "variational invariant learning", "Bayesian inference"], "abstract": "Domain generalization addresses the out-of-distribution problem, which is challenging due to the domain shift and the uncertainty caused by the inaccessibility to data from the target domains. In this paper, we propose variational invariant learning, a probabilistic inference framework that jointly models domain invariance and uncertainty. We introduce variational Bayesian approximation into both the feature representation and classifier layers to facilitate invariant learning for better generalization across domains. In the probabilistic modeling framework, we introduce a domain-invariant principle to explore invariance across domains in a unified way. We incorporate the principle into the variational Bayesian layers in neural networks, achieving domain-invariant representations and classifier. We empirically demonstrate the effectiveness of our proposal on four widely used cross-domain visual recognition benchmarks. Ablation studies demonstrate the benefits of our proposal and on all benchmarks our variational invariant learning consistently delivers state-of-the-art performance. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiao|variational_invariant_learning_for_bayesian_domain_generalization", "one-sentence_summary": "We propose variational invariant learning, a probabilistic inference framework that jointly models domain invariance and uncertainty for Bayesian domain generalization", "pdf": "/pdf/4964403e8dbac758c6a7c684ffdf59b5fef70e7a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=diKKbSEhGe", "_bibtex": "@misc{\nxiao2021variational,\ntitle={Variational Invariant Learning for Bayesian Domain Generalization},\nauthor={Zehao Xiao and Jiayi Shen and Xiantong Zhen and Ling Shao and Cees G. M. Snoek},\nyear={2021},\nurl={https://openreview.net/forum?id=gHsr-v8Tz6l}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "gHsr-v8Tz6l", "replyto": "gHsr-v8Tz6l", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1550/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538116150, "tmdate": 1606915788221, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1550/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1550/-/Official_Review"}}}, {"id": "2UG1dHNNbX5", "original": null, "number": 4, "cdate": 1603920762687, "ddate": null, "tcdate": 1603920762687, "tmdate": 1605024416679, "tddate": null, "forum": "gHsr-v8Tz6l", "replyto": "gHsr-v8Tz6l", "invitation": "ICLR.cc/2021/Conference/Paper1550/-/Official_Review", "content": {"title": "The paper introduces a variational invariant learning approach with Bayesian approximation for domain generalization.", "review": "########################\n\nSummary:\n\nThe paper introduces a variational invariant learning approach with Bayesian approximation for domain generalization.\n\n####################\n\nReason for score:\n\nOverall, the paper is above the borderline. I like the idea of utilizing the Bayesian variational learning to address the domain generalization problem, which is very novel and promising. My major concern is about some unclear parts described in the paper and insufficient experimental comparison (see cons below). Hopefully, it would be grateful that the authors could address my concerns during the rebuttal period.\n\n########################\n\nPros:\n\n(1) The proposed variational Bayesian learning framework in the paper to represent uncertainty and enhance the generalization is reasonable and interesting, which can inspire other researchers in related fields.\n\n(2) The introduced domain-invariant principle to establish a domain-invariant feature extractor and classifier seems promising, which can lead to an end-to-end framework with CNN and a Bayesian network.\n\n(3) Extensive experimental results on four domain generalization benchmarks show the proposed method obtains a new state-of-the-art performance, which is appealing and convincing.\n\n########################\n\nCons:\n\n(1) In the paper, the authors claim that they only apply the domain invariance to the last feature extraction layer. In order to examine the relevance between the number of layers and the performance, an ablation study about the number of feature extraction layers should be added. In addition, the efficiency discussion about the model layers will be better to denote the setting chosen in the work.\n\n(2) In Table 1, what happens if without Bayesian and with both invariant, i.e., only introducing invariant loss into both classifier and the feature extractor? I notice that such a certain situation is missing in the experiments.\n\n(3) As shown in Table 2 and Table 3, DSON and L2A-OT can outperform the proposed model in terms of certain metrics, it would be better if the authors can analyze the results and give some comparisons about that.\n\n(4) As for the uncertainty estimated by the proposed Bayesian network, is there any threshold being set in the model to determine which samples can be used in the training\uff1f Moreover, how to utilized such estimated uncertainty should be discussed further.\n\n########################\n\nQuestions during the rebuttal period: \n\nPlease address and clarify the cons above. Thank you!\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1550/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1550/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variational Invariant Learning for Bayesian Domain Generalization", "authorids": ["~Zehao_Xiao1", "~Jiayi_Shen3", "~Xiantong_Zhen1", "~Ling_Shao1", "~Cees_G._M._Snoek1"], "authors": ["Zehao Xiao", "Jiayi Shen", "Xiantong Zhen", "Ling Shao", "Cees G. M. Snoek"], "keywords": ["domain generalization", "variational invariant learning", "Bayesian inference"], "abstract": "Domain generalization addresses the out-of-distribution problem, which is challenging due to the domain shift and the uncertainty caused by the inaccessibility to data from the target domains. In this paper, we propose variational invariant learning, a probabilistic inference framework that jointly models domain invariance and uncertainty. We introduce variational Bayesian approximation into both the feature representation and classifier layers to facilitate invariant learning for better generalization across domains. In the probabilistic modeling framework, we introduce a domain-invariant principle to explore invariance across domains in a unified way. We incorporate the principle into the variational Bayesian layers in neural networks, achieving domain-invariant representations and classifier. We empirically demonstrate the effectiveness of our proposal on four widely used cross-domain visual recognition benchmarks. Ablation studies demonstrate the benefits of our proposal and on all benchmarks our variational invariant learning consistently delivers state-of-the-art performance. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiao|variational_invariant_learning_for_bayesian_domain_generalization", "one-sentence_summary": "We propose variational invariant learning, a probabilistic inference framework that jointly models domain invariance and uncertainty for Bayesian domain generalization", "pdf": "/pdf/4964403e8dbac758c6a7c684ffdf59b5fef70e7a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=diKKbSEhGe", "_bibtex": "@misc{\nxiao2021variational,\ntitle={Variational Invariant Learning for Bayesian Domain Generalization},\nauthor={Zehao Xiao and Jiayi Shen and Xiantong Zhen and Ling Shao and Cees G. M. Snoek},\nyear={2021},\nurl={https://openreview.net/forum?id=gHsr-v8Tz6l}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "gHsr-v8Tz6l", "replyto": "gHsr-v8Tz6l", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1550/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538116150, "tmdate": 1606915788221, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1550/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1550/-/Official_Review"}}}], "count": 10}