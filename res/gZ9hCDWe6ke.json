{"notes": [{"id": "gZ9hCDWe6ke", "original": "BznQpRqf6yB", "number": 1041, "cdate": 1601308117454, "ddate": null, "tcdate": 1601308117454, "tmdate": 1616037062777, "tddate": null, "forum": "gZ9hCDWe6ke", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection", "authorids": ["~Xizhou_Zhu1", "~Weijie_Su2", "luotto@sensetime.com", "binli@ustc.edu.cn", "~Xiaogang_Wang2", "~Jifeng_Dai1"], "authors": ["Xizhou Zhu", "Weijie Su", "Lewei Lu", "Bin Li", "Xiaogang Wang", "Jifeng Dai"], "keywords": ["Efficient Attention Mechanism", "Deformation Modeling", "Multi-scale Representation", "End-to-End Object Detection"], "abstract": "DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10$\\times$ less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code is released at https://github.com/fundamentalvision/Deformable-DETR.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|deformable_detr_deformable_transformers_for_endtoend_object_detection", "one-sentence_summary": "Deformable DETR is an efficient and fast-converging end-to-end object detector. It mitigates the high complexity and slow convergence issues of DETR via a novel sampling-based efficient attention mechanism. ", "supplementary_material": "/attachment/75de09ec685bf33268166fcaad64aeb855ed2da2.zip", "pdf": "/pdf/758d4b5c0d63033d526ff8744d872a03543bb674.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhu2021deformable,\ntitle={Deformable {\\{}DETR{\\}}: Deformable Transformers for End-to-End Object Detection},\nauthor={Xizhou Zhu and Weijie Su and Lewei Lu and Bin Li and Xiaogang Wang and Jifeng Dai},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=gZ9hCDWe6ke}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "hCYsFFhKivQ", "original": null, "number": 1, "cdate": 1610040414434, "ddate": null, "tcdate": 1610040414434, "tmdate": 1610474012292, "tddate": null, "forum": "gZ9hCDWe6ke", "replyto": "gZ9hCDWe6ke", "invitation": "ICLR.cc/2021/Conference/Paper1041/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Oral)", "comment": "Accept. The paper proposes Deformable DETR that builds on DETR and solves the slow convergence and limited spatial resolution problem while getting impressive results.\nThe authors should think about comparing with other linear attention mechanisms to show the applicability of the method."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection", "authorids": ["~Xizhou_Zhu1", "~Weijie_Su2", "luotto@sensetime.com", "binli@ustc.edu.cn", "~Xiaogang_Wang2", "~Jifeng_Dai1"], "authors": ["Xizhou Zhu", "Weijie Su", "Lewei Lu", "Bin Li", "Xiaogang Wang", "Jifeng Dai"], "keywords": ["Efficient Attention Mechanism", "Deformation Modeling", "Multi-scale Representation", "End-to-End Object Detection"], "abstract": "DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10$\\times$ less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code is released at https://github.com/fundamentalvision/Deformable-DETR.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|deformable_detr_deformable_transformers_for_endtoend_object_detection", "one-sentence_summary": "Deformable DETR is an efficient and fast-converging end-to-end object detector. It mitigates the high complexity and slow convergence issues of DETR via a novel sampling-based efficient attention mechanism. ", "supplementary_material": "/attachment/75de09ec685bf33268166fcaad64aeb855ed2da2.zip", "pdf": "/pdf/758d4b5c0d63033d526ff8744d872a03543bb674.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhu2021deformable,\ntitle={Deformable {\\{}DETR{\\}}: Deformable Transformers for End-to-End Object Detection},\nauthor={Xizhou Zhu and Weijie Su and Lewei Lu and Bin Li and Xiaogang Wang and Jifeng Dai},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=gZ9hCDWe6ke}\n}"}, "tags": [], "invitation": {"reply": {"forum": "gZ9hCDWe6ke", "replyto": "gZ9hCDWe6ke", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040414420, "tmdate": 1610474012276, "id": "ICLR.cc/2021/Conference/Paper1041/-/Decision"}}}, {"id": "LZ9NJonoMnL", "original": null, "number": 4, "cdate": 1603790982697, "ddate": null, "tcdate": 1603790982697, "tmdate": 1606314623799, "tddate": null, "forum": "gZ9hCDWe6ke", "replyto": "gZ9hCDWe6ke", "invitation": "ICLR.cc/2021/Conference/Paper1041/-/Official_Review", "content": {"title": "Official Blind Review #4", "review": "Summary:\n\nThis paper proposes Deformable DETR with multi-scale deformable attention modules to solve the problems of DETR: slow convergence and limited feature spatial resolution.  In particular, it has faster convergence and achieves better performance(especially on small objects) than DETR.\n\nReasons for score:\n\nOverall, I vote for accepting. I like this paper because it solves the main problems suffered by DETR. My major concern is about the clarity of the paper and some additional ablation studies (see cons below). Hopefully the authors can address my concern in the rebuttal period.\n\nPros:\n\n1.This paper solves the main problems suffered by DETR: slow convergence and limited feature spatial resolution. In my opinion, it makes DETR more practical.\n\n2.The proposed Deformable DETR can obtain multi-scale features without a huge cost. In this way, it can be optimized easily and detect objects precisely, especially small objects.\n\n3.This paper also introduces some improvements and variants to boost the performance of Deformable DETR.\n\nCons:\n\n1.In the experiments, focal loss is used for bounding box classification. What is the reason for this choice? In addition, the number of object queries is increased from 100 to 300. Why? In the test, how to choose 100 objects from 300 objects? \n\n2.From Table 1, we can find DETR (500 epochs) has better performance than Deformable DETR on large objects (61.1 vs. 58.0), though the overall performance of Deformable DETR is better. Why?\n\n3.In the Table 1, there is only DETR-DC5+ (50 epochs). Could you provide DETR-DC5+ (500 epochs) ?\n\nQuestions during rebuttal period:\n\nPlease address and clarify the cons above", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1041/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1041/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection", "authorids": ["~Xizhou_Zhu1", "~Weijie_Su2", "luotto@sensetime.com", "binli@ustc.edu.cn", "~Xiaogang_Wang2", "~Jifeng_Dai1"], "authors": ["Xizhou Zhu", "Weijie Su", "Lewei Lu", "Bin Li", "Xiaogang Wang", "Jifeng Dai"], "keywords": ["Efficient Attention Mechanism", "Deformation Modeling", "Multi-scale Representation", "End-to-End Object Detection"], "abstract": "DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10$\\times$ less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code is released at https://github.com/fundamentalvision/Deformable-DETR.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|deformable_detr_deformable_transformers_for_endtoend_object_detection", "one-sentence_summary": "Deformable DETR is an efficient and fast-converging end-to-end object detector. It mitigates the high complexity and slow convergence issues of DETR via a novel sampling-based efficient attention mechanism. ", "supplementary_material": "/attachment/75de09ec685bf33268166fcaad64aeb855ed2da2.zip", "pdf": "/pdf/758d4b5c0d63033d526ff8744d872a03543bb674.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhu2021deformable,\ntitle={Deformable {\\{}DETR{\\}}: Deformable Transformers for End-to-End Object Detection},\nauthor={Xizhou Zhu and Weijie Su and Lewei Lu and Bin Li and Xiaogang Wang and Jifeng Dai},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=gZ9hCDWe6ke}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "gZ9hCDWe6ke", "replyto": "gZ9hCDWe6ke", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1041/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538128520, "tmdate": 1606915776638, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1041/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1041/-/Official_Review"}}}, {"id": "mbHIFotxQy6", "original": null, "number": 8, "cdate": 1606183058139, "ddate": null, "tcdate": 1606183058139, "tmdate": 1606183058139, "tddate": null, "forum": "gZ9hCDWe6ke", "replyto": "OyGK4BOH5Eb", "invitation": "ICLR.cc/2021/Conference/Paper1041/-/Official_Comment", "content": {"title": "Reply to AnonReviewer2 (2/2)", "comment": "Questions:\n\n----------\n\nQ#6: Do the last 5 rows in Tab.3 use \"iterative bounding box refinement\" and \"two-stage framework\"? The difference between these 5 lines is just the backbone?\n\nA#6: Sorry for the confusion. The differences among the last 5 rows in Tab.3 are just the backbone and TTA. They all use \"iterative bounding box refinement\" and \"two-stage framework\". We shall clarify this in revision.\n\n----------\n\nQ#7: Why stick to 50 epochs? How many epochs do the models in Tab.3 get trained?\n\nA#7: Models in Tab.3 are all trained for 50 epochs. The reason we use 50 epochs for most experiments is that it is within an affordable training time. Otherwise, the experiments would take too much time for us.\n\n----------\n\nQ#8: In Tab.1, why do \"iterative bounding box refinement\" and \"two-stage Deformable DETR\" have no influence on FPS? Does this FPS mean training FPS of Deformable DETR only? If that's the case, please also provide the total training time of everything.\n\nA#8: \"FPS\" in Tab.1 refers to the inference speed as usual. To be precise, the inference speed of Deformable DETR is 19.4 FPS. Adding \"iterative bounding box refinement\" causes <0.1 FPS drop. Further adding \"two-stage Deformable DETR\" makes the inference speed drop to 18.8 FPS. The total training time of models in Tab.1 are shown as follows (measured on NVIDIA Tesla V100 GPU), which will be added in the revision: \n\n| Model                               | Epochs | Total Training Time (GPU hours) |\n| ----------------------------------- | :----: | :-----------------------------: |\n| Faster R-CNN + FPN                  |  109   |               380               |\n| DETR                                |  500   |              2000               |\n| DETR-DC5                            |  500   |              7000               |\n| DETR-DC5                            |   50   |               700               |\n| DETR-DC5+                           |   50   |               700               |\n| Deformable DETR                     |   50   |               325               |\n| + iterative bounding box refinement |   50   |               325               |\n| ++ two-stage Deformable DETR        |   50   |               340               |\n\n----------\n\nQ#9: Definition/explanation of \"DETR-DC5\".\n\nA#9: We quote the word of \"DETR-DC5\" from the original DETR paper (see Technical details in Page 8 of [b]). The backbone of DETR-DC5 is ResNet50 with dilated C5 stage.\n\n[b] End-to-end object detection with transformers. In ECCV, 2020."}, "signatures": ["ICLR.cc/2021/Conference/Paper1041/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1041/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection", "authorids": ["~Xizhou_Zhu1", "~Weijie_Su2", "luotto@sensetime.com", "binli@ustc.edu.cn", "~Xiaogang_Wang2", "~Jifeng_Dai1"], "authors": ["Xizhou Zhu", "Weijie Su", "Lewei Lu", "Bin Li", "Xiaogang Wang", "Jifeng Dai"], "keywords": ["Efficient Attention Mechanism", "Deformation Modeling", "Multi-scale Representation", "End-to-End Object Detection"], "abstract": "DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10$\\times$ less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code is released at https://github.com/fundamentalvision/Deformable-DETR.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|deformable_detr_deformable_transformers_for_endtoend_object_detection", "one-sentence_summary": "Deformable DETR is an efficient and fast-converging end-to-end object detector. It mitigates the high complexity and slow convergence issues of DETR via a novel sampling-based efficient attention mechanism. ", "supplementary_material": "/attachment/75de09ec685bf33268166fcaad64aeb855ed2da2.zip", "pdf": "/pdf/758d4b5c0d63033d526ff8744d872a03543bb674.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhu2021deformable,\ntitle={Deformable {\\{}DETR{\\}}: Deformable Transformers for End-to-End Object Detection},\nauthor={Xizhou Zhu and Weijie Su and Lewei Lu and Bin Li and Xiaogang Wang and Jifeng Dai},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=gZ9hCDWe6ke}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "gZ9hCDWe6ke", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1041/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1041/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1041/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1041/Authors|ICLR.cc/2021/Conference/Paper1041/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1041/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864361, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1041/-/Official_Comment"}}}, {"id": "OyGK4BOH5Eb", "original": null, "number": 7, "cdate": 1606183029365, "ddate": null, "tcdate": 1606183029365, "tmdate": 1606183029365, "tddate": null, "forum": "gZ9hCDWe6ke", "replyto": "8In3YKsvOX7", "invitation": "ICLR.cc/2021/Conference/Paper1041/-/Official_Comment", "content": {"title": "Reply to AnonReviewer2 (1/2)", "comment": "We thank the reviewer for the careful reviews and constructive suggestions. We answer the questions as follows. \n\n----------\nQ#1: Comparisons between the deformable attention module and other \"linear\"/\"efficient\" implementations of attention should be performed. \n\nA#1: Thanks for the great suggestion. It is very valuable to conduct a survey to benchmark these efficient attention mechanisms on object detection. However, most of these efficient attention mechanisms are developed for the NLP domain, which haven been rarely studied in the image domain. Thus, conducting this survey requires lots of effort, which is beyond the scope of this paper. On the other hand, as we discussed in the related works, in the image domain, the designs of efficient attention mechanism are still limited to using pre-defined sparse attention patterns. Despite the theoretically reduced complexity, such approaches are much slower in implementation than traditional convolution with the same FLOPs (at least 3\u00d7 slower), due to the intrinsic limitation in memory access patterns. Compared to these efficient attention mechanisms in the image domain, our deformable attention module learns data-dependent sparse attention patterns, and just slightly slower than the traditional convolution under the same FLOPs.\n\n----------\nQ#2: \"Another missed baseline is DETR with multi-scale input and attention (vanilla version) for decently long epochs.\"\n\nA#2: Thanks for your suggestion. However, it is impossible to conduct such an experiment. In particular, training DETR with multi-scale inputs and vanilla transformer attention modules would require more than 500G of GPU memory (with batchsize of 1), which far exceeds the GPU memory limit. This is because the vanilla transformer attention modules suffer from a quadratic complexity w.r.t the number of input elements, which is very large in the case of multi-scale inputs.\n\n----------\nQ#3: Effect of level embedding.\n\nA#3: Adding level embedding is a natural design to identify which feature level each query pixel lies in. It will not introduce any additional computations, but improves the AP with 0.5 points. Due to the page limit, we did not include this ablation experiment, which will be added in the revision.\n\n| Deformable DETR     | Epochs |  AP  | AP50 | AP75 | AP@S | AP@M | AP@L |\n| ------------------- | :----: | :--: | :--: | :--: | :--: | :--: | :--: |\n| w. level embedding  |   50   | 43.8 | 62.6 | 47.7 | 26.4 | 47.1 | 58.0 |\n| w/o level embedding |   50   | 43.3 | 62.3 | 47.2 | 25.9 | 46.5 | 57.6 |\n\n----------\nQ#4: Why using multi-scale feature maps instead of a high-resolution feature map (e.g., H/4 x W/4) to mitigate the issue of small objects?\n\nA#4: It is a common practice in object detection, that large objects are detected on low-resolution feature maps while small objects are detected on high-resolution feature maps. As shown in [a], using multi-scale feature maps is more effective than using the single-scale high-resolution feature map in object detection.\n\n[a] Feature pyramid networks for object detection. In CVPR, 2017.\n\n----------\nMinor:\n\nQ#5: Add a lookup table for notations in the Appendix. Move the discussion about FPN into Appendix.\n\nA#5: Thanks for the suggestions. We shall consider them in revision.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1041/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1041/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection", "authorids": ["~Xizhou_Zhu1", "~Weijie_Su2", "luotto@sensetime.com", "binli@ustc.edu.cn", "~Xiaogang_Wang2", "~Jifeng_Dai1"], "authors": ["Xizhou Zhu", "Weijie Su", "Lewei Lu", "Bin Li", "Xiaogang Wang", "Jifeng Dai"], "keywords": ["Efficient Attention Mechanism", "Deformation Modeling", "Multi-scale Representation", "End-to-End Object Detection"], "abstract": "DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10$\\times$ less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code is released at https://github.com/fundamentalvision/Deformable-DETR.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|deformable_detr_deformable_transformers_for_endtoend_object_detection", "one-sentence_summary": "Deformable DETR is an efficient and fast-converging end-to-end object detector. It mitigates the high complexity and slow convergence issues of DETR via a novel sampling-based efficient attention mechanism. ", "supplementary_material": "/attachment/75de09ec685bf33268166fcaad64aeb855ed2da2.zip", "pdf": "/pdf/758d4b5c0d63033d526ff8744d872a03543bb674.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhu2021deformable,\ntitle={Deformable {\\{}DETR{\\}}: Deformable Transformers for End-to-End Object Detection},\nauthor={Xizhou Zhu and Weijie Su and Lewei Lu and Bin Li and Xiaogang Wang and Jifeng Dai},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=gZ9hCDWe6ke}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "gZ9hCDWe6ke", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1041/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1041/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1041/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1041/Authors|ICLR.cc/2021/Conference/Paper1041/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1041/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864361, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1041/-/Official_Comment"}}}, {"id": "Df3QO93G1QK", "original": null, "number": 6, "cdate": 1606182798927, "ddate": null, "tcdate": 1606182798927, "tmdate": 1606182798927, "tddate": null, "forum": "gZ9hCDWe6ke", "replyto": "C2eDl-piNdR", "invitation": "ICLR.cc/2021/Conference/Paper1041/-/Official_Comment", "content": {"title": "Reply to AnonReviewer3", "comment": "We thank the reviewer for the careful reviews and constructive suggestions. We answer the questions as follows.\n\n----------\nQ#1: \"Since the deformable attention module is the core of the contribution, it should be interesting to add a figure dedicated to this component.\"\n\nA#1: Thanks for the good suggestion. Due to the page limit, we did not include the figure that demonstrates the deformable attention module. We shall add it in the revision, where one more page is available.\n\n----------\nQ#2: \"It seems that the Axial-DeepLab paper presented in ECCV-2020 misses in the references.\"\n\nA#2: Thanks for your kind reminder. We shall add it in the revision.\n\n----------\nQ#3: The computation speed of deformable attention modules is slower than classical convolution.\n\nA#3: As we discussed in the Related Work and Section 5.1, the vanilla Transformer attention suffers from the large amount of memory access, meanwhile the previously proposed sparse attention mechanisms suffer from the intrinsic limitation in memory access patterns. Our proposed deformable attention can mitigate these issues, but at the cost of small amount of unordered memory access. Thus, it is still slightly slower than the traditional convolution under the same FLOPs.\n\nOn the other hand, the speed issue is because modern GPUs are equipped with highly optimized hardware and implementations for traditional convolution, while the random memory access required by the deformable attention modules is hardware unfriendly. The improvement in hardware and implementations may mitigate this issue.\n\n----------\nQ#4: \"It should be interesting to also report fps in the state of the art comparison table 3.\"\n\nA#4: It is difficult to compare FPS fairly with other methods at system level in table 3, because they use different codebases, DL frameworks (TensorFlow, PyTorch, etc.), and hardware platforms. We would try our best to add this comparison in revision. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1041/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1041/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection", "authorids": ["~Xizhou_Zhu1", "~Weijie_Su2", "luotto@sensetime.com", "binli@ustc.edu.cn", "~Xiaogang_Wang2", "~Jifeng_Dai1"], "authors": ["Xizhou Zhu", "Weijie Su", "Lewei Lu", "Bin Li", "Xiaogang Wang", "Jifeng Dai"], "keywords": ["Efficient Attention Mechanism", "Deformation Modeling", "Multi-scale Representation", "End-to-End Object Detection"], "abstract": "DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10$\\times$ less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code is released at https://github.com/fundamentalvision/Deformable-DETR.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|deformable_detr_deformable_transformers_for_endtoend_object_detection", "one-sentence_summary": "Deformable DETR is an efficient and fast-converging end-to-end object detector. It mitigates the high complexity and slow convergence issues of DETR via a novel sampling-based efficient attention mechanism. ", "supplementary_material": "/attachment/75de09ec685bf33268166fcaad64aeb855ed2da2.zip", "pdf": "/pdf/758d4b5c0d63033d526ff8744d872a03543bb674.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhu2021deformable,\ntitle={Deformable {\\{}DETR{\\}}: Deformable Transformers for End-to-End Object Detection},\nauthor={Xizhou Zhu and Weijie Su and Lewei Lu and Bin Li and Xiaogang Wang and Jifeng Dai},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=gZ9hCDWe6ke}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "gZ9hCDWe6ke", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1041/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1041/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1041/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1041/Authors|ICLR.cc/2021/Conference/Paper1041/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1041/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864361, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1041/-/Official_Comment"}}}, {"id": "8-6OJAZ3Ea9", "original": null, "number": 5, "cdate": 1606182761530, "ddate": null, "tcdate": 1606182761530, "tmdate": 1606182761530, "tddate": null, "forum": "gZ9hCDWe6ke", "replyto": "LZ9NJonoMnL", "invitation": "ICLR.cc/2021/Conference/Paper1041/-/Official_Comment", "content": {"title": "Reply to AnonReviewer4", "comment": "We thank the reviewer for the careful reviews and constructive suggestions. We answer the questions as follows.\n\n----------\nQ#1(a): \"In the experiments, focal loss is used for bounding box classification. What is the reason for this choice? In addition, the number of object queries is increased from 100 to 300. Why? \n\nA#1(a): These two modifications are both general improvements for object detectors, which are also applicable on vanilla DETR. We also include an improved version of DETR (namely \"DETR-DC5+\") in table 1 for a fair comparison.\n\nQ#1(b): \"In the test, how to choose 100 objects from 300 objects?\"\n\nA#1(b): For each image, following the standard evaluation protocol for COCO detection, the top-scored 100 detections (across all categories) will be chosen as the final predictions.\n\n----------\nQ#2: \"From Table 1, we can find DETR (500 epochs) has better performance than Deformable DETR on large objects (61.1 vs. 58.0), though the overall performance of Deformable DETR is better. Why?\"\n\nA#2: Indeed, we also find it is interesting that the original DETR is better than our Deformable DETR on large objects. However, we still don\u2019t know what the reason is. Further study is needed.\n\n----------\nQ#3: \"In Table 1, there is only DETR-DC5+ (50 epochs). Could you provide DETR-DC5+ (500 epochs)?\"\n\nA#3: Thanks for your suggestion. However, training DETR-DC5 for 500 epochs is extremely slow. Around 18 days are required on 16 NVIDIA Tesla V100 GPUs. Due to limited computing resources, it is not affordable for us. By the way, from this point, Deformable DETR opens up the possibilities to explore these variants thanks to its fast convergence and efficiency.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1041/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1041/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection", "authorids": ["~Xizhou_Zhu1", "~Weijie_Su2", "luotto@sensetime.com", "binli@ustc.edu.cn", "~Xiaogang_Wang2", "~Jifeng_Dai1"], "authors": ["Xizhou Zhu", "Weijie Su", "Lewei Lu", "Bin Li", "Xiaogang Wang", "Jifeng Dai"], "keywords": ["Efficient Attention Mechanism", "Deformation Modeling", "Multi-scale Representation", "End-to-End Object Detection"], "abstract": "DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10$\\times$ less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code is released at https://github.com/fundamentalvision/Deformable-DETR.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|deformable_detr_deformable_transformers_for_endtoend_object_detection", "one-sentence_summary": "Deformable DETR is an efficient and fast-converging end-to-end object detector. It mitigates the high complexity and slow convergence issues of DETR via a novel sampling-based efficient attention mechanism. ", "supplementary_material": "/attachment/75de09ec685bf33268166fcaad64aeb855ed2da2.zip", "pdf": "/pdf/758d4b5c0d63033d526ff8744d872a03543bb674.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhu2021deformable,\ntitle={Deformable {\\{}DETR{\\}}: Deformable Transformers for End-to-End Object Detection},\nauthor={Xizhou Zhu and Weijie Su and Lewei Lu and Bin Li and Xiaogang Wang and Jifeng Dai},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=gZ9hCDWe6ke}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "gZ9hCDWe6ke", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1041/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1041/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1041/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1041/Authors|ICLR.cc/2021/Conference/Paper1041/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1041/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864361, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1041/-/Official_Comment"}}}, {"id": "uc0foSNbd8u", "original": null, "number": 4, "cdate": 1606182720558, "ddate": null, "tcdate": 1606182720558, "tmdate": 1606182720558, "tddate": null, "forum": "gZ9hCDWe6ke", "replyto": "fya7XUpEcg", "invitation": "ICLR.cc/2021/Conference/Paper1041/-/Official_Comment", "content": {"title": "Reply to AnonReviewer1", "comment": "We thank the reviewer for the careful reviews and constructive suggestions. We answer the questions as follows.\n\n----------\nQ#1: \"The training and testing times could be reported in the paper, which is useful for other researchers to implement and use this method.\"\n\nA#1: Thanks for your good suggestion. The testing time is already reported in the column \"FPS\" of Table 1. The total training time of each model in Table 1 is shown as follows (measured on NVIDIA Tesla V100 GPU), which will be added in the revision:\n\n| Model                               | Epochs | Total Training Time (GPU hours) |\n| ----------------------------------- | :----: | :-----------------------------: |\n| Faster R-CNN + FPN                  |  109   |               380               |\n| DETR                                |  500   |              2000               |\n| DETR-DC5                            |  500   |              7000               |\n| DETR-DC5                            |   50   |               700               |\n| DETR-DC5+                           |   50   |               700               |\n| Deformable DETR                     |   50   |               325               |\n| + iterative bounding box refinement |   50   |               325               |\n| ++ two-stage Deformable DETR        |   50   |               340               |\n\n----------\nQ#2: \"Some related methods on sparse connected self-attention/transformer [a,b,c] should be cited and discussed.\"\n\nA#2: Thanks for your kind reminder. We shall cite and discuss them in revision. By the way, [c] has already been cited.\n\n[a] Representative Graph Neural Network, CVPR 2020\n\n[b] Dynamic Graph Message Passing Networks, CVPR 2020\n\n[c] CCNet: Criss-Cross Attention for Semantic Segmentation in ICCV 19 & TPAMI 2020"}, "signatures": ["ICLR.cc/2021/Conference/Paper1041/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1041/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection", "authorids": ["~Xizhou_Zhu1", "~Weijie_Su2", "luotto@sensetime.com", "binli@ustc.edu.cn", "~Xiaogang_Wang2", "~Jifeng_Dai1"], "authors": ["Xizhou Zhu", "Weijie Su", "Lewei Lu", "Bin Li", "Xiaogang Wang", "Jifeng Dai"], "keywords": ["Efficient Attention Mechanism", "Deformation Modeling", "Multi-scale Representation", "End-to-End Object Detection"], "abstract": "DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10$\\times$ less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code is released at https://github.com/fundamentalvision/Deformable-DETR.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|deformable_detr_deformable_transformers_for_endtoend_object_detection", "one-sentence_summary": "Deformable DETR is an efficient and fast-converging end-to-end object detector. It mitigates the high complexity and slow convergence issues of DETR via a novel sampling-based efficient attention mechanism. ", "supplementary_material": "/attachment/75de09ec685bf33268166fcaad64aeb855ed2da2.zip", "pdf": "/pdf/758d4b5c0d63033d526ff8744d872a03543bb674.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhu2021deformable,\ntitle={Deformable {\\{}DETR{\\}}: Deformable Transformers for End-to-End Object Detection},\nauthor={Xizhou Zhu and Weijie Su and Lewei Lu and Bin Li and Xiaogang Wang and Jifeng Dai},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=gZ9hCDWe6ke}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "gZ9hCDWe6ke", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1041/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1041/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1041/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1041/Authors|ICLR.cc/2021/Conference/Paper1041/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1041/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864361, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1041/-/Official_Comment"}}}, {"id": "jrgsufuYPUz", "original": null, "number": 3, "cdate": 1606182642356, "ddate": null, "tcdate": 1606182642356, "tmdate": 1606182642356, "tddate": null, "forum": "gZ9hCDWe6ke", "replyto": "WoBeM7mA97O", "invitation": "ICLR.cc/2021/Conference/Paper1041/-/Official_Comment", "content": {"title": "Reply to Nicolas Carion (2/2)", "comment": "----------\nQ#5: \"disentangle the effect of the multi-resolution and the effect of the proposed deformable module\"\n\nA#5: Thanks for your good suggestion. We tried training Deformable DETR (K = 4) with single-scale input feature maps (of stride 32) for 50 epochs and 150 epochs. The results are shown as follows. The single-scale model of 50 epochs is still slightly worse than DETR (500 epochs) in total AP, while the single-scale model of 150 epochs achieves on par accuracy with DETR (500 epochs). However, the phenomenon is still slightly different. The single-scale Deformable DETR performs better on small objects and performs worse on large objects, compared with vanilla DETR. Further study is needed.\n\n| Method                         |  K   | Epochs |  AP  | AP50 | AP75 | AP@S | AP@M | AP@L |\n| ------------------------------ | :--: | :----: | :--: | :--: | :--: | :--: | :--: | :--: |\n| DETR                           |  -   |  500   | 42.0 | 62.4 | 44.2 | 20.5 | 45.8 | 61.1 |\n| DETR                           |  -   |   50   | 33.3 | 54.1 | 34.2 | 13.3 | 35.9 | 52.0 |\n| Deformable DETR (single scale) |  4   |   50   | 40.5 | 60.6 | 43.0 | 21.5 | 44.7 | 56.7 |\n| Deformable DETR (multi scale)  |  4   |   50   | 43.8 | 62.6 | 47.7 | 26.4 | 47.1 | 58.0 |\n| DETR                           |  -   |  150   | 39.5 | 60.3 | 41.4 | 17.5 | 43.0 | 59.1 |\n| Deformable DETR (single scale) |  4   |  150   | 41.6 | 61.9 | 44.6 | 22.8 | 45.3 | 58.0 |\n| Deformable DETR (multi scale)  |  4   |  150   | 45.3 | 64.3 | 49.1 | 27.1 | 48.4 | 60.0 |"}, "signatures": ["ICLR.cc/2021/Conference/Paper1041/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1041/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection", "authorids": ["~Xizhou_Zhu1", "~Weijie_Su2", "luotto@sensetime.com", "binli@ustc.edu.cn", "~Xiaogang_Wang2", "~Jifeng_Dai1"], "authors": ["Xizhou Zhu", "Weijie Su", "Lewei Lu", "Bin Li", "Xiaogang Wang", "Jifeng Dai"], "keywords": ["Efficient Attention Mechanism", "Deformation Modeling", "Multi-scale Representation", "End-to-End Object Detection"], "abstract": "DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10$\\times$ less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code is released at https://github.com/fundamentalvision/Deformable-DETR.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|deformable_detr_deformable_transformers_for_endtoend_object_detection", "one-sentence_summary": "Deformable DETR is an efficient and fast-converging end-to-end object detector. It mitigates the high complexity and slow convergence issues of DETR via a novel sampling-based efficient attention mechanism. ", "supplementary_material": "/attachment/75de09ec685bf33268166fcaad64aeb855ed2da2.zip", "pdf": "/pdf/758d4b5c0d63033d526ff8744d872a03543bb674.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhu2021deformable,\ntitle={Deformable {\\{}DETR{\\}}: Deformable Transformers for End-to-End Object Detection},\nauthor={Xizhou Zhu and Weijie Su and Lewei Lu and Bin Li and Xiaogang Wang and Jifeng Dai},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=gZ9hCDWe6ke}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "gZ9hCDWe6ke", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1041/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1041/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1041/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1041/Authors|ICLR.cc/2021/Conference/Paper1041/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1041/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864361, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1041/-/Official_Comment"}}}, {"id": "WoBeM7mA97O", "original": null, "number": 2, "cdate": 1606182611302, "ddate": null, "tcdate": 1606182611302, "tmdate": 1606182611302, "tddate": null, "forum": "gZ9hCDWe6ke", "replyto": "x1VT5henOtF", "invitation": "ICLR.cc/2021/Conference/Paper1041/-/Official_Comment", "content": {"title": "Reply to Nicolas Carion (1/2)", "comment": "Hi Nicolas,\n\nThanks for your good questions and constructive suggestions. First of all, we answer the question in your title. In our understanding, attention mechanisms are known by the property that it enables the neural networks to focus more on relevant elements of the input than on irrelevant parts. It is not defined by the formulation of how the attention weights are calculate. In this context, the proposed deformable attention is indeed an attention mechanism. Then, we answer the other questions as follows.\n\n----------\nQ#1: \"...attention weight are obtained via linear projection over the query feature...by definition an attention should incorporate both features from the query and the key...As a side note, in the proposed formulation, the $A_{mqk}$ could very well be computed as a dot-product as well (between the query and each of the sampled point), making it a 'true' attention mechanism. Have you tried such thing?\"\n\nA#1: Yes, we tried using dot-product to obtain the attention weight in some early experiments, where $K=1$ and other design choices are very similar with the default setting of Deformable DETR. It achieves on par performance (AP difference <0.5%) compared with that of linear projection. However, we experimentally found that using dot-product results ~25% slower speed than that of linear projection. Therefore, we choose to obtain the attention weight by linear projection for efficiency.\n\nWe guess the reason of their very close performance is that the stacked convolutions and attention layers have provided enough contextual information for the query feature to determine the attention weights. We are also inspired by [a], which shows that the dot-product between the query and key content features (without the positional encodings) plays a minor role in the transformer self-attention.\n\nIn terms of speed, using dot-product has the same computational complexity as the linear projection. The inefficiency of the dot-product may be related to the implementation. In comparison to linear projection, the dot-product requires additional random memory access for sampling key features and the batch matrix-matrix product, which may be the reason of inefficiency. In particular, for the dot-product, we first sample the key features (with the shape of $N_Q \\times N_K \\times C$) related to each query, and then apply the batch matrix-matrix product with the query features (with the shape of $N_Q \\times C \\times 1$), in order to obtain the attention weights (with the shape of $N_Q \\times N_K$). Meanwhile, for the linear projection, we only need to compute a matrix multiplication between the query features (with the shape of $N_Q \\times C$) and the weights of linear projection (with the shape of $C \\times N_K$). \n\nWe shall add these comparisons and discussions in the revision. Thanks for your suggestion.\n\n[a] An Empirical Study of Spatial Attention Mechanisms in Deep Networks. In ICCV, 2019.\n\n----------\nQ#2: \"showcase visualizations of the 'attention' maps in the encoder and the decoder\"\n\nA#2: In the encoder, we observed very similar attention patterns between Deformable DETR and vanilla DETR, namely \"instance separation\". However, in the decoder, Deformable DETR focuses on the whole foreground instance instead of extreme points in vanilla DETR. We shall add the visualization in the revision.\n\n----------\nQ#3: Adding \"two-stage DETR\" and \"iterative bounding box refinement\" to vanilla DETR. \n\nA#3: Actually, applying these two techniques to vanilla DETR is not straight-forward. Both techniques require a mechanism to guide the attention module w.r.t. the previously predicted bounding boxes, which is missing in vanilla DETR. Besides, trial and error are required to make these techniques compatible with vanilla DETR. However, vanilla DETR requires a very long training time. Due to limited computing resources, it is not affordable for us (also for many labs) to do many trials. From this point, Deformable DETR opens up the possibilities to explore these variants with its fast convergence and efficiency.\n\n----------\nQ#4: \"It's surprising that it helps mainly for large objects in the deformable setting, I would have guessed it would help for small instead.\"\n\nA#4: We are also surprised by the phenomenon that \"iterative bounding box refinement\" helps mainly for large objects, while \"two-stage Deformable DETR\" helps mainly for small objects. It is very interesting. However, we still don\u2019t know what the reason is. Further study is needed.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1041/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1041/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection", "authorids": ["~Xizhou_Zhu1", "~Weijie_Su2", "luotto@sensetime.com", "binli@ustc.edu.cn", "~Xiaogang_Wang2", "~Jifeng_Dai1"], "authors": ["Xizhou Zhu", "Weijie Su", "Lewei Lu", "Bin Li", "Xiaogang Wang", "Jifeng Dai"], "keywords": ["Efficient Attention Mechanism", "Deformation Modeling", "Multi-scale Representation", "End-to-End Object Detection"], "abstract": "DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10$\\times$ less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code is released at https://github.com/fundamentalvision/Deformable-DETR.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|deformable_detr_deformable_transformers_for_endtoend_object_detection", "one-sentence_summary": "Deformable DETR is an efficient and fast-converging end-to-end object detector. It mitigates the high complexity and slow convergence issues of DETR via a novel sampling-based efficient attention mechanism. ", "supplementary_material": "/attachment/75de09ec685bf33268166fcaad64aeb855ed2da2.zip", "pdf": "/pdf/758d4b5c0d63033d526ff8744d872a03543bb674.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhu2021deformable,\ntitle={Deformable {\\{}DETR{\\}}: Deformable Transformers for End-to-End Object Detection},\nauthor={Xizhou Zhu and Weijie Su and Lewei Lu and Bin Li and Xiaogang Wang and Jifeng Dai},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=gZ9hCDWe6ke}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "gZ9hCDWe6ke", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1041/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1041/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1041/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1041/Authors|ICLR.cc/2021/Conference/Paper1041/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1041/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864361, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1041/-/Official_Comment"}}}, {"id": "x1VT5henOtF", "original": null, "number": 1, "cdate": 1605044998269, "ddate": null, "tcdate": 1605044998269, "tmdate": 1605045163227, "tddate": null, "forum": "gZ9hCDWe6ke", "replyto": "gZ9hCDWe6ke", "invitation": "ICLR.cc/2021/Conference/Paper1041/-/Public_Comment", "content": {"title": "Is deformable attention an attention mechanism?", "comment": "Hello,\n\nI have a question about the \"Deformable Attention Module\" presented in this paper. According to the text following equation (2), the attention coefficients $A_{m,q,k}$ \"are obtained via linear projection over the query feature\". In my opinion, that does not constitute an attention mechanism per-se, since by definition an attention should incorporate both features from the query and the key (the most popular instantiation of an attention mechanism being dot-product attention).\nIn other words, the presented mechanism aggregates features in a neighborhood irrespective of what the actual features are.\nFrom a purely terminological point of view, the role played by these $A_{mqk}$ is more akin to a gating, and in my opinion a better name for the proposed module would be something like \"multi-resolution gated deformable convolution\". What is your opinion on this?\nAs a side note, in the proposed formulation, the $A_{m,q,k}$ could very well be computed as a dot-product as well (between the query and each of the sampled point), making it a \"true\" attention mechanism. Have you tried such thing?\n\nAside from the numerical results, which are impressive, it would provide invaluable insights to showcase visualizations of the \"attention\" maps in the encoder and the decoder, similar to the original DETR paper. In particular, is the proposed model learning an approximation of what the original DETR attention attends to, or is it attending to completely different things? Do you still observe instance separation inside the encoder?\n\nThe two additions on top of the vanilla model (namely \"two stage detr\" and \"iterative bounding box refinement\") are interesting and seem to be directly applicable to the original DETR as well. Do you have a ball-park estimate of the performance reached when applying them on DETR-DC5?\nAs a side note, the gains obtained thanks to the iterative bounding box refinement might hint at limitations in the original auxiliary-loss formulation, or the L1+GIOU combinations. It would be interesting to analyze this more deeply. It's surprising that it helps mainly for large objects in the deformable setting, I would have guessed it would help for small instead.\n\nFinally, an ablation that would be valuable in my opinion would be to more carefully disentangle the effect of the multi-resolution and the effect of the proposed deformable module. In particular, in table2, the only experiment with no multi-scale input is done with k=1. What would be the performance with k=4 or even k=8 (which would be the most similar to DETR)? I'm curious to see if the proposed deformable module can match original DETR's performance on a low-res feature map, and if it requires long training schedule to do so. That would help understanding where the convergence speed boost comes from, and the exact performance trade-offs that are being made by using this different deformable mechanism as opposed to traditional attention.\n\n"}, "signatures": ["~Nicolas_Carion1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Nicolas_Carion1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection", "authorids": ["~Xizhou_Zhu1", "~Weijie_Su2", "luotto@sensetime.com", "binli@ustc.edu.cn", "~Xiaogang_Wang2", "~Jifeng_Dai1"], "authors": ["Xizhou Zhu", "Weijie Su", "Lewei Lu", "Bin Li", "Xiaogang Wang", "Jifeng Dai"], "keywords": ["Efficient Attention Mechanism", "Deformation Modeling", "Multi-scale Representation", "End-to-End Object Detection"], "abstract": "DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10$\\times$ less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code is released at https://github.com/fundamentalvision/Deformable-DETR.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|deformable_detr_deformable_transformers_for_endtoend_object_detection", "one-sentence_summary": "Deformable DETR is an efficient and fast-converging end-to-end object detector. It mitigates the high complexity and slow convergence issues of DETR via a novel sampling-based efficient attention mechanism. ", "supplementary_material": "/attachment/75de09ec685bf33268166fcaad64aeb855ed2da2.zip", "pdf": "/pdf/758d4b5c0d63033d526ff8744d872a03543bb674.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhu2021deformable,\ntitle={Deformable {\\{}DETR{\\}}: Deformable Transformers for End-to-End Object Detection},\nauthor={Xizhou Zhu and Weijie Su and Lewei Lu and Bin Li and Xiaogang Wang and Jifeng Dai},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=gZ9hCDWe6ke}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "gZ9hCDWe6ke", "readers": {"description": "User groups that will be able to read this comment.", "values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed."}}, "expdate": 1605630600000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1041/Authors", "ICLR.cc/2021/Conference/Paper1041/Reviewers", "ICLR.cc/2021/Conference/Paper1041/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1605024976426, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1041/-/Public_Comment"}}}, {"id": "8In3YKsvOX7", "original": null, "number": 1, "cdate": 1603606841864, "ddate": null, "tcdate": 1603606841864, "tmdate": 1605024545227, "tddate": null, "forum": "gZ9hCDWe6ke", "replyto": "gZ9hCDWe6ke", "invitation": "ICLR.cc/2021/Conference/Paper1041/-/Official_Review", "content": {"title": "Good paper; solid results on established benchmark; improved a recent detection model; some evaluations are missing;", "review": "### Summary\nThis paper aims to improve a very recent detection model -- DETR, which suffers from two issues: long training time and limited feature spatial resolution. Targeting these issues, this paper proposes (1) deformable attention (2) multi-scale processing (inputs/attention) for DETR, which have greatly reduced the  training time and improved the performance. Moreover, it develops two additional modules \"iterative bounding box refinement\" and  \"two-stage framework\", which help to achieve the SOTA detection results on COCO.\n\n### Pros\n - This paper is insightful as it studies the two biggest issues of DETR. The vanilla attention is slow but hard to replace as the performance may be harmed. Similarly, it's not super straightforward to incorporate multi-scale processing into DETR due to the novel framework architecture. This paper has addressed these two issues, demonstrated outstanding performance on COCO, and properly studied the effectiveness of each single module. \n - The deformable attention module is a new kind of attention implementation. It samples a fix number of feature points on spatial feature map and thus greatly reduces the complexity. \n - The additional techniques make lots of sense to me, and improve the results greatly. I'm impressed that transformer based model can achieve very competitive  as shown in Tab.3. \n\n### Cons\n - The deformable attention is considered the most important contribution of this paper and thus should be studied more thoroughly. Specifically, comparisons between this module and other \"linear\"/\"efficient\" implementations of attention should be performed. As discussed in related work, baselines like \"pre-defined sparse attention\", \"data-dependent sparse attention\", and \"low-rank property attention\" should be considered. This kind of comparisons will help us to understand better about the proposed module, and also to researchers from other field who are interested in using this.\n  - Another missed baseline is DETR with multi-scale input and attention (vanilla version) for decently long epochs. I would want to know whether the proposed multi-scale thing could help the vanilla DETR.\n - Does the level embedding help? I don't find experiments to support this design choice.\n - If small objects are the issue, why not have a feature map of H/4 x W/4? Why are multi-scale feature maps constructed like in Appendix Fig.3? \n\n### Minor\n - There are lots of notations. It's pretty hard for me to remember \"mlqk, A, W, \\phi, ...\". I would recommend the authors providing a lookup table in Appendix.\n - Personally, I think the discussion about FPN in main paper is distractive. The authors may want to move all of them into Appendix.\n\n### Questions\n - Do the last 5 rows in Tab.3 use \"iterative bounding box refinement\" and  \"two-stage framework\"? The difference between these 5 lines are just the backbone? \n - Fig.2 shows that Deformable DETR keeps improving over time. It doesn't converge at Ep.50. Why stick to this number in most experiments? How many epochs do the models in Tab.3 get trained?\n - In Tab.1, why do \"iterative bounding box refinement\" and \"two-stage Deformable DETR\" have no influence on FPS? Shouldn't the speed become slower as they are iterative and stage-wise. Does this FPS mean training FPS of Deformable DETR only? If that's the case, please also provide the total training time of everything.\n- Please guide me to the definition/explanation of \"DETR-DC5\". Does it mean the backbone in DETE is changed to ResNet-DC5?\n\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1041/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1041/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection", "authorids": ["~Xizhou_Zhu1", "~Weijie_Su2", "luotto@sensetime.com", "binli@ustc.edu.cn", "~Xiaogang_Wang2", "~Jifeng_Dai1"], "authors": ["Xizhou Zhu", "Weijie Su", "Lewei Lu", "Bin Li", "Xiaogang Wang", "Jifeng Dai"], "keywords": ["Efficient Attention Mechanism", "Deformation Modeling", "Multi-scale Representation", "End-to-End Object Detection"], "abstract": "DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10$\\times$ less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code is released at https://github.com/fundamentalvision/Deformable-DETR.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|deformable_detr_deformable_transformers_for_endtoend_object_detection", "one-sentence_summary": "Deformable DETR is an efficient and fast-converging end-to-end object detector. It mitigates the high complexity and slow convergence issues of DETR via a novel sampling-based efficient attention mechanism. ", "supplementary_material": "/attachment/75de09ec685bf33268166fcaad64aeb855ed2da2.zip", "pdf": "/pdf/758d4b5c0d63033d526ff8744d872a03543bb674.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhu2021deformable,\ntitle={Deformable {\\{}DETR{\\}}: Deformable Transformers for End-to-End Object Detection},\nauthor={Xizhou Zhu and Weijie Su and Lewei Lu and Bin Li and Xiaogang Wang and Jifeng Dai},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=gZ9hCDWe6ke}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "gZ9hCDWe6ke", "replyto": "gZ9hCDWe6ke", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1041/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538128520, "tmdate": 1606915776638, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1041/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1041/-/Official_Review"}}}, {"id": "C2eDl-piNdR", "original": null, "number": 2, "cdate": 1603725021854, "ddate": null, "tcdate": 1603725021854, "tmdate": 1605024545149, "tddate": null, "forum": "gZ9hCDWe6ke", "replyto": "gZ9hCDWe6ke", "invitation": "ICLR.cc/2021/Conference/Paper1041/-/Official_Review", "content": {"title": "This paper introduces Deformable DETR: A modified version of the recent DETR paper for end-to-end object detection with transformers", "review": "The main contribution is a new attention module called deformable attention module. Like deformable convolution, it adds a translation term into the expression of the transformer, allowing a sparse spatial sampling. The resulting model is very interesting in terms of convergence and complexity compared to the original DETR. A Multi-scale deformable attention module is also proposed. it needs to add a scale function in the attention module equation. Experiments shows that it increases the AP detection rate on MSCOCO compared to FasterR-CNN and DETR. \nContributions are clearly state and validated. The complexity study is very interesting and shows the interest of deformable attention module.\nFigure 1 presents a general view of the model. Since the deformable attention module is the core of the contribution, it should be interesting to add a figure dedicated to this component. Combined with eq.2, it will give a better understanding of the method. \nIt seems that the Axial-DeepLab paper presented in ECCV-2020 misses in the references. This paper proposes a simple strategy for attention modules that also reduce complexity. \nResults clearly show that deformable DETR provides better AP than DETR for less training-epochs. Moreover, the convergence is better than for Faster R-CNN (FPN). \nOne of the concerns with deformable convolution is that the computation speed is slower than classical convolution. The same drawback appears with deformable attention modules. Fps decrease from 26 to 19 compared to DETR. It should be interesting to also report fps in the state of the art comparison table 3. ", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1041/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1041/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection", "authorids": ["~Xizhou_Zhu1", "~Weijie_Su2", "luotto@sensetime.com", "binli@ustc.edu.cn", "~Xiaogang_Wang2", "~Jifeng_Dai1"], "authors": ["Xizhou Zhu", "Weijie Su", "Lewei Lu", "Bin Li", "Xiaogang Wang", "Jifeng Dai"], "keywords": ["Efficient Attention Mechanism", "Deformation Modeling", "Multi-scale Representation", "End-to-End Object Detection"], "abstract": "DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10$\\times$ less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code is released at https://github.com/fundamentalvision/Deformable-DETR.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|deformable_detr_deformable_transformers_for_endtoend_object_detection", "one-sentence_summary": "Deformable DETR is an efficient and fast-converging end-to-end object detector. It mitigates the high complexity and slow convergence issues of DETR via a novel sampling-based efficient attention mechanism. ", "supplementary_material": "/attachment/75de09ec685bf33268166fcaad64aeb855ed2da2.zip", "pdf": "/pdf/758d4b5c0d63033d526ff8744d872a03543bb674.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhu2021deformable,\ntitle={Deformable {\\{}DETR{\\}}: Deformable Transformers for End-to-End Object Detection},\nauthor={Xizhou Zhu and Weijie Su and Lewei Lu and Bin Li and Xiaogang Wang and Jifeng Dai},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=gZ9hCDWe6ke}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "gZ9hCDWe6ke", "replyto": "gZ9hCDWe6ke", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1041/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538128520, "tmdate": 1606915776638, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1041/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1041/-/Official_Review"}}}, {"id": "fya7XUpEcg", "original": null, "number": 5, "cdate": 1604068150979, "ddate": null, "tcdate": 1604068150979, "tmdate": 1605024545028, "tddate": null, "forum": "gZ9hCDWe6ke", "replyto": "gZ9hCDWe6ke", "invitation": "ICLR.cc/2021/Conference/Paper1041/-/Official_Review", "content": {"title": "It solves the slow convergence problem in the recent successful DETR framework and obtains SOTA results", "review": "As a new framework for object detection, DETR is very important. However,  it suffers from slow convergence and limited feature spatial resolution. This paper proposes deformable attention, which attends to a small set of sampling locations rather than all the locations in the original DETR. Besides, the paper applies multi-scale deformable attention for better results.\n\nThe paper is well-written and obtains very impressive results. Traning for only 50 epochs, deformable Detr obtain results similar to DETR which is trained for 500 epochs. By implementing a two-stage detector based on deformable Detr, the paper obtain state-of-the-art object detection results with a very high AP (52.3) on AP.\n\n A few suggestions for improving the paper are given as follows.\n (1) The training and testing times could be reported in the paper, which is useful for other researchers to implement and use this method.\n (2) Some related methods on sparse connected self-attention/transformer [a,b,c] should be cited and discussed.\n\n [a] Representative Graph Neural Network, CVPR 2020\n [b] Dynamic Graph Message Passing Networks, CVPR 2020\n [c] CCNet: Criss-Cross Attention for Semantic Segmentation in ICCV 19 & TPAMI 2020\n", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1041/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1041/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection", "authorids": ["~Xizhou_Zhu1", "~Weijie_Su2", "luotto@sensetime.com", "binli@ustc.edu.cn", "~Xiaogang_Wang2", "~Jifeng_Dai1"], "authors": ["Xizhou Zhu", "Weijie Su", "Lewei Lu", "Bin Li", "Xiaogang Wang", "Jifeng Dai"], "keywords": ["Efficient Attention Mechanism", "Deformation Modeling", "Multi-scale Representation", "End-to-End Object Detection"], "abstract": "DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10$\\times$ less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code is released at https://github.com/fundamentalvision/Deformable-DETR.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|deformable_detr_deformable_transformers_for_endtoend_object_detection", "one-sentence_summary": "Deformable DETR is an efficient and fast-converging end-to-end object detector. It mitigates the high complexity and slow convergence issues of DETR via a novel sampling-based efficient attention mechanism. ", "supplementary_material": "/attachment/75de09ec685bf33268166fcaad64aeb855ed2da2.zip", "pdf": "/pdf/758d4b5c0d63033d526ff8744d872a03543bb674.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhu2021deformable,\ntitle={Deformable {\\{}DETR{\\}}: Deformable Transformers for End-to-End Object Detection},\nauthor={Xizhou Zhu and Weijie Su and Lewei Lu and Bin Li and Xiaogang Wang and Jifeng Dai},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=gZ9hCDWe6ke}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "gZ9hCDWe6ke", "replyto": "gZ9hCDWe6ke", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1041/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538128520, "tmdate": 1606915776638, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1041/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1041/-/Official_Review"}}}], "count": 14}