{"notes": [{"id": "r1xa9TVFvH", "original": "S1xWFVRPDr", "number": 724, "cdate": 1569439125025, "ddate": null, "tcdate": 1569439125025, "tmdate": 1577168212987, "tddate": null, "forum": "r1xa9TVFvH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["drzhou@cs.ucla.edu", "lihongli.cs@gmail.com", "qgu@cs.ucla.edu"], "title": "NeuralUCB: Contextual Bandits with Neural Network-Based Exploration", "authors": ["Dongruo Zhou", "Lihong Li", "Quanquan Gu"], "pdf": "/pdf/33a0927d6e52ad2cacb15acfe6503121d2daf589.pdf", "abstract": "We study the stochastic contextual bandit problem, where the reward is generated from an unknown bounded function with additive noise. We propose the NeuralUCB algorithm, which leverages the representation power of deep neural networks and uses the neural network-based random feature mapping to construct an upper confidence bound (UCB) of reward for efficient exploration. We prove that, under mild assumptions, NeuralUCB achieves $\\tilde O(\\sqrt{T})$ regret bound, where $T$ is the number of rounds. To the best of our knowledge, our algorithm is the first neural network-based contextual bandit algorithm with near-optimal regret guarantee.  Preliminary experiment results on synthetic data corroborate our theory, and shed light on potential applications of our algorithm to real-world problems.", "keywords": ["contextual bandits", "neural network", "upper confidence bound"], "paperhash": "zhou|neuralucb_contextual_bandits_with_neural_networkbased_exploration", "original_pdf": "/attachment/03444ebd6e167ae858b16860a9c5e45474c2f215.pdf", "_bibtex": "@misc{\nzhou2020neuralucb,\ntitle={Neural{\\{}UCB{\\}}: Contextual Bandits with Neural Network-Based Exploration},\nauthor={Dongruo Zhou and Lihong Li and Quanquan Gu},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xa9TVFvH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 16, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "pewV34cNX", "original": null, "number": 1, "cdate": 1576798704299, "ddate": null, "tcdate": 1576798704299, "tmdate": 1576800931752, "tddate": null, "forum": "r1xa9TVFvH", "replyto": "r1xa9TVFvH", "invitation": "ICLR.cc/2020/Conference/Paper724/-/Decision", "content": {"decision": "Reject", "comment": "As the reviewers have pointed out and the authors have confirmed, the original version of this paper was not a significant leap beyond combining recent understanding of Neural Tangent Kernels and previous techniques for kernelized bandits. In a revision, the authors updated their draft to allow the point at which gradients are centered around, theta_0, to now equal theta_t. This seems like a more reasonable algorithm and it is satisfying that the authors were able to maintain their regret bound for this dynamic setting. However, the revision is substantial and it seems unreasonable to expect reviewers to read the revised results in detail--the reviewers also felt it may be unfair to other ICLR submissions. All reviewers believe the paper has introduced valuable contributions to the area but should go under a full review process at a future venue. A reviewer would also like to see a comparison to Kernel UCB run on the true NTK (or a good approximation thereof). ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["drzhou@cs.ucla.edu", "lihongli.cs@gmail.com", "qgu@cs.ucla.edu"], "title": "NeuralUCB: Contextual Bandits with Neural Network-Based Exploration", "authors": ["Dongruo Zhou", "Lihong Li", "Quanquan Gu"], "pdf": "/pdf/33a0927d6e52ad2cacb15acfe6503121d2daf589.pdf", "abstract": "We study the stochastic contextual bandit problem, where the reward is generated from an unknown bounded function with additive noise. We propose the NeuralUCB algorithm, which leverages the representation power of deep neural networks and uses the neural network-based random feature mapping to construct an upper confidence bound (UCB) of reward for efficient exploration. We prove that, under mild assumptions, NeuralUCB achieves $\\tilde O(\\sqrt{T})$ regret bound, where $T$ is the number of rounds. To the best of our knowledge, our algorithm is the first neural network-based contextual bandit algorithm with near-optimal regret guarantee.  Preliminary experiment results on synthetic data corroborate our theory, and shed light on potential applications of our algorithm to real-world problems.", "keywords": ["contextual bandits", "neural network", "upper confidence bound"], "paperhash": "zhou|neuralucb_contextual_bandits_with_neural_networkbased_exploration", "original_pdf": "/attachment/03444ebd6e167ae858b16860a9c5e45474c2f215.pdf", "_bibtex": "@misc{\nzhou2020neuralucb,\ntitle={Neural{\\{}UCB{\\}}: Contextual Bandits with Neural Network-Based Exploration},\nauthor={Dongruo Zhou and Lihong Li and Quanquan Gu},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xa9TVFvH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "r1xa9TVFvH", "replyto": "r1xa9TVFvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795713032, "tmdate": 1576800262551, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper724/-/Decision"}}}, {"id": "SkgraylCYr", "original": null, "number": 2, "cdate": 1571844029249, "ddate": null, "tcdate": 1571844029249, "tmdate": 1574312914296, "tddate": null, "forum": "r1xa9TVFvH", "replyto": "r1xa9TVFvH", "invitation": "ICLR.cc/2020/Conference/Paper724/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #1", "review": "This paper proposes to use the Neural Tangent Kernel (NTK) with the Upper Confidence Bound for stochastic contextual bandits. \n- The paper instantiates Kernel UCB (Valko, 2013) with the NTK and the novelty is limited from a theoretical point of view. \n- There is no experimental comparison with Neural Linear or Kernel UCB using a fixed kernel, (for example, the RBF kernel) or to methods like Thompson sampling that work well in practice even with non-linearities. \n\nDetailed review below:\n- Section 2.2 is not relevant to the paper and it might be more useful to use this space to explain NTK better. \n- Please explain NTK before instantiating the algorithm in Section 4. \n- The \"Efficient Implementation\" section in Section 4 is standard and done in all the linear bandit papers. Please acknowledge this or say how it is different. \n- The NTK description in Definition 5.1 needs to be clarified. At the moment, it is difficult to parse. Please give some intuition about it. \n- For the regret analysis, could you explain how the analysis is different from that of a fixed kernel in Valko, 2013. \n- What is the intuition for having a lower bound on \"S\", the norm parameter? Why is there no upper bound? \n- The width of the neural network depends on T^4. How does this affect the effective dimension \\tilde{d} in the worst case? Can it result in linear regret?\n- For Lemma 6.2, 6.3, please say that these are directly borrowed from Valko, 2013 and Abbasi, 2011. \n- From an experimental perspective, the width of the neural network is a constant wrt to T, K and L and clearly doesn't align with the theoretical bounds. Please justify why this is a valid thing to do? \n- As mentioned earlier, there is no comparison with Kernel UCB with a fixed kernel, Neural Linear or Thompson sampling, methods that work well in practice. \n- Finally, real-world experiments are necessary to show the benefit of using NTK in practice. ", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper724/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper724/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["drzhou@cs.ucla.edu", "lihongli.cs@gmail.com", "qgu@cs.ucla.edu"], "title": "NeuralUCB: Contextual Bandits with Neural Network-Based Exploration", "authors": ["Dongruo Zhou", "Lihong Li", "Quanquan Gu"], "pdf": "/pdf/33a0927d6e52ad2cacb15acfe6503121d2daf589.pdf", "abstract": "We study the stochastic contextual bandit problem, where the reward is generated from an unknown bounded function with additive noise. We propose the NeuralUCB algorithm, which leverages the representation power of deep neural networks and uses the neural network-based random feature mapping to construct an upper confidence bound (UCB) of reward for efficient exploration. We prove that, under mild assumptions, NeuralUCB achieves $\\tilde O(\\sqrt{T})$ regret bound, where $T$ is the number of rounds. To the best of our knowledge, our algorithm is the first neural network-based contextual bandit algorithm with near-optimal regret guarantee.  Preliminary experiment results on synthetic data corroborate our theory, and shed light on potential applications of our algorithm to real-world problems.", "keywords": ["contextual bandits", "neural network", "upper confidence bound"], "paperhash": "zhou|neuralucb_contextual_bandits_with_neural_networkbased_exploration", "original_pdf": "/attachment/03444ebd6e167ae858b16860a9c5e45474c2f215.pdf", "_bibtex": "@misc{\nzhou2020neuralucb,\ntitle={Neural{\\{}UCB{\\}}: Contextual Bandits with Neural Network-Based Exploration},\nauthor={Dongruo Zhou and Lihong Li and Quanquan Gu},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xa9TVFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1xa9TVFvH", "replyto": "r1xa9TVFvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper724/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper724/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575666836952, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper724/Reviewers"], "noninvitees": [], "tcdate": 1570237748010, "tmdate": 1575666836966, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper724/-/Official_Review"}}}, {"id": "S1lfDtjXqr", "original": null, "number": 3, "cdate": 1572219225818, "ddate": null, "tcdate": 1572219225818, "tmdate": 1574117822561, "tddate": null, "forum": "r1xa9TVFvH", "replyto": "r1xa9TVFvH", "invitation": "ICLR.cc/2020/Conference/Paper724/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "This paper proposes Neural UCB for the neural-linear bandit setting. The main contribution of the paper is the theorem that the proposed method, Neural UCB, is guarantee to achieved a good regret bound, which for the first time extends bandits result to neural networks. Overall the paper is well written and easy to follow. \n\nWhile the result of this paper seems to be interesting, the idea of the paper is simply combining a recent progress on the neural tangent kernel for overparametrized neural networks and a standard linear UCB algorithm. \n\nThe main concern I have is about the constant S in the regret bound. Note that this constant is an upper bound of \\sqrt{h^T H h}, where h is in the dimension of TK and H is in the dimension of TK by TK. A naive bound for S could be sup-linear in T, which makes the bound vacuous. What would be a lower bound for \\lambda_0 for eg. the setting in the experiments?\n\nOther comment:\n1. It should be explicitly stated somewhere in the paper that x_{t,k} are assumed to be deterministic. Thus \\theta^* is deterministic. It is more important that \\theta^* does NOT depends on a_t. Otherwise lemma 6.2 could be problematic. \n\n=====================\nBased on the new version of the paper and the discussions, I change the score to weak accept.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper724/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper724/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["drzhou@cs.ucla.edu", "lihongli.cs@gmail.com", "qgu@cs.ucla.edu"], "title": "NeuralUCB: Contextual Bandits with Neural Network-Based Exploration", "authors": ["Dongruo Zhou", "Lihong Li", "Quanquan Gu"], "pdf": "/pdf/33a0927d6e52ad2cacb15acfe6503121d2daf589.pdf", "abstract": "We study the stochastic contextual bandit problem, where the reward is generated from an unknown bounded function with additive noise. We propose the NeuralUCB algorithm, which leverages the representation power of deep neural networks and uses the neural network-based random feature mapping to construct an upper confidence bound (UCB) of reward for efficient exploration. We prove that, under mild assumptions, NeuralUCB achieves $\\tilde O(\\sqrt{T})$ regret bound, where $T$ is the number of rounds. To the best of our knowledge, our algorithm is the first neural network-based contextual bandit algorithm with near-optimal regret guarantee.  Preliminary experiment results on synthetic data corroborate our theory, and shed light on potential applications of our algorithm to real-world problems.", "keywords": ["contextual bandits", "neural network", "upper confidence bound"], "paperhash": "zhou|neuralucb_contextual_bandits_with_neural_networkbased_exploration", "original_pdf": "/attachment/03444ebd6e167ae858b16860a9c5e45474c2f215.pdf", "_bibtex": "@misc{\nzhou2020neuralucb,\ntitle={Neural{\\{}UCB{\\}}: Contextual Bandits with Neural Network-Based Exploration},\nauthor={Dongruo Zhou and Lihong Li and Quanquan Gu},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xa9TVFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1xa9TVFvH", "replyto": "r1xa9TVFvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper724/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper724/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575666836952, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper724/Reviewers"], "noninvitees": [], "tcdate": 1570237748010, "tmdate": 1575666836966, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper724/-/Official_Review"}}}, {"id": "BJxKcgi2sr", "original": null, "number": 15, "cdate": 1573855377195, "ddate": null, "tcdate": 1573855377195, "tmdate": 1573855412922, "tddate": null, "forum": "r1xa9TVFvH", "replyto": "HkgkIJj2sB", "invitation": "ICLR.cc/2020/Conference/Paper724/-/Official_Comment", "content": {"title": "Thank you", "comment": "1) Is NTK a universal kernel?\n\nThis very recent paper (https://arxiv.org/abs/1910.06956 ) establishes rates of universal approximation for the shallow neural tangent kernel (NTK). "}, "signatures": ["ICLR.cc/2020/Conference/Paper724/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper724/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["drzhou@cs.ucla.edu", "lihongli.cs@gmail.com", "qgu@cs.ucla.edu"], "title": "NeuralUCB: Contextual Bandits with Neural Network-Based Exploration", "authors": ["Dongruo Zhou", "Lihong Li", "Quanquan Gu"], "pdf": "/pdf/33a0927d6e52ad2cacb15acfe6503121d2daf589.pdf", "abstract": "We study the stochastic contextual bandit problem, where the reward is generated from an unknown bounded function with additive noise. We propose the NeuralUCB algorithm, which leverages the representation power of deep neural networks and uses the neural network-based random feature mapping to construct an upper confidence bound (UCB) of reward for efficient exploration. We prove that, under mild assumptions, NeuralUCB achieves $\\tilde O(\\sqrt{T})$ regret bound, where $T$ is the number of rounds. To the best of our knowledge, our algorithm is the first neural network-based contextual bandit algorithm with near-optimal regret guarantee.  Preliminary experiment results on synthetic data corroborate our theory, and shed light on potential applications of our algorithm to real-world problems.", "keywords": ["contextual bandits", "neural network", "upper confidence bound"], "paperhash": "zhou|neuralucb_contextual_bandits_with_neural_networkbased_exploration", "original_pdf": "/attachment/03444ebd6e167ae858b16860a9c5e45474c2f215.pdf", "_bibtex": "@misc{\nzhou2020neuralucb,\ntitle={Neural{\\{}UCB{\\}}: Contextual Bandits with Neural Network-Based Exploration},\nauthor={Dongruo Zhou and Lihong Li and Quanquan Gu},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xa9TVFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1xa9TVFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper724/Authors", "ICLR.cc/2020/Conference/Paper724/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper724/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper724/Reviewers", "ICLR.cc/2020/Conference/Paper724/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper724/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper724/Authors|ICLR.cc/2020/Conference/Paper724/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167180, "tmdate": 1576860561924, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper724/Authors", "ICLR.cc/2020/Conference/Paper724/Reviewers", "ICLR.cc/2020/Conference/Paper724/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper724/-/Official_Comment"}}}, {"id": "HkgkIJj2sB", "original": null, "number": 14, "cdate": 1573855046622, "ddate": null, "tcdate": 1573855046622, "tmdate": 1573855139733, "tddate": null, "forum": "r1xa9TVFvH", "replyto": "H1l9psc2oH", "invitation": "ICLR.cc/2020/Conference/Paper724/-/Official_Comment", "content": {"title": "Thanks for the quick response", "comment": "1) Is NTK a universal kernel? What is its approximation power? But given the evidence you provided in your response, I am not very concerned about this question now.\n\n2) I do think the analysis in Abbasi-Yadkori et al., 2011 requires \\theta^* to be a constant not depending on x_{t,a}. I will double check with the paper of Abbasi-Yadkori et al., 2011. I understand the deadline for rebuttal is approaching. I will adapt my rating accordingly after I check the correctness of the response."}, "signatures": ["ICLR.cc/2020/Conference/Paper724/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper724/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["drzhou@cs.ucla.edu", "lihongli.cs@gmail.com", "qgu@cs.ucla.edu"], "title": "NeuralUCB: Contextual Bandits with Neural Network-Based Exploration", "authors": ["Dongruo Zhou", "Lihong Li", "Quanquan Gu"], "pdf": "/pdf/33a0927d6e52ad2cacb15acfe6503121d2daf589.pdf", "abstract": "We study the stochastic contextual bandit problem, where the reward is generated from an unknown bounded function with additive noise. We propose the NeuralUCB algorithm, which leverages the representation power of deep neural networks and uses the neural network-based random feature mapping to construct an upper confidence bound (UCB) of reward for efficient exploration. We prove that, under mild assumptions, NeuralUCB achieves $\\tilde O(\\sqrt{T})$ regret bound, where $T$ is the number of rounds. To the best of our knowledge, our algorithm is the first neural network-based contextual bandit algorithm with near-optimal regret guarantee.  Preliminary experiment results on synthetic data corroborate our theory, and shed light on potential applications of our algorithm to real-world problems.", "keywords": ["contextual bandits", "neural network", "upper confidence bound"], "paperhash": "zhou|neuralucb_contextual_bandits_with_neural_networkbased_exploration", "original_pdf": "/attachment/03444ebd6e167ae858b16860a9c5e45474c2f215.pdf", "_bibtex": "@misc{\nzhou2020neuralucb,\ntitle={Neural{\\{}UCB{\\}}: Contextual Bandits with Neural Network-Based Exploration},\nauthor={Dongruo Zhou and Lihong Li and Quanquan Gu},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xa9TVFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1xa9TVFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper724/Authors", "ICLR.cc/2020/Conference/Paper724/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper724/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper724/Reviewers", "ICLR.cc/2020/Conference/Paper724/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper724/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper724/Authors|ICLR.cc/2020/Conference/Paper724/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167180, "tmdate": 1576860561924, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper724/Authors", "ICLR.cc/2020/Conference/Paper724/Reviewers", "ICLR.cc/2020/Conference/Paper724/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper724/-/Official_Comment"}}}, {"id": "H1l9psc2oH", "original": null, "number": 13, "cdate": 1573854146436, "ddate": null, "tcdate": 1573854146436, "tmdate": 1573854146436, "tddate": null, "forum": "r1xa9TVFvH", "replyto": "HkgY5MthoS", "invitation": "ICLR.cc/2020/Conference/Paper724/-/Official_Comment", "content": {"title": "To Reviewer 2\u2019s new comments", "comment": "1): \u201cI am not familiar with the literature of NTK but I think it needs further justification that this setting still has enough modelling capability.\u201d\n\nThe function class induced by NTK has a strong representative power. For instance, recent work [1,2,3] have shown that a variety of NTKs are able to achieve comparable results as the corresponding neural networks on different tasks on CIFAR10, UCI database, VOC07, and achieve better results than Gaussian kernel methods. We will add this justification in the final version of our paper.\n\n\n2): \u201cx_{t,a} being deterministic\u201d\n\nWe have carefully checked the entire proof in Abbasi-Yadkori et al., 2011, and confirmed that their proof does not require $\\theta^*$ to be independent of $x_{t,a}$. They only require $\\|\\theta^*\\|_2 \\leq S$. Therefore, we can indeed use the corresponding result in Abbasi-Yadkori et al., 2011 in our proof, and our analysis is correct even if $x_{t,a}$\u2019s are not deterministic.\n\n[1] Arora, S., Du, S. S., Hu, W., Li, Z., Salakhutdinov, R., & Wang, R. (2019). On exact computation with an infinitely wide neural net. arXiv preprint arXiv:1904.11955.\n[2] Arora, S., Du, S. S., Li, Z., Salakhutdinov, R., Wang, R., & Yu, D. (2019). Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks. arXiv preprint arXiv:1910.01663.\n[3] Li, Z., Wang, R., Yu, D., Du, S. S., Hu, W., Salakhutdinov, R., & Arora, S. (2019). Enhanced Convolutional Neural Tangent Kernels. arXiv preprint arXiv:1911.00809.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper724/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper724/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["drzhou@cs.ucla.edu", "lihongli.cs@gmail.com", "qgu@cs.ucla.edu"], "title": "NeuralUCB: Contextual Bandits with Neural Network-Based Exploration", "authors": ["Dongruo Zhou", "Lihong Li", "Quanquan Gu"], "pdf": "/pdf/33a0927d6e52ad2cacb15acfe6503121d2daf589.pdf", "abstract": "We study the stochastic contextual bandit problem, where the reward is generated from an unknown bounded function with additive noise. We propose the NeuralUCB algorithm, which leverages the representation power of deep neural networks and uses the neural network-based random feature mapping to construct an upper confidence bound (UCB) of reward for efficient exploration. We prove that, under mild assumptions, NeuralUCB achieves $\\tilde O(\\sqrt{T})$ regret bound, where $T$ is the number of rounds. To the best of our knowledge, our algorithm is the first neural network-based contextual bandit algorithm with near-optimal regret guarantee.  Preliminary experiment results on synthetic data corroborate our theory, and shed light on potential applications of our algorithm to real-world problems.", "keywords": ["contextual bandits", "neural network", "upper confidence bound"], "paperhash": "zhou|neuralucb_contextual_bandits_with_neural_networkbased_exploration", "original_pdf": "/attachment/03444ebd6e167ae858b16860a9c5e45474c2f215.pdf", "_bibtex": "@misc{\nzhou2020neuralucb,\ntitle={Neural{\\{}UCB{\\}}: Contextual Bandits with Neural Network-Based Exploration},\nauthor={Dongruo Zhou and Lihong Li and Quanquan Gu},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xa9TVFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1xa9TVFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper724/Authors", "ICLR.cc/2020/Conference/Paper724/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper724/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper724/Reviewers", "ICLR.cc/2020/Conference/Paper724/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper724/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper724/Authors|ICLR.cc/2020/Conference/Paper724/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167180, "tmdate": 1576860561924, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper724/Authors", "ICLR.cc/2020/Conference/Paper724/Reviewers", "ICLR.cc/2020/Conference/Paper724/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper724/-/Official_Comment"}}}, {"id": "HkgY5MthoS", "original": null, "number": 12, "cdate": 1573847697216, "ddate": null, "tcdate": 1573847697216, "tmdate": 1573847697216, "tddate": null, "forum": "r1xa9TVFvH", "replyto": "HJgdUk6Msr", "invitation": "ICLR.cc/2020/Conference/Paper724/-/Official_Comment", "content": {"title": "After Rebuttal", "comment": "Thanks for the response. \n\n1) I am not sure how restrictive this assumption would be that h is a function in the unit ball (let's say constant RKHS norm) in the RKHS space of the NTK. How representative this space is, given NTK. I am not familiar with the literature of NTK but I think it needs further justification that this setting still has enough modelling capability.\n\n2) The reason of saying $x_{t,a}$ being deterministic: to apply the result of Abbasi-Yadkori et al., 2011, the setting in this paper has to be the same with that in the referred paper. Note that \\theta^* depends on $x_{t,a}$ which is not the case in Abbasi-Yadkori et al., 2011. Here it needs more careful checking if the result in  Abbasi-Yadkori et al., 2011 is applicable or not."}, "signatures": ["ICLR.cc/2020/Conference/Paper724/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper724/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["drzhou@cs.ucla.edu", "lihongli.cs@gmail.com", "qgu@cs.ucla.edu"], "title": "NeuralUCB: Contextual Bandits with Neural Network-Based Exploration", "authors": ["Dongruo Zhou", "Lihong Li", "Quanquan Gu"], "pdf": "/pdf/33a0927d6e52ad2cacb15acfe6503121d2daf589.pdf", "abstract": "We study the stochastic contextual bandit problem, where the reward is generated from an unknown bounded function with additive noise. We propose the NeuralUCB algorithm, which leverages the representation power of deep neural networks and uses the neural network-based random feature mapping to construct an upper confidence bound (UCB) of reward for efficient exploration. We prove that, under mild assumptions, NeuralUCB achieves $\\tilde O(\\sqrt{T})$ regret bound, where $T$ is the number of rounds. To the best of our knowledge, our algorithm is the first neural network-based contextual bandit algorithm with near-optimal regret guarantee.  Preliminary experiment results on synthetic data corroborate our theory, and shed light on potential applications of our algorithm to real-world problems.", "keywords": ["contextual bandits", "neural network", "upper confidence bound"], "paperhash": "zhou|neuralucb_contextual_bandits_with_neural_networkbased_exploration", "original_pdf": "/attachment/03444ebd6e167ae858b16860a9c5e45474c2f215.pdf", "_bibtex": "@misc{\nzhou2020neuralucb,\ntitle={Neural{\\{}UCB{\\}}: Contextual Bandits with Neural Network-Based Exploration},\nauthor={Dongruo Zhou and Lihong Li and Quanquan Gu},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xa9TVFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1xa9TVFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper724/Authors", "ICLR.cc/2020/Conference/Paper724/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper724/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper724/Reviewers", "ICLR.cc/2020/Conference/Paper724/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper724/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper724/Authors|ICLR.cc/2020/Conference/Paper724/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167180, "tmdate": 1576860561924, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper724/Authors", "ICLR.cc/2020/Conference/Paper724/Reviewers", "ICLR.cc/2020/Conference/Paper724/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper724/-/Official_Comment"}}}, {"id": "rJeGYZEDjH", "original": null, "number": 6, "cdate": 1573499258027, "ddate": null, "tcdate": 1573499258027, "tmdate": 1573795475655, "tddate": null, "forum": "r1xa9TVFvH", "replyto": "B1lvbPHXsB", "invitation": "ICLR.cc/2020/Conference/Paper724/-/Official_Comment", "content": {"title": "To Reviewer 3\u2019s new comment: ", "comment": "Q1: The RKHS norm of $h$ in Remark 5.8\nA1: We believe this is a misunderstanding about the definition of RKHS norm of $h$. Let $K$ be the NTK kernel function, then by the definition of $\\mathbf{H}$, we have $\\mathbf{H}_{i,j} = K(x^i, x^j)$. First, let us suppose the function $h$  can be represented by $h(x) = \\sum_{i=1}^{TK} \\alpha_i K(x, x^i)$, where $\\{\\alpha_i\\}$\u2019s are the coefficients. Then, according to Theorem 22 (page 118) in https://web.stanford.edu/class/cs229t/notes.pdf, the RKHS norm of $h$ is $\\|h\\|_{\\mathcal{H}}=\\sqrt{\\sum_{i,j=1}^{TK} \\alpha_i \\alpha_j K(x^i, x^j)}= \\sqrt{\\mathbf{\\alpha}^\\top\\mathbf{H}\\mathbf{\\alpha}}$, where $\\mathbf{\\alpha} = (\\alpha_i,\\dots,\\alpha_{TK})^\\top$. On the other hand, we can show that $\\mathbf{\\alpha} = \\mathbf{H}^{-1}\\mathbf{h}$ by substituting $x = x^i$ into $h(x) = \\sum_{i=1}^{TK} \\alpha_i K(x, x^i)$ and solving the system of linear equations. Then the RKHS norm of $h$ can be rewritten as $\\|h\\|_{\\mathcal{H}} = \\sqrt{\\mathbf{\\alpha}^\\top\\mathbf{H}\\mathbf{\\alpha}} = \\sqrt{\\mathbf{h}^\\top\\mathbf{H}^{-1}\\mathbf{h}}$.  In the more general case where $h$ may not be represented by a weighted sum of kernel functions over a finite number of data points, we show in Appendix A.3 that the RKHS norm of $h$ can be lower bounded by $\\sqrt{\\mathbf{h}^\\top \\mathbf{H}^{-1}\\mathbf{h}}$.\n\nQ2: I don\u2019t think the core idea is far more different from the original version\u2026 lemmas can be found in [1] and [2].\nA2: Our proof structure is indeed similar to regret analyses of many UCB-based bandit algorithms, where a common key step is to find a proper confidence region/interval that vanishes fast enough.  That said, the proof is not trivial, and at various places requires different proof techniques.  For example,\n* We present a novel instantaneous regret bound in Lemm 6.3, which contains the function approximation error between the neural network function $f(x_{t,a}; \\theta_{t-1})$ and its first-order approximation $f(x_{t,a}; \\theta_{0}) + g(x_{t,a}; \\theta_{t-1})^\\top(\\theta_{t-1} - \\theta_0)$ as what we have shown in (B.10) in the proof of Lemma 6.3.  This bound is different from those in previous results such as Eq. (7) of Abbasi et al., (2011) and Lemma 1 of Valko et al., (2013).\n* We show in Lemma B.2 that applying gradient descent on the objective function $L$ at round $t$, the output parameter of TrainNN $\\theta_t$ belongs to a neighborhood of the initial parameter $\\theta_0$. While the lemma looks similar to previous work [1,2], its proof is quite different. In specific, our objective function $L$ has an extra term of $\\ell_2$ regularization, thus its global minimum value is not zero, even the neural network is overparameterized. So the proof techniques in previous work [1,2] which highly rely on the zero global minimum are not directly applicable to our setting. We need to carry out a new analysis to show that GD still enjoys a linear rate of convergence for optimizing the regularized loss function $L$ to prove the statement of Lemma B.2.\n\nQ3: Can you address why NeuralUCB outperforms NeuralUCB_0 in the experiments in theory?\nA3: We believe the performance gap between NeuralUCB and NeuralUCB$_0$ is analogous to the gap between neural networks and the corresponding NTK in supervised learning, which has been observed and studied both in theory [3,4] and in practice [5].  However, even in the simpler case of supervised learning, a thorough understanding of this phenomenon remains an open problem, which is beyond the scope of this paper and will be an interesting topic for future work.\n\nQ4: Some typos.\nA4: Thanks for pointing them out. We have fixed these typos and polished other parts of the paper as well. \n\n[1] Yuan Cao, and Quanquan Gu. Generalization Bounds of Stochastic Gradient Descent for Wide and Deep Neural Networks. arXiv preprint arXiv:1905.13210 (2019).\n[2] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via overparameterization. In ICML, 2019.\n[3] Zeyuan Allen-Zhu and Yuanzhi Li. What Can ResNet Learn Efficiently, Going Beyond Kernels? ArXiv e-prints, abs/1905.10337, May 2019. \n[4] Colin Wei, Jason D Lee, Qiang Liu, and Tengyu Ma. Regularization Matters: Generalization and Optimization of Neural Nets v.s. their Induced Kernel. arXiv preprint arXiv:1810.05369v3, 2019. \n[5] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On exact computation with an infinitely wide neural net. arXiv preprint arXiv:1904.11955, 2019\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper724/Authors"], "readers": ["ICLR.cc/2020/Conference/Paper724/Authors", "ICLR.cc/2020/Conference/Paper724/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper724/Reviewers", "ICLR.cc/2020/Conference/Paper724/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper724/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["drzhou@cs.ucla.edu", "lihongli.cs@gmail.com", "qgu@cs.ucla.edu"], "title": "NeuralUCB: Contextual Bandits with Neural Network-Based Exploration", "authors": ["Dongruo Zhou", "Lihong Li", "Quanquan Gu"], "pdf": "/pdf/33a0927d6e52ad2cacb15acfe6503121d2daf589.pdf", "abstract": "We study the stochastic contextual bandit problem, where the reward is generated from an unknown bounded function with additive noise. We propose the NeuralUCB algorithm, which leverages the representation power of deep neural networks and uses the neural network-based random feature mapping to construct an upper confidence bound (UCB) of reward for efficient exploration. We prove that, under mild assumptions, NeuralUCB achieves $\\tilde O(\\sqrt{T})$ regret bound, where $T$ is the number of rounds. To the best of our knowledge, our algorithm is the first neural network-based contextual bandit algorithm with near-optimal regret guarantee.  Preliminary experiment results on synthetic data corroborate our theory, and shed light on potential applications of our algorithm to real-world problems.", "keywords": ["contextual bandits", "neural network", "upper confidence bound"], "paperhash": "zhou|neuralucb_contextual_bandits_with_neural_networkbased_exploration", "original_pdf": "/attachment/03444ebd6e167ae858b16860a9c5e45474c2f215.pdf", "_bibtex": "@misc{\nzhou2020neuralucb,\ntitle={Neural{\\{}UCB{\\}}: Contextual Bandits with Neural Network-Based Exploration},\nauthor={Dongruo Zhou and Lihong Li and Quanquan Gu},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xa9TVFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1xa9TVFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper724/Authors", "ICLR.cc/2020/Conference/Paper724/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper724/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper724/Reviewers", "ICLR.cc/2020/Conference/Paper724/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper724/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper724/Authors|ICLR.cc/2020/Conference/Paper724/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167180, "tmdate": 1576860561924, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper724/Authors", "ICLR.cc/2020/Conference/Paper724/Reviewers", "ICLR.cc/2020/Conference/Paper724/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper724/-/Official_Comment"}}}, {"id": "B1lvbPHXsB", "original": null, "number": 5, "cdate": 1573242622718, "ddate": null, "tcdate": 1573242622718, "tmdate": 1573758691697, "tddate": null, "forum": "r1xa9TVFvH", "replyto": "HJg_9gTMiB", "invitation": "ICLR.cc/2020/Conference/Paper724/-/Official_Comment", "content": {"title": "Thanks for your response. Below are some concerns after the main revision.", "comment": "Thanks for your response. However, I still find there are some issues on the current version.\n\n1. As $S\\geq \\sqrt{h H^{-1}h}$, it is still a issue of $S$. The authors can only argue that when $h$ projects to the spectral basis of $H$, there are no components on the eigenfunction corresponding to the small eigenvalue, which I think it\u2019s still too restrictive. I don\u2019t notice the mismatch of proof and the main text before, sorry for that. I also think this is a main drawback of the current theory on NTK, and it\u2019s unfair to omit $S$ in the regret bound.\n2. I don\u2019t think the core idea is far more different from the original version. Notice that m should be sufficient large and \\lambda is larger than 1, so the regularization term dominates the update of the neural network and the parameter would not be far away from the initialization. Thus we can use the similar strategy, with an additional step bound the difference between the update version and no-update version, (I think just as the authors do). The relevant theorems are all proposed in [1] and [2], so I still find the technique is not novel.\n3. Can you address why NeuralUCB outperforms NeuralUCB_0 in the experiments in theory? Currently I don\u2019t find it convincing, as if we over-parameterized the network, the neural tangent feature will not change more if the movement of the parameters is small.\n\nMinor comments:\n1. In theorem 5.5, the constant before the optimization error term should be C_2.\n2. The LHS of the conclusion in Lemma B.5 have some typos.\n3. Please polish the paper again to eliminate all of the possible typos like I mentioned before.\n\nI have raised my score to 3, as the authors exactly do some kinds of non-trivial thing that exactly (following the idea I mentioned before) to finish the whole proof. However, due to the reason I mentioned, I still tend to reject this paper. I don\u2019t have enough time to go through the new proof, so there can be some misunderstanding on it. If the authors can convince me that the proof strategy is totally different from the strategy I mentioned that explicitly shows the newly proposed methods is significantly different to the previous NTK-KernelUCB, and show that the issue of S can be solved properly, I will consider raise the score to 6.\n\n[1] Cao, Yuan, and Quanquan Gu. \"Generalization Bounds of Stochastic Gradient Descent for Wide and Deep Neural Networks.\" arXiv preprint arXiv:1905.13210 (2019).\n[2] Allen-Zhu, Zeyuan, Yuanzhi Li, and Zhao Song. \"A Convergence Theory for Deep Learning via Over-Parameterization.\" International Conference on Machine Learning. 2019."}, "signatures": ["ICLR.cc/2020/Conference/Paper724/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper724/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["drzhou@cs.ucla.edu", "lihongli.cs@gmail.com", "qgu@cs.ucla.edu"], "title": "NeuralUCB: Contextual Bandits with Neural Network-Based Exploration", "authors": ["Dongruo Zhou", "Lihong Li", "Quanquan Gu"], "pdf": "/pdf/33a0927d6e52ad2cacb15acfe6503121d2daf589.pdf", "abstract": "We study the stochastic contextual bandit problem, where the reward is generated from an unknown bounded function with additive noise. We propose the NeuralUCB algorithm, which leverages the representation power of deep neural networks and uses the neural network-based random feature mapping to construct an upper confidence bound (UCB) of reward for efficient exploration. We prove that, under mild assumptions, NeuralUCB achieves $\\tilde O(\\sqrt{T})$ regret bound, where $T$ is the number of rounds. To the best of our knowledge, our algorithm is the first neural network-based contextual bandit algorithm with near-optimal regret guarantee.  Preliminary experiment results on synthetic data corroborate our theory, and shed light on potential applications of our algorithm to real-world problems.", "keywords": ["contextual bandits", "neural network", "upper confidence bound"], "paperhash": "zhou|neuralucb_contextual_bandits_with_neural_networkbased_exploration", "original_pdf": "/attachment/03444ebd6e167ae858b16860a9c5e45474c2f215.pdf", "_bibtex": "@misc{\nzhou2020neuralucb,\ntitle={Neural{\\{}UCB{\\}}: Contextual Bandits with Neural Network-Based Exploration},\nauthor={Dongruo Zhou and Lihong Li and Quanquan Gu},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xa9TVFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1xa9TVFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper724/Authors", "ICLR.cc/2020/Conference/Paper724/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper724/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper724/Reviewers", "ICLR.cc/2020/Conference/Paper724/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper724/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper724/Authors|ICLR.cc/2020/Conference/Paper724/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167180, "tmdate": 1576860561924, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper724/Authors", "ICLR.cc/2020/Conference/Paper724/Reviewers", "ICLR.cc/2020/Conference/Paper724/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper724/-/Official_Comment"}}}, {"id": "rJeeYcLztH", "original": null, "number": 1, "cdate": 1571084920113, "ddate": null, "tcdate": 1571084920113, "tmdate": 1573758670289, "tddate": null, "forum": "r1xa9TVFvH", "replyto": "r1xa9TVFvH", "invitation": "ICLR.cc/2020/Conference/Paper724/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "The authors proposed a neural network based UCB algorithm for bounded reward contextual bandit problems with theoretical guarantee thanks to the recent development of Neural Tangent Kernel (NTK).\n\nThroughout the paper, the authors do not use much of the specific property of neural networks and NTK. They only use the gradient of neural networks as the feature and use the NTK as the kernel in the kernelized contextual bandits. This can be beneficial, for example, it can enriching the class of kernels. However, I feel the whole paper lacks novelty, and have some technical flaws.\n\nDetailed Comments:\n1. In Sec 3, the authors argued that kernelized contextual bandits suffers from the unknown RKHS problem and RKHS realizability problem. However, with universal kernel, RKHS is dense in L^2 space, thus can in principle approximate any function in L^2 space within any precision. So generally, this is not a problem. Moreover, bounded function does not necessarily contain the linear function, generalized linear function and bounded RKHS norm function. At least if we do not add some assumption on the input, linear function can be unbounded. On the other hand, the proposed methods also need p>TK to guarantee the realizability, and we can also design some kernel with feature map dimension larger than TK with some good property to guarantee realizability, so I think this claim is not fair.\n2. In Assumption 5.2, the authors assume that the norm of contexts is smaller than 1. However, as far as I know, most of the existing work assumed the context have norm 1, and can be only relaxed to the norm upper lower bounded by two positive constant c1 and c2 (see [1]). Otherwise, there can be some issue on the positive definiteness of the NTK. Can the authors carefully check this? Meanwhile, I think it is not suitable to directly assume the NTK is positive definite. It is better to follow and refer the readers to the existing work.\n3. It is better to introduce \\theta^* before Sec 6, like for example the parameter that can perfectly predict the mean reward.\n4. I am confusing on the proof of Lemma 6.1 in Page 12. When the authors calculate the norm of \\theta^* - \\theta_0, how to transform Q^\\top A^{-2} Q to G^\\top G? If we use the singular value decomposition, we only have that G^\\top G=QA^2 Q^\\top. If I understand correctly, here we should do an inverse, and we cannot simply get the desired results, as the minimum singular value of G can be small under current assumption. However, it is still possible to upper bound this distance to derive the remaining proof.\n5. In the first line of Equation (B.3), there is a typo that omits the \\phi(x)^\\top.\n6. How does the second inequality of (B.4) derives? I can understand that the authors may use the Cauchy-Schwartz inequality, but Frobenius norm cannot be directly upper bounded by spectral norm (though they are equivalent, but we need to add an additional constant like \\sqrt{TK}). If the authors use the spectral norm, then the second term should be nuclear norm, not Frobenius norm. Probably I do not understand it correctly and it is not the core issue, but I think it is better to clarify it.\n7. There is a typo in the fourth line of (B.4), it should be \\lambda / \\lambda_0,\n8. The last derivation of Appendix B.3 have several typos omitting det(\\lambda I).\n9. The authors should better include the kernelized contextual bandits for a fair comparison, as LinUCB and Neural \\epsilon-greedy both have theoretical issue that can be solved by kernel methods. I doubt that kernelized contextual bandits can solve these cases well.\n\nOverall, I feel that most of the proof can be derived similarly from [2][3]. And Lemma C.1 is also from [4]. The authors only verify some conditions that when use NTK as the kernel in kernelized contextual bandits to adjust the main result from [2][3]. Thus, I think the technique used in this paper is not novel as well.\n\nIn my opinion, the communities are interested in solving contextual bandits with ``''gradient based'' neural network methods that use the neural network to predict the rewards given some contexts as input. But just as the Equation (6.1) shows, the prediction is not based on the neural network, but with a linear model taken \\phi(x_i) as input. Also, throughout the paper, the authors never use the network output f. To this end, I feel this paper is over-claimed on ''neural''. On the other hand, I think it can be interesting to think about how kernel methods can benefit from NTK. Directly use the gradient as the feature map seems not an interesting and meaningful method, I think.\n\n[1] Cao, Yuan, and Quanquan Gu. \"A generalization theory of gradient descent for learning over-parameterized deep relu networks.\" arXiv preprint arXiv:1902.01384 (2019).\n[2] Michal Valko, Nathan Korda, R\u00e9mi Munos, Ilias Flaounas, and Nello Cristianini. 2013. Finite-time analysis of kernelised contextual bandits. In Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence (UAI'13), Ann Nicholson and Padhraic Smyth (Eds.). AUAI Press, Arlington, Virginia, United States, 654-663.\n[3] Abbasi-Yadkori, Yasin, D\u00e1vid P\u00e1l, and Csaba Szepesv\u00e1ri. \"Improved algorithms for linear stochastic bandits.\" Advances in Neural Information Processing Systems. 2011.\n[4] Arora, Sanjeev, et al. \"On exact computation with an infinitely wide neural net.\" arXiv preprint arXiv:1904.11955 (2019).", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper724/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper724/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["drzhou@cs.ucla.edu", "lihongli.cs@gmail.com", "qgu@cs.ucla.edu"], "title": "NeuralUCB: Contextual Bandits with Neural Network-Based Exploration", "authors": ["Dongruo Zhou", "Lihong Li", "Quanquan Gu"], "pdf": "/pdf/33a0927d6e52ad2cacb15acfe6503121d2daf589.pdf", "abstract": "We study the stochastic contextual bandit problem, where the reward is generated from an unknown bounded function with additive noise. We propose the NeuralUCB algorithm, which leverages the representation power of deep neural networks and uses the neural network-based random feature mapping to construct an upper confidence bound (UCB) of reward for efficient exploration. We prove that, under mild assumptions, NeuralUCB achieves $\\tilde O(\\sqrt{T})$ regret bound, where $T$ is the number of rounds. To the best of our knowledge, our algorithm is the first neural network-based contextual bandit algorithm with near-optimal regret guarantee.  Preliminary experiment results on synthetic data corroborate our theory, and shed light on potential applications of our algorithm to real-world problems.", "keywords": ["contextual bandits", "neural network", "upper confidence bound"], "paperhash": "zhou|neuralucb_contextual_bandits_with_neural_networkbased_exploration", "original_pdf": "/attachment/03444ebd6e167ae858b16860a9c5e45474c2f215.pdf", "_bibtex": "@misc{\nzhou2020neuralucb,\ntitle={Neural{\\{}UCB{\\}}: Contextual Bandits with Neural Network-Based Exploration},\nauthor={Dongruo Zhou and Lihong Li and Quanquan Gu},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xa9TVFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1xa9TVFvH", "replyto": "r1xa9TVFvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper724/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper724/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575666836952, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper724/Reviewers"], "noninvitees": [], "tcdate": 1570237748010, "tmdate": 1575666836966, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper724/-/Official_Review"}}}, {"id": "HJg_9gTMiB", "original": null, "number": 4, "cdate": 1573208207912, "ddate": null, "tcdate": 1573208207912, "tmdate": 1573499608581, "tddate": null, "forum": "r1xa9TVFvH", "replyto": "rJeeYcLztH", "invitation": "ICLR.cc/2020/Conference/Paper724/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "Thank you for your feedback. We address your questions as follows. We have also revised our paper accordingly, and highlighted those places in blue. \n\n\nQ1: The statement of RKHS realizability problem is not fair.\nA1: Thanks for pointing this out.  We have removed such claims in the revision. \n\nQ2: The norm of contexts is smaller than 1. \nA2: Thanks for pointing out this minor issue. We have changed our assumption to $\\|x_i\\| = 1$.\n\nQ3. It is better to introduce $\\theta^*$ before Sec 6, like for example the parameter that can perfectly predict the mean reward.\nA3: Thanks for the suggestion. $\\theta^*$ is introduced only for the sake of analysis. It does not appear in our main results in Section 5. We have added a comment on $\\theta^*$ right after Lemma 6.1 to explain its use.\n\nQ4: Confusing on the proof of Lemma 6.1\nA4: We have revised the proof of Lemma 6.1 and add more explanation in the proof. \n\nQ5: There exist some typos in the paper. \nA5: Thanks for pointing them out. We have corrected all the typos we found in the new version of this paper. \n\nQ6: How does the second inequality of (B.4) derives?\nA6: In our revision, the revised (B.4) is now (B.16). We have revised the inequality of (B.16), and add more explanation about its derivation. \n\nQ7: The authors should better include the kernelized contextual bandits\nA7: We will add the KernelUCB baseline in an upcoming revision. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper724/Authors"], "readers": ["ICLR.cc/2020/Conference/Paper724/Authors", "ICLR.cc/2020/Conference/Paper724/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper724/Reviewers", "ICLR.cc/2020/Conference/Paper724/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper724/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["drzhou@cs.ucla.edu", "lihongli.cs@gmail.com", "qgu@cs.ucla.edu"], "title": "NeuralUCB: Contextual Bandits with Neural Network-Based Exploration", "authors": ["Dongruo Zhou", "Lihong Li", "Quanquan Gu"], "pdf": "/pdf/33a0927d6e52ad2cacb15acfe6503121d2daf589.pdf", "abstract": "We study the stochastic contextual bandit problem, where the reward is generated from an unknown bounded function with additive noise. We propose the NeuralUCB algorithm, which leverages the representation power of deep neural networks and uses the neural network-based random feature mapping to construct an upper confidence bound (UCB) of reward for efficient exploration. We prove that, under mild assumptions, NeuralUCB achieves $\\tilde O(\\sqrt{T})$ regret bound, where $T$ is the number of rounds. To the best of our knowledge, our algorithm is the first neural network-based contextual bandit algorithm with near-optimal regret guarantee.  Preliminary experiment results on synthetic data corroborate our theory, and shed light on potential applications of our algorithm to real-world problems.", "keywords": ["contextual bandits", "neural network", "upper confidence bound"], "paperhash": "zhou|neuralucb_contextual_bandits_with_neural_networkbased_exploration", "original_pdf": "/attachment/03444ebd6e167ae858b16860a9c5e45474c2f215.pdf", "_bibtex": "@misc{\nzhou2020neuralucb,\ntitle={Neural{\\{}UCB{\\}}: Contextual Bandits with Neural Network-Based Exploration},\nauthor={Dongruo Zhou and Lihong Li and Quanquan Gu},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xa9TVFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1xa9TVFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper724/Authors", "ICLR.cc/2020/Conference/Paper724/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper724/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper724/Reviewers", "ICLR.cc/2020/Conference/Paper724/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper724/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper724/Authors|ICLR.cc/2020/Conference/Paper724/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167180, "tmdate": 1576860561924, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper724/Authors", "ICLR.cc/2020/Conference/Paper724/Reviewers", "ICLR.cc/2020/Conference/Paper724/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper724/-/Official_Comment"}}}, {"id": "rJg7MgTfor", "original": null, "number": 3, "cdate": 1573208075282, "ddate": null, "tcdate": 1573208075282, "tmdate": 1573499599995, "tddate": null, "forum": "r1xa9TVFvH", "replyto": "SkgraylCYr", "invitation": "ICLR.cc/2020/Conference/Paper724/-/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "Thank you for your constructive feedback. We address your comments and questions as follows. We have also revised our paper accordingly, and highlighted those places in blue. \n\nQ1: \"Section 2.2 is not relevant to the paper and it might be more useful to use this space to explain NTK better. \"\nA1: We have added more explanation about NTK in the revision.  Neural tangent kernel (NTK) is originally defined in Jacob et al. (2018) by the gradient of the output of a randomly initialized neural network. In recent development of deep learning theory, a deep neural network can be characterized by its NTK in certain regime. Thus, we present the definition of NTK in Section 5. \n\nQ2: \"The \"Efficient Implementation\" section in Section 4 is standard and done in all the linear bandit papers. Please acknowledge this or say how it is different. \"\nA2: Thanks for your suggestion. This is indeed a standard technique. We have acknowledged this in the revision.\n\nQ3: \"For the regret analysis, could you explain how the analysis is different from that of a fixed kernel in Valko, 2013. \"\nA3: We have modified the NeuralUCB algorithm in the revision, so that it better reflects how DNNs are used to solve contextual bandits.  A key difference from the old version is that it now uses the most recent network parameter vector $\\theta_t$, *not* the initial one $\\theta_0$, to construct the upper confidence bound.  This makes the analysis substantially more challenging, and different from previous bandit analysis.  Specifically,\n  * Our previous algorithm (now called NeuralUCB$_0$ in Appendix E) can be regarded as KernelUCB with Neural Tangent kernel. However, our new algorithm NeuralUCB directly uses deep neural networks to predict the underlying reward function $h(x)$, which is less similar to KernelUCB due to its approximation error between the neural networks and their corresponding NTK kernel. \n  * Valko et al. (2013) analyzed the regret bound of a meta algorithm SupKernelUCB to handle the independence of rewards $r_{t, a_t}$, while we analyze the regret bound directly on NeuralUCB.\n\n\nQ4: \"What is the intuition for having a lower bound on 'S', the norm parameter? Why is there no upper bound?\" \nA4: S is tuning parameter in our algorithm, which in our proof needs to be chosen such that $\\|\\theta^*-\\theta_0\\|_2\\leq S$. This is analogous to the condition $\\|\\theta^*\\|_2\\leq S$ in Theorem 2 of Abbasi-Yadkori et al. (2011). Our Lemma 6.1 suggests that $\\|\\theta^*-\\theta_0\\|_2 \\leq \\sqrt{h^T H^{-1}h}$. Therefore, in order to make $\\|\\theta^*-\\theta_0\\|_2 \\leq S$ hold, it suffices to choose $S \\geq \\sqrt{h^\\top H^{-1}h}$.\n\n\nQ5: \" The width of the neural network depends on T^4. How does this affect the effective dimension $\\tilde{d}$ in the worst case? Can it result in linear regret?\"\nA5: The effective dimension $\\tilde d$ is not related to the width of neural network due to definition 5.3, since it is only determined by the NTK matrix $H$. $\\tilde d$ can be regarded as a measure of how quickly the eigenvalues of $H$ decay, and it only depends on $T$ logarithmically in some specific cases (Valko et al., 2013). Thus, the use of effective dimension will not result in linear regret. \n\n\nQ6: \u201cFor Lemma 6.2, 6.3, please say that these are directly borrowed from Valko, 2013 and Abbasi, 2011.\u201d\nA6: In our revision, the new version of Lemmas 6.2 and 6.3 are not directly implied by the results in Valko et al. (2013) and Abbasi-Yadkori et al. (2011). We also cite Valko et al. (2013) and Abbasi-Yadkori et al. (2011) in the corresponding proofs of these two lemmas. \n\nQ7: \"From an experimental perspective, the width of the neural network is a constant wrt to T, K and L and clearly doesn't align with the theoretical bounds. Please justify why this is a valid thing to do?\" \nA7: While existing over-parameterized NN analyses give interesting insights on optimization and generalization, the bounds on the required network width $m$ are likely not tight. Therefore, in experiments we choose m to be relatively large (but not as large as theory requires).\n\nQ8: \"There is no experimental comparison with Neural Linear or Kernel UCB using a fixed kernel\"\nA8: We added the comparison between NeuralUCB and NeuralUCB$_0$ (which can be seen as Kernel UCB  with NTK kernel). The experiments suggest that NeuralUCB is better than NeuralUCB$_0$. We will add the KernelUCB with Gaussian kernel in an upcoming revision.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper724/Authors"], "readers": ["ICLR.cc/2020/Conference/Paper724/Authors", "ICLR.cc/2020/Conference/Paper724/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper724/Reviewers", "ICLR.cc/2020/Conference/Paper724/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper724/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["drzhou@cs.ucla.edu", "lihongli.cs@gmail.com", "qgu@cs.ucla.edu"], "title": "NeuralUCB: Contextual Bandits with Neural Network-Based Exploration", "authors": ["Dongruo Zhou", "Lihong Li", "Quanquan Gu"], "pdf": "/pdf/33a0927d6e52ad2cacb15acfe6503121d2daf589.pdf", "abstract": "We study the stochastic contextual bandit problem, where the reward is generated from an unknown bounded function with additive noise. We propose the NeuralUCB algorithm, which leverages the representation power of deep neural networks and uses the neural network-based random feature mapping to construct an upper confidence bound (UCB) of reward for efficient exploration. We prove that, under mild assumptions, NeuralUCB achieves $\\tilde O(\\sqrt{T})$ regret bound, where $T$ is the number of rounds. To the best of our knowledge, our algorithm is the first neural network-based contextual bandit algorithm with near-optimal regret guarantee.  Preliminary experiment results on synthetic data corroborate our theory, and shed light on potential applications of our algorithm to real-world problems.", "keywords": ["contextual bandits", "neural network", "upper confidence bound"], "paperhash": "zhou|neuralucb_contextual_bandits_with_neural_networkbased_exploration", "original_pdf": "/attachment/03444ebd6e167ae858b16860a9c5e45474c2f215.pdf", "_bibtex": "@misc{\nzhou2020neuralucb,\ntitle={Neural{\\{}UCB{\\}}: Contextual Bandits with Neural Network-Based Exploration},\nauthor={Dongruo Zhou and Lihong Li and Quanquan Gu},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xa9TVFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1xa9TVFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper724/Authors", "ICLR.cc/2020/Conference/Paper724/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper724/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper724/Reviewers", "ICLR.cc/2020/Conference/Paper724/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper724/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper724/Authors|ICLR.cc/2020/Conference/Paper724/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167180, "tmdate": 1576860561924, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper724/Authors", "ICLR.cc/2020/Conference/Paper724/Reviewers", "ICLR.cc/2020/Conference/Paper724/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper724/-/Official_Comment"}}}, {"id": "HJgdUk6Msr", "original": null, "number": 2, "cdate": 1573207888446, "ddate": null, "tcdate": 1573207888446, "tmdate": 1573499591131, "tddate": null, "forum": "r1xa9TVFvH", "replyto": "S1lfDtjXqr", "invitation": "ICLR.cc/2020/Conference/Paper724/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "Thank you for your constructive comments. We address your questions as follows. We have also revised our paper accordingly, and highlighted those places in blue. \n\n\nQ1: \"The main concern I have is about the constant S in the regret bound.\"   \nA1: It is correct that in the worst case our bound will not be sublinear and may be dependent on $\\lambda_0$. However, in remark 5.7 we have shown a specific case that when the reward function h belongs to the RKHS space induced by NTK with bounded norm $\\|h\\|$, then $\\sqrt{h^\\top H^{-1}h}$, the lower bound of S, is less than $\\|h\\|$, which is a constant independent of T and K. \n\nQ2: \"It should be explicitly stated somewhere in the paper that x_{t,k} are assumed to be deterministic. Thus \\theta^ is deterministic. It is more important that \\theta^ does NOT depends on a_t. Otherwise lemma 6.2 could be problematic.\" \nA2: We believe this is a misunderstanding. Our analysis does not require $\\{x_{t,a}\\}$ to be deterministic, which can be demonstrated as follows. In our revision, the revised Lemma 6.2 now became Lemma 6.1. Due to the proof of Lemma 6.1, it can be seen that $\\theta^* = \\theta_0 + PA^{-1}Q^\\top h/\\sqrt{m}$, where $PAQ^\\top$ is the SVD of $G$, $G = [g(x^1; \\theta_0)\\dots g(x^{TK}; \\theta_0)]$. By the definition of $\\theta^*$, it can be seen that: \n* $\\theta^*$ is not deterministic since $\\theta^*$ depends on $\\theta_0$. \n* $\\theta^*$ does not depend on $a_t$ because $G$ does not depend on $a_t$.\nTherefore, we do not require $\\{x_{t,a}\\}$ to be deterministic, as long as they are independent of $\\theta_0$. \n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper724/Authors"], "readers": ["ICLR.cc/2020/Conference/Paper724/Authors", "ICLR.cc/2020/Conference/Paper724/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper724/Reviewers", "ICLR.cc/2020/Conference/Paper724/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper724/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["drzhou@cs.ucla.edu", "lihongli.cs@gmail.com", "qgu@cs.ucla.edu"], "title": "NeuralUCB: Contextual Bandits with Neural Network-Based Exploration", "authors": ["Dongruo Zhou", "Lihong Li", "Quanquan Gu"], "pdf": "/pdf/33a0927d6e52ad2cacb15acfe6503121d2daf589.pdf", "abstract": "We study the stochastic contextual bandit problem, where the reward is generated from an unknown bounded function with additive noise. We propose the NeuralUCB algorithm, which leverages the representation power of deep neural networks and uses the neural network-based random feature mapping to construct an upper confidence bound (UCB) of reward for efficient exploration. We prove that, under mild assumptions, NeuralUCB achieves $\\tilde O(\\sqrt{T})$ regret bound, where $T$ is the number of rounds. To the best of our knowledge, our algorithm is the first neural network-based contextual bandit algorithm with near-optimal regret guarantee.  Preliminary experiment results on synthetic data corroborate our theory, and shed light on potential applications of our algorithm to real-world problems.", "keywords": ["contextual bandits", "neural network", "upper confidence bound"], "paperhash": "zhou|neuralucb_contextual_bandits_with_neural_networkbased_exploration", "original_pdf": "/attachment/03444ebd6e167ae858b16860a9c5e45474c2f215.pdf", "_bibtex": "@misc{\nzhou2020neuralucb,\ntitle={Neural{\\{}UCB{\\}}: Contextual Bandits with Neural Network-Based Exploration},\nauthor={Dongruo Zhou and Lihong Li and Quanquan Gu},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xa9TVFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1xa9TVFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper724/Authors", "ICLR.cc/2020/Conference/Paper724/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper724/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper724/Reviewers", "ICLR.cc/2020/Conference/Paper724/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper724/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper724/Authors|ICLR.cc/2020/Conference/Paper724/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167180, "tmdate": 1576860561924, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper724/Authors", "ICLR.cc/2020/Conference/Paper724/Reviewers", "ICLR.cc/2020/Conference/Paper724/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper724/-/Official_Comment"}}}, {"id": "r1eVXJpMjH", "original": null, "number": 1, "cdate": 1573207836510, "ddate": null, "tcdate": 1573207836510, "tmdate": 1573499579780, "tddate": null, "forum": "r1xa9TVFvH", "replyto": "r1xa9TVFvH", "invitation": "ICLR.cc/2020/Conference/Paper724/-/Official_Comment", "content": {"title": "[To all reviewers] Major change in revised version", "comment": "Thank you very much for your constructive comments. We have uploaded a revised version of our paper.\n\nIn the revision, we modified the NeuralUCB algorithm, while the original algorithm in the initial submission will be referred to as NeuralUCB$_0$.  The new algorithm better reflects how DNNs are used to solve contextual bandits in practice.\n\nThe main differences between NeuralUCB and NeuralUCB$_0$ are:\n*NeuralUCB uses gradient descent to train a deep neural network to learn the reward function $h(x)$ based on observed contexts and rewards.  In contrast, NeuralUCB$_0$ uses matrix inversions to obtain parameters in closed forms.\n*At each round, NeuralUCB uses the current DNN parameters ($\\theta_t$) to compute an upper confidence bound.  In contrast, NeuralUCB$_0$ computes the UCB using the initial parameters ($\\theta_0$).\n*We compared NeuralUCB with NeuralUCB$_0$ empirically in Section 7. NeuralUCB outperforms NeuralUCB$_0$ in all experiment settings, which suggests that the adaptive feature mapping brought by NeuralUCB is better than fixed feature mapping used by NeuralUCB$_0$.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper724/Authors"], "readers": ["ICLR.cc/2020/Conference/Paper724/Authors", "ICLR.cc/2020/Conference/Paper724/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper724/Reviewers", "ICLR.cc/2020/Conference/Paper724/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper724/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["drzhou@cs.ucla.edu", "lihongli.cs@gmail.com", "qgu@cs.ucla.edu"], "title": "NeuralUCB: Contextual Bandits with Neural Network-Based Exploration", "authors": ["Dongruo Zhou", "Lihong Li", "Quanquan Gu"], "pdf": "/pdf/33a0927d6e52ad2cacb15acfe6503121d2daf589.pdf", "abstract": "We study the stochastic contextual bandit problem, where the reward is generated from an unknown bounded function with additive noise. We propose the NeuralUCB algorithm, which leverages the representation power of deep neural networks and uses the neural network-based random feature mapping to construct an upper confidence bound (UCB) of reward for efficient exploration. We prove that, under mild assumptions, NeuralUCB achieves $\\tilde O(\\sqrt{T})$ regret bound, where $T$ is the number of rounds. To the best of our knowledge, our algorithm is the first neural network-based contextual bandit algorithm with near-optimal regret guarantee.  Preliminary experiment results on synthetic data corroborate our theory, and shed light on potential applications of our algorithm to real-world problems.", "keywords": ["contextual bandits", "neural network", "upper confidence bound"], "paperhash": "zhou|neuralucb_contextual_bandits_with_neural_networkbased_exploration", "original_pdf": "/attachment/03444ebd6e167ae858b16860a9c5e45474c2f215.pdf", "_bibtex": "@misc{\nzhou2020neuralucb,\ntitle={Neural{\\{}UCB{\\}}: Contextual Bandits with Neural Network-Based Exploration},\nauthor={Dongruo Zhou and Lihong Li and Quanquan Gu},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xa9TVFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1xa9TVFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper724/Authors", "ICLR.cc/2020/Conference/Paper724/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper724/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper724/Reviewers", "ICLR.cc/2020/Conference/Paper724/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper724/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper724/Authors|ICLR.cc/2020/Conference/Paper724/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167180, "tmdate": 1576860561924, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper724/Authors", "ICLR.cc/2020/Conference/Paper724/Reviewers", "ICLR.cc/2020/Conference/Paper724/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper724/-/Official_Comment"}}}, {"id": "BJgJMB2U_S", "original": null, "number": 2, "cdate": 1570321671400, "ddate": null, "tcdate": 1570321671400, "tmdate": 1570332900879, "tddate": null, "forum": "r1xa9TVFvH", "replyto": "r1xa9TVFvH", "invitation": "ICLR.cc/2020/Conference/Paper724/-/Public_Comment", "content": {"comment": "(I am adding to Sean Kanne's comment. I was attracted to the title but got disappointed by the paper.)\n\nDespite the very attractive title, this paper is simply restating a special case of kernelized contextual bandit, which is originally developed by Valko et al. (2013). In particular, this paper specializes the kernel of Valko et al. (2013) to be the neural tangent kernel, which is induced by the random feature corresponding to the initial weights. Given this fact, the whole paper can be summarized into a half-page corollary of Valko et al. (2013).\n\nInterestingly, the whole algorithm in this paper does not use SGD to train a neural network at all. Instead, it just uses the random feature corresponding to the initial weights to perform linear regression. I am not sure if it is proper to call it \"neural\", which is very misleading.\n\nValko et al. (2013), Finite-Time Analysis of Kernelised Contextual Bandits.", "title": "exactly random feature (neural tangent feature); proof almost identical to Valko et al. (2013)"}, "signatures": ["~Anna_Dudley1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Anna_Dudley1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["drzhou@cs.ucla.edu", "lihongli.cs@gmail.com", "qgu@cs.ucla.edu"], "title": "NeuralUCB: Contextual Bandits with Neural Network-Based Exploration", "authors": ["Dongruo Zhou", "Lihong Li", "Quanquan Gu"], "pdf": "/pdf/33a0927d6e52ad2cacb15acfe6503121d2daf589.pdf", "abstract": "We study the stochastic contextual bandit problem, where the reward is generated from an unknown bounded function with additive noise. We propose the NeuralUCB algorithm, which leverages the representation power of deep neural networks and uses the neural network-based random feature mapping to construct an upper confidence bound (UCB) of reward for efficient exploration. We prove that, under mild assumptions, NeuralUCB achieves $\\tilde O(\\sqrt{T})$ regret bound, where $T$ is the number of rounds. To the best of our knowledge, our algorithm is the first neural network-based contextual bandit algorithm with near-optimal regret guarantee.  Preliminary experiment results on synthetic data corroborate our theory, and shed light on potential applications of our algorithm to real-world problems.", "keywords": ["contextual bandits", "neural network", "upper confidence bound"], "paperhash": "zhou|neuralucb_contextual_bandits_with_neural_networkbased_exploration", "original_pdf": "/attachment/03444ebd6e167ae858b16860a9c5e45474c2f215.pdf", "_bibtex": "@misc{\nzhou2020neuralucb,\ntitle={Neural{\\{}UCB{\\}}: Contextual Bandits with Neural Network-Based Exploration},\nauthor={Dongruo Zhou and Lihong Li and Quanquan Gu},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xa9TVFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1xa9TVFvH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504205070, "tmdate": 1576860595002, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper724/Authors", "ICLR.cc/2020/Conference/Paper724/Reviewers", "ICLR.cc/2020/Conference/Paper724/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper724/-/Public_Comment"}}}, {"id": "ByeFendB_S", "original": null, "number": 1, "cdate": 1570241521506, "ddate": null, "tcdate": 1570241521506, "tmdate": 1570241521506, "tddate": null, "forum": "r1xa9TVFvH", "replyto": "r1xa9TVFvH", "invitation": "ICLR.cc/2020/Conference/Paper724/-/Public_Comment", "content": {"comment": "It seems the algorithm does not utilize any feature of deep neural network. The authors directly use the neural tangent kernel in the algorithm. This makes the algorithm a direct application of linear UCB with features functions given by the random features produced by the neural tangent features. Thus, it would be better to change the name to ``\"contextual bandit with random features\". ", "title": "Not Neural Network, it is just linear contextual bandit with random features"}, "signatures": ["~Sean_Kanne1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Sean_Kanne1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["drzhou@cs.ucla.edu", "lihongli.cs@gmail.com", "qgu@cs.ucla.edu"], "title": "NeuralUCB: Contextual Bandits with Neural Network-Based Exploration", "authors": ["Dongruo Zhou", "Lihong Li", "Quanquan Gu"], "pdf": "/pdf/33a0927d6e52ad2cacb15acfe6503121d2daf589.pdf", "abstract": "We study the stochastic contextual bandit problem, where the reward is generated from an unknown bounded function with additive noise. We propose the NeuralUCB algorithm, which leverages the representation power of deep neural networks and uses the neural network-based random feature mapping to construct an upper confidence bound (UCB) of reward for efficient exploration. We prove that, under mild assumptions, NeuralUCB achieves $\\tilde O(\\sqrt{T})$ regret bound, where $T$ is the number of rounds. To the best of our knowledge, our algorithm is the first neural network-based contextual bandit algorithm with near-optimal regret guarantee.  Preliminary experiment results on synthetic data corroborate our theory, and shed light on potential applications of our algorithm to real-world problems.", "keywords": ["contextual bandits", "neural network", "upper confidence bound"], "paperhash": "zhou|neuralucb_contextual_bandits_with_neural_networkbased_exploration", "original_pdf": "/attachment/03444ebd6e167ae858b16860a9c5e45474c2f215.pdf", "_bibtex": "@misc{\nzhou2020neuralucb,\ntitle={Neural{\\{}UCB{\\}}: Contextual Bandits with Neural Network-Based Exploration},\nauthor={Dongruo Zhou and Lihong Li and Quanquan Gu},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xa9TVFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1xa9TVFvH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504205070, "tmdate": 1576860595002, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper724/Authors", "ICLR.cc/2020/Conference/Paper724/Reviewers", "ICLR.cc/2020/Conference/Paper724/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper724/-/Public_Comment"}}}], "count": 17}