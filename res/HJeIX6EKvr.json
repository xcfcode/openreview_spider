{"notes": [{"id": "HJeIX6EKvr", "original": "rklkpP1wPB", "number": 451, "cdate": 1569439006361, "ddate": null, "tcdate": 1569439006361, "tmdate": 1577168291343, "tddate": null, "forum": "HJeIX6EKvr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["fy11@rice.edu", "rh43@rice.edu"], "title": "Leveraging inductive bias of neural networks for learning without explicit human annotations", "authors": ["Fatih Furkan Yilmaz", "Reinhard Heckel"], "pdf": "/pdf/043785892f7f76b774721ae7f19d169224cb3b75.pdf", "abstract": "Classification problems today are typically solved by first collecting examples along with candidate labels, second obtaining clean labels from workers, \nand third training a large, overparameterized deep neural network on the clean examples. The second, labeling step is often the most expensive one as it requires manually going through all examples.\nIn this paper we skip the labeling step entirely and propose to directly train the deep neural network on the noisy raw labels and early stop the training to avoid overfitting.\nWith this procedure we exploit an intriguing property of large overparameterized neural networks: While they are capable of perfectly fitting the noisy data, gradient descent fits clean labels much faster than the noisy ones, thus early stopping resembles training on the clean labels.\nOur results show that early stopping the training of standard deep networks such as ResNet-18 on part of the Tiny Images dataset, which does not involve any human labeled data, and of which only about half of the labels are correct, gives a significantly higher test performance than when trained on the clean CIFAR-10 training dataset, which is a labeled version of the Tiny Images dataset, for the same classification problem.\nIn addition, our results show that the noise generated through the label collection process is not nearly as adversarial for learning as the noise generated by randomly flipping labels, which is the noise most prevalent in works demonstrating noise robustness of neural networks.", "keywords": ["dataset construction", "deep learning", "candidate examples"], "paperhash": "yilmaz|leveraging_inductive_bias_of_neural_networks_for_learning_without_explicit_human_annotations", "code": "https://www.dropbox.com/sh/z3kt1rpk61idg0m/AAA--xI-5QWnArath3aM-ztha?dl=0", "original_pdf": "/attachment/81e3d8426493fbb507217528f7f678022a1b9a7b.pdf", "_bibtex": "@misc{\nyilmaz2020leveraging,\ntitle={Leveraging inductive bias of neural networks for learning without explicit human annotations},\nauthor={Fatih Furkan Yilmaz and Reinhard Heckel},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeIX6EKvr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "_H0U8Glemg", "original": null, "number": 1, "cdate": 1576798696844, "ddate": null, "tcdate": 1576798696844, "tmdate": 1576800938845, "tddate": null, "forum": "HJeIX6EKvr", "replyto": "HJeIX6EKvr", "invitation": "ICLR.cc/2020/Conference/Paper451/-/Decision", "content": {"decision": "Reject", "comment": "The authors present an approach to learning from noisy labels. The reviews were mixed and several issues remain unresolved. I do not accept the following as a valid response: \"We fully agree that noisily collected labels are common for many problems other than image classification. However, the focus of our paper is image classification, and we thus concentrate on classification problems related to the widely popular CIFAR-10 and ImageNet classification problems.\" ICLR is a conference on theoretical and applied ML, and the fact that a technique has not been used for image classification before, does not mean you bring something to the table by doing so. The NLP literature is abundant with interesting work on label noise and should obviously be considered related work. That said, there's also missing references directly related to the connection between early stopping/regularization and label bias correction, including: \n\n[0]\u00a0https://arxiv.org/pdf/1904.11238.pdf\n[1]\u00a0https://arxiv.org/pdf/1705.03419.pdf\n[2] http://proceedings.mlr.press/v80/ma18d/ma18d.pdf\n\nSee also this paper submitted to this conference: https://openreview.net/forum?id=SJldu6EtDS", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fy11@rice.edu", "rh43@rice.edu"], "title": "Leveraging inductive bias of neural networks for learning without explicit human annotations", "authors": ["Fatih Furkan Yilmaz", "Reinhard Heckel"], "pdf": "/pdf/043785892f7f76b774721ae7f19d169224cb3b75.pdf", "abstract": "Classification problems today are typically solved by first collecting examples along with candidate labels, second obtaining clean labels from workers, \nand third training a large, overparameterized deep neural network on the clean examples. The second, labeling step is often the most expensive one as it requires manually going through all examples.\nIn this paper we skip the labeling step entirely and propose to directly train the deep neural network on the noisy raw labels and early stop the training to avoid overfitting.\nWith this procedure we exploit an intriguing property of large overparameterized neural networks: While they are capable of perfectly fitting the noisy data, gradient descent fits clean labels much faster than the noisy ones, thus early stopping resembles training on the clean labels.\nOur results show that early stopping the training of standard deep networks such as ResNet-18 on part of the Tiny Images dataset, which does not involve any human labeled data, and of which only about half of the labels are correct, gives a significantly higher test performance than when trained on the clean CIFAR-10 training dataset, which is a labeled version of the Tiny Images dataset, for the same classification problem.\nIn addition, our results show that the noise generated through the label collection process is not nearly as adversarial for learning as the noise generated by randomly flipping labels, which is the noise most prevalent in works demonstrating noise robustness of neural networks.", "keywords": ["dataset construction", "deep learning", "candidate examples"], "paperhash": "yilmaz|leveraging_inductive_bias_of_neural_networks_for_learning_without_explicit_human_annotations", "code": "https://www.dropbox.com/sh/z3kt1rpk61idg0m/AAA--xI-5QWnArath3aM-ztha?dl=0", "original_pdf": "/attachment/81e3d8426493fbb507217528f7f678022a1b9a7b.pdf", "_bibtex": "@misc{\nyilmaz2020leveraging,\ntitle={Leveraging inductive bias of neural networks for learning without explicit human annotations},\nauthor={Fatih Furkan Yilmaz and Reinhard Heckel},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeIX6EKvr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJeIX6EKvr", "replyto": "HJeIX6EKvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795706423, "tmdate": 1576800254473, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper451/-/Decision"}}}, {"id": "SJxHs0L3or", "original": null, "number": 2, "cdate": 1573838492839, "ddate": null, "tcdate": 1573838492839, "tmdate": 1573838492839, "tddate": null, "forum": "HJeIX6EKvr", "replyto": "H1x3TdB6Fr", "invitation": "ICLR.cc/2020/Conference/Paper451/-/Official_Comment", "content": {"title": "Response to review 3", "comment": "On the idea seems to be known/on the fly denoising:\nThe very interesting paper Song et al. ```  `  Better generalization with on-the-fly dataset denoising'' considers a similar problem (learning in the presence of noisy labels) but studies another technique. Specifically, Song et al. train a residual network with a large learning rate and use the resulting losses to separate the clean examples from mislabeled ones, based on some identifying examples that exceed a threshold. Our work, in contrast, does not attempt to separate clean labels from noisy ones but instead simply trains the network on the noisy examples. \n\nFunction of learning rates: In order to address this, we performed additional experiments on ResNet18 with smaller and larger learning rates, relative to the learning rate we choose initially. Initially, we choose the default learning rate/schedule which starts at 0.1 and decays every 30 epochs. We did experiments starting from 0.05 and 0.02 and found that it changes the rate at which the training set is fitted, but not the general behavior of the curves, performance, or necessity of early stopping.\n\nRegarding under what conditions early stopping prevents memorizing bad labels: Our numerical results show that standard neural networks such as ResNet, Shake-Shake, VGG, DenseNet, and others do not memorize the data if trained for a few epochs (see Fig. 2, showing that after a few epochs only the clean labels are fitted, but not the noisy ones). We do, however, not have formal conditions stating under which conditions data is not memorized, and we are also not aware of such conditions in the literature. \n\nRegarding whether the claims hold beyond classification, e.g., for object detection or segmentation: We do not know the extent to which this claim continues to holds, but suspect that the general insight that the network imposes sufficient structure to learn from noisy labels continues to hold for other domains/problems. A concrete example where this is true is image denoising: As first shown by Ulyanov et al. ``  `  deep image prior'', a convolutional network fits a natural image faster than noise and thus enables denoising by early stopping. This is conceptually similar to what we find albeit for a different problem (image denoising) vs. learning from noisy labels.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper451/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper451/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fy11@rice.edu", "rh43@rice.edu"], "title": "Leveraging inductive bias of neural networks for learning without explicit human annotations", "authors": ["Fatih Furkan Yilmaz", "Reinhard Heckel"], "pdf": "/pdf/043785892f7f76b774721ae7f19d169224cb3b75.pdf", "abstract": "Classification problems today are typically solved by first collecting examples along with candidate labels, second obtaining clean labels from workers, \nand third training a large, overparameterized deep neural network on the clean examples. The second, labeling step is often the most expensive one as it requires manually going through all examples.\nIn this paper we skip the labeling step entirely and propose to directly train the deep neural network on the noisy raw labels and early stop the training to avoid overfitting.\nWith this procedure we exploit an intriguing property of large overparameterized neural networks: While they are capable of perfectly fitting the noisy data, gradient descent fits clean labels much faster than the noisy ones, thus early stopping resembles training on the clean labels.\nOur results show that early stopping the training of standard deep networks such as ResNet-18 on part of the Tiny Images dataset, which does not involve any human labeled data, and of which only about half of the labels are correct, gives a significantly higher test performance than when trained on the clean CIFAR-10 training dataset, which is a labeled version of the Tiny Images dataset, for the same classification problem.\nIn addition, our results show that the noise generated through the label collection process is not nearly as adversarial for learning as the noise generated by randomly flipping labels, which is the noise most prevalent in works demonstrating noise robustness of neural networks.", "keywords": ["dataset construction", "deep learning", "candidate examples"], "paperhash": "yilmaz|leveraging_inductive_bias_of_neural_networks_for_learning_without_explicit_human_annotations", "code": "https://www.dropbox.com/sh/z3kt1rpk61idg0m/AAA--xI-5QWnArath3aM-ztha?dl=0", "original_pdf": "/attachment/81e3d8426493fbb507217528f7f678022a1b9a7b.pdf", "_bibtex": "@misc{\nyilmaz2020leveraging,\ntitle={Leveraging inductive bias of neural networks for learning without explicit human annotations},\nauthor={Fatih Furkan Yilmaz and Reinhard Heckel},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeIX6EKvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJeIX6EKvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper451/Authors", "ICLR.cc/2020/Conference/Paper451/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper451/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper451/Reviewers", "ICLR.cc/2020/Conference/Paper451/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper451/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper451/Authors|ICLR.cc/2020/Conference/Paper451/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171305, "tmdate": 1576860530741, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper451/Authors", "ICLR.cc/2020/Conference/Paper451/Reviewers", "ICLR.cc/2020/Conference/Paper451/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper451/-/Official_Comment"}}}, {"id": "rkxPGAL2jB", "original": null, "number": 1, "cdate": 1573838350808, "ddate": null, "tcdate": 1573838350808, "tmdate": 1573838350808, "tddate": null, "forum": "HJeIX6EKvr", "replyto": "HyxyyOuBqr", "invitation": "ICLR.cc/2020/Conference/Paper451/-/Official_Comment", "content": {"title": "Response to review 4", "comment": "Thanks for the review and for acknowledging that the connection between early stopping and fitting true label distributions before noisy ones is of potential impact. In our revision, we provide experiments on another dataset in order to further justify our findings and to address the reviewers concerns.\n\nRegarding context/references:\n- We fully agree that noisily collected labels are common for many problems other than image classification. However, the focus of our paper is image classification, and we thus concentrate on classification problems related to the widely popular CIFAR-10 and ImageNet classification problems. For the CIFAR-10 classification problem, it has not shown before that learning from candidate examples only is possible.\n- Thanks for bringing the paper by Plank, Hovy, and Sogaard to our attention. While a very interesting work, we do not see a connection to our work since Plank, Hovy, and Sogaard focus on NLP and incorporating measured inter-annotator agreement in the loss function. In contrast, we consider image classification and leverage the inductive bias of neural networks to learn from noisy candidate labels by early stopping. There is no mention of fitting examples slow or fast in the Plank, Hovy, and Sogaard paper.\n- Yes, we have also performed experiments with l2 regularization, but without early stopping cannot replicate our results with simply L2 regularization. Please note that our results rely on convolutional networks fitting clean labels faster than noisy ones, so early stopping is a natural way to exploit this property, while L2 regularization is not.\n- On cost-sensitive learning of systematic disagreements: This is a topic of the Plank, Howey, and Sogaard paper but not of our paper, since in our paper we do have noisy data, but no disagreements between labelers (because our data is not labeled by explicit labelers).\n- The observation that clean examples are fitted faster than noise is not related to active learning.\n\nExperimental details:\n- We fully agree with the downsides of using a search engine to collect labels pointed out by the reviewer. However, both the collection of CIFAR and ImageNet, the arguably most accepted datasets for image classification, are based on using search engines for data collection. The reproducibility of our results is not compromised by using the TinyImages dataset, because this dataset is publicly available and because we made our code available.\nRegarding reporting results only for a single dataset: In order to show that our findings extend to other datasets, we have performed an additional experiment on a subset of ImageNet and a candidate dataset that we have collected on Flickr. The results, reported in our revised paper, confirm our finding that early stopping enables learning from a candidate data set.\n- Regarding error analysis: we have added a section in the appendix showing the accuracy per class for both i) training on the candidate dataset and ii) the original CIFAR training set. The results show that the model trained on the candidate examples has a higher accuracy for every single class compared to the model trained on the original CIFAR-10 training set. \n- We did perform an experiment on a synthetic noisy dataset that directly evaluates the hypothesis that true labels are fitted faster than noise, please see Figure 4, right panel. There, we have flipped about half of the labels and the results show that the clean labels are fitted faster than the noisy ones (otherwise the test accuracy would not go first up and then drop significantly). We would also like to point out that our objective is to skip the expensive labeling step by directly training on the candidate labels. The real noise is a natural bi-product of such constructed datasets. \n- Please note that we cannot present confusion matrices as in the aforementioned paper by Plank, Hovy, and Sogaard, since we have a different setup and error mode: Our error mode is that an image does not belong to the class it is labeled with, but also not to one of the other classes (see Section 5 where we explain this).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper451/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper451/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fy11@rice.edu", "rh43@rice.edu"], "title": "Leveraging inductive bias of neural networks for learning without explicit human annotations", "authors": ["Fatih Furkan Yilmaz", "Reinhard Heckel"], "pdf": "/pdf/043785892f7f76b774721ae7f19d169224cb3b75.pdf", "abstract": "Classification problems today are typically solved by first collecting examples along with candidate labels, second obtaining clean labels from workers, \nand third training a large, overparameterized deep neural network on the clean examples. The second, labeling step is often the most expensive one as it requires manually going through all examples.\nIn this paper we skip the labeling step entirely and propose to directly train the deep neural network on the noisy raw labels and early stop the training to avoid overfitting.\nWith this procedure we exploit an intriguing property of large overparameterized neural networks: While they are capable of perfectly fitting the noisy data, gradient descent fits clean labels much faster than the noisy ones, thus early stopping resembles training on the clean labels.\nOur results show that early stopping the training of standard deep networks such as ResNet-18 on part of the Tiny Images dataset, which does not involve any human labeled data, and of which only about half of the labels are correct, gives a significantly higher test performance than when trained on the clean CIFAR-10 training dataset, which is a labeled version of the Tiny Images dataset, for the same classification problem.\nIn addition, our results show that the noise generated through the label collection process is not nearly as adversarial for learning as the noise generated by randomly flipping labels, which is the noise most prevalent in works demonstrating noise robustness of neural networks.", "keywords": ["dataset construction", "deep learning", "candidate examples"], "paperhash": "yilmaz|leveraging_inductive_bias_of_neural_networks_for_learning_without_explicit_human_annotations", "code": "https://www.dropbox.com/sh/z3kt1rpk61idg0m/AAA--xI-5QWnArath3aM-ztha?dl=0", "original_pdf": "/attachment/81e3d8426493fbb507217528f7f678022a1b9a7b.pdf", "_bibtex": "@misc{\nyilmaz2020leveraging,\ntitle={Leveraging inductive bias of neural networks for learning without explicit human annotations},\nauthor={Fatih Furkan Yilmaz and Reinhard Heckel},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeIX6EKvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJeIX6EKvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper451/Authors", "ICLR.cc/2020/Conference/Paper451/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper451/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper451/Reviewers", "ICLR.cc/2020/Conference/Paper451/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper451/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper451/Authors|ICLR.cc/2020/Conference/Paper451/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171305, "tmdate": 1576860530741, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper451/Authors", "ICLR.cc/2020/Conference/Paper451/Reviewers", "ICLR.cc/2020/Conference/Paper451/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper451/-/Official_Comment"}}}, {"id": "H1x3TdB6Fr", "original": null, "number": 1, "cdate": 1571801283680, "ddate": null, "tcdate": 1571801283680, "tmdate": 1572972593705, "tddate": null, "forum": "HJeIX6EKvr", "replyto": "HJeIX6EKvr", "invitation": "ICLR.cc/2020/Conference/Paper451/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary\n\nThe paper exploits the training dynamics of deep nets: gradient descent fits clean labels much faster than the noisy ones, therefore early stopping resembles training on the clean labels. The paper shows that early stopping the training on datasets with noisy labels can achieve classification performance higher than when training on clean labels (on condition that the total number of clean labels in the noisy training set is sufficiently large).\n\nThe paper also makes the point that noise introduced during data collection are different from artificially generated noise through randomly flipping the labels of a clean dataset. The latter is often done in the literature. The \u201creal\u201d noise is more structured and therefore it is easier to fit and less harmful to the classification performance.\n\nStrengths\n\nThe paper is well written. The results are very interesting even though they are very intuitive and simple.\n\nWeaknesses\n\nThe idea seems to be known. For example,\nBetter Generalization with On-the-fly Dataset Denoising\nhttps://openreview.net/forum?id=HyGDdsCcFQ\n\nThe paper talks about early stopping. As shown by the above paper, it is also a function of the learning rate. Please comment on what happens in the case of small learning rate and early stopping.\n\nIt would have been great to prove the theorem for deep learning. The result is limited to linear models with large number of random features.\n\nThe paper does not make it clear under what conditions early stopping prevents the model from memorizing bad labels.\n\nThe paper focuses on classification. Will the claims hold for other task types such as object detection, segmentation, etc?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper451/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper451/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fy11@rice.edu", "rh43@rice.edu"], "title": "Leveraging inductive bias of neural networks for learning without explicit human annotations", "authors": ["Fatih Furkan Yilmaz", "Reinhard Heckel"], "pdf": "/pdf/043785892f7f76b774721ae7f19d169224cb3b75.pdf", "abstract": "Classification problems today are typically solved by first collecting examples along with candidate labels, second obtaining clean labels from workers, \nand third training a large, overparameterized deep neural network on the clean examples. The second, labeling step is often the most expensive one as it requires manually going through all examples.\nIn this paper we skip the labeling step entirely and propose to directly train the deep neural network on the noisy raw labels and early stop the training to avoid overfitting.\nWith this procedure we exploit an intriguing property of large overparameterized neural networks: While they are capable of perfectly fitting the noisy data, gradient descent fits clean labels much faster than the noisy ones, thus early stopping resembles training on the clean labels.\nOur results show that early stopping the training of standard deep networks such as ResNet-18 on part of the Tiny Images dataset, which does not involve any human labeled data, and of which only about half of the labels are correct, gives a significantly higher test performance than when trained on the clean CIFAR-10 training dataset, which is a labeled version of the Tiny Images dataset, for the same classification problem.\nIn addition, our results show that the noise generated through the label collection process is not nearly as adversarial for learning as the noise generated by randomly flipping labels, which is the noise most prevalent in works demonstrating noise robustness of neural networks.", "keywords": ["dataset construction", "deep learning", "candidate examples"], "paperhash": "yilmaz|leveraging_inductive_bias_of_neural_networks_for_learning_without_explicit_human_annotations", "code": "https://www.dropbox.com/sh/z3kt1rpk61idg0m/AAA--xI-5QWnArath3aM-ztha?dl=0", "original_pdf": "/attachment/81e3d8426493fbb507217528f7f678022a1b9a7b.pdf", "_bibtex": "@misc{\nyilmaz2020leveraging,\ntitle={Leveraging inductive bias of neural networks for learning without explicit human annotations},\nauthor={Fatih Furkan Yilmaz and Reinhard Heckel},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeIX6EKvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJeIX6EKvr", "replyto": "HJeIX6EKvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper451/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper451/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575625390956, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper451/Reviewers"], "noninvitees": [], "tcdate": 1570237751939, "tmdate": 1575625390974, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper451/-/Official_Review"}}}, {"id": "HyxyyOuBqr", "original": null, "number": 2, "cdate": 1572337623428, "ddate": null, "tcdate": 1572337623428, "tmdate": 1572972593663, "tddate": null, "forum": "HJeIX6EKvr", "replyto": "HJeIX6EKvr", "invitation": "ICLR.cc/2020/Conference/Paper451/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper reads well, the topic is interesting, and the connection between early stopping and fitting true label distributions before noisy ones, if generally true, is of potential impact. I do not feel the paper is ready for publication, though: \n\nMissing context/references:\n\t\u2043\tNoisily collected labels are standard elsewhere, e.g., in sentiment analysis (self-ratings), in discourse parsing (explicit discourse markers), in weakly supervised POS tagging (crowdsourced dictionaries), in NER (Wikidata links), and in machine translation from mined parallel corpora or comparable corpora. \n\t\u2043\tLearning a ground truth from a population of turkers (MACE and subsequent work). It is well-known that the noise in such silver standard or non-adjudicated annotations is also \u201cmore structured\u201d and to a large extent predictable (Plank et al., EACL 2014 and subsequent work on learning/predicting inter-annotator disagreements). \n\t\u2043\tConnection between L2 and early stopping. Can you replicate your results with simple L2 regularisation?\n\t\u2043\tCost-sensitive learning of systematic disagreements (use class coherence scores for cost-sensitive learning). \n\t\u2043\tThe observation that \u201cclean examples are fitted faster than noise\u201d is obviously related to baby steps training regimes (training on easy examples first), including active learning. \n\nExperimental details and flaws: \n\t\u2043\tUsing a black box search engine to collect labels is problematic for a few reasons: (a) Search engines change, so results are hard to reproduce. (b) Search engines are biased toward certain types of categories. Such biases will be reinforced by any model trained on this data. \n\t\u2043\tI\u2019m always a little worried about papers that only report results for a single dataset. \n\t\u2043\tI would have liked to see some more error analysis. Are the improvements on classes with more or less support in the original dataset, for example?\n\t\u2043\tIt seems to be that it would have been relatively straight-forward to construct synthetic datasets that would more directly evaluate the hypothesis that the true labels are learned first. I realize you\u2019re interested in \u201creal\u201d noise rather than \u201crandom\u201d noise - but synthetic noise doesn\u2019t have to be random. \n\t\u2043\tThat \u201cthe noise generated through the label collection process is not nearly as adversarial for learning as the noise generated by randomly flipping labels\u201d is no surprise, and I would not present this as a finding. It would be interesting to describe the bias, e.g., by presenting confusion matrices (see Plank et al., EACL 2014). \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper451/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper451/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fy11@rice.edu", "rh43@rice.edu"], "title": "Leveraging inductive bias of neural networks for learning without explicit human annotations", "authors": ["Fatih Furkan Yilmaz", "Reinhard Heckel"], "pdf": "/pdf/043785892f7f76b774721ae7f19d169224cb3b75.pdf", "abstract": "Classification problems today are typically solved by first collecting examples along with candidate labels, second obtaining clean labels from workers, \nand third training a large, overparameterized deep neural network on the clean examples. The second, labeling step is often the most expensive one as it requires manually going through all examples.\nIn this paper we skip the labeling step entirely and propose to directly train the deep neural network on the noisy raw labels and early stop the training to avoid overfitting.\nWith this procedure we exploit an intriguing property of large overparameterized neural networks: While they are capable of perfectly fitting the noisy data, gradient descent fits clean labels much faster than the noisy ones, thus early stopping resembles training on the clean labels.\nOur results show that early stopping the training of standard deep networks such as ResNet-18 on part of the Tiny Images dataset, which does not involve any human labeled data, and of which only about half of the labels are correct, gives a significantly higher test performance than when trained on the clean CIFAR-10 training dataset, which is a labeled version of the Tiny Images dataset, for the same classification problem.\nIn addition, our results show that the noise generated through the label collection process is not nearly as adversarial for learning as the noise generated by randomly flipping labels, which is the noise most prevalent in works demonstrating noise robustness of neural networks.", "keywords": ["dataset construction", "deep learning", "candidate examples"], "paperhash": "yilmaz|leveraging_inductive_bias_of_neural_networks_for_learning_without_explicit_human_annotations", "code": "https://www.dropbox.com/sh/z3kt1rpk61idg0m/AAA--xI-5QWnArath3aM-ztha?dl=0", "original_pdf": "/attachment/81e3d8426493fbb507217528f7f678022a1b9a7b.pdf", "_bibtex": "@misc{\nyilmaz2020leveraging,\ntitle={Leveraging inductive bias of neural networks for learning without explicit human annotations},\nauthor={Fatih Furkan Yilmaz and Reinhard Heckel},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeIX6EKvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJeIX6EKvr", "replyto": "HJeIX6EKvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper451/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper451/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575625390956, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper451/Reviewers"], "noninvitees": [], "tcdate": 1570237751939, "tmdate": 1575625390974, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper451/-/Official_Review"}}}], "count": 6}