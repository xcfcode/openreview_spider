{"notes": [{"id": "QcqsxI6rKDs", "original": "0dFg5RbjMGM", "number": 2438, "cdate": 1601308269178, "ddate": null, "tcdate": 1601308269178, "tmdate": 1614985640008, "tddate": null, "forum": "QcqsxI6rKDs", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Meta Gradient Boosting Neural Networks", "authorids": ["~Manqing_Dong1", "~Lina_Yao2", "xianzhi.wang@uts.edu.au", "xiwei.xu@data61.csiro.au", "liming.zhu@data61.csiro.au"], "authors": ["Manqing Dong", "Lina Yao", "Xianzhi Wang", "Xiwei Xu", "Liming Zhu"], "keywords": ["meta learning", "deep learning"], "abstract": "Meta-optimization is an effective approach that learns a shared set of parameters across tasks for parameter initialization in meta-learning.\nA key challenge for meta-optimization based approaches is to determine whether an initialization condition can be generalized to tasks with diverse distributions to accelerate learning. \nTo address this issue, we design a meta-gradient boosting framework that uses a base learner to learn shared information across tasks and a series of gradient-boosted modules to capture task-specific information to fit diverse distributions.\nWe evaluate the proposed model on both regression and classification tasks with multi-mode distributions. \nThe results demonstrate both the effectiveness of our model in modulating task-specific meta-learned priors and its advantages on multi-mode distributions.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dong|meta_gradient_boosting_neural_networks", "pdf": "/pdf/4943864d76084cbafbab40eeb5331907a152fa7f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LgDMzSSh2", "_bibtex": "@misc{\ndong2021meta,\ntitle={Meta Gradient Boosting Neural Networks},\nauthor={Manqing Dong and Lina Yao and Xianzhi Wang and Xiwei Xu and Liming Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=QcqsxI6rKDs}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "zDxau35GQ-k", "original": null, "number": 1, "cdate": 1610040521681, "ddate": null, "tcdate": 1610040521681, "tmdate": 1610474130485, "tddate": null, "forum": "QcqsxI6rKDs", "replyto": "QcqsxI6rKDs", "invitation": "ICLR.cc/2021/Conference/Paper2438/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper proposes a meta-gradient boosting framework to tackle the model-agnostic meta-learning problem. The idea is to use a base learn that learns shared information across tasks, and gradient boosted modules to capture task-specific modules. The experiments show that the proposed meta-gradient boosting framework (with 5 gradient boosting modules) achieves better or competitive results compared to the baselines. However, there were several issues that the author feedback did not addressed properly. For instance, R2 were not satisfied by discussing briefly the suggested baselines without adding the comparison, or R1 pointed out that the claim \u201cthe learning and updating strategy proposed in the method ensured a weak base learner\u201d because clear separable datasets could convergence quickly and weak is not anymore applicable. Besides these two specific concerns, the reviewers expected a large revision of the paper due to several cons about the paper. All reviewers agreed a mayor revision is needed before acceptance. Therefore I recommend rejection."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta Gradient Boosting Neural Networks", "authorids": ["~Manqing_Dong1", "~Lina_Yao2", "xianzhi.wang@uts.edu.au", "xiwei.xu@data61.csiro.au", "liming.zhu@data61.csiro.au"], "authors": ["Manqing Dong", "Lina Yao", "Xianzhi Wang", "Xiwei Xu", "Liming Zhu"], "keywords": ["meta learning", "deep learning"], "abstract": "Meta-optimization is an effective approach that learns a shared set of parameters across tasks for parameter initialization in meta-learning.\nA key challenge for meta-optimization based approaches is to determine whether an initialization condition can be generalized to tasks with diverse distributions to accelerate learning. \nTo address this issue, we design a meta-gradient boosting framework that uses a base learner to learn shared information across tasks and a series of gradient-boosted modules to capture task-specific information to fit diverse distributions.\nWe evaluate the proposed model on both regression and classification tasks with multi-mode distributions. \nThe results demonstrate both the effectiveness of our model in modulating task-specific meta-learned priors and its advantages on multi-mode distributions.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dong|meta_gradient_boosting_neural_networks", "pdf": "/pdf/4943864d76084cbafbab40eeb5331907a152fa7f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LgDMzSSh2", "_bibtex": "@misc{\ndong2021meta,\ntitle={Meta Gradient Boosting Neural Networks},\nauthor={Manqing Dong and Lina Yao and Xianzhi Wang and Xiwei Xu and Liming Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=QcqsxI6rKDs}\n}"}, "tags": [], "invitation": {"reply": {"forum": "QcqsxI6rKDs", "replyto": "QcqsxI6rKDs", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040521668, "tmdate": 1610474130469, "id": "ICLR.cc/2021/Conference/Paper2438/-/Decision"}}}, {"id": "3DhIID3GIlO", "original": null, "number": 4, "cdate": 1603895231830, "ddate": null, "tcdate": 1603895231830, "tmdate": 1606740180633, "tddate": null, "forum": "QcqsxI6rKDs", "replyto": "QcqsxI6rKDs", "invitation": "ICLR.cc/2021/Conference/Paper2438/-/Official_Review", "content": {"title": "Review", "review": "### Paper summary\nThis paper addresses a problem of model-agnostic meta-learning (MAML) and most of its variations/extensions - learning only a single parameter initialization for the entire task distribution, which might not be effective when the task distribution is too diverse. Inspired by gradient boosting, which aims to train a new learner to predict the residuals of the previously predicted result for each step, this paper proposes a meta gradient boosting framework. Specifically, the proposed framework represents the meta-learned global initialization as a base learner, consisting of the first few weak learners, which is responsible for acquiring sharable transferable knowledge by learning across all tasks (or task modes). Then, many gradient-boosting modules learn to capture task-specific information to fit diverse distributions more effectively. This paper evaluates the proposed framework and the baselines, including MAML, Multimodal MAML, and LEO, on both the few-shot regression task and the few-shot image classification task. To create diverse task distributions, it combines different function families (linear, sinusoidal, etc.) for the regression and merges multiple datasets (Omniglot, miniImageNet, etc.) for the classification. The experiments show that the proposed meta-gradient boosting framework (with 5 gradient boosting modules) achieves better or competitive results compared to the baselines. Ablation studies justify some design choices of the proposed framework, including the architecture of the weak learner, the number of gradient-boosting modules, the updating strategy for the gradient boosting modules, the boosting rate, etc. I believe this work studies an important problem and proposes an interesting framework. Yet, I have a few concerns regarding experimental setup and results which prevent me from accepting this paper (see below for details).\n\n### Paper strengths\n**motivation**\nThe motivation for addressing the issue of model-agnostic meta-learning (MAML) - learning only a single parameter initialization for the entire task distribution, which might not be effective when the task distribution is too diverse, is convincing.\n\n**novelty**\nAs far as I am concerned, the idea of utilizing the intuition of gradient boosting is novel and interesting. This paper presents a reasonable way to implement this idea.\n\n**technical contribution & ablation study**\nAblation studies provide insights that help understand the proposed framework and justify many design choices. This includes the architecture of the weak learner, the number of gradient boosting modules, the updating strategy for the gradient boosting modules, the boosting rate, etc. \n\n**clarity**\nThe writing is very clear and the figures illustrate the ideas well. Also, the organization of the paper is easy to follow.\n\n**experimental results**\n- The description of the experimental setup is comprehensive. The presentation of the experimental results is clear. \n- The proposed meta-gradient boosting framework outperforms or at least performs competitively compared to the representative baselines.\n- This paper studies a variety (i.e. different numbers of task modes) of settings, which provides great insights.\n\n### Paper weaknesses\n**baselines**\nI believe this paper ignores many important baselines. As a result, the experimental conclusions are less convincing. I list some of the baselines and brief reasons why I believe they should be included as below:\n- Hierarchically Structured Meta-Learning (HSML): is designed to perform soft clustering on tasks. It would be interesting to see if HSML can handle the task distributions considered in this paper. Also, it shows superior performance compared to MAML and a workshop version of Multimodal MAML. So, it is a stronger baseline to be included.\n- Probalistic MAML / Bayesian MAML: both of these two methods consider , outperforming MAML. Intuitively, this probabilistic schema should deal with multimodal task distributions better since it inherently learns to handle uncertainty. It would be great to compare against at least one of them.\n- Proto-MAML: presented in the meta-dataset paper (Triantafillou et al. in ICLR 2020), is designed to deal with multiple datasets combined and therefore should be able to perform well on the setup considered in this paper. Proto-MAML shows the strongest performance compared to MAML based methods and even outperforms some metric-based meta-learning methods. Showing the proposed framework can outperform or perform competitively compared to Proto-MAML would make this paper much stronger.\n- Metric-based meta-learning methods: this paper does not include comparisons against any metric-based meta-learning methods such as matching networks, prototypical networks, relation networks, TADAM, etc. Yet, the state-of-the-art results on the few-shot image classification are mostly achieved by metric-based meta-learning methods. Therefore, I believe it would be essential to include representative metric-based meta-learning methods.\n\n**MAML with a comparable number of parameters**\nSince the proposed meta-gradient boosting framework has more parameters than the vanilla MAML, it is possible that the performance gain comes from the larger capacity. It would important to include a MAML baseline that has a comparable number of parameters to justify this. \n\n**RL experiments**\nAs far as I am concerned, the few-shot regression task is more a task for detailed analysis for research rather than a task with a wide range of applications, and the state-of-the-art results of the few-shot image classification task have been mostly achieved by metric-based meta-learning methods. Therefore, I am mainly interested in the model-agnostic meta-learning \u00a0line of work because of its potential in reinforcement learning where the ability to adapt to unseen scenarios is crucial. Yet, this paper does not include any experiments on RL without any reasons, which makes the paper less convincing to me.\n\n**MGB-1 vs. MAML**\nIt seems that the proposed meta-gradient boosting model with one gradient-boosting module outperforms the vanilla MAML by a significant margin on the regression task yet only performs similarly to the vanilla MAML on the classification. Can the authors give some intuition about why this is the case?\n\n**meta-dataset**\nTo evaluate if the proposed framework and the baselines can deal with diverse task distributions on the few-shot image classification task, this paper combines four different few-shot learning datasets to produce a 4-mode classification task. Yet, the meta-dataset has been proposed for this purpose. Also, the meta-dataset paper provides a comprehensive comparison of recent meta-learning methods. Therefore, I believe this paper should evaluate the proposed framework on the meta-dataset and see if it outperforms the baselines.\n\n**stddev of the classification task**\nIt seems that the performance gap between the baselines and the proposed framework is insignificant on the image classification task. In this case, it would be important to also provide the standard deviation of each task.\n\n=== After rebuttal ===\n\nI am not satisfied with the response from the authors. I can only hardly recognize the effort made by the authors during the rebuttal - most of my points were only briefly discussed in the response without revising the paper. \n\n- The suggested baselines were merely briefly discussed but not added to the comparison.\n- The results of the meta-dataset, which in my opinion is the most suitable dataset for the purpose, are still not included in the revised paper.\n- The stddev of the classification task is not still not provided, making it hard to justify the performance gain.\n- Why is RL left to future work?\n\nI have read the reviews from other reviewers. With the little revision from the authors, I have decided to keep my original rating and would not recommend this paper to be accepted.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2438/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2438/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta Gradient Boosting Neural Networks", "authorids": ["~Manqing_Dong1", "~Lina_Yao2", "xianzhi.wang@uts.edu.au", "xiwei.xu@data61.csiro.au", "liming.zhu@data61.csiro.au"], "authors": ["Manqing Dong", "Lina Yao", "Xianzhi Wang", "Xiwei Xu", "Liming Zhu"], "keywords": ["meta learning", "deep learning"], "abstract": "Meta-optimization is an effective approach that learns a shared set of parameters across tasks for parameter initialization in meta-learning.\nA key challenge for meta-optimization based approaches is to determine whether an initialization condition can be generalized to tasks with diverse distributions to accelerate learning. \nTo address this issue, we design a meta-gradient boosting framework that uses a base learner to learn shared information across tasks and a series of gradient-boosted modules to capture task-specific information to fit diverse distributions.\nWe evaluate the proposed model on both regression and classification tasks with multi-mode distributions. \nThe results demonstrate both the effectiveness of our model in modulating task-specific meta-learned priors and its advantages on multi-mode distributions.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dong|meta_gradient_boosting_neural_networks", "pdf": "/pdf/4943864d76084cbafbab40eeb5331907a152fa7f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LgDMzSSh2", "_bibtex": "@misc{\ndong2021meta,\ntitle={Meta Gradient Boosting Neural Networks},\nauthor={Manqing Dong and Lina Yao and Xianzhi Wang and Xiwei Xu and Liming Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=QcqsxI6rKDs}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "QcqsxI6rKDs", "replyto": "QcqsxI6rKDs", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2438/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538096331, "tmdate": 1606915806222, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2438/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2438/-/Official_Review"}}}, {"id": "CD7G1JUE_MF", "original": null, "number": 5, "cdate": 1606057704959, "ddate": null, "tcdate": 1606057704959, "tmdate": 1606057704959, "tddate": null, "forum": "QcqsxI6rKDs", "replyto": "dgdgiZjtZZH", "invitation": "ICLR.cc/2021/Conference/Paper2438/-/Official_Comment", "content": {"title": "To reviewer 4", "comment": "Regarding the **motivation and clarity of the gradient boosting model**, we suppose the audience to have some basic knowledge about the gradient boosting framework (Friedman, 2001). Besides, we have introduced the core idea of gradient boosting right at the start of the paper and given more details in our method. \u201cRecent research \u2026\u201d suggests that the potential of decomposing deep neural nets into an ensemble of sub-networks; our framework of gradient boosting neural networks is one of such promising ensemble of sub-networks.\n1.\tWeak learner (defined in Section 3) and base learner (defined in Section 3.1) are two concepts in Gradient Boosting. The model makes predictions based on the weak learners, each of which can be considered as a neural network that provides its own prediction. Specially, the first or first few weak learners serve as the base learner to provide an initial guess for the prediction. We introduced the meaning of \u201cstep\u201d in constructing a gradient boosting framework in lines 3-7 on page 4. \n2.\tThe core idea of MAML is to learn a sharing parameter initialization for all tasks. The model shares structure across all tasks but is locally updated for task-specific predictions. Therefore, our framework can be decomposed into several sub-networks, where we only share the parameter initialization in the base learner where gradient boosting modules (the other weak learners) focus on making task-specific predictions. \n3.\tWe have explained \u201cstep k\u201d and gradient boost in Section 3. Specifically, the beginning of Section 3 gives the high-level idea of gradient boosting, and Section 3.1 gives detailed definitions. As each weak learner can be regarded as a neural network, theta-k represents the parameter of the k-th weak learner. We think it has been clearly stated in the paper. \n\nRegarding the **figure quality**, we will improve the figure quality in the next version.\n \nRegarding the **results**, we attribute the better results obtained by the proposed framework and related work than the plain MAML to that \u201cproviding more task-specific initialization tends to deliver better performance\u201d. \n\nRegarding the **ability in dealing with multi-mode distributions**, the core idea of MAML is learning a sharing parameter initialization for all tasks. The model shares structure across all the tasks but is locally updated for task-specific predictions. The single initialization means that all the task models start from a single sharing condition. For the case that the tasks have multi-mode distributions, the traditional MAML simply delivers a 'mean' situation for all tasks. Under such a situation, tasks with \u201cdiverse\u201d situations (as opposed to the 'mean' situation) will take more time in learning their task-specific model. A model with more flexibility in providing diverse (or task-specific) model initialization could alleviate this issue. Since our model aims to capture more task-specific knowledge, more gradient boost modules will weaken the contribution of the base learner to the final prediction. That is why we say the updating strategy ensures a \u201cweak base learner\u201d.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2438/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2438/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta Gradient Boosting Neural Networks", "authorids": ["~Manqing_Dong1", "~Lina_Yao2", "xianzhi.wang@uts.edu.au", "xiwei.xu@data61.csiro.au", "liming.zhu@data61.csiro.au"], "authors": ["Manqing Dong", "Lina Yao", "Xianzhi Wang", "Xiwei Xu", "Liming Zhu"], "keywords": ["meta learning", "deep learning"], "abstract": "Meta-optimization is an effective approach that learns a shared set of parameters across tasks for parameter initialization in meta-learning.\nA key challenge for meta-optimization based approaches is to determine whether an initialization condition can be generalized to tasks with diverse distributions to accelerate learning. \nTo address this issue, we design a meta-gradient boosting framework that uses a base learner to learn shared information across tasks and a series of gradient-boosted modules to capture task-specific information to fit diverse distributions.\nWe evaluate the proposed model on both regression and classification tasks with multi-mode distributions. \nThe results demonstrate both the effectiveness of our model in modulating task-specific meta-learned priors and its advantages on multi-mode distributions.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dong|meta_gradient_boosting_neural_networks", "pdf": "/pdf/4943864d76084cbafbab40eeb5331907a152fa7f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LgDMzSSh2", "_bibtex": "@misc{\ndong2021meta,\ntitle={Meta Gradient Boosting Neural Networks},\nauthor={Manqing Dong and Lina Yao and Xianzhi Wang and Xiwei Xu and Liming Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=QcqsxI6rKDs}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "QcqsxI6rKDs", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2438/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2438/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2438/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2438/Authors|ICLR.cc/2021/Conference/Paper2438/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2438/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848359, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2438/-/Official_Comment"}}}, {"id": "VKPBwzs9Z0r", "original": null, "number": 3, "cdate": 1606057263859, "ddate": null, "tcdate": 1606057263859, "tmdate": 1606057584770, "tddate": null, "forum": "QcqsxI6rKDs", "replyto": "fMtoqy3yqt", "invitation": "ICLR.cc/2021/Conference/Paper2438/-/Official_Comment", "content": {"title": "To reviewer 1", "comment": "Thanks for your careful reviews that help improve the quality of our work. We respond to your comments point by point below.\n1. Regarding the weak learner, we believe the reviewer has indeed pointed out a very good question. Some theoretical studies have found that the gradient for the earlier weak learners decays with the increasing number of gradient boost modules. We did some experiments to evaluate the contribution of each part to the final prediction with different gradient boosting steps. We will put the results and some discussion in the future version.\n2. Thanks for your suggestion on the regression tasks. We will consider line search methods in future works. \n3. We tried the best to apply the same settings to the compared methods to make a fair comparison. Our framework takes more time for training because it contains more gradient boosting modules and requires more update steps. But our experimental results show that it does not take a large number of gradient boosting modules and update steps to obtain an acceptable result.\n4. Several studies [1, 2] have shown that the choice for feature extractor can affect the final prediction results. We decided to choose a commonly used feature extractor, 4-CONV, to ensure a fair comparison. We tested the performance of several pre-trained models to obtain the image embedding (e.g. ResNet-12), and the results showed the pre-trained image embedding delivers better performance.\n\n[1] Sun et al. Meta-Transfer Learning for Few-Shot Learning, CVPR 2019.\n\n[2] Tian et al. Rethinking Few-Shot Image Classification: a Good Embedding Is All You Need? ECCV 2020.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2438/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2438/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta Gradient Boosting Neural Networks", "authorids": ["~Manqing_Dong1", "~Lina_Yao2", "xianzhi.wang@uts.edu.au", "xiwei.xu@data61.csiro.au", "liming.zhu@data61.csiro.au"], "authors": ["Manqing Dong", "Lina Yao", "Xianzhi Wang", "Xiwei Xu", "Liming Zhu"], "keywords": ["meta learning", "deep learning"], "abstract": "Meta-optimization is an effective approach that learns a shared set of parameters across tasks for parameter initialization in meta-learning.\nA key challenge for meta-optimization based approaches is to determine whether an initialization condition can be generalized to tasks with diverse distributions to accelerate learning. \nTo address this issue, we design a meta-gradient boosting framework that uses a base learner to learn shared information across tasks and a series of gradient-boosted modules to capture task-specific information to fit diverse distributions.\nWe evaluate the proposed model on both regression and classification tasks with multi-mode distributions. \nThe results demonstrate both the effectiveness of our model in modulating task-specific meta-learned priors and its advantages on multi-mode distributions.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dong|meta_gradient_boosting_neural_networks", "pdf": "/pdf/4943864d76084cbafbab40eeb5331907a152fa7f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LgDMzSSh2", "_bibtex": "@misc{\ndong2021meta,\ntitle={Meta Gradient Boosting Neural Networks},\nauthor={Manqing Dong and Lina Yao and Xianzhi Wang and Xiwei Xu and Liming Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=QcqsxI6rKDs}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "QcqsxI6rKDs", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2438/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2438/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2438/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2438/Authors|ICLR.cc/2021/Conference/Paper2438/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2438/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848359, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2438/-/Official_Comment"}}}, {"id": "7V3I9F5a247", "original": null, "number": 2, "cdate": 1606057094609, "ddate": null, "tcdate": 1606057094609, "tmdate": 1606057551270, "tddate": null, "forum": "QcqsxI6rKDs", "replyto": "3DhIID3GIlO", "invitation": "ICLR.cc/2021/Conference/Paper2438/-/Official_Comment", "content": {"title": "To reviewer 2 ", "comment": "Thank you for the detailed reviews and suggestions. \n\n**Baseline.** Regarding the choice of baselines, we believe some of the baselines suggested by the reviewer are unsuitable for our case. We present the reasons below:\n1. Regarding HSML, we did investigate HSML in our related work (in the 2nd paragraph of Related Work) but chose MMAML as a baseline instead of HSML, in light of that they share a similar spirit. We appreciate that you kindly suggest that \u201cHSML shows superior performance compared to MAML and a workshop version of MMAML\u201d. We will definitely look into the performance of HSML and add more experiments in the future version.\n2. Regarding probabilistic MAML, the compared method, LEO, is, actually, a probabilistic MAML and should suffice to serve as a representative of methods in the same category. Specially, LEO considers the multiple conditions by borrowing the idea of variational autoencoders and uses an encoder to learn the mean and variance for generating a weight matrix for the predictor. \n3. Proto-MAML is a metric-based approach for image classification. For reasons why we did not compare our work with metric-based methods, please see below (our response #4).\n4. We did not choose metric-based methods for comparison for two reasons. First, we aim to provide a general framework for solving a wide variety of tasks in this work. However, metric-based methods usually calculate the relationship between new samples and history samples in terms of a relation score or a probabilistic distribution (for classification), which is unsuitable for few-shot regression tasks because it is hard to learn from very few points to generate a wide range for the outputs. Second, our focus is the single initialization problem addressed by traditional MAML-based frameworks. Therefore, MAML-based frameworks naturally become the major baselines to compare. And we think the reason why \u2018metric-based methods superior other methods in few-shot image classification tasks\u2019 will be another interesting research topic, due to the learning strategy of the metric-based approaches. \n\n**meta-dataset** About the meta-dataset. In this work, we followed the previous settings (e.g., those in MMAML) to set up the multi-mode tasks. But we reckon that the meta-dataset could be a valuable source and would definitely consider it as a good option for few-shot image classification tasks.\n\n**MGB-1 vs. MAML** we also noticed the difference between MGB-1 vs. MAML in their regression and classification results. We have explained in the paper that \u201cMGB-1 can make only a slight improvement over MAML because images contain more complex information than real numbers\u201d for regression tasks. The MGB-1 with one gradient boost module can capture more information than MAML. The reason may be related to the network dimension and structure. So we also appreciate your comments about \u2018MAML with a comparable number of parameters\u2019. We will add some experiments about \u201cMAML with a comparable number of parameters\u201d in the future version.\n\n**MAML with a comparable number of parameters** We have set the same feature extractor for all methods in our current experimental settings to ensure a fair comparison. In further experiments, we will explore whether the improvement comes from the gradient boosting module or the enlarged network structure.\n\n**stddev of the classification task**: we did not present stddev for the classification task due to the limited space. We will add more experimental details regarding this in the future version. \n\n**RL experiments**: We will leave experiments on reinforcement learning to our future work.\n \n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2438/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2438/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta Gradient Boosting Neural Networks", "authorids": ["~Manqing_Dong1", "~Lina_Yao2", "xianzhi.wang@uts.edu.au", "xiwei.xu@data61.csiro.au", "liming.zhu@data61.csiro.au"], "authors": ["Manqing Dong", "Lina Yao", "Xianzhi Wang", "Xiwei Xu", "Liming Zhu"], "keywords": ["meta learning", "deep learning"], "abstract": "Meta-optimization is an effective approach that learns a shared set of parameters across tasks for parameter initialization in meta-learning.\nA key challenge for meta-optimization based approaches is to determine whether an initialization condition can be generalized to tasks with diverse distributions to accelerate learning. \nTo address this issue, we design a meta-gradient boosting framework that uses a base learner to learn shared information across tasks and a series of gradient-boosted modules to capture task-specific information to fit diverse distributions.\nWe evaluate the proposed model on both regression and classification tasks with multi-mode distributions. \nThe results demonstrate both the effectiveness of our model in modulating task-specific meta-learned priors and its advantages on multi-mode distributions.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dong|meta_gradient_boosting_neural_networks", "pdf": "/pdf/4943864d76084cbafbab40eeb5331907a152fa7f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LgDMzSSh2", "_bibtex": "@misc{\ndong2021meta,\ntitle={Meta Gradient Boosting Neural Networks},\nauthor={Manqing Dong and Lina Yao and Xianzhi Wang and Xiwei Xu and Liming Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=QcqsxI6rKDs}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "QcqsxI6rKDs", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2438/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2438/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2438/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2438/Authors|ICLR.cc/2021/Conference/Paper2438/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2438/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848359, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2438/-/Official_Comment"}}}, {"id": "A6rsDF8cFR", "original": null, "number": 4, "cdate": 1606057502131, "ddate": null, "tcdate": 1606057502131, "tmdate": 1606057502131, "tddate": null, "forum": "QcqsxI6rKDs", "replyto": "RtheyDjxFrf", "invitation": "ICLR.cc/2021/Conference/Paper2438/-/Official_Comment", "content": {"title": "To reviewer 3", "comment": "Thank you for your comments. Regarding Figure 4 (a), the model\u2019s performance exhibits more fluctuations when it is equipped with two or more weak learners as the base learner. This might be attributed to the updating strategy for the base learner and the following weak learners. For the case of two weak learners, the model\u2019s performance fluctuates in the first few learning epochs but ramps up rapidly afterward, indicating the weak learners can capture more sharing knowledge across tasks, although the knowledge may be corrupted by the \u201cdynamic\u201d updating strategy. Some previous theoretical studies found that the gradient for the earlier weak learner decays with the increasing number of gradient boost modules. Our other training results for the model with two weak learners as the base learner also fluctuate in the first few epochs. We will do more experiments to analyze how the contribution of each part changes during the training process in the future version. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2438/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2438/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta Gradient Boosting Neural Networks", "authorids": ["~Manqing_Dong1", "~Lina_Yao2", "xianzhi.wang@uts.edu.au", "xiwei.xu@data61.csiro.au", "liming.zhu@data61.csiro.au"], "authors": ["Manqing Dong", "Lina Yao", "Xianzhi Wang", "Xiwei Xu", "Liming Zhu"], "keywords": ["meta learning", "deep learning"], "abstract": "Meta-optimization is an effective approach that learns a shared set of parameters across tasks for parameter initialization in meta-learning.\nA key challenge for meta-optimization based approaches is to determine whether an initialization condition can be generalized to tasks with diverse distributions to accelerate learning. \nTo address this issue, we design a meta-gradient boosting framework that uses a base learner to learn shared information across tasks and a series of gradient-boosted modules to capture task-specific information to fit diverse distributions.\nWe evaluate the proposed model on both regression and classification tasks with multi-mode distributions. \nThe results demonstrate both the effectiveness of our model in modulating task-specific meta-learned priors and its advantages on multi-mode distributions.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dong|meta_gradient_boosting_neural_networks", "pdf": "/pdf/4943864d76084cbafbab40eeb5331907a152fa7f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LgDMzSSh2", "_bibtex": "@misc{\ndong2021meta,\ntitle={Meta Gradient Boosting Neural Networks},\nauthor={Manqing Dong and Lina Yao and Xianzhi Wang and Xiwei Xu and Liming Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=QcqsxI6rKDs}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "QcqsxI6rKDs", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2438/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2438/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2438/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2438/Authors|ICLR.cc/2021/Conference/Paper2438/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2438/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848359, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2438/-/Official_Comment"}}}, {"id": "dgdgiZjtZZH", "original": null, "number": 1, "cdate": 1603465461845, "ddate": null, "tcdate": 1603465461845, "tmdate": 1605024210789, "tddate": null, "forum": "QcqsxI6rKDs", "replyto": "QcqsxI6rKDs", "invitation": "ICLR.cc/2021/Conference/Paper2438/-/Official_Review", "content": {"title": "Marginal contribution", "review": "The authors propose a meta-gradient boosting framework that uses a base learner to learn shared information across tasks and gradient-boosted modules to capture task-specific information. The proposed approach is applied to various regression and classification tasks.\n\nTo me the motivation of using a gradient boosting framework remained unclear throughout the paper. It is stated in the last paragraph of the Introduction that \u2018Recent research.. with achieving low training errors\u2019. However, why this is important to the meta-learning problem remains unclear.\n\n\nIn Section 3, the model is not explained clearly:\n-\tWhat is a weak learner? In the introduction it is mentioned that \u2018Gradient boosting aims to build a new learner towards\u2026We call the learner for each step as weak learner.\u2019 What is a step here?\n-\t\u2018..the first few learners are regarded as the base learner for learning the shared information across tasks..\u2019 This was also not evident how it is designed to achieve this. Given that I have no prior on working with gradient boosting directly, I found the motivation and the method quite hard to follow.\n-\t\nSection 3.1. Second sentence: \u2018..where K is the number of adaptions\u2019-what is adaptions?\nWhat is the definition of a gradient boost?\nHow is $\\theta_k$ defined in equation (1)?\n\nSec 3.2:\nThe first sentence- what does it mean?\nIt is mentioned here that a first-order MAML type approach is taken to learn the \u2018base learner\u2019. But how the two goal of using gradient boost modules to ensure task-specific  information is achieved is not clear.\n\nFigure 2,3,  and 4: The axes and fonts are unreadable.\n\nIn Results, the authors state that \u2018 the results show that incorporating task identities can significantly improve the performance of multi-mode learning\u2019. It was not clear to me how this is true. First, how is task identity even abstracted or assigned here? What features of the approach are able to get this information is not clear.\n\nThe gain classification tasks also seems marginal compared to the other approaches. The key emphasis of the authors is on the importance of the approach on multi-mode distributions. It is not made clear why the approach would be suited to this setting. This is supported numerically only in the case of regression experiments.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2438/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2438/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta Gradient Boosting Neural Networks", "authorids": ["~Manqing_Dong1", "~Lina_Yao2", "xianzhi.wang@uts.edu.au", "xiwei.xu@data61.csiro.au", "liming.zhu@data61.csiro.au"], "authors": ["Manqing Dong", "Lina Yao", "Xianzhi Wang", "Xiwei Xu", "Liming Zhu"], "keywords": ["meta learning", "deep learning"], "abstract": "Meta-optimization is an effective approach that learns a shared set of parameters across tasks for parameter initialization in meta-learning.\nA key challenge for meta-optimization based approaches is to determine whether an initialization condition can be generalized to tasks with diverse distributions to accelerate learning. \nTo address this issue, we design a meta-gradient boosting framework that uses a base learner to learn shared information across tasks and a series of gradient-boosted modules to capture task-specific information to fit diverse distributions.\nWe evaluate the proposed model on both regression and classification tasks with multi-mode distributions. \nThe results demonstrate both the effectiveness of our model in modulating task-specific meta-learned priors and its advantages on multi-mode distributions.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dong|meta_gradient_boosting_neural_networks", "pdf": "/pdf/4943864d76084cbafbab40eeb5331907a152fa7f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LgDMzSSh2", "_bibtex": "@misc{\ndong2021meta,\ntitle={Meta Gradient Boosting Neural Networks},\nauthor={Manqing Dong and Lina Yao and Xianzhi Wang and Xiwei Xu and Liming Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=QcqsxI6rKDs}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "QcqsxI6rKDs", "replyto": "QcqsxI6rKDs", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2438/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538096331, "tmdate": 1606915806222, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2438/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2438/-/Official_Review"}}}, {"id": "RtheyDjxFrf", "original": null, "number": 2, "cdate": 1603659107242, "ddate": null, "tcdate": 1603659107242, "tmdate": 1605024210728, "tddate": null, "forum": "QcqsxI6rKDs", "replyto": "QcqsxI6rKDs", "invitation": "ICLR.cc/2021/Conference/Paper2438/-/Official_Review", "content": {"title": "This paper presents a gradient boosting framework for Meta-optimization, wherein an initialization condition can be generalized to different distributions related to different tasks. The main idea is learning a general base learner across all the distributions of different tasks, and later learning gradient boosted modules that are adjusted to each specific task. In this study, a deep neural net is used to build the ensemble of sub-networks.", "review": "This study is presented clearly, and the core idea is interesting. However, the presented novelty is limited to a globally (for all tasks) and locally (task-specific)  learning paradigm using a framework inspired by (Badirli et al., 2020). The authors have presented experimental results for both regression and classification setups, which are interesting.\nIn my opinion, the paper has relatively high quality and can be interesting for the ICLR community. \nOne question regarding Figure 4 (a), you have mentioned that adding more weak learners causes difficulties in capturing multi-mode patterns, but from this figure, one can see that it is not completely true for the early epochs (before 400), comparing the two weak learners versus one weak learner case and it seems more like fluctuations, how would you explain it. \n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2438/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2438/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta Gradient Boosting Neural Networks", "authorids": ["~Manqing_Dong1", "~Lina_Yao2", "xianzhi.wang@uts.edu.au", "xiwei.xu@data61.csiro.au", "liming.zhu@data61.csiro.au"], "authors": ["Manqing Dong", "Lina Yao", "Xianzhi Wang", "Xiwei Xu", "Liming Zhu"], "keywords": ["meta learning", "deep learning"], "abstract": "Meta-optimization is an effective approach that learns a shared set of parameters across tasks for parameter initialization in meta-learning.\nA key challenge for meta-optimization based approaches is to determine whether an initialization condition can be generalized to tasks with diverse distributions to accelerate learning. \nTo address this issue, we design a meta-gradient boosting framework that uses a base learner to learn shared information across tasks and a series of gradient-boosted modules to capture task-specific information to fit diverse distributions.\nWe evaluate the proposed model on both regression and classification tasks with multi-mode distributions. \nThe results demonstrate both the effectiveness of our model in modulating task-specific meta-learned priors and its advantages on multi-mode distributions.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dong|meta_gradient_boosting_neural_networks", "pdf": "/pdf/4943864d76084cbafbab40eeb5331907a152fa7f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LgDMzSSh2", "_bibtex": "@misc{\ndong2021meta,\ntitle={Meta Gradient Boosting Neural Networks},\nauthor={Manqing Dong and Lina Yao and Xianzhi Wang and Xiwei Xu and Liming Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=QcqsxI6rKDs}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "QcqsxI6rKDs", "replyto": "QcqsxI6rKDs", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2438/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538096331, "tmdate": 1606915806222, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2438/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2438/-/Official_Review"}}}, {"id": "fMtoqy3yqt", "original": null, "number": 3, "cdate": 1603853595813, "ddate": null, "tcdate": 1603853595813, "tmdate": 1605024210661, "tddate": null, "forum": "QcqsxI6rKDs", "replyto": "QcqsxI6rKDs", "invitation": "ICLR.cc/2021/Conference/Paper2438/-/Official_Review", "content": {"title": "An effective yet quite restricted approach", "review": "The paper proposed a method to incorporate neural networks into gradient boosting for meta-learning with a limited number of samples in each task. \n\nThe simulated experiments, where the generated tasks contain four continuous function, demonstrated that the proposed model outperformed MAML and its related variants, and on the classification task, it performed similarity to a previously proposed method MMAML.\n\nIn general, I am very interested in seeing neural networks being combined with boosting algorithms, but I do have quite a few questions.\n\n1. The paper claimed that the learning and updating strategy proposed in the method ensured a weak base learner. I have to say that I am not satisfied by the claim. One can simply construct a linearly-separable dataset, which doesn't contribute to the overall tasks, and it only takes few updates for a two-layer neural network to converge, then the learner itself is not weak anymore. In addition, it adds noise to other tasks in the task set.\n\n2. Normally, when a weak learner is learnt, a 1-D line search is conducted to find the optimal contribution of this particular learner. Although, shrinkage is usually applied in practice to anneal the contribution of new learners as a regularisation method to avoid overfitting. The paper seemed to have ignored the line-search part which can be done efficiently in 1D, whilst only the shrinkage was applied in the modelling, and I was wondering if there was a specific reason for that. \n\n3. In terms of the application of the proposed algorithm, I think it is rather limited. My understanding is that, although the proposed algorithm worked better than MAML and its variants on simulated data, the algorithm at the same time involves many hyperparameters, including (1) the design of the base feature extractor, (2) the design of gradient boost modules, (3) the boosting rates, and the annealing of them, (4) number of local update steps,  (5) number of global update steps, and many other hyperparams regarding to the training of neural networks. The first two hyperparameters or designs are critical in the success of the gradient boosting algorithm as weak learners are needed for learning so that the gradient is informative for the subsequent learners. Tuning those hyperparameters could be a time-consuming and also non-trivial task itself already, compared to the MAML algorithm itself, or generally multi-task learning approaches for meta learning, this algorithm doesn't seem to be outstanding. \n\n4. A fair comparison, IMO, could be to use the same feature extractor for producing vector representations of samples, and then directly apply gradient boosting with trees for meta learning. As mentioned in the paper, the authors also agreed that the feature extractors themselves have a huge impact on the final performance, therefore, it might be a good practice to check how well gradient boosting is able to handle meta-learning on top of extracted features.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2438/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2438/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta Gradient Boosting Neural Networks", "authorids": ["~Manqing_Dong1", "~Lina_Yao2", "xianzhi.wang@uts.edu.au", "xiwei.xu@data61.csiro.au", "liming.zhu@data61.csiro.au"], "authors": ["Manqing Dong", "Lina Yao", "Xianzhi Wang", "Xiwei Xu", "Liming Zhu"], "keywords": ["meta learning", "deep learning"], "abstract": "Meta-optimization is an effective approach that learns a shared set of parameters across tasks for parameter initialization in meta-learning.\nA key challenge for meta-optimization based approaches is to determine whether an initialization condition can be generalized to tasks with diverse distributions to accelerate learning. \nTo address this issue, we design a meta-gradient boosting framework that uses a base learner to learn shared information across tasks and a series of gradient-boosted modules to capture task-specific information to fit diverse distributions.\nWe evaluate the proposed model on both regression and classification tasks with multi-mode distributions. \nThe results demonstrate both the effectiveness of our model in modulating task-specific meta-learned priors and its advantages on multi-mode distributions.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dong|meta_gradient_boosting_neural_networks", "pdf": "/pdf/4943864d76084cbafbab40eeb5331907a152fa7f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LgDMzSSh2", "_bibtex": "@misc{\ndong2021meta,\ntitle={Meta Gradient Boosting Neural Networks},\nauthor={Manqing Dong and Lina Yao and Xianzhi Wang and Xiwei Xu and Liming Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=QcqsxI6rKDs}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "QcqsxI6rKDs", "replyto": "QcqsxI6rKDs", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2438/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538096331, "tmdate": 1606915806222, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2438/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2438/-/Official_Review"}}}], "count": 10}