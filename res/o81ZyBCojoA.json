{"notes": [{"id": "o81ZyBCojoA", "original": "9pRv9wTSE-9", "number": 2301, "cdate": 1601308253631, "ddate": null, "tcdate": 1601308253631, "tmdate": 1613089398401, "tddate": null, "forum": "o81ZyBCojoA", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning", "authorids": ["~Ren_Wang1", "~Kaidi_Xu1", "~Sijia_Liu1", "~Pin-Yu_Chen1", "~Tsui-Wei_Weng1", "~Chuang_Gan1", "~Meng_Wang4"], "authors": ["Ren Wang", "Kaidi Xu", "Sijia Liu", "Pin-Yu Chen", "Tsui-Wei Weng", "Chuang Gan", "Meng Wang"], "keywords": [], "abstract": "Model-agnostic meta-learning (MAML) has emerged as one of the most successful meta-learning techniques in few-shot learning. It enables us to learn a $\\textit{meta-initialization}$ of model parameters (that we call $\\textit{meta-model}$) to rapidly adapt to new tasks using a small amount of labeled training data. Despite the generalization power of the meta-model, it remains elusive that how $\\textit{adversarial robustness}$ can be maintained by MAML in few-shot learning. In addition to generalization, robustness is also desired for a meta-model to defend adversarial examples (attacks). Toward promoting adversarial robustness in MAML, we first study $\\textit{when}$ a robustness-promoting regularization should be incorporated, given the fact that MAML adopts a bi-level (fine-tuning vs. meta-update) learning procedure. We show that robustifying the meta-update stage is sufficient to make robustness adapted to the task-specific fine-tuning stage even if the latter uses a standard training protocol. We also make additional justification on the acquired robustness adaptation by peering into the interpretability of neurons' activation maps. Furthermore, we investigate $\\textit{how}$ robust regularization can $\\textit{efficiently}$ be designed in MAML. We propose a general but easily-optimized robustness-regularized meta-learning framework, which allows the use of unlabeled data augmentation, fast adversarial attack generation, and computationally-light fine-tuning. In particular, we for the first time show that the auxiliary contrastive learning task can enhance the adversarial robustness of MAML. Finally, extensive experiments are conducted to demonstrate the effectiveness of our proposed methods in robust few-shot learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|on_fast_adversarial_robustness_adaptation_in_modelagnostic_metalearning", "pdf": "/pdf/c29f970186b2b2658cd52eea7aac2b5266c649f4.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021on,\ntitle={On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning},\nauthor={Ren Wang and Kaidi Xu and Sijia Liu and Pin-Yu Chen and Tsui-Wei Weng and Chuang Gan and Meng Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=o81ZyBCojoA}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "53t9z6n6hII", "original": null, "number": 1, "cdate": 1610040453161, "ddate": null, "tcdate": 1610040453161, "tmdate": 1610474055557, "tddate": null, "forum": "o81ZyBCojoA", "replyto": "o81ZyBCojoA", "invitation": "ICLR.cc/2021/Conference/Paper2301/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "The reviewers\u2019 main concern was a lack of experiments, and additional experiments were provided by the authors.  While the rebuttal was not addressed by the reviewers, the AC feels that the rebuttal did address a number of experimental concerns well enough to justify accepting this paper."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning", "authorids": ["~Ren_Wang1", "~Kaidi_Xu1", "~Sijia_Liu1", "~Pin-Yu_Chen1", "~Tsui-Wei_Weng1", "~Chuang_Gan1", "~Meng_Wang4"], "authors": ["Ren Wang", "Kaidi Xu", "Sijia Liu", "Pin-Yu Chen", "Tsui-Wei Weng", "Chuang Gan", "Meng Wang"], "keywords": [], "abstract": "Model-agnostic meta-learning (MAML) has emerged as one of the most successful meta-learning techniques in few-shot learning. It enables us to learn a $\\textit{meta-initialization}$ of model parameters (that we call $\\textit{meta-model}$) to rapidly adapt to new tasks using a small amount of labeled training data. Despite the generalization power of the meta-model, it remains elusive that how $\\textit{adversarial robustness}$ can be maintained by MAML in few-shot learning. In addition to generalization, robustness is also desired for a meta-model to defend adversarial examples (attacks). Toward promoting adversarial robustness in MAML, we first study $\\textit{when}$ a robustness-promoting regularization should be incorporated, given the fact that MAML adopts a bi-level (fine-tuning vs. meta-update) learning procedure. We show that robustifying the meta-update stage is sufficient to make robustness adapted to the task-specific fine-tuning stage even if the latter uses a standard training protocol. We also make additional justification on the acquired robustness adaptation by peering into the interpretability of neurons' activation maps. Furthermore, we investigate $\\textit{how}$ robust regularization can $\\textit{efficiently}$ be designed in MAML. We propose a general but easily-optimized robustness-regularized meta-learning framework, which allows the use of unlabeled data augmentation, fast adversarial attack generation, and computationally-light fine-tuning. In particular, we for the first time show that the auxiliary contrastive learning task can enhance the adversarial robustness of MAML. Finally, extensive experiments are conducted to demonstrate the effectiveness of our proposed methods in robust few-shot learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|on_fast_adversarial_robustness_adaptation_in_modelagnostic_metalearning", "pdf": "/pdf/c29f970186b2b2658cd52eea7aac2b5266c649f4.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021on,\ntitle={On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning},\nauthor={Ren Wang and Kaidi Xu and Sijia Liu and Pin-Yu Chen and Tsui-Wei Weng and Chuang Gan and Meng Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=o81ZyBCojoA}\n}"}, "tags": [], "invitation": {"reply": {"forum": "o81ZyBCojoA", "replyto": "o81ZyBCojoA", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040453148, "tmdate": 1610474055541, "id": "ICLR.cc/2021/Conference/Paper2301/-/Decision"}}}, {"id": "zjbmixlcD4p", "original": null, "number": 4, "cdate": 1604214501790, "ddate": null, "tcdate": 1604214501790, "tmdate": 1606807302885, "tddate": null, "forum": "o81ZyBCojoA", "replyto": "o81ZyBCojoA", "invitation": "ICLR.cc/2021/Conference/Paper2301/-/Official_Review", "content": {"title": "explores adversarial robustness and contrastive learning of MAML", "review": "Summary\n\nThis paper investigates the adversarial robustness of model agnostic meta-learning (MAML). Adversarial robustness can be added to MAML in two places, meta-update stage and and fine-tune stage. It shows that robustifying the meta-update stage via fast attack generation method is sufficient to achieve fast robustness adaptation without losing generalization and computation efficiency in general. The paper also demonstrates that unlabeled data can help using contrastive representation learning to improve generalization and robustness. \n\nStrengths\n\nIt shows:\n1. adversarial training with the projected gradient descent (PGD) attack generation method is only needed at meta-update stage for MAML.\n\n2. computationally efficient adversarial training methods such as  almost-no-inner-loop (ANIL) fine-tuning strategy and FGSM are enough.\n\n3. training with unlabeled data using contrastive learning further improves generalization and robustness. \n\nWeaknesses\n\nAdversarial robustness of MAML has been studied in (Goldblum et al., 2019). This paper provides further investigation. The study seems to be straightforward and incremental. \n\nAll results are based on miniImageNet. MAML has also been applied in reinforcement learning. It is not clear the results in this paper carries out to the RL setting.\n\nDecision\n\nThe paper improves MAML with adversarial training at meta-update stage and unlabeled data through contrastive learning. However, the studies and techniques seem to be incremental compared with related work. \n\n\n=====POST-REBUTTAL COMMENTS======== \n\nThe authors provided additional experiments on CIFAR-FS and Omniglot. The results show that their methods outperform the baseline method adversarial querying (AQ). \n\nIt is still not clear whether the methods work in the reinforcement learning setting. As the original MAML paper shows that MAML works for RL problems, it is important to address this question. Otherwise, it could potentially limits the applicability of the proposed methods in the paper.\n\nI still have concerns over their novelty and the significance of their contributions. \n\nOverall, I applaud their effort to address my comments. I am more positive on the paper than before. My rating is a solid 6. The paper in the current stage does not warrant a higher rating for ICLR in my opinion.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2301/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2301/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning", "authorids": ["~Ren_Wang1", "~Kaidi_Xu1", "~Sijia_Liu1", "~Pin-Yu_Chen1", "~Tsui-Wei_Weng1", "~Chuang_Gan1", "~Meng_Wang4"], "authors": ["Ren Wang", "Kaidi Xu", "Sijia Liu", "Pin-Yu Chen", "Tsui-Wei Weng", "Chuang Gan", "Meng Wang"], "keywords": [], "abstract": "Model-agnostic meta-learning (MAML) has emerged as one of the most successful meta-learning techniques in few-shot learning. It enables us to learn a $\\textit{meta-initialization}$ of model parameters (that we call $\\textit{meta-model}$) to rapidly adapt to new tasks using a small amount of labeled training data. Despite the generalization power of the meta-model, it remains elusive that how $\\textit{adversarial robustness}$ can be maintained by MAML in few-shot learning. In addition to generalization, robustness is also desired for a meta-model to defend adversarial examples (attacks). Toward promoting adversarial robustness in MAML, we first study $\\textit{when}$ a robustness-promoting regularization should be incorporated, given the fact that MAML adopts a bi-level (fine-tuning vs. meta-update) learning procedure. We show that robustifying the meta-update stage is sufficient to make robustness adapted to the task-specific fine-tuning stage even if the latter uses a standard training protocol. We also make additional justification on the acquired robustness adaptation by peering into the interpretability of neurons' activation maps. Furthermore, we investigate $\\textit{how}$ robust regularization can $\\textit{efficiently}$ be designed in MAML. We propose a general but easily-optimized robustness-regularized meta-learning framework, which allows the use of unlabeled data augmentation, fast adversarial attack generation, and computationally-light fine-tuning. In particular, we for the first time show that the auxiliary contrastive learning task can enhance the adversarial robustness of MAML. Finally, extensive experiments are conducted to demonstrate the effectiveness of our proposed methods in robust few-shot learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|on_fast_adversarial_robustness_adaptation_in_modelagnostic_metalearning", "pdf": "/pdf/c29f970186b2b2658cd52eea7aac2b5266c649f4.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021on,\ntitle={On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning},\nauthor={Ren Wang and Kaidi Xu and Sijia Liu and Pin-Yu Chen and Tsui-Wei Weng and Chuang Gan and Meng Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=o81ZyBCojoA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "o81ZyBCojoA", "replyto": "o81ZyBCojoA", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2301/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538099516, "tmdate": 1606915787586, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2301/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2301/-/Official_Review"}}}, {"id": "AJmMnk1mI_", "original": null, "number": 2, "cdate": 1605822314261, "ddate": null, "tcdate": 1605822314261, "tmdate": 1606106056763, "tddate": null, "forum": "o81ZyBCojoA", "replyto": "o81ZyBCojoA", "invitation": "ICLR.cc/2021/Conference/Paper2301/-/Official_Comment", "content": {"title": "Response Highlights", "comment": "We thank all reviewers for their time and efforts in reviewing the paper and providing helpful suggestions/comments. The major changes have been highlighted in blue in the revision. In what follows, we highlighted our contributions and newly added experiments suggested by reviewers. Details will be provided in point-wise responses.\n\n**Contribution**\n\nIn our paper, we propose a general framework for robustness-regularized meta-learning (Section 2). We also provide insights and step-by-step investigations to show when to incorporate robust training and why it works (Section 3). Noticing the long running time of the MAML inner-loop update and adversarial example generation for the outer-loop, we have made additional contributions on scalable training (Section 4). Finally, we show that incorporating unlabeled data and contrastive learning can further enhance the model\u2019s robustness (Section 5).\n\n**Added experiments**\n1. In the revised paper, we added new experiments on CIFAR-FS [1] (See Tables S2, S3) and Omniglot [2] (See Figure S3). The results justify the improvement of our approaches over the baseline method across various datasets.\n2. We added new experiments on various few-shot settings, e.g. 1-Shot 5-Way and 5-Shot 5-Way on CIFAR-FS (See Tables S2, S3), and 1-Shot (5, 10,15,20)-Way on Omniglot (See Figure S3). The results show that our proposed methods keep effective in different few-shot learning regimes.\n\n[1] Bertinetto et al., \u201cMeta-learning with differentiable closed-form solvers\u201d, 2018\n\n[2] Lake et al., \u201cHuman-level concept learning through probabilistic program induction\u201d, 2015"}, "signatures": ["ICLR.cc/2021/Conference/Paper2301/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2301/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning", "authorids": ["~Ren_Wang1", "~Kaidi_Xu1", "~Sijia_Liu1", "~Pin-Yu_Chen1", "~Tsui-Wei_Weng1", "~Chuang_Gan1", "~Meng_Wang4"], "authors": ["Ren Wang", "Kaidi Xu", "Sijia Liu", "Pin-Yu Chen", "Tsui-Wei Weng", "Chuang Gan", "Meng Wang"], "keywords": [], "abstract": "Model-agnostic meta-learning (MAML) has emerged as one of the most successful meta-learning techniques in few-shot learning. It enables us to learn a $\\textit{meta-initialization}$ of model parameters (that we call $\\textit{meta-model}$) to rapidly adapt to new tasks using a small amount of labeled training data. Despite the generalization power of the meta-model, it remains elusive that how $\\textit{adversarial robustness}$ can be maintained by MAML in few-shot learning. In addition to generalization, robustness is also desired for a meta-model to defend adversarial examples (attacks). Toward promoting adversarial robustness in MAML, we first study $\\textit{when}$ a robustness-promoting regularization should be incorporated, given the fact that MAML adopts a bi-level (fine-tuning vs. meta-update) learning procedure. We show that robustifying the meta-update stage is sufficient to make robustness adapted to the task-specific fine-tuning stage even if the latter uses a standard training protocol. We also make additional justification on the acquired robustness adaptation by peering into the interpretability of neurons' activation maps. Furthermore, we investigate $\\textit{how}$ robust regularization can $\\textit{efficiently}$ be designed in MAML. We propose a general but easily-optimized robustness-regularized meta-learning framework, which allows the use of unlabeled data augmentation, fast adversarial attack generation, and computationally-light fine-tuning. In particular, we for the first time show that the auxiliary contrastive learning task can enhance the adversarial robustness of MAML. Finally, extensive experiments are conducted to demonstrate the effectiveness of our proposed methods in robust few-shot learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|on_fast_adversarial_robustness_adaptation_in_modelagnostic_metalearning", "pdf": "/pdf/c29f970186b2b2658cd52eea7aac2b5266c649f4.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021on,\ntitle={On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning},\nauthor={Ren Wang and Kaidi Xu and Sijia Liu and Pin-Yu Chen and Tsui-Wei Weng and Chuang Gan and Meng Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=o81ZyBCojoA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "o81ZyBCojoA", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2301/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2301/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2301/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2301/Authors|ICLR.cc/2021/Conference/Paper2301/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2301/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850010, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2301/-/Official_Comment"}}}, {"id": "EtfHgnsVZZt", "original": null, "number": 9, "cdate": 1605827454325, "ddate": null, "tcdate": 1605827454325, "tmdate": 1606060100971, "tddate": null, "forum": "o81ZyBCojoA", "replyto": "gImFr42ZFu0", "invitation": "ICLR.cc/2021/Conference/Paper2301/-/Official_Comment", "content": {"title": "Our Response to Reviewer 2", "comment": "We thank the review for the encouraging feedback. We list our response below. Please don\u2019t hesitate to let us know for any additional comments.\n\n**Q1:** *The claims would be more convincing with more datasets.*\n\n**A1:** Thank you for the suggestion. We conducted additional experiments on CIFAR-FS [1] (Table S2 and S3 in the revised paper) and Omniglot [2]  (Figure S3 in the revised paper under various few-shot settings). The results show that our methods work well on various datasets and outperform the baseline methods. For example, as shown in Table S2 and S3 (copied below), the use of unlabeled data augmentation (R-MAML$_\\mathrm{out}$-CL) on CIFAR-FS can provide 10% (or 5.6%) standard accuracy (SA) improvement and 3% (or 1.3%) robust accuracy (RA) improvement over AQ under the 1-Shot 5-Way (or 5-Shot 5-Way) setting.\n\n**Table S2.** SA/RA performance of our proposed methods on CIFAR-FS (1-Shot 5-Way, $\\epsilon = 8$).\n\n|| Standard Accuracy (SA) &nbsp; | Robust Accuracy (RA) |\n|:-:|:-:|:-:|\n|MAML|**51.07%**|0.235%|\n|AQ|31.25%|26.34%|\n|R-MAML$_\\mathrm{out}$(AT) (ours)|39.76%|26.15%|\n|R-MAML$_\\mathrm{out}$(TRADES) (ours)|39.76%|26.15%|\n|R-MAML$_\\mathrm{out}$-TRADES (ours)|40.59%|28.06%|\n|R-MAML$_\\mathrm{out}$-CL (ours)|41.25%|**29.33%**|\n\n**Table S3.** SA/RA performance of our proposed methods on CIFAR-FS (5-Shot 5-Way, $\\epsilon = 8$).\n\n|| Standard Accuracy (SA) &nbsp; | Robust Accuracy (RA) |\n|:-:|:-:|:-:|\n|MAML|**67.2%**|0.225%|\n|AQ|52.32%|33.96%|\n|R-MAML$_\\mathrm{out}$(AT) (ours)|57.18%|32.62%|\n|R-MAML$_\\mathrm{out}$(TRADES) (ours)|57.46%|34.72%|\n|R-MAML$_\\mathrm{out}$-TRADES (ours)|57.62%|34.76%|\n|R-MAML$_\\mathrm{out}$-CL (ours)|57.95%|**35.3%**|\n\n[1] Bertinetto et al., \u201cMeta-learning with differentiable closed-form solvers\u201d, 2018\n\n[2] Lake et al., \u201cHuman-level concept learning through probabilistic program induction\u201d, 2015\n\n**Q2:** *The key point is not properly emphasized. For example, Section 4 and Section 5.1 (TRADES) do not provide very important insights compared with Section 5.2 (CL) \u2014 maybe Section 5.2 could be extended further with some other contents deferred in the Appendix.*\n\n**A2:** Thanks for the suggestion. We would like to re-emphasize the important insights from Sec. 4 and Sec. 5.1 (TRADES).\n\n1. The studies in Sec. 4 were motivated to address the problem \u201chow to efficiently train robustness-regularized MAML?\u201d. This is a critical problem since both MAML and adversarial training take extensive computational complexities, where MAML is in the form of bi-level optimization and adversarial training is given by a min-max optimization problem. We addressed this problem by leveraging 1) fast adversarial example generation (namely, FGSM), and 2) partial model fine-tuning (namely, ANIL). We showed in Sec. 4 that both FGSM and ANIL can improve the computation efficiency of robust MAML, which takes the similar training cost to the standard MAML (Table 2). Moreover, compared to ANIL, FGSM-enabled robust MAML yields the most graceful tradeoff between computation efficiency, robustness and accuracy.\n\n2. The studies in Sec. 5.1 (TRADES) is a \u2018must-try\u2019 step when generalizing robustness-regularized MAML from supervision to semi-supervision, since TRADES is directly applicable to data augmentation and has been used in state-of-the-art adversarial defenses [3,4]. Indeed, we showed that for TRADES-enabled MAML that uses unlabeled data augmentation can improve the robustness adaptation in MAML (Figure 3). Furthermore, TRADES-enabled MAML provides us a semi-supervised baseline when comparing it with CL-enabled MAML. To the best of our knowledge, the semi-supervised robustness-promoting MAML has not yet been studied. \n\n3. In addition to Sec. 4 and Sec. 5, Sec. 2 and 3 also offer us important insights. \n- In Sec. 2, we propose a principled problem formulation  (3), which covers the baseline approach adversarial querying (AQ) (Goldblum et al., 2019) as a special case. As we clarified in the last paragraph of page 5, our proposed problem is simplified to AQ as  $\\gamma_{\\mathrm{in}} = 0$ and  $\\gamma_{\\mathrm{out}} = \\infty $. The formulation (3) also motivates us to \u2018when\u2019 to incorporate robustness regularization in MAML (inner level vs. outer level). \n- In Sec. 3,  we show that it is sufficient to promote adversarial robustness at the meta-update level. Most importantly, we explain our conclusion from two insightful perspectives, 1) explanation of learned neuron\u2019s representation; and 2) resilience to different fine-tuning schemes at the meta-testing phase.\n\n[3] Uesato, Jonathan, et al. \"Are labels required for improving adversarial robustness?.\" arXiv preprint arXiv:1905.13725 (2019).\n\n[4] Zhang, Hongyang, et al. \"Theoretically principled trade-off between robustness and accuracy.\" arXiv preprint arXiv:1901.08573 (2019)."}, "signatures": ["ICLR.cc/2021/Conference/Paper2301/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2301/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning", "authorids": ["~Ren_Wang1", "~Kaidi_Xu1", "~Sijia_Liu1", "~Pin-Yu_Chen1", "~Tsui-Wei_Weng1", "~Chuang_Gan1", "~Meng_Wang4"], "authors": ["Ren Wang", "Kaidi Xu", "Sijia Liu", "Pin-Yu Chen", "Tsui-Wei Weng", "Chuang Gan", "Meng Wang"], "keywords": [], "abstract": "Model-agnostic meta-learning (MAML) has emerged as one of the most successful meta-learning techniques in few-shot learning. It enables us to learn a $\\textit{meta-initialization}$ of model parameters (that we call $\\textit{meta-model}$) to rapidly adapt to new tasks using a small amount of labeled training data. Despite the generalization power of the meta-model, it remains elusive that how $\\textit{adversarial robustness}$ can be maintained by MAML in few-shot learning. In addition to generalization, robustness is also desired for a meta-model to defend adversarial examples (attacks). Toward promoting adversarial robustness in MAML, we first study $\\textit{when}$ a robustness-promoting regularization should be incorporated, given the fact that MAML adopts a bi-level (fine-tuning vs. meta-update) learning procedure. We show that robustifying the meta-update stage is sufficient to make robustness adapted to the task-specific fine-tuning stage even if the latter uses a standard training protocol. We also make additional justification on the acquired robustness adaptation by peering into the interpretability of neurons' activation maps. Furthermore, we investigate $\\textit{how}$ robust regularization can $\\textit{efficiently}$ be designed in MAML. We propose a general but easily-optimized robustness-regularized meta-learning framework, which allows the use of unlabeled data augmentation, fast adversarial attack generation, and computationally-light fine-tuning. In particular, we for the first time show that the auxiliary contrastive learning task can enhance the adversarial robustness of MAML. Finally, extensive experiments are conducted to demonstrate the effectiveness of our proposed methods in robust few-shot learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|on_fast_adversarial_robustness_adaptation_in_modelagnostic_metalearning", "pdf": "/pdf/c29f970186b2b2658cd52eea7aac2b5266c649f4.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021on,\ntitle={On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning},\nauthor={Ren Wang and Kaidi Xu and Sijia Liu and Pin-Yu Chen and Tsui-Wei Weng and Chuang Gan and Meng Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=o81ZyBCojoA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "o81ZyBCojoA", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2301/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2301/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2301/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2301/Authors|ICLR.cc/2021/Conference/Paper2301/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2301/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850010, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2301/-/Official_Comment"}}}, {"id": "L2-NLAN5c4U", "original": null, "number": 8, "cdate": 1605826997440, "ddate": null, "tcdate": 1605826997440, "tmdate": 1605990539680, "tddate": null, "forum": "o81ZyBCojoA", "replyto": "fpH2KlYuoVd", "invitation": "ICLR.cc/2021/Conference/Paper2301/-/Official_Comment", "content": {"title": "Our Response to Reviewer 4 - Part 1", "comment": "We thank the reviewer for the suggestions and comments. We reply individually to each raised point below. Please feel free to let us know if you have additional comments.\n\n**Q1:** *Marginal novelty and lacks theoretical analysis.*\n\n**A1:** Even if our conclusion is drawn based on empirical results, we do not think that this limits our novelty. In fact, we have tried our best to offer motivations and insights behind our approach and empirical results. For example, \n- In Sec. 3, we have justified our conclusion on the sufficiency of adversarial robustness at meta-update level from two insightful perspectives, 1) explanation of learned neuron\u2019s representation; and 2) resilience to different fine-tuning schemes at the meta-testing phase. \n- In Sec. 4, we contributed to scalable training. Computation efficiency is critical since both robust training (a min-max optimization problem) and MAML (a bi-level optimization problem) are involved in robust MAML. We found 1) fast (one-step) adversarial example generation and 2) partial model training (only fine-tuning the classifier\u2019s head) effective in training acceleration (30% speed-up) without losing much performance. \n- In Sec. 5, we have generalized the robust MAML to the semi-supervised setting. We have shown that the use of unlabeled data augmentation, particularly introducing an auxiliary contrastive learning task, can provide additional benefits on adversarial robustness of MAML in the low data regime: 2% robust accuracy improvement and 9% standard accuracy improvement over the baseline method (Table 3). Additional justifications on CIFAR-FS have also been provided in Table S2 and S3; see the revised paper or Q2-A2.\n\nOn the other hand, spurred by the reviewer\u2019s suggestion, we review more literature to track the state-of-the-art theoretical analysis in solving problems of MAML and adversarial training (AT). Unfortunately, both directions have very limited progress in theory, since both MAML and AT are in the form of quite challenging optimization problems, where the former is given by a bi-level optimization problem and the latter is given by a min-max optimization problem.  The existing theoretical analysis for MAML and AT [1,2,3] is restricted to the convergence rate of bi-level and min-max optimization algorithms under strong assumptions, e.g. convexity required for the inner-level optimization problem.  What is more, our setting becomes more difficult to analyze as AT (min-max optimization) is embedded in MAML (bi-level optimization). Thus, although having theoretical analysis is attractive, it is much beyond the scope of our work. We will add this discussion in a further work session. \n\n[1] Hong et al., \u201cA Two-Timescale Framework for Bilevel Optimization: Complexity Analysis and Application to Actor-Critic\u201d, 2020\n\n[2] Fallah et al., \"On the convergence theory of gradient-based model-agnostic meta-learning algorithms.\" International Conference on Artificial Intelligence and Statistics. 2020.\n\n[3] Gao, Ruiqi, et al. \"Convergence of adversarial training in overparametrized neural networks.\" Advances in Neural Information Processing Systems. 2019.\n\n**Q2:** *The experiments only are conducted on few-shot image classification tasks on miniImageNet dataset. This makes this conclusion lack sufficient credibility.*\n\n**A2:** Thank you for the suggestion. We conducted additional experiments on CIFAR-FS [4] (Table S2 and S3) and Omniglot [5]  (Figure S3 in our revised paper across different few-shot setups). The results show that our methods work well on various datasets and outperform the baseline methods. For example, as shown in Table S2 and S3, the use of unlabeled data augmentation (R-MAML$_\\mathrm{out}$-CL) on CIFAR-FS can provide 10% (or 5.6%) standard accuracy (SA) improvement and 3% (or 1.3%) robust accuracy (RA) improvement over AQ under the 1-Shot 5-Way (or 5-Shot 5-Way) setting.\n\n**Table S2.** SA/RA performance of our proposed methods on CIFAR-FS (1-Shot 5-Way, $\\epsilon = 8$).\n\n|| Standard Accuracy (SA) &nbsp; | Robust Accuracy (RA) |\n|:-:|:-:|:-:|\n|MAML|**51.07%**|0.235%|\n|AQ|31.25%|26.34%|\n|R-MAML$_\\mathrm{out}$(AT) (ours)|39.76%|26.15%|\n|R-MAML$_\\mathrm{out}$(TRADES) (ours)|39.76%|26.15%|\n|R-MAML$_\\mathrm{out}$-TRADES (ours)|40.59%|28.06%|\n|R-MAML$_\\mathrm{out}$-CL (ours)|41.25%|**29.33%**|\n\n**Table S3.** SA/RA performance of our proposed methods on CIFAR-FS (5-Shot 5-Way, $\\epsilon = 8$).\n\n|| Standard Accuracy (SA) &nbsp; | Robust Accuracy (RA) |\n|:-:|:-:|:-:|\n|MAML|**67.2%**|0.225%|\n|AQ|52.32%|33.96%|\n|R-MAML$_\\mathrm{out}$(AT) (ours)|57.18%|32.62%|\n|R-MAML$_\\mathrm{out}$(TRADES) (ours)|57.46%|34.72%|\n|R-MAML$_\\mathrm{out}$-TRADES (ours)|57.62%|34.76%|\n|R-MAML$_\\mathrm{out}$-CL (ours)|57.95%|**35.3%**|\n\n[4] Bertinetto et al., \u201cMeta-learning with differentiable closed-form solvers\u201d, 2018\n[5] Lake et al., \u201cHuman-level concept learning through probabilistic program induction\u201d, 2015"}, "signatures": ["ICLR.cc/2021/Conference/Paper2301/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2301/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning", "authorids": ["~Ren_Wang1", "~Kaidi_Xu1", "~Sijia_Liu1", "~Pin-Yu_Chen1", "~Tsui-Wei_Weng1", "~Chuang_Gan1", "~Meng_Wang4"], "authors": ["Ren Wang", "Kaidi Xu", "Sijia Liu", "Pin-Yu Chen", "Tsui-Wei Weng", "Chuang Gan", "Meng Wang"], "keywords": [], "abstract": "Model-agnostic meta-learning (MAML) has emerged as one of the most successful meta-learning techniques in few-shot learning. It enables us to learn a $\\textit{meta-initialization}$ of model parameters (that we call $\\textit{meta-model}$) to rapidly adapt to new tasks using a small amount of labeled training data. Despite the generalization power of the meta-model, it remains elusive that how $\\textit{adversarial robustness}$ can be maintained by MAML in few-shot learning. In addition to generalization, robustness is also desired for a meta-model to defend adversarial examples (attacks). Toward promoting adversarial robustness in MAML, we first study $\\textit{when}$ a robustness-promoting regularization should be incorporated, given the fact that MAML adopts a bi-level (fine-tuning vs. meta-update) learning procedure. We show that robustifying the meta-update stage is sufficient to make robustness adapted to the task-specific fine-tuning stage even if the latter uses a standard training protocol. We also make additional justification on the acquired robustness adaptation by peering into the interpretability of neurons' activation maps. Furthermore, we investigate $\\textit{how}$ robust regularization can $\\textit{efficiently}$ be designed in MAML. We propose a general but easily-optimized robustness-regularized meta-learning framework, which allows the use of unlabeled data augmentation, fast adversarial attack generation, and computationally-light fine-tuning. In particular, we for the first time show that the auxiliary contrastive learning task can enhance the adversarial robustness of MAML. Finally, extensive experiments are conducted to demonstrate the effectiveness of our proposed methods in robust few-shot learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|on_fast_adversarial_robustness_adaptation_in_modelagnostic_metalearning", "pdf": "/pdf/c29f970186b2b2658cd52eea7aac2b5266c649f4.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021on,\ntitle={On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning},\nauthor={Ren Wang and Kaidi Xu and Sijia Liu and Pin-Yu Chen and Tsui-Wei Weng and Chuang Gan and Meng Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=o81ZyBCojoA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "o81ZyBCojoA", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2301/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2301/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2301/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2301/Authors|ICLR.cc/2021/Conference/Paper2301/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2301/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850010, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2301/-/Official_Comment"}}}, {"id": "MRiho5XJlHd", "original": null, "number": 10, "cdate": 1605827701619, "ddate": null, "tcdate": 1605827701619, "tmdate": 1605988161047, "tddate": null, "forum": "o81ZyBCojoA", "replyto": "W9ymJJrdE6t", "invitation": "ICLR.cc/2021/Conference/Paper2301/-/Official_Comment", "content": {"title": "Our Response to Reviewer 3", "comment": "We thank the reviewer very much for the positive rating on our work and  the insightful suggestions. We also add the suggested experiments to further address your question; see details below. Please don\u2019t hesitate to let us know for any additional comments on the paper or on the changes.\n\n**Q:** *More other few-shot learning datasets (such as Omniglot) or other few-shot settings (such as 5-way or 20-way).*\n\n**A:** Thank you for the suggestion. We conducted additional experiments on Omniglot [1] (Figure S3 in the revised paper over ${5,10,15,20}$-way setups) as well as  CIFAR-FS [2] (Tables S2, S3). \n\n1. As shown in Table S2 and S3 (copied below), our methods outperform the baseline methods MAML and AQ in robust accuracy (RA) and/or standard accuracy (SA). For example,  the proposed R-MAML$_\\mathrm{out}$-CL leads to 10% improvement in SA and 3% improvement in RA compared to AQ under the MAML 1-Shot 5-Way setting, and 5.6% SA improvement and 1.3% RA improvement under the 5-Shot 5-Way setting. \n\n2. We also conducted new experiments in more few-shot settings. Experiments on 1-Shot 5-Way and 5-Shot 5-Way are conducted on CIFAR-FS (Tables S2, S3). On Omniglot, we show R-MAML$_\\mathrm{out}$(TRADES) can obtain better performance than AQ in the 1-Shot (5,10,15,20)-Way settings (see Figure S3 in the revised paper).\n\n**Table S2.** SA/RA performance of our proposed methods on CIFAR-FS (1-Shot 5-Way, $\\epsilon = 8$).\n\n|| Standard Accuracy (SA) &nbsp; | Robust Accuracy (RA) |\n|:-:|:-:|:-:|\n|MAML|**51.07%**|0.235%|\n|AQ|31.25%|26.34%|\n|R-MAML$_\\mathrm{out}$(AT) (ours)|39.76%|26.15%|\n|R-MAML$_\\mathrm{out}$(TRADES) (ours)|39.76%|26.15%|\n|R-MAML$_\\mathrm{out}$-TRADES (ours)|40.59%|28.06%|\n|R-MAML$_\\mathrm{out}$-CL (ours)|41.25%|**29.33%**|\n\n**Table S3.** SA/RA performance of our proposed methods on CIFAR-FS (5-Shot 5-Way, $\\epsilon = 8$).\n\n|| Standard Accuracy (SA) &nbsp; | Robust Accuracy (RA) |\n|:-:|:-:|:-:|\n|MAML|**67.2%**|0.225%|\n|AQ|52.32%|33.96%|\n|R-MAML$_\\mathrm{out}$(AT) (ours)|57.18%|32.62%|\n|R-MAML$_\\mathrm{out}$(TRADES) (ours)|57.46%|34.72%|\n|R-MAML$_\\mathrm{out}$-TRADES (ours)|57.62%|34.76%|\n|R-MAML$_\\mathrm{out}$-CL (ours)|57.95%|**35.3%**|\n\n[1] Lake et al., \u201cHuman-level concept learning through probabilistic program induction\u201d, 2015\n\n[2] Bertinetto et al., \u201cMeta-learning with differentiable closed-form solvers\u201d, 2018"}, "signatures": ["ICLR.cc/2021/Conference/Paper2301/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2301/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning", "authorids": ["~Ren_Wang1", "~Kaidi_Xu1", "~Sijia_Liu1", "~Pin-Yu_Chen1", "~Tsui-Wei_Weng1", "~Chuang_Gan1", "~Meng_Wang4"], "authors": ["Ren Wang", "Kaidi Xu", "Sijia Liu", "Pin-Yu Chen", "Tsui-Wei Weng", "Chuang Gan", "Meng Wang"], "keywords": [], "abstract": "Model-agnostic meta-learning (MAML) has emerged as one of the most successful meta-learning techniques in few-shot learning. It enables us to learn a $\\textit{meta-initialization}$ of model parameters (that we call $\\textit{meta-model}$) to rapidly adapt to new tasks using a small amount of labeled training data. Despite the generalization power of the meta-model, it remains elusive that how $\\textit{adversarial robustness}$ can be maintained by MAML in few-shot learning. In addition to generalization, robustness is also desired for a meta-model to defend adversarial examples (attacks). Toward promoting adversarial robustness in MAML, we first study $\\textit{when}$ a robustness-promoting regularization should be incorporated, given the fact that MAML adopts a bi-level (fine-tuning vs. meta-update) learning procedure. We show that robustifying the meta-update stage is sufficient to make robustness adapted to the task-specific fine-tuning stage even if the latter uses a standard training protocol. We also make additional justification on the acquired robustness adaptation by peering into the interpretability of neurons' activation maps. Furthermore, we investigate $\\textit{how}$ robust regularization can $\\textit{efficiently}$ be designed in MAML. We propose a general but easily-optimized robustness-regularized meta-learning framework, which allows the use of unlabeled data augmentation, fast adversarial attack generation, and computationally-light fine-tuning. In particular, we for the first time show that the auxiliary contrastive learning task can enhance the adversarial robustness of MAML. Finally, extensive experiments are conducted to demonstrate the effectiveness of our proposed methods in robust few-shot learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|on_fast_adversarial_robustness_adaptation_in_modelagnostic_metalearning", "pdf": "/pdf/c29f970186b2b2658cd52eea7aac2b5266c649f4.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021on,\ntitle={On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning},\nauthor={Ren Wang and Kaidi Xu and Sijia Liu and Pin-Yu Chen and Tsui-Wei Weng and Chuang Gan and Meng Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=o81ZyBCojoA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "o81ZyBCojoA", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2301/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2301/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2301/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2301/Authors|ICLR.cc/2021/Conference/Paper2301/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2301/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850010, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2301/-/Official_Comment"}}}, {"id": "83p04Cdkwlz", "original": null, "number": 4, "cdate": 1605825891701, "ddate": null, "tcdate": 1605825891701, "tmdate": 1605921674624, "tddate": null, "forum": "o81ZyBCojoA", "replyto": "zjbmixlcD4p", "invitation": "ICLR.cc/2021/Conference/Paper2301/-/Official_Comment", "content": {"title": "Our Response to Reviewer 1 - Part 2", "comment": "**Q2:** *All results are based on MiniImageNet. MAML has also been applied in Reinforcement Learning (RL). It is not clear the results in this paper carry out to the RL setting.*\n\n**A2:** Thank you for the suggestion. We conducted additional experiments on CIFAR-FS [1] (see Tables S2 and S3) and Omniglot [2]  (see Figure S3 in Appendix 7 of the revised paper revision). The results show that our methods consistently work well on various datasets and setups and outperform the baseline method AQ. In particular, as shown by Table S2 and Table S3, the proposed R-MAML$_\\mathrm{out}$-CL leads to 10% SA improvement and 3% RA improvement compared to AQ under the MAML 1-Shot 5-Way setting, and 5.6% SA improvement and 1.3% RA improvement under the 5-Shot 5-Way setting. \n\nFor the comment about applying MAML to Reinforcement learning, we believe that our proposed problem formulation in (3), together with its training method and insights, could help solve problems in the generic form of min-max involved bi-level optimization. However, since in this paper we mainly focused on the tasks of image classification, we will leave it as a future research direction and add a future work section in the revision. Thank you for your great comment! We will also release our code so that future studies on RL+MAML can benefit from our works.\n\n**Table S2.** SA/RA performance of our proposed methods on CIFAR-FS (1-Shot 5-Way, $\\epsilon = 8$).\n\n|| Standard Accuracy (SA) &nbsp; | Robust Accuracy (RA) |\n|:-:|:-:|:-:|\n|MAML|**51.07%**|0.235%|\n|AQ|31.25%|26.34%|\n|R-MAML$_\\mathrm{out}$(AT) (ours)|39.76%|26.15%|\n|R-MAML$_\\mathrm{out}$(TRADES) (ours)|39.76%|26.15%|\n|R-MAML$_\\mathrm{out}$-TRADES (ours)|40.59%|28.06%|\n|R-MAML$_\\mathrm{out}$-CL (ours)|41.25%|**29.33%**|\n\n**Table S3.** SA/RA performance of our proposed methods on CIFAR-FS (5-Shot 5-Way, $\\epsilon = 8$).\n\n|| Standard Accuracy (SA) &nbsp; | Robust Accuracy (RA) |\n|:-:|:-:|:-:|\n|MAML|**67.2%**|0.225%|\n|AQ|52.32%|33.96%|\n|R-MAML$_\\mathrm{out}$(AT) (ours)|57.18%|32.62%|\n|R-MAML$_\\mathrm{out}$(TRADES) (ours)|57.46%|34.72%|\n|R-MAML$_\\mathrm{out}$-TRADES (ours)|57.62%|34.76%|\n|R-MAML$_\\mathrm{out}$-CL (ours)|57.95%|**35.3%**|\n\n[2] Lake et al., \u201cHuman-level concept learning through probabilistic program induction\u201d, 2015"}, "signatures": ["ICLR.cc/2021/Conference/Paper2301/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2301/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning", "authorids": ["~Ren_Wang1", "~Kaidi_Xu1", "~Sijia_Liu1", "~Pin-Yu_Chen1", "~Tsui-Wei_Weng1", "~Chuang_Gan1", "~Meng_Wang4"], "authors": ["Ren Wang", "Kaidi Xu", "Sijia Liu", "Pin-Yu Chen", "Tsui-Wei Weng", "Chuang Gan", "Meng Wang"], "keywords": [], "abstract": "Model-agnostic meta-learning (MAML) has emerged as one of the most successful meta-learning techniques in few-shot learning. It enables us to learn a $\\textit{meta-initialization}$ of model parameters (that we call $\\textit{meta-model}$) to rapidly adapt to new tasks using a small amount of labeled training data. Despite the generalization power of the meta-model, it remains elusive that how $\\textit{adversarial robustness}$ can be maintained by MAML in few-shot learning. In addition to generalization, robustness is also desired for a meta-model to defend adversarial examples (attacks). Toward promoting adversarial robustness in MAML, we first study $\\textit{when}$ a robustness-promoting regularization should be incorporated, given the fact that MAML adopts a bi-level (fine-tuning vs. meta-update) learning procedure. We show that robustifying the meta-update stage is sufficient to make robustness adapted to the task-specific fine-tuning stage even if the latter uses a standard training protocol. We also make additional justification on the acquired robustness adaptation by peering into the interpretability of neurons' activation maps. Furthermore, we investigate $\\textit{how}$ robust regularization can $\\textit{efficiently}$ be designed in MAML. We propose a general but easily-optimized robustness-regularized meta-learning framework, which allows the use of unlabeled data augmentation, fast adversarial attack generation, and computationally-light fine-tuning. In particular, we for the first time show that the auxiliary contrastive learning task can enhance the adversarial robustness of MAML. Finally, extensive experiments are conducted to demonstrate the effectiveness of our proposed methods in robust few-shot learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|on_fast_adversarial_robustness_adaptation_in_modelagnostic_metalearning", "pdf": "/pdf/c29f970186b2b2658cd52eea7aac2b5266c649f4.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021on,\ntitle={On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning},\nauthor={Ren Wang and Kaidi Xu and Sijia Liu and Pin-Yu Chen and Tsui-Wei Weng and Chuang Gan and Meng Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=o81ZyBCojoA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "o81ZyBCojoA", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2301/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2301/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2301/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2301/Authors|ICLR.cc/2021/Conference/Paper2301/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2301/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850010, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2301/-/Official_Comment"}}}, {"id": "_rQmGGiAuDH", "original": null, "number": 5, "cdate": 1605826091487, "ddate": null, "tcdate": 1605826091487, "tmdate": 1605829378688, "tddate": null, "forum": "o81ZyBCojoA", "replyto": "zjbmixlcD4p", "invitation": "ICLR.cc/2021/Conference/Paper2301/-/Official_Comment", "content": {"title": "Our Response to Reviewer 1 - Part 1", "comment": "We thank the reviewer very much for the insightful comments. We list pointwise responses to your questions below. Please feel free to let us know if you have additional comments.\n\n**Q1:** *Adversarial robustness of MAML has been studied in (Goldblum et al., 2019). This paper provides further investigation. The study seems to be straightforward and incremental.* \n\n**A1:** The differences between our work and the method in (Goldblum et al., 2019) are in **threefolds**. \n\n1. We have  provided insights and explanation to why the adversarial meta-update is critical from two perspectives while (Goldblum et al., 2019) does not (as also recognized by Reviewer 3). Our insights are based on: \n(a) learned data feature representation (paragraph \u201cLearned signature of neuron\u2019s representation\u201d at page 5) \n(b) robustness adaptation in meta-testing (paragraph \u201crobust meta-update provides robustness adaptation\u201d at page 5). \nBesides,  as we clarified in the last paragraph of page 5, adversarial querying (AQ) (Goldblum et al., 2019) is actually a *special case* of our proposed general framework (3) with  $\\gamma_{\\mathrm{in}} = 0$ and  $\\gamma_{\\mathrm{out}} = \\infty $. Unlike our proposal,  the meta-update of AQ is overridden by the AT regularization without incorporating the standard validation loss. As a result, AQ leads to a worse clean accuracy than our proposed R-MAML$_\\mathrm{out}$; see Tables 3 in our paper and Tables 6, 21 in (Goldblum et al., 2019). The newly added experiment also supports this conclusion (see Tables S2 - S3 in Q2-A2, and Figure S3 in the revision).\n\n2. As training a robustness-regularized MAML is very expensive, in this paper we propose a more efficient training for robustness-regularized MAML while (Goldblum et al., 2019) does not. Computation efficiency is a critical problem in robustness-regularized MAML, since we have both robust training (a min-max optimization problem) and MAML (a bi-level optimization problem) at the same time. We addressed this problem by leveraging 1) fast (one-step) adversarial example generation, and 2) partial model training during fine-tuning (only fine-tuning the classifier\u2019s head). The speed can be increased by 30% using these acceleration techniques. Please see Sec. 4 for more details. \n\n3. We have generalized the robust MAML to the semi-supervised setting while (Goldblum et al., 2019) does not. In Sec. 5, we show that the use of unlabeled data augmentation, particularly introducing an auxiliary contrastive learning task, can provide additional benefits on adversarial robustness of MAML in the low data regime: 2% robust accuracy improvement and 9% clean accuracy improvement over AQ (Table 3). Furthermore, in the revision, we have conducted additional experiments and show that the use of unlabeled data augmentation on CIFAR-FS [1] can provide 10% (or 5.6%) standard accuracy (SA) improvement and 3% (or 1.3%) robust accuracy (RA) improvement over AQ under the 1-Shot 5-Way (or 5-Shot 5-Way) setting (see Table S2 and S3 in the next response - Part 2).\n\n[1] Bertinetto et al., \u201cMeta-learning with differentiable closed-form solvers\u201d, 2018"}, "signatures": ["ICLR.cc/2021/Conference/Paper2301/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2301/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning", "authorids": ["~Ren_Wang1", "~Kaidi_Xu1", "~Sijia_Liu1", "~Pin-Yu_Chen1", "~Tsui-Wei_Weng1", "~Chuang_Gan1", "~Meng_Wang4"], "authors": ["Ren Wang", "Kaidi Xu", "Sijia Liu", "Pin-Yu Chen", "Tsui-Wei Weng", "Chuang Gan", "Meng Wang"], "keywords": [], "abstract": "Model-agnostic meta-learning (MAML) has emerged as one of the most successful meta-learning techniques in few-shot learning. It enables us to learn a $\\textit{meta-initialization}$ of model parameters (that we call $\\textit{meta-model}$) to rapidly adapt to new tasks using a small amount of labeled training data. Despite the generalization power of the meta-model, it remains elusive that how $\\textit{adversarial robustness}$ can be maintained by MAML in few-shot learning. In addition to generalization, robustness is also desired for a meta-model to defend adversarial examples (attacks). Toward promoting adversarial robustness in MAML, we first study $\\textit{when}$ a robustness-promoting regularization should be incorporated, given the fact that MAML adopts a bi-level (fine-tuning vs. meta-update) learning procedure. We show that robustifying the meta-update stage is sufficient to make robustness adapted to the task-specific fine-tuning stage even if the latter uses a standard training protocol. We also make additional justification on the acquired robustness adaptation by peering into the interpretability of neurons' activation maps. Furthermore, we investigate $\\textit{how}$ robust regularization can $\\textit{efficiently}$ be designed in MAML. We propose a general but easily-optimized robustness-regularized meta-learning framework, which allows the use of unlabeled data augmentation, fast adversarial attack generation, and computationally-light fine-tuning. In particular, we for the first time show that the auxiliary contrastive learning task can enhance the adversarial robustness of MAML. Finally, extensive experiments are conducted to demonstrate the effectiveness of our proposed methods in robust few-shot learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|on_fast_adversarial_robustness_adaptation_in_modelagnostic_metalearning", "pdf": "/pdf/c29f970186b2b2658cd52eea7aac2b5266c649f4.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021on,\ntitle={On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning},\nauthor={Ren Wang and Kaidi Xu and Sijia Liu and Pin-Yu Chen and Tsui-Wei Weng and Chuang Gan and Meng Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=o81ZyBCojoA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "o81ZyBCojoA", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2301/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2301/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2301/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2301/Authors|ICLR.cc/2021/Conference/Paper2301/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2301/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850010, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2301/-/Official_Comment"}}}, {"id": "hokIK5zcY-", "original": null, "number": 7, "cdate": 1605826928881, "ddate": null, "tcdate": 1605826928881, "tmdate": 1605826928881, "tddate": null, "forum": "o81ZyBCojoA", "replyto": "fpH2KlYuoVd", "invitation": "ICLR.cc/2021/Conference/Paper2301/-/Official_Comment", "content": {"title": "Our Response to Reviewer 4 - Part 2", "comment": "**Q3:** *It would be better to give more insights on the way to integrate the adversarial robustness rather than list the experimental results.*\n\n**A3:** Thank you for the suggestion. We summarize our findings and list the related insights. \n\n1. In Section 2 - Robustness-promoting MAML, we propose a principled robustness-regularized meta-learning framework in (3), which covers the baseline approach adversarial querying (AQ) (Goldblum et al., 2019) as a special case. As we clarified in the last paragraph of page 5, our proposed problem is simplified to AQ as  $\\gamma_{\\mathrm{in}} = 0$ and  $\\gamma_{\\mathrm{out}} = \\infty$. \n\n2. In Section 3, motivated by (3), we address the question of when to incorporate robust training in MAML (outer meta-learning level vs. inner fine-tuning level) . \nOur empirical finding is that it is sufficient to promote adversarial robustness at the meta-update level.  Most importantly, we explain our conclusion from two insightful perspectives, 1) explanation of learned neuron\u2019s representation; and 2) resilience to different fine-tuning schemes at the meta-testing phase.\n\n3. In Section 4, we then address the question \u201chow to efficiently train robustness-regularized MAML?\u201d Note that robust training is given by a min-max optimization, and MAML is given by a bi-level optimization. Thus, the computation efficiency is a critical question. We addressed this problem by leveraging 1) fast (one-step) adversarial example generation, and 2) partial model training during fine-tuning (only fine-tuning the classifier\u2019s head). \n\n4. In Section 5, we find that proper unlabeled data augmentation can further improve the accuracy-robustness tradeoff in MAML  (Figure 3). In particular, we clearly motivate why self-supervised contrastive representation learning is favored in MAML (see paragraph after Figure 3). And we show that self-supervised contrastive representation learning is beneficial to MAML in both robustness and accuracy (Table 3).\n\n**Q4:** *Detailed architecture explanation with novelty is lacking.*\n\n**A4:** We would like to remark that our proposed methods do not rely on the specific model architecture. In the paper, we show that our methods obtain good performance under both four-layer CNN (see Table 3) and ResNet18 (see Table S1 in the supplementary). As we responded to the reviewer's first question, we believe that the lack of theoretical analysis does not restrict our contributions. Our technical contribution is also not marginal. The proposed optimization framework (3) and the solutions to scalable training of robustness-regularized MAML and semi-supervision enabled robustness-regularized MAML are all technically new to the meta-learning. \n\nWe hope that our responses have mostly addressed your concerns. If you have further comments, please feel free to let us know. We will try our best to address them. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2301/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2301/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning", "authorids": ["~Ren_Wang1", "~Kaidi_Xu1", "~Sijia_Liu1", "~Pin-Yu_Chen1", "~Tsui-Wei_Weng1", "~Chuang_Gan1", "~Meng_Wang4"], "authors": ["Ren Wang", "Kaidi Xu", "Sijia Liu", "Pin-Yu Chen", "Tsui-Wei Weng", "Chuang Gan", "Meng Wang"], "keywords": [], "abstract": "Model-agnostic meta-learning (MAML) has emerged as one of the most successful meta-learning techniques in few-shot learning. It enables us to learn a $\\textit{meta-initialization}$ of model parameters (that we call $\\textit{meta-model}$) to rapidly adapt to new tasks using a small amount of labeled training data. Despite the generalization power of the meta-model, it remains elusive that how $\\textit{adversarial robustness}$ can be maintained by MAML in few-shot learning. In addition to generalization, robustness is also desired for a meta-model to defend adversarial examples (attacks). Toward promoting adversarial robustness in MAML, we first study $\\textit{when}$ a robustness-promoting regularization should be incorporated, given the fact that MAML adopts a bi-level (fine-tuning vs. meta-update) learning procedure. We show that robustifying the meta-update stage is sufficient to make robustness adapted to the task-specific fine-tuning stage even if the latter uses a standard training protocol. We also make additional justification on the acquired robustness adaptation by peering into the interpretability of neurons' activation maps. Furthermore, we investigate $\\textit{how}$ robust regularization can $\\textit{efficiently}$ be designed in MAML. We propose a general but easily-optimized robustness-regularized meta-learning framework, which allows the use of unlabeled data augmentation, fast adversarial attack generation, and computationally-light fine-tuning. In particular, we for the first time show that the auxiliary contrastive learning task can enhance the adversarial robustness of MAML. Finally, extensive experiments are conducted to demonstrate the effectiveness of our proposed methods in robust few-shot learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|on_fast_adversarial_robustness_adaptation_in_modelagnostic_metalearning", "pdf": "/pdf/c29f970186b2b2658cd52eea7aac2b5266c649f4.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021on,\ntitle={On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning},\nauthor={Ren Wang and Kaidi Xu and Sijia Liu and Pin-Yu Chen and Tsui-Wei Weng and Chuang Gan and Meng Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=o81ZyBCojoA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "o81ZyBCojoA", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2301/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2301/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2301/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2301/Authors|ICLR.cc/2021/Conference/Paper2301/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2301/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850010, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2301/-/Official_Comment"}}}, {"id": "W9ymJJrdE6t", "original": null, "number": 1, "cdate": 1603900557547, "ddate": null, "tcdate": 1603900557547, "tmdate": 1605024243645, "tddate": null, "forum": "o81ZyBCojoA", "replyto": "o81ZyBCojoA", "invitation": "ICLR.cc/2021/Conference/Paper2301/-/Official_Review", "content": {"title": "submission 2301 review", "review": "The paper proposes to integrate meta-learning (MAML specifically) and adversarial training (AT) to improve adversarial robustness of meta-learning in terms of fast and effective adaptation on few-shot test tasks. To achieve this, the authors provide extensive investigation and solid answers on when, how and why their method works.\n\nOverall I vote for accepting. The authors found that it seems to be natural to combine MAML and AT together to enhance the adversarial robustness of meta-learning, however, due to computational complexity and bi-level optimization in MAML and AT, they further pose the above three questions when, how, and why step by step, to which they also provide affirmative answers in the paper. Some suggestions are given in the cons part below.\n\nPros:\n\n1. Overall the paper is well-written. In particular, its motivation and contribution are clearly explained and summarized, which is quite fluent when reading.\n\n2. The paper presents visual evidence of when to incorporate robustness regularization in MAML by leveraging input attribution maps of neurons, which is reasonable and interesting for me.\n\n3. The paper leverages contrastive learning into MAML to help the model\u2019s adversarial robustness, which is novel and interesting to me.\n\nCons:\n\n1. For the experiment part, more other few-shot learning datasets (such as Omniglot) or other few-shot settings (such as 5-way or 20-way) can be utilized to evaluate the proposed method as well as to present more data points.", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper2301/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2301/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning", "authorids": ["~Ren_Wang1", "~Kaidi_Xu1", "~Sijia_Liu1", "~Pin-Yu_Chen1", "~Tsui-Wei_Weng1", "~Chuang_Gan1", "~Meng_Wang4"], "authors": ["Ren Wang", "Kaidi Xu", "Sijia Liu", "Pin-Yu Chen", "Tsui-Wei Weng", "Chuang Gan", "Meng Wang"], "keywords": [], "abstract": "Model-agnostic meta-learning (MAML) has emerged as one of the most successful meta-learning techniques in few-shot learning. It enables us to learn a $\\textit{meta-initialization}$ of model parameters (that we call $\\textit{meta-model}$) to rapidly adapt to new tasks using a small amount of labeled training data. Despite the generalization power of the meta-model, it remains elusive that how $\\textit{adversarial robustness}$ can be maintained by MAML in few-shot learning. In addition to generalization, robustness is also desired for a meta-model to defend adversarial examples (attacks). Toward promoting adversarial robustness in MAML, we first study $\\textit{when}$ a robustness-promoting regularization should be incorporated, given the fact that MAML adopts a bi-level (fine-tuning vs. meta-update) learning procedure. We show that robustifying the meta-update stage is sufficient to make robustness adapted to the task-specific fine-tuning stage even if the latter uses a standard training protocol. We also make additional justification on the acquired robustness adaptation by peering into the interpretability of neurons' activation maps. Furthermore, we investigate $\\textit{how}$ robust regularization can $\\textit{efficiently}$ be designed in MAML. We propose a general but easily-optimized robustness-regularized meta-learning framework, which allows the use of unlabeled data augmentation, fast adversarial attack generation, and computationally-light fine-tuning. In particular, we for the first time show that the auxiliary contrastive learning task can enhance the adversarial robustness of MAML. Finally, extensive experiments are conducted to demonstrate the effectiveness of our proposed methods in robust few-shot learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|on_fast_adversarial_robustness_adaptation_in_modelagnostic_metalearning", "pdf": "/pdf/c29f970186b2b2658cd52eea7aac2b5266c649f4.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021on,\ntitle={On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning},\nauthor={Ren Wang and Kaidi Xu and Sijia Liu and Pin-Yu Chen and Tsui-Wei Weng and Chuang Gan and Meng Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=o81ZyBCojoA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "o81ZyBCojoA", "replyto": "o81ZyBCojoA", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2301/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538099516, "tmdate": 1606915787586, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2301/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2301/-/Official_Review"}}}, {"id": "gImFr42ZFu0", "original": null, "number": 2, "cdate": 1603983945435, "ddate": null, "tcdate": 1603983945435, "tmdate": 1605024243581, "tddate": null, "forum": "o81ZyBCojoA", "replyto": "o81ZyBCojoA", "invitation": "ICLR.cc/2021/Conference/Paper2301/-/Official_Review", "content": {"title": "Clear and interesting paper but may not be enough convincing", "review": "It is an interesting paper empirically addressing adversarial robustness of model agnostic meta learning (MAML). The paper investigates where to incorporate robust regularization in MAML in order to improve adversarial robustness, and based on that *efficient* robust MAML methods are proposed. Interestingly, contrastive learning is incorporated and derive a more robust MAML model. \n\nMy concerns are listed as below:\n\n- The paper is highly empirical, whearas only one dataset is employed. The claims would be more convincing with more datasets.\n- The key point is not properly emphasized. For example, Section 4 and Section 5.1 (TRADES) do not provide very important insights compared with Section 5.2 (CL) \u2014 maybe Section 5.2  could be extended furthur with some other contents defered in the Appendix.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2301/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2301/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning", "authorids": ["~Ren_Wang1", "~Kaidi_Xu1", "~Sijia_Liu1", "~Pin-Yu_Chen1", "~Tsui-Wei_Weng1", "~Chuang_Gan1", "~Meng_Wang4"], "authors": ["Ren Wang", "Kaidi Xu", "Sijia Liu", "Pin-Yu Chen", "Tsui-Wei Weng", "Chuang Gan", "Meng Wang"], "keywords": [], "abstract": "Model-agnostic meta-learning (MAML) has emerged as one of the most successful meta-learning techniques in few-shot learning. It enables us to learn a $\\textit{meta-initialization}$ of model parameters (that we call $\\textit{meta-model}$) to rapidly adapt to new tasks using a small amount of labeled training data. Despite the generalization power of the meta-model, it remains elusive that how $\\textit{adversarial robustness}$ can be maintained by MAML in few-shot learning. In addition to generalization, robustness is also desired for a meta-model to defend adversarial examples (attacks). Toward promoting adversarial robustness in MAML, we first study $\\textit{when}$ a robustness-promoting regularization should be incorporated, given the fact that MAML adopts a bi-level (fine-tuning vs. meta-update) learning procedure. We show that robustifying the meta-update stage is sufficient to make robustness adapted to the task-specific fine-tuning stage even if the latter uses a standard training protocol. We also make additional justification on the acquired robustness adaptation by peering into the interpretability of neurons' activation maps. Furthermore, we investigate $\\textit{how}$ robust regularization can $\\textit{efficiently}$ be designed in MAML. We propose a general but easily-optimized robustness-regularized meta-learning framework, which allows the use of unlabeled data augmentation, fast adversarial attack generation, and computationally-light fine-tuning. In particular, we for the first time show that the auxiliary contrastive learning task can enhance the adversarial robustness of MAML. Finally, extensive experiments are conducted to demonstrate the effectiveness of our proposed methods in robust few-shot learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|on_fast_adversarial_robustness_adaptation_in_modelagnostic_metalearning", "pdf": "/pdf/c29f970186b2b2658cd52eea7aac2b5266c649f4.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021on,\ntitle={On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning},\nauthor={Ren Wang and Kaidi Xu and Sijia Liu and Pin-Yu Chen and Tsui-Wei Weng and Chuang Gan and Meng Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=o81ZyBCojoA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "o81ZyBCojoA", "replyto": "o81ZyBCojoA", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2301/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538099516, "tmdate": 1606915787586, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2301/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2301/-/Official_Review"}}}, {"id": "fpH2KlYuoVd", "original": null, "number": 3, "cdate": 1604069135160, "ddate": null, "tcdate": 1604069135160, "tmdate": 1605024243525, "tddate": null, "forum": "o81ZyBCojoA", "replyto": "o81ZyBCojoA", "invitation": "ICLR.cc/2021/Conference/Paper2301/-/Official_Review", "content": {"title": "clear motivation and reasonable method, but marginal novelty", "review": "This paper explores a way to promote the adversarial robustness in Model-agnostic meta-learning (MAML). It conducts extensive experiments to show regularizing adversarial robustness at meta-update level is sufficient to offer fast and effective robustness adaptation on few-shot test tasks. However, it lacks the theoretical analysis for this conclusion. Also, the experiments only are conducted on few-shot image classification task on miniImageNet dataset. This makes this conclusion lack sufficient credibility.\n\nThis paper also shows the use of unlabeled data augmentation can provide additional benefits on adversarial robustness of MAML. Due to the low-data regime in few-shot classification, it is reasonable to introduce contrastive learning task to few-shot learning, which may facilitate the development of following research.\n\nIt is of great importance to investigate the adversarial robustness in meta-learning framework, such as MAML in this paper. It might give us insights on designing network structure. However, it would be better to give more insights on the way to integrate the adversarial robustness rather than list the experimental results.\n\nNovelty of this paper seems to be technically marginal. While the aggregation of adversarial robustness on meta-learning framework is new, detailed architecture explanation with novelty is lacked.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2301/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2301/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning", "authorids": ["~Ren_Wang1", "~Kaidi_Xu1", "~Sijia_Liu1", "~Pin-Yu_Chen1", "~Tsui-Wei_Weng1", "~Chuang_Gan1", "~Meng_Wang4"], "authors": ["Ren Wang", "Kaidi Xu", "Sijia Liu", "Pin-Yu Chen", "Tsui-Wei Weng", "Chuang Gan", "Meng Wang"], "keywords": [], "abstract": "Model-agnostic meta-learning (MAML) has emerged as one of the most successful meta-learning techniques in few-shot learning. It enables us to learn a $\\textit{meta-initialization}$ of model parameters (that we call $\\textit{meta-model}$) to rapidly adapt to new tasks using a small amount of labeled training data. Despite the generalization power of the meta-model, it remains elusive that how $\\textit{adversarial robustness}$ can be maintained by MAML in few-shot learning. In addition to generalization, robustness is also desired for a meta-model to defend adversarial examples (attacks). Toward promoting adversarial robustness in MAML, we first study $\\textit{when}$ a robustness-promoting regularization should be incorporated, given the fact that MAML adopts a bi-level (fine-tuning vs. meta-update) learning procedure. We show that robustifying the meta-update stage is sufficient to make robustness adapted to the task-specific fine-tuning stage even if the latter uses a standard training protocol. We also make additional justification on the acquired robustness adaptation by peering into the interpretability of neurons' activation maps. Furthermore, we investigate $\\textit{how}$ robust regularization can $\\textit{efficiently}$ be designed in MAML. We propose a general but easily-optimized robustness-regularized meta-learning framework, which allows the use of unlabeled data augmentation, fast adversarial attack generation, and computationally-light fine-tuning. In particular, we for the first time show that the auxiliary contrastive learning task can enhance the adversarial robustness of MAML. Finally, extensive experiments are conducted to demonstrate the effectiveness of our proposed methods in robust few-shot learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|on_fast_adversarial_robustness_adaptation_in_modelagnostic_metalearning", "pdf": "/pdf/c29f970186b2b2658cd52eea7aac2b5266c649f4.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021on,\ntitle={On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning},\nauthor={Ren Wang and Kaidi Xu and Sijia Liu and Pin-Yu Chen and Tsui-Wei Weng and Chuang Gan and Meng Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=o81ZyBCojoA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "o81ZyBCojoA", "replyto": "o81ZyBCojoA", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2301/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538099516, "tmdate": 1606915787586, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2301/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2301/-/Official_Review"}}}], "count": 13}