{"notes": [{"id": "Z532uNJyG5y", "original": "z_ELIqqypqe", "number": 1033, "cdate": 1601308116712, "ddate": null, "tcdate": 1601308116712, "tmdate": 1614985660580, "tddate": null, "forum": "Z532uNJyG5y", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Iterative Graph Self-Distillation", "authorids": ["~Hanlin_Zhang1", "~Shuai_Lin1", "~Weiyang_Liu1", "~Pan_Zhou3", "~Jian_Tang1", "~Xiaodan_Liang2", "~Eric_Xing1"], "authors": ["Hanlin Zhang", "Shuai Lin", "Weiyang Liu", "Pan Zhou", "Jian Tang", "Xiaodan Liang", "Eric Xing"], "keywords": ["graph-level representation learning", "knowledge distillation"], "abstract": "How to discriminatively vectorize graphs is a fundamental challenge that attracts increasing attentions in recent years. Motivated by the recent success of unsupervised contrastive learning, we aim to learn graph-level representation in an unsupervised manner. Specifically, we propose a novel unsupervised graph learning paradigm called Iterative Graph Self-Distillation (IGSD) which iteratively performs the teacher-student distillation with graph augmentations. Different from conventional knowledge distillation, IGSD constructs the teacher with an exponential moving average of the student model and distills the knowledge of itself. The intuition behind IGSD is to predict the teacher network representation of the graph pairs under different augmented views. As a natural extension, we also apply IGSD to semi-supervised scenarios by jointly regularizing the network with both supervised and unsupervised contrastive loss. Finally, we show that finetuning the IGSD-trained models with self-training can further improve the graph representation power. Empirically, we achieve significant and consistent performance gain on various graph datasets in both unsupervised and semi-supervised settings, which well validates the superiority of IGSD.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|iterative_graph_selfdistillation", "supplementary_material": "/attachment/654a567fb3318e72b661d094196bcb030e58e15f.zip", "pdf": "/pdf/accdb4500871b1a64dfa37738f2c08f65583a742.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ugHLj63uvr", "_bibtex": "@misc{\nzhang2021iterative,\ntitle={Iterative Graph Self-Distillation},\nauthor={Hanlin Zhang and Shuai Lin and Weiyang Liu and Pan Zhou and Jian Tang and Xiaodan Liang and Eric Xing},\nyear={2021},\nurl={https://openreview.net/forum?id=Z532uNJyG5y}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 15, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "hedglXLEtcl", "original": null, "number": 1, "cdate": 1610040501416, "ddate": null, "tcdate": 1610040501416, "tmdate": 1610474108203, "tddate": null, "forum": "Z532uNJyG5y", "replyto": "Z532uNJyG5y", "invitation": "ICLR.cc/2021/Conference/Paper1033/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper proposes an unsupervised graph learning method [Iterative Graph Self-Distillation (IGSD)] by iteratively performing self-distillation to contrast graph pairs under different augmented views. This idea is then extended to semi-supervised setting where via a supervised contrastive loss and self-training. The method is empirically evaluated on some semi-supervised graph classification and molecular property prediction tasks, and has achieved promising results.\n\nReviewers agree that the method is interesting and the paper is well-written. The biggest concern from reviewers related to experimental evaluations of the method. The authors responded to this and included additional experiments. Although the reviewers appreciate the provided results and explanations, at the end they were not convinced about the empirical assessments. In particular, R1's post rebuttal comment indicates concerns about the reported performance of GCKN, which is different from the published one in Table 1 of GCKN paper. I encourage authors to improve on these experimental discrepancies and resubmit. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterative Graph Self-Distillation", "authorids": ["~Hanlin_Zhang1", "~Shuai_Lin1", "~Weiyang_Liu1", "~Pan_Zhou3", "~Jian_Tang1", "~Xiaodan_Liang2", "~Eric_Xing1"], "authors": ["Hanlin Zhang", "Shuai Lin", "Weiyang Liu", "Pan Zhou", "Jian Tang", "Xiaodan Liang", "Eric Xing"], "keywords": ["graph-level representation learning", "knowledge distillation"], "abstract": "How to discriminatively vectorize graphs is a fundamental challenge that attracts increasing attentions in recent years. Motivated by the recent success of unsupervised contrastive learning, we aim to learn graph-level representation in an unsupervised manner. Specifically, we propose a novel unsupervised graph learning paradigm called Iterative Graph Self-Distillation (IGSD) which iteratively performs the teacher-student distillation with graph augmentations. Different from conventional knowledge distillation, IGSD constructs the teacher with an exponential moving average of the student model and distills the knowledge of itself. The intuition behind IGSD is to predict the teacher network representation of the graph pairs under different augmented views. As a natural extension, we also apply IGSD to semi-supervised scenarios by jointly regularizing the network with both supervised and unsupervised contrastive loss. Finally, we show that finetuning the IGSD-trained models with self-training can further improve the graph representation power. Empirically, we achieve significant and consistent performance gain on various graph datasets in both unsupervised and semi-supervised settings, which well validates the superiority of IGSD.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|iterative_graph_selfdistillation", "supplementary_material": "/attachment/654a567fb3318e72b661d094196bcb030e58e15f.zip", "pdf": "/pdf/accdb4500871b1a64dfa37738f2c08f65583a742.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ugHLj63uvr", "_bibtex": "@misc{\nzhang2021iterative,\ntitle={Iterative Graph Self-Distillation},\nauthor={Hanlin Zhang and Shuai Lin and Weiyang Liu and Pan Zhou and Jian Tang and Xiaodan Liang and Eric Xing},\nyear={2021},\nurl={https://openreview.net/forum?id=Z532uNJyG5y}\n}"}, "tags": [], "invitation": {"reply": {"forum": "Z532uNJyG5y", "replyto": "Z532uNJyG5y", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040501402, "tmdate": 1610474108187, "id": "ICLR.cc/2021/Conference/Paper1033/-/Decision"}}}, {"id": "0Cs2HNnWG0W", "original": null, "number": 4, "cdate": 1604122210096, "ddate": null, "tcdate": 1604122210096, "tmdate": 1607422493720, "tddate": null, "forum": "Z532uNJyG5y", "replyto": "Z532uNJyG5y", "invitation": "ICLR.cc/2021/Conference/Paper1033/-/Official_Review", "content": {"title": "Motivation of key elements and significance of the idea are unclear", "review": "Summary:\n\nThis paper proposes a self-distillation based graph augmentation mechanism to alleviate the drawbacks of existing MI based models w.r.t. their high dependency towards negative sampling. Quantitatively the proposed model achieves encouraging results. However it would have been better if the system designs and significant difference of IGSD from existing work are discussed.\n\n\nStrength:\n\n- This work has clearly discussed a drawback of existing unsupervised MI based models which is the leading approach in graph classification\n- They propose a mechanism to address this issue with satisfiable quantitative results on unsupervised setting and extended semi-supervised setting with self-training also supported quantitatively.\n- Paper is clear in general, with a clear research problem, proposes mechanism for unsupervised/semi-supervised graph representation domain and encouraging quantitative results.\n\nWeakness:\n\n-There is a lack of qualitative analysis and discussion of the proposed method. \n-In Section 4.3 \"Performance with different amount of negative pairs\", it is not clear the reasoning of the provided observation from Figure 3a.\n-It is not clear the motivation behind selecting a teacher-student network for obtaining different views of the graph. These networks are normally used for knowledge transfer, but here used for contrastive learning. How is this more beneficial than an ensemble model w/o knowledge transfer step of Eq. 3. \n-The core difference of IGSD from CMC-graph is that CMC uses MI based contrastive between local patch representation and graph rep, while IGSD uses L2 based contrastive between 2 graph representations. Input, Encoders and projections are the same for both architectures. It could be useful to add some analysis to discuss these differences and their contributions to clearly understand the significance of IGSD.\n-This paper seems to have state-of-the-art results (although it is based on graph kernel). Why the results are not included?\n\nConvolutional Kernel Networks for Graph-Structured Data, ICML-2020\n\n\n\n=======================\n\nafter rebuttal:\n\n\nI thank the authors for the response. I still have concerns re their comparison with GCKN (ICML-2020). The reproduced results by the authors are different significantly from the published one in Table 1 of GCKN paper (~5%). E.g. MUTAG is 92.8 in original paper, but the authors report 87.2 for GCKN. The difference is significant.\n\nTherefore, I will keep my original rating.\n \n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1033/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1033/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterative Graph Self-Distillation", "authorids": ["~Hanlin_Zhang1", "~Shuai_Lin1", "~Weiyang_Liu1", "~Pan_Zhou3", "~Jian_Tang1", "~Xiaodan_Liang2", "~Eric_Xing1"], "authors": ["Hanlin Zhang", "Shuai Lin", "Weiyang Liu", "Pan Zhou", "Jian Tang", "Xiaodan Liang", "Eric Xing"], "keywords": ["graph-level representation learning", "knowledge distillation"], "abstract": "How to discriminatively vectorize graphs is a fundamental challenge that attracts increasing attentions in recent years. Motivated by the recent success of unsupervised contrastive learning, we aim to learn graph-level representation in an unsupervised manner. Specifically, we propose a novel unsupervised graph learning paradigm called Iterative Graph Self-Distillation (IGSD) which iteratively performs the teacher-student distillation with graph augmentations. Different from conventional knowledge distillation, IGSD constructs the teacher with an exponential moving average of the student model and distills the knowledge of itself. The intuition behind IGSD is to predict the teacher network representation of the graph pairs under different augmented views. As a natural extension, we also apply IGSD to semi-supervised scenarios by jointly regularizing the network with both supervised and unsupervised contrastive loss. Finally, we show that finetuning the IGSD-trained models with self-training can further improve the graph representation power. Empirically, we achieve significant and consistent performance gain on various graph datasets in both unsupervised and semi-supervised settings, which well validates the superiority of IGSD.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|iterative_graph_selfdistillation", "supplementary_material": "/attachment/654a567fb3318e72b661d094196bcb030e58e15f.zip", "pdf": "/pdf/accdb4500871b1a64dfa37738f2c08f65583a742.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ugHLj63uvr", "_bibtex": "@misc{\nzhang2021iterative,\ntitle={Iterative Graph Self-Distillation},\nauthor={Hanlin Zhang and Shuai Lin and Weiyang Liu and Pan Zhou and Jian Tang and Xiaodan Liang and Eric Xing},\nyear={2021},\nurl={https://openreview.net/forum?id=Z532uNJyG5y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Z532uNJyG5y", "replyto": "Z532uNJyG5y", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1033/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538128689, "tmdate": 1606915800515, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1033/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1033/-/Official_Review"}}}, {"id": "g0FAaD_-Pp", "original": null, "number": 2, "cdate": 1603817586666, "ddate": null, "tcdate": 1603817586666, "tmdate": 1606765168625, "tddate": null, "forum": "Z532uNJyG5y", "replyto": "Z532uNJyG5y", "invitation": "ICLR.cc/2021/Conference/Paper1033/-/Official_Review", "content": {"title": "Experiments need to be improved", "review": "------------------------------------\n\nupdate after reading the authors' response.\n\nThe authors didn't address my question \"Did the authors perform a significance study?\" A significance test such as double-sided t-test is needed to verifying whether the proposed method is significantly better than baselines.\n\n-------------------------------\n\nThis paper proposed a distillation approach for unsupervised graph representation learning. The approach partially builds upon contrastive self-supervised learning which contrasts pairs of augmented graphs. The approach is extended to the semi-supervised setting. The authors performed evaluation in graph classification and regression tasks. \n\nI recommend to reject this paper, due to the following major concerns: 1) experimental results are not strong; 2) important baselines were not compared; 3) important details such as optimal hyperparameter values are missing. \n\nMy major concerns of this paper include:\n1. The improvement of the proposed approach over baselines seem not significant. For example, in Table 1, comparing the mean and standard deviation of the proposed approach and CMC-GRAPH, it seems that the difference is not statistically significant. Did the authors perform a significance study?\n2. In the experiments, why the authors didn't compare with GCC, which is a contrastive self-supervised learning approach applied to graph classification?\n3. There are many other unsupervised graph representation learning methods. The authors need to compare with more to substantiate this work.\n4. In hyperparameter tuning, the authors gave the range of hyperparameters tuned, but didn't give the optimal value of the hyperparameters, which make the paper difficult to reproduce.\n5. In table 1, the authors excluded some results since they need more than 1 day to obtain. It is common for deep learning models to run several days to obtain results. I don't think it is proper to exclude these results simply because the runtime is more than 24 hours.\n\nHowever, the paper does have a few strong points.\n1. The ablation studies are well designed and the results are insightful.\n\n2. The paper is well-written and easy to follow, with a clear organization.\n\n3. The experiments were conducted on a rich collection of datasets. \n\nOther comments.\n1. In equation (3), the authors can draw a connection with MoCo.\n2. In Table, why didn't report mean and standard deviation of the results?\n3. For this result \"When batch size is\ngreater than 32, IGSD outperforms CMC-Graph and the performance gap becomes larger as the batch\nsize increases.\",  can the authors provide a reason that can possibly explain this phenomenon?\n4. The authors can add some statistics of the datasets used in Figure 2.\n\n\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1033/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1033/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterative Graph Self-Distillation", "authorids": ["~Hanlin_Zhang1", "~Shuai_Lin1", "~Weiyang_Liu1", "~Pan_Zhou3", "~Jian_Tang1", "~Xiaodan_Liang2", "~Eric_Xing1"], "authors": ["Hanlin Zhang", "Shuai Lin", "Weiyang Liu", "Pan Zhou", "Jian Tang", "Xiaodan Liang", "Eric Xing"], "keywords": ["graph-level representation learning", "knowledge distillation"], "abstract": "How to discriminatively vectorize graphs is a fundamental challenge that attracts increasing attentions in recent years. Motivated by the recent success of unsupervised contrastive learning, we aim to learn graph-level representation in an unsupervised manner. Specifically, we propose a novel unsupervised graph learning paradigm called Iterative Graph Self-Distillation (IGSD) which iteratively performs the teacher-student distillation with graph augmentations. Different from conventional knowledge distillation, IGSD constructs the teacher with an exponential moving average of the student model and distills the knowledge of itself. The intuition behind IGSD is to predict the teacher network representation of the graph pairs under different augmented views. As a natural extension, we also apply IGSD to semi-supervised scenarios by jointly regularizing the network with both supervised and unsupervised contrastive loss. Finally, we show that finetuning the IGSD-trained models with self-training can further improve the graph representation power. Empirically, we achieve significant and consistent performance gain on various graph datasets in both unsupervised and semi-supervised settings, which well validates the superiority of IGSD.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|iterative_graph_selfdistillation", "supplementary_material": "/attachment/654a567fb3318e72b661d094196bcb030e58e15f.zip", "pdf": "/pdf/accdb4500871b1a64dfa37738f2c08f65583a742.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ugHLj63uvr", "_bibtex": "@misc{\nzhang2021iterative,\ntitle={Iterative Graph Self-Distillation},\nauthor={Hanlin Zhang and Shuai Lin and Weiyang Liu and Pan Zhou and Jian Tang and Xiaodan Liang and Eric Xing},\nyear={2021},\nurl={https://openreview.net/forum?id=Z532uNJyG5y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Z532uNJyG5y", "replyto": "Z532uNJyG5y", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1033/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538128689, "tmdate": 1606915800515, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1033/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1033/-/Official_Review"}}}, {"id": "aIdepDO8z74", "original": null, "number": 15, "cdate": 1606297514966, "ddate": null, "tcdate": 1606297514966, "tmdate": 1606307190509, "tddate": null, "forum": "Z532uNJyG5y", "replyto": "iNmBHNzrjii", "invitation": "ICLR.cc/2021/Conference/Paper1033/-/Official_Comment", "content": {"title": "Response to the AnonReviewer2.", "comment": "Thanks for your constructive comment. The response to your question is listed below:\n\nEmploying t-test or not is an evaluation choice: most related works on unsupervised graph classification [1,2] didn't perform the t-test for significance study (though in some domains like the human evaluation in NLP is favorable). \n\nWe didn\u2019t perform the t-test results in the draft considering the following reasons: \n\n1) We cannot assume the variances of our methods and baselines are the same, which doesn\u2019t obey the assumption of t-test; \n\n2) Performing the t-test with small sample sizes (5 numbers for each method) would not be very effective and could give misleading results. \n\nDespite that, per your question and similar to [3], we perform the two-sided t-test and obtained the significance level at p < 10%, which means IGSD is comparable to the previous SOTA, CMC-Graph.\n\nWe hope the above response could address your concerns. \n\n[1] Hassani et al., Contrastive Multi-View Representation Learning on Graphs. ICML 2020.\n\n[2] Sun et al., InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization. ICLR 2020.\n\n[3] Xu et al., How Powerful are Graph Neural Networks?. ICLR 2019"}, "signatures": ["ICLR.cc/2021/Conference/Paper1033/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1033/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterative Graph Self-Distillation", "authorids": ["~Hanlin_Zhang1", "~Shuai_Lin1", "~Weiyang_Liu1", "~Pan_Zhou3", "~Jian_Tang1", "~Xiaodan_Liang2", "~Eric_Xing1"], "authors": ["Hanlin Zhang", "Shuai Lin", "Weiyang Liu", "Pan Zhou", "Jian Tang", "Xiaodan Liang", "Eric Xing"], "keywords": ["graph-level representation learning", "knowledge distillation"], "abstract": "How to discriminatively vectorize graphs is a fundamental challenge that attracts increasing attentions in recent years. Motivated by the recent success of unsupervised contrastive learning, we aim to learn graph-level representation in an unsupervised manner. Specifically, we propose a novel unsupervised graph learning paradigm called Iterative Graph Self-Distillation (IGSD) which iteratively performs the teacher-student distillation with graph augmentations. Different from conventional knowledge distillation, IGSD constructs the teacher with an exponential moving average of the student model and distills the knowledge of itself. The intuition behind IGSD is to predict the teacher network representation of the graph pairs under different augmented views. As a natural extension, we also apply IGSD to semi-supervised scenarios by jointly regularizing the network with both supervised and unsupervised contrastive loss. Finally, we show that finetuning the IGSD-trained models with self-training can further improve the graph representation power. Empirically, we achieve significant and consistent performance gain on various graph datasets in both unsupervised and semi-supervised settings, which well validates the superiority of IGSD.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|iterative_graph_selfdistillation", "supplementary_material": "/attachment/654a567fb3318e72b661d094196bcb030e58e15f.zip", "pdf": "/pdf/accdb4500871b1a64dfa37738f2c08f65583a742.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ugHLj63uvr", "_bibtex": "@misc{\nzhang2021iterative,\ntitle={Iterative Graph Self-Distillation},\nauthor={Hanlin Zhang and Shuai Lin and Weiyang Liu and Pan Zhou and Jian Tang and Xiaodan Liang and Eric Xing},\nyear={2021},\nurl={https://openreview.net/forum?id=Z532uNJyG5y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Z532uNJyG5y", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1033/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1033/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1033/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1033/Authors|ICLR.cc/2021/Conference/Paper1033/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1033/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864438, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1033/-/Official_Comment"}}}, {"id": "iNmBHNzrjii", "original": null, "number": 14, "cdate": 1606280328997, "ddate": null, "tcdate": 1606280328997, "tmdate": 1606280442286, "tddate": null, "forum": "Z532uNJyG5y", "replyto": "qbw_OlNrgMF", "invitation": "ICLR.cc/2021/Conference/Paper1033/-/Official_Comment", "content": {"title": "statistical significance of results", "comment": "The authors didn't address my question \"Did the authors perform a significance study?\"\nA significance test such as double-sided t-test is needed to verifying whether the proposed method is significantly better than baselines. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1033/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1033/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterative Graph Self-Distillation", "authorids": ["~Hanlin_Zhang1", "~Shuai_Lin1", "~Weiyang_Liu1", "~Pan_Zhou3", "~Jian_Tang1", "~Xiaodan_Liang2", "~Eric_Xing1"], "authors": ["Hanlin Zhang", "Shuai Lin", "Weiyang Liu", "Pan Zhou", "Jian Tang", "Xiaodan Liang", "Eric Xing"], "keywords": ["graph-level representation learning", "knowledge distillation"], "abstract": "How to discriminatively vectorize graphs is a fundamental challenge that attracts increasing attentions in recent years. Motivated by the recent success of unsupervised contrastive learning, we aim to learn graph-level representation in an unsupervised manner. Specifically, we propose a novel unsupervised graph learning paradigm called Iterative Graph Self-Distillation (IGSD) which iteratively performs the teacher-student distillation with graph augmentations. Different from conventional knowledge distillation, IGSD constructs the teacher with an exponential moving average of the student model and distills the knowledge of itself. The intuition behind IGSD is to predict the teacher network representation of the graph pairs under different augmented views. As a natural extension, we also apply IGSD to semi-supervised scenarios by jointly regularizing the network with both supervised and unsupervised contrastive loss. Finally, we show that finetuning the IGSD-trained models with self-training can further improve the graph representation power. Empirically, we achieve significant and consistent performance gain on various graph datasets in both unsupervised and semi-supervised settings, which well validates the superiority of IGSD.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|iterative_graph_selfdistillation", "supplementary_material": "/attachment/654a567fb3318e72b661d094196bcb030e58e15f.zip", "pdf": "/pdf/accdb4500871b1a64dfa37738f2c08f65583a742.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ugHLj63uvr", "_bibtex": "@misc{\nzhang2021iterative,\ntitle={Iterative Graph Self-Distillation},\nauthor={Hanlin Zhang and Shuai Lin and Weiyang Liu and Pan Zhou and Jian Tang and Xiaodan Liang and Eric Xing},\nyear={2021},\nurl={https://openreview.net/forum?id=Z532uNJyG5y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Z532uNJyG5y", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1033/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1033/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1033/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1033/Authors|ICLR.cc/2021/Conference/Paper1033/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1033/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864438, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1033/-/Official_Comment"}}}, {"id": "gWQecQlmuS8", "original": null, "number": 12, "cdate": 1605540860886, "ddate": null, "tcdate": 1605540860886, "tmdate": 1605540860886, "tddate": null, "forum": "Z532uNJyG5y", "replyto": "Z532uNJyG5y", "invitation": "ICLR.cc/2021/Conference/Paper1033/-/Official_Comment", "content": {"title": "Submission Revision 1: Summary of Changes:", "comment": "We would like to thank all the reviewers for your constructive reviews. We\u2019ve revised the paper accordingly. The revised part is highlighted as blue. Specifically, we have made the following changes: \n\n1. We conduct additional experiments to compare our proposed method IGSD with two baselines GCC and GCKN. Results are shown in Table 1, showing that IGSD can still get competitive results on most datasets we use. \n2. We conduct ablation experiments to show the effect of projections on the performance of IGSD with results in Appendix A.3.\n3. We move the related work section after the introduction, which helps readers better understand the motivations and differences with existing works on graph representation learning.\n4. In sec 4.1, We discuss the difference between the EMA update in our work and that in existing work MoCo in more detail.\n5. For consistency, we change all $\\tilde{G}$ to be $G^\\prime$ and $\\mathcal{L}^{sup}$ to be $\\mathcal{L}^{supcon}$.\n6. We revise the second paragraph in the sec 3.1 to include the descriptions of contrastive learning procedures with both positive and negative pairs. And we change the notation of $v$ to be $G$ in Figure 1 accordingly. We give specific definitions of f, h and g in sec 4.1.\n7. We revise the Eq. 2 from $\\mathcal{L}^{con}(G_i, G_i)$ to be a more general form $\\mathcal{L}^{con}(G_i, G_j)$, which is applicable for both positive and negative pairs.\n8. We add some statistics of the QM9 dataset: the amount of atoms in each molecule is 9 in the **datasets** part in section 4.1. And additional details of the QM9 dataset can be found in the Appendix C of the InfoGraph paper.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1033/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1033/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterative Graph Self-Distillation", "authorids": ["~Hanlin_Zhang1", "~Shuai_Lin1", "~Weiyang_Liu1", "~Pan_Zhou3", "~Jian_Tang1", "~Xiaodan_Liang2", "~Eric_Xing1"], "authors": ["Hanlin Zhang", "Shuai Lin", "Weiyang Liu", "Pan Zhou", "Jian Tang", "Xiaodan Liang", "Eric Xing"], "keywords": ["graph-level representation learning", "knowledge distillation"], "abstract": "How to discriminatively vectorize graphs is a fundamental challenge that attracts increasing attentions in recent years. Motivated by the recent success of unsupervised contrastive learning, we aim to learn graph-level representation in an unsupervised manner. Specifically, we propose a novel unsupervised graph learning paradigm called Iterative Graph Self-Distillation (IGSD) which iteratively performs the teacher-student distillation with graph augmentations. Different from conventional knowledge distillation, IGSD constructs the teacher with an exponential moving average of the student model and distills the knowledge of itself. The intuition behind IGSD is to predict the teacher network representation of the graph pairs under different augmented views. As a natural extension, we also apply IGSD to semi-supervised scenarios by jointly regularizing the network with both supervised and unsupervised contrastive loss. Finally, we show that finetuning the IGSD-trained models with self-training can further improve the graph representation power. Empirically, we achieve significant and consistent performance gain on various graph datasets in both unsupervised and semi-supervised settings, which well validates the superiority of IGSD.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|iterative_graph_selfdistillation", "supplementary_material": "/attachment/654a567fb3318e72b661d094196bcb030e58e15f.zip", "pdf": "/pdf/accdb4500871b1a64dfa37738f2c08f65583a742.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ugHLj63uvr", "_bibtex": "@misc{\nzhang2021iterative,\ntitle={Iterative Graph Self-Distillation},\nauthor={Hanlin Zhang and Shuai Lin and Weiyang Liu and Pan Zhou and Jian Tang and Xiaodan Liang and Eric Xing},\nyear={2021},\nurl={https://openreview.net/forum?id=Z532uNJyG5y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Z532uNJyG5y", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1033/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1033/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1033/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1033/Authors|ICLR.cc/2021/Conference/Paper1033/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1033/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864438, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1033/-/Official_Comment"}}}, {"id": "8fvzLHjbXMk", "original": null, "number": 10, "cdate": 1605447710509, "ddate": null, "tcdate": 1605447710509, "tmdate": 1605489131143, "tddate": null, "forum": "Z532uNJyG5y", "replyto": "0KSolCXlVNq", "invitation": "ICLR.cc/2021/Conference/Paper1033/-/Official_Comment", "content": {"title": "Response to the AnonReviewer4", "comment": "Thanks a lot for your constructive comments! We have conducted ablation studies and revised the paper according to your suggestions. The response to some of your questions are listed below:\n\nQ1: Why does using self-distillation alleviate the dependency on negative samples mining? The unsupervised objective in Equation 4 still depends on negative samples.\n\nA1: Note that the use of negative samples doesn\u2019t necessarily imply the need of negative mining. In IGSD, we maintain a slow-moving average student network that provides predicted results for consistency-based training in the teacher network, allowing two networks to enhance each other. In our experiments, we search over all possible combinations of views from different graph instances in a batch to construct negative pairs. We don\u2019t increase the batch size dramatically as SimCLR [1] does or keep an extra memory bank for negative samples as MOCO [2] does. We also add additional experiments to investigate the performance with and without a memory bank (with size of three times batch size) to contain past projections as negative examples in the IMDB-B dataset:\n\nIGSD |\t74.7\u00b10.6\n\nIGSD+memory bank| 75.0\u00b11.2\n\nWe show that vanilla IGSD performs well and can be improved slightly with special negative mining mechanisms.\n\nQ2: In Section 2.1, the notation for augmentations. Why are the graphs G_L attached without labels after being augmented?\n\nA2: Thanks for pointing out the typo, we correct the notation. Graphs G_L will be attached with labels after being augmented since we perform label-preserving data augmentation.\n\nQ3: In Section 2.3, authors firstly use PPR to augment node features, then randomly remove edges to create a corrupted graph. According to the description, the question is how many views will be used in the following sections? I guess that the graph feature from the original graph will be fed to the student network, and the augmented corrupted graph will be fed to the teacher network.\n\nA3: Thanks for pointing out the confusion of description and we clarify it in the latest draft. Indeed, random and graph diffusion are two different kinds of data augmentation methods rather than a two-stage procedure. (Randomly corrupting graph features serves as another augmentation choice to validate the robustness of models on different augmentation strategies.) We employ one of these two choices to get an augmented view. In our work, the augmented view and original view are passed interchangeably to the teacher and the student network as in Equation 2.\n\nQ4: The proposed method contains an encoder, projector, and predictor. The question is why do we need a projector g to get a higher dimension z? Does it have a big influence on the performance? Could you please give the complete definition of function g and h?\n\nA4: While we could directly predict the representation y and not a projection z, previous contrastive learning work [1] in the image domain have empirically shown that using this projection improves performance. Per your question, we also further investigate the performance with and without the projector on 4 datasets:\n\n|Dataset | MUTAG| IMDB-B | IMDB-M | PTC\n\n|IGSD | 90.2 \u00b1 0.7 | 74.7 \u00b1 0.6 | 51.5 \u00b1 0.3 | 61.4 \u00b1 1.7\n\n|IGSD w/o projector | 87.68 \u00b1 0.9 | 74.2 \u00b1 0.6 | 51.1 \u00b1 0.6 | 56.7 \u00b1 0.9\n\nResults above show that dropping the projector degrades the performance, which indicates the necessity of a projector.\nThe project hidden size is treated as a hyper-parameter over {1024, 2048}. To investigate its influence on model performance, we fix the size of layers in encoders so that their output size is always 512. Then we conducted ablation experiments on different size of the projection head on IMDB-B with the following results:\n\n|Size | 218 | 512 | 1024 | 2048\n\n|Acc  | 74.5\u00b10.6 | 74.8\u00b10.4 | 74.7\u00b10.6 | 74.9 \u00b1 0.8\n\nIn general, the performance is insensitive to the projection size while a larger projection size could slightly improve the unsupervised learning performance.\nThanks for pointing the definition out. We give a clear definition of function g, h and L^{G_i, G_j}, the procedure of the contrastive learning process in section 3.1 in the latest draft.\n\nQ5: The definition of L^{con} in Equation 2 is for positive samples extracted from the same graph G_i. However, the complete unsupervised loss needs negative samples G_j. Could you please also give the definition for L^{G_i, G_j}?\n\nA5: Thanks for pointing out the confusion. We change the equation to be the definition of L^{G_i, G_j}, so that L^{G_i, G_i} will be a special case that also obeys Equation 2.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1033/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1033/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterative Graph Self-Distillation", "authorids": ["~Hanlin_Zhang1", "~Shuai_Lin1", "~Weiyang_Liu1", "~Pan_Zhou3", "~Jian_Tang1", "~Xiaodan_Liang2", "~Eric_Xing1"], "authors": ["Hanlin Zhang", "Shuai Lin", "Weiyang Liu", "Pan Zhou", "Jian Tang", "Xiaodan Liang", "Eric Xing"], "keywords": ["graph-level representation learning", "knowledge distillation"], "abstract": "How to discriminatively vectorize graphs is a fundamental challenge that attracts increasing attentions in recent years. Motivated by the recent success of unsupervised contrastive learning, we aim to learn graph-level representation in an unsupervised manner. Specifically, we propose a novel unsupervised graph learning paradigm called Iterative Graph Self-Distillation (IGSD) which iteratively performs the teacher-student distillation with graph augmentations. Different from conventional knowledge distillation, IGSD constructs the teacher with an exponential moving average of the student model and distills the knowledge of itself. The intuition behind IGSD is to predict the teacher network representation of the graph pairs under different augmented views. As a natural extension, we also apply IGSD to semi-supervised scenarios by jointly regularizing the network with both supervised and unsupervised contrastive loss. Finally, we show that finetuning the IGSD-trained models with self-training can further improve the graph representation power. Empirically, we achieve significant and consistent performance gain on various graph datasets in both unsupervised and semi-supervised settings, which well validates the superiority of IGSD.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|iterative_graph_selfdistillation", "supplementary_material": "/attachment/654a567fb3318e72b661d094196bcb030e58e15f.zip", "pdf": "/pdf/accdb4500871b1a64dfa37738f2c08f65583a742.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ugHLj63uvr", "_bibtex": "@misc{\nzhang2021iterative,\ntitle={Iterative Graph Self-Distillation},\nauthor={Hanlin Zhang and Shuai Lin and Weiyang Liu and Pan Zhou and Jian Tang and Xiaodan Liang and Eric Xing},\nyear={2021},\nurl={https://openreview.net/forum?id=Z532uNJyG5y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Z532uNJyG5y", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1033/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1033/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1033/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1033/Authors|ICLR.cc/2021/Conference/Paper1033/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1033/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864438, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1033/-/Official_Comment"}}}, {"id": "hHM8hNsr8kZ", "original": null, "number": 8, "cdate": 1605447126899, "ddate": null, "tcdate": 1605447126899, "tmdate": 1605483616017, "tddate": null, "forum": "Z532uNJyG5y", "replyto": "0Cs2HNnWG0W", "invitation": "ICLR.cc/2021/Conference/Paper1033/-/Official_Comment", "content": {"title": "Response to the AnonReviewer1", "comment": "Thanks a lot for your constructive comments! We have conducted more experiments and revised the paper according to your suggestions. The responses to some of your questions are listed below:\n\nQ1: Motivations & Differences of IGSD from existing work.\n\nA1: Note that we discuss the differences of IGSD with existing work in the introduction, related work section and appendix. In the related work, we categories the contrastive learning into context-instance contrast and context-context contrast [1]. Most graph representation learning works focus on context-instance contrast like MI-based models while IGSD belongs to context-context contrast.\n\nIn the unsupervised learning part, the introduction of contrastive learning is well-motivated since it avoids hand-crafting domain-specific graph kernels by bringing closer samples from the same instance and separating samples from different instances. It doesn\u2019t suffer from scalability and huge memory consumption issues of graph kernels. Specifically, we approach the graph-level representation learning problem via instance-wise contrastive learning, which is under-explored. With instance-wise discrimination, we are able to alleviate the drawbacks of related works like MI based models.\n\nFor semi-supervised learning, we show that task-agnostic self-distillation would benefit semi-supervised graph-level representation learning by introducing a supervised contrastive loss. By contrast, defining such kind of loss to alleviate biased sampling problems is infeasible for existing works like InfoGraph since subgraphs don\u2019t contain information about graph categories. Besides, we develop a self-training algorithm based on the supervised contrastive loss to leverage the information from pseudo-labels with high confidence. We discuss the benefits of it and the distinctions from conventional self-training algorithms in the last paragraph of sec 4.3.\n\nQ2: In Section 4.3 \"Performance with different amounts of negative pairs\", it is not clear the reasoning of the provided observation from Figure 3a.\n\nA2: Note that the scale of the vertical axis in Fig. 3a is smaller than that of Fig. 2 in the Appendix of CMC-Graph. The performance of CMC-Graph with respect to Batch Size (Negative Samples)  in IMDB-BIN dataset in our paper corresponds to that in original CMC-Graph paper, which both vary slightly as batch size increases. The observation of the increasing performance gain indicates IGSD is better at leveraging negative pairs for learning effective representations than CMC-Graph.\n\nQ3: It is not clear the motivation behind selecting a teacher-student network for obtaining different views of the graph. These networks are normally used for knowledge transfer, but here used for contrastive learning. How is this more beneficial than an ensemble model w/o knowledge transfer step of Eq. 3.\n\nA3: We note that different views of graphs are obtained via data augmentation rather than by teacher-student network. \nOur intuition to use teacher-student networks is that the EMA teacher can distill crucial knowledge to the current model. Such an iterative self-distillation process can (1) effectively distill important knowledge learned in the past to itself, and (2) serve as a regularization to stabilize the training. Our motivation is also validated by [4], in the sense that EMA outperforms ensembling in $\\Pi$ and Mean Teacher models.\nThe motivation of the teacher-student model is to introduce a slow-moving average teacher network that measures consistency against a student one, thus providing a consistency-based training paradigm where two networks can mutually enhance each other [4]. In this way, we don\u2019t need extra designs for MI estimator, subgraph sampling strategies and negative sample mining approaches like increasing batch size dramatically [2] or keeping a memory bank [3]. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1033/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1033/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterative Graph Self-Distillation", "authorids": ["~Hanlin_Zhang1", "~Shuai_Lin1", "~Weiyang_Liu1", "~Pan_Zhou3", "~Jian_Tang1", "~Xiaodan_Liang2", "~Eric_Xing1"], "authors": ["Hanlin Zhang", "Shuai Lin", "Weiyang Liu", "Pan Zhou", "Jian Tang", "Xiaodan Liang", "Eric Xing"], "keywords": ["graph-level representation learning", "knowledge distillation"], "abstract": "How to discriminatively vectorize graphs is a fundamental challenge that attracts increasing attentions in recent years. Motivated by the recent success of unsupervised contrastive learning, we aim to learn graph-level representation in an unsupervised manner. Specifically, we propose a novel unsupervised graph learning paradigm called Iterative Graph Self-Distillation (IGSD) which iteratively performs the teacher-student distillation with graph augmentations. Different from conventional knowledge distillation, IGSD constructs the teacher with an exponential moving average of the student model and distills the knowledge of itself. The intuition behind IGSD is to predict the teacher network representation of the graph pairs under different augmented views. As a natural extension, we also apply IGSD to semi-supervised scenarios by jointly regularizing the network with both supervised and unsupervised contrastive loss. Finally, we show that finetuning the IGSD-trained models with self-training can further improve the graph representation power. Empirically, we achieve significant and consistent performance gain on various graph datasets in both unsupervised and semi-supervised settings, which well validates the superiority of IGSD.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|iterative_graph_selfdistillation", "supplementary_material": "/attachment/654a567fb3318e72b661d094196bcb030e58e15f.zip", "pdf": "/pdf/accdb4500871b1a64dfa37738f2c08f65583a742.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ugHLj63uvr", "_bibtex": "@misc{\nzhang2021iterative,\ntitle={Iterative Graph Self-Distillation},\nauthor={Hanlin Zhang and Shuai Lin and Weiyang Liu and Pan Zhou and Jian Tang and Xiaodan Liang and Eric Xing},\nyear={2021},\nurl={https://openreview.net/forum?id=Z532uNJyG5y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Z532uNJyG5y", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1033/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1033/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1033/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1033/Authors|ICLR.cc/2021/Conference/Paper1033/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1033/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864438, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1033/-/Official_Comment"}}}, {"id": "eUUwkIWsklu", "original": null, "number": 11, "cdate": 1605447811054, "ddate": null, "tcdate": 1605447811054, "tmdate": 1605448189106, "tddate": null, "forum": "Z532uNJyG5y", "replyto": "8fvzLHjbXMk", "invitation": "ICLR.cc/2021/Conference/Paper1033/-/Official_Comment", "content": {"title": "Response to the AnonReviewer4 Cont.", "comment": "Q6: The overall loss consists of supervised and unsupervised loss. The L^{Sup} has conflict to the first term in Equation 7. Both of them use labels but it's difficult to tell which one should be aligned with the supervised loss shown in the ablation study (Table 2). The SupCon has never been shown in the main content before. Please pay attention to make it clear.\n\nA6: Thanks for pointing it out. We change the L^{Sup} to be L^{Supcon} for clarity. Note that in the semi-supervised settings, we always include standard supervised loss like cross-entropy or mean square error as described in the text right above the Equation 7. Our ablation study aims to show the supervised contrastive loss can provide extra benefits with standard supervised training.\n\n[1] Chen et al., A Simple Framework for Contrastive Learning of Visual Representations. ICML 2020.\n\n[2] He et al., Momentum Contrast for Unsupervised Visual Representation Learning. CVPR 2020."}, "signatures": ["ICLR.cc/2021/Conference/Paper1033/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1033/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterative Graph Self-Distillation", "authorids": ["~Hanlin_Zhang1", "~Shuai_Lin1", "~Weiyang_Liu1", "~Pan_Zhou3", "~Jian_Tang1", "~Xiaodan_Liang2", "~Eric_Xing1"], "authors": ["Hanlin Zhang", "Shuai Lin", "Weiyang Liu", "Pan Zhou", "Jian Tang", "Xiaodan Liang", "Eric Xing"], "keywords": ["graph-level representation learning", "knowledge distillation"], "abstract": "How to discriminatively vectorize graphs is a fundamental challenge that attracts increasing attentions in recent years. Motivated by the recent success of unsupervised contrastive learning, we aim to learn graph-level representation in an unsupervised manner. Specifically, we propose a novel unsupervised graph learning paradigm called Iterative Graph Self-Distillation (IGSD) which iteratively performs the teacher-student distillation with graph augmentations. Different from conventional knowledge distillation, IGSD constructs the teacher with an exponential moving average of the student model and distills the knowledge of itself. The intuition behind IGSD is to predict the teacher network representation of the graph pairs under different augmented views. As a natural extension, we also apply IGSD to semi-supervised scenarios by jointly regularizing the network with both supervised and unsupervised contrastive loss. Finally, we show that finetuning the IGSD-trained models with self-training can further improve the graph representation power. Empirically, we achieve significant and consistent performance gain on various graph datasets in both unsupervised and semi-supervised settings, which well validates the superiority of IGSD.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|iterative_graph_selfdistillation", "supplementary_material": "/attachment/654a567fb3318e72b661d094196bcb030e58e15f.zip", "pdf": "/pdf/accdb4500871b1a64dfa37738f2c08f65583a742.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ugHLj63uvr", "_bibtex": "@misc{\nzhang2021iterative,\ntitle={Iterative Graph Self-Distillation},\nauthor={Hanlin Zhang and Shuai Lin and Weiyang Liu and Pan Zhou and Jian Tang and Xiaodan Liang and Eric Xing},\nyear={2021},\nurl={https://openreview.net/forum?id=Z532uNJyG5y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Z532uNJyG5y", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1033/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1033/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1033/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1033/Authors|ICLR.cc/2021/Conference/Paper1033/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1033/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864438, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1033/-/Official_Comment"}}}, {"id": "NCyneDT55GX", "original": null, "number": 9, "cdate": 1605447201135, "ddate": null, "tcdate": 1605447201135, "tmdate": 1605447294225, "tddate": null, "forum": "Z532uNJyG5y", "replyto": "hHM8hNsr8kZ", "invitation": "ICLR.cc/2021/Conference/Paper1033/-/Official_Comment", "content": {"title": "Response to the AnonReviewer1 Cont.", "comment": "Q4:The core difference of IGSD from CMC-graph is that CMC uses MI based contrastive between local patch representation and graph rep, while IGSD uses L2 based contrastive between 2 graph representations. Input, Encoders and projections are the same for both architectures. It could be useful to add some analysis to discuss these differences and their contributions to clearly understand the significance of IGSD.\n\nA4: Note that we include the discussions on the differences and motivations in the 2nd, 3rd, 4th paragraph in the introduction section. Although we keep only the encoders the same with baselines for fair comparisons, the components and training paradigms of IGSD and CMC-Graph are substantially different: \n\nCMC-Graph: Two networks in CMC-graph are symmetrical. Two different MLPs are used for encoding the local and global representations, respectively. Then an additional discriminator is used for estimating the mutual information of global and local embeddings for both positive and negative pairs following the InfoMax principle. \n\nIGSD: Two networks in IGSD are asymmetric. MLPs are employed as projectors to attain graph representations via ReLU nonlinearity, defining a consistency loss in Equation 2 for training. The student network is trainable with L2 norm while the teacher network is updated with EMA. The contribution of the design is that two networks can mutually enhance each other iteratively.\n\nQ5:This paper seems to have state-of-the-art results (although it is based on graph kernel). Why are the results not included? Convolutional Kernel Networks for Graph-Structured Data, ICML-2020\n\nA5: Thanks for pointing out this related work. Note that one motivation of IGSD is using contrastive graph representation learning to alleviate the flaws of graph kernels and we have already included 6 representative graph kernel baselines. The setting in original GCKN paper is different from ours and other contrastive learning works [5, 6]: GCKN uses different train-test splits for nested 10-fold cross validation with $C$ parameter of SVC is in 1/n \u00d7 np.logspace(-3, 4, 60), requiring 10 times more computation. \n\nWe conducted experiments using Stratified10fold cross validation with $C$ parameters in SVC in {1e-3, 1e-2, . . . , 1e2, 1e3} to compare with GCKN with results as follows:\n\nDataset | MUTAG | IMDB-B | IMDB-M | NCI1 | COLLAB | PTC\n\nOurs | 90.2\u00b10.7 | 74.7\u00b10.6 | 51.5\u00b10.3 | 75.4\u00b10.3 | 70.4\u00b11.1 | 61.4\u00b11.7\n\nGCKN | 87.2\u00b16.8 | 70.5\u00b13.1 | 50.8\u00b10.8 | 70.6\u00b12.0 | 54.3\u00b11.0 | 58.4\u00b17.6\n\nIn the same setting, GCKN outperforms other graph kernels but IGSD performs consistently better than GCKN in all datasets, especially in large-scale ones like NCI1 and COLLAB. \n\nDespite the effectiveness of GCKN in the unsupervised tasks, there is no free lunch: \nFor COLLAB, GCKN learns the anchor points $Z_j$ for each layer by K-means over 300000 extracted paths from each training fold, which requires huge memory and computation consumption. As indicated in the original paper of GCKN, GCKN\u2019s major limitation is the exponential complexity of (long) path enumeration, which requires to compute the feature map $\\phi$ and prevents us from using long-length extraction as soon as the graph is dense (which corresponds to the inferior performance in COLLAB and NCI1 datasets). \nBesides, as a multilayer kernel for graphs based on paths, GCKN also needs hand-crafted (differentiable) kernel functions (such as the exponential function described in the \u201cInterpretation as a GNN\u201d subsection of section 3 in the original paper), which is one of the motivations for adopting end-to-end or iterative contrastive learning (with common GNNs in the loop). \nThe process of GCKN for attaining node representations is similar to that of message-passing process in GNNs: path extraction, kernel mapping and path aggregation, which only serves as the role of encoder in our model. \n\n\n[1] Liu et al., Self-supervised Learning: Generative or Contrastive. Arxiv.\n\n[2] Chen et al., A Simple Framework for Contrastive Learning of Visual Representations. ICML 2020.\n\n[3] He et al., Momentum Contrast for Unsupervised Visual Representation Learning. CVPR 2020.\n\n[4] Athiwaratkun et al., There Are Many Consistent Explanations of Unlabeled Data: Why You Should Average. ICLR 2019.\n\n[5] Sun et al., Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization. ICLR 2019.\n\n[6] Hassani et al., Contrastive Multi-View Representation Learning on Graphs. ICML 2020."}, "signatures": ["ICLR.cc/2021/Conference/Paper1033/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1033/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterative Graph Self-Distillation", "authorids": ["~Hanlin_Zhang1", "~Shuai_Lin1", "~Weiyang_Liu1", "~Pan_Zhou3", "~Jian_Tang1", "~Xiaodan_Liang2", "~Eric_Xing1"], "authors": ["Hanlin Zhang", "Shuai Lin", "Weiyang Liu", "Pan Zhou", "Jian Tang", "Xiaodan Liang", "Eric Xing"], "keywords": ["graph-level representation learning", "knowledge distillation"], "abstract": "How to discriminatively vectorize graphs is a fundamental challenge that attracts increasing attentions in recent years. Motivated by the recent success of unsupervised contrastive learning, we aim to learn graph-level representation in an unsupervised manner. Specifically, we propose a novel unsupervised graph learning paradigm called Iterative Graph Self-Distillation (IGSD) which iteratively performs the teacher-student distillation with graph augmentations. Different from conventional knowledge distillation, IGSD constructs the teacher with an exponential moving average of the student model and distills the knowledge of itself. The intuition behind IGSD is to predict the teacher network representation of the graph pairs under different augmented views. As a natural extension, we also apply IGSD to semi-supervised scenarios by jointly regularizing the network with both supervised and unsupervised contrastive loss. Finally, we show that finetuning the IGSD-trained models with self-training can further improve the graph representation power. Empirically, we achieve significant and consistent performance gain on various graph datasets in both unsupervised and semi-supervised settings, which well validates the superiority of IGSD.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|iterative_graph_selfdistillation", "supplementary_material": "/attachment/654a567fb3318e72b661d094196bcb030e58e15f.zip", "pdf": "/pdf/accdb4500871b1a64dfa37738f2c08f65583a742.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ugHLj63uvr", "_bibtex": "@misc{\nzhang2021iterative,\ntitle={Iterative Graph Self-Distillation},\nauthor={Hanlin Zhang and Shuai Lin and Weiyang Liu and Pan Zhou and Jian Tang and Xiaodan Liang and Eric Xing},\nyear={2021},\nurl={https://openreview.net/forum?id=Z532uNJyG5y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Z532uNJyG5y", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1033/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1033/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1033/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1033/Authors|ICLR.cc/2021/Conference/Paper1033/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1033/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864438, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1033/-/Official_Comment"}}}, {"id": "nXItuB33Ov7", "original": null, "number": 7, "cdate": 1605292529823, "ddate": null, "tcdate": 1605292529823, "tmdate": 1605430519670, "tddate": null, "forum": "Z532uNJyG5y", "replyto": "qbw_OlNrgMF", "invitation": "ICLR.cc/2021/Conference/Paper1033/-/Official_Comment", "content": {"title": "Response to the AnonReviewer2 Cont.", "comment": "Q6: In equation (3), the authors can draw a connection with MoCo.\n\nA6: Thanks for the suggestion. We add the discussion in the latest draft: MoCo uses an EMA of encoder and momentum encoder to update the encoder to ensure the rich consistency of dictionary keys in the memory bank. On the other hand, IGSD uses a moving average network to produce prediction targets between latents, enforcing the consistency of teacher and student for training the student network.\n\nQ7: In Table 2, why didn't report mean and standard deviation of the results?\n\nA7: We report the best result in semi-supervised graph classification, which is widely employed in existing works. Note that in the unsupervised settings, all models use GCNs as backbones to learn low-dimensional embeddings of graphs. Then SVM classifiers (SVCs) are trained on top. And the result is more sensitive to factors like random seed than learning an end-to-end model in the semi-supervised settings since we also include standard supervised loss like cross entropy or mean square error. Besides, the self-training algorithms in the semi-supervised settings are trained iteratively with the augmented labeled dataset, which is very time-consuming if we repeat every experiment a few times to report the mean and standard deviation. Moreover, we train all models for 300 epochs while they all converge within 250 epochs in this setting, which means reporting the best results can manifest favorable model performance.\nWe give a few examples below to show there is basically no gap between mean+std and our reported ones and will run the experiments in the revision:\n\n|Dataset | IMDB-B| IMDB-M| COLLAB| NCI1\n\n|Ours(Supcon) |75.0| 52.0| 73.4| 67.9\n\n|Ours(Supcon) |74.5\u00b10.8| 52.8\u00b11.2| 72.2\u00b10.8| 67.5\u00b11.6\n\nQ8: For this result \"When batch size is greater than 32, IGSD outperforms CMC-Graph and the performance gap becomes larger as the batch size increases.\", can the authors provide a reason that can possibly explain this phenomenon?\n\nA8: Note that the scale of the vertical axis in Fig. 3a is smaller than that of Fig. 2 in the Appendix of CMC-Graph paper. The performance of CMC-Graph with respect to Batch Size (Negative Samples) in IMDB-BIN dataset in our paper corresponds to that in original CMC-Graph paper: the change of the results are both not significant as batch size increases. Moreover, since contrastive learning on negative samples is done batch-wise, larger batch size indicates more combinations of negative pairs. Thus the phenomenon can be explained that IGSD are better at leveraging negative samples to learn effective representations than CMC-Graph.\n\nQ9: The authors can add some statistics of the datasets used in Figure 2.\n\nA9: Note that we have introduced the statistics of QM9 datasets in section 4.1 in the primary draft: QM9 [4] contains 134,000 drug-like organic molecules. We select the first ten physicochemical properties as regression targets for training and evaluation. (Each molecule can have 12 properties and every property will be an independent regression task). We also describe the number of atoms of each molecule to be 9 in the latest draft.\n\n[1] Chen et al., Convolutional Kernel Networks for Graph-Structured Data. ICML 2020.\n\n[2] Hassani et al., Contrastive Multi-View Representation Learning on Graphs. ICML 2020.\n\n[3] Sun et al., InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization. ICLR 2020.\n\n[4] Ramakrishnan et al., Quantum chemistry structures and properties of 134 kilo molecules. Nature 2014.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1033/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1033/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterative Graph Self-Distillation", "authorids": ["~Hanlin_Zhang1", "~Shuai_Lin1", "~Weiyang_Liu1", "~Pan_Zhou3", "~Jian_Tang1", "~Xiaodan_Liang2", "~Eric_Xing1"], "authors": ["Hanlin Zhang", "Shuai Lin", "Weiyang Liu", "Pan Zhou", "Jian Tang", "Xiaodan Liang", "Eric Xing"], "keywords": ["graph-level representation learning", "knowledge distillation"], "abstract": "How to discriminatively vectorize graphs is a fundamental challenge that attracts increasing attentions in recent years. Motivated by the recent success of unsupervised contrastive learning, we aim to learn graph-level representation in an unsupervised manner. Specifically, we propose a novel unsupervised graph learning paradigm called Iterative Graph Self-Distillation (IGSD) which iteratively performs the teacher-student distillation with graph augmentations. Different from conventional knowledge distillation, IGSD constructs the teacher with an exponential moving average of the student model and distills the knowledge of itself. The intuition behind IGSD is to predict the teacher network representation of the graph pairs under different augmented views. As a natural extension, we also apply IGSD to semi-supervised scenarios by jointly regularizing the network with both supervised and unsupervised contrastive loss. Finally, we show that finetuning the IGSD-trained models with self-training can further improve the graph representation power. Empirically, we achieve significant and consistent performance gain on various graph datasets in both unsupervised and semi-supervised settings, which well validates the superiority of IGSD.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|iterative_graph_selfdistillation", "supplementary_material": "/attachment/654a567fb3318e72b661d094196bcb030e58e15f.zip", "pdf": "/pdf/accdb4500871b1a64dfa37738f2c08f65583a742.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ugHLj63uvr", "_bibtex": "@misc{\nzhang2021iterative,\ntitle={Iterative Graph Self-Distillation},\nauthor={Hanlin Zhang and Shuai Lin and Weiyang Liu and Pan Zhou and Jian Tang and Xiaodan Liang and Eric Xing},\nyear={2021},\nurl={https://openreview.net/forum?id=Z532uNJyG5y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Z532uNJyG5y", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1033/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1033/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1033/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1033/Authors|ICLR.cc/2021/Conference/Paper1033/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1033/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864438, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1033/-/Official_Comment"}}}, {"id": "qbw_OlNrgMF", "original": null, "number": 6, "cdate": 1605291867960, "ddate": null, "tcdate": 1605291867960, "tmdate": 1605430171330, "tddate": null, "forum": "Z532uNJyG5y", "replyto": "g0FAaD_-Pp", "invitation": "ICLR.cc/2021/Conference/Paper1033/-/Official_Comment", "content": {"title": "Response to the AnonReviewer2", "comment": "Thanks a lot for your constructive comments and suggestions! We have conducted more experiments and revised the paper according to your suggestions. The response to some of your questions are listed below:\n\nQ1: The improvement of the proposed approach over baselines seems not significant. For example, in Table 1, comparing the mean and standard deviation of the proposed approach and CMC-GRAPH, it seems that the difference is not statistically significant. Did the authors perform a significance study?\n\nA1: In general, instead of proposing \u201ca distillation approach for unsupervised graph representation learning\u201d only, IGSD provides a feasible approach to learn graph-level representations in both unsupervised and **semi-supervised** settings. In unsupervised learning tasks, we get competitive results with current state-of-the-art models like CMC-Graph. In particular, we achieve significantly better results than baselines in the semi-supervised settings across various datasets as shown in Table 2 and Figure 2. By contrast, some MI-based methods like InfoGraph and CMC-Graph can only be extended to semi-supervised settings in the form of multi-task learning $ \\mathcal{L}_l (D_l, \\theta) + w \\mathcal{L}_u (D_u, \\theta)$ while the supervised contrastive loss in our model can bring significant performance gains indicated in Table 2. \n\nQ2: In the experiments, why the authors didn't compare with GCC, which is a contrastive self-supervised learning approach applied to graph classification?\n\nA2: As indicated in the related work section, our work is fundamentally different from GCC. GCC focus on learning transferable graph representation across different graph datasets and the contrastive scheme is done through subgraph instance discrimination. By contrast, IGSD aims at learning graph-level representation by directly contrasting graph instances such that data augmentation strategies and graph labels can be utilized naturally and effectively. \nWe compare our model with GCC and the results are in A3.\n\nQ3: There are many other unsupervised graph representation learning methods. The authors need to compare with more to substantiate this work.\n\nA3: Thanks for pointing it out. We add a (sub)graph instance-wise discrimination baseline GCC and a graph kernel baseline GCKN [1] for comparisons. we summarize include the results as follows:\n\n|Dataset| MUTAG| IMDB-B| IMDB-M| NCI1| COLLAB| PTC\n\n|GCKN| 87.2\u00b16.8|70.5\u00b13.1|50.8\u00b10.8|70.6\u00b12.0|54.3\u00b11.0|58.4\u00b17.6\n\n|GCC| 86.4\u00b10.5| 71.9\u00b10.5| 48.9\u00b10.8| 66.9\u00b10.2| 75.2\u00b10.3| 58.4\u00b11.2\n\n|Ours | 90.2\u00b10.7| 74.7\u00b10.6| 51.5\u00b10.3| 75.4\u00b10.3| 70.4\u00b11.1| 61.4\u00b11.7\n\nAlthough GCC and GCKN can learn graph-level representations in an unsupervised manner, the methodologies of them are fundamentally different from ours: \nGCKN imitates the message passing process of GNNs via path extraction, kernel mapping and path aggregation. But it suffers from common drawbacks of graph kernels: poor scalability, great memory consumption and hand-crafted kernel functions. And it gives inferior results shown in our additional experimental results. (Note that we include the drawbacks of graph kernels in the \u201cGraph Representation Learning\u201d subsection of the related work section in the latest draft.).\nOn the other hand, The goal of GCC is to learn transferable graph representations across different graph datasets and the contrastive scheme is done through subgraph instance discrimination. However, subgraphs are hard to annotate in general, which means incorporating supervised contrastive loss on graph labels in the semi-supervised settings is infeasible. By contrast, IGSD aims at learning graph-level representation by contrasting graph instances directly such that data augmentation strategies and graph labels can be utilized naturally and effectively.\n\nQ4: In hyperparameter tuning, the authors gave the range of hyperparameters tuned, but didn't give the optimal value of the hyperparameters, which make the paper difficult to reproduce.\n\nA4: Following the settings in [2, 3] and most other related works, we give the range of hyperparameters, conduct comprehensive experiments and release the code in the supplemental materials, which make sure the reproducibility of results reported.\n\nQ5: In table 1, the authors excluded some results since they need more than 1 day to obtain. It is common for deep learning models to run several days to obtain results. I don't think it is proper to exclude these results simply because the runtime is more than 24 hours.\n\nA5: Similar to GCKN, MLG needs to recursively build a hierarchy of nested subgraphs, which is extremely unscalable for large-scale graph data. For this reason and following the setting in [3], we didn\u2019t report the result in large-scale datasets like NCI1 and COLLAB."}, "signatures": ["ICLR.cc/2021/Conference/Paper1033/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1033/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterative Graph Self-Distillation", "authorids": ["~Hanlin_Zhang1", "~Shuai_Lin1", "~Weiyang_Liu1", "~Pan_Zhou3", "~Jian_Tang1", "~Xiaodan_Liang2", "~Eric_Xing1"], "authors": ["Hanlin Zhang", "Shuai Lin", "Weiyang Liu", "Pan Zhou", "Jian Tang", "Xiaodan Liang", "Eric Xing"], "keywords": ["graph-level representation learning", "knowledge distillation"], "abstract": "How to discriminatively vectorize graphs is a fundamental challenge that attracts increasing attentions in recent years. Motivated by the recent success of unsupervised contrastive learning, we aim to learn graph-level representation in an unsupervised manner. Specifically, we propose a novel unsupervised graph learning paradigm called Iterative Graph Self-Distillation (IGSD) which iteratively performs the teacher-student distillation with graph augmentations. Different from conventional knowledge distillation, IGSD constructs the teacher with an exponential moving average of the student model and distills the knowledge of itself. The intuition behind IGSD is to predict the teacher network representation of the graph pairs under different augmented views. As a natural extension, we also apply IGSD to semi-supervised scenarios by jointly regularizing the network with both supervised and unsupervised contrastive loss. Finally, we show that finetuning the IGSD-trained models with self-training can further improve the graph representation power. Empirically, we achieve significant and consistent performance gain on various graph datasets in both unsupervised and semi-supervised settings, which well validates the superiority of IGSD.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|iterative_graph_selfdistillation", "supplementary_material": "/attachment/654a567fb3318e72b661d094196bcb030e58e15f.zip", "pdf": "/pdf/accdb4500871b1a64dfa37738f2c08f65583a742.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ugHLj63uvr", "_bibtex": "@misc{\nzhang2021iterative,\ntitle={Iterative Graph Self-Distillation},\nauthor={Hanlin Zhang and Shuai Lin and Weiyang Liu and Pan Zhou and Jian Tang and Xiaodan Liang and Eric Xing},\nyear={2021},\nurl={https://openreview.net/forum?id=Z532uNJyG5y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Z532uNJyG5y", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1033/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1033/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1033/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1033/Authors|ICLR.cc/2021/Conference/Paper1033/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1033/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864438, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1033/-/Official_Comment"}}}, {"id": "sh8EWcCJvE", "original": null, "number": 5, "cdate": 1605149178940, "ddate": null, "tcdate": 1605149178940, "tmdate": 1605292605916, "tddate": null, "forum": "Z532uNJyG5y", "replyto": "k5uTGNPlPFH", "invitation": "ICLR.cc/2021/Conference/Paper1033/-/Official_Comment", "content": {"title": "Response to the AnonReviewer3", "comment": "Thanks a lot for your constructive comments! We have revised the paper according to your suggestions. The responses to some of your questions are listed below:\n\nQ1: why use InfoNCE objective instead of the Jensen-Shannon mutual information objective used in InfoGraph [1] ?\n\nA1: Note that IGSD doesn\u2019t follow the InfoMax principle, which means we don\u2019t need extra discriminator to estimate the mutual information between local subgraphs and global graphs [1, 2]. Instead, we use the InfoNCE objective for instance discrimination, which is widely employed in instance-wise contrastive learning. With the InfoNCE objective, we can bring closer samples from the same instance and separate samples from different instances.\n\nQ2: The major concern about this paper is that the proposed method encourages the closeness of augmented views from the same graph instances but provides no guarantee that the transformation used (graph diffusion and sparsification with PPR + random remove edges in this paper) would be label-preserving. For example, in molecular datasets, if we drop an edge and that edge happens to be in a structural motif, it will drastically change the attributes/labels of the molecule. \n\nA2: Indeed, random and graph diffusion (with PPR) are two different kinds of data augmentation methods (rather than two stages, which is clarified in the latest draft). We only use random dropping for unsupervised graph classification for validating the robustness on different choices on data augmentations. In more structural data like molecular graphs, we only use graph diffusion to reconstruct these continuous relationships. Previous work shows that graph diffusion works well in GNNs in both supervised node classification [3] and unsupervised graph classification with deep InfoMax approach [1,2] on bioinformatics datasets like PTC. Intuitively, GDC amplifies large, well-connected communities and suppresses noisy signals over small-scale structure, which is well-motivated for augmenting molecular graphs with structural motifs. Empirical results shown in Figure 2 demonstrate the superior performance of IGSD with graph diffusion augmentation.\n\nQ3: $v$ represents nodes in section 2.1 and 2.2 and it represents graph instances in section 3.1 and Figure 1. This can be confusing I suggest changing the notation in section 3.1 and Figure 1.1 to $G$\n\nA3: Thanks for the suggestions. We\u2019ve changed the notation in section 3.1 and figure 1.1 for consistency in the latest draft.\n\n\n[1] Sun et al. Infograph: Unsupervised and semisupervised graph-level representation learning via mutual information maximization. ICLR 2019.\n\n[2] Hassani et al. Contrastive Multi-View Representation Learning on Graphs ICML 2020\n\n[3] Klicpera et al. Diffusion Improves Graph Learning NeurIPS 2019\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1033/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1033/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterative Graph Self-Distillation", "authorids": ["~Hanlin_Zhang1", "~Shuai_Lin1", "~Weiyang_Liu1", "~Pan_Zhou3", "~Jian_Tang1", "~Xiaodan_Liang2", "~Eric_Xing1"], "authors": ["Hanlin Zhang", "Shuai Lin", "Weiyang Liu", "Pan Zhou", "Jian Tang", "Xiaodan Liang", "Eric Xing"], "keywords": ["graph-level representation learning", "knowledge distillation"], "abstract": "How to discriminatively vectorize graphs is a fundamental challenge that attracts increasing attentions in recent years. Motivated by the recent success of unsupervised contrastive learning, we aim to learn graph-level representation in an unsupervised manner. Specifically, we propose a novel unsupervised graph learning paradigm called Iterative Graph Self-Distillation (IGSD) which iteratively performs the teacher-student distillation with graph augmentations. Different from conventional knowledge distillation, IGSD constructs the teacher with an exponential moving average of the student model and distills the knowledge of itself. The intuition behind IGSD is to predict the teacher network representation of the graph pairs under different augmented views. As a natural extension, we also apply IGSD to semi-supervised scenarios by jointly regularizing the network with both supervised and unsupervised contrastive loss. Finally, we show that finetuning the IGSD-trained models with self-training can further improve the graph representation power. Empirically, we achieve significant and consistent performance gain on various graph datasets in both unsupervised and semi-supervised settings, which well validates the superiority of IGSD.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|iterative_graph_selfdistillation", "supplementary_material": "/attachment/654a567fb3318e72b661d094196bcb030e58e15f.zip", "pdf": "/pdf/accdb4500871b1a64dfa37738f2c08f65583a742.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ugHLj63uvr", "_bibtex": "@misc{\nzhang2021iterative,\ntitle={Iterative Graph Self-Distillation},\nauthor={Hanlin Zhang and Shuai Lin and Weiyang Liu and Pan Zhou and Jian Tang and Xiaodan Liang and Eric Xing},\nyear={2021},\nurl={https://openreview.net/forum?id=Z532uNJyG5y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Z532uNJyG5y", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1033/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1033/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1033/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1033/Authors|ICLR.cc/2021/Conference/Paper1033/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1033/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864438, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1033/-/Official_Comment"}}}, {"id": "k5uTGNPlPFH", "original": null, "number": 1, "cdate": 1603802130461, "ddate": null, "tcdate": 1603802130461, "tmdate": 1605024546898, "tddate": null, "forum": "Z532uNJyG5y", "replyto": "Z532uNJyG5y", "invitation": "ICLR.cc/2021/Conference/Paper1033/-/Official_Review", "content": {"title": "Official Blind Review #3", "review": "This paper proposed a method for learning graph-level representation in an unsupervised contrastive way. Instead of contrasting between graph-level representation and patch representation like InfoGraph [1], they contrast graph-level representation of a graph to its augmented variation using a teacher-student framework.\n\n* why use InfoNCE objective instead of the Jensen-Shannon mutual information objective used in InfoGraph [1] ? \n\n* The major concern about this paer is that the proposed method encourages the closeness of augmented views from the same graph instances but provide no guarantee that the transformation used (graph diffusion and sparsification with PPR + random remove edges in this paper) would be label-preserving. For example, in molecular datasets, if we drop an edge and that edge happens to be in a structural motif, it will drastically change the attributes/labels of the molecule. \n\n* $v$ represents nodes in section 2.1 and 2.2 and it represents graph instances in section 3.1 and Figure 1. This can be confusing I suggest changing the notation in section 3.1 and Figure 1.1 to $G$\n\n[1] Fan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. Infograph: Unsupervised and semisupervised graph-level representation learning via mutual information maximization. arXiv preprint arXiv:1908.01000, 2019.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1033/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1033/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterative Graph Self-Distillation", "authorids": ["~Hanlin_Zhang1", "~Shuai_Lin1", "~Weiyang_Liu1", "~Pan_Zhou3", "~Jian_Tang1", "~Xiaodan_Liang2", "~Eric_Xing1"], "authors": ["Hanlin Zhang", "Shuai Lin", "Weiyang Liu", "Pan Zhou", "Jian Tang", "Xiaodan Liang", "Eric Xing"], "keywords": ["graph-level representation learning", "knowledge distillation"], "abstract": "How to discriminatively vectorize graphs is a fundamental challenge that attracts increasing attentions in recent years. Motivated by the recent success of unsupervised contrastive learning, we aim to learn graph-level representation in an unsupervised manner. Specifically, we propose a novel unsupervised graph learning paradigm called Iterative Graph Self-Distillation (IGSD) which iteratively performs the teacher-student distillation with graph augmentations. Different from conventional knowledge distillation, IGSD constructs the teacher with an exponential moving average of the student model and distills the knowledge of itself. The intuition behind IGSD is to predict the teacher network representation of the graph pairs under different augmented views. As a natural extension, we also apply IGSD to semi-supervised scenarios by jointly regularizing the network with both supervised and unsupervised contrastive loss. Finally, we show that finetuning the IGSD-trained models with self-training can further improve the graph representation power. Empirically, we achieve significant and consistent performance gain on various graph datasets in both unsupervised and semi-supervised settings, which well validates the superiority of IGSD.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|iterative_graph_selfdistillation", "supplementary_material": "/attachment/654a567fb3318e72b661d094196bcb030e58e15f.zip", "pdf": "/pdf/accdb4500871b1a64dfa37738f2c08f65583a742.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ugHLj63uvr", "_bibtex": "@misc{\nzhang2021iterative,\ntitle={Iterative Graph Self-Distillation},\nauthor={Hanlin Zhang and Shuai Lin and Weiyang Liu and Pan Zhou and Jian Tang and Xiaodan Liang and Eric Xing},\nyear={2021},\nurl={https://openreview.net/forum?id=Z532uNJyG5y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Z532uNJyG5y", "replyto": "Z532uNJyG5y", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1033/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538128689, "tmdate": 1606915800515, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1033/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1033/-/Official_Review"}}}, {"id": "0KSolCXlVNq", "original": null, "number": 3, "cdate": 1603865606255, "ddate": null, "tcdate": 1603865606255, "tmdate": 1605024546696, "tddate": null, "forum": "Z532uNJyG5y", "replyto": "Z532uNJyG5y", "invitation": "ICLR.cc/2021/Conference/Paper1033/-/Official_Review", "content": {"title": "An interesting paper", "review": "Overall Comments:\nLearning graph-level representations with only labels has been explored by many works. However, it's not easy to annotate every graph. This paper applies the ideas from semi-supervised classification task to improve the representation quality learned by graph neural network. Specifically the proposed solution combines several kinds of existing techniques including diffusion graph augmentation, mean teacher consistency, debiased contrastive loss and pseudo class consistency. Finally they are combined together to act as a regularization term by utilizing the unlabelled data. From this point of view, the novelty of this work is incremental, but it's still an interesting work for improving graph-level representations.\n\nClarity:\nThe presentation is not clear enough. There exists many claims that are not clear, shown as follows:\n\n1. In the last sentence of 3rd paragraph in introduction section, it's difficult to get the connection between negative samples mining and self-distillation strategy. Why using the self-distillation can alleviate the dependency on negative samples mining? The unsupervised objective in Equation 4 still depends on negative samples. \n2. In Section 2.1, the notation for augmentations. Why are the graphs G_L attached without labels after being augmented?\n3. In Section 2.3, authors firstly use PPR to augment node features, then randomly remove edges to create a corrupted graphs. According to the description, the question is how many views that will be used in follow sections? I guess that the graph feature from original graph will be fed to student network, and the augmented corrupted graph will be fed to teacher network.\n\nQuestions for Rebuttal:\n1. Please clarify the mentioned questions above.\n\n2. The proposed method contains an encoder, projector, and predictor. The question is why we need a projector g to get a higher dimension z? Does it have a big influence on the performance? Could you please give the complete definition of function g and h?\n\n3. The definition of L^{con}  in Equation 2 is for positive sample extracted from the same graph G_i. However, the complete unsupervised loss needs negative samples G_j. Could you please also give the definition for L^{G_i, G_j}?\n\n4. The overall loss consists of supervised and unsupervised loss. The L^{sup} has conflict to the first term in Equation 7. Both of them use labels but it's difficult to tell which one should be aligned with the supervised loss shown in the ablation study (Table 2). The SupCon has never been shown in the main content before. Please pay attention to make it clear.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1033/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1033/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Iterative Graph Self-Distillation", "authorids": ["~Hanlin_Zhang1", "~Shuai_Lin1", "~Weiyang_Liu1", "~Pan_Zhou3", "~Jian_Tang1", "~Xiaodan_Liang2", "~Eric_Xing1"], "authors": ["Hanlin Zhang", "Shuai Lin", "Weiyang Liu", "Pan Zhou", "Jian Tang", "Xiaodan Liang", "Eric Xing"], "keywords": ["graph-level representation learning", "knowledge distillation"], "abstract": "How to discriminatively vectorize graphs is a fundamental challenge that attracts increasing attentions in recent years. Motivated by the recent success of unsupervised contrastive learning, we aim to learn graph-level representation in an unsupervised manner. Specifically, we propose a novel unsupervised graph learning paradigm called Iterative Graph Self-Distillation (IGSD) which iteratively performs the teacher-student distillation with graph augmentations. Different from conventional knowledge distillation, IGSD constructs the teacher with an exponential moving average of the student model and distills the knowledge of itself. The intuition behind IGSD is to predict the teacher network representation of the graph pairs under different augmented views. As a natural extension, we also apply IGSD to semi-supervised scenarios by jointly regularizing the network with both supervised and unsupervised contrastive loss. Finally, we show that finetuning the IGSD-trained models with self-training can further improve the graph representation power. Empirically, we achieve significant and consistent performance gain on various graph datasets in both unsupervised and semi-supervised settings, which well validates the superiority of IGSD.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|iterative_graph_selfdistillation", "supplementary_material": "/attachment/654a567fb3318e72b661d094196bcb030e58e15f.zip", "pdf": "/pdf/accdb4500871b1a64dfa37738f2c08f65583a742.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ugHLj63uvr", "_bibtex": "@misc{\nzhang2021iterative,\ntitle={Iterative Graph Self-Distillation},\nauthor={Hanlin Zhang and Shuai Lin and Weiyang Liu and Pan Zhou and Jian Tang and Xiaodan Liang and Eric Xing},\nyear={2021},\nurl={https://openreview.net/forum?id=Z532uNJyG5y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Z532uNJyG5y", "replyto": "Z532uNJyG5y", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1033/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538128689, "tmdate": 1606915800515, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1033/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1033/-/Official_Review"}}}], "count": 16}