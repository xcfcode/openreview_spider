{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1486781864938, "tcdate": 1478288450270, "number": 378, "id": "rJqBEPcxe", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "rJqBEPcxe", "signatures": ["~David_Krueger1"], "readers": ["everyone"], "content": {"title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations", "abstract": "We propose zoneout, a novel method for regularizing RNNs.\nAt each timestep, zoneout stochastically forces some hidden units to maintain their previous values.\nLike dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization.\nBut by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks.\nWe perform an empirical investigation of various RNN regularizers, and find that zoneout gives significant performance improvements across tasks. We achieve competitive results with relatively simple models in character- and word-level language modelling on the Penn Treebank and Text8 datasets, and combining with recurrent batch normalization yields state-of-the-art results on permuted sequential MNIST.", "pdf": "/pdf/fdd2ff27bd56cb0045a9dee87e9dce9f8c134b2e.pdf", "TL;DR": "Zoneout is like dropout (for RNNs) but uses identity masks instead of zero masks", "paperhash": "krueger|zoneout_regularizing_rnns_by_randomly_preserving_hidden_activations", "keywords": ["Deep learning"], "conflicts": ["umontreal.ca"], "authors": ["David Krueger", "Tegan Maharaj", "Janos Kramar", "Mohammad Pezeshki", "Nicolas Ballas", "Nan Rosemary Ke", "Anirudh  Goyal", "Yoshua Bengio", "Aaron Courville", "Christopher Pal"], "authorids": ["davidscottkrueger@gmail.com", "tegan.jrm@gmail.com", "ballas.n@gmail.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 15, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396551230, "tcdate": 1486396551230, "number": 1, "id": "BkJ5nG8_x", "invitation": "ICLR.cc/2017/conference/-/paper378/acceptance", "forum": "rJqBEPcxe", "replyto": "rJqBEPcxe", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "Very nice paper, with simple, intuitive idea that works quite well, solving the problem of how to do recurrent dropout.\n \n Pros:\n - Improved results\n - Very simple method\n \n Cons:\n - Almost the best results (aside from Variational Dropout)", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations", "abstract": "We propose zoneout, a novel method for regularizing RNNs.\nAt each timestep, zoneout stochastically forces some hidden units to maintain their previous values.\nLike dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization.\nBut by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks.\nWe perform an empirical investigation of various RNN regularizers, and find that zoneout gives significant performance improvements across tasks. We achieve competitive results with relatively simple models in character- and word-level language modelling on the Penn Treebank and Text8 datasets, and combining with recurrent batch normalization yields state-of-the-art results on permuted sequential MNIST.", "pdf": "/pdf/fdd2ff27bd56cb0045a9dee87e9dce9f8c134b2e.pdf", "TL;DR": "Zoneout is like dropout (for RNNs) but uses identity masks instead of zero masks", "paperhash": "krueger|zoneout_regularizing_rnns_by_randomly_preserving_hidden_activations", "keywords": ["Deep learning"], "conflicts": ["umontreal.ca"], "authors": ["David Krueger", "Tegan Maharaj", "Janos Kramar", "Mohammad Pezeshki", "Nicolas Ballas", "Nan Rosemary Ke", "Anirudh  Goyal", "Yoshua Bengio", "Aaron Courville", "Christopher Pal"], "authorids": ["davidscottkrueger@gmail.com", "tegan.jrm@gmail.com", "ballas.n@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396551710, "id": "ICLR.cc/2017/conference/-/paper378/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "rJqBEPcxe", "replyto": "rJqBEPcxe", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396551710}}}, {"tddate": null, "tmdate": 1484335795846, "tcdate": 1484335795846, "number": 6, "id": "r1hhqjIIe", "invitation": "ICLR.cc/2017/conference/-/paper378/public/comment", "forum": "rJqBEPcxe", "replyto": "HyDxBKMNx", "signatures": ["~Tegan_Maharaj1"], "readers": ["everyone"], "writers": ["~Tegan_Maharaj1"], "content": {"title": "Response", "comment": "Thank you for your review, and great to hear that zoneout has helped in your experiments!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations", "abstract": "We propose zoneout, a novel method for regularizing RNNs.\nAt each timestep, zoneout stochastically forces some hidden units to maintain their previous values.\nLike dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization.\nBut by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks.\nWe perform an empirical investigation of various RNN regularizers, and find that zoneout gives significant performance improvements across tasks. We achieve competitive results with relatively simple models in character- and word-level language modelling on the Penn Treebank and Text8 datasets, and combining with recurrent batch normalization yields state-of-the-art results on permuted sequential MNIST.", "pdf": "/pdf/fdd2ff27bd56cb0045a9dee87e9dce9f8c134b2e.pdf", "TL;DR": "Zoneout is like dropout (for RNNs) but uses identity masks instead of zero masks", "paperhash": "krueger|zoneout_regularizing_rnns_by_randomly_preserving_hidden_activations", "keywords": ["Deep learning"], "conflicts": ["umontreal.ca"], "authors": ["David Krueger", "Tegan Maharaj", "Janos Kramar", "Mohammad Pezeshki", "Nicolas Ballas", "Nan Rosemary Ke", "Anirudh  Goyal", "Yoshua Bengio", "Aaron Courville", "Christopher Pal"], "authorids": ["davidscottkrueger@gmail.com", "tegan.jrm@gmail.com", "ballas.n@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287600046, "id": "ICLR.cc/2017/conference/-/paper378/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJqBEPcxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper378/reviewers", "ICLR.cc/2017/conference/paper378/areachairs"], "cdate": 1485287600046}}}, {"tddate": null, "tmdate": 1484335621355, "tcdate": 1484335621355, "number": 5, "id": "S1T-9i8Ue", "invitation": "ICLR.cc/2017/conference/-/paper378/public/comment", "forum": "rJqBEPcxe", "replyto": "BkoqCtNVx", "signatures": ["~Tegan_Maharaj1"], "readers": ["everyone"], "writers": ["~Tegan_Maharaj1"], "content": {"title": "Suggestion implemented", "comment": "Thank you for your detailed review and thoughtful suggestions. \n\n------\nZoneout has two things working for it - the noise and\n  the ability to pass gradients back without decay. It might help to tease apart\nthe contribution from these two factors. For example, if we use a fixed\nmask over the unrolled network (different at each time step) instead of resampling\nit again for every training case, it would tell us how much help comes from the\nidentity connections alone.\n------\n\nThis seemed like a pretty cool experiment; we have run it and added it in the Appendix, Figure 7. We're grateful for this interesting contribution to the paper!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations", "abstract": "We propose zoneout, a novel method for regularizing RNNs.\nAt each timestep, zoneout stochastically forces some hidden units to maintain their previous values.\nLike dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization.\nBut by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks.\nWe perform an empirical investigation of various RNN regularizers, and find that zoneout gives significant performance improvements across tasks. We achieve competitive results with relatively simple models in character- and word-level language modelling on the Penn Treebank and Text8 datasets, and combining with recurrent batch normalization yields state-of-the-art results on permuted sequential MNIST.", "pdf": "/pdf/fdd2ff27bd56cb0045a9dee87e9dce9f8c134b2e.pdf", "TL;DR": "Zoneout is like dropout (for RNNs) but uses identity masks instead of zero masks", "paperhash": "krueger|zoneout_regularizing_rnns_by_randomly_preserving_hidden_activations", "keywords": ["Deep learning"], "conflicts": ["umontreal.ca"], "authors": ["David Krueger", "Tegan Maharaj", "Janos Kramar", "Mohammad Pezeshki", "Nicolas Ballas", "Nan Rosemary Ke", "Anirudh  Goyal", "Yoshua Bengio", "Aaron Courville", "Christopher Pal"], "authorids": ["davidscottkrueger@gmail.com", "tegan.jrm@gmail.com", "ballas.n@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287600046, "id": "ICLR.cc/2017/conference/-/paper378/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJqBEPcxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper378/reviewers", "ICLR.cc/2017/conference/paper378/areachairs"], "cdate": 1485287600046}}}, {"tddate": null, "tmdate": 1484236877827, "tcdate": 1482224940978, "number": 3, "id": "HkSNBu8Vx", "invitation": "ICLR.cc/2017/conference/-/paper378/official/review", "forum": "rJqBEPcxe", "replyto": "rJqBEPcxe", "signatures": ["ICLR.cc/2017/conference/paper378/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper378/AnonReviewer1"], "content": {"title": "Incremental improvement, not convincing enough", "rating": "7: Good paper, accept", "review": "The authors propose a conceptually simple method for regularisation of recurrent neural networks. The idea is related to dropout, but instead of zeroing out units, they are instead set to their respective values at the preceding time step element-wise with a certain probability.\n\nOverall, the paper is well written. The method is clearly represented up to issues raised by reviewers during the pre-review question phase. The related work is complete and probably the best currently available on the matter of regularising RNNs.\n\nThe experimental section focuses on comparing the method with the current SOTA on a set of NLP benchmarks and a synthetic problem. All of the experiments focus on sequences over discrete values. An additional experiment also shows that the sequential Jacobian is far higher for long-term dependencies than in the dropout case.\n\nOverall, the paper bears great potential. However, I do see some points.\n\n1) As raised during the pre-review questions, I would like to see the results of experiments that feature a complete hyper parameter search. I.e. a proper model selection process,as it should be standard in the community. I do not see why this was not done, especially as the author count seems to indicate that the necessary resources are available.\n\nI want to repeat at this point that Table 2 of the paper shows that validation error is not a reliable estimator for testing error in the respective data set. Thus, overfitting the model selection process is a serious concern here.\nZoneout does not seem to improve that much in the other tasks.\n\n2) Zoneout is not investigated well mathematically. E.g. an analysis of the of the form of gradients from unit K at time step T to unit K\u2019 at time step T-R would have been interesting, especially as these are not necessarily non-zero for dropout. Also, the question whether zoneout has a variational interpretation in the spirit of Yarin Gal\u2019s work is an obvious one. I can see that it is if we treat zoneout in a resnet framework and dropout on the incremental parts. Overall, little effort is done answering the question *why* zoneout works well, even though the literature bears plenty of starting points for such analysis.\n\n3) The data sets used are only symbolic. It would have been great if more ground was covered, i.e. continuous data such as from dynamical systems. To me it is not obvious whether it will transfer right away.\n\n\nAn extreme amount of \u201ctricks\u201d is being published currently for improved RNN training. How does zoneout stand out? It is a nice idea, and simple to implement. However, the paper under delivers: the experiments do not convince me (see 1) and 3)). There authors do not provide convincing theoretical insights either. (2)\n\nConsequently, the paper reduces to a \u201cepsilon improvement, great text, mediocre experimental evaluation, little theoretical insight\u201d.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations", "abstract": "We propose zoneout, a novel method for regularizing RNNs.\nAt each timestep, zoneout stochastically forces some hidden units to maintain their previous values.\nLike dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization.\nBut by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks.\nWe perform an empirical investigation of various RNN regularizers, and find that zoneout gives significant performance improvements across tasks. We achieve competitive results with relatively simple models in character- and word-level language modelling on the Penn Treebank and Text8 datasets, and combining with recurrent batch normalization yields state-of-the-art results on permuted sequential MNIST.", "pdf": "/pdf/fdd2ff27bd56cb0045a9dee87e9dce9f8c134b2e.pdf", "TL;DR": "Zoneout is like dropout (for RNNs) but uses identity masks instead of zero masks", "paperhash": "krueger|zoneout_regularizing_rnns_by_randomly_preserving_hidden_activations", "keywords": ["Deep learning"], "conflicts": ["umontreal.ca"], "authors": ["David Krueger", "Tegan Maharaj", "Janos Kramar", "Mohammad Pezeshki", "Nicolas Ballas", "Nan Rosemary Ke", "Anirudh  Goyal", "Yoshua Bengio", "Aaron Courville", "Christopher Pal"], "authorids": ["davidscottkrueger@gmail.com", "tegan.jrm@gmail.com", "ballas.n@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512604564, "id": "ICLR.cc/2017/conference/-/paper378/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper378/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper378/AnonReviewer3", "ICLR.cc/2017/conference/paper378/AnonReviewer2", "ICLR.cc/2017/conference/paper378/AnonReviewer1"], "reply": {"forum": "rJqBEPcxe", "replyto": "rJqBEPcxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper378/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper378/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512604564}}}, {"tddate": null, "tmdate": 1484236845673, "tcdate": 1484236845673, "number": 2, "id": "SJU4umHLl", "invitation": "ICLR.cc/2017/conference/-/paper378/official/comment", "forum": "rJqBEPcxe", "replyto": "ryloI_EIl", "signatures": ["ICLR.cc/2017/conference/paper378/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper378/AnonReviewer1"], "content": {"title": "Some things addresses", "comment": "Thanks for the improvements. I will adapt my rating."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations", "abstract": "We propose zoneout, a novel method for regularizing RNNs.\nAt each timestep, zoneout stochastically forces some hidden units to maintain their previous values.\nLike dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization.\nBut by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks.\nWe perform an empirical investigation of various RNN regularizers, and find that zoneout gives significant performance improvements across tasks. We achieve competitive results with relatively simple models in character- and word-level language modelling on the Penn Treebank and Text8 datasets, and combining with recurrent batch normalization yields state-of-the-art results on permuted sequential MNIST.", "pdf": "/pdf/fdd2ff27bd56cb0045a9dee87e9dce9f8c134b2e.pdf", "TL;DR": "Zoneout is like dropout (for RNNs) but uses identity masks instead of zero masks", "paperhash": "krueger|zoneout_regularizing_rnns_by_randomly_preserving_hidden_activations", "keywords": ["Deep learning"], "conflicts": ["umontreal.ca"], "authors": ["David Krueger", "Tegan Maharaj", "Janos Kramar", "Mohammad Pezeshki", "Nicolas Ballas", "Nan Rosemary Ke", "Anirudh  Goyal", "Yoshua Bengio", "Aaron Courville", "Christopher Pal"], "authorids": ["davidscottkrueger@gmail.com", "tegan.jrm@gmail.com", "ballas.n@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287599927, "id": "ICLR.cc/2017/conference/-/paper378/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "rJqBEPcxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper378/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper378/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper378/reviewers", "ICLR.cc/2017/conference/paper378/areachairs"], "cdate": 1485287599927}}}, {"tddate": null, "tmdate": 1484191384372, "tcdate": 1484191384372, "number": 4, "id": "ryloI_EIl", "invitation": "ICLR.cc/2017/conference/-/paper378/public/comment", "forum": "rJqBEPcxe", "replyto": "HkSNBu8Vx", "signatures": ["~Tegan_Maharaj1"], "readers": ["everyone"], "writers": ["~Tegan_Maharaj1"], "content": {"title": "Results and model selection clarified; consistent improvement across tasks, and validation error *is* a reliable estimator for testing error ", "comment": "Thank you for your thoughtful and detailed review.  Your comments raise some important concerns which we address individually below.\n\n\nQ:\nI want to repeat at this point that Table 2 of the paper shows that validation error is not a reliable estimator for testing error in the respective data set. Thus, overfitting the model selection process is a serious concern here.\n\nA:\nThank you for pointing out this issue. Our initial table was not clear or representative of our results. We have reformatted the table to clarify both that our test-validation gaps are very consistent within our experiments, and that zoneout does achieve the best validation performance within our experiments. These experiments include a search over regularization techniques and strengths, as detailed below.  We believe this addresses your concerns about model selection.\n\n(To fit everything in one table (Table 1), we had included a mix of results from other papers and from our own experiments. Semeniuta et al.\u2019s recurrent dropout got worse validation performance than zoneout in our experiments, but we then reported results from their paper. This was confusing, and has been corrected.)\n\nWe have also corrected the validation and test values of recurrent dropout on Text8. They were incorrectly reported, and we apologize for any confusion caused by this error.\n\nFor the purposes of early stopping and model selection, we measured validation BPC on sequences of length 100 as in training (as opposed to computing it over the entire sequence, which is what we do at test time). This is why we have a larger gap between validation and test BPC than previous works.\n\nIn order to update Table 1, we reran the best performing recurrent dropout (p=0.25) and zoneout (p_cells=0.5 and p_hiddens = 0.05) models *with* sequence overlap.  In doing so, we also noted that neither of these best-performing models had entirely overfit, so we ran both for longer.  Our implementation of recurrent dropout achieves better performance (1.286 BPC test) than reported by Semeniuta et al. (2016), and zoneout achieves near state of the art 1.252 BPC.\n\n\nQ:\n1) As raised during the pre-review questions, I would like to see the results of experiments that feature a complete hyper parameter search. I.e. a proper model selection process,as it should be standard in the community. I do not see why this was not done, especially as the author count seems to indicate that the necessary resources are available.\n\nA:\nWe had previously run a suite of experiments searching over regularization strength for zoneout, recurrent dropout, and recurrent stochastic depth, but without sequence overlap. We show this search for zoneout in Figure 3, and report the best results achieved with all models in Table 1, and as noted, zoneout achieves the lowest validation score.\n\nGiven our updates to Table 1, if you feel that additional hyperparameter search is necessary, please clarify which hyperparameters you think are most relevant to compare.  We can probably run 20-60 experiments within a few days given our available computing resources.\n\n\nQ:\nZoneout does not seem to improve that much in the other tasks.\n\nA:\nWe respectfully disagree. Zoneout achieves a consistent improvement of more than 4%, relative to a vanilla LSTM  baseline, on all datasets (cf Table 1 and Table 2),  just by plug-and-play with existing models.\n\n\nQ:\n3) The data sets used are only symbolic. It would have been great if more ground was covered, i.e. continuous data such as from dynamical systems. To me it is not obvious whether it will transfer right away.\n\nA:\nThese datasets are standard benchmarks for RNNs and allow us to compare to previously published results. Permuted sequential MNIST does take continuous pixel values as input, and we don\u2019t see any reason to believe that zoneout would not improve performance on tasks with continuous outputs.\n\n\nQ:\n2) Zoneout is not investigated well mathematically.\n\nA:\nWe propose a novel technique and perform a thorough empirical evaluation to demonstrate the benefit of our proposal. We investigate gradient propagation and relate zoneout to other techniques and theory (pseudoensembles, stochastic depth, residual networks, etc.). While it would be great to see further work and theoretically grounded analysis, we think that our technique and results are already of interest to the community."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations", "abstract": "We propose zoneout, a novel method for regularizing RNNs.\nAt each timestep, zoneout stochastically forces some hidden units to maintain their previous values.\nLike dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization.\nBut by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks.\nWe perform an empirical investigation of various RNN regularizers, and find that zoneout gives significant performance improvements across tasks. We achieve competitive results with relatively simple models in character- and word-level language modelling on the Penn Treebank and Text8 datasets, and combining with recurrent batch normalization yields state-of-the-art results on permuted sequential MNIST.", "pdf": "/pdf/fdd2ff27bd56cb0045a9dee87e9dce9f8c134b2e.pdf", "TL;DR": "Zoneout is like dropout (for RNNs) but uses identity masks instead of zero masks", "paperhash": "krueger|zoneout_regularizing_rnns_by_randomly_preserving_hidden_activations", "keywords": ["Deep learning"], "conflicts": ["umontreal.ca"], "authors": ["David Krueger", "Tegan Maharaj", "Janos Kramar", "Mohammad Pezeshki", "Nicolas Ballas", "Nan Rosemary Ke", "Anirudh  Goyal", "Yoshua Bengio", "Aaron Courville", "Christopher Pal"], "authorids": ["davidscottkrueger@gmail.com", "tegan.jrm@gmail.com", "ballas.n@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287600046, "id": "ICLR.cc/2017/conference/-/paper378/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJqBEPcxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper378/reviewers", "ICLR.cc/2017/conference/paper378/areachairs"], "cdate": 1485287600046}}}, {"tddate": null, "tmdate": 1482100370895, "tcdate": 1482100370895, "number": 2, "id": "BkoqCtNVx", "invitation": "ICLR.cc/2017/conference/-/paper378/official/review", "forum": "rJqBEPcxe", "replyto": "rJqBEPcxe", "signatures": ["ICLR.cc/2017/conference/paper378/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper378/AnonReviewer2"], "content": {"title": "Simple idea, well executed.", "rating": "8: Top 50% of accepted papers, clear accept", "review": "Paper Summary\nThis paper proposes a variant of dropout, applicable to RNNs, in which the state\nof a unit is randomly retained, as opposed to being set to zero. This provides\nnoise which gives the regularization effect, but also prevents loss of\ninformation over time, in fact making it easier to send gradients back because\nthey can flow right through the identity connections without attenuation.\nExperiments show that this model works quite well. It is still worse that\nvariational dropout on Penn Tree bank language modeling task, but given the\nsimplicity of the idea it is likely to become widely useful.\n\nStrengths\n- Simple idea that works well.\n- Detailed experiments help understand the effects of the zoneout probabilities\n  and validate its applicability to different tasks/domains.\n\nWeaknesses\n- Does not beat variational dropout (but maybe better hyper-parameter tuning\n  will help).\n\nQuality\nThe experimental design and writeup is high quality.\n\nClarity\nThe paper clear and well written, experimental details seem adequate.\n\nOriginality\nThe proposed idea is novel.\n\nSignificance\nThis paper will be of interest to anyone working with RNNs (which is a large\ngroup of people!).\n\nMinor suggestion-\n- As the authors mention - Zoneout has two things working for it - the noise and\n  the ability to pass gradients back without decay. It might help to tease apart\nthe contribution from these two factors. For example, if we use a fixed\nmask over the unrolled network (different at each time step) instead of resampling\nit again for every training case, it would tell us how much help comes from the\nidentity connections alone.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations", "abstract": "We propose zoneout, a novel method for regularizing RNNs.\nAt each timestep, zoneout stochastically forces some hidden units to maintain their previous values.\nLike dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization.\nBut by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks.\nWe perform an empirical investigation of various RNN regularizers, and find that zoneout gives significant performance improvements across tasks. We achieve competitive results with relatively simple models in character- and word-level language modelling on the Penn Treebank and Text8 datasets, and combining with recurrent batch normalization yields state-of-the-art results on permuted sequential MNIST.", "pdf": "/pdf/fdd2ff27bd56cb0045a9dee87e9dce9f8c134b2e.pdf", "TL;DR": "Zoneout is like dropout (for RNNs) but uses identity masks instead of zero masks", "paperhash": "krueger|zoneout_regularizing_rnns_by_randomly_preserving_hidden_activations", "keywords": ["Deep learning"], "conflicts": ["umontreal.ca"], "authors": ["David Krueger", "Tegan Maharaj", "Janos Kramar", "Mohammad Pezeshki", "Nicolas Ballas", "Nan Rosemary Ke", "Anirudh  Goyal", "Yoshua Bengio", "Aaron Courville", "Christopher Pal"], "authorids": ["davidscottkrueger@gmail.com", "tegan.jrm@gmail.com", "ballas.n@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512604564, "id": "ICLR.cc/2017/conference/-/paper378/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper378/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper378/AnonReviewer3", "ICLR.cc/2017/conference/paper378/AnonReviewer2", "ICLR.cc/2017/conference/paper378/AnonReviewer1"], "reply": {"forum": "rJqBEPcxe", "replyto": "rJqBEPcxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper378/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper378/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512604564}}}, {"tddate": null, "tmdate": 1481966831231, "tcdate": 1481966831231, "number": 1, "id": "HyDxBKMNx", "invitation": "ICLR.cc/2017/conference/-/paper378/official/review", "forum": "rJqBEPcxe", "replyto": "rJqBEPcxe", "signatures": ["ICLR.cc/2017/conference/paper378/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper378/AnonReviewer3"], "content": {"title": "Review", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper tests zoneout against a variety of datasets - character level, word level, and pMNIST classification - showing applicability in a wide range of scenarios. While zoneout acts as a regularizer to prevent overfitting, it also has similarities to residual connections. The continued analysis of this aspect, including analyzing how the gradient flow improves the given tasks, is of great interest and helps show it as an inherent property of zoneout.\n\nThis is a well written paper with a variety of experiments that support the claims. I have also previously used this technique in a recurrent setting and am confident on the positive impact it can have upon tasks. This is likely to become a standard technique used within RNNs across various frameworks.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations", "abstract": "We propose zoneout, a novel method for regularizing RNNs.\nAt each timestep, zoneout stochastically forces some hidden units to maintain their previous values.\nLike dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization.\nBut by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks.\nWe perform an empirical investigation of various RNN regularizers, and find that zoneout gives significant performance improvements across tasks. We achieve competitive results with relatively simple models in character- and word-level language modelling on the Penn Treebank and Text8 datasets, and combining with recurrent batch normalization yields state-of-the-art results on permuted sequential MNIST.", "pdf": "/pdf/fdd2ff27bd56cb0045a9dee87e9dce9f8c134b2e.pdf", "TL;DR": "Zoneout is like dropout (for RNNs) but uses identity masks instead of zero masks", "paperhash": "krueger|zoneout_regularizing_rnns_by_randomly_preserving_hidden_activations", "keywords": ["Deep learning"], "conflicts": ["umontreal.ca"], "authors": ["David Krueger", "Tegan Maharaj", "Janos Kramar", "Mohammad Pezeshki", "Nicolas Ballas", "Nan Rosemary Ke", "Anirudh  Goyal", "Yoshua Bengio", "Aaron Courville", "Christopher Pal"], "authorids": ["davidscottkrueger@gmail.com", "tegan.jrm@gmail.com", "ballas.n@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512604564, "id": "ICLR.cc/2017/conference/-/paper378/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper378/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper378/AnonReviewer3", "ICLR.cc/2017/conference/paper378/AnonReviewer2", "ICLR.cc/2017/conference/paper378/AnonReviewer1"], "reply": {"forum": "rJqBEPcxe", "replyto": "rJqBEPcxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper378/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper378/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512604564}}}, {"tddate": null, "tmdate": 1481812951036, "tcdate": 1481812951029, "number": 1, "id": "rJJyhXlEe", "invitation": "ICLR.cc/2017/conference/-/paper378/official/comment", "forum": "rJqBEPcxe", "replyto": "BJ27BQlVe", "signatures": ["ICLR.cc/2017/conference/paper378/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper378/AnonReviewer1"], "content": {"title": "...", "comment": "\"We see no reason to suspect that a hyper-parameter search over model architectures would lead to worse results for zoneout; we presume that tuning the architecture in conjunction with the zoneout probabilities could only improve our results. Could you clarify why you think this would lead to worse results?\"\n\nZoneout has arguably a much less regularizing effect than just doing dropout: information is not completely forgotten, instead the transition to the next step is perturbed. Table 2 also indicates that the validation error is not a good estimate of the testing error. Consequently, there is a chance that a certain setting of hyper parameters will lead to lower validation loss but higher testing loss. Maybe one of the other reviewers is experienced enough with the data sets to comment.\n\nNevertheless, the hypothesis \"zoneout leads to lower test error when model selection is done with zoneout models\" is not tested, although this is arguably of the highest interest to the reader.\n\n\u201cWas it out of convenience for not having to do it, or was it because the authors want to stress this behaviour as a feature of zoneout?\u201d\n\nPut more simply: why did the authors not run a full model selection for zoneout?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations", "abstract": "We propose zoneout, a novel method for regularizing RNNs.\nAt each timestep, zoneout stochastically forces some hidden units to maintain their previous values.\nLike dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization.\nBut by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks.\nWe perform an empirical investigation of various RNN regularizers, and find that zoneout gives significant performance improvements across tasks. We achieve competitive results with relatively simple models in character- and word-level language modelling on the Penn Treebank and Text8 datasets, and combining with recurrent batch normalization yields state-of-the-art results on permuted sequential MNIST.", "pdf": "/pdf/fdd2ff27bd56cb0045a9dee87e9dce9f8c134b2e.pdf", "TL;DR": "Zoneout is like dropout (for RNNs) but uses identity masks instead of zero masks", "paperhash": "krueger|zoneout_regularizing_rnns_by_randomly_preserving_hidden_activations", "keywords": ["Deep learning"], "conflicts": ["umontreal.ca"], "authors": ["David Krueger", "Tegan Maharaj", "Janos Kramar", "Mohammad Pezeshki", "Nicolas Ballas", "Nan Rosemary Ke", "Anirudh  Goyal", "Yoshua Bengio", "Aaron Courville", "Christopher Pal"], "authorids": ["davidscottkrueger@gmail.com", "tegan.jrm@gmail.com", "ballas.n@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287599927, "id": "ICLR.cc/2017/conference/-/paper378/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "rJqBEPcxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper378/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper378/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper378/reviewers", "ICLR.cc/2017/conference/paper378/areachairs"], "cdate": 1485287599927}}}, {"tddate": null, "tmdate": 1481812779495, "tcdate": 1481811235843, "number": 1, "id": "BJ27BQlVe", "invitation": "ICLR.cc/2017/conference/-/paper378/public/comment", "forum": "rJqBEPcxe", "replyto": "rJbZGW57g", "signatures": ["~David_Krueger1"], "readers": ["everyone"], "writers": ["~David_Krueger1"], "content": {"title": "Our hypothesis: zoneout improves generalization performance.", "comment": "We chose our experiments in order to demonstrate that adding zoneout to existing models improves performance without requiring additional tuning. We believe that using these references architectures without any hyper-parameter provides a strong and fair comparison of zoneout with previous approaches (on the same task).\n\nWe see no reason to suspect that a hyper-parameter search over model architectures would lead to worse results for zoneout (except in the under-fitting regime).  Rather, tuning the architecture in conjunction with the zoneout probabilities could only improve our results. \n\nCould you please clarify what you mean by this sentence:\n\u201cWas it out of convenience for not having to do it, or was it because the authors want to stress this behaviour as a feature of zoneout?\u201d\n\nBased on our experiments, our advice to readers who want to reduce overfitting in their RNN models is to use zoneout probability ~0.5 on cells and/or ~0.1 on states."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations", "abstract": "We propose zoneout, a novel method for regularizing RNNs.\nAt each timestep, zoneout stochastically forces some hidden units to maintain their previous values.\nLike dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization.\nBut by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks.\nWe perform an empirical investigation of various RNN regularizers, and find that zoneout gives significant performance improvements across tasks. We achieve competitive results with relatively simple models in character- and word-level language modelling on the Penn Treebank and Text8 datasets, and combining with recurrent batch normalization yields state-of-the-art results on permuted sequential MNIST.", "pdf": "/pdf/fdd2ff27bd56cb0045a9dee87e9dce9f8c134b2e.pdf", "TL;DR": "Zoneout is like dropout (for RNNs) but uses identity masks instead of zero masks", "paperhash": "krueger|zoneout_regularizing_rnns_by_randomly_preserving_hidden_activations", "keywords": ["Deep learning"], "conflicts": ["umontreal.ca"], "authors": ["David Krueger", "Tegan Maharaj", "Janos Kramar", "Mohammad Pezeshki", "Nicolas Ballas", "Nan Rosemary Ke", "Anirudh  Goyal", "Yoshua Bengio", "Aaron Courville", "Christopher Pal"], "authorids": ["davidscottkrueger@gmail.com", "tegan.jrm@gmail.com", "ballas.n@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287600046, "id": "ICLR.cc/2017/conference/-/paper378/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJqBEPcxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper378/reviewers", "ICLR.cc/2017/conference/paper378/areachairs"], "cdate": 1485287600046}}}, {"tddate": null, "tmdate": 1481811477155, "tcdate": 1481811477151, "number": 3, "id": "BJTM8mxNe", "invitation": "ICLR.cc/2017/conference/-/paper378/public/comment", "forum": "rJqBEPcxe", "replyto": "SyD7p_szg", "signatures": ["~David_Krueger1"], "readers": ["everyone"], "writers": ["~David_Krueger1"], "content": {"title": "RE: Variational Dropout, (non-)overlapping confusion", "comment": "In addition to variational dropout, (Gal et al. 2015) drop out rows of the embedding layer (i.e. words of the input), and use weight decay.  It is possible this additional regularization helps Gal et al. (2015) achieve superior performance on this task.\n\nThank you for pointing out the inconsistency in the text wrt (non-)overlapping sequences; we\u2019ll update the paper to clarify.  In fact, we originally ran all models without overlapping sequences.  Then to compare with Cooijmans et al. (2016), we used overlapping sequences as data augmentation for the LSTM experiments (only), which reduced BPC from 1.29 to 1.27."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations", "abstract": "We propose zoneout, a novel method for regularizing RNNs.\nAt each timestep, zoneout stochastically forces some hidden units to maintain their previous values.\nLike dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization.\nBut by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks.\nWe perform an empirical investigation of various RNN regularizers, and find that zoneout gives significant performance improvements across tasks. We achieve competitive results with relatively simple models in character- and word-level language modelling on the Penn Treebank and Text8 datasets, and combining with recurrent batch normalization yields state-of-the-art results on permuted sequential MNIST.", "pdf": "/pdf/fdd2ff27bd56cb0045a9dee87e9dce9f8c134b2e.pdf", "TL;DR": "Zoneout is like dropout (for RNNs) but uses identity masks instead of zero masks", "paperhash": "krueger|zoneout_regularizing_rnns_by_randomly_preserving_hidden_activations", "keywords": ["Deep learning"], "conflicts": ["umontreal.ca"], "authors": ["David Krueger", "Tegan Maharaj", "Janos Kramar", "Mohammad Pezeshki", "Nicolas Ballas", "Nan Rosemary Ke", "Anirudh  Goyal", "Yoshua Bengio", "Aaron Courville", "Christopher Pal"], "authorids": ["davidscottkrueger@gmail.com", "tegan.jrm@gmail.com", "ballas.n@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287600046, "id": "ICLR.cc/2017/conference/-/paper378/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJqBEPcxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper378/reviewers", "ICLR.cc/2017/conference/paper378/areachairs"], "cdate": 1485287600046}}}, {"tddate": null, "tmdate": 1481811392230, "tcdate": 1481811392222, "number": 2, "id": "Sy_TSmgVx", "invitation": "ICLR.cc/2017/conference/-/paper378/public/comment", "forum": "rJqBEPcxe", "replyto": "Syqxit1mx", "signatures": ["~Tegan_Maharaj1"], "readers": ["everyone"], "writers": ["~Tegan_Maharaj1"], "content": {"title": "Regularization (and thus generalization)", "comment": "Our focus is the regularization effect of zoneout, which provides consistent improvements in generalization (validation and test set cost) in our experiments.\n\nOur experiment on gradient propagation (section 4.4) does not demonstrate that zoneout improves optimization, only that zoneout (unlike a naive implementation of dropout on recurrent connections) does not suffer from vanishing gradients.\n\nThe effect on optimization (i.e. training curve) is mixed: train cost decreases faster on character-level Penn Treebank, but not on other datasets. In general, we mostly observe lower training costs with unregularized models than models with zoneout, as is often the case for regularization - by reduce overfitting, it\u2019s common to see a rise in training cost."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations", "abstract": "We propose zoneout, a novel method for regularizing RNNs.\nAt each timestep, zoneout stochastically forces some hidden units to maintain their previous values.\nLike dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization.\nBut by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks.\nWe perform an empirical investigation of various RNN regularizers, and find that zoneout gives significant performance improvements across tasks. We achieve competitive results with relatively simple models in character- and word-level language modelling on the Penn Treebank and Text8 datasets, and combining with recurrent batch normalization yields state-of-the-art results on permuted sequential MNIST.", "pdf": "/pdf/fdd2ff27bd56cb0045a9dee87e9dce9f8c134b2e.pdf", "TL;DR": "Zoneout is like dropout (for RNNs) but uses identity masks instead of zero masks", "paperhash": "krueger|zoneout_regularizing_rnns_by_randomly_preserving_hidden_activations", "keywords": ["Deep learning"], "conflicts": ["umontreal.ca"], "authors": ["David Krueger", "Tegan Maharaj", "Janos Kramar", "Mohammad Pezeshki", "Nicolas Ballas", "Nan Rosemary Ke", "Anirudh  Goyal", "Yoshua Bengio", "Aaron Courville", "Christopher Pal"], "authorids": ["davidscottkrueger@gmail.com", "tegan.jrm@gmail.com", "ballas.n@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287600046, "id": "ICLR.cc/2017/conference/-/paper378/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJqBEPcxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper378/reviewers", "ICLR.cc/2017/conference/paper378/areachairs"], "cdate": 1485287600046}}}, {"tddate": null, "tmdate": 1481409017181, "tcdate": 1481409017171, "number": 3, "id": "rJbZGW57g", "invitation": "ICLR.cc/2017/conference/-/paper378/pre-review/question", "forum": "rJqBEPcxe", "replyto": "rJqBEPcxe", "signatures": ["ICLR.cc/2017/conference/paper378/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper378/AnonReviewer1"], "content": {"title": "What hypothesis do the experiments test?", "question": "Very nice paper. Simple idea, good results and a strong writeup.\n\n\nFrom what I understand from the conclusion, no hyper parameter search was conducted. Instead, the hyper parameters from other works are taken over.\n\nI am undecided whether this is the best for readers. Being pedantic, the hypothesis tested is \"does adding zoneout improve things given we perform model selection on a different, although related, task\". Consequently, a hyper parameter search performed on zoneout might even lead to worse results.\n\nI would like to ask why the authors chose not to just perform an extensive hyper parameter search and report those results instead. Was it out of convenience for not having to do it, or was it because the authors want to stress this behaviour as a feature of zoneout? If so, what is the authors advice for a reader (e.g. \"we conjecture that zoneout will also lead to superior results if hyper parameter search is performed with a zoneout model.\")"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations", "abstract": "We propose zoneout, a novel method for regularizing RNNs.\nAt each timestep, zoneout stochastically forces some hidden units to maintain their previous values.\nLike dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization.\nBut by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks.\nWe perform an empirical investigation of various RNN regularizers, and find that zoneout gives significant performance improvements across tasks. We achieve competitive results with relatively simple models in character- and word-level language modelling on the Penn Treebank and Text8 datasets, and combining with recurrent batch normalization yields state-of-the-art results on permuted sequential MNIST.", "pdf": "/pdf/fdd2ff27bd56cb0045a9dee87e9dce9f8c134b2e.pdf", "TL;DR": "Zoneout is like dropout (for RNNs) but uses identity masks instead of zero masks", "paperhash": "krueger|zoneout_regularizing_rnns_by_randomly_preserving_hidden_activations", "keywords": ["Deep learning"], "conflicts": ["umontreal.ca"], "authors": ["David Krueger", "Tegan Maharaj", "Janos Kramar", "Mohammad Pezeshki", "Nicolas Ballas", "Nan Rosemary Ke", "Anirudh  Goyal", "Yoshua Bengio", "Aaron Courville", "Christopher Pal"], "authorids": ["davidscottkrueger@gmail.com", "tegan.jrm@gmail.com", "ballas.n@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481409017747, "id": "ICLR.cc/2017/conference/-/paper378/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper378/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper378/AnonReviewer3", "ICLR.cc/2017/conference/paper378/AnonReviewer2", "ICLR.cc/2017/conference/paper378/AnonReviewer1"], "reply": {"forum": "rJqBEPcxe", "replyto": "rJqBEPcxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper378/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper378/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481409017747}}}, {"tddate": null, "tmdate": 1480723186468, "tcdate": 1480723186464, "number": 2, "id": "Syqxit1mx", "invitation": "ICLR.cc/2017/conference/-/paper378/pre-review/question", "forum": "rJqBEPcxe", "replyto": "rJqBEPcxe", "signatures": ["ICLR.cc/2017/conference/paper378/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper378/AnonReviewer2"], "content": {"title": "Better Regularization or Optimization ?", "question": "Zoneout is both a regularizer (due to the noise) and a better optimizer (due to the ability to pass gradients back through the identity connections). Can any conclusions be drawn about how much each factor is helping ? Is there a way to tease apart the contributions to the improvements from these two factors ?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations", "abstract": "We propose zoneout, a novel method for regularizing RNNs.\nAt each timestep, zoneout stochastically forces some hidden units to maintain their previous values.\nLike dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization.\nBut by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks.\nWe perform an empirical investigation of various RNN regularizers, and find that zoneout gives significant performance improvements across tasks. We achieve competitive results with relatively simple models in character- and word-level language modelling on the Penn Treebank and Text8 datasets, and combining with recurrent batch normalization yields state-of-the-art results on permuted sequential MNIST.", "pdf": "/pdf/fdd2ff27bd56cb0045a9dee87e9dce9f8c134b2e.pdf", "TL;DR": "Zoneout is like dropout (for RNNs) but uses identity masks instead of zero masks", "paperhash": "krueger|zoneout_regularizing_rnns_by_randomly_preserving_hidden_activations", "keywords": ["Deep learning"], "conflicts": ["umontreal.ca"], "authors": ["David Krueger", "Tegan Maharaj", "Janos Kramar", "Mohammad Pezeshki", "Nicolas Ballas", "Nan Rosemary Ke", "Anirudh  Goyal", "Yoshua Bengio", "Aaron Courville", "Christopher Pal"], "authorids": ["davidscottkrueger@gmail.com", "tegan.jrm@gmail.com", "ballas.n@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481409017747, "id": "ICLR.cc/2017/conference/-/paper378/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper378/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper378/AnonReviewer3", "ICLR.cc/2017/conference/paper378/AnonReviewer2", "ICLR.cc/2017/conference/paper378/AnonReviewer1"], "reply": {"forum": "rJqBEPcxe", "replyto": "rJqBEPcxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper378/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper378/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481409017747}}}, {"tddate": null, "tmdate": 1480457727579, "tcdate": 1480457503114, "number": 1, "id": "SyD7p_szg", "invitation": "ICLR.cc/2017/conference/-/paper378/pre-review/question", "forum": "rJqBEPcxe", "replyto": "rJqBEPcxe", "signatures": ["ICLR.cc/2017/conference/paper378/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper378/AnonReviewer3"], "content": {"title": "Clarification re: performance of zoneout vs variational LSTM and char-level PTB", "question": "I'm glad to see that the setup for the word level PTB task is similar to many other related works, making it easy to compare at a glance. Do you have any intuition as to the different between a zoneout LSTM and the variational LSTM when it comes to perplexity? You mention elsewhere (conclusion?) that you don't perform extensive hyperparameter tuning, which Gal et al. 2015 do for the variational LSTM. Intuitively I prefer zoneout to the variational LSTM as the entire hidden state remains active, essentially retaining more \"bandwidth\", but it's unfortunate the results don't quite line up there.\n\nI am confused over the training of the character level PTB task too. You state that you train LSTMs, GRUs, and tanh-RNNs on non-overlapping sequences of 100 in batches of 32. In the next sentence however you then state that you train on overlapping sequences.\n\nThe investigation into the properties of gradient flow are interesting and help justify the claim that gradient propagation benefits are an inherent property of zoneout. I hope to see these types of analyses become standard when investigating new techniques, especially when they may impact gradient propagation.\n\nOverall this is a good paper with many experiments to support the paper's claims. The introduction of zoneout is a positive contribution to the community - I'm looking forward to seeing it used more extensively and being added as a standard component in various frameworks."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations", "abstract": "We propose zoneout, a novel method for regularizing RNNs.\nAt each timestep, zoneout stochastically forces some hidden units to maintain their previous values.\nLike dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization.\nBut by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks.\nWe perform an empirical investigation of various RNN regularizers, and find that zoneout gives significant performance improvements across tasks. We achieve competitive results with relatively simple models in character- and word-level language modelling on the Penn Treebank and Text8 datasets, and combining with recurrent batch normalization yields state-of-the-art results on permuted sequential MNIST.", "pdf": "/pdf/fdd2ff27bd56cb0045a9dee87e9dce9f8c134b2e.pdf", "TL;DR": "Zoneout is like dropout (for RNNs) but uses identity masks instead of zero masks", "paperhash": "krueger|zoneout_regularizing_rnns_by_randomly_preserving_hidden_activations", "keywords": ["Deep learning"], "conflicts": ["umontreal.ca"], "authors": ["David Krueger", "Tegan Maharaj", "Janos Kramar", "Mohammad Pezeshki", "Nicolas Ballas", "Nan Rosemary Ke", "Anirudh  Goyal", "Yoshua Bengio", "Aaron Courville", "Christopher Pal"], "authorids": ["davidscottkrueger@gmail.com", "tegan.jrm@gmail.com", "ballas.n@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481409017747, "id": "ICLR.cc/2017/conference/-/paper378/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper378/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper378/AnonReviewer3", "ICLR.cc/2017/conference/paper378/AnonReviewer2", "ICLR.cc/2017/conference/paper378/AnonReviewer1"], "reply": {"forum": "rJqBEPcxe", "replyto": "rJqBEPcxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper378/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper378/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481409017747}}}], "count": 16}