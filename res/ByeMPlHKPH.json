{"notes": [{"id": "ByeMPlHKPH", "original": "rkg8Hixtwr", "number": 2353, "cdate": 1569439834413, "ddate": null, "tcdate": 1569439834413, "tmdate": 1587965957525, "tddate": null, "forum": "ByeMPlHKPH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["zhanghao.wu@outlook.com", "zhijian@mit.edu", "jilin@mit.edu", "yujunlin@mit.edu", "songhan@mit.edu"], "title": "Lite Transformer with Long-Short Range Attention", "authors": ["Zhanghao Wu*", "Zhijian Liu*", "Ji Lin", "Yujun Lin", "Song Han"], "pdf": "/pdf/92011124da47fbcf659f29791eb2b1eb72ec16ed.pdf", "abstract": "Transformer has become ubiquitous in natural language processing (e.g., machine translation, question answering); however, it requires enormous amount of computations to achieve high performance, which makes it not suitable for mobile applications that are tightly constrained by the hardware resources and battery. In this paper, we present an efficient mobile NLP architecture, Lite Transformer to facilitate deploying mobile NLP applications on edge devices. The key primitive is the Long-Short Range Attention (LSRA), where one group of heads specializes in the local context modeling (by convolution) while another group specializes in the long-distance relationship modeling (by attention). Such specialization brings consistent improvement over the vanilla transformer on three well-established language tasks: machine translation, abstractive summarization, and language modeling. Under constrained resources (500M/100M MACs), Lite Transformer outperforms transformer on WMT'14 English-French by 1.2/1.7 BLEU, respectively. Lite Transformer reduces the computation of transformer base model by 2.5x with 0.3 BLEU score degradation. Combining with pruning and quantization, we further compressed the model size of Lite Transformer by 18.2x. For language modeling, Lite Transformer achieves 1.8 lower perplexity than the transformer at around 500M MACs. Notably, Lite Transformer outperforms the AutoML-based Evolved Transformer by 0.5 higher BLEU for the mobile NLP setting without the costly architecture search that requires more than 250 GPU years. Code has been made available at https://github.com/mit-han-lab/lite-transformer.", "keywords": ["efficient model", "transformer"], "paperhash": "wu|lite_transformer_with_longshort_range_attention", "_bibtex": "@inproceedings{\nWu*2020Lite,\ntitle={Lite Transformer with Long-Short Range Attention},\nauthor={Zhanghao Wu* and Zhijian Liu* and Ji Lin and Yujun Lin and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByeMPlHKPH}\n}", "original_pdf": "/attachment/25cd999b7b58f27536252c6bbed5a240037a58c3.pdf"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "JTjzaDPu5k", "original": null, "number": 1, "cdate": 1576798746921, "ddate": null, "tcdate": 1576798746921, "tmdate": 1576800889158, "tddate": null, "forum": "ByeMPlHKPH", "replyto": "ByeMPlHKPH", "invitation": "ICLR.cc/2020/Conference/Paper2353/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper presents an efficient architecture of Transformer to facilitate implementations on mobile settings. The core idea is to decompose the self-attention layers to focus on local and global information separately. In the experiments on machine translation, it is shown to outperform baseline Transformer as well as the Evolved Transformer obtained by a costly architecture search. \nWhile all reviewers admitted the practical impact of the results in terms of engineering, the main concerns in the initial paper were the clarification of the mobile settings and scientific contributions. Through the discussion, reviewers are fairly satisfied with the authors\u2019 response and are now all positive to the acceptance. Although we are still curious how it works on other tasks (as the title says \u201cmobile applications\u201d), I think the paper provides enough insights valuable to the community, so I\u2019d like to recommend acceptance. \n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhanghao.wu@outlook.com", "zhijian@mit.edu", "jilin@mit.edu", "yujunlin@mit.edu", "songhan@mit.edu"], "title": "Lite Transformer with Long-Short Range Attention", "authors": ["Zhanghao Wu*", "Zhijian Liu*", "Ji Lin", "Yujun Lin", "Song Han"], "pdf": "/pdf/92011124da47fbcf659f29791eb2b1eb72ec16ed.pdf", "abstract": "Transformer has become ubiquitous in natural language processing (e.g., machine translation, question answering); however, it requires enormous amount of computations to achieve high performance, which makes it not suitable for mobile applications that are tightly constrained by the hardware resources and battery. In this paper, we present an efficient mobile NLP architecture, Lite Transformer to facilitate deploying mobile NLP applications on edge devices. The key primitive is the Long-Short Range Attention (LSRA), where one group of heads specializes in the local context modeling (by convolution) while another group specializes in the long-distance relationship modeling (by attention). Such specialization brings consistent improvement over the vanilla transformer on three well-established language tasks: machine translation, abstractive summarization, and language modeling. Under constrained resources (500M/100M MACs), Lite Transformer outperforms transformer on WMT'14 English-French by 1.2/1.7 BLEU, respectively. Lite Transformer reduces the computation of transformer base model by 2.5x with 0.3 BLEU score degradation. Combining with pruning and quantization, we further compressed the model size of Lite Transformer by 18.2x. For language modeling, Lite Transformer achieves 1.8 lower perplexity than the transformer at around 500M MACs. Notably, Lite Transformer outperforms the AutoML-based Evolved Transformer by 0.5 higher BLEU for the mobile NLP setting without the costly architecture search that requires more than 250 GPU years. Code has been made available at https://github.com/mit-han-lab/lite-transformer.", "keywords": ["efficient model", "transformer"], "paperhash": "wu|lite_transformer_with_longshort_range_attention", "_bibtex": "@inproceedings{\nWu*2020Lite,\ntitle={Lite Transformer with Long-Short Range Attention},\nauthor={Zhanghao Wu* and Zhijian Liu* and Ji Lin and Yujun Lin and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByeMPlHKPH}\n}", "original_pdf": "/attachment/25cd999b7b58f27536252c6bbed5a240037a58c3.pdf"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "ByeMPlHKPH", "replyto": "ByeMPlHKPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795717909, "tmdate": 1576800268297, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2353/-/Decision"}}}, {"id": "ryl7mTePFB", "original": null, "number": 1, "cdate": 1571388698746, "ddate": null, "tcdate": 1571388698746, "tmdate": 1574411241365, "tddate": null, "forum": "ByeMPlHKPH", "replyto": "ByeMPlHKPH", "invitation": "ICLR.cc/2020/Conference/Paper2353/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "N/A", "review_assessment:_thoroughness_in_paper_reading": "N/A", "title": "Official Blind Review #2", "review": "This paper presents a new technique (LSRA) improving Transformer for constrained scenarios (e.g., mobile settings). It combines two attention modules to provide both global and local information separately for a translation task. In this manner, the authors place the attention and the convolutional module side by side, thus having different perspectives (globally and locally) of the sentence. They test their approach to 2 common translation benchmarks.\n\nEnhancing deep learning model efficiency is very important, and the authors succeeded in reducing the computation costs and consequently, the CO2 emissions. But the evaluation results are not so impressive and go in line with other previous efficient deep learning approaches for different domains. I\u2019m not an expert in NLP, but overall results of 10-1000x or more wall clock time reduction with <1-5% loss in accuracy are usually obtained for domains that have seen more optimization for mobile deployment (especially mobile-optimized CNNs like MobileNet). LSRA-based appraoch is slightly better than the original version of Transformer and its evolved version. From the latter (ET) authors seem to take the idea of parallel branches for their architecture. Also, adaptive attention span in Transformer models and all-attention layers have already been investigated to make networks more efficient and simpler for longer sentences. Include clearer ablation studies would be also interesting to support their findings and superior performance.\n\nTo summarize, the paper is addressing an important and interesting idea. It is, in general terms a nice engineering paper, but I am not sure about whether the developments and results are relevant/novel enough yet at this point to publish at ICLR. \n\n-------------------------------------------------\n\nLooking at the other comments and the feedback provided by the authors, I have a more positive feeling about the contributions of the paper which are now sufficiently demonstrated. Therefore, I increase my original recommendation to \"Weak Accept\".\n\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper2353/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2353/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhanghao.wu@outlook.com", "zhijian@mit.edu", "jilin@mit.edu", "yujunlin@mit.edu", "songhan@mit.edu"], "title": "Lite Transformer with Long-Short Range Attention", "authors": ["Zhanghao Wu*", "Zhijian Liu*", "Ji Lin", "Yujun Lin", "Song Han"], "pdf": "/pdf/92011124da47fbcf659f29791eb2b1eb72ec16ed.pdf", "abstract": "Transformer has become ubiquitous in natural language processing (e.g., machine translation, question answering); however, it requires enormous amount of computations to achieve high performance, which makes it not suitable for mobile applications that are tightly constrained by the hardware resources and battery. In this paper, we present an efficient mobile NLP architecture, Lite Transformer to facilitate deploying mobile NLP applications on edge devices. The key primitive is the Long-Short Range Attention (LSRA), where one group of heads specializes in the local context modeling (by convolution) while another group specializes in the long-distance relationship modeling (by attention). Such specialization brings consistent improvement over the vanilla transformer on three well-established language tasks: machine translation, abstractive summarization, and language modeling. Under constrained resources (500M/100M MACs), Lite Transformer outperforms transformer on WMT'14 English-French by 1.2/1.7 BLEU, respectively. Lite Transformer reduces the computation of transformer base model by 2.5x with 0.3 BLEU score degradation. Combining with pruning and quantization, we further compressed the model size of Lite Transformer by 18.2x. For language modeling, Lite Transformer achieves 1.8 lower perplexity than the transformer at around 500M MACs. Notably, Lite Transformer outperforms the AutoML-based Evolved Transformer by 0.5 higher BLEU for the mobile NLP setting without the costly architecture search that requires more than 250 GPU years. Code has been made available at https://github.com/mit-han-lab/lite-transformer.", "keywords": ["efficient model", "transformer"], "paperhash": "wu|lite_transformer_with_longshort_range_attention", "_bibtex": "@inproceedings{\nWu*2020Lite,\ntitle={Lite Transformer with Long-Short Range Attention},\nauthor={Zhanghao Wu* and Zhijian Liu* and Ji Lin and Yujun Lin and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByeMPlHKPH}\n}", "original_pdf": "/attachment/25cd999b7b58f27536252c6bbed5a240037a58c3.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ByeMPlHKPH", "replyto": "ByeMPlHKPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2353/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2353/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575665707548, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2353/Reviewers"], "noninvitees": [], "tcdate": 1570237724044, "tmdate": 1575665707561, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2353/-/Official_Review"}}}, {"id": "SJgrvtiYjH", "original": null, "number": 4, "cdate": 1573661021262, "ddate": null, "tcdate": 1573661021262, "tmdate": 1573661297596, "tddate": null, "forum": "ByeMPlHKPH", "replyto": "ByeMPlHKPH", "invitation": "ICLR.cc/2020/Conference/Paper2353/-/Official_Comment", "content": {"title": "Our General Response", "comment": "We thank all reviewers for their comments. In addition to the specific response below, here we list the answers for some common questions, some additional experiments and the changes we made in the revision.\n\n1. Clarifications of mobile setting\nIn this paper, we defined the mobile setting for NLP tasks, which is under 500M Mult-Adds and 10M parameters. These two constraints are based on the real-world requirements for mobile devices and the conventions in the computer vision community.\n(a) Mult-Add constraints: The floating-point performance of the ARM Cortex-A72 mobile CPU is about 48G FLOPS (4 cores @1.5GHz). To achieve the peak performance of 50 sentences per second, the model should be less than 960M FLOPs (480M Mult-Adds). This is a common constraint in the computer vision community. For example, MobileNet [1, 2] is less than 500M Mult-Adds; PNAS [3] uses 500M Mult-Adds as the constraint of its mobile setting.\n(b) Parameter constraints: The constraint for the parameters is based on the download limitation. When an application is larger than 100MB, it cannot be downloaded with 4G LTE (only with WIFI) in the App Store. Therefore, the number of parameters for a mobile model should be limited. As MobileNet V2 (1.4) contains around 7M parameters, we round it to the nearest magnitude, 10M parameters, as our constraint. \n\n2. Additional experiments\n(a) We conducted experiments on one additional language pair, WMT\u201914 English to French. Our Mobile Transformer outperforms the vanilla Transformer by more than 1 BLEU score.\n(b) We evaluated our Mobile Transformer on the standard setting. It outperforms the vanilla Transformer by 0.6 BLEU score on WMT\u201914 English to French.\n(c) We conducted some additional ablation studies including different combinations of two branches and different numbers of heads in LSRA.\n\n3. Other changes\n(a) We included an additional comparison with the Evolved Transformer under the 360M Mult-Adds constraints, where the Evolved Transformer achieves 25.4 BLEU @ 364M Mult-Adds, and our Mobile Transformer achieves 25.8 BLEU @ 360M Mult-Adds.\n(b) We removed the extremely efficient computation constraint for clarity.\n(c) We added more discussions with some related papers.\n\nReferences:\n[1] Andrew Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam, \"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications\", arXiv 2017.\n[2] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen, \"MobileNetV2: Inverted Residuals and Linear Bottlenecks\", CVPR 2018.\n[3] Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy, \"Progressive Neural Architecture Search\", ECCV 2018."}, "signatures": ["ICLR.cc/2020/Conference/Paper2353/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2353/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhanghao.wu@outlook.com", "zhijian@mit.edu", "jilin@mit.edu", "yujunlin@mit.edu", "songhan@mit.edu"], "title": "Lite Transformer with Long-Short Range Attention", "authors": ["Zhanghao Wu*", "Zhijian Liu*", "Ji Lin", "Yujun Lin", "Song Han"], "pdf": "/pdf/92011124da47fbcf659f29791eb2b1eb72ec16ed.pdf", "abstract": "Transformer has become ubiquitous in natural language processing (e.g., machine translation, question answering); however, it requires enormous amount of computations to achieve high performance, which makes it not suitable for mobile applications that are tightly constrained by the hardware resources and battery. In this paper, we present an efficient mobile NLP architecture, Lite Transformer to facilitate deploying mobile NLP applications on edge devices. The key primitive is the Long-Short Range Attention (LSRA), where one group of heads specializes in the local context modeling (by convolution) while another group specializes in the long-distance relationship modeling (by attention). Such specialization brings consistent improvement over the vanilla transformer on three well-established language tasks: machine translation, abstractive summarization, and language modeling. Under constrained resources (500M/100M MACs), Lite Transformer outperforms transformer on WMT'14 English-French by 1.2/1.7 BLEU, respectively. Lite Transformer reduces the computation of transformer base model by 2.5x with 0.3 BLEU score degradation. Combining with pruning and quantization, we further compressed the model size of Lite Transformer by 18.2x. For language modeling, Lite Transformer achieves 1.8 lower perplexity than the transformer at around 500M MACs. Notably, Lite Transformer outperforms the AutoML-based Evolved Transformer by 0.5 higher BLEU for the mobile NLP setting without the costly architecture search that requires more than 250 GPU years. Code has been made available at https://github.com/mit-han-lab/lite-transformer.", "keywords": ["efficient model", "transformer"], "paperhash": "wu|lite_transformer_with_longshort_range_attention", "_bibtex": "@inproceedings{\nWu*2020Lite,\ntitle={Lite Transformer with Long-Short Range Attention},\nauthor={Zhanghao Wu* and Zhijian Liu* and Ji Lin and Yujun Lin and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByeMPlHKPH}\n}", "original_pdf": "/attachment/25cd999b7b58f27536252c6bbed5a240037a58c3.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByeMPlHKPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2353/Authors", "ICLR.cc/2020/Conference/Paper2353/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2353/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2353/Reviewers", "ICLR.cc/2020/Conference/Paper2353/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2353/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2353/Authors|ICLR.cc/2020/Conference/Paper2353/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142630, "tmdate": 1576860529031, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2353/Authors", "ICLR.cc/2020/Conference/Paper2353/Reviewers", "ICLR.cc/2020/Conference/Paper2353/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2353/-/Official_Comment"}}}, {"id": "HylA7KiFjH", "original": null, "number": 3, "cdate": 1573660965603, "ddate": null, "tcdate": 1573660965603, "tmdate": 1573661213639, "tddate": null, "forum": "ByeMPlHKPH", "replyto": "rkebus9xqB", "invitation": "ICLR.cc/2020/Conference/Paper2353/-/Official_Comment", "content": {"title": "Our Response to Reviewer 1", "comment": "Thank you very much for your constructive comments.\n\n1. Specialization for mobile setting\nAs mentioned in Section 4, the original multi-head self-attention captures \u2018global\u2019 and \u2018local\u2019 information in a single module. This unified design is not efficient especially when the computation budget is very limited (i.e., under the mobile setting). We need to make the module more specialized so that it can capture the context in a more efficient way. To this end, we proposed LSRA that captures the short-range information by convolution and the long-range information by attention. As shown in Figures 4 and 5, our LSRA achieves more improvements when the computation constraint is tighter. This is because the redundancy of the unified design is more severe under the mobile setting.\n\nWe also conducted experiments under the standard setting on the WMT En-Fr dataset. Our Mobile Transformer still outperforms the vanilla Transformer under the standard setting:\nBase Transformer:                   40.0 BLEU @ 1336M Mult-Adds\nMobile Transformer (Ours):   40.6 BLEU @ 1328M Mult-Adds\n\n2. Clarification of mobile settings\nWe defined our mobile setting based on the real-world requirements for mobile applications and the conventions in the computer vision community. Please refer to our general response for more information. We revised our paper accordingly in Section 5.\n\nWe have also listed all other changes in our general response above. Please don\u2019t hesitate to let us know for any additional comments on the paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper2353/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2353/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhanghao.wu@outlook.com", "zhijian@mit.edu", "jilin@mit.edu", "yujunlin@mit.edu", "songhan@mit.edu"], "title": "Lite Transformer with Long-Short Range Attention", "authors": ["Zhanghao Wu*", "Zhijian Liu*", "Ji Lin", "Yujun Lin", "Song Han"], "pdf": "/pdf/92011124da47fbcf659f29791eb2b1eb72ec16ed.pdf", "abstract": "Transformer has become ubiquitous in natural language processing (e.g., machine translation, question answering); however, it requires enormous amount of computations to achieve high performance, which makes it not suitable for mobile applications that are tightly constrained by the hardware resources and battery. In this paper, we present an efficient mobile NLP architecture, Lite Transformer to facilitate deploying mobile NLP applications on edge devices. The key primitive is the Long-Short Range Attention (LSRA), where one group of heads specializes in the local context modeling (by convolution) while another group specializes in the long-distance relationship modeling (by attention). Such specialization brings consistent improvement over the vanilla transformer on three well-established language tasks: machine translation, abstractive summarization, and language modeling. Under constrained resources (500M/100M MACs), Lite Transformer outperforms transformer on WMT'14 English-French by 1.2/1.7 BLEU, respectively. Lite Transformer reduces the computation of transformer base model by 2.5x with 0.3 BLEU score degradation. Combining with pruning and quantization, we further compressed the model size of Lite Transformer by 18.2x. For language modeling, Lite Transformer achieves 1.8 lower perplexity than the transformer at around 500M MACs. Notably, Lite Transformer outperforms the AutoML-based Evolved Transformer by 0.5 higher BLEU for the mobile NLP setting without the costly architecture search that requires more than 250 GPU years. Code has been made available at https://github.com/mit-han-lab/lite-transformer.", "keywords": ["efficient model", "transformer"], "paperhash": "wu|lite_transformer_with_longshort_range_attention", "_bibtex": "@inproceedings{\nWu*2020Lite,\ntitle={Lite Transformer with Long-Short Range Attention},\nauthor={Zhanghao Wu* and Zhijian Liu* and Ji Lin and Yujun Lin and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByeMPlHKPH}\n}", "original_pdf": "/attachment/25cd999b7b58f27536252c6bbed5a240037a58c3.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByeMPlHKPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2353/Authors", "ICLR.cc/2020/Conference/Paper2353/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2353/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2353/Reviewers", "ICLR.cc/2020/Conference/Paper2353/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2353/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2353/Authors|ICLR.cc/2020/Conference/Paper2353/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142630, "tmdate": 1576860529031, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2353/Authors", "ICLR.cc/2020/Conference/Paper2353/Reviewers", "ICLR.cc/2020/Conference/Paper2353/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2353/-/Official_Comment"}}}, {"id": "rJgULPjYjS", "original": null, "number": 1, "cdate": 1573660493803, "ddate": null, "tcdate": 1573660493803, "tmdate": 1573660908728, "tddate": null, "forum": "ByeMPlHKPH", "replyto": "ryl7mTePFB", "invitation": "ICLR.cc/2020/Conference/Paper2353/-/Official_Comment", "content": {"title": "Our Response to Reviewer 2", "comment": "Thank you very much for your constructive comments.\n\n1. Evaluation results\nWe evaluated our Mobile Transformer on multiple machine translation datasets, including IWSLT De-En and WMT En-De. In our revision, we also added experimental results on WMT En-Fr (see Table 3 and Figure 4). Across all datasets, our Mobile Transformer consistently outperforms the vanilla Transformer and the AutoML-based Evolved Transformer. These are two strong baselines. Compared to the vanilla Transformer, we achieved twice the BLEU score improvement as the Evolved Transformer under similar constraints, which should be considered significant.\n\nOn the other hand, our Mobile Transformer can reduce the computation by 2.5x with only 0.3 BLEU degradation and 15x with 5 BLEU degradation on the WMT En-Fr dataset. To achieve further compression, some general techniques (e.g., pruning, quantization) can be applied. In principle, we can safely quantize the model to 8 bits (4x) and prune the model by 50% (2x) without much loss of accuracy, which will give us around 120x reduction in model size.\n\n2. Comparison with Evolved Transformer\nOur manual design is not the one presented in the ET\u2019s paper even though it lies within its search space. This, we believe, is because NAS is limited by the sample efficiency and therefore cannot fully explore all the samples within its design space. We hope that our paper can raise people\u2019s awareness about the importance of design insights: e.g., flatten the transformer to enlarge the attention\u2019s computation, incorporate parallel branches that specialize in extracting features from different ranges.\n\n3. Previous work\nWe included the reference for adaptive span [1] and the all-attention [2] in our revision. Both methods are designed for the character-level language modeling, where the sequence is typically very long (>1000 tokens). This is drastically different from the machine translation, where the sequence is much shorter (<30 tokens). The adaptive span applies masks for the long-range relations, which will induce significant information loss when the sequence is relatively short. Also, these methods are orthogonal to our LSRA and can potentially be applied together.\n\n4. Ablation study\nWe included several ablation studies on the IWSLT dataset. We explored different combinations of two branches (attention+attention, convolution+convolution), and we also validated the effectiveness of flattening the FFN.\n---------------------------------------------------------------------------------------------------\n| Model\t\t\t\t\t\t\t\t|\t#Mult-Adds\t|\tBLEU\t|\n---------------------------------------------------------------------------------------------------\n| Mobile Transformer (Ours)\t\t\t|\t209M\t\t|\t34.5\t\t|\n| - with two branches of attention\t\t|\t232M\t\t|\t33.6\t\t|\n| - with two branches of convolution\t|\t217M\t\t|\t33.8\t\t|\n| - without flattening the FFN\t\t\t|\t207M\t\t|\t34.0\t\t|\n---------------------------------------------------------------------------------------------------\n\nWe have also listed all other changes in our general response above. Please don\u2019t hesitate to let us know for any additional comments on the paper.\n\nReferences:\n[1] Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin, \"Adaptive Attention Span in Transformers\", ACL 2019.\n[2] Anonymous, \"Augmenting Self-attention with Persistent Memory\".\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2353/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2353/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhanghao.wu@outlook.com", "zhijian@mit.edu", "jilin@mit.edu", "yujunlin@mit.edu", "songhan@mit.edu"], "title": "Lite Transformer with Long-Short Range Attention", "authors": ["Zhanghao Wu*", "Zhijian Liu*", "Ji Lin", "Yujun Lin", "Song Han"], "pdf": "/pdf/92011124da47fbcf659f29791eb2b1eb72ec16ed.pdf", "abstract": "Transformer has become ubiquitous in natural language processing (e.g., machine translation, question answering); however, it requires enormous amount of computations to achieve high performance, which makes it not suitable for mobile applications that are tightly constrained by the hardware resources and battery. In this paper, we present an efficient mobile NLP architecture, Lite Transformer to facilitate deploying mobile NLP applications on edge devices. The key primitive is the Long-Short Range Attention (LSRA), where one group of heads specializes in the local context modeling (by convolution) while another group specializes in the long-distance relationship modeling (by attention). Such specialization brings consistent improvement over the vanilla transformer on three well-established language tasks: machine translation, abstractive summarization, and language modeling. Under constrained resources (500M/100M MACs), Lite Transformer outperforms transformer on WMT'14 English-French by 1.2/1.7 BLEU, respectively. Lite Transformer reduces the computation of transformer base model by 2.5x with 0.3 BLEU score degradation. Combining with pruning and quantization, we further compressed the model size of Lite Transformer by 18.2x. For language modeling, Lite Transformer achieves 1.8 lower perplexity than the transformer at around 500M MACs. Notably, Lite Transformer outperforms the AutoML-based Evolved Transformer by 0.5 higher BLEU for the mobile NLP setting without the costly architecture search that requires more than 250 GPU years. Code has been made available at https://github.com/mit-han-lab/lite-transformer.", "keywords": ["efficient model", "transformer"], "paperhash": "wu|lite_transformer_with_longshort_range_attention", "_bibtex": "@inproceedings{\nWu*2020Lite,\ntitle={Lite Transformer with Long-Short Range Attention},\nauthor={Zhanghao Wu* and Zhijian Liu* and Ji Lin and Yujun Lin and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByeMPlHKPH}\n}", "original_pdf": "/attachment/25cd999b7b58f27536252c6bbed5a240037a58c3.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByeMPlHKPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2353/Authors", "ICLR.cc/2020/Conference/Paper2353/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2353/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2353/Reviewers", "ICLR.cc/2020/Conference/Paper2353/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2353/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2353/Authors|ICLR.cc/2020/Conference/Paper2353/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142630, "tmdate": 1576860529031, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2353/Authors", "ICLR.cc/2020/Conference/Paper2353/Reviewers", "ICLR.cc/2020/Conference/Paper2353/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2353/-/Official_Comment"}}}, {"id": "SJezRdjtoB", "original": null, "number": 2, "cdate": 1573660874113, "ddate": null, "tcdate": 1573660874113, "tmdate": 1573660874113, "tddate": null, "forum": "ByeMPlHKPH", "replyto": "rkxtlq35FB", "invitation": "ICLR.cc/2020/Conference/Paper2353/-/Official_Comment", "content": {"title": "Our Response to Reviewer 3", "comment": "Thank you very much for your encouraging and constructive comments.\n\n1. Clarification of mobile settings\nWe defined our mobile setting based on the real-world requirements for mobile applications and the conventions in the computer vision community. Please refer to our general response for more information.\n\nInstead of the memory footprint and inference time, we set our mobile setting based on the number of Mult-Adds and parameters due to the following reasons:\n(a) These two metrics are good indicators of the hardware resources required by the model: the number of Mult-Adds is correlated with energy consumption and inference time; the number of parameters indicates the storage on the mobile device.\n(b) The number of Mult-Adds and parameters are independent of the specific hardware (e.g., CPU, GPU), deep learning framework (e.g., PyTorch, TensorFlow), and backend acceleration (e.g., cuDNN, MKL-DNN).\n\n2. Measured latency\nWe measured the latency of our model and baselines on Raspberry Pi 4 (4 cores@1.5GHz):\n---------------------------------------------------------------------------------\n|\t\t\t\t\t\t\t|\tLatency (ms/word)\t|\n---------------------------------------------------------------------------------\n| Transformer\t\t\t\t|\t95.0\t\t\t\t\t|\n| LightConv\t\t\t\t\t|\t73.4\t\t\t\t\t|\n| Mobile Transformer (Ours)\t|\t65.2\t\t\t\t\t|\n---------------------------------------------------------------------------------\nAll of these models have the same BLEU score (34.1) on the IWSLT dataset. Our Mobile Transformer is 1.5x faster than the vanilla Transformer in terms of the measured latency.\n\n3. Design cost\nWe have run 5 experiments in total (on WMT En-De) to explore different implementations of convolution (i.e., vanilla convolution, depthwise convolution). However, we have not tuned our model architecture heavily: the two branches have the same embedding dimension and the same number of heads. Therefore, our total training cost (including tuning the model architecture) is around 16 * 5 = 80 GPU days, which is of the same magnitude as the training cost of the vanilla Transformer and is much lower than the search cost of the Evolved Transformer (about 250 GPU years).\n\n4. Experiments on other language pair\nWe also conducted experiments on one more language pair, WMT\u201914 English to French. Our Mobile Transformer consistently outperforms the Transformer by more than 1 BLEU score:\n----------------------------------------------------------------------------------------------------------------------------------\n|\t\t\t\t\t\t\t|\t#Params\t|\t#Mult-Adds\t|\tBLEU\t|\t\u0394BLEU\t|\n----------------------------------------------------------------------------------------------------------------------------------\n| Transformer\t\t\t\t|\t2.8M\t\t|\t87M\t\t|\t33.6\t\t|\t--\t\t|\n| Mobile Transformer (Ours)\t|\t2.9M\t\t|\t90M\t\t|\t34.9\t\t|\t+1.3\t\t|\n| Transformer\t\t\t\t|\t11.1M\t\t|\t338M\t\t|\t37.6\t\t|\t--\t\t|\n| Mobile Transformer (Ours)\t|\t11.7M\t\t|\t360M\t\t|\t38.7\t\t|\t+1.1\t\t|\n| Transformer\t\t\t\t|\t17.3M\t\t|\t527M\t\t|\t38.4\t\t|\t--\t\t|\n| Mobile Transformer (Ours)\t|\t17.3M\t\t|\t527M\t\t|\t39.6\t\t|\t+1.2\t\t|\n----------------------------------------------------------------------------------------------------------------------------------\nPlease refer to Table 3 and Figure 5 in the revised PDF for more information.\n\n5. Attention maps\nIn Figure 3, the visualization is based on the average attention maps of all heads in the same layer.\n\n6. <EOS> token\nAccording to Clark et al. [1], each head of attention implicitly learns a function over the input sequence. As some head of attention only focuses on a subset of the sequence (e.g., nouns), the attention weights in the rows of all the other tokens (e.g., non-nouns) will be aggregated into some special entries (e.g., <EOS>). Thus, the attention weights on these special tokens will have a high value after taking the average.\n\n7. Trade-offs between encoder and decoder\nWe only experimented with the same number of layers in the encoder and decoder to have a fair comparison with the vanilla Transformer.\n\n8. Other tasks\nOur LSRA is a general module that can, in principle, be plugged into the models for other tasks, including language modeling and abstractive summarization. This remains future work.\n\nWe have also listed all other changes in our general response above. Please don\u2019t hesitate to let us know for any additional comments on the paper.\n\nReferences:\n[1] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher Manning, \"What Does BERT Look At? An Analysis of BERT\u2019s Attention\", BlackBoxNLP 2019.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2353/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2353/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhanghao.wu@outlook.com", "zhijian@mit.edu", "jilin@mit.edu", "yujunlin@mit.edu", "songhan@mit.edu"], "title": "Lite Transformer with Long-Short Range Attention", "authors": ["Zhanghao Wu*", "Zhijian Liu*", "Ji Lin", "Yujun Lin", "Song Han"], "pdf": "/pdf/92011124da47fbcf659f29791eb2b1eb72ec16ed.pdf", "abstract": "Transformer has become ubiquitous in natural language processing (e.g., machine translation, question answering); however, it requires enormous amount of computations to achieve high performance, which makes it not suitable for mobile applications that are tightly constrained by the hardware resources and battery. In this paper, we present an efficient mobile NLP architecture, Lite Transformer to facilitate deploying mobile NLP applications on edge devices. The key primitive is the Long-Short Range Attention (LSRA), where one group of heads specializes in the local context modeling (by convolution) while another group specializes in the long-distance relationship modeling (by attention). Such specialization brings consistent improvement over the vanilla transformer on three well-established language tasks: machine translation, abstractive summarization, and language modeling. Under constrained resources (500M/100M MACs), Lite Transformer outperforms transformer on WMT'14 English-French by 1.2/1.7 BLEU, respectively. Lite Transformer reduces the computation of transformer base model by 2.5x with 0.3 BLEU score degradation. Combining with pruning and quantization, we further compressed the model size of Lite Transformer by 18.2x. For language modeling, Lite Transformer achieves 1.8 lower perplexity than the transformer at around 500M MACs. Notably, Lite Transformer outperforms the AutoML-based Evolved Transformer by 0.5 higher BLEU for the mobile NLP setting without the costly architecture search that requires more than 250 GPU years. Code has been made available at https://github.com/mit-han-lab/lite-transformer.", "keywords": ["efficient model", "transformer"], "paperhash": "wu|lite_transformer_with_longshort_range_attention", "_bibtex": "@inproceedings{\nWu*2020Lite,\ntitle={Lite Transformer with Long-Short Range Attention},\nauthor={Zhanghao Wu* and Zhijian Liu* and Ji Lin and Yujun Lin and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByeMPlHKPH}\n}", "original_pdf": "/attachment/25cd999b7b58f27536252c6bbed5a240037a58c3.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByeMPlHKPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2353/Authors", "ICLR.cc/2020/Conference/Paper2353/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2353/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2353/Reviewers", "ICLR.cc/2020/Conference/Paper2353/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2353/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2353/Authors|ICLR.cc/2020/Conference/Paper2353/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142630, "tmdate": 1576860529031, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2353/Authors", "ICLR.cc/2020/Conference/Paper2353/Reviewers", "ICLR.cc/2020/Conference/Paper2353/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2353/-/Official_Comment"}}}, {"id": "rkxtlq35FB", "original": null, "number": 2, "cdate": 1571633648756, "ddate": null, "tcdate": 1571633648756, "tmdate": 1572972349483, "tddate": null, "forum": "ByeMPlHKPH", "replyto": "ByeMPlHKPH", "invitation": "ICLR.cc/2020/Conference/Paper2353/-/Official_Review", "content": {"rating": "8: Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper proposes Mobile Transformer, an efficient machine translation model, which achieves state-of-the-art results on IWSLT and WMT. The Mobile Transformer is base on long-short range attention (LSRA) modules that combine a depthwise convolution branch to encode the local information and a self-attention branch to capture the long-range information.\n\nThe main contribution of this paper includes\n1. bottlenecks are not beneficial to 1D attention models\n2. having both convolution and attention modules in parallel performs better and more efficient than having one of them alone. While LSRA is included in the search space of Evolved Transformer, surprisingly, their searching algorithm doesn't discover it. Evolved Transformer has either two convolution branches or two attention branches in parallel.\n\nThe paper is well written and easy to follow. The experiments are quite solid; however, it would be if the authors can report how Mobile Transformer performs on other language pairs or other NLP tasks. \n\nQuestions:\n1. Do the attention maps in Figure 3 come from the average of multiple heads or just one of them? \n2. The constraint for the mobile setting is set to 10M parameters. Can you justify why you choose it? In my opinion, memory footprint or inference time on mobile devices can be more realistic. \n3. Regarding the design cost shown in Figure (b). Does the number for Mobile Transformer include the cost of all the experiments you ran to search for your Mobile Transformer? \n4. I wonder if LSRA can also be applied to other tasks such as language modeling or reading comprehension.\n5. In terms of inference latency, how much faster Mobile Transformer is compared to Transformer and LightConv?\n6. Have you considered having the trade-off between having more parameters in the encoder or decoder?\n7. Have you done any analysis on why all tokens in Figure (c) assign high weights to the <EOS> token?"}, "signatures": ["ICLR.cc/2020/Conference/Paper2353/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2353/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhanghao.wu@outlook.com", "zhijian@mit.edu", "jilin@mit.edu", "yujunlin@mit.edu", "songhan@mit.edu"], "title": "Lite Transformer with Long-Short Range Attention", "authors": ["Zhanghao Wu*", "Zhijian Liu*", "Ji Lin", "Yujun Lin", "Song Han"], "pdf": "/pdf/92011124da47fbcf659f29791eb2b1eb72ec16ed.pdf", "abstract": "Transformer has become ubiquitous in natural language processing (e.g., machine translation, question answering); however, it requires enormous amount of computations to achieve high performance, which makes it not suitable for mobile applications that are tightly constrained by the hardware resources and battery. In this paper, we present an efficient mobile NLP architecture, Lite Transformer to facilitate deploying mobile NLP applications on edge devices. The key primitive is the Long-Short Range Attention (LSRA), where one group of heads specializes in the local context modeling (by convolution) while another group specializes in the long-distance relationship modeling (by attention). Such specialization brings consistent improvement over the vanilla transformer on three well-established language tasks: machine translation, abstractive summarization, and language modeling. Under constrained resources (500M/100M MACs), Lite Transformer outperforms transformer on WMT'14 English-French by 1.2/1.7 BLEU, respectively. Lite Transformer reduces the computation of transformer base model by 2.5x with 0.3 BLEU score degradation. Combining with pruning and quantization, we further compressed the model size of Lite Transformer by 18.2x. For language modeling, Lite Transformer achieves 1.8 lower perplexity than the transformer at around 500M MACs. Notably, Lite Transformer outperforms the AutoML-based Evolved Transformer by 0.5 higher BLEU for the mobile NLP setting without the costly architecture search that requires more than 250 GPU years. Code has been made available at https://github.com/mit-han-lab/lite-transformer.", "keywords": ["efficient model", "transformer"], "paperhash": "wu|lite_transformer_with_longshort_range_attention", "_bibtex": "@inproceedings{\nWu*2020Lite,\ntitle={Lite Transformer with Long-Short Range Attention},\nauthor={Zhanghao Wu* and Zhijian Liu* and Ji Lin and Yujun Lin and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByeMPlHKPH}\n}", "original_pdf": "/attachment/25cd999b7b58f27536252c6bbed5a240037a58c3.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ByeMPlHKPH", "replyto": "ByeMPlHKPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2353/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2353/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575665707548, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2353/Reviewers"], "noninvitees": [], "tcdate": 1570237724044, "tmdate": 1575665707561, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2353/-/Official_Review"}}}, {"id": "rkebus9xqB", "original": null, "number": 3, "cdate": 1572019048573, "ddate": null, "tcdate": 1572019048573, "tmdate": 1572972349448, "tddate": null, "forum": "ByeMPlHKPH", "replyto": "ByeMPlHKPH", "invitation": "ICLR.cc/2020/Conference/Paper2353/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "\n\nThis paper claims to propose an extension of the Transformer architecture specialized for the mobile environment (under 500M Mult-Adds).\nThe authors propose their method called \"Long-Short Range Attention (LSRA),\" which separates the self-attention layers into two different purposes, where some heads focus on the local context modeling while the others capture the long-distance relationship. \nThey also demonstrate consistent improvement over the transformer on multiple datasets under the mobile setting. \nIt also surpasses the recently developed comparative method called \"Evolved Transformer\" that requires a far costly architecture search under the mobile setting.\n \nThis paper is basically well written and easy to follow what they have done.\nThe experimental results look good.\n \nHowever, I have several concerns that I listed as follows.\n \n1,\nI am not sure whether my understanding is correct or not, but it seems that the proposed method, LSRA, is not a method specialized for mobile computation.\nIn fact, in the paper, they say, \"To tackle the problem, instead of having one module for \"general\" information, we propose a more specialized architecture, Long-Short Range Attention (LSRA), that captures the global and local information separately.\" \n \nThere is no explicit discussion that LSRA is somehow tackling the mobile setting.\nThere is a large mismatch (gap) between the main claim and what they have done.\nIn other words, LSRA can be simply applied to standard-setting (non-mobile setting). Is there any reason that the proposed method cannot be applied to the standard-setting?\nIf my understanding is correct, the paper must be revised and appropriately reorganized to clear this gap.\n \n2,\nI am not convinced of the condition of the so-called \"mobile setting (and also extremely efficient constraint).\" \nPlease provide a clear justification for it.\n \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2353/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2353/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhanghao.wu@outlook.com", "zhijian@mit.edu", "jilin@mit.edu", "yujunlin@mit.edu", "songhan@mit.edu"], "title": "Lite Transformer with Long-Short Range Attention", "authors": ["Zhanghao Wu*", "Zhijian Liu*", "Ji Lin", "Yujun Lin", "Song Han"], "pdf": "/pdf/92011124da47fbcf659f29791eb2b1eb72ec16ed.pdf", "abstract": "Transformer has become ubiquitous in natural language processing (e.g., machine translation, question answering); however, it requires enormous amount of computations to achieve high performance, which makes it not suitable for mobile applications that are tightly constrained by the hardware resources and battery. In this paper, we present an efficient mobile NLP architecture, Lite Transformer to facilitate deploying mobile NLP applications on edge devices. The key primitive is the Long-Short Range Attention (LSRA), where one group of heads specializes in the local context modeling (by convolution) while another group specializes in the long-distance relationship modeling (by attention). Such specialization brings consistent improvement over the vanilla transformer on three well-established language tasks: machine translation, abstractive summarization, and language modeling. Under constrained resources (500M/100M MACs), Lite Transformer outperforms transformer on WMT'14 English-French by 1.2/1.7 BLEU, respectively. Lite Transformer reduces the computation of transformer base model by 2.5x with 0.3 BLEU score degradation. Combining with pruning and quantization, we further compressed the model size of Lite Transformer by 18.2x. For language modeling, Lite Transformer achieves 1.8 lower perplexity than the transformer at around 500M MACs. Notably, Lite Transformer outperforms the AutoML-based Evolved Transformer by 0.5 higher BLEU for the mobile NLP setting without the costly architecture search that requires more than 250 GPU years. Code has been made available at https://github.com/mit-han-lab/lite-transformer.", "keywords": ["efficient model", "transformer"], "paperhash": "wu|lite_transformer_with_longshort_range_attention", "_bibtex": "@inproceedings{\nWu*2020Lite,\ntitle={Lite Transformer with Long-Short Range Attention},\nauthor={Zhanghao Wu* and Zhijian Liu* and Ji Lin and Yujun Lin and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByeMPlHKPH}\n}", "original_pdf": "/attachment/25cd999b7b58f27536252c6bbed5a240037a58c3.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ByeMPlHKPH", "replyto": "ByeMPlHKPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2353/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2353/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575665707548, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2353/Reviewers"], "noninvitees": [], "tcdate": 1570237724044, "tmdate": 1575665707561, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2353/-/Official_Review"}}}], "count": 9}