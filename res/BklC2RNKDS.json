{"notes": [{"id": "BklC2RNKDS", "original": "BJlrZaYdDS", "number": 1377, "cdate": 1569439414489, "ddate": null, "tcdate": 1569439414489, "tmdate": 1577168274905, "tddate": null, "forum": "BklC2RNKDS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["sdathath@caltech.edu", "johannes.welbl.14@ucl.ac.uk", "dvij@google.com", "ramanakumar@google.com", "akanade@google.com", "juesato@google.com", "sgowal@google.com", "posenhuang@google.com", "pushmeet@google.com"], "title": "Scalable Neural Learning for Verifiable Consistency with Temporal Specifications", "authors": ["Sumanth Dathathri", "Johannes Welbl", "Krishnamurthy (Dj) Dvijotham", "Ramana Kumar", "Aditya Kanade", "Jonathan Uesato", "Sven Gowal", "Po-Sen Huang", "Pushmeet Kohli"], "pdf": "/pdf/425f0f93b6578e15ef92cb9ea8b3cab68968990e.pdf", "TL;DR": "Neural Network Verification for Temporal Properties and Sequence Generation Models", "abstract": "Formal verification of machine learning models has attracted attention recently, and significant progress has been made on proving simple properties like robustness to small perturbations of the input features. In this context, it has also been observed that folding the verification procedure into training makes it easier to train verifiably robust models. In this paper, we extend the applicability of verified training by extending it to (1) recurrent neural network architectures and (2) complex specifications that go beyond simple adversarial robustness, particularly specifications that capture temporal properties like requiring that a robot periodically visits a charging station or that a language model always produces sentences of bounded length. Experiments show that while models trained using standard training often violate desired specifications, our verified training method produces models that both perform well (in terms of test error or reward) and can be shown to be provably consistent with specifications.", "keywords": ["Verification", "Recurrent Neural Networks", "Reinforcement Learning", "Temporal Logic", "Adversarial Robustness"], "paperhash": "dathathri|scalable_neural_learning_for_verifiable_consistency_with_temporal_specifications", "original_pdf": "/attachment/41ffb7334cb3a23d465f74f15e3f29412077b3c9.pdf", "_bibtex": "@misc{\ndathathri2020scalable,\ntitle={Scalable Neural Learning for Verifiable Consistency with Temporal Specifications},\nauthor={Sumanth Dathathri and Johannes Welbl and Krishnamurthy (Dj) Dvijotham and Ramana Kumar and Aditya Kanade and Jonathan Uesato and Sven Gowal and Po-Sen Huang and Pushmeet Kohli},\nyear={2020},\nurl={https://openreview.net/forum?id=BklC2RNKDS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "rDePEOO_O", "original": null, "number": 1, "cdate": 1576798721912, "ddate": null, "tcdate": 1576798721912, "tmdate": 1576800914677, "tddate": null, "forum": "BklC2RNKDS", "replyto": "BklC2RNKDS", "invitation": "ICLR.cc/2020/Conference/Paper1377/-/Decision", "content": {"decision": "Reject", "comment": "This submission proposes a deep network training method to verify desired temporal properties of the resultant model.\n\nStrengths:\n-The proposed approach is valid and has some interesting components.\n\nWeaknesses:\n-The novelty is limited.\n-The experimental validation could be improved.\n\nOpinion on this paper was mixed but the more confident reviewers believed that novelty is insufficient for acceptance.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sdathath@caltech.edu", "johannes.welbl.14@ucl.ac.uk", "dvij@google.com", "ramanakumar@google.com", "akanade@google.com", "juesato@google.com", "sgowal@google.com", "posenhuang@google.com", "pushmeet@google.com"], "title": "Scalable Neural Learning for Verifiable Consistency with Temporal Specifications", "authors": ["Sumanth Dathathri", "Johannes Welbl", "Krishnamurthy (Dj) Dvijotham", "Ramana Kumar", "Aditya Kanade", "Jonathan Uesato", "Sven Gowal", "Po-Sen Huang", "Pushmeet Kohli"], "pdf": "/pdf/425f0f93b6578e15ef92cb9ea8b3cab68968990e.pdf", "TL;DR": "Neural Network Verification for Temporal Properties and Sequence Generation Models", "abstract": "Formal verification of machine learning models has attracted attention recently, and significant progress has been made on proving simple properties like robustness to small perturbations of the input features. In this context, it has also been observed that folding the verification procedure into training makes it easier to train verifiably robust models. In this paper, we extend the applicability of verified training by extending it to (1) recurrent neural network architectures and (2) complex specifications that go beyond simple adversarial robustness, particularly specifications that capture temporal properties like requiring that a robot periodically visits a charging station or that a language model always produces sentences of bounded length. Experiments show that while models trained using standard training often violate desired specifications, our verified training method produces models that both perform well (in terms of test error or reward) and can be shown to be provably consistent with specifications.", "keywords": ["Verification", "Recurrent Neural Networks", "Reinforcement Learning", "Temporal Logic", "Adversarial Robustness"], "paperhash": "dathathri|scalable_neural_learning_for_verifiable_consistency_with_temporal_specifications", "original_pdf": "/attachment/41ffb7334cb3a23d465f74f15e3f29412077b3c9.pdf", "_bibtex": "@misc{\ndathathri2020scalable,\ntitle={Scalable Neural Learning for Verifiable Consistency with Temporal Specifications},\nauthor={Sumanth Dathathri and Johannes Welbl and Krishnamurthy (Dj) Dvijotham and Ramana Kumar and Aditya Kanade and Jonathan Uesato and Sven Gowal and Po-Sen Huang and Pushmeet Kohli},\nyear={2020},\nurl={https://openreview.net/forum?id=BklC2RNKDS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BklC2RNKDS", "replyto": "BklC2RNKDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795714046, "tmdate": 1576800263807, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1377/-/Decision"}}}, {"id": "S1lT9sNZqr", "original": null, "number": 3, "cdate": 1572060053054, "ddate": null, "tcdate": 1572060053054, "tmdate": 1574415914450, "tddate": null, "forum": "BklC2RNKDS", "replyto": "BklC2RNKDS", "invitation": "ICLR.cc/2020/Conference/Paper1377/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "This paper extends bound propagation based robust training method to complicated settings where temporal specifications are given. Previous works mainly focus on using bound propagation for robust classification only. The authors first extend bound propagation to more complex networks with gates and softmax, and designed a loss function that replies on a lower bound for quantitative semantics of specifications over an input set S. The proposed framework is demonstrated on three tasks: Multi-MNIST captioning, vacuum cleaning robot agent and language generation. The authors formulate specifications using temporal logic, train models with bound propagation to enforce these specifications, and verify them after training. \n\nMy questions regarding this paper are mostly on the three demonstrated tasks:\n\n1. For the Multi-MNIST dataset, I have the following questions:\n\n1(a). In Table 2, it is surprising that even with perturbation of 0.5, the verified and adversarial accuracy is very high. At perturbation epsilon=0.5, it should be possible to perturb the entire image to gray (value 0.5), so I believe the accuracy should be very low here. It is hard to believe under this setting the verified accuracy is still 99%.\n\n1(b). For Table 3, nominal accuracy should also be reported.\n\n1(c). Additionally, how do you define the nominal accuracy here? An example is nominally correct when all digits are predicted correctly in the sequence, or just when the sequence length is predicted correctly?\n\n1(d). For the termination accuracy, do we only care about the sequence length being predicted correctly, or does it also cover the case that all digits in the sequence are predicted correctly? If it is only concerning about the sequence length, this property is a little bit weak.\n\n2. For the RL Robot agent experiment, I have the following questions:\n\n2(a). Because T=10, are you saying that we can only guarantee that the battery is always recharged for any rollouts less than 10 steps? After 10 steps beyond the initial position, can we get any guarantees? A 10-step only guarantee seems too restrictive.\n\n2(b). Are all the properties only verified assuming that the agent starts from the center? I think this assumption is probably also too strong in practice.\n\n2(c). Since all the %verified cells reported in Table 4 are all 100%, it is probably better to make the problem more challenging, by increasing T or considering different initial positions. It is important to show when the performance of the proposed method starts to degrade, to understand the power of the proposed method.\n\n3. For the language generation experiment, the perplexity of the verified training model looks significantly worse than nominal or sampled models. With a perplexity as high as this, I believe the model actually produces garbage. Can you provide some examples of generated texts? I feel language generation is probably not a suitable task for the proposed training method. \n\nOther minor issues:\n\n1. Table 1 should have some horizontal lines - it is hard to align the works with categories on the right.\n\n2. Several papers appear multiple times in references, including \"Differentiable abstract interpretation for provably robust neural networks\", \"Towards fast computation of certified robustness for relu networks\" (and probably others). Also on page 2, Shiqi et al., should be Wang et al. (Shiqi is the first name).\n\n3. I feel the writing is a bit rushed and the authors should make a few more passes on the paper.\n\n\nThis paper makes valid technical contributions, especially the conversion from STL specifications to lower bounds of the quantitative semantics is interesting. Although bound propagation based robust training method is simple to extend to softmax/GRU with interval analysis, applying robust training techniques to the three interesting applications are good contributions. Since the main contribution of this paper is the empirical results on the three tasks, my concerns regarding the experiments need to be addressed before I can vote for accepting this paper. Also, because this paper uses 10 pages, I am expecting the paper to meet a higher standard. Thus, I am voting for a weak reject at this time.\n\n\n****** After author response\n\nThe author response addressed some of my concerns. However I do believe this paper is relatively weak in contribution, especially the experiments can be done more thoroughly. I also appreciate that the authors reduced paper length to 8 pages. I am okay with accepting this paper as it does have some interesting bits, but it is clearly on the borderline and can be further improved.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1377/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1377/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sdathath@caltech.edu", "johannes.welbl.14@ucl.ac.uk", "dvij@google.com", "ramanakumar@google.com", "akanade@google.com", "juesato@google.com", "sgowal@google.com", "posenhuang@google.com", "pushmeet@google.com"], "title": "Scalable Neural Learning for Verifiable Consistency with Temporal Specifications", "authors": ["Sumanth Dathathri", "Johannes Welbl", "Krishnamurthy (Dj) Dvijotham", "Ramana Kumar", "Aditya Kanade", "Jonathan Uesato", "Sven Gowal", "Po-Sen Huang", "Pushmeet Kohli"], "pdf": "/pdf/425f0f93b6578e15ef92cb9ea8b3cab68968990e.pdf", "TL;DR": "Neural Network Verification for Temporal Properties and Sequence Generation Models", "abstract": "Formal verification of machine learning models has attracted attention recently, and significant progress has been made on proving simple properties like robustness to small perturbations of the input features. In this context, it has also been observed that folding the verification procedure into training makes it easier to train verifiably robust models. In this paper, we extend the applicability of verified training by extending it to (1) recurrent neural network architectures and (2) complex specifications that go beyond simple adversarial robustness, particularly specifications that capture temporal properties like requiring that a robot periodically visits a charging station or that a language model always produces sentences of bounded length. Experiments show that while models trained using standard training often violate desired specifications, our verified training method produces models that both perform well (in terms of test error or reward) and can be shown to be provably consistent with specifications.", "keywords": ["Verification", "Recurrent Neural Networks", "Reinforcement Learning", "Temporal Logic", "Adversarial Robustness"], "paperhash": "dathathri|scalable_neural_learning_for_verifiable_consistency_with_temporal_specifications", "original_pdf": "/attachment/41ffb7334cb3a23d465f74f15e3f29412077b3c9.pdf", "_bibtex": "@misc{\ndathathri2020scalable,\ntitle={Scalable Neural Learning for Verifiable Consistency with Temporal Specifications},\nauthor={Sumanth Dathathri and Johannes Welbl and Krishnamurthy (Dj) Dvijotham and Ramana Kumar and Aditya Kanade and Jonathan Uesato and Sven Gowal and Po-Sen Huang and Pushmeet Kohli},\nyear={2020},\nurl={https://openreview.net/forum?id=BklC2RNKDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BklC2RNKDS", "replyto": "BklC2RNKDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1377/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1377/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575527490713, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1377/Reviewers"], "noninvitees": [], "tcdate": 1570237738271, "tmdate": 1575527490727, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1377/-/Official_Review"}}}, {"id": "HJlf4MBiiS", "original": null, "number": 13, "cdate": 1573765674152, "ddate": null, "tcdate": 1573765674152, "tmdate": 1573765674152, "tddate": null, "forum": "BklC2RNKDS", "replyto": "S1lT9sNZqr", "invitation": "ICLR.cc/2020/Conference/Paper1377/-/Official_Comment", "content": {"title": "Thank you for the detailed feedback!", "comment": "Dear Reviewer, \n\nThank you for the detailed review, and feedback. Please let us know if our response addresses your concerns about the empirical results. We would be happy to provide additional details if required."}, "signatures": ["ICLR.cc/2020/Conference/Paper1377/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1377/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sdathath@caltech.edu", "johannes.welbl.14@ucl.ac.uk", "dvij@google.com", "ramanakumar@google.com", "akanade@google.com", "juesato@google.com", "sgowal@google.com", "posenhuang@google.com", "pushmeet@google.com"], "title": "Scalable Neural Learning for Verifiable Consistency with Temporal Specifications", "authors": ["Sumanth Dathathri", "Johannes Welbl", "Krishnamurthy (Dj) Dvijotham", "Ramana Kumar", "Aditya Kanade", "Jonathan Uesato", "Sven Gowal", "Po-Sen Huang", "Pushmeet Kohli"], "pdf": "/pdf/425f0f93b6578e15ef92cb9ea8b3cab68968990e.pdf", "TL;DR": "Neural Network Verification for Temporal Properties and Sequence Generation Models", "abstract": "Formal verification of machine learning models has attracted attention recently, and significant progress has been made on proving simple properties like robustness to small perturbations of the input features. In this context, it has also been observed that folding the verification procedure into training makes it easier to train verifiably robust models. In this paper, we extend the applicability of verified training by extending it to (1) recurrent neural network architectures and (2) complex specifications that go beyond simple adversarial robustness, particularly specifications that capture temporal properties like requiring that a robot periodically visits a charging station or that a language model always produces sentences of bounded length. Experiments show that while models trained using standard training often violate desired specifications, our verified training method produces models that both perform well (in terms of test error or reward) and can be shown to be provably consistent with specifications.", "keywords": ["Verification", "Recurrent Neural Networks", "Reinforcement Learning", "Temporal Logic", "Adversarial Robustness"], "paperhash": "dathathri|scalable_neural_learning_for_verifiable_consistency_with_temporal_specifications", "original_pdf": "/attachment/41ffb7334cb3a23d465f74f15e3f29412077b3c9.pdf", "_bibtex": "@misc{\ndathathri2020scalable,\ntitle={Scalable Neural Learning for Verifiable Consistency with Temporal Specifications},\nauthor={Sumanth Dathathri and Johannes Welbl and Krishnamurthy (Dj) Dvijotham and Ramana Kumar and Aditya Kanade and Jonathan Uesato and Sven Gowal and Po-Sen Huang and Pushmeet Kohli},\nyear={2020},\nurl={https://openreview.net/forum?id=BklC2RNKDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BklC2RNKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1377/Authors", "ICLR.cc/2020/Conference/Paper1377/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1377/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1377/Reviewers", "ICLR.cc/2020/Conference/Paper1377/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1377/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1377/Authors|ICLR.cc/2020/Conference/Paper1377/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504156926, "tmdate": 1576860537593, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1377/Authors", "ICLR.cc/2020/Conference/Paper1377/Reviewers", "ICLR.cc/2020/Conference/Paper1377/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1377/-/Official_Comment"}}}, {"id": "SkgNmtmjsr", "original": null, "number": 12, "cdate": 1573759259527, "ddate": null, "tcdate": 1573759259527, "tmdate": 1573759259527, "tddate": null, "forum": "BklC2RNKDS", "replyto": "SyexW71osB", "invitation": "ICLR.cc/2020/Conference/Paper1377/-/Official_Comment", "content": {"title": "Response", "comment": "Dear Reviewer, \n\nWe thank you for the follow up comments. \n\n>>The property of robustness to bounded perturbation to pixel values does not say much about the correctness of the image classifiers. Having said that, it does prove robustness to some local perturbations.\n\nOur work follows a long line of research motivated by the influential papers (e.g., [1], [2]) which demonstrated lack of robustness of neural networks to small, local perturbations. In particular, we endeavor to bring the tools of verification to address this problem. We specifically go beyond the robustness of classification in this paper, and express and verify temporal properties of sequence-producing neural networks. In addition to multi-MNIST captioning, we also present results on text generation and RL policy networks. For multi-MNIST, we consider bounded perturbation to pixel values. The termination property is motivated by the vulnerability of RNNs to generating longer sequences under small bounded perturbations [3]. Further, our work is not limited to pixel perturbations -- for text generation, we consider word substitutions (e.g., synonym substitutions) and for RL agents, we consider uncertainty in initial pose. Word substitutions are interesting and have been studied in the classification setting as well [4].\n\n>> So training against robustness seems like a high price to pay in training.\n\nTraining for robustness is performed efficiently with only additional cost of 2x slow down (Section 4.2). Further we also show that it does not result in significant degradation of the task objective (Section 5).\n\n>>As I have mentioned before the connection between STL and training is interesting, but does not meet the technical expectation of this conference. It's a trivial interval adaption of the basic semantics of STL in SGD framework,  and I am not convinced that there is anything significant in that. \n\nWe believe that this connection between STL and verified training is important. While simple, we show that this is indeed effective in verified training for properties that go beyond misclassification. Further, we note that our contributions extend beyond this connection -- we extend worst-case analysis to novel architectures, and we also empirically demonstrate that these extensions are effective in practice (Section 5). \n\n[1] Intriguing properties of neural networks, Szegedy et al., 2014\n[2] Towards Evaluating the Robustness of Neural Networks, Carlini et al., https://arxiv.org/abs/1608.04644\n[3] Knowing When to Stop: Evaluation and Verification of Conformity to Output-size Specs, Wang et al.,  CVPR\u201919\n[4] Certified Robustness to Adversarial Word Substitutions, Jia R. et al., EMNLP\u201919 https://arxiv.org/abs/1909.00986"}, "signatures": ["ICLR.cc/2020/Conference/Paper1377/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1377/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sdathath@caltech.edu", "johannes.welbl.14@ucl.ac.uk", "dvij@google.com", "ramanakumar@google.com", "akanade@google.com", "juesato@google.com", "sgowal@google.com", "posenhuang@google.com", "pushmeet@google.com"], "title": "Scalable Neural Learning for Verifiable Consistency with Temporal Specifications", "authors": ["Sumanth Dathathri", "Johannes Welbl", "Krishnamurthy (Dj) Dvijotham", "Ramana Kumar", "Aditya Kanade", "Jonathan Uesato", "Sven Gowal", "Po-Sen Huang", "Pushmeet Kohli"], "pdf": "/pdf/425f0f93b6578e15ef92cb9ea8b3cab68968990e.pdf", "TL;DR": "Neural Network Verification for Temporal Properties and Sequence Generation Models", "abstract": "Formal verification of machine learning models has attracted attention recently, and significant progress has been made on proving simple properties like robustness to small perturbations of the input features. In this context, it has also been observed that folding the verification procedure into training makes it easier to train verifiably robust models. In this paper, we extend the applicability of verified training by extending it to (1) recurrent neural network architectures and (2) complex specifications that go beyond simple adversarial robustness, particularly specifications that capture temporal properties like requiring that a robot periodically visits a charging station or that a language model always produces sentences of bounded length. Experiments show that while models trained using standard training often violate desired specifications, our verified training method produces models that both perform well (in terms of test error or reward) and can be shown to be provably consistent with specifications.", "keywords": ["Verification", "Recurrent Neural Networks", "Reinforcement Learning", "Temporal Logic", "Adversarial Robustness"], "paperhash": "dathathri|scalable_neural_learning_for_verifiable_consistency_with_temporal_specifications", "original_pdf": "/attachment/41ffb7334cb3a23d465f74f15e3f29412077b3c9.pdf", "_bibtex": "@misc{\ndathathri2020scalable,\ntitle={Scalable Neural Learning for Verifiable Consistency with Temporal Specifications},\nauthor={Sumanth Dathathri and Johannes Welbl and Krishnamurthy (Dj) Dvijotham and Ramana Kumar and Aditya Kanade and Jonathan Uesato and Sven Gowal and Po-Sen Huang and Pushmeet Kohli},\nyear={2020},\nurl={https://openreview.net/forum?id=BklC2RNKDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BklC2RNKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1377/Authors", "ICLR.cc/2020/Conference/Paper1377/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1377/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1377/Reviewers", "ICLR.cc/2020/Conference/Paper1377/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1377/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1377/Authors|ICLR.cc/2020/Conference/Paper1377/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504156926, "tmdate": 1576860537593, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1377/Authors", "ICLR.cc/2020/Conference/Paper1377/Reviewers", "ICLR.cc/2020/Conference/Paper1377/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1377/-/Official_Comment"}}}, {"id": "HkghtdJiiB", "original": null, "number": 11, "cdate": 1573742723644, "ddate": null, "tcdate": 1573742723644, "tmdate": 1573742723644, "tddate": null, "forum": "BklC2RNKDS", "replyto": "ryepDdTFor", "invitation": "ICLR.cc/2020/Conference/Paper1377/-/Official_Comment", "content": {"title": "Maintin my score", "comment": "Thanks for the response. I do not know much about the area and I feel that I am unable to properly evaluate this paper. I keep my recommendation of accepting this paper, although my confidence is low."}, "signatures": ["ICLR.cc/2020/Conference/Paper1377/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1377/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sdathath@caltech.edu", "johannes.welbl.14@ucl.ac.uk", "dvij@google.com", "ramanakumar@google.com", "akanade@google.com", "juesato@google.com", "sgowal@google.com", "posenhuang@google.com", "pushmeet@google.com"], "title": "Scalable Neural Learning for Verifiable Consistency with Temporal Specifications", "authors": ["Sumanth Dathathri", "Johannes Welbl", "Krishnamurthy (Dj) Dvijotham", "Ramana Kumar", "Aditya Kanade", "Jonathan Uesato", "Sven Gowal", "Po-Sen Huang", "Pushmeet Kohli"], "pdf": "/pdf/425f0f93b6578e15ef92cb9ea8b3cab68968990e.pdf", "TL;DR": "Neural Network Verification for Temporal Properties and Sequence Generation Models", "abstract": "Formal verification of machine learning models has attracted attention recently, and significant progress has been made on proving simple properties like robustness to small perturbations of the input features. In this context, it has also been observed that folding the verification procedure into training makes it easier to train verifiably robust models. In this paper, we extend the applicability of verified training by extending it to (1) recurrent neural network architectures and (2) complex specifications that go beyond simple adversarial robustness, particularly specifications that capture temporal properties like requiring that a robot periodically visits a charging station or that a language model always produces sentences of bounded length. Experiments show that while models trained using standard training often violate desired specifications, our verified training method produces models that both perform well (in terms of test error or reward) and can be shown to be provably consistent with specifications.", "keywords": ["Verification", "Recurrent Neural Networks", "Reinforcement Learning", "Temporal Logic", "Adversarial Robustness"], "paperhash": "dathathri|scalable_neural_learning_for_verifiable_consistency_with_temporal_specifications", "original_pdf": "/attachment/41ffb7334cb3a23d465f74f15e3f29412077b3c9.pdf", "_bibtex": "@misc{\ndathathri2020scalable,\ntitle={Scalable Neural Learning for Verifiable Consistency with Temporal Specifications},\nauthor={Sumanth Dathathri and Johannes Welbl and Krishnamurthy (Dj) Dvijotham and Ramana Kumar and Aditya Kanade and Jonathan Uesato and Sven Gowal and Po-Sen Huang and Pushmeet Kohli},\nyear={2020},\nurl={https://openreview.net/forum?id=BklC2RNKDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BklC2RNKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1377/Authors", "ICLR.cc/2020/Conference/Paper1377/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1377/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1377/Reviewers", "ICLR.cc/2020/Conference/Paper1377/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1377/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1377/Authors|ICLR.cc/2020/Conference/Paper1377/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504156926, "tmdate": 1576860537593, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1377/Authors", "ICLR.cc/2020/Conference/Paper1377/Reviewers", "ICLR.cc/2020/Conference/Paper1377/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1377/-/Official_Comment"}}}, {"id": "SyexW71osB", "original": null, "number": 10, "cdate": 1573741304022, "ddate": null, "tcdate": 1573741304022, "tmdate": 1573741304022, "tddate": null, "forum": "BklC2RNKDS", "replyto": "HyeROP6toS", "invitation": "ICLR.cc/2020/Conference/Paper1377/-/Official_Comment", "content": {"title": "Reply to Author's Response", "comment": "The property of robustness to bounded perturbation to pixel values does not say much about the correctness\nof the image classifiers. Having said that, it does prove robustness to some local perturbations. So training\nagainst robustness seems like a high price to pay in training. In fact the perturbation epsilons\nquickly start losing meaning, without an understanding of the image manifold.\n\nThe contributions pertaining to robust training of neural networks presented in this paper, in my opinion,\nis not significant enough  to be considered for publication in this conference. As I have mentioned before\nthe connection between STL and training is interesting, but does not meet the technical expectation of this\nconference. It's a trivial interval adaption of the basic semantics of STL in SGD framework,  and I am not convinced that there is anything significant in that. \n\nI would stay with my suggestion to reject the paper.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1377/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1377/AnonReviewer5", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sdathath@caltech.edu", "johannes.welbl.14@ucl.ac.uk", "dvij@google.com", "ramanakumar@google.com", "akanade@google.com", "juesato@google.com", "sgowal@google.com", "posenhuang@google.com", "pushmeet@google.com"], "title": "Scalable Neural Learning for Verifiable Consistency with Temporal Specifications", "authors": ["Sumanth Dathathri", "Johannes Welbl", "Krishnamurthy (Dj) Dvijotham", "Ramana Kumar", "Aditya Kanade", "Jonathan Uesato", "Sven Gowal", "Po-Sen Huang", "Pushmeet Kohli"], "pdf": "/pdf/425f0f93b6578e15ef92cb9ea8b3cab68968990e.pdf", "TL;DR": "Neural Network Verification for Temporal Properties and Sequence Generation Models", "abstract": "Formal verification of machine learning models has attracted attention recently, and significant progress has been made on proving simple properties like robustness to small perturbations of the input features. In this context, it has also been observed that folding the verification procedure into training makes it easier to train verifiably robust models. In this paper, we extend the applicability of verified training by extending it to (1) recurrent neural network architectures and (2) complex specifications that go beyond simple adversarial robustness, particularly specifications that capture temporal properties like requiring that a robot periodically visits a charging station or that a language model always produces sentences of bounded length. Experiments show that while models trained using standard training often violate desired specifications, our verified training method produces models that both perform well (in terms of test error or reward) and can be shown to be provably consistent with specifications.", "keywords": ["Verification", "Recurrent Neural Networks", "Reinforcement Learning", "Temporal Logic", "Adversarial Robustness"], "paperhash": "dathathri|scalable_neural_learning_for_verifiable_consistency_with_temporal_specifications", "original_pdf": "/attachment/41ffb7334cb3a23d465f74f15e3f29412077b3c9.pdf", "_bibtex": "@misc{\ndathathri2020scalable,\ntitle={Scalable Neural Learning for Verifiable Consistency with Temporal Specifications},\nauthor={Sumanth Dathathri and Johannes Welbl and Krishnamurthy (Dj) Dvijotham and Ramana Kumar and Aditya Kanade and Jonathan Uesato and Sven Gowal and Po-Sen Huang and Pushmeet Kohli},\nyear={2020},\nurl={https://openreview.net/forum?id=BklC2RNKDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BklC2RNKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1377/Authors", "ICLR.cc/2020/Conference/Paper1377/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1377/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1377/Reviewers", "ICLR.cc/2020/Conference/Paper1377/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1377/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1377/Authors|ICLR.cc/2020/Conference/Paper1377/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504156926, "tmdate": 1576860537593, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1377/Authors", "ICLR.cc/2020/Conference/Paper1377/Reviewers", "ICLR.cc/2020/Conference/Paper1377/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1377/-/Official_Comment"}}}, {"id": "SJljG_TKoH", "original": null, "number": 5, "cdate": 1573668882777, "ddate": null, "tcdate": 1573668882777, "tmdate": 1573726242297, "tddate": null, "forum": "BklC2RNKDS", "replyto": "S1lT9sNZqr", "invitation": "ICLR.cc/2020/Conference/Paper1377/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "We thank the reviewer for the detailed and helpful comments. We have improved the presentation of the experimental results as suggested by the reviewer. We have also reduced the length of the paper. We respond to the comments in details below.\n\n1(a). Note that the specification we consider from Wang et al., CVPR\u201919  is only violated when the network produces a longer sequence than the true number of digits in the input image. At large perturbations (eps ~ 0.5), when the digits become indistinguishable, the network learns to predict that the image has no more than one digit. This satisfies the specification (we make this observation at the end of page 8 in the original submission, have clarified further during revision -- page 7, paragraph above run-time considerations), even if the nominal accuracy is very low. This is in contrast with standard adversarial robustness, where at large perturbations the epsilon balls of images from different classes start to overlap and it is not possible to be robust, resulting in low verified accuracies.\n\n1(b). We have added nominal accuracy in Table 3 (Caption, Table 3).\n\n1(c). We use token-level accuracy as a measure of nominal accuracy, that is, the percentage of correctly predicted tokens -- including the end of sequence token.\n\n1(d). By definition, termination accuracy only covers sequence length. This specification is motivated by Wang et al., CVPR\u201919 where it was shown that sequence generating models are often vulnerable to violating this property. Nevertheless, richer temporal properties can be specified in STL and analyzed through our method.\n\n2(a). We guarantee that at every time-step within the first T steps, the battery is recharged within the next T_recharge steps. For experimentation purposes, we use T=10, but it can be configured. In this work, we focus on the core problem of training neural networks in a verified manner for a finite number of time steps.\n\nIt is worth noting in this context that verifiably trained policies satisfying such bounded-time properties can be chained to plan for infinite time horizons in a receding horizon manner with on-line monitoring. For instance, in a scenario where the agent has bounded uncertainty on it\u2019s pose at each time instant, we can monitor online to guarantee that there is no failure in the next T steps at every time instant of an infinite horizon execution (a similar approach is used in https://arxiv.org/abs/1703.09563 ).\n\n\n\n2(b). No, we don\u2019t assume that the agent starts at the center. The agent starts in an epsilon-neighborhood of the center of any cell on the map (Figure 3). We\u2019ve reworded Appendix F1 to make this more clear.\n\n2(c). Different initial positions would not degrade the performance with regard to verification guarantees we are able to provide, assuming the new initial positions are considered during verified training. The two key things that would cause a drop in verification performance are i) the size of the initial region, ii) the time horizon.\n\nAs we increase the size of initial regions we are verifying, due to the increased variations, the performance can degrade, but the verification approach still achieves better results. At \\epsilon=2.0, the fraction of cells for which we can verify the property drops to ~64% while that for the baseline approaches drops to under 20%. Further, depending on the coefficient for the verification loss, even for longer horizon problems, the verification performance might stay similar but with a possible reduction in the reward. We will consider including experiments with longer horizons for the final version.\n\n3. The text generation model is able to produce reasonable text. We have included some samples (Appendix I). The perplexity is exponentiated negative log likelihood (NLL). Here, NLL is the task objective and on the test, the mean NLL is around 5.1, 5.2, and 5.4 for nominal, sampling and verified training respectively. In this regard, the degradation does not compromise the text generating model.\n\nMinor issues and presentation length:\nWe have fixed the minor issues, thanks for pointing them out. In order to improve accessibility of the paper, we did not want to assume specific background from the readers. This resulted in longer length of the paper. We have now moved some of the bound propagation details to the appendix  and restructured the  task description details to shorten the paper to recommended 8 pages.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1377/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1377/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sdathath@caltech.edu", "johannes.welbl.14@ucl.ac.uk", "dvij@google.com", "ramanakumar@google.com", "akanade@google.com", "juesato@google.com", "sgowal@google.com", "posenhuang@google.com", "pushmeet@google.com"], "title": "Scalable Neural Learning for Verifiable Consistency with Temporal Specifications", "authors": ["Sumanth Dathathri", "Johannes Welbl", "Krishnamurthy (Dj) Dvijotham", "Ramana Kumar", "Aditya Kanade", "Jonathan Uesato", "Sven Gowal", "Po-Sen Huang", "Pushmeet Kohli"], "pdf": "/pdf/425f0f93b6578e15ef92cb9ea8b3cab68968990e.pdf", "TL;DR": "Neural Network Verification for Temporal Properties and Sequence Generation Models", "abstract": "Formal verification of machine learning models has attracted attention recently, and significant progress has been made on proving simple properties like robustness to small perturbations of the input features. In this context, it has also been observed that folding the verification procedure into training makes it easier to train verifiably robust models. In this paper, we extend the applicability of verified training by extending it to (1) recurrent neural network architectures and (2) complex specifications that go beyond simple adversarial robustness, particularly specifications that capture temporal properties like requiring that a robot periodically visits a charging station or that a language model always produces sentences of bounded length. Experiments show that while models trained using standard training often violate desired specifications, our verified training method produces models that both perform well (in terms of test error or reward) and can be shown to be provably consistent with specifications.", "keywords": ["Verification", "Recurrent Neural Networks", "Reinforcement Learning", "Temporal Logic", "Adversarial Robustness"], "paperhash": "dathathri|scalable_neural_learning_for_verifiable_consistency_with_temporal_specifications", "original_pdf": "/attachment/41ffb7334cb3a23d465f74f15e3f29412077b3c9.pdf", "_bibtex": "@misc{\ndathathri2020scalable,\ntitle={Scalable Neural Learning for Verifiable Consistency with Temporal Specifications},\nauthor={Sumanth Dathathri and Johannes Welbl and Krishnamurthy (Dj) Dvijotham and Ramana Kumar and Aditya Kanade and Jonathan Uesato and Sven Gowal and Po-Sen Huang and Pushmeet Kohli},\nyear={2020},\nurl={https://openreview.net/forum?id=BklC2RNKDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BklC2RNKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1377/Authors", "ICLR.cc/2020/Conference/Paper1377/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1377/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1377/Reviewers", "ICLR.cc/2020/Conference/Paper1377/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1377/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1377/Authors|ICLR.cc/2020/Conference/Paper1377/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504156926, "tmdate": 1576860537593, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1377/Authors", "ICLR.cc/2020/Conference/Paper1377/Reviewers", "ICLR.cc/2020/Conference/Paper1377/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1377/-/Official_Comment"}}}, {"id": "HyeROP6toS", "original": null, "number": 4, "cdate": 1573668725724, "ddate": null, "tcdate": 1573668725724, "tmdate": 1573716116941, "tddate": null, "forum": "BklC2RNKDS", "replyto": "rylcDWV_5S", "invitation": "ICLR.cc/2020/Conference/Paper1377/-/Official_Comment", "content": {"title": "Response to Reviewer 5", "comment": "We thank the reviewer for the helpful comments.\n\n>>Response to \u201clack of sufficient contributions\u201d\n\nSTL has been shown to be an expressive formalism to specify temporal properties of real-valued signals. As the reviewer points out, it has been used in detecting violations of specifications (falsification) in the hybrid systems domain by approaches like S-Tarilo and Breach. The novel  contributions of our paper are as follows:\nWe make a novel connection between STL and verified training of neural networks that generate sequential outputs which facilitates training networks that can be easily verified to be consistent with an STL specification. Unlike falsification which typically employs stochastic search for falsifying inputs, we design a novel loss function derived by computing a differentiable bound on the worst-case violation of the STL specification (under quantitative semantics) and incorporate this into an SGD-based training scheme for the network.\nWe demonstrate the practical utility of this approach on neural networks (including auto-regressive RNN/GRUs) for captioning, language generation and reinforcement learning -- tasks and architectures that have not been considered for verified training before.\n\nWe have added a discussion to relate our work with S-Tarilo and Breach in Section 2 (first paragraph, page 3).\n\n>>Response to \u201cgeneral negative opinion about the use of interval propagation to train neural networks\u201d\n\nWe respectfully disagree with the reviewer\u2019s claim. On the contrary, [1, 2] have shown that interval propagation can be used very effectively to verifiably train large neural networks that were beyond the scope of other methods. Specific to our paper, we clearly demonstrate that interval propagation can be leveraged effectively to train complex networks such as RNNs and for complex temporal specifications. Very recent papers such as [3, 4] are some other examples of successful use of interval propagation for verified training of neural networks. Further, we note that the overall framework of training neural networks to be verifiable against STL that we present in this work can also be instantiated using any other method of over-approximating reachable states of neural networks (for example using LP relaxations or other abstractions).\n\n[1] On the Effectiveness of Interval Bound Propagation for Training Verifiably, Gowal, S. et al., ICCV\u201919\n[2] A Provable Defense for Deep Residual Networks. Mirman, M. et al., https://arxiv.org/abs/1903.12519\n[3] Certified Robustness to Adversarial Word Substitutions, Jia R. et al., EMNLP\u201919 https://arxiv.org/abs/1909.00986\n[4]  Achieving Verified Robustness to Symbol Substitutions via Interval Bound Propagation, Huang, P. et al., EMNLP\u201919 https://arxiv.org/abs/1909.01492\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1377/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1377/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sdathath@caltech.edu", "johannes.welbl.14@ucl.ac.uk", "dvij@google.com", "ramanakumar@google.com", "akanade@google.com", "juesato@google.com", "sgowal@google.com", "posenhuang@google.com", "pushmeet@google.com"], "title": "Scalable Neural Learning for Verifiable Consistency with Temporal Specifications", "authors": ["Sumanth Dathathri", "Johannes Welbl", "Krishnamurthy (Dj) Dvijotham", "Ramana Kumar", "Aditya Kanade", "Jonathan Uesato", "Sven Gowal", "Po-Sen Huang", "Pushmeet Kohli"], "pdf": "/pdf/425f0f93b6578e15ef92cb9ea8b3cab68968990e.pdf", "TL;DR": "Neural Network Verification for Temporal Properties and Sequence Generation Models", "abstract": "Formal verification of machine learning models has attracted attention recently, and significant progress has been made on proving simple properties like robustness to small perturbations of the input features. In this context, it has also been observed that folding the verification procedure into training makes it easier to train verifiably robust models. In this paper, we extend the applicability of verified training by extending it to (1) recurrent neural network architectures and (2) complex specifications that go beyond simple adversarial robustness, particularly specifications that capture temporal properties like requiring that a robot periodically visits a charging station or that a language model always produces sentences of bounded length. Experiments show that while models trained using standard training often violate desired specifications, our verified training method produces models that both perform well (in terms of test error or reward) and can be shown to be provably consistent with specifications.", "keywords": ["Verification", "Recurrent Neural Networks", "Reinforcement Learning", "Temporal Logic", "Adversarial Robustness"], "paperhash": "dathathri|scalable_neural_learning_for_verifiable_consistency_with_temporal_specifications", "original_pdf": "/attachment/41ffb7334cb3a23d465f74f15e3f29412077b3c9.pdf", "_bibtex": "@misc{\ndathathri2020scalable,\ntitle={Scalable Neural Learning for Verifiable Consistency with Temporal Specifications},\nauthor={Sumanth Dathathri and Johannes Welbl and Krishnamurthy (Dj) Dvijotham and Ramana Kumar and Aditya Kanade and Jonathan Uesato and Sven Gowal and Po-Sen Huang and Pushmeet Kohli},\nyear={2020},\nurl={https://openreview.net/forum?id=BklC2RNKDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BklC2RNKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1377/Authors", "ICLR.cc/2020/Conference/Paper1377/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1377/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1377/Reviewers", "ICLR.cc/2020/Conference/Paper1377/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1377/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1377/Authors|ICLR.cc/2020/Conference/Paper1377/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504156926, "tmdate": 1576860537593, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1377/Authors", "ICLR.cc/2020/Conference/Paper1377/Reviewers", "ICLR.cc/2020/Conference/Paper1377/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1377/-/Official_Comment"}}}, {"id": "ryxnVICFsB", "original": null, "number": 8, "cdate": 1573672500472, "ddate": null, "tcdate": 1573672500472, "tmdate": 1573672500472, "tddate": null, "forum": "BklC2RNKDS", "replyto": "BklC2RNKDS", "invitation": "ICLR.cc/2020/Conference/Paper1377/-/Official_Comment", "content": {"title": "Revised manuscript", "comment": "We thank the reviewers for the feedback. We have responded to their comments below.\n\nWe have revised the paper to address the comments from the reviewers. The main changes to the paper are:\n1) An improved experiments section to address Reviewer 2\u2019s comments\n2) Shortening of the paper to address the comment about the paper being longer than the recommended length from Reviewers 1 and 3.\n3) We have also restructured Section 3 as suggested by Reviewer 2, and moved some of the details to the Appendix.\n\nWe would be happy to answer any further queries.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1377/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1377/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sdathath@caltech.edu", "johannes.welbl.14@ucl.ac.uk", "dvij@google.com", "ramanakumar@google.com", "akanade@google.com", "juesato@google.com", "sgowal@google.com", "posenhuang@google.com", "pushmeet@google.com"], "title": "Scalable Neural Learning for Verifiable Consistency with Temporal Specifications", "authors": ["Sumanth Dathathri", "Johannes Welbl", "Krishnamurthy (Dj) Dvijotham", "Ramana Kumar", "Aditya Kanade", "Jonathan Uesato", "Sven Gowal", "Po-Sen Huang", "Pushmeet Kohli"], "pdf": "/pdf/425f0f93b6578e15ef92cb9ea8b3cab68968990e.pdf", "TL;DR": "Neural Network Verification for Temporal Properties and Sequence Generation Models", "abstract": "Formal verification of machine learning models has attracted attention recently, and significant progress has been made on proving simple properties like robustness to small perturbations of the input features. In this context, it has also been observed that folding the verification procedure into training makes it easier to train verifiably robust models. In this paper, we extend the applicability of verified training by extending it to (1) recurrent neural network architectures and (2) complex specifications that go beyond simple adversarial robustness, particularly specifications that capture temporal properties like requiring that a robot periodically visits a charging station or that a language model always produces sentences of bounded length. Experiments show that while models trained using standard training often violate desired specifications, our verified training method produces models that both perform well (in terms of test error or reward) and can be shown to be provably consistent with specifications.", "keywords": ["Verification", "Recurrent Neural Networks", "Reinforcement Learning", "Temporal Logic", "Adversarial Robustness"], "paperhash": "dathathri|scalable_neural_learning_for_verifiable_consistency_with_temporal_specifications", "original_pdf": "/attachment/41ffb7334cb3a23d465f74f15e3f29412077b3c9.pdf", "_bibtex": "@misc{\ndathathri2020scalable,\ntitle={Scalable Neural Learning for Verifiable Consistency with Temporal Specifications},\nauthor={Sumanth Dathathri and Johannes Welbl and Krishnamurthy (Dj) Dvijotham and Ramana Kumar and Aditya Kanade and Jonathan Uesato and Sven Gowal and Po-Sen Huang and Pushmeet Kohli},\nyear={2020},\nurl={https://openreview.net/forum?id=BklC2RNKDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BklC2RNKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1377/Authors", "ICLR.cc/2020/Conference/Paper1377/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1377/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1377/Reviewers", "ICLR.cc/2020/Conference/Paper1377/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1377/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1377/Authors|ICLR.cc/2020/Conference/Paper1377/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504156926, "tmdate": 1576860537593, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1377/Authors", "ICLR.cc/2020/Conference/Paper1377/Reviewers", "ICLR.cc/2020/Conference/Paper1377/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1377/-/Official_Comment"}}}, {"id": "ryepDdTFor", "original": null, "number": 7, "cdate": 1573668964817, "ddate": null, "tcdate": 1573668964817, "tmdate": 1573668964817, "tddate": null, "forum": "BklC2RNKDS", "replyto": "rJlni8-AYS", "invitation": "ICLR.cc/2020/Conference/Paper1377/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "We thank the reviewer for the helpful comments. We have incorporated the suggestions regarding restructuring Section 3. We address the other points below.\n\n      1. We have included language samples from the different types of training (Appendix I).\n\nThe perplexity is exponentiated negative log likelihood (NLL). Here, NLL is the task objective and on the test, the mean NLL is around 5.1, 5.2, and 5.4 for nominal, sampling and verified training respectively. In this regard, the degradation does not compromise the language model.\n      \n      2. Baselines: \n\nWe consider task-specific architectures as baseline models. Since this is the first work that uses STL based verified training of auto-regressive neural networks, there are no other verified training approaches to compare with. Nevertheless, we compare against the other prominent approaches in the literature for improving or verifying robustness of neural networks such as adversarial training and also consider the only currently available method for verifying auto-regressive networks with the MILP based approach from Wang et al., CVPR\u201919 .\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1377/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1377/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sdathath@caltech.edu", "johannes.welbl.14@ucl.ac.uk", "dvij@google.com", "ramanakumar@google.com", "akanade@google.com", "juesato@google.com", "sgowal@google.com", "posenhuang@google.com", "pushmeet@google.com"], "title": "Scalable Neural Learning for Verifiable Consistency with Temporal Specifications", "authors": ["Sumanth Dathathri", "Johannes Welbl", "Krishnamurthy (Dj) Dvijotham", "Ramana Kumar", "Aditya Kanade", "Jonathan Uesato", "Sven Gowal", "Po-Sen Huang", "Pushmeet Kohli"], "pdf": "/pdf/425f0f93b6578e15ef92cb9ea8b3cab68968990e.pdf", "TL;DR": "Neural Network Verification for Temporal Properties and Sequence Generation Models", "abstract": "Formal verification of machine learning models has attracted attention recently, and significant progress has been made on proving simple properties like robustness to small perturbations of the input features. In this context, it has also been observed that folding the verification procedure into training makes it easier to train verifiably robust models. In this paper, we extend the applicability of verified training by extending it to (1) recurrent neural network architectures and (2) complex specifications that go beyond simple adversarial robustness, particularly specifications that capture temporal properties like requiring that a robot periodically visits a charging station or that a language model always produces sentences of bounded length. Experiments show that while models trained using standard training often violate desired specifications, our verified training method produces models that both perform well (in terms of test error or reward) and can be shown to be provably consistent with specifications.", "keywords": ["Verification", "Recurrent Neural Networks", "Reinforcement Learning", "Temporal Logic", "Adversarial Robustness"], "paperhash": "dathathri|scalable_neural_learning_for_verifiable_consistency_with_temporal_specifications", "original_pdf": "/attachment/41ffb7334cb3a23d465f74f15e3f29412077b3c9.pdf", "_bibtex": "@misc{\ndathathri2020scalable,\ntitle={Scalable Neural Learning for Verifiable Consistency with Temporal Specifications},\nauthor={Sumanth Dathathri and Johannes Welbl and Krishnamurthy (Dj) Dvijotham and Ramana Kumar and Aditya Kanade and Jonathan Uesato and Sven Gowal and Po-Sen Huang and Pushmeet Kohli},\nyear={2020},\nurl={https://openreview.net/forum?id=BklC2RNKDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BklC2RNKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1377/Authors", "ICLR.cc/2020/Conference/Paper1377/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1377/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1377/Reviewers", "ICLR.cc/2020/Conference/Paper1377/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1377/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1377/Authors|ICLR.cc/2020/Conference/Paper1377/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504156926, "tmdate": 1576860537593, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1377/Authors", "ICLR.cc/2020/Conference/Paper1377/Reviewers", "ICLR.cc/2020/Conference/Paper1377/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1377/-/Official_Comment"}}}, {"id": "H1gcV_6Ysr", "original": null, "number": 6, "cdate": 1573668913692, "ddate": null, "tcdate": 1573668913692, "tmdate": 1573668913692, "tddate": null, "forum": "BklC2RNKDS", "replyto": "Skxg_LZW5r", "invitation": "ICLR.cc/2020/Conference/Paper1377/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "We thank the reviewer for the helpful comments. In order to improve accessibility of the paper, we did not want to assume specific background from the readers. This resulted in longer length of the paper. We have now moved some of the bound propagation details, and the task description details to the appendix along with a rewrite of Sections 2, 3, 4 to shorten the paper to recommended 8 pages.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1377/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1377/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sdathath@caltech.edu", "johannes.welbl.14@ucl.ac.uk", "dvij@google.com", "ramanakumar@google.com", "akanade@google.com", "juesato@google.com", "sgowal@google.com", "posenhuang@google.com", "pushmeet@google.com"], "title": "Scalable Neural Learning for Verifiable Consistency with Temporal Specifications", "authors": ["Sumanth Dathathri", "Johannes Welbl", "Krishnamurthy (Dj) Dvijotham", "Ramana Kumar", "Aditya Kanade", "Jonathan Uesato", "Sven Gowal", "Po-Sen Huang", "Pushmeet Kohli"], "pdf": "/pdf/425f0f93b6578e15ef92cb9ea8b3cab68968990e.pdf", "TL;DR": "Neural Network Verification for Temporal Properties and Sequence Generation Models", "abstract": "Formal verification of machine learning models has attracted attention recently, and significant progress has been made on proving simple properties like robustness to small perturbations of the input features. In this context, it has also been observed that folding the verification procedure into training makes it easier to train verifiably robust models. In this paper, we extend the applicability of verified training by extending it to (1) recurrent neural network architectures and (2) complex specifications that go beyond simple adversarial robustness, particularly specifications that capture temporal properties like requiring that a robot periodically visits a charging station or that a language model always produces sentences of bounded length. Experiments show that while models trained using standard training often violate desired specifications, our verified training method produces models that both perform well (in terms of test error or reward) and can be shown to be provably consistent with specifications.", "keywords": ["Verification", "Recurrent Neural Networks", "Reinforcement Learning", "Temporal Logic", "Adversarial Robustness"], "paperhash": "dathathri|scalable_neural_learning_for_verifiable_consistency_with_temporal_specifications", "original_pdf": "/attachment/41ffb7334cb3a23d465f74f15e3f29412077b3c9.pdf", "_bibtex": "@misc{\ndathathri2020scalable,\ntitle={Scalable Neural Learning for Verifiable Consistency with Temporal Specifications},\nauthor={Sumanth Dathathri and Johannes Welbl and Krishnamurthy (Dj) Dvijotham and Ramana Kumar and Aditya Kanade and Jonathan Uesato and Sven Gowal and Po-Sen Huang and Pushmeet Kohli},\nyear={2020},\nurl={https://openreview.net/forum?id=BklC2RNKDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BklC2RNKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1377/Authors", "ICLR.cc/2020/Conference/Paper1377/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1377/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1377/Reviewers", "ICLR.cc/2020/Conference/Paper1377/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1377/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1377/Authors|ICLR.cc/2020/Conference/Paper1377/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504156926, "tmdate": 1576860537593, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1377/Authors", "ICLR.cc/2020/Conference/Paper1377/Reviewers", "ICLR.cc/2020/Conference/Paper1377/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1377/-/Official_Comment"}}}, {"id": "rJlni8-AYS", "original": null, "number": 1, "cdate": 1571849891712, "ddate": null, "tcdate": 1571849891712, "tmdate": 1572972476643, "tddate": null, "forum": "BklC2RNKDS", "replyto": "BklC2RNKDS", "invitation": "ICLR.cc/2020/Conference/Paper1377/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper concerns verification of neural networks through verified training and interval bound propagation. Namely, the authors rely on the fact (reported in the literature earlier, but also confirmed here) that verified training leads to neural networks that are easier to verify. The main contributions of this work are 1) extending interval bound propagation to recurrent computation and auto-regressive models as often encountered in NLP or RL settings which allows verified training of these models, 2) introducing the Signal Temporal Logic (STL) for specifying temporal constraints for a model, and extending its quantitative semantics for reasoning about sets of inputs, 3) providing empirical proof that the STL with bound propagation can be used to ensure that neural models conform to temporal specification without large performance losses.\n\nThe introduced method is well motivated and well-placed within the literature, with the related works section providing a good overview of the field; it also clearly mentions how the proposed method differs from the prior art. Section 3 describes the STL syntax and three specifications for different tasks; while the provided examples are nice, they are also long-winded and make the exposition difficult to follow---I think that their details should be moved to the appendix. Section 4 is very technical, and I do not have enough knowledge to verify it thoroughly, but the proposed approach seems to make sense. Finally, Section 5 presents experimental evaluation, which is performed on three different tasks: image caption generation, RL with a mobile robot, and language generation. These three experiments seem to be enough variety to prove the utility of the method. My only concerns are that 1) the loss of perplexity with verified training in the language modelling setup is disturbingly high as compared to the nominal method, and 2) the proposed method is compared to only one baseline in each experiment, and it is unclear whether the baseline are state-of-the-art without knowing the literature (which I do not know).\n\nI recommend ACCEPTing this paper, albeit with low confidence. This is because the paper addresses an interesting and important problem, and the provided results are convincing. Having said that, I know nothing about the area of formal verification."}, "signatures": ["ICLR.cc/2020/Conference/Paper1377/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1377/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sdathath@caltech.edu", "johannes.welbl.14@ucl.ac.uk", "dvij@google.com", "ramanakumar@google.com", "akanade@google.com", "juesato@google.com", "sgowal@google.com", "posenhuang@google.com", "pushmeet@google.com"], "title": "Scalable Neural Learning for Verifiable Consistency with Temporal Specifications", "authors": ["Sumanth Dathathri", "Johannes Welbl", "Krishnamurthy (Dj) Dvijotham", "Ramana Kumar", "Aditya Kanade", "Jonathan Uesato", "Sven Gowal", "Po-Sen Huang", "Pushmeet Kohli"], "pdf": "/pdf/425f0f93b6578e15ef92cb9ea8b3cab68968990e.pdf", "TL;DR": "Neural Network Verification for Temporal Properties and Sequence Generation Models", "abstract": "Formal verification of machine learning models has attracted attention recently, and significant progress has been made on proving simple properties like robustness to small perturbations of the input features. In this context, it has also been observed that folding the verification procedure into training makes it easier to train verifiably robust models. In this paper, we extend the applicability of verified training by extending it to (1) recurrent neural network architectures and (2) complex specifications that go beyond simple adversarial robustness, particularly specifications that capture temporal properties like requiring that a robot periodically visits a charging station or that a language model always produces sentences of bounded length. Experiments show that while models trained using standard training often violate desired specifications, our verified training method produces models that both perform well (in terms of test error or reward) and can be shown to be provably consistent with specifications.", "keywords": ["Verification", "Recurrent Neural Networks", "Reinforcement Learning", "Temporal Logic", "Adversarial Robustness"], "paperhash": "dathathri|scalable_neural_learning_for_verifiable_consistency_with_temporal_specifications", "original_pdf": "/attachment/41ffb7334cb3a23d465f74f15e3f29412077b3c9.pdf", "_bibtex": "@misc{\ndathathri2020scalable,\ntitle={Scalable Neural Learning for Verifiable Consistency with Temporal Specifications},\nauthor={Sumanth Dathathri and Johannes Welbl and Krishnamurthy (Dj) Dvijotham and Ramana Kumar and Aditya Kanade and Jonathan Uesato and Sven Gowal and Po-Sen Huang and Pushmeet Kohli},\nyear={2020},\nurl={https://openreview.net/forum?id=BklC2RNKDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BklC2RNKDS", "replyto": "BklC2RNKDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1377/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1377/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575527490713, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1377/Reviewers"], "noninvitees": [], "tcdate": 1570237738271, "tmdate": 1575527490727, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1377/-/Official_Review"}}}, {"id": "Skxg_LZW5r", "original": null, "number": 2, "cdate": 1572046439897, "ddate": null, "tcdate": 1572046439897, "tmdate": 1572972476597, "tddate": null, "forum": "BklC2RNKDS", "replyto": "BklC2RNKDS", "invitation": "ICLR.cc/2020/Conference/Paper1377/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper presents a way to train time-series regressors verifiably with respect to a set of rules defined by signal temporal logic (STL). The bulk of work is in deriving bound propagation rules for the STL language. The resulting lower bound of an auxiliary quantity (which is required to be non-negative) is then maximized for verifiability.\n\nThis technique is demonstrated on three tasks and compares favorably to the baseline from Wang et al. (2019).\n\nI am not an expert in this area. However, to the best of my knowledge I don't see anything immediately wrong with this and it seems novel. Therefore I recommend acceptance.\n\nThe paper is also above recommended length."}, "signatures": ["ICLR.cc/2020/Conference/Paper1377/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1377/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sdathath@caltech.edu", "johannes.welbl.14@ucl.ac.uk", "dvij@google.com", "ramanakumar@google.com", "akanade@google.com", "juesato@google.com", "sgowal@google.com", "posenhuang@google.com", "pushmeet@google.com"], "title": "Scalable Neural Learning for Verifiable Consistency with Temporal Specifications", "authors": ["Sumanth Dathathri", "Johannes Welbl", "Krishnamurthy (Dj) Dvijotham", "Ramana Kumar", "Aditya Kanade", "Jonathan Uesato", "Sven Gowal", "Po-Sen Huang", "Pushmeet Kohli"], "pdf": "/pdf/425f0f93b6578e15ef92cb9ea8b3cab68968990e.pdf", "TL;DR": "Neural Network Verification for Temporal Properties and Sequence Generation Models", "abstract": "Formal verification of machine learning models has attracted attention recently, and significant progress has been made on proving simple properties like robustness to small perturbations of the input features. In this context, it has also been observed that folding the verification procedure into training makes it easier to train verifiably robust models. In this paper, we extend the applicability of verified training by extending it to (1) recurrent neural network architectures and (2) complex specifications that go beyond simple adversarial robustness, particularly specifications that capture temporal properties like requiring that a robot periodically visits a charging station or that a language model always produces sentences of bounded length. Experiments show that while models trained using standard training often violate desired specifications, our verified training method produces models that both perform well (in terms of test error or reward) and can be shown to be provably consistent with specifications.", "keywords": ["Verification", "Recurrent Neural Networks", "Reinforcement Learning", "Temporal Logic", "Adversarial Robustness"], "paperhash": "dathathri|scalable_neural_learning_for_verifiable_consistency_with_temporal_specifications", "original_pdf": "/attachment/41ffb7334cb3a23d465f74f15e3f29412077b3c9.pdf", "_bibtex": "@misc{\ndathathri2020scalable,\ntitle={Scalable Neural Learning for Verifiable Consistency with Temporal Specifications},\nauthor={Sumanth Dathathri and Johannes Welbl and Krishnamurthy (Dj) Dvijotham and Ramana Kumar and Aditya Kanade and Jonathan Uesato and Sven Gowal and Po-Sen Huang and Pushmeet Kohli},\nyear={2020},\nurl={https://openreview.net/forum?id=BklC2RNKDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BklC2RNKDS", "replyto": "BklC2RNKDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1377/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1377/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575527490713, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1377/Reviewers"], "noninvitees": [], "tcdate": 1570237738271, "tmdate": 1575527490727, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1377/-/Official_Review"}}}, {"id": "rylcDWV_5S", "original": null, "number": 4, "cdate": 1572516193872, "ddate": null, "tcdate": 1572516193872, "tmdate": 1572972476505, "tddate": null, "forum": "BklC2RNKDS", "replyto": "BklC2RNKDS", "invitation": "ICLR.cc/2020/Conference/Paper1377/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #5", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper focuses on verifying sequential properties of deep beural networks. Linear Temporal Logic (LTL) is a \nnatural way to express temporal properties, and has been extensively studied in the formal methods community. \nSignal temporal logic (STL) is a natural extension,  of LTL. STL specifications provide a rich set of formulations to encode intent for real valued signal over time. Formally proving STL formulae is intractable. But, it is possible to falsify such properties. This has been the main goal for various tools like Breach, and S-Taliro.\n\n\nPros :\nA  very  interesting avenue  explored in this paper, is using the syntax of\nSTL to formulate properties about multiple-MNIST, Safe RL and NLP applications.\nEven though the conversion from STL specifications to scalar valued function is a very well known technique.\n\nCons :\nIn my opinion, the paper lacks sufficient contributions in itself to be accepted at this conference. The idea of training\nfor robustness using intervals, has been well known for a while. The authors extend that to get conservative estimates of the level of satisfaction of the STL formula, and use that in the training process. Though training for robustness is an\ninteresting idea in itself, but the general opinion about using interval propagation to train networks is negative.\n\nOverall : Though the direction of this work is interesting but lacks sufficient technical novelty."}, "signatures": ["ICLR.cc/2020/Conference/Paper1377/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1377/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sdathath@caltech.edu", "johannes.welbl.14@ucl.ac.uk", "dvij@google.com", "ramanakumar@google.com", "akanade@google.com", "juesato@google.com", "sgowal@google.com", "posenhuang@google.com", "pushmeet@google.com"], "title": "Scalable Neural Learning for Verifiable Consistency with Temporal Specifications", "authors": ["Sumanth Dathathri", "Johannes Welbl", "Krishnamurthy (Dj) Dvijotham", "Ramana Kumar", "Aditya Kanade", "Jonathan Uesato", "Sven Gowal", "Po-Sen Huang", "Pushmeet Kohli"], "pdf": "/pdf/425f0f93b6578e15ef92cb9ea8b3cab68968990e.pdf", "TL;DR": "Neural Network Verification for Temporal Properties and Sequence Generation Models", "abstract": "Formal verification of machine learning models has attracted attention recently, and significant progress has been made on proving simple properties like robustness to small perturbations of the input features. In this context, it has also been observed that folding the verification procedure into training makes it easier to train verifiably robust models. In this paper, we extend the applicability of verified training by extending it to (1) recurrent neural network architectures and (2) complex specifications that go beyond simple adversarial robustness, particularly specifications that capture temporal properties like requiring that a robot periodically visits a charging station or that a language model always produces sentences of bounded length. Experiments show that while models trained using standard training often violate desired specifications, our verified training method produces models that both perform well (in terms of test error or reward) and can be shown to be provably consistent with specifications.", "keywords": ["Verification", "Recurrent Neural Networks", "Reinforcement Learning", "Temporal Logic", "Adversarial Robustness"], "paperhash": "dathathri|scalable_neural_learning_for_verifiable_consistency_with_temporal_specifications", "original_pdf": "/attachment/41ffb7334cb3a23d465f74f15e3f29412077b3c9.pdf", "_bibtex": "@misc{\ndathathri2020scalable,\ntitle={Scalable Neural Learning for Verifiable Consistency with Temporal Specifications},\nauthor={Sumanth Dathathri and Johannes Welbl and Krishnamurthy (Dj) Dvijotham and Ramana Kumar and Aditya Kanade and Jonathan Uesato and Sven Gowal and Po-Sen Huang and Pushmeet Kohli},\nyear={2020},\nurl={https://openreview.net/forum?id=BklC2RNKDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BklC2RNKDS", "replyto": "BklC2RNKDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1377/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1377/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575527490713, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1377/Reviewers"], "noninvitees": [], "tcdate": 1570237738271, "tmdate": 1575527490727, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1377/-/Official_Review"}}}], "count": 15}