{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1363744920000, "tcdate": 1363744920000, "number": 1, "id": "MqwZf2jPZCJ-n", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "i87JIQTAnB8AQ", "replyto": "i87JIQTAnB8AQ", "signatures": ["Hugo Van hamme"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "First: sorry for the multiple postings. Browser acting weird. Can't remove them ...\r\n\r\nUpdate: I was able to get the sbcd code to work. Two mods required (refer to Algorithm 1 in the Li, Lebanon & Park paper - ref [18] in v2 paper on arxiv):\r\n1) you have to be careful with initialization. If the estimates for W or H are too large, E = A - WH could potentially contain too many zeros in line 3 and the update maps H to all zeros. Solution: I first perform a multiplicative update on W and H so you have reasonably scaled estimates.\r\n2) line 16 is wrongly implemented in the publicly available ffhals5.m \r\n\r\nI reran the comparison (different machine though - the one I used before was fully loaded):\r\n1) CCD (ref [17]) - the c++ code compiled to a matlab mex file as downloaded from the author's website and following their instructions. \r\n2) DNA - fully implemented in matlab as available from http://www.esat.kuleuven.be/psi/spraak/downloads/\r\n3) SBCD (ref [18]) - code fully in matlab  with mods above\r\n4) MU (multiplicative updates) - implementation fully in matlab as available from http://www.esat.kuleuven.be/psi/spraak/downloads/\r\n\r\nThe KLD as a function of the iteration for the rank-10 random 1000x500 matrix is shown in  https://dl.dropbox.com/u/915791/iteration.pdf. \r\nWe observe that SBCD takes a good start but then slows down. DNA is best after the 5th iteration.\r\n\r\nThe KLD as a function of CPU time is shown in https://dl.dropbox.com/u/915791/time.pdf\r\nDNA is the clear winner, followed by MU which beats both SBCD and CCD.   This may be surprising, but as I mentioned earlier, there are some implementation issues. CCD is a single-thread implementation, while matlab is multi-threaded and works in parrallel. However, the cyclic updates in CCD are not very suitable for parallelization. The SBCD needs reimplementation, honestly.\r\n\r\nIn summary, DNA does compare favourably to the state-of-the-art, but I don't really feel comfortable about including such a comparison in a scientific paper if there is such a dominant effect of programming style/skills on the result."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "The Diagonalized Newton Algorithm for Nonnegative Matrix Factorization", "decision": "conferencePoster-iclr2013-conference", "abstract": "Non-negative matrix factorization (NMF) has become a popular machine learning approach to many problems in text mining, speech and image processing, bio-informatics and seismic data analysis to name a few. In NMF, a matrix of non-negative data is approximated by the low-rank product of two matrices with non-negative entries. In this paper, the approximation quality is measured by the Kullback-Leibler divergence between the data and its low-rank reconstruction. The existence of the simple multiplicative update (MU) algorithm for computing the matrix factors has contributed to the success of NMF. Despite the availability of algorithms showing faster convergence, MU remains popular due to its simplicity. In this paper, a diagonalized Newton algorithm (DNA) is proposed showing faster convergence while the implementation remains simple and suitable for high-rank problems. The DNA algorithm is applied to various publicly available data sets, showing a substantial speed-up on modern hardware.", "pdf": "https://arxiv.org/abs/1301.3389", "paperhash": "hamme|the_diagonalized_newton_algorithm_for_nonnegative_matrix_factorization", "authors": ["Hugo Van hamme"], "authorids": ["hugo.vanhamme@esat.kuleuven.be"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363615980000, "tcdate": 1363615980000, "number": 2, "id": "aplzZcXNokptc", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "i87JIQTAnB8AQ", "replyto": "i87JIQTAnB8AQ", "signatures": ["Hugo Van hamme"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "About the comparison with Cyclic Coordinate Descent (as described in C.-J. Hsieh and I. S. Dhillon, \u201cFast Coordinate Descent Methods with Variable Selection for Non-negative Matrix Factorization,\u201d in proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD), San Diego, CA, USA, August 2011) using their software:\r\nthe plots of the KLD as a function of iteration number and cpu time are located at https://dl.dropbox.com/u/915791/iteration.pdf and https://dl.dropbox.com/u/915791/time.pdf\r\nThe data is the synthetic  1000x500 random matrix of rank 10. They show DNA has comparable convergence behaviour and the implementation is  faster, despite it's matlab (DNA) vs. c++ (CCD)."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "The Diagonalized Newton Algorithm for Nonnegative Matrix Factorization", "decision": "conferencePoster-iclr2013-conference", "abstract": "Non-negative matrix factorization (NMF) has become a popular machine learning approach to many problems in text mining, speech and image processing, bio-informatics and seismic data analysis to name a few. In NMF, a matrix of non-negative data is approximated by the low-rank product of two matrices with non-negative entries. In this paper, the approximation quality is measured by the Kullback-Leibler divergence between the data and its low-rank reconstruction. The existence of the simple multiplicative update (MU) algorithm for computing the matrix factors has contributed to the success of NMF. Despite the availability of algorithms showing faster convergence, MU remains popular due to its simplicity. In this paper, a diagonalized Newton algorithm (DNA) is proposed showing faster convergence while the implementation remains simple and suitable for high-rank problems. The DNA algorithm is applied to various publicly available data sets, showing a substantial speed-up on modern hardware.", "pdf": "https://arxiv.org/abs/1301.3389", "paperhash": "hamme|the_diagonalized_newton_algorithm_for_nonnegative_matrix_factorization", "authors": ["Hugo Van hamme"], "authorids": ["hugo.vanhamme@esat.kuleuven.be"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363574460000, "tcdate": 1363574460000, "number": 5, "id": "RzSh7m1KhlzKg", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "i87JIQTAnB8AQ", "replyto": "i87JIQTAnB8AQ", "signatures": ["Hugo Van hamme"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "I would like to thank the reviewers for their investment of time and effort to formulate their valued comments. The paper was updated according to your comments. Below I address your concerns:\r\n\r\nA common remark is the lack of comparison with state-of-the-art NMF solvers for Kullback-Leibler divergence (KLD). I compared the performance of the diagonalized Newton algorithm (DNA) with the wide-spread multiplicative updates (MU) exactly because it is the most common baseline and almost every algorithm has been compared against it. As you suggested, I did run comparison tests and I will present the results here. I need to find a method to post some figures to make the point clear. First, I compared against the Cyclic Coordinate Descent (CCD) by Hsieh & Dhillon using the software they provide on their website. I ran the synthetic 1000x500 example (rank 10). The KLD as a function of iteration number for DNA and CCD are very close (I did not find a way to post a plot on this forum). However, in terms of CPU (ran on the machine I mention in the paper) DNA is a lot faster with about 200ms per iteration for CCD and about 50ms for DNA. Note that CCD is completely implemented in C++ (embedded in a mex-file) while DNA is implemented in matlab (with one routine in mex - see the download page mentioned in the paper). As for the comparison with SBCD (scalar block coordinate descent), I also ran their code on the same example, but unfortunately, one of the matrix factors is projected to an all-zero matrix in the first iteration. I have not found the cause yet.\r\nWhat definitely needs investigation is that I observe CCD to be 4 times slower than DNA. Using my implementation for MU, 1200 MU iterations are actually as fast as the 100 CCD iteration. (My matlab MU implementation is 10 times faster than the one provided by Hsieh&Dhillon). For these reasons, I am not too keen on quickly including a comparison in terms of CPU time (which is really the bottom line), as implementation issues seem not so trivial. Even more so for a comparison on a GPU, where the picture could be different from the CPU for the cyclic updates in CCD. A thorough comparison on these two architectures seems like a substantial amount of future work.  But I hope the data above data convince you the present paper and public code are significant work. \r\n\r\nReply to Anonymous 57f3\r\n' it's not clear that matrix factorization is a problem for which optimization speed is a primary concern (all of the experiments in the paper terminate after only a few minutes)'\r\n\r\n>> There are practical problems where NMF takes hours, e.g. the problems of [6], which is essentially learning a speech recognizer model from data. We are now applying NMF-based speech recognition in learning paradigms that learn from user interaction examples. In such cases, you want to wait seconds, not minutes. Also, there is an increased interest in 'large-sccale NMF problems'.\r\n\r\n'Using a KL-divergence objective seems strange to me since there aren't any distributions involved, just matrices, whose entries, while positive, need not sum to 1 along any row or column. Are the entries of the matrices supposed to represent probabilities? '\r\n\r\n>> Notice that the second and third term in the expression for KLD (Eq. 1) are normalization terms such that we don't require V or Z to sum to unity. This very common in the NMF literature, and was motivated in a.o. [1]. KLD is appropriate if the data follow a (mixture of) Poisson distribution. While this is realistic for counts data (like in the Newsgroup corpus), the KLD is also applied on Fourier spectra, e.g. for speaker separation or speech enhancement, with success. Imho, the relevance of KLD does not need to be motivated in a paper on algorithms, see also [18] and [20] ( numbering in the new paper).\r\n\r\n'I understand that this is a formulation used in previous work ([1]), but it should be briefly explained. '\r\n>> Added a sentence about the Poisson hypothesis after Eq. 1.\r\n\r\n'You should explain the connection between your work and [17] more carefully. Exactly how is it similar/different? '\r\n>> Reformulated. [17] (now [18]) uses a totally different motivation, but also involves the second order derivatives, like a Newton method.\r\n\r\n'Has a diagonal Newton-type approach ever been used for the squared error objective? '\r\n>> A reference is given now. Note however that KLD behaves substantially different.\r\n'the smallest cost' -> 'leading to the greatest reduction in d_{KL}(V,Z)'\r\n 'the variables required to compute' -> 'the quantities required to compute' \r\n>> corrected\r\n\r\nYou should avoid using two meanings of the word 'regularized' as this can lead to confusion. Maybe 'damped' would work better to refer to the modifications made to the Newton updates that prevent divergence? \r\n>> Yes. A lot better. Corrected.\r\n\r\n'Have you compared to using damped/'regularized' Newton updates instead of your method of selecting the best between the Newton and MU updates? In my experience, damping, along the lines of the LM algorithm or something similar, can help a great deal. '\r\n>> yes. I initially tried to control the damping by adding lambda*I to the Hessian, where lambda is decreased on success and increased if the KLD increases. I found it difficult to find a setting that worked well on a variety of problems. \r\n\r\nI would recommend using '\top' to denote matrix transposition instead of what you are doing. Section 2 needs to be reorganized. It's hard for me to follow what you are trying to say here. First, you introduce some regularization terms. Then, you derive a particular fixed-point update scheme. When you say 'Minimizing [...] is achieved by alternative updates...' surely you mean that this is just one particular way it might be done. \r\n>> That's indeed what I meant to say. 'is' => 'can be'\r\n\r\nYou say you are applying the KKT conditions, but your derivation is strange and you seem to skip a bunch of steps and neglect to use explicit KKT multipliers (although the result seems correct based on my independent derivation). But when you say: 'If h_r = 0, the partial derivative is positive. Hence the product of h_r and the partial derivative is always zero', I don't see how this is a correct logical implication. Rather, the product is zero for any solution satisfying complementary slackness. \r\n>> I meant this holds for any solution of (5). This is corrected.\r\n\r\nAnd I don't understand why it is particularly important that the sum over equation (6) is zero (which is how the normalization in eqn 10 is justified). Surely this is only a (weak) necessary condition, but not a sufficient one, for a valid optimal solution. Or is there some reason why this is sufficient (if so, please state it in the paper!). \r\n>> A Newton update may yield a guess that does not satisfy this (weak) necessary condition. We can satisfy this condition easily with the renormalization (10), which is reflected in steps 16 and 29.\r\n\r\nI don't understand how the sentence on line 122 'Therefor...' is not a valid logical implication. Did you actually mean to use the word 'therefor' here? The lower bound is, however, correct. 'floor resp. ceiling'??\r\n>> 'Therefore' => 'To respect the nonnegativity and to avoid the singularity\u201d\r\n\r\nReply to Anonymous 4322\r\nSee comparison described above.\r\nI added more about the differences with the prior work you mention.\r\n\r\nReply to Anonymous 482c\r\nSee also comparison data detailed above.\r\nYou are right there is a lot of generic work on Hessian preconditioning. I refer to papers that work on damping and line search in the context of NMF ([10], [11], [12], [14] ...). Diagonalization is only related in the sense that it ensures the Hessian to be positive definite (not in general, but here is does)."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "The Diagonalized Newton Algorithm for Nonnegative Matrix Factorization", "decision": "conferencePoster-iclr2013-conference", "abstract": "Non-negative matrix factorization (NMF) has become a popular machine learning approach to many problems in text mining, speech and image processing, bio-informatics and seismic data analysis to name a few. In NMF, a matrix of non-negative data is approximated by the low-rank product of two matrices with non-negative entries. In this paper, the approximation quality is measured by the Kullback-Leibler divergence between the data and its low-rank reconstruction. The existence of the simple multiplicative update (MU) algorithm for computing the matrix factors has contributed to the success of NMF. Despite the availability of algorithms showing faster convergence, MU remains popular due to its simplicity. In this paper, a diagonalized Newton algorithm (DNA) is proposed showing faster convergence while the implementation remains simple and suitable for high-rank problems. The DNA algorithm is applied to various publicly available data sets, showing a substantial speed-up on modern hardware.", "pdf": "https://arxiv.org/abs/1301.3389", "paperhash": "hamme|the_diagonalized_newton_algorithm_for_nonnegative_matrix_factorization", "authors": ["Hugo Van hamme"], "authorids": ["hugo.vanhamme@esat.kuleuven.be"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362382860000, "tcdate": 1362382860000, "number": 3, "id": "EW5mE9upmnWp1", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "i87JIQTAnB8AQ", "replyto": "i87JIQTAnB8AQ", "signatures": ["anonymous reviewer 482c"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of The Diagonalized Newton Algorithm for Nonnegative Matrix Factorization", "review": "Overview:\r\n\r\nThis paper proposes an element-wise (diagonal Hessian) Newton method to speed up convergence of the multiplicative update algorithm (MU) for NMF problems. Monotonic progress is guaranteed by an element-wise fall-back mechanism to MU. At a minimal computational overhead, this is shown to be effective in a number of experiments. \r\n\r\nThe paper is well-written, the experimental validation is convincing, and the author provides detailed pseudocode and a matlab implementation.\r\n\r\n\r\nComments:\r\n\r\nThere is a large body of related work outside of the NMF field that considers diagonal Hessian preconditioning of updates, going back (at least) as early as Becker & LeCun in 1988.\r\n\r\nSwitching between EM and Newton update (using whichever is best, element-wise) is an interesting alternative to more classical forms of line search: it may be worth doing a more detailed comparison to such established techniques.\r\n\r\nI would appreciate a discussion of the potential of extending the idea to non KL-divergence costs."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "The Diagonalized Newton Algorithm for Nonnegative Matrix Factorization", "decision": "conferencePoster-iclr2013-conference", "abstract": "Non-negative matrix factorization (NMF) has become a popular machine learning approach to many problems in text mining, speech and image processing, bio-informatics and seismic data analysis to name a few. In NMF, a matrix of non-negative data is approximated by the low-rank product of two matrices with non-negative entries. In this paper, the approximation quality is measured by the Kullback-Leibler divergence between the data and its low-rank reconstruction. The existence of the simple multiplicative update (MU) algorithm for computing the matrix factors has contributed to the success of NMF. Despite the availability of algorithms showing faster convergence, MU remains popular due to its simplicity. In this paper, a diagonalized Newton algorithm (DNA) is proposed showing faster convergence while the implementation remains simple and suitable for high-rank problems. The DNA algorithm is applied to various publicly available data sets, showing a substantial speed-up on modern hardware.", "pdf": "https://arxiv.org/abs/1301.3389", "paperhash": "hamme|the_diagonalized_newton_algorithm_for_nonnegative_matrix_factorization", "authors": ["Hugo Van hamme"], "authorids": ["hugo.vanhamme@esat.kuleuven.be"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362210360000, "tcdate": 1362210360000, "number": 4, "id": "FFkZF49pZx-pS", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "i87JIQTAnB8AQ", "replyto": "i87JIQTAnB8AQ", "signatures": ["anonymous reviewer 4322"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of The Diagonalized Newton Algorithm for Nonnegative Matrix Factorization", "review": "Summary:\r\n\r\nThe paper presents a new algorithm for solving L1 regularized NMF problems in which the fitting term is the Kullback-Leiber divergence. The strategy combines the classic multiplicative updates with a diagonal approximation of Newton's method for solving the KKT conditions of the NMF optimization problem. This approximation results in a multiplicative update that is computationally light. Since the objective function might increase under the Newton updates, the author proposes to simultaneously compute both multiplicative and Newton updates and choose the one that produces the largest descent. The algorithm is tested on several datasets, generally producing improvements in both number of iterations and computational time with respect to the standard multiplicative updates.\r\n\r\nI believe that the paper is well written. It proposes an efficient optimization algorithm for solving a problem that is not novel but very important in many applications. The author should highlight the strengths of the proposed approach and the differences with recent works presented in the literature.\r\n\r\nPros.:\r\n\r\n- the paper addresses an important problem in matrix factorization,\r\nextensively used in audio processing applications\r\n- the experimental results show that the method is more efficient than the multiplicative algorithm (which is the most widely used optimization tool), without significantly increasing the algorithmic complexity\r\n\r\nCons:\r\n\r\n- experimental comparisons against related approaches is missing\r\n- this approach seems limited to only work for the Kullback-Leiber\r\ndivergence as fitting cost.\r\n\r\n\r\nGeneral comments:\r\n\r\nI believe that the paper lacks of experimental comparisons with other accelerated optimization schemes for solving the same problem. In particular, I believe that the author should include comparisons with [17] and the work,\r\n\r\nC.-J. Hsieh and I. S. Dhillon. Fast coordinate descent methods with variable selection for non-negative matrix factorization. In Proceedings of the 17th ACM SIGKDD, pages 1064\u20131072, 2011.\r\n\r\nwhich should also be cited.\r\n\r\nAs the author points out, the approach in [17] is very similar to the one proposed in this paper (they have code available online).  The work by Hsieh and Dhillon is also very related to this paper. They propose a coordinate descent method using Newton's method to solve the individual one-variable sub-problems. More details on the differences with these two works should be provided in Section 1.\r\n\r\nThe experimental setting itself seems convincing. Figures 2 and 3 are never cited in the paper."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "The Diagonalized Newton Algorithm for Nonnegative Matrix Factorization", "decision": "conferencePoster-iclr2013-conference", "abstract": "Non-negative matrix factorization (NMF) has become a popular machine learning approach to many problems in text mining, speech and image processing, bio-informatics and seismic data analysis to name a few. In NMF, a matrix of non-negative data is approximated by the low-rank product of two matrices with non-negative entries. In this paper, the approximation quality is measured by the Kullback-Leibler divergence between the data and its low-rank reconstruction. The existence of the simple multiplicative update (MU) algorithm for computing the matrix factors has contributed to the success of NMF. Despite the availability of algorithms showing faster convergence, MU remains popular due to its simplicity. In this paper, a diagonalized Newton algorithm (DNA) is proposed showing faster convergence while the implementation remains simple and suitable for high-rank problems. The DNA algorithm is applied to various publicly available data sets, showing a substantial speed-up on modern hardware.", "pdf": "https://arxiv.org/abs/1301.3389", "paperhash": "hamme|the_diagonalized_newton_algorithm_for_nonnegative_matrix_factorization", "authors": ["Hugo Van hamme"], "authorids": ["hugo.vanhamme@esat.kuleuven.be"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362192540000, "tcdate": 1362192540000, "number": 6, "id": "oo1KoBhzu3CGs", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "i87JIQTAnB8AQ", "replyto": "i87JIQTAnB8AQ", "signatures": ["anonymous reviewer 57f3"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of The Diagonalized Newton Algorithm for Nonnegative Matrix Factorization", "review": "This paper develops a new iterative optimization algorithm for performing non-negative matrix factorization, assuming a standard 'KL-divergence' objective function.  The method proposed combines the use of a traditional updating scheme ('multiplicative updates' from [1]) in the initial phase of optimization, with a diagonal Newton approach which is automatically switched to when it will help.  This switching is accomplished by always computing both updates and taking whichever is best, which will typically be MU at the start and the more rapidly converging (but less stable) Newton method towards the end.  Additionally, the diagonal Newton updates are made more stable using a few tricks, some of which are standard and some of which may not be.  It is found that this can provide speed-ups which may be mild or significant, depending on the application, versus a standard approach which only uses multiplicative updates.  As pointed out by the authors, Newton-type methods have been explored for non-negative matrix factorization before, but not for this particularly objective with a diagonal approximation (except perhaps [17]?).\r\n\r\nThe writing is rough in a few places but okay overall.  The experimental results seem satisfactory compared to the classical algorithm from [1], although comparisons to other potentially more recent approaches is conspicuously absent.  I'm not an experiment on matrix factorization or these particular datasets so it's hard for me to independently judge if these results are competitive with state of the art methods.\r\n\r\nThe paper doesn't seem particularly novel to me, but matrix factorization isn't a topic I find particularly interesting, so this probably biases me against the paper somewhat.  \r\n\r\nPros:\r\n- reasonably well presented\r\n- empirical results seem okay\r\nCons:\r\n- comparisons to more recent approaches is lacking\r\n- it's not clear that matrix factorization is a problem for which optimization speed is a primary concern (all of the experiments in the paper terminate after only a few minutes)\r\n- writing is rough in a few places\r\n\r\n\r\n\r\nDetailed comments:\r\n\r\nUsing a KL-divergence objective seems strange to me since there aren't any distributions involved, just matrices, whose entries, while positive, need not sum to 1 along any row or column.  Are the entries of the matrices supposed to represent probabilities?  I understand that this is a formulation used in previous work ([1]), but it should be briefly explained.\r\n\r\nYou should explain the connection between your work and [17] more carefully.  Exactly how is it similar/different?\r\n\r\nHas a diagonal Newton-type approach ever been used for the squared error objective?\r\n\r\n'the smallest cost' -> 'leading to the greatest reduction in d_{KL}(V,Z)'\r\n\r\n'the variables required to compute' -> 'the quantities required to compute'\r\n\r\nYou should avoid using two meanings of the word 'regularized' as this can lead to confusion.  Maybe 'damped' would work better to refer to the modifications made to the Newton updates that prevent divergence?\r\n\r\nHave you compared to using damped/'regularized' Newton updates instead of your method of selecting the best between the Newton and MU updates?  In my experience, damping, along the lines of the LM algorithm or something similar, can help a great deal.\r\n\r\nI would recommend using '\top' to denote matrix transposition instead of what you are doing.\r\n\r\nSection 2 needs to be reorganized.  It's hard for me to follow what you are trying to say here.  First, you introduce some regularization terms.  Then, you derive a particular fixed-point update scheme.  When you say 'Minimizing [...] is achieved by alternative updates...' surely you mean that this is just one particular way it might be done.  Also, are these derivation prior work (e.g. from [1])?  If so, it should be stated.\r\n\r\nIt's hard to follow the derivations in this section.  You say you are applying the KKT conditions, but your derivation is strange and you seem to skip a bunch of steps and neglect to use explicit KKT multipliers (although the result seems correct based on my independent derivation).  But when you say: 'If h_r = 0, the partial derivative is positive. Hence the product of h_r and the partial derivative is always zero', I don't see how this is a correct logical implication.  Rather, the product is zero for any solution satisfying complementary slackness.  And I don't understand why it is particularly important that the sum over equation (6) is zero (which is how the normalization in eqn 10 is justified).  Surely this is only a (weak) necessary condition, but not a sufficient one, for a valid optimal solution.  Or is there some reason why this is sufficient (if so, please state it in the paper!).\r\n\r\nI don't understand how the sentence on line 122 'Therefor...' is not a valid logical implication.  Did you actually mean to use the word 'therefor' here?  The lower bound is, however, correct.\r\n\r\n'floor resp. ceiling'??"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "The Diagonalized Newton Algorithm for Nonnegative Matrix Factorization", "decision": "conferencePoster-iclr2013-conference", "abstract": "Non-negative matrix factorization (NMF) has become a popular machine learning approach to many problems in text mining, speech and image processing, bio-informatics and seismic data analysis to name a few. In NMF, a matrix of non-negative data is approximated by the low-rank product of two matrices with non-negative entries. In this paper, the approximation quality is measured by the Kullback-Leibler divergence between the data and its low-rank reconstruction. The existence of the simple multiplicative update (MU) algorithm for computing the matrix factors has contributed to the success of NMF. Despite the availability of algorithms showing faster convergence, MU remains popular due to its simplicity. In this paper, a diagonalized Newton algorithm (DNA) is proposed showing faster convergence while the implementation remains simple and suitable for high-rank problems. The DNA algorithm is applied to various publicly available data sets, showing a substantial speed-up on modern hardware.", "pdf": "https://arxiv.org/abs/1301.3389", "paperhash": "hamme|the_diagonalized_newton_algorithm_for_nonnegative_matrix_factorization", "authors": ["Hugo Van hamme"], "authorids": ["hugo.vanhamme@esat.kuleuven.be"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1358343900000, "tcdate": 1358343900000, "number": 60, "id": "i87JIQTAnB8AQ", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "i87JIQTAnB8AQ", "signatures": ["hugo.vanhamme@esat.kuleuven.be"], "readers": ["everyone"], "content": {"title": "The Diagonalized Newton Algorithm for Nonnegative Matrix Factorization", "decision": "conferencePoster-iclr2013-conference", "abstract": "Non-negative matrix factorization (NMF) has become a popular machine learning approach to many problems in text mining, speech and image processing, bio-informatics and seismic data analysis to name a few. In NMF, a matrix of non-negative data is approximated by the low-rank product of two matrices with non-negative entries. In this paper, the approximation quality is measured by the Kullback-Leibler divergence between the data and its low-rank reconstruction. The existence of the simple multiplicative update (MU) algorithm for computing the matrix factors has contributed to the success of NMF. Despite the availability of algorithms showing faster convergence, MU remains popular due to its simplicity. In this paper, a diagonalized Newton algorithm (DNA) is proposed showing faster convergence while the implementation remains simple and suitable for high-rank problems. The DNA algorithm is applied to various publicly available data sets, showing a substantial speed-up on modern hardware.", "pdf": "https://arxiv.org/abs/1301.3389", "paperhash": "hamme|the_diagonalized_newton_algorithm_for_nonnegative_matrix_factorization", "authors": ["Hugo Van hamme"], "authorids": ["hugo.vanhamme@esat.kuleuven.be"], "keywords": [], "conflicts": []}, "writers": [], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 7}