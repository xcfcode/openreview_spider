{"notes": [{"id": "HkeGhoA5FX", "original": "r1eMkfaqt7", "number": 693, "cdate": 1538087850417, "ddate": null, "tcdate": 1538087850417, "tmdate": 1550694028364, "tddate": null, "forum": "HkeGhoA5FX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Residual Non-local Attention Networks for Image Restoration", "abstract": "In this paper, we propose a residual non-local attention network for high-quality image restoration. Without considering the uneven distribution of information in the corrupted images, previous methods are restricted by local convolutional operation and equal treatment of spatial- and channel-wise features. To address this issue, we design local and non-local attention blocks to extract features that capture the long-range dependencies between pixels and pay more attention to the challenging parts. Specifically, we design trunk branch and (non-)local mask branch in each (non-)local attention block. The trunk branch is used to extract hierarchical features. Local and non-local mask branches aim to adaptively rescale these hierarchical features with mixed attentions. The local mask branch concentrates on more local structures with convolutional operations, while non-local attention considers more about long-range dependencies in the whole feature map. Furthermore, we propose residual local and non-local attention learning to train the very deep network, which further enhance the representation ability of the network. Our proposed method can be generalized for various image restoration applications, such as image denoising, demosaicing, compression artifacts reduction, and super-resolution. Experiments demonstrate that our method obtains comparable or better results compared with recently leading methods quantitatively and visually.", "keywords": ["Non-local network", "attention network", "image restoration", "residual learning"], "authorids": ["yulun100@gmail.com", "kunpengli@ece.neu.edu", "li.kai.gml@gmail.com", "bnzhong@hqu.edu.cn", "yunfu@ece.neu.edu"], "authors": ["Yulun Zhang", "Kunpeng Li", "Kai Li", "Bineng Zhong", "Yun Fu"], "TL;DR": "New state-of-the-art framework for image restoration", "pdf": "/pdf/04121d8887f84077f6486db18444e9113aa62a24.pdf", "paperhash": "zhang|residual_nonlocal_attention_networks_for_image_restoration", "_bibtex": "@inproceedings{\nzhang2018residual,\ntitle={Residual Non-local Attention Networks for Image Restoration},\nauthor={Yulun Zhang and Kunpeng Li and Kai Li and Bineng Zhong and Yun Fu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HkeGhoA5FX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "ByxCIyGegE", "original": null, "number": 1, "cdate": 1544720214387, "ddate": null, "tcdate": 1544720214387, "tmdate": 1545354508756, "tddate": null, "forum": "HkeGhoA5FX", "replyto": "HkeGhoA5FX", "invitation": "ICLR.cc/2019/Conference/-/Paper693/Meta_Review", "content": {"metareview": "1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.\n\n- strong qualitative and quantitative results\n- a good ablative analysis of the proposed method.\n \n2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.\n\n- clarity could be improved (and was much improved in the revision).\n- somewhat limited novelty.\n \n3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it\u2019s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.\n\nNo major points of contention.\n \n4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.\n\nThe reviewers reached a consensus that the paper should be accepted.\n", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "somewhat limited novelty but significant advancement of SOTA"}, "signatures": ["ICLR.cc/2019/Conference/Paper693/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper693/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Residual Non-local Attention Networks for Image Restoration", "abstract": "In this paper, we propose a residual non-local attention network for high-quality image restoration. Without considering the uneven distribution of information in the corrupted images, previous methods are restricted by local convolutional operation and equal treatment of spatial- and channel-wise features. To address this issue, we design local and non-local attention blocks to extract features that capture the long-range dependencies between pixels and pay more attention to the challenging parts. Specifically, we design trunk branch and (non-)local mask branch in each (non-)local attention block. The trunk branch is used to extract hierarchical features. Local and non-local mask branches aim to adaptively rescale these hierarchical features with mixed attentions. The local mask branch concentrates on more local structures with convolutional operations, while non-local attention considers more about long-range dependencies in the whole feature map. Furthermore, we propose residual local and non-local attention learning to train the very deep network, which further enhance the representation ability of the network. Our proposed method can be generalized for various image restoration applications, such as image denoising, demosaicing, compression artifacts reduction, and super-resolution. Experiments demonstrate that our method obtains comparable or better results compared with recently leading methods quantitatively and visually.", "keywords": ["Non-local network", "attention network", "image restoration", "residual learning"], "authorids": ["yulun100@gmail.com", "kunpengli@ece.neu.edu", "li.kai.gml@gmail.com", "bnzhong@hqu.edu.cn", "yunfu@ece.neu.edu"], "authors": ["Yulun Zhang", "Kunpeng Li", "Kai Li", "Bineng Zhong", "Yun Fu"], "TL;DR": "New state-of-the-art framework for image restoration", "pdf": "/pdf/04121d8887f84077f6486db18444e9113aa62a24.pdf", "paperhash": "zhang|residual_nonlocal_attention_networks_for_image_restoration", "_bibtex": "@inproceedings{\nzhang2018residual,\ntitle={Residual Non-local Attention Networks for Image Restoration},\nauthor={Yulun Zhang and Kunpeng Li and Kai Li and Bineng Zhong and Yun Fu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HkeGhoA5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper693/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353122542, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkeGhoA5FX", "replyto": "HkeGhoA5FX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper693/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper693/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper693/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353122542}}}, {"id": "BJeBKz333m", "original": null, "number": 2, "cdate": 1541354109452, "ddate": null, "tcdate": 1541354109452, "tmdate": 1543489142332, "tddate": null, "forum": "HkeGhoA5FX", "replyto": "HkeGhoA5FX", "invitation": "ICLR.cc/2019/Conference/-/Paper693/Official_Review", "content": {"title": "excellent results, but unclear novelty and lacking explanations", "review": "The paper proposes a convolutional neural network architecture that includes blocks for local and non-local attention mechanisms, which are claimed to be responsible for achieving excellent results in four image restoration applications.\n\n\n# Results\nThe strongest point of the paper is that the quantitative and qualitative image restoration results appear to be very good, although they seem almost a bit too good.\n\n\n# Novelty\nI'm not sure about the novelty of the paper, but I suspect it to be rather incremental. The paper says \"To the best of our knowledge, this is the first time to consider residual non-local attention for image restoration problems.\" Does that mean non-local attention (in a very similar way) has already been used, just not in a residual fashion? If so, that would not constitute much novelty. I have to admit that I'm not familiar with the related work on attention, but I did not understand *why* the results of the proposed method are supposed to be much better than that of previous work.\n\n\n# Clarity\nI think the paper is not self-contained enough, since it seems to implicitly assume substantial background knowledge on attention mechanisms in CNNs. \n\nFurthermore, the introduction of the paper identifies three problems with existing CNNs that I don't necessarily fully agree with. None of these supposed problems are backed up by (experimental) evidence.\n\nI don't think it is sufficient to just show superior results than previous methods. It is also important to disentangle why the results are better. However, the presented ablation experiments are not very illuminating to me.\n\nThe attempts at explaining what the novel attention blocks do and why they lead to superior results are very vague to me. Maybe they are understandable in the context of related work, but I found many statements, such as the following, devoid of meaning:\n- \"Without considering the uneven distribution of information in the corrupted images, [...]\"\n- \"However, in this paper, we mainly focus on learning non-local attention to better guide feature extraction in trunk branch.\"\n- \"We only incorporate residual non-local attention block in low-level and high-level feature space. This is mainly because a few non-local modules can well offer non-local ability to the network for image restoration.\"\n- \"The key point in mask branch is how to grasp information of larger scope, namely larger receptive field size, so that it\u2019s possible to obtain more sophisticated attention map.\"\n\n\n# Experiments\n- The experimental results are the best part of the paper. However, it would've been nice to include some qualitative results in the main paper.\n- The proposed RNAN model is trained on a big dataset (800 images with ~2 million pixels each). Are the competing methods trained on datasets of similar size? If not, this could be a major reason for improved performance of RNAN over competing methods. At least in the appendix, RNAN and FFDNet are compared more fairly since they are trained with the same/similar data.\n- The qualitative examples in the appendix mostly show close-ups/details of very structured regions (mostly stripy patterns). Please also show some other regions without self-similar structures.\n\n\n# Misc\n- Residual non-local attention learning (section 3.3) was not clear to me.\n- The word \"trunk\" is used without definition or explanation.\n- Fig. 2 caption is too short, please expand.\n\n# Update (2018-11-29)\nGiven the substantial author feedback, I'm willing to raise my score.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper693/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Residual Non-local Attention Networks for Image Restoration", "abstract": "In this paper, we propose a residual non-local attention network for high-quality image restoration. Without considering the uneven distribution of information in the corrupted images, previous methods are restricted by local convolutional operation and equal treatment of spatial- and channel-wise features. To address this issue, we design local and non-local attention blocks to extract features that capture the long-range dependencies between pixels and pay more attention to the challenging parts. Specifically, we design trunk branch and (non-)local mask branch in each (non-)local attention block. The trunk branch is used to extract hierarchical features. Local and non-local mask branches aim to adaptively rescale these hierarchical features with mixed attentions. The local mask branch concentrates on more local structures with convolutional operations, while non-local attention considers more about long-range dependencies in the whole feature map. Furthermore, we propose residual local and non-local attention learning to train the very deep network, which further enhance the representation ability of the network. Our proposed method can be generalized for various image restoration applications, such as image denoising, demosaicing, compression artifacts reduction, and super-resolution. Experiments demonstrate that our method obtains comparable or better results compared with recently leading methods quantitatively and visually.", "keywords": ["Non-local network", "attention network", "image restoration", "residual learning"], "authorids": ["yulun100@gmail.com", "kunpengli@ece.neu.edu", "li.kai.gml@gmail.com", "bnzhong@hqu.edu.cn", "yunfu@ece.neu.edu"], "authors": ["Yulun Zhang", "Kunpeng Li", "Kai Li", "Bineng Zhong", "Yun Fu"], "TL;DR": "New state-of-the-art framework for image restoration", "pdf": "/pdf/04121d8887f84077f6486db18444e9113aa62a24.pdf", "paperhash": "zhang|residual_nonlocal_attention_networks_for_image_restoration", "_bibtex": "@inproceedings{\nzhang2018residual,\ntitle={Residual Non-local Attention Networks for Image Restoration},\nauthor={Yulun Zhang and Kunpeng Li and Kai Li and Bineng Zhong and Yun Fu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HkeGhoA5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper693/Official_Review", "cdate": 1542234401403, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkeGhoA5FX", "replyto": "HkeGhoA5FX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper693/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335782053, "tmdate": 1552335782053, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper693/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1lVIRTKAX", "original": null, "number": 6, "cdate": 1543261771924, "ddate": null, "tcdate": 1543261771924, "tmdate": 1543261771924, "tddate": null, "forum": "HkeGhoA5FX", "replyto": "BJeBKz333m", "invitation": "ICLR.cc/2019/Conference/-/Paper693/Official_Comment", "content": {"title": "Author response to Reviewer3 (part 3 of 3) ", "comment": "Q3-8: - The proposed RNAN model is trained on a big dataset (800 images with ~2 million pixels each). Are the competing methods trained on datasets of similar size? If not, this could be a major reason for improved performance of RNAN over competing methods. At least in the appendix, RNAN and FFDNet are compared more fairly since they are trained with the same/similar data.\nA3-8: First, for image super-resolution, EDSR and our RNAN used DIV2K 800 images for training. SRMDNF and D-DBPN used DIV2K 800 images and Flickr2K 2650 images for training, much more images than ours. Our ANAN obtains better results, while using similar or smaller training set and much less network parameters than those of EDSR and D-DBPN.\nSecond, for image denoising, demosaicing, and compression artifacts reduction, the compared methods use smaller training size. It\u2019s hard to use their official released code to retrain their models with DIV2K 800 images mainly for two reasons. One is that it\u2019s very hard to preprocess data with their codes for DIV2K training data. Second, some of the compared methods (e.g., MemNet) would need large-memory GPU (e.g., Nvidia P40 with 24G memory to train MemNet) and very long training time (e.g., 5 days to train MemNet). \nHowever, to make fair comparisons, we retrain our RNAN with smaller dataset and show the results in Table 8. As we can see, our RNAN still achieves better results, even using smaller training data (e.g., for denoising, we use BSD400, FFDNet uses BSD400+, which has 5144 more images than BSD400). It should also be noted that we only train our network about 2 hours, being far away from well-trained. While, other compared methods would have to take much longer training time. For example, MemNet trains for about 5 days, almost 60 times longer than ours.\n\nQ3-9: - The qualitative examples in the appendix mostly show close-ups/details of very structured regions (mostly stripy patterns). Please also show some other regions without self-similar structures.\nA3-9: First, our RNAN obtains pretty good results for regions with self-similar structures. This comparison also demonstrates the effectiveness of our proposed residual non-local attention network. Thanks for the suggestions, we further add more qualitative results without self-similar structures in the revised paper.\n\nQ3-10: # Misc\n- Residual non-local attention learning (section 3.3) was not clear to me.\n- The word \"trunk\" is used without definition or explanation.\n- Fig. 2 caption is too short, please expand.\nA3-10: Thanks for pointing them out. We have revised the paper to make it better understandable and easy to follow. The word \u201ctrunk\u201d mainly means main body to extract features, just being distinguished with mask branch. Moreover, we show it in the Fig. 2. We also expand the caption of Fig. 2."}, "signatures": ["ICLR.cc/2019/Conference/Paper693/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper693/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper693/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Residual Non-local Attention Networks for Image Restoration", "abstract": "In this paper, we propose a residual non-local attention network for high-quality image restoration. Without considering the uneven distribution of information in the corrupted images, previous methods are restricted by local convolutional operation and equal treatment of spatial- and channel-wise features. To address this issue, we design local and non-local attention blocks to extract features that capture the long-range dependencies between pixels and pay more attention to the challenging parts. Specifically, we design trunk branch and (non-)local mask branch in each (non-)local attention block. The trunk branch is used to extract hierarchical features. Local and non-local mask branches aim to adaptively rescale these hierarchical features with mixed attentions. The local mask branch concentrates on more local structures with convolutional operations, while non-local attention considers more about long-range dependencies in the whole feature map. Furthermore, we propose residual local and non-local attention learning to train the very deep network, which further enhance the representation ability of the network. Our proposed method can be generalized for various image restoration applications, such as image denoising, demosaicing, compression artifacts reduction, and super-resolution. Experiments demonstrate that our method obtains comparable or better results compared with recently leading methods quantitatively and visually.", "keywords": ["Non-local network", "attention network", "image restoration", "residual learning"], "authorids": ["yulun100@gmail.com", "kunpengli@ece.neu.edu", "li.kai.gml@gmail.com", "bnzhong@hqu.edu.cn", "yunfu@ece.neu.edu"], "authors": ["Yulun Zhang", "Kunpeng Li", "Kai Li", "Bineng Zhong", "Yun Fu"], "TL;DR": "New state-of-the-art framework for image restoration", "pdf": "/pdf/04121d8887f84077f6486db18444e9113aa62a24.pdf", "paperhash": "zhang|residual_nonlocal_attention_networks_for_image_restoration", "_bibtex": "@inproceedings{\nzhang2018residual,\ntitle={Residual Non-local Attention Networks for Image Restoration},\nauthor={Yulun Zhang and Kunpeng Li and Kai Li and Bineng Zhong and Yun Fu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HkeGhoA5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper693/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609047, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkeGhoA5FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper693/Authors", "ICLR.cc/2019/Conference/Paper693/Reviewers", "ICLR.cc/2019/Conference/Paper693/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper693/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper693/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper693/Authors|ICLR.cc/2019/Conference/Paper693/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper693/Reviewers", "ICLR.cc/2019/Conference/Paper693/Authors", "ICLR.cc/2019/Conference/Paper693/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609047}}}, {"id": "H1llV0aYAX", "original": null, "number": 5, "cdate": 1543261736339, "ddate": null, "tcdate": 1543261736339, "tmdate": 1543261736339, "tddate": null, "forum": "HkeGhoA5FX", "replyto": "BJeBKz333m", "invitation": "ICLR.cc/2019/Conference/-/Paper693/Official_Comment", "content": {"title": "Author response to Reviewer3 (part 2 of 3) ", "comment": "Q3-3: # Clarity\nI think the paper is not self-contained enough, since it seems to implicitly assume substantial background knowledge on attention mechanisms in CNNs. \nA3-3: Due to the limited space, we only included key references about attention mechanisms in the previous paper. Thanks for the reviewer\u2019s suggestions, in the revised paper, we add more descriptions about attention mechanisms.\n\nQ3-4: Furthermore, the introduction of the paper identifies three problems with existing CNNs that I don't necessarily fully agree with. None of these supposed problems are backed up by (experimental) evidence.\nA3-4: For the first issue, Zhang et al. [R2] investigated that larger patch size contributes more for image denoising to make better use of receptive field size, especially when the noise level is high. In this paper, we use non-local attention to make full use of all the pixels of the inputs simultaneously. We compared DnCNN in [R2] to show the effectiveness of our method.\nFor the second issue, we provide analyses about previous methods, which didn\u2019t use non-local attention for image restoration and lacked discriminative ability according to the specific noisy content. We also provide visual results to demonstrate our analyses. For example, to denoise the kodim11 in Fig. 4, all the previous methods cannot recover the line above the boat. They take the tiny line as a part of plain sky and just remove it. However, our RNAN could keep the line and remove the noise by distinctively treat the line and sky. \nFor the third issue, previous methods seldomly take the features distinctively in channel-wise or spatial-wise. Namely, they take the feature maps equally, which lacks flexibility in the real cases. Instead, we learn non-local mixed attention to guide the network training and obtain stronger representational ability. We support this claim with the ablation study and comparisons with other methods quantitatively and qualitatively.\n[R2] Zhang, Kai, et al. \"Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising.\" TIP 2017.\n\nQ3-5: I don't think it is sufficient to just show superior results than previous methods. It is also important to disentangle why the results are better. However, the presented ablation experiments are not very illuminating to me.\nA3-5: Please refer to A3-2 for the reasons and analyses why our results are better. On the other hand, the ablation study is used to verify the effects of each proposed component. It also serves as a guidance for us to decide the final network structure.\n\nQ3-6: The attempts at explaining what the novel attention blocks do and why they lead to superior results are very vague to me. Maybe they are understandable in the context of related work, but I found many statements, such as the following, devoid of meaning:\n- \"Without considering the uneven distribution of information in the corrupted images, [...]\"\n- \"However, in this paper, we mainly focus on learning non-local attention to better guide feature extraction in trunk branch.\"\n- \"We only incorporate residual non-local attention block in low-level and high-level feature space. This is mainly because a few non-local modules can well offer non-local ability to the network for image restoration.\"\n- \"The key point in mask branch is how to grasp information of larger scope, namely larger receptive field size, so that it\u2019s possible to obtain more sophisticated attention map.\"\nA3-6: We summarize our main contribution as three-fold and corresponding brief explanations at the end of Introduction. We also try our best to revise the, aiming to make it better understandable to readers.\n\nQ3-7: # Experiments\n- The experimental results are the best part of the paper. However, it would've been nice to include some qualitative results in the main paper.\nA3-7: Due to the limited space, we didn\u2019t include some qualitative results in the main body of the paper. Thanks for the reviewer\u2019s suggestion, we add some qualitative results in the main body of the revised one."}, "signatures": ["ICLR.cc/2019/Conference/Paper693/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper693/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper693/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Residual Non-local Attention Networks for Image Restoration", "abstract": "In this paper, we propose a residual non-local attention network for high-quality image restoration. Without considering the uneven distribution of information in the corrupted images, previous methods are restricted by local convolutional operation and equal treatment of spatial- and channel-wise features. To address this issue, we design local and non-local attention blocks to extract features that capture the long-range dependencies between pixels and pay more attention to the challenging parts. Specifically, we design trunk branch and (non-)local mask branch in each (non-)local attention block. The trunk branch is used to extract hierarchical features. Local and non-local mask branches aim to adaptively rescale these hierarchical features with mixed attentions. The local mask branch concentrates on more local structures with convolutional operations, while non-local attention considers more about long-range dependencies in the whole feature map. Furthermore, we propose residual local and non-local attention learning to train the very deep network, which further enhance the representation ability of the network. Our proposed method can be generalized for various image restoration applications, such as image denoising, demosaicing, compression artifacts reduction, and super-resolution. Experiments demonstrate that our method obtains comparable or better results compared with recently leading methods quantitatively and visually.", "keywords": ["Non-local network", "attention network", "image restoration", "residual learning"], "authorids": ["yulun100@gmail.com", "kunpengli@ece.neu.edu", "li.kai.gml@gmail.com", "bnzhong@hqu.edu.cn", "yunfu@ece.neu.edu"], "authors": ["Yulun Zhang", "Kunpeng Li", "Kai Li", "Bineng Zhong", "Yun Fu"], "TL;DR": "New state-of-the-art framework for image restoration", "pdf": "/pdf/04121d8887f84077f6486db18444e9113aa62a24.pdf", "paperhash": "zhang|residual_nonlocal_attention_networks_for_image_restoration", "_bibtex": "@inproceedings{\nzhang2018residual,\ntitle={Residual Non-local Attention Networks for Image Restoration},\nauthor={Yulun Zhang and Kunpeng Li and Kai Li and Bineng Zhong and Yun Fu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HkeGhoA5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper693/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609047, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkeGhoA5FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper693/Authors", "ICLR.cc/2019/Conference/Paper693/Reviewers", "ICLR.cc/2019/Conference/Paper693/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper693/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper693/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper693/Authors|ICLR.cc/2019/Conference/Paper693/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper693/Reviewers", "ICLR.cc/2019/Conference/Paper693/Authors", "ICLR.cc/2019/Conference/Paper693/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609047}}}, {"id": "SJlreA6FCQ", "original": null, "number": 4, "cdate": 1543261677125, "ddate": null, "tcdate": 1543261677125, "tmdate": 1543261677125, "tddate": null, "forum": "HkeGhoA5FX", "replyto": "BJeBKz333m", "invitation": "ICLR.cc/2019/Conference/-/Paper693/Official_Comment", "content": {"title": "Author response to Reviewer3 (part 1 of 3) ", "comment": "We thank Reviewer3 for his/her valuable comments. We will release the code and pretrained model reproducing the results in the paper soon. Our responses are as follows:\n\nQ3-1: # Results\nThe strongest point of the paper is that the quantitative and qualitative image restoration results appear to be very good, although they seem almost a bit too good.\nA3-1: We mainly show the effectiveness of our idea and don\u2019t pursue higher performance. We were surprising to find that our current model has achieved much better performance than most previous methods in image restoration. Actually, in our later research, we further obtained better results based on the idea in this paper. Anyway, we will release the train/test codes and pretrained models soon, which reproduce the exact results in this paper. \n\nQ3-2: # Novelty\nI'm not sure about the novelty of the paper, but I suspect it to be rather incremental. The paper says \"To the best of our knowledge, this is the first time to consider residual non-local attention for image restoration problems.\" Does that mean non-local attention (in a very similar way) has already been used, just not in a residual fashion? If so, that would not constitute much novelty. I have to admit that I'm not familiar with the related work on attention, but I did not understand *why* the results of the proposed method are supposed to be much better than that of previous work.\nA3-2: Non-local attention was NOT used for image restoration in terms of papers in CVPR/ICCV/ECCV/NIPS/ICML/ICLR. We are the first to investigate non-local attention for image denoising, demosaicing, compression artifact reduction, and super-resolution simultaneously. The reasons why we propose residual non-local attention learning (in Section 3.3 of the main paper) are mainly as follows:\n(1) It is a proper way to incorporate non-local attention into the network and contribute to the image restoration performance.\n(2) It allows us to train very deep networks by preserving more low-level features, being more suitable for image restoration. \n(3) It allows the network to pursue better representational ability. We demonstrate its effectiveness in both the main paper and our response to Reviewer2.\nThe reasons why our proposed method achieves much better results than that of previous works are as follows:\n(1) Our residual non-local attention network is an effective network structure for high-quality image restoration. No matter we use small training data (e.g., Table 8 in main paper) or DIV2K (e.g., Table 6 in the main paper), our method achieves better results than most compared ones. Let\u2019s take image super-resolution as an example, even though some other methods have larger number of network parameters (e.g., EDSR and D-DBPN), our method still achieves better performance.\n(2) Our proposed residual attention learning allows we train very deep network, achieve stronger representation ability. We\u2019re the first to investigate such a deep network for image denoising, demosiacing, and compression artifacts reduction.\n(3) Our proposed method is powerful enough to further take advantage of larger training set (e.g., DIV2K). As we show Table 8 in the main paper, for small training data, we only train our network about 2 hours, being far away from well-trained. While, other compared methods would have to take much longer training time. For example, MemNet (Tai et al., 2017) trains for about 5 days, almost 60 times longer than ours. "}, "signatures": ["ICLR.cc/2019/Conference/Paper693/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper693/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper693/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Residual Non-local Attention Networks for Image Restoration", "abstract": "In this paper, we propose a residual non-local attention network for high-quality image restoration. Without considering the uneven distribution of information in the corrupted images, previous methods are restricted by local convolutional operation and equal treatment of spatial- and channel-wise features. To address this issue, we design local and non-local attention blocks to extract features that capture the long-range dependencies between pixels and pay more attention to the challenging parts. Specifically, we design trunk branch and (non-)local mask branch in each (non-)local attention block. The trunk branch is used to extract hierarchical features. Local and non-local mask branches aim to adaptively rescale these hierarchical features with mixed attentions. The local mask branch concentrates on more local structures with convolutional operations, while non-local attention considers more about long-range dependencies in the whole feature map. Furthermore, we propose residual local and non-local attention learning to train the very deep network, which further enhance the representation ability of the network. Our proposed method can be generalized for various image restoration applications, such as image denoising, demosaicing, compression artifacts reduction, and super-resolution. Experiments demonstrate that our method obtains comparable or better results compared with recently leading methods quantitatively and visually.", "keywords": ["Non-local network", "attention network", "image restoration", "residual learning"], "authorids": ["yulun100@gmail.com", "kunpengli@ece.neu.edu", "li.kai.gml@gmail.com", "bnzhong@hqu.edu.cn", "yunfu@ece.neu.edu"], "authors": ["Yulun Zhang", "Kunpeng Li", "Kai Li", "Bineng Zhong", "Yun Fu"], "TL;DR": "New state-of-the-art framework for image restoration", "pdf": "/pdf/04121d8887f84077f6486db18444e9113aa62a24.pdf", "paperhash": "zhang|residual_nonlocal_attention_networks_for_image_restoration", "_bibtex": "@inproceedings{\nzhang2018residual,\ntitle={Residual Non-local Attention Networks for Image Restoration},\nauthor={Yulun Zhang and Kunpeng Li and Kai Li and Bineng Zhong and Yun Fu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HkeGhoA5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper693/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609047, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkeGhoA5FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper693/Authors", "ICLR.cc/2019/Conference/Paper693/Reviewers", "ICLR.cc/2019/Conference/Paper693/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper693/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper693/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper693/Authors|ICLR.cc/2019/Conference/Paper693/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper693/Reviewers", "ICLR.cc/2019/Conference/Paper693/Authors", "ICLR.cc/2019/Conference/Paper693/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609047}}}, {"id": "BylyOpaKRX", "original": null, "number": 3, "cdate": 1543261543019, "ddate": null, "tcdate": 1543261543019, "tmdate": 1543261584011, "tddate": null, "forum": "HkeGhoA5FX", "replyto": "BJx1ow5K2X", "invitation": "ICLR.cc/2019/Conference/-/Paper693/Official_Comment", "content": {"title": "Author response to Reviewer1 (part 2 of 2)", "comment": "Q1-3: - The contribution of the non-local operation is not clear to me. For example, how does the global information (i.e., long-range dependencies between pixels) help to solve image denoising tasks such as image denoising?\nA1-3: Zhang et al. [R2] investigated that larger patch size contributes more for image denoising to make better use of receptive field size, especially when the noise level is high. Similar observation could also be found in image super-resolution [R3]. Although large patch size makes better use of larger receptive field size, previous methods are restricted by local convolutional operation and equal treatment of spatial and channel-wise features. \nIn this paper, we use non-local attention to make full use of all the pixels of the inputs simultaneously. Namely, all the positions are considered to obtain better attention maps. Such non-local mixed attention enhances the network with distinguished power for noise and image content. For example, to denoise the kodim11 in Fig. 4, all the previous methods cannot recover the line above the boat. They take the tiny line as a part of plain sky and just remove it. However, our RNAN could keep the line and remove the noise by distinctively treat the line and sky with non-local mixed attention.\n[R2] Zhang, Kai, et al. \"Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising.\" TIP 2017.\n[R3] Wang, Xintao, et al. \"ESRGAN: Enhanced super-resolution generative adversarial networks.\" ECCVW  2018.\n\nQ1-4: Overall, the technical contribution of the proposed method is not so high, but the proposed method is valuable and promising if we focus on the performance.\nA1-4: As Reviewer2 said \u2018However, up to some point all the new ConvNet designs can be seen as incremental developments of the older ones, yet they are needed for the progress of the field.\u2019, we have to admit that too many CNN based works focus on performance. What\u2019s more, some works by famous companies need hundreds and thousands of high-performance GPUs, use tons of data, and take tens of days to train their networks. Although they achieve very impressive results based on existing network structures, researchers (e.g., students in most universities) without so much resource cannot even run their released codes. Such works consumes so much resource that it becomes undoable for researchers with limited resources. However, such kinds of works are not challenged or blamed with their \u2018novelty\u2019 very much and there tends to be more and more such very-large-resource-consuming works.\nIn contrast, in this work, we design a compact yet effective network for image restoration. We conduct extensive experiments to demonstrate the positive contributions of each component and the effectiveness of the idea. We are the first to investigate non-local attention in image restoration tasks. Although we can make more complex network structures to achieve more \u2018novelty\u2019 and better performance, we didn\u2019t. In fact, in our later works, we obtained much better results based on the idea in this paper. \nWe want to inspire other researchers to investigate more about non-local attention for the large community, image restoration, with limited resource. All the experiments can be done with one regular GPU (e.g., 12G memory). The results are also reproducible, as we will release the train/test codes and pretrained models. "}, "signatures": ["ICLR.cc/2019/Conference/Paper693/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper693/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper693/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Residual Non-local Attention Networks for Image Restoration", "abstract": "In this paper, we propose a residual non-local attention network for high-quality image restoration. Without considering the uneven distribution of information in the corrupted images, previous methods are restricted by local convolutional operation and equal treatment of spatial- and channel-wise features. To address this issue, we design local and non-local attention blocks to extract features that capture the long-range dependencies between pixels and pay more attention to the challenging parts. Specifically, we design trunk branch and (non-)local mask branch in each (non-)local attention block. The trunk branch is used to extract hierarchical features. Local and non-local mask branches aim to adaptively rescale these hierarchical features with mixed attentions. The local mask branch concentrates on more local structures with convolutional operations, while non-local attention considers more about long-range dependencies in the whole feature map. Furthermore, we propose residual local and non-local attention learning to train the very deep network, which further enhance the representation ability of the network. Our proposed method can be generalized for various image restoration applications, such as image denoising, demosaicing, compression artifacts reduction, and super-resolution. Experiments demonstrate that our method obtains comparable or better results compared with recently leading methods quantitatively and visually.", "keywords": ["Non-local network", "attention network", "image restoration", "residual learning"], "authorids": ["yulun100@gmail.com", "kunpengli@ece.neu.edu", "li.kai.gml@gmail.com", "bnzhong@hqu.edu.cn", "yunfu@ece.neu.edu"], "authors": ["Yulun Zhang", "Kunpeng Li", "Kai Li", "Bineng Zhong", "Yun Fu"], "TL;DR": "New state-of-the-art framework for image restoration", "pdf": "/pdf/04121d8887f84077f6486db18444e9113aa62a24.pdf", "paperhash": "zhang|residual_nonlocal_attention_networks_for_image_restoration", "_bibtex": "@inproceedings{\nzhang2018residual,\ntitle={Residual Non-local Attention Networks for Image Restoration},\nauthor={Yulun Zhang and Kunpeng Li and Kai Li and Bineng Zhong and Yun Fu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HkeGhoA5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper693/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609047, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkeGhoA5FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper693/Authors", "ICLR.cc/2019/Conference/Paper693/Reviewers", "ICLR.cc/2019/Conference/Paper693/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper693/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper693/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper693/Authors|ICLR.cc/2019/Conference/Paper693/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper693/Reviewers", "ICLR.cc/2019/Conference/Paper693/Authors", "ICLR.cc/2019/Conference/Paper693/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609047}}}, {"id": "B1gPVaaYAQ", "original": null, "number": 2, "cdate": 1543261487485, "ddate": null, "tcdate": 1543261487485, "tmdate": 1543261509066, "tddate": null, "forum": "HkeGhoA5FX", "replyto": "BJx1ow5K2X", "invitation": "ICLR.cc/2019/Conference/-/Paper693/Official_Comment", "content": {"title": "Author response to Reviewer1 (part 1 of 2)", "comment": "We thank Reviewer1 for his/her valuable comments. We will release the code and pretrained model reproducing the results in the paper soon. Our responses are as follows:\n\nQ1-1: - Cons\n  - It would be better to provide the state-of-the-art method[1] in the super-resolution task. \n    [1] Y. Zhang et al., Image Super-Resolution Using Very Deep Residual Channel Attention Networks, ECCV, 2018.\nA1-1: Thanks for the suggestion. RCAN [1] is very powerful and shows great performance gains over previous SR methods. We include the RCAN [1] for comparison in the revised paper. It should be noted that RCAN mainly focus on much deeper network design and channel attention. Our network depth is much shallower than that of RCAN. Our RNAN mainly focus on investigating residual non-local attention and its application for image restoration. We believe that our RNAN could also contribute to RCAN to obtain better performance.\n\nQ1-2: - The technical contribution of the proposed method is not high, because the proposed method seems to be just using existing methods.\nA1-2: Our main principle of network design is to make it \u2018Compact yet work\u2019. This work mainly focuses on investigating the usage of residual local and non-local attention for image restoration. Based on some existing concepts (e.g., residual block, non-local network), we conduct extensive experiments to obtain such a compact network structure and demonstrate its effectiveness. We mainly show the effectiveness of our idea and don\u2019t pursue higher performance by further refining the network modules. We believe that more and more related works could be done to further improve such a compact network."}, "signatures": ["ICLR.cc/2019/Conference/Paper693/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper693/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper693/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Residual Non-local Attention Networks for Image Restoration", "abstract": "In this paper, we propose a residual non-local attention network for high-quality image restoration. Without considering the uneven distribution of information in the corrupted images, previous methods are restricted by local convolutional operation and equal treatment of spatial- and channel-wise features. To address this issue, we design local and non-local attention blocks to extract features that capture the long-range dependencies between pixels and pay more attention to the challenging parts. Specifically, we design trunk branch and (non-)local mask branch in each (non-)local attention block. The trunk branch is used to extract hierarchical features. Local and non-local mask branches aim to adaptively rescale these hierarchical features with mixed attentions. The local mask branch concentrates on more local structures with convolutional operations, while non-local attention considers more about long-range dependencies in the whole feature map. Furthermore, we propose residual local and non-local attention learning to train the very deep network, which further enhance the representation ability of the network. Our proposed method can be generalized for various image restoration applications, such as image denoising, demosaicing, compression artifacts reduction, and super-resolution. Experiments demonstrate that our method obtains comparable or better results compared with recently leading methods quantitatively and visually.", "keywords": ["Non-local network", "attention network", "image restoration", "residual learning"], "authorids": ["yulun100@gmail.com", "kunpengli@ece.neu.edu", "li.kai.gml@gmail.com", "bnzhong@hqu.edu.cn", "yunfu@ece.neu.edu"], "authors": ["Yulun Zhang", "Kunpeng Li", "Kai Li", "Bineng Zhong", "Yun Fu"], "TL;DR": "New state-of-the-art framework for image restoration", "pdf": "/pdf/04121d8887f84077f6486db18444e9113aa62a24.pdf", "paperhash": "zhang|residual_nonlocal_attention_networks_for_image_restoration", "_bibtex": "@inproceedings{\nzhang2018residual,\ntitle={Residual Non-local Attention Networks for Image Restoration},\nauthor={Yulun Zhang and Kunpeng Li and Kai Li and Bineng Zhong and Yun Fu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HkeGhoA5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper693/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609047, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkeGhoA5FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper693/Authors", "ICLR.cc/2019/Conference/Paper693/Reviewers", "ICLR.cc/2019/Conference/Paper693/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper693/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper693/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper693/Authors|ICLR.cc/2019/Conference/Paper693/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper693/Reviewers", "ICLR.cc/2019/Conference/Paper693/Authors", "ICLR.cc/2019/Conference/Paper693/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609047}}}, {"id": "Bygp3i6YA7", "original": null, "number": 1, "cdate": 1543261108909, "ddate": null, "tcdate": 1543261108909, "tmdate": 1543261108909, "tddate": null, "forum": "HkeGhoA5FX", "replyto": "B1lKz511pQ", "invitation": "ICLR.cc/2019/Conference/-/Paper693/Official_Comment", "content": {"title": "Author response to Reviewer2", "comment": "We thank Reviewer2 for his/her valuable comments and approval for our work. We will release the code and pretrained model reproducing the results in the paper soon. Our responses are as follows:\n\nQ2-1: The main weakness of the paper is the limited novelty, as the proposed design builds upon existing ideas and concepts. However, up to some point all the new ConvNet designs can be seen as incremental developments of the older ones, yet they are needed for the progress of the field.\nA2-1: Our main principle of network design is to make it \u2018Compact yet work\u2019. This work mainly focuses on investigating the usage of residual local and non-local attention for image restoration. Based on some existing concepts (e.g., residual block, non-local network), we conduct extensive experiments to obtain such a compact network structure and demonstrate its effectiveness. We mainly show the effectiveness of our idea and don\u2019t pursue higher performance by further refining the network modules. We believe that more and more related works could be done to further improve such a compact network.\n\nQ2-2: Inclusion of more related works, such as: \nTimofte et al., \"NTIRE 2018 Challenge on Single Image Super-Resolution: Methods and Results\", CVPRW 2018\nWang et al., \"A fully progressive approach to single-image super-resolution\", CVPRW 2018\nAgustsson et al., NTIRE 2017 Challenge on Single Image Super-Resolution: Dataset and Study, CVPRW 2017\nBlau et al., \"2018 PIRM Challenge on Perceptual Image Super-resolution\", ECCVW 2018\nZhang et al., \"Image Super-Resolution Using Very Deep Residual Channel Attention Networks\", ECCV 2018\nA2-2: The NTIRE and PIRM challenges and recent related works really contribute to the image restoration community very much. We have included those valuable works and given corresponding analyses in the revised paper.\n\nQ2-3: Why not using dilated convolutions instead of or complementary with the mask branch or other design choices from this paper?\nA2-3: First of all, we investigated the usage of dilated convolutions in mask branch before and found that it didn\u2019t make obvious difference. Dilated convolution may be a good choice to obtain spatial attention, as done in BAM [R1]. While, in this paper, we target to obtain non-local mixed attention, including channel and spatial attention simultaneously.\n[R1] Park, Jongchan, et al. \"BAM: bottleneck attention module.\" BMVC 2018.\nFurthermore, we provide more experiments using dilated convolutions in mask branch to demonstrate our above claims. Here we give a brief introduction to the experiments. As dilated convolutions are good at obtaining larger receptive field size, we remove all the non-local blocks in our network. We divide the experiments as 4 cases.\nCase-1: we replace the mask branch with two dilated convolutions and remove our proposed residual attention learning (in Section 3.3 of the main paper) strategy. Namely, we use Eq. (7) for attention learning.\nCase-2: we replace the mask branch with two dilated convolutions and keep our proposed residual attention learning (in Section 3.3 of the main paper) strategy. Namely, we use Eq. (8) for attention learning.\nCase-3: we add two dilated convolutions in the previous mask branch and remove our proposed residual attention learning (in Section 3.3 of the main paper) strategy. Namely, we use Eq. (7) for attention learning.\nCase-4: we add two dilated convolutions in the previous mask branch and keep our proposed residual attention learning (in Section 3.3 of the main paper) strategy. Namely, we use Eq. (8) for attention learning.\nWe test the performance on Set5 for color image denoising with noise level=30. To save training time, we set path size as 48, block number as 7. The performance comparisons (in terms of PSNR (dB) within 200 epochs) are as follows:\nCase-1: 31.486 dB; Case-2: 31.508 dB; Case-3: 31.535 dB; Case-4: 31.552; RNAN: 31.602 dB. \nCompare Case-1 and -2, or Case-3 and -4, we can see that our proposed residual attention learning is more suitable for image restoration and contributes to the performance.\nCompare Case-2 and RNAN, we find that mix attention works better than simple spatial attention.\nCompare Case-4 and RNAN, we find that non-local block helps to learn better attention by taking long-range dependencies between pixels than that with dilated convolutions."}, "signatures": ["ICLR.cc/2019/Conference/Paper693/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper693/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper693/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Residual Non-local Attention Networks for Image Restoration", "abstract": "In this paper, we propose a residual non-local attention network for high-quality image restoration. Without considering the uneven distribution of information in the corrupted images, previous methods are restricted by local convolutional operation and equal treatment of spatial- and channel-wise features. To address this issue, we design local and non-local attention blocks to extract features that capture the long-range dependencies between pixels and pay more attention to the challenging parts. Specifically, we design trunk branch and (non-)local mask branch in each (non-)local attention block. The trunk branch is used to extract hierarchical features. Local and non-local mask branches aim to adaptively rescale these hierarchical features with mixed attentions. The local mask branch concentrates on more local structures with convolutional operations, while non-local attention considers more about long-range dependencies in the whole feature map. Furthermore, we propose residual local and non-local attention learning to train the very deep network, which further enhance the representation ability of the network. Our proposed method can be generalized for various image restoration applications, such as image denoising, demosaicing, compression artifacts reduction, and super-resolution. Experiments demonstrate that our method obtains comparable or better results compared with recently leading methods quantitatively and visually.", "keywords": ["Non-local network", "attention network", "image restoration", "residual learning"], "authorids": ["yulun100@gmail.com", "kunpengli@ece.neu.edu", "li.kai.gml@gmail.com", "bnzhong@hqu.edu.cn", "yunfu@ece.neu.edu"], "authors": ["Yulun Zhang", "Kunpeng Li", "Kai Li", "Bineng Zhong", "Yun Fu"], "TL;DR": "New state-of-the-art framework for image restoration", "pdf": "/pdf/04121d8887f84077f6486db18444e9113aa62a24.pdf", "paperhash": "zhang|residual_nonlocal_attention_networks_for_image_restoration", "_bibtex": "@inproceedings{\nzhang2018residual,\ntitle={Residual Non-local Attention Networks for Image Restoration},\nauthor={Yulun Zhang and Kunpeng Li and Kai Li and Bineng Zhong and Yun Fu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HkeGhoA5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper693/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609047, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkeGhoA5FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper693/Authors", "ICLR.cc/2019/Conference/Paper693/Reviewers", "ICLR.cc/2019/Conference/Paper693/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper693/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper693/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper693/Authors|ICLR.cc/2019/Conference/Paper693/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper693/Reviewers", "ICLR.cc/2019/Conference/Paper693/Authors", "ICLR.cc/2019/Conference/Paper693/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609047}}}, {"id": "B1lKz511pQ", "original": null, "number": 3, "cdate": 1541499409350, "ddate": null, "tcdate": 1541499409350, "tmdate": 1541533769586, "tddate": null, "forum": "HkeGhoA5FX", "replyto": "HkeGhoA5FX", "invitation": "ICLR.cc/2019/Conference/-/Paper693/Official_Review", "content": {"title": "excellent application oriented paper; new state-of-the-art results; yet limited novelty", "review": "The authors propose a residual non-local attention net (RNAN) which combines local and non-local blocks to form a deep CNN architecture with application to image restoration.\n\nThe paper has a compact description, provides sufficient details, and including the appendix has an excellent experimental validation.\n\nThe proposed approach provides top results on several image restoration tasks:  image denoising, demosaicing, compression artifacts reduction, and single image super-resolution.\n\nThe main weakness of the paper is the limited novelty, as the proposed design builds upon existing ideas and concepts. However, up to some point all the new ConvNet designs can be seen as incremental developments of the older ones, yet they are needed for the progress of the field.\n\nI would suggest to the authors the inclusion of related works such as:\nTimofte et al., \"NTIRE 2018 Challenge on Single Image Super-Resolution: Methods and Results\", CVPRW 2018\nWang et al., \"A fully progressive approach to single-image super-resolution\", CVPRW 2018\nNote that DIV2K dataset was introduced in:\nAgustsson et al., NTIRE 2017 Challenge on Single Image Super-Resolution: Dataset and Study, CVPRW 2017\n\nalso, the more recent related works:\nBlau et al., \"2018 PIRM Challenge on Perceptual Image Super-resolution\", ECCVW 2018\nZhang et al., \"Image Super-Resolution Using Very Deep Residual Channel Attention Networks\", ECCV 2018\n\nAlso, I would like a response from the authors on the following:\nWhy not using dilated convolutions instead of or complementary with the mask branch or other design choices from this paper?\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper693/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Residual Non-local Attention Networks for Image Restoration", "abstract": "In this paper, we propose a residual non-local attention network for high-quality image restoration. Without considering the uneven distribution of information in the corrupted images, previous methods are restricted by local convolutional operation and equal treatment of spatial- and channel-wise features. To address this issue, we design local and non-local attention blocks to extract features that capture the long-range dependencies between pixels and pay more attention to the challenging parts. Specifically, we design trunk branch and (non-)local mask branch in each (non-)local attention block. The trunk branch is used to extract hierarchical features. Local and non-local mask branches aim to adaptively rescale these hierarchical features with mixed attentions. The local mask branch concentrates on more local structures with convolutional operations, while non-local attention considers more about long-range dependencies in the whole feature map. Furthermore, we propose residual local and non-local attention learning to train the very deep network, which further enhance the representation ability of the network. Our proposed method can be generalized for various image restoration applications, such as image denoising, demosaicing, compression artifacts reduction, and super-resolution. Experiments demonstrate that our method obtains comparable or better results compared with recently leading methods quantitatively and visually.", "keywords": ["Non-local network", "attention network", "image restoration", "residual learning"], "authorids": ["yulun100@gmail.com", "kunpengli@ece.neu.edu", "li.kai.gml@gmail.com", "bnzhong@hqu.edu.cn", "yunfu@ece.neu.edu"], "authors": ["Yulun Zhang", "Kunpeng Li", "Kai Li", "Bineng Zhong", "Yun Fu"], "TL;DR": "New state-of-the-art framework for image restoration", "pdf": "/pdf/04121d8887f84077f6486db18444e9113aa62a24.pdf", "paperhash": "zhang|residual_nonlocal_attention_networks_for_image_restoration", "_bibtex": "@inproceedings{\nzhang2018residual,\ntitle={Residual Non-local Attention Networks for Image Restoration},\nauthor={Yulun Zhang and Kunpeng Li and Kai Li and Bineng Zhong and Yun Fu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HkeGhoA5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper693/Official_Review", "cdate": 1542234401403, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkeGhoA5FX", "replyto": "HkeGhoA5FX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper693/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335782053, "tmdate": 1552335782053, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper693/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJx1ow5K2X", "original": null, "number": 1, "cdate": 1541150614804, "ddate": null, "tcdate": 1541150614804, "tmdate": 1541533769135, "tddate": null, "forum": "HkeGhoA5FX", "replyto": "HkeGhoA5FX", "invitation": "ICLR.cc/2019/Conference/-/Paper693/Official_Review", "content": {"title": "Technical contribution is not high, but good performing approach on several image restoration tasks", "review": "- Summary\nThis paper proposes a residual non-local attention network for image restoration. Specifically, the proposed method has local and non-local attention blocks to extract features which capture long-range dependencies. The local and non-local blocks consist of trunk branch and (non-) local mask branch. The proposed method is evaluated on image denoising, demosaicing, compression artifacts reduction, and super-resolution.\n\n- Pros\n  - The proposed method shows better performance than existing image restoration methods.\n  - The effect of each proposed technique such as the mask branch and the non-local block is appropriately evaluated.\n\n- Cons\n  - It would be better to provide the state-of-the-art method[1] in the super-resolution task. \n    [1] Y. Zhang et al., Image Super-Resolution Using Very Deep Residual Channel Attention Networks, ECCV, 2018.\n  - The technical contribution of the proposed method is not high, because the proposed method seems to be just using existing methods.\n  - The contribution of the non-local operation is not clear to me. For example, how does the global information (i.e., long-range dependencies between pixels) help to solve image denoising tasks such as image denoising?\n\nOverall, the technical contribution of the proposed method is not so high, but the proposed method is valuable and promising if we focus on the performance.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper693/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Residual Non-local Attention Networks for Image Restoration", "abstract": "In this paper, we propose a residual non-local attention network for high-quality image restoration. Without considering the uneven distribution of information in the corrupted images, previous methods are restricted by local convolutional operation and equal treatment of spatial- and channel-wise features. To address this issue, we design local and non-local attention blocks to extract features that capture the long-range dependencies between pixels and pay more attention to the challenging parts. Specifically, we design trunk branch and (non-)local mask branch in each (non-)local attention block. The trunk branch is used to extract hierarchical features. Local and non-local mask branches aim to adaptively rescale these hierarchical features with mixed attentions. The local mask branch concentrates on more local structures with convolutional operations, while non-local attention considers more about long-range dependencies in the whole feature map. Furthermore, we propose residual local and non-local attention learning to train the very deep network, which further enhance the representation ability of the network. Our proposed method can be generalized for various image restoration applications, such as image denoising, demosaicing, compression artifacts reduction, and super-resolution. Experiments demonstrate that our method obtains comparable or better results compared with recently leading methods quantitatively and visually.", "keywords": ["Non-local network", "attention network", "image restoration", "residual learning"], "authorids": ["yulun100@gmail.com", "kunpengli@ece.neu.edu", "li.kai.gml@gmail.com", "bnzhong@hqu.edu.cn", "yunfu@ece.neu.edu"], "authors": ["Yulun Zhang", "Kunpeng Li", "Kai Li", "Bineng Zhong", "Yun Fu"], "TL;DR": "New state-of-the-art framework for image restoration", "pdf": "/pdf/04121d8887f84077f6486db18444e9113aa62a24.pdf", "paperhash": "zhang|residual_nonlocal_attention_networks_for_image_restoration", "_bibtex": "@inproceedings{\nzhang2018residual,\ntitle={Residual Non-local Attention Networks for Image Restoration},\nauthor={Yulun Zhang and Kunpeng Li and Kai Li and Bineng Zhong and Yun Fu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HkeGhoA5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper693/Official_Review", "cdate": 1542234401403, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkeGhoA5FX", "replyto": "HkeGhoA5FX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper693/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335782053, "tmdate": 1552335782053, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper693/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 11}