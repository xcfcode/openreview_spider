{"notes": [{"id": "rkgZ3oR9FX", "original": "Skle81P7FQ", "number": 684, "cdate": 1538087848817, "ddate": null, "tcdate": 1538087848817, "tmdate": 1545355430514, "tddate": null, "forum": "rkgZ3oR9FX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Learning to Refer to 3D Objects with Natural Language", "abstract": "Human world knowledge is both structured and flexible. When people see an object, they represent it not as a pixel array but as a meaningful arrangement of semantic parts. Moreover, when people refer to an object, they provide descriptions that are not merely true but also relevant in the current context. Here, we combine these two observations in order to learn fine-grained correspondences between language and contextually relevant geometric properties of 3D objects. To do this, we employed an interactive communication task with human participants to construct a large dataset containing natural utterances referring to 3D objects from ShapeNet in a wide variety of contexts. Using this dataset, we developed neural listener and speaker models with strong capacity for generalization. By performing targeted lesions of visual and linguistic input, we discovered that the neural listener depends heavily on part-related words and associates these words correctly with the corresponding geometric properties of objects, suggesting that it has learned task-relevant structure linking the two input modalities. We further show that a neural speaker that is `listener-aware' --- that plans its utterances according to how an imagined listener would interpret its words in context --- produces more discriminative referring expressions than an `listener-unaware' speaker, as measured by human performance in identifying the correct object.", "keywords": ["Referential Language", "3D Objects", "Part-Awareness", "Neural Speakers", "Neural Listeners"], "authorids": ["optas@cs.stanford.edu", "jefan@stanford.edu", "rxdh@stanford.edu", "ngoodman@stanford.edu", "guibas@cs.stanford.edu"], "authors": ["Panos Achlioptas", "Judy E. Fan", "Robert X.D. Hawkins", "Noah D. Goodman", "Leo Guibas"], "TL;DR": "How to build neural-speakers/listeners that learn fine-grained characteristics of 3D objects, from referential language.", "pdf": "/pdf/e5b8e3028243983f16ddd69f526e89a974536558.pdf", "paperhash": "achlioptas|learning_to_refer_to_3d_objects_with_natural_language", "_bibtex": "@misc{\nachlioptas2019learning,\ntitle={Learning to Refer to 3D Objects with Natural Language},\nauthor={Panos Achlioptas and Judy E. Fan and Robert X.D. Hawkins and Noah D. Goodman and Leo Guibas},\nyear={2019},\nurl={https://openreview.net/forum?id=rkgZ3oR9FX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HketMd9EeE", "original": null, "number": 1, "cdate": 1545017360789, "ddate": null, "tcdate": 1545017360789, "tmdate": 1545354485763, "tddate": null, "forum": "rkgZ3oR9FX", "replyto": "rkgZ3oR9FX", "invitation": "ICLR.cc/2019/Conference/-/Paper684/Meta_Review", "content": {"metareview": "Paper develops a dataset and model for learning to refer to 3D objects. Reviewers raised concerns about lack of novelty. Fundamentally, it seems unclear what (if any) the take-away for an ML-audience would be after reading this paper. We encourage the authors to incorporate reviewer feedback and submit a stronger manuscript at a future (perhaps a more applied) venue. ", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "meta-review"}, "signatures": ["ICLR.cc/2019/Conference/Paper684/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper684/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Refer to 3D Objects with Natural Language", "abstract": "Human world knowledge is both structured and flexible. When people see an object, they represent it not as a pixel array but as a meaningful arrangement of semantic parts. Moreover, when people refer to an object, they provide descriptions that are not merely true but also relevant in the current context. Here, we combine these two observations in order to learn fine-grained correspondences between language and contextually relevant geometric properties of 3D objects. To do this, we employed an interactive communication task with human participants to construct a large dataset containing natural utterances referring to 3D objects from ShapeNet in a wide variety of contexts. Using this dataset, we developed neural listener and speaker models with strong capacity for generalization. By performing targeted lesions of visual and linguistic input, we discovered that the neural listener depends heavily on part-related words and associates these words correctly with the corresponding geometric properties of objects, suggesting that it has learned task-relevant structure linking the two input modalities. We further show that a neural speaker that is `listener-aware' --- that plans its utterances according to how an imagined listener would interpret its words in context --- produces more discriminative referring expressions than an `listener-unaware' speaker, as measured by human performance in identifying the correct object.", "keywords": ["Referential Language", "3D Objects", "Part-Awareness", "Neural Speakers", "Neural Listeners"], "authorids": ["optas@cs.stanford.edu", "jefan@stanford.edu", "rxdh@stanford.edu", "ngoodman@stanford.edu", "guibas@cs.stanford.edu"], "authors": ["Panos Achlioptas", "Judy E. Fan", "Robert X.D. Hawkins", "Noah D. Goodman", "Leo Guibas"], "TL;DR": "How to build neural-speakers/listeners that learn fine-grained characteristics of 3D objects, from referential language.", "pdf": "/pdf/e5b8e3028243983f16ddd69f526e89a974536558.pdf", "paperhash": "achlioptas|learning_to_refer_to_3d_objects_with_natural_language", "_bibtex": "@misc{\nachlioptas2019learning,\ntitle={Learning to Refer to 3D Objects with Natural Language},\nauthor={Panos Achlioptas and Judy E. Fan and Robert X.D. Hawkins and Noah D. Goodman and Leo Guibas},\nyear={2019},\nurl={https://openreview.net/forum?id=rkgZ3oR9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper684/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353125444, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkgZ3oR9FX", "replyto": "rkgZ3oR9FX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper684/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper684/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper684/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353125444}}}, {"id": "rkgk-fZ63X", "original": null, "number": 3, "cdate": 1541374454792, "ddate": null, "tcdate": 1541374454792, "tmdate": 1544560849961, "tddate": null, "forum": "rkgZ3oR9FX", "replyto": "rkgZ3oR9FX", "invitation": "ICLR.cc/2019/Conference/-/Paper684/Official_Review", "content": {"title": "Well executed paper, however weak contributions", "review": "Update: I have read author's response (sorry for being super late). The response better indicates and brings out the contributions made in the paper, and in my opinion is a strong application paper. But as before, and in agreement with R1 I still do not see technical novelty in the paper. For an application driven conference, I think this paper will make a great contribution and will have a large impact. I am slightly unsure as to what the impact will be at ICLR. I leave this judgement call to the AC. I won't fight on the paper in either direction.\n\nThe paper studies the problem of how to refer to 3D objects with natural language. It collects a dataset for the same, by setting up a reference game between two people. It then trains speaker and listener models that learn how to describe a shape, and how to identify shapes given a discriminative referring expression. The paper seems to follows state-of-the-art in the design of these models, and investigates different choices for encoding the image / 3D shape, use if attention in the listener, and context and listener aware models.\n\nStrengths:\n1. Overall, I think this is a very well executed paper. It collects a dataset for studying the problem of interest, trains state-of-the-art models for the tasks, and conducts interesting ablations and tests insightful hypothesis.\n\nWeaknesses\n1. I am not sure what is the technical contribution being made in the paper? Contrastive referential expressions have been used to collect datasets for referring to objects in images. Use of listeners and speakers have been used in NLP (Andreas et al.) as well as in vision and language (Fried et al.). Thus, while I like the application, I am not sure if there is any novel contributions being made in the paper.\n\nOverall, this is a well executed paper, however I am not sure about the novelty of contributions made in the paper.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper684/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Learning to Refer to 3D Objects with Natural Language", "abstract": "Human world knowledge is both structured and flexible. When people see an object, they represent it not as a pixel array but as a meaningful arrangement of semantic parts. Moreover, when people refer to an object, they provide descriptions that are not merely true but also relevant in the current context. Here, we combine these two observations in order to learn fine-grained correspondences between language and contextually relevant geometric properties of 3D objects. To do this, we employed an interactive communication task with human participants to construct a large dataset containing natural utterances referring to 3D objects from ShapeNet in a wide variety of contexts. Using this dataset, we developed neural listener and speaker models with strong capacity for generalization. By performing targeted lesions of visual and linguistic input, we discovered that the neural listener depends heavily on part-related words and associates these words correctly with the corresponding geometric properties of objects, suggesting that it has learned task-relevant structure linking the two input modalities. We further show that a neural speaker that is `listener-aware' --- that plans its utterances according to how an imagined listener would interpret its words in context --- produces more discriminative referring expressions than an `listener-unaware' speaker, as measured by human performance in identifying the correct object.", "keywords": ["Referential Language", "3D Objects", "Part-Awareness", "Neural Speakers", "Neural Listeners"], "authorids": ["optas@cs.stanford.edu", "jefan@stanford.edu", "rxdh@stanford.edu", "ngoodman@stanford.edu", "guibas@cs.stanford.edu"], "authors": ["Panos Achlioptas", "Judy E. Fan", "Robert X.D. Hawkins", "Noah D. Goodman", "Leo Guibas"], "TL;DR": "How to build neural-speakers/listeners that learn fine-grained characteristics of 3D objects, from referential language.", "pdf": "/pdf/e5b8e3028243983f16ddd69f526e89a974536558.pdf", "paperhash": "achlioptas|learning_to_refer_to_3d_objects_with_natural_language", "_bibtex": "@misc{\nachlioptas2019learning,\ntitle={Learning to Refer to 3D Objects with Natural Language},\nauthor={Panos Achlioptas and Judy E. Fan and Robert X.D. Hawkins and Noah D. Goodman and Leo Guibas},\nyear={2019},\nurl={https://openreview.net/forum?id=rkgZ3oR9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper684/Official_Review", "cdate": 1542234403550, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rkgZ3oR9FX", "replyto": "rkgZ3oR9FX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper684/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335780004, "tmdate": 1552335780004, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper684/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Ske3XSpdhm", "original": null, "number": 1, "cdate": 1541096740155, "ddate": null, "tcdate": 1541096740155, "tmdate": 1544397606139, "tddate": null, "forum": "rkgZ3oR9FX", "replyto": "rkgZ3oR9FX", "invitation": "ICLR.cc/2019/Conference/-/Paper684/Official_Review", "content": {"title": "Paper review", "review": "#update: I've read the authors comments but unfortunately my main concerns about the contributions and novelty of this work are not answered. As such, I cannot increase my score.\n\n------------------ \n\nThe authors provide a study on learning to refer to 3D objects. The authors collect a dataset of referential expressions and train several models by experimenting with a number of architectural choices.\n\nThis is an interesting study reporting results on the effect that several architectural choices have generating referential expressions. Overall, while I appreciate all the experiments and results, I don't really feel I've learned something from this paper. \n\nFirst and foremost, the paper, from the title already starts to build up expectations about the 3d nature of the study, however this is pretty much ignored at the rest of the paper. I would expect the paper to provide  some results and insights regarding the 3D nature of the dataset and how this affects referential expressions, however, there is no experiment that has used this 3d-ness in any way. Even the representations of the objects are stripped down to essentially 2D (a single-view of a 3D object used to derived VGG features is as 3D as any image dataset used for similar studies, right?). \nMy major question is then: why should all this research take place in a 3D dataset? Is it to validate that research like this is at all possible with 3D objects? \n\nMoreover, all interesting aspects of referential expressions are stripped out since the authors experiment only with this geometric visual property (which has again nothing to do with 3d-ness, you could totally get that out of images). An interesting study would be to have all objects in the same image and have referential expressions that have to do with spatial expressions, something that the depth or a different view of the of the object could play a role.\n\nGiven the fact that there are no technical innovations, I can't vouch for accepting this paper, since there has been quite a lot of research on generating  referential expressions on image datasets (e.g., Kazemzadeh., 2014 and related papers). However, learning to refer to 3D objects is a very interesting topic, and of great importance given the growing interest of training agents in 3D virtual environments, and I would really encourage the authors to embrace the 3d-ness of objects and design studies that highlight the challenges and opportunities that the third dimension brings.\n\n\nKazemzadeh et al.: ReferIt Game: Referring to Objects in Photographs of Natural Scenes", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper684/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Learning to Refer to 3D Objects with Natural Language", "abstract": "Human world knowledge is both structured and flexible. When people see an object, they represent it not as a pixel array but as a meaningful arrangement of semantic parts. Moreover, when people refer to an object, they provide descriptions that are not merely true but also relevant in the current context. Here, we combine these two observations in order to learn fine-grained correspondences between language and contextually relevant geometric properties of 3D objects. To do this, we employed an interactive communication task with human participants to construct a large dataset containing natural utterances referring to 3D objects from ShapeNet in a wide variety of contexts. Using this dataset, we developed neural listener and speaker models with strong capacity for generalization. By performing targeted lesions of visual and linguistic input, we discovered that the neural listener depends heavily on part-related words and associates these words correctly with the corresponding geometric properties of objects, suggesting that it has learned task-relevant structure linking the two input modalities. We further show that a neural speaker that is `listener-aware' --- that plans its utterances according to how an imagined listener would interpret its words in context --- produces more discriminative referring expressions than an `listener-unaware' speaker, as measured by human performance in identifying the correct object.", "keywords": ["Referential Language", "3D Objects", "Part-Awareness", "Neural Speakers", "Neural Listeners"], "authorids": ["optas@cs.stanford.edu", "jefan@stanford.edu", "rxdh@stanford.edu", "ngoodman@stanford.edu", "guibas@cs.stanford.edu"], "authors": ["Panos Achlioptas", "Judy E. Fan", "Robert X.D. Hawkins", "Noah D. Goodman", "Leo Guibas"], "TL;DR": "How to build neural-speakers/listeners that learn fine-grained characteristics of 3D objects, from referential language.", "pdf": "/pdf/e5b8e3028243983f16ddd69f526e89a974536558.pdf", "paperhash": "achlioptas|learning_to_refer_to_3d_objects_with_natural_language", "_bibtex": "@misc{\nachlioptas2019learning,\ntitle={Learning to Refer to 3D Objects with Natural Language},\nauthor={Panos Achlioptas and Judy E. Fan and Robert X.D. Hawkins and Noah D. Goodman and Leo Guibas},\nyear={2019},\nurl={https://openreview.net/forum?id=rkgZ3oR9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper684/Official_Review", "cdate": 1542234403550, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rkgZ3oR9FX", "replyto": "rkgZ3oR9FX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper684/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335780004, "tmdate": 1552335780004, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper684/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BygDtFc9nQ", "original": null, "number": 2, "cdate": 1541216639251, "ddate": null, "tcdate": 1541216639251, "tmdate": 1541533777062, "tddate": null, "forum": "rkgZ3oR9FX", "replyto": "rkgZ3oR9FX", "invitation": "ICLR.cc/2019/Conference/-/Paper684/Official_Review", "content": {"title": "an interesting and creative paper", "review": "The paper investigates how chairs are being described \"in the context of other similar or not-so-similar chairs\", by humans and neural networks. Humans perceive an object's structure, and use it to describe the differences to other objects \"in context\". The authors collected a corresponding \"chairs in context\" corpus, and build models that can describe the \"target\" chair, that can be used to retrieve the described object, and that can create more discriminative language if given information about the listener.\n\nThe paper is well written, in particular the appendix is very informative. The work seems novel in combination with the dataset, and an interesting and well executed study with interesting analysis that is very relevant to both situated natural language understanding and language generation. The \"3D\" aspect is a bit weak, given that all chairs seem to have essentially been pictured in very similar positions.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper684/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Refer to 3D Objects with Natural Language", "abstract": "Human world knowledge is both structured and flexible. When people see an object, they represent it not as a pixel array but as a meaningful arrangement of semantic parts. Moreover, when people refer to an object, they provide descriptions that are not merely true but also relevant in the current context. Here, we combine these two observations in order to learn fine-grained correspondences between language and contextually relevant geometric properties of 3D objects. To do this, we employed an interactive communication task with human participants to construct a large dataset containing natural utterances referring to 3D objects from ShapeNet in a wide variety of contexts. Using this dataset, we developed neural listener and speaker models with strong capacity for generalization. By performing targeted lesions of visual and linguistic input, we discovered that the neural listener depends heavily on part-related words and associates these words correctly with the corresponding geometric properties of objects, suggesting that it has learned task-relevant structure linking the two input modalities. We further show that a neural speaker that is `listener-aware' --- that plans its utterances according to how an imagined listener would interpret its words in context --- produces more discriminative referring expressions than an `listener-unaware' speaker, as measured by human performance in identifying the correct object.", "keywords": ["Referential Language", "3D Objects", "Part-Awareness", "Neural Speakers", "Neural Listeners"], "authorids": ["optas@cs.stanford.edu", "jefan@stanford.edu", "rxdh@stanford.edu", "ngoodman@stanford.edu", "guibas@cs.stanford.edu"], "authors": ["Panos Achlioptas", "Judy E. Fan", "Robert X.D. Hawkins", "Noah D. Goodman", "Leo Guibas"], "TL;DR": "How to build neural-speakers/listeners that learn fine-grained characteristics of 3D objects, from referential language.", "pdf": "/pdf/e5b8e3028243983f16ddd69f526e89a974536558.pdf", "paperhash": "achlioptas|learning_to_refer_to_3d_objects_with_natural_language", "_bibtex": "@misc{\nachlioptas2019learning,\ntitle={Learning to Refer to 3D Objects with Natural Language},\nauthor={Panos Achlioptas and Judy E. Fan and Robert X.D. Hawkins and Noah D. Goodman and Leo Guibas},\nyear={2019},\nurl={https://openreview.net/forum?id=rkgZ3oR9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper684/Official_Review", "cdate": 1542234403550, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rkgZ3oR9FX", "replyto": "rkgZ3oR9FX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper684/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335780004, "tmdate": 1552335780004, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper684/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}