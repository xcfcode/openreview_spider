{"notes": [{"id": "HklkmjKDfB", "original": null, "number": 19, "cdate": 1564085014570, "ddate": null, "tcdate": 1564085014570, "tmdate": 1564085014570, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "S1eK3i09YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Public_Comment", "content": {"comment": "Thank authors for the interesting work. We found some parts of proof of Lemma 3.1 (page 15) to be confusing. Any clarification is greatly appreciated!\n\n(a) Could anybody please elaborate on how to derive 2nd inequality formula from 1st inequality formula in Proof of Lemma 3.1? It writes \"Setting \u03b4\u2032 = n^2 \u03b4 and applying union bound over (i,j) pairs\", but how does the upper bound\n    2 \\sqrt{log(1 / \u03b4')} / \\sqrt{m}\nrelax to\n    4 \\sqrt{log(n / \u03b4)} / \\sqrt{m}\nby setting \"\u03b4' = n^2\"? \n\n(b) Besides, we also find it confusing to derive the 1st inequality by Hoeffding's inequality. The paper writes: with probability 1 - \u03b4', we have\n    |H_{ij}(0) - H_{ij}^\u221e| \u2264 2 \\sqrt{log (1 / \u03b4')} / \\sqrt{m}.\n\nBut using the Hoeffding's inequality (as formulated in Corollary 7 of lecture note [1]), it derives an upper bound to be\n    \\sqrt{log(2 / \u03b4')} / \\sqrt{2m}\ninstead of the\n    2 \\sqrt{log (1 / \u03b4')} / \\sqrt{m}.\n\nAre we wrong anywhere in understanding the proof? Thanks a lot in advance.\n\n\n[1] http://www.stat.cmu.edu/~larry/=stat705/Lecture2.pdf\n", "title": "Proof of Lemma 3.1"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311766111, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1eK3i09YQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311766111}}}, {"id": "S1eK3i09YQ", "original": "BJed6OecYm", "number": 729, "cdate": 1538087856767, "ddate": null, "tcdate": 1538087856767, "tmdate": 1549332089323, "tddate": null, "forum": "S1eK3i09YQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 42, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rJgd1KdCJV", "original": null, "number": 1, "cdate": 1544616160329, "ddate": null, "tcdate": 1544616160329, "tmdate": 1545354500986, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "S1eK3i09YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Meta_Review", "content": {"metareview": "This paper proves that gradient descent with random initialization converges to global minima for a squared loss penalty over a two layer ReLU network and arbitrarily labeled data. The paper has several weakness such as, 1) assuming top layer is fixed, 2) large number of hidden units 'm', 3) analysis is for squared loss. Despite these weaknesses the paper makes a novel contribution to a relatively challenging problem, and is able to show convergence results without strong assumptions on the input data or the model. Reviewers find the results mostly interesting and have some concerns about the \\lambda_0 requirement. I believe the authors have sufficiently addressed this issue in their response and  I suggest acceptance. ", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "ICLR 2019 decision"}, "signatures": ["ICLR.cc/2019/Conference/Paper729/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper729/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353105441, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1eK3i09YQ", "replyto": "S1eK3i09YQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper729/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper729/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper729/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353105441}}}, {"id": "HygVLQBmkV", "original": null, "number": 18, "cdate": 1543881547844, "ddate": null, "tcdate": 1543881547844, "tmdate": 1543881547844, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "ryeydjfWJE", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Public_Comment", "content": {"comment": "Though assuming w fixed and only randomness of w(0), the random event still depends on w. I believe Lemma 3.2 actually proved that Prob[H(w) eigenvalues are lower bounded]>1 -delta, for any fixed w. But what is used in the latter proof seems to be Prob[for any fixed w, H(w) eigenvalues are lower bounded]>1 -delta.\n\nThink the following simple example.  If Z is N(0,1), then E[(Z-1)^2] = 2 and E[(Z+1)^2] = 2. By Markov inequality, Prob[(Z-1)^2 >2/delta]<delta, Prob[(Z+1)^2 >2/delta]<delta, but '(Z-1)^2 >2/delta' and '(Z+1)^2 >2/delta' are certainly different random events.", "title": "Random event independent of t but still depend on choice of w"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311766111, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1eK3i09YQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311766111}}}, {"id": "HJeK5b6f14", "original": null, "number": 27, "cdate": 1543848337021, "ddate": null, "tcdate": 1543848337021, "tmdate": 1543848337021, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "ryeydjfWJE", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Official_Comment", "content": {"title": "Thanks!", "comment": "Thanks for increasing your score! We will fix the typo in our final version."}, "signatures": ["ICLR.cc/2019/Conference/Paper729/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617556, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1eK3i09YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper729/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper729/Authors|ICLR.cc/2019/Conference/Paper729/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617556}}}, {"id": "HygOMNNqhQ", "original": null, "number": 2, "cdate": 1541190671576, "ddate": null, "tcdate": 1541190671576, "tmdate": 1543842000046, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "S1eK3i09YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Official_Review", "content": {"title": "Interesting result on optimization of two-layer network with ReLU activations", "review": "This work considers optimizing a two-layer over-parameterized ReLU network with the squared loss and given a data set with arbitrary labels. It is shown that for a sufficiently large number of hidden neurons (polynomially in number of samples) gradient descent converges to a global minimum with a linear convergence rate. The proof idea is to show that a certain Gram matrix of the data, which depends also on the weights, has a lower bounded minimum eigenvalue throughout the optimization process. Then, it is shown that this property implies convergence of gradient descent.\n\nThis work is very interesting. Proving convergence of gradient descent for over-parameterized networks with ReLU activations and data with arbitrary labels is a major challenge. It is surprising that the authors found a relatively concise proof in the case of two-layer networks. The insight on the connection between the spectral properties of the Gram matrix and convergence of gradient descent is nice and seems to be a very promising technique for future work. One weakness of the result is the extremely large number of hidden neurons that are required to guarantee convergence.\n\nThe paper is clearly written in most parts. The statement of Lemma 3.2 and its application appear to be incorrect as mentioned in the comments. I am convinced by the authors' response and the current proof that it can be fixed by defining an event which is independent of t. Moreover, I think it would be nice to include experiments that corroborate the theoretical findings. Specifically, it would be interesting to see if in practice most of the patterns of ReLUs do not change or if there is some other phenomenon.\n\nAs mentioned in the comments, it would be good to add a discussion on the assumption of non-degeneracy of the H^{infty} matrix and include a proof (or exact reference) which shows under which conditions the minimum eigenvalue is positive.\n\n-------------Revision--------------\n\nI disagree with most of the points that AnonReviewer3 raised (e.g., second layer fixed is not hard, contribution is limited). I do agree that the main weakness is the number of neurons.  However, I think that the result is significant nonetheless. I did not change my original score.\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper729/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Official_Review", "cdate": 1542234389427, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1eK3i09YQ", "replyto": "S1eK3i09YQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper729/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335790246, "tmdate": 1552335790246, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper729/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1es44k-JN", "original": null, "number": 17, "cdate": 1543726130729, "ddate": null, "tcdate": 1543726130729, "tmdate": 1543765671124, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "rJgDnCeICm", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Public_Comment", "content": {"comment": "This paper seems to be one of the most popular papers in ICLR though. It got a lot of attention on social media as well as in academia. The impact of the paper is definitely huge as it's closely correlated with the popularity. ", "title": "One of the best papers in ICLR "}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311766111, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1eK3i09YQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311766111}}}, {"id": "ryeydjfWJE", "original": null, "number": 26, "cdate": 1543740263277, "ddate": null, "tcdate": 1543740263277, "tmdate": 1543740263277, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "B1l4CQpeAm", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Official_Comment", "content": {"title": "I believe the revision addressed my concerns", "comment": "The revised lemma is much clearer than the initial version. \n\nThe proof of Lemma 3.2 only uses the randomness of w_i(0) s, and the result holds for any weight vectors satisfying the distance assumption in the Lemma, including the setting where the weight vectors are random and dependent on w_i(0) s. \n\nTypo: In the first line of Lemma 3.2, 'w_1, ..., w_m' should be 'w_1(0), ..., w_m(0)'. \n\nI have adjusted my score accordingly. "}, "signatures": ["ICLR.cc/2019/Conference/Paper729/AnonReviewer4"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper729/AnonReviewer4", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617556, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1eK3i09YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper729/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper729/Authors|ICLR.cc/2019/Conference/Paper729/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617556}}}, {"id": "BJeygSVrTm", "original": null, "number": 3, "cdate": 1541911783429, "ddate": null, "tcdate": 1541911783429, "tmdate": 1543739331366, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "S1eK3i09YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Official_Review", "content": {"title": "Interesting paper studying gradient descent in over-parameterized simple NNs", "review": "This paper studies one hidden layer neural networks with square loss, where they show that in over-parameterized setting, random initialization + gradient descent gets to zero loss. The results depend on the property of data matrix, but not the output values.\n\nThe high level idea of the proof is quite different from recent papers, and it would be quite interesting to see how powerful this is for deep neural nets, and whether any insights could help practitioners in the future. \n\nSome discussions regarding the results: \n\nI would suggest the authors to be specific about \u2018with high probability\u2019, whether it is 1-c, or 1-n^{-c}. The proof step using Markov\u2019s inequality gives 1-c probability, which is stated as \u2018with high probability\u2019. What about other \u2018high probability\u2019 statements?\n\nIn the statement of Theorem 3.1 and 4.1, please add \u2018i.i.d.\u2019 (independence) for generating w_r s.\n\nThe current statement of Lemma 3.2 is confusing. The authors state that given t, w.h.p. (let\u2019s say 0.9 for now) over initialization, the minimum eigenvalue is lower bounded. This does not imply, for example, that there exists an initialization, such that for 20 different t s, the minimum eigenvalue is lower bounded. The proof uses Markov\u2019s inequality for a single t. Therefore, I am slightly worried about its correctness. I hope the authors could address my concern. \n\nAlso, in the proof of Lemma 3.2, (just to improve the readability,) I would suggest the authors to make it clear that the expectation is taken over the initialization of the weights. \n\nSome typos: \n\n\u2018converges\u2019 -> \u2018converges to\u2019 in the abstract\n\u2018close\u2019 -> \u2018close to\u2019 on page 5\n\u2018a crucial\u2019 -> \u2018a crucial role\u2019 on page 5\nIn the proof of Lemma 3.2, x_0 should be x_i\nwhether using boldface for H_{ij} should be consistent\n'The next lemma shows we show' in page 6\n'Markov inequality' -> \u2018Markov\u2019s inequality\u2019\n\u2018a fixed a neural network architecture\u2019 in page 8\n\nIt is good to see other comments and discussions on this paper. I believe the authors will make a revision and I would be happy to see the new version of the paper and re-evaluate if some of my comments are not correct.  \n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper729/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Official_Review", "cdate": 1542234389427, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1eK3i09YQ", "replyto": "S1eK3i09YQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper729/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335790246, "tmdate": 1552335790246, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper729/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SkxPEuIhCQ", "original": null, "number": 24, "cdate": 1543428142949, "ddate": null, "tcdate": 1543428142949, "tmdate": 1543428142949, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "BkgphZviC7", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Official_Comment", "content": {"title": "Thanks!", "comment": "Thanks for your encouraging comments!"}, "signatures": ["ICLR.cc/2019/Conference/Paper729/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617556, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1eK3i09YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper729/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper729/Authors|ICLR.cc/2019/Conference/Paper729/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617556}}}, {"id": "BkgphZviC7", "original": null, "number": 16, "cdate": 1543365044559, "ddate": null, "tcdate": 1543365044559, "tmdate": 1543365044559, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "S1eK3i09YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Public_Comment", "content": {"comment": "This paper seems to be an interesting and important paper for neural networks theory. It gets rid of the distributional input assumption common in previous works. It also gives a linear convergence rate which could not be made possible solely by landscape analysis. \n\nIn the analysis of this paper, the H^infty matrix appears naturally and seems to reveal a connection between neural networks and kernels. Moreover, I would like to mention that the ideas presented in the current submission have recently been generalized to deal with multi-layer neural networks, which clearly illustrates the potential its proof structure and techniques.", "title": "Interesting paper and recent follow-ups "}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311766111, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1eK3i09YQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311766111}}}, {"id": "B1lTu7zsC7", "original": null, "number": 23, "cdate": 1543345012745, "ddate": null, "tcdate": 1543345012745, "tmdate": 1543345012745, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "r1gzcd-5aX", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Official_Comment", "content": {"title": "Response to Additional Review", "comment": "Thanks for your thorough reading! We will add more discussions on non-differentiability!\n\n1. Proof idea and experiments:\nWe think viewing our proof from a \"noisy\" linear regression perspective is an interesting observation. Indeed, analyzing a hard non-linear problem from a \"linear\" perspective is a common practice in mathematics.\n\nIn our proof, R' < R is a sufficient condition to show most patterns do not change, which we have verified in Figure 1 (b). It is possible that through other types of analysis, one can show most patterns do not change. For Figure 1 (c), we just want to verify that as $m$ becomes larger, the maximum distance becomes smaller.\n\n2. Network size.\nWe have discussed this point many times in the response. \nOur current bound requires m = \\Omega(n^6). In this paper, to present the cleanest proof, we only use the simplest concentration inequalities (Hoeffding and Markov). We do not think this bound is tight, and we believe using more advanced techniques from probability theory, this bound can be tightened. \n\n3. Dependency on lambda_0.\nFirst of all, your example is not valid in our setting. If x=0, y=1, it is not possible that ReLU(w*x) can achieve zero training error. \nFurthermore, it is easy to prove linear convergence for your example because we can just study the Gram matrix defined over other data points, which has a positive lambda_0. \nWe will add a remark about this in our final version. Thanks for pointing out.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper729/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617556, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1eK3i09YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper729/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper729/Authors|ICLR.cc/2019/Conference/Paper729/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617556}}}, {"id": "BylyV4KqRX", "original": null, "number": 21, "cdate": 1543308326821, "ddate": null, "tcdate": 1543308326821, "tmdate": 1543308463249, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "H1llXbvcC7", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Official_Comment", "content": {"title": "Thanks for your comments!", "comment": "Thanks for your clarifications and we are happy to address your concerns.\n\n1.    On H^{\\infty} and \\lambda_0. \nWe have discussed this in length in our Response to Common Questions and Summary of Revisions. In short, because Equation (7) is an equality, at least in the large $m$ regime, $H^{\\infty}$ determines the whole optimization dynamics, and as a consequence, $\\lambda_0$ is the correct complexity measure. See more discussions in Remark 3.1. \n\nWe are not hiding the difficulty because we have identified the correct complexity measure. We believe it is indeed an interesting problem about how the spectrum of $H^{\\infty}$ is related to other assumptions on the training data. We will list this problem in the Discussion section in our final version.\n\nAs a side note, before this paper, even if we allow $m$ to be exponential in $n$, there is no analysis showing that randomly initialized gradient descent can achieve zero training loss.\n\n2.    On discrete time analysis.\nOur discrete time analysis follows closely to the continuous time analysis. Note we analyze $u(k+1) \u2013 u(k)$ which is analog to $du/dt$. Furthermore, in the equation in the middle of page 9, in the third equality, we decompose the loss at the (k+1)-th iteration into several terms. Note the second term just corresponds to $d(\\|y-u(t)\\|_2^2)/dt$ in the proof of Lemma 3.3 and the other terms are perturbations terms due to discretization. We will make the connection between continuous time analysis and discrete time analysis clearer in our final version. Thanks for pointing out!\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper729/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617556, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1eK3i09YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper729/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper729/Authors|ICLR.cc/2019/Conference/Paper729/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617556}}}, {"id": "r1gzcd-5aX", "original": null, "number": 4, "cdate": 1542228105750, "ddate": null, "tcdate": 1542228105750, "tmdate": 1543302294356, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "S1eK3i09YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Official_Review", "content": {"title": "Review", "review": "Additional Review\n\nThis paper did NOT handle the non-differentiability and non-linearity very well. We can see this from the following three perspectives:\n\n1. Proof idea: the proof of this paper is noisy version of the convergence analysis of  a simple convex problem --it treats the contribution of the non-linearity and non-differentiability as bounded noise.\n2. The network size is of order n^6.\n3. Network size requirement is dependent on \\lambda_0. \n\n1.Proof idea: The proof is essentially a noisy version of the convergence analysis of a linear regression problem provided in Appendix (at the end of this updated review). The only difference between linear regression and the problem in this paper is the changing patterns due to the non-linearity of ReLU. However, this paper views the changing patterns as noises compared to those unchanging patterns (e.g., S_i v.s. S_i^\\perpendicular). The key trick is that if the actual trajectory radius (i.e.,the largest deviation from the initial point) R\u2019 is much smaller than the desired trajectory radius R (given by a formula), then along the trajectory, the contribution of non-linearity is just O(n^2 R), which is small compared to the contribution of linearity, i.e., -\\lambda_0 (shown in proof on page 9). \n\nFollowing the above analysis, if the experiment shows that R\u2019 is really small compared to R, then the approach of treating non-linearity as noise is fine. However, it is not the case for the problem studied in the experiments (Sec 5, Fig 1). In figure 1, we can easily see that the maximum distance R\u2019 is O(1), which is far larger than R = c*\\lambda_0/n^2 =10^-6 when n=1k. Therefore, the proof idea used in this paper is fundamentally not able to explain the phenomenon shown in the experiment. In fact, to address this issue, authors need to consider significant contribution of non-linearity, instead of just viewing them as noises. \n\n2. The network size is too large. This paper requires O(n^6) neurons, that is 10^18 neurons for n=1000 samples used in the experiment. The theoretical trick to make R\u2019< R is to note that R\u2019 can be bounded by O(1/sqrt{m}) while R is independent of m, thus picking a sufficiently large m can make R\u2019 very small. In a word, the reason that this paper requires so many neurons because of the inability of properly addressing non-linearity. \n\n3. I found the dependence of the network size on the least eigenvalue funny, although the authors claim this tool is elegant. After authors add Thm 3.1 in the revision, I realize that the dependence on \\lambda_0 might come from the fact that authors do NOT handle the issue of non-differentiability. \n\nLet us see a simple example. Assume I have a dataset with \\lambda_0 = 1. Now I am adding one more data point (x=0_d, y=1) to the dataset. After adding this sample, \\lambda_0 clearly becomes 0. It seems I am just adding a constant 1 to the loss function and the gradient descent can also converge to the global min with a linear convergence rate since the constant does NOT contribution to the gradient. However, it seems the proof does NOT work. This is due to the fact that the \u201cgradient\u201d of the non-differentiable points are NOT well defined. Here is a simple example: h(w)=(y-ReLU(w*x))^2, where x= 0, y =1. By the definition provided in this paper (Eq.4), we can easily see that dh/dw = 1 for any w, even if h(w) = 1 for any w. This means that the constant can provide \u201cfake\u201d gradient information and make  the maximum distance become infinity, (R\u2019=\\inf). Therefore, the whole proof collapses. In fact, changing the gradient definition from I{z>=0} to I{z>0} does not address the issue and we can see this from this example w=g(w)=Relu(w)-Relu(-w) has a zero gradient at w=0. \n\nIn summary, the problem considered in this paper where the size m=O(n^6), maximum distance R\u2019= O(1/n^2) is too easy compared to most problems in practice where m=\\Theta(n), R\u2019=O(1). To address the latter problem, we need a better definition of subgradient and need to analyze the significant contribution of non-linearity and non-differentiability, instead of just viewing them as noises. \n\n=================================Appendix===============================\n\nThe proof basically follows from the convergence analysis of the following linear regression problem (note that u_j is fixed):\n           \\min_{w_1,...,w_m}\\sum_{i=1}^{n}(f(x_i;w_1,...,w_m)-y_i)^2 = L(w_1,...,w_m)\nwhere   f(x;w_1,...,w_m)=1/\\sqrt{m}\\sum_{j=1}^{m} a_j*(w_j^T x)*1{u_j^T x>=0}\n\nGradient Descent Algorithm:\n-Initialization:\n-For each j=1,...,m: a_j ~ U({-1,1}), u_j~N(0, I)\n-Fix a_1,...,a_m, u_1,...,u_m\n-Update:\n-For t = 1,...,T\n    w_j(t+1)  = w_j(t) - \\eta* \\nabla_{w_j}L(w_1,...,w_m) for j=1,..., m.\n\nIn this problem, since a_j and u_j are fixed, then model f is just a linear model w.r.t. w_j\u2019s and the above problem is just a simple linear regression problem. Therefore, it is not difficult to prove the linear convergence rate for the gradient descent for the above problem under some mild assumptions.  Note that in this paper, u_j(t)= w_j(t) and are not fixed in iterations, i.e., patterns can change. \n\n=========================\nFirst, I apologize to the authors and ACs for the late review, since this paper desearves much more time to judge the quality. \n\nSummary: This paper proves that the gradient descent/flow converges to the global optimum with zero training error under the settings (1) the neural network is a heavily over-parameterized ReLU network (i.e., requiting Omega(n^6) neurons); (2) the algorithm update rule \u201cignores\u201d the non-differentiable point; (3) the parameters in the output layer (i.e., a_i\u2019s) are fixed; (4) the data set has some non-degenerate properties and comes from a unit ball. The proof relies on the fact that the Gram matrix is always positive definite on the converging trajectory. \n\nPros: The proof is simple and seems to be correct. The paper is paper is written clearly  and easy to follow. \n\nCons:\n\nThe problem setting considered in this paper does not seem to be difficult enough. The difficulty of analyzing the landscape property of a ReLU network and proving the global convergence of the gradient descent mainly lies in the following three perspective and this paper does not try to tackle any one of them. \n\nFirst, it is very hard to characterize the landscape or the convergence trajectory at/ near the non-differentiable point and this paper fails to touch it. The parameter space is separated into several regions by the hyperplanes and the loss function is differentiable in the interior of each region and non-differentiable on the boundary. I believe the very first question authors need to answer is wether there are critical points on the boundary and why the sub-gradient descent escapes from  any of these points. However, in this paper, authors avoid this problem by defining an update rule used  in practice and this rule does not use the sub-gradient at the non-differentiable point. Thus, it is totally unclear to me wether this global convergence result comes from the fact that this update rule can generally avoid the non-differentiable  points on the boundary or the fact that the landscape is so nice such that there are no critical points on the boundary or the fact that all points on the convergence trajectory is differentiable only in this unique problem.\n\nSecond, the problem is much easier if the loss is not jointly optimized over the parameters in the first and second layer. Having parameters in one layer fixed does not seem to be a big problem at first glance, but then I realize it indeed makes the problem much easier, which can be seen in the following example. If we randomly sample the weight vector w_i from N(0, I) and only optimize  over the parameters in the second layer, then it is straightforward to show the following result.\n\nResult: If \\lambda_\\min(H^\\inf)>0 and m=\\Omega(n\\log n), then with high probability, the loss function L is strongly convex with respect to a=(a_1,\u2026, a_m) and the loss function is zero at the global minimum.\n\nThe above result shows that if we fix the parameters in the first layer and only optimize the parameters in the second layer, it is easy to prove the global convergence with a linear convergence rate. In fact, this result does not require the samples coming from a unit ball and the network size is only slightly over-parameterized. Therefore, if we are allowed to fix the parameters in some layer, how are the result presented in this paper fundamentally different from the above result. \n\nAuthors may say that the loss is not convex with respect to the weights in the first layer even if the second layer is fixed. However, when the second layer is fixed, the loss function is  smooth and convex in each parameter region and some recent works have shown that in this case, the loss function is a weakly global function. This means that the loss function is similar to a convex function except those plateaus and this further indicates that if the initial point is chosen in a strictly convex basin, the gradient descent is able to converge to a global min.  However, the problem becomes far more difficult if the loss is jointly optimized over  all parameters in the first and second layer. This can be easily seen since in each parameter region, the loss is no longer a convex function and this may lead to some high order saddle points such that the gradient descent cannot provably escape. Furthermore, the critical points on the boundary can be much more difficult to characterize for this joint optimization problem. \n\n\nThird, the dataset considered in this paper does not seem to be a fundamental pattern and it seems more like a technical condition required by the proof. It is easy to see that a linearly separable dataset does not necessarily satisfy the conditions that 1) the gram matrix is positive definite and that 2) samples come from the surface of a unit ball. Therefore, I do not understand the reason why we need to analyze this pattern. Clearly, in practice, the data samples is unlikely sampled from a ball surface and it is totally unclear to me why the gram matrix is necessarily positive definite. I understand that some technical assumptions are needed in a theoretical work, but I would like to see more discussions on the dataset, e.g., some necessary conditions on the dataset such that the global convergence is possible.\n\n\nLast, I understand that the over-parameterization assumption is needed. In fact, I expect the network size to be of the order Omega(n*ploylog(n)). I am wondering wether Omega(n^6) is a necessary condition or wether there exists a case such that Theta(n^6) is required. \n\n\nAbove all, I believe this paper is a half-baked paper with some interesting explorations. In summary, it cannot deal with non-differential points, which is considered a major difficulty for analyzing ReLU. In addition, it makes an un-justified assumption on some matrix, it requires too many neurons, and fixed 2nd layer. With so many strong assumptions, and compared to related works like [1], Mei et al., Bach and ..., its contribution is rather limited.\n\n[1] https://arxiv.org/abs/1702.05777\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper729/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Official_Review", "cdate": 1542234389427, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1eK3i09YQ", "replyto": "S1eK3i09YQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper729/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335790246, "tmdate": 1552335790246, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper729/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "H1llXbvcC7", "original": null, "number": 15, "cdate": 1543299352116, "ddate": null, "tcdate": 1543299352116, "tmdate": 1543299459206, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "Skeo1ukt07", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Public_Comment", "content": {"comment": "Thanks so much for your response. I would like to clarify my concerns.\n\n1. I mean the number of hidden nodes m will depend on \\lambda_0.\n\n2. The current paper fails to give an explicit relationship between \\lambda_0 and n, thus the requirement of m may be meaningless. What if this dependence is exponential? The authors should at least prove that the dependence of \\lambda_0 on n is polynomial under some more natural assumptions on data distribution. If this cannot be proved, does it imply that this eigenvalue lower bound assumption hides the major difficulty of this problem?\n\n3. In the current paper, the authors provide (i) continuous time convergence result (ii) discrete time convergence result (iii) discussion on how the proof method for continuous case can be generalized to deep networks. However, the connection between the continuous time analysis and the discrete time analysis is unclear in the current paper. It seems that the current discrete time analysis is not really a discretization of the continuous time proof, and the proof method looks independent of the continuous time analysis. As a result, it is unclear if the current discrete time analysis can provide enough insight on the training of deep networks, especially since the non-smoothness of ReLU activation function is one of the major difficulties.\n", "title": "Further discussion"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311766111, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1eK3i09YQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311766111}}}, {"id": "rJgDnCeICm", "original": null, "number": 14, "cdate": 1543012015272, "ddate": null, "tcdate": 1543012015272, "tmdate": 1543299271719, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "S1eK3i09YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Public_Comment", "content": {"comment": "Although this paper provides a theoretical guarantee for one hidden layer ReLU based neural networks, the proposed analysis seems very limited, and I\u2019m wondering whether this analysis can give us some insights for analyzing deep networks to get meaningful results.\n\nIn detail, the lower bound assumption of H will introduce a quantity \\lambda_0 into the dependence of m. This quantity can be extremely small in the case of deep networks, which gives us meaningless requirement of the number of hidden nodes. Most part of the current paper discuss about the continuous time analysis. However, this kind of analysis can get rid of the smoothness requirement of the loss function, which is one of the biggest challenges for analyzing ReLU based networks. In addition, the discrete time analysis is based on some loss concentration bounds, which may lead to meaningless results for deep networks. \n\nI think the proposed analysis of the current paper looks very limited.", "title": "The impact of the current paper looks very limited"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311766111, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1eK3i09YQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311766111}}}, {"id": "Skeo1ukt07", "original": null, "number": 20, "cdate": 1543202786697, "ddate": null, "tcdate": 1543202786697, "tmdate": 1543202786697, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "rJgDnCeICm", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Official_Comment", "content": {"title": "Response", "comment": "Thanks for your comments. However, we disagree with your comments. \n\nFirst, this paper is only about the training error, so we are confused why you talked about sample complexity.\n\nSecond, you wrote, \u201c\\lambda_0 can be extremely small in the case of deep networks\u201d. However, you did not give any concrete evidence about this claim.\n\nThird, we are confused about why using continuous analysis to gain intuition is a wrong approach. Many previous papers used this approach to analyze convex optimization problems and deep learning optimization problems [1,2,3,4].\n\nFourth, you wrote \u201cthe discrete analysis based on some loss concentration bounds, which may lead to meaningless results for deep networks.\u201d Again, you did not give any concrete evidence on why for deep networks our analysis will be meaningless, and we are confused about what are the \u201closs concentration bounds\u201d you are referring to. \n\n[1] Ashia C Wilson, Benjamin Recht, and Michael I Jordan. A Lyapunov analysis of momentum methods in optimization. arXiv preprint arXiv:1611.02635, 2016.\n[2] Zhang, J., Mokhtari, A., Sra, S., & Jadbabaie, A. (2018). Direct Runge-Kutta discretization achieves acceleration. arXiv preprint arXiv:1805.00521.\n[3] S Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit acceleration by overparameterization. arXiv preprint arXiv:1802.06509, 201\n[4] Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. arXiv preprint arXiv:1806.00900, 2018."}, "signatures": ["ICLR.cc/2019/Conference/Paper729/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617556, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1eK3i09YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper729/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper729/Authors|ICLR.cc/2019/Conference/Paper729/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617556}}}, {"id": "B1l4CQpeAm", "original": null, "number": 15, "cdate": 1542669260023, "ddate": null, "tcdate": 1542669260023, "tmdate": 1542672568811, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "BJeygSVrTm", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Official_Comment", "content": {"title": "Response to Reviewer 4", "comment": "We thank for your careful review. \n\nWe have modified our draft according to your suggestions:\n\u2022    We changed the statement of Lemma 3.2, and now it is independent of t.\n\u2022    We have added more discussions on how to generalize our technique to analyze multiple layers. In the conclusion section, we have described a concrete plan for analysis. \n\u2022    For all theorems and lemmas, we have added failure probability and how the amount of over-parameterization depends on this failure probability. \n\u2022    We have fixed the typos.\n\u2022    We have modified the statement of Theorem 3.1, 4.1 and the proof of Lemma 3.2 according to your suggestions. \n\nRegarding your question on how our insights could help practitioners in the future since we have characterized the convergence rate of gradient descent from the Gram matrix perspective, we believe our insights can inspire practitioners to design faster optimization algorithms from this perspective. \n\nWe kindly ask you to read our revised paper and our response to common questions and re-evaluate your comments. \n\nWe thank the reviewer again and welcome all further comments. \n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper729/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617556, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1eK3i09YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper729/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper729/Authors|ICLR.cc/2019/Conference/Paper729/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617556}}}, {"id": "BkxSoWagAm", "original": null, "number": 11, "cdate": 1542668701144, "ddate": null, "tcdate": 1542668701144, "tmdate": 1542670140782, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "S1eK3i09YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Official_Comment", "content": {"title": "Response to Common Questions and Summary of Revisions", "comment": "Dear reviewers, \n\nWe thank for all your comments. Especially all reviewers agree our proof is simple. Here we address some common questions from reviews and other comments.\n\n1.    H^{\\infty} matrix. \nMany comments asked when the least eigenvalue of H^{\\infty} is strictly positive and what is the intuition of H^{\\infty} matrix. We thank Dougal J Sutherland and Olivier Grisel for providing numerical evidence showing that on real datasets, this quantity is indeed strictly positive. \n\na.    Theoretically, in our revised version, we give a theorem (c.f. Theorem 3.1) which shows if no two inputs are parallel, then the H^{\\infty} is full rank and thus it has a strictly positive eigenvalue. \n\nb.    Here we also want to discuss informally on why we think H^{\\infty} is the fundamental quantity that determines the convergence rate. In Equation (7), the time derivative of the predictions u(t) is EQUAL to -H(t) (y-u(t)), i.e., the dynamics of the predictions is completely determined by H(t). Furthermore, in our analysis, we show if m -> \\infty, H(t) -> H^{\\infty} for all t >0. Therefore, the worst case scenario is that at the beginning y-u(0) is in the span of the eigenvector of H^{\\infty} that corresponds to the least eigenvalue of H^{\\infty}. In this case, y-u(t) will stay in this space and by one-dimensional linear ODE theory, we see that y-u(t) converges to 0 at a rate exp(-\\lambda_0 t). Also, see Remark 3.1.\n\n\n\n2.    Why fixing the output layer and only training the first layer? The analysis will be much harder if one trains both the first and the output layer.\nThis is the concern raised by Reviewer 2 and Reviewer 3. In our original version, we only analyzed the convergence of gradient optimizing the first layer because we believe this problem already demonstrated the main challenge as many previous works tried to understand the same problem, but none of them has a polynomial time convergence guarantee towards zero training loss. For reviewers\u2019 concern:    \n\na.    First, we disagree with Reviewer 3 that analyzing the case that only the first layer is trained is a trivial problem. For the same setting, there are many previous attempts to answer this question, but these results often rely upon strong assumptions on the labels and input distributions or do not imply why randomly initialized first order method can achieve zero training loss. Please see the second paragraph on Page 2 and Section 3 for detailed discussions. \n\nb.    Second, if we fix the first layer and only train the second layer, the learned function is different from the function learned by fixing the second layer and training the first layer. We have added this point in footnote 3.\n\nc.    Lastly, in our revised version, we added a new theorem (c.f. Theorem 3.3) which shows using gradient flow to train both layers jointly, we can still enjoy linear convergence rate towards zero loss. To prove Theorem 3.3, we use the same arguments as we used to prove Theorem 3.1 with slightly more calculations. Therefore, we have shown analyzing the case that both layers are trained is just as hard as analyzing the case where only the first layer is trained. \n\n\n3.    Amount of over-parameterization. \nOur current bound requires m = \\Omega(n^6). In this paper, to present the cleanest proof, we only use the simplest concentration inequalities (Hoeffding and Markov). As we discussed in the conclusion section, we do not think this bound is tight, and we believe using more advanced techniques from probability theory, this bound can be tightened. \n\n\n\n4.    Lemma 3.2. \nWe are sorry about the confusion in the statement in our original version. We have changed the statement, and the new statement is independent of t. \n\n\n\n5.    Extending to more layers. \nWe have added more discussions in the conclusion section on how to extend our analysis to deeper neural networks, including a very concrete plan. In short, for deep neural networks, we can also consider the dynamics of the n predictions, and the dynamics are determined by the summation of H (number of layers) Gram matrices. We conjecture that 1) at the initialization phase as m -> \\infty, the summation converges to a fixed n by n matrix and 2) as m -> \\infty, these matrices do not change by much over iterations. Thus, as long as the least eigenvalue of that fixed matrix is strictly positive and m is large enough, we can still have linear convergence for deep neural networks.\n\n\n\n\nSummary of Revisions:\n1.    We add a new theorem (Theorem 3.1) which shows as long as no two inputs are parallel, H^{\\infty} matrix is non-degenerate.\n2.    We add a new theorem (Theorem 3.3) on the convergence of gradient flow for jointly training both layers.\n3.    We add experimental results to verify our theoretical findings. \n4.    More discussions on how to extend our analysis to more layers and why H^{\\infty} is a fundamental quantity.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper729/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617556, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1eK3i09YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper729/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper729/Authors|ICLR.cc/2019/Conference/Paper729/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617556}}}, {"id": "rygEFV6l0m", "original": null, "number": 17, "cdate": 1542669435774, "ddate": null, "tcdate": 1542669435774, "tmdate": 1542669435774, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "HklcRVD4hm", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Official_Comment", "content": {"title": "Thanks for your experiments", "comment": "Thanks, Olivier. \n\nWe have acknowledged your experiments in our response."}, "signatures": ["ICLR.cc/2019/Conference/Paper729/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617556, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1eK3i09YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper729/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper729/Authors|ICLR.cc/2019/Conference/Paper729/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617556}}}, {"id": "HylJ7E6xAm", "original": null, "number": 16, "cdate": 1542669334789, "ddate": null, "tcdate": 1542669334789, "tmdate": 1542669334789, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "SklA8vtVh7", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Official_Comment", "content": {"title": "Thanks for your suggestion!", "comment": "We thank for your suggestion. We have changed \"non-degenerate data\" to \"no parallel inputs\"."}, "signatures": ["ICLR.cc/2019/Conference/Paper729/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617556, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1eK3i09YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper729/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper729/Authors|ICLR.cc/2019/Conference/Paper729/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617556}}}, {"id": "HJg5_7pe0m", "original": null, "number": 14, "cdate": 1542669170364, "ddate": null, "tcdate": 1542669170364, "tmdate": 1542669170364, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "r1gzcd-5aX", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "Thank for your long review. Unfortunately, we disagree with most of your comments. First, we would like to point out two wrong statements in your review.\n\nFirst, the \u201cresult\u201d you claim is wrong. If the first layer is fixed and m = \\Omega(n \\logn), and only a=(a_1,\u2026, a_m) is being optimized, this is a linear regression problem with respect to a=(a_1,\u2026, a_m). Since m > n, this problem has more features than the number of samples, and the covariance matrix (Hessian) is degenerate. There is no way this problem is a strongly convex one.\n\nSecond, you claimed there exists a linearly separable dataset whose corresponding H^{\\infty} is degenerate. However, we are considering a regression problem whereas linearly separable condition is only a favorable condition for classification problems. We don\u2019t understand what does linearly separable mean for regression.\n\nNow regarding your main complaint that the problem is not difficult enough:\n\n1.    This is not true at all. Reviewer #1 and Reviewer #2 both explicitly agreed this is a challenging/difficult problem and we have devoted a whole paragraph (second paragraph on page 2) and many sentences in Section 2 to describe the difficulty. \n\n\n2.    You complained that we are not analyzing the landscape of this non-differentiable function and we are using the \u201cpractically used update rule instead of subgradient.\u201d We don\u2019t understand the point here. Our primary goal is to understand why practically used rule (gradient descent) can achieve zero training loss. We have stated our goal at the beginning of the abstract and the introduction. For the non-differentiability issue, in the revised version we have cited papers and added discussions in the fourth paragraph of Section 2 on recent progress in dealing with non-differentiability. \n\n\n3.    You claimed fixing one layer and optimizing the other one is a trivial problem. We agree if one fixes the first layer and optimizes the output layer, then this is trivial because this is a convex problem. However, if one fixes the output layer and optimizes the first layer, the problem is significantly harder. You claimed in this case \n\n\u201cthe loss function is a weakly global function. This means that the loss function is similar to a convex function except those plateaus and this further indicates that if the initial point is chosen in a strictly convex basin, the gradient descent is able to converge to a global min. \u201d\n\nWe kindly ask for a reference and why it can imply the global convergence of gradient descent analyzed in our paper. To our knowledge, none of the previous results implies the global convergence of gradient descent in the setting we are analyzing. We have discussed this point in Section 2. Furthermore, we have never heard of the notion \u201cweakly global function\u201d. \n\n4.    You believed that the inputs are generated from a unit sphere is a strong assumption. In our original version, we said making this assumption is only for simplicity. In our revised version, we added more details on this assumption. Please check footnote 7. \n\n5.    For your other concerns, we kindly ask you to read our response to common questions. \n\n\nWe thank the reviewer again. We welcome all further comments!\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper729/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617556, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1eK3i09YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper729/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper729/Authors|ICLR.cc/2019/Conference/Paper729/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617556}}}, {"id": "HJlyYM6gCQ", "original": null, "number": 13, "cdate": 1542668918860, "ddate": null, "tcdate": 1542668918860, "tmdate": 1542668918860, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "H1g-pI4Dhm", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "We thank for your careful and encouraging review.  We believe our revised version has addressed most of your concerns. \n\n1.    We have added discussions on the problem of fixing the first layer and only training the output layer in footnote 3. We believe the learned function is different from the function learned by fixing the output layer and only training the first layer. We would also like to point out that many previous papers considered the same setting but did not rigorously prove the global convergence of gradient descent. \n\n2.    We have added a new theorem (Theorem 3.3) which shows applying gradient flow to optimize all variables still enjoys a linear convergence rate. To prove Theorem 3.3, we use the same arguments as we used to prove Theorem 3.1 with slightly more calculations. Therefore, we have shown analyzing the case that both layers are trained is just as hard as analyzing the case where only the first layer is trained.\n\n3.    We have added a new theorem (Theorem 3.1) which shows as long as no two inputs are parallel, H^{\\infty} is non-degenerate. \n\nWe thank the reviewer again. We welcome all further comments! \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper729/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617556, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1eK3i09YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper729/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper729/Authors|ICLR.cc/2019/Conference/Paper729/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617556}}}, {"id": "rJeQQfalRm", "original": null, "number": 12, "cdate": 1542668827010, "ddate": null, "tcdate": 1542668827010, "tmdate": 1542668827010, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "HygOMNNqhQ", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "We thank for your encouraging review. \nWe have modified our paper according to your suggestions:\n\u2022\tWe fixed lemma 3.2.\n\u2022\tWe added a new theorem (Theorem 3.1) showing the non-degeneracy of H^{\\infty} matrix.\n\u2022\tWe also added some experiments to corroborate our theoretical findings. Indeed, most of the patterns of ReLUs do not change. Furthermore, over-parameterization leads to faster convergence rate.\n\nWe thank the reviewer again. We welcome all further comments!\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper729/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617556, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1eK3i09YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper729/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper729/Authors|ICLR.cc/2019/Conference/Paper729/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617556}}}, {"id": "H1g-pI4Dhm", "original": null, "number": 1, "cdate": 1540994745068, "ddate": null, "tcdate": 1540994745068, "tmdate": 1541533736062, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "S1eK3i09YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Official_Review", "content": {"title": "An elegant proof on convergence of gradient descent for over-parameterized two-layer ReLU neural networks", "review": "This paper studies convergence of gradient descent on a two-layer fully connected ReLU network with binary output and square loss. The main result is that if the number of hidden units is polynomially large in terms of the number of training samples, then under suitable randomly initialization conditions and given that the output weights are fixed, gradient descent necessarily converge to zero training loss.\n\nPros:\nThe paper is presented clearly enough, but I still urge the authors to carefully check for typos and grammatical mistakes as they revise the paper. As far as I have checked, the proofs are correct. The analysis is quite simple and elegant. This is one thing that I really like about this paper compared to previous work. \n\nCons:\nThe current setting and conditions for the main result to hold are quite a bit limited. If one has polynomially large number of neurons (i.e. on the order of n^6 where n is number of training samples) as stated in the paper, then the weights of the hidden layer can be easily chosen so that the outputs of all training samples become linearly independent in the hidden layer (see e.g. [1] for the construction, which requires only n neurons even with weight sharing) , and thus fixing these weights and optimizing for the output weights would lead directly to a convex problem with the same theoretical guarantee. At this point, it would be good to explain why this paper is focusing on the opposite setting, namely fixing the output weights and learning just the hidden layer weights, because it seems that this just makes the problem become more non-trivial compared to the previous case while yielding almost the same results . Either way, this is not the way how practical neural networks are trained as only a subset of the weights are optimized. Thus it's hard to conclude from here why the commonly used GD w.r.t. all variables converges to zero loss as stated in the abstract.\n\nThe condition on the Gram matrix H_infty in Theorem 3.1 seems to be critical. I would like to see the proof that this condition can be fulfilled under certain conditions on the training data.\n\nIn Lemma 3.1, it seems that \"log^2(n/delta)\" should be \"log(n^2/delta)\"? \n\nDespite the above limitations, I think that the analysis in this paper is still interesting (mainly due to its simplicity) from a theoretical perspective. Given the difficulty of the problem, I'm happy to vote for its acceptance.\n\n[1] Optimization landscape and expressivity of deep CNNs", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper729/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Official_Review", "cdate": 1542234389427, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1eK3i09YQ", "replyto": "S1eK3i09YQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper729/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335790246, "tmdate": 1552335790246, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper729/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SklA8vtVh7", "original": null, "number": 11, "cdate": 1540818774309, "ddate": null, "tcdate": 1540818774309, "tmdate": 1540913501553, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "S1eK3i09YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Public_Comment", "content": {"comment": "I found the paper interesting to read (although I did not try to check the mathematical correctness of the results).\n\nOne point could be improved though: several times the text mentions that the main assumption is that \"data is non-degenerate\" without formally defining what is meant by this. The data matrix is not square so the traditional definition of non-degeneracy does not apply here.\n\nWhen reading the theorems, I believe that the informal \"non-degenerate data\" assumption of the main text corresponds to the double assumptions that each input vectors has unit norm and more importantly that the H_inf kernel matrix is full-rank (non-degenerate).\n\nIn practice, this full-rank H_inf kernel assumption is typically not met if there exists duplicated samples in the training set (if there are duplicated samples with different labels, it's not possible to have zero training loss for any model).\n\nI just read in your reply (https://openreview.net/forum?id=S1eK3i09YQ&noteId=SJeNOPOU9Q) that you can prove that this assumption is met as soon as there are no two parallel samples in the training set. But I assume that this is not necessarily a problem if the labels of such parallel samples are the same. Furthermore, since you also assume that all x_i have unit norm, a pair of parallel samples is actually a pair of duplicated samples.\n\nSo to conclude I would suggest editing your text to change the \"non-degenerate data\" phrase to something more specific (such as \"record-wise normed data without duplicated records\" or alternatively \"non-degenerate extended feature matrix\") so as to avoid any confusion.", "title": "non-degenerate data"}, "signatures": ["~Olivier_Grisel1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"], "writers": ["~Olivier_Grisel1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311766111, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1eK3i09YQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311766111}}}, {"id": "S1eU0bxSn7", "original": null, "number": 13, "cdate": 1540846029945, "ddate": null, "tcdate": 1540846029945, "tmdate": 1540846029945, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "S1xpytvfnX", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Public_Comment", "content": {"comment": "Thanks for your reply, but sorry, I couldn't see how it helps to answer my question. Looking forward to the revision though.", "title": "Reply"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311766111, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1eK3i09YQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311766111}}}, {"id": "HklcRVD4hm", "original": null, "number": 10, "cdate": 1540809937826, "ddate": null, "tcdate": 1540809937826, "tmdate": 1540809956966, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "SkxQT2fFcQ", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Public_Comment", "content": {"comment": "Interesting numerical study. I did not know about the analytical relationship between H and the data Gram matrix. I did a more brute-force numerical study of H on a non-random toy dataset (8x8-pixels gray level digits, d=64, n~=1797) and found lambda_0 > 1.3e-2 which is in line with your random data study:\n\nhttps://gist.github.com/ogrisel/1b430b2bf1e83173f6061676c62b9f18", "title": "Another numerical study"}, "signatures": ["~Olivier_Grisel1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"], "writers": ["~Olivier_Grisel1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311766111, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1eK3i09YQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311766111}}}, {"id": "S1xpytvfnX", "original": null, "number": 6, "cdate": 1540679909249, "ddate": null, "tcdate": 1540679909249, "tmdate": 1540679909249, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "Hkgk1YwgnX", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Official_Comment", "content": {"title": "Clarification", "comment": "Thanks for your question.\n\nWe proved that with high probability over initialization, for any weight matrix $W(t)$ that satisfies $w_r(t)$ is close to $w_r(0)$ for all $r \\in [m]$,  the induced Gram matrix $H(t)$ has lower bounded eigenvalue. Here $t$ is just an index relating the weight matrix and the induced Gram matrix. Note there is only one event which is independent of $t$. \n\nWe are sorry about the confusion and we will modify the statement of the lemma to make it more clear in the revised version.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper729/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617556, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1eK3i09YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper729/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper729/Authors|ICLR.cc/2019/Conference/Paper729/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617556}}}, {"id": "Hkgk1YwgnX", "original": null, "number": 9, "cdate": 1540548823051, "ddate": null, "tcdate": 1540548823051, "tmdate": 1540548823051, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "S1eK3i09YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Public_Comment", "content": {"comment": "Thanks for the inspiring work. I found something confusing about the probability part though.\n\nDenote B(t) = the event that at time/iteration t, || w_r(t)-w_r(0)||_2\\leq R for all r happens. Denote C(t) =  the event that at time/iteration t, the smallest eigenvalue of H(t) is at least \\lambda_0/2 happens\n\nThen Lemma 3.2 states that the conditional probability Prob[C(k)|B(k)] is large( > 1- c) when c ~ R*n^2/\\lambda_0 for a fixed k. However, it is unclear whether C(k)|B(k) implies C(k+1)|B(k+1) from the paper. It is possible that Prob[\\cap_k=1^N (C(k)|B(k))] is not high at all, and even could be zero when N approaches infinity.\n\nIn the last few lines of proving the induction hypothesis on page 10,  it uses Lemma 3.2 that C(k)|B(k) holds with high probability over initialization. But if we review the WHOLE process of proof by induction, in k=1,2.. till infinity, we assume different events hold (assume C(k)|B(k) when proving case k+1), and their relationships are unclear. Thus the \"with high probability\" statement seems to be not solid to me. No lower bound on Prob[\\cap_k=1^\\infty (C(k)|B(k))] is proved.\n\nI would really appreciate your answer to this!", "title": "Lemma 3.2 and w.h.p. in proving induction hypothesis"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311766111, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1eK3i09YQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311766111}}}, {"id": "HklKuGwacm", "original": null, "number": 8, "cdate": 1539302000924, "ddate": null, "tcdate": 1539302000924, "tmdate": 1539302018430, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "ryxgnoz357", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Public_Comment", "content": {"comment": "I see now; Lemma 3.2 says that the expected number of total changes is small, not zero. Whoops; thanks.", "title": "Change in activation patterns"}, "signatures": ["~Dougal_J_Sutherland1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"], "writers": ["~Dougal_J_Sutherland1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311766111, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1eK3i09YQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311766111}}}, {"id": "ryxgnoz357", "original": null, "number": 5, "cdate": 1539218344511, "ddate": null, "tcdate": 1539218344511, "tmdate": 1539218344511, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "SkxQT2fFcQ", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Official_Comment", "content": {"title": "Thanks for your comments and the numerical study!", "comment": "Thanks for your comments and the numerical study! They are very inspiring!\n\nFor the analysis:\nYour intuition is basically correct. We want to clarify that our current proof cannot show for continuous time gradient flow there is no activation pattern change. What we can show is the number of pattern changes is small and only incur small perturbation on H. See Lemma 3.2 and its proof.\n\nExtension to deep neural networks:\nYes, it would be very interesting to investigate empirically whether there is only a small amount of pattern changes when training deeper models.\n\nOn lambda_0:\nThanks for your numerical study! We agree it would be very interesting to obtain some bounds on lambda_0 under certain distributional assumptions. "}, "signatures": ["ICLR.cc/2019/Conference/Paper729/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617556, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1eK3i09YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper729/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper729/Authors|ICLR.cc/2019/Conference/Paper729/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617556}}}, {"id": "SkgYjPMn97", "original": null, "number": 4, "cdate": 1539217313221, "ddate": null, "tcdate": 1539217313221, "tmdate": 1539217313221, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "HygpnikO9X", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Official_Comment", "content": {"title": "Thanks for your comments and questions!", "comment": "Thanks for your comments and questions!\n\nAs stated in our paper, the results in the two papers you mentioned do not imply why randomly initialized gradient descent can achieve 0 training loss with arbitrary labels. Furthermore, there are many subtle differences in the assumptions. We will definitely expand our discussions on these two papers in the revised version. \n\n\nDependency on d and n: our bound depends on lambda_0, which is a dataset-dependent quantity. In general, this quantity is related to d,n and the input distribution. \n\n\nOn generalization: in general, population risk bound can be obtained only if there are additional assumptions on the input distribution and labels. It is an interesting direction to extend our analysis to incorporate structures in the input distributions and labels.\n\n\nWhy using uniform random initialization for the second layer:\nThere are two purposes for using this initialization scheme.\nFirst, as already explained by Dougal, $a_r^2 =1$ makes H matrix independent of a_r and in turn, makes our calculation much easier.\nSecond, this initialization makes ||y-u(0)||_2 = O(\\sqrt{n}). If the output layer are all ones, then u(0) is of order \\sqrt{m}  which makes  ||y-u(0)||_2  be of order \\sqrt{mn}. In this case, R' cannot be smaller than R."}, "signatures": ["ICLR.cc/2019/Conference/Paper729/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617556, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1eK3i09YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper729/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper729/Authors|ICLR.cc/2019/Conference/Paper729/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617556}}}, {"id": "SklNmL-nq7", "original": null, "number": 3, "cdate": 1539212827846, "ddate": null, "tcdate": 1539212827846, "tmdate": 1539212827846, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "SyemVAXv9Q", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Official_Comment", "content": {"title": "Reply", "comment": "Thanks for bringing concerns from others! We are happy to answer these concerns. In fact, point 3 and point 4 already resolved some of the issues. \n\n\nTo point 1: Comparison with the universal approximation theorem.\nResponse: The universal approximation theorem only establishes that there exists a wide neural network that can approximate continuous function on compact subsets of $R^d$. It does not imply a wide neural network trained by randomly initialized gradient descent has the same approximation property.\nWe will add more discussions on universal approximation theorem in the revised version.\n\nTo point 2: Is this a convex problem?\nResponse: because of the use of ReLU activation, this is not a convex problem. The l2 loss is convex with respect to the predictions but is not convex with respect to the parameters we are optimizing.\n\nTo point 5,6: Degeneracy of the Gram matrix:\nResponse: This has been addressed in our previous reply."}, "signatures": ["ICLR.cc/2019/Conference/Paper729/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617556, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1eK3i09YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper729/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper729/Authors|ICLR.cc/2019/Conference/Paper729/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617556}}}, {"id": "BkeKdmWn9Q", "original": null, "number": 2, "cdate": 1539212145129, "ddate": null, "tcdate": 1539212145129, "tmdate": 1539212145129, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "r1el9Jqv5X", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Official_Comment", "content": {"title": "Correct!", "comment": "Yes that's the correct formula. Thanks!"}, "signatures": ["ICLR.cc/2019/Conference/Paper729/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617556, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1eK3i09YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper729/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper729/Authors|ICLR.cc/2019/Conference/Paper729/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617556}}}, {"id": "H1gUINXtqX", "original": null, "number": 6, "cdate": 1539023949660, "ddate": null, "tcdate": 1539023949660, "tmdate": 1539091622623, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "HygpnikO9X", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Public_Comment", "content": {"comment": "Not an author and haven't super-carefully checked the proof, but the derivation of (5), at the start of Proof of Theorem 3.1, assumes that a_r^2 = 1. Otherwise H would contain an a_r^2 term multiplying the indicator; if you used a different distribution for a, then everything to do with H is going to depend on that too. That could make things a lot messier....\n\nBut that doesn't prevent you from choosing a_r as some weighted distribution on +-1. In particular, you could pick all of the a_r = 1. The only place I see that affecting the continuous-time proof is the Markov's inequality bound for ||y - u(0)|| at the end, which uses E[a_r] = 0. But if you had some other high-probability bound on ||y - u(0)||, which you could definitely get just based on the distribution of W, it seems that the rest of the proof carries through with possibly a bigger m. But that can't be right \u2013 if all the a_r = 1, f can't output negative values, and nothing else stops any of the y from being negative.... Authors, what am I missing here?", "title": "Comment on output weights"}, "signatures": ["~Dougal_J_Sutherland1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"], "writers": ["~Dougal_J_Sutherland1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311766111, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1eK3i09YQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311766111}}}, {"id": "B1gUqYQK5m", "original": null, "number": 7, "cdate": 1539025293891, "ddate": null, "tcdate": 1539025293891, "tmdate": 1539026999647, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "H1gUINXtqX", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Public_Comment", "content": {"comment": "Thank you Dougal. The assumption on a_r is stated in Theorem 3.1, that a_r \\in {-1, 1} (and hence a_r^2=1). This is a perfectly fine assumption for ReLU given its homogeneity. There is also randomization: a_r ~ Unif({-1, 1}). Somehow the role of randomness of a_r is not transparent in the proof. But I suspect it should be important: suppose that I use a_r = 1 for all r (hence no randomization), and since ReLU is non-negative, with strictly negative labels y, there is no way that the network can find y...\n\nP.S.: I somehow missed the second part of Dougal's reply, which pointed to the same concern.", "title": "Reply"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311766111, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1eK3i09YQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311766111}}}, {"id": "SkxQT2fFcQ", "original": null, "number": 5, "cdate": 1539022010797, "ddate": null, "tcdate": 1539022010797, "tmdate": 1539024115346, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "S1eK3i09YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Public_Comment", "content": {"comment": "We discussed this in our reading group today, and I'd like to relay some of our thoughts to other readers.\n\nThe paper randomly initializes an extremely overparameterized network: m = Omega(n^6 / lambda_0^4), where lambda_0's dependence on n will vary with the dataset, but presumably it decays with n, making the overall rate for m worse than n^6. Then, here's another way to think about the results of the paper; with high probability:\n\n1. There is a global optimum without switching any of the activation patterns, i.e. keeping sign(w_r^T x_i) the same for all i, r. (This isn't directly shown as a separate step in the paper, but it's implied by Theorem 3.1.)\n\n2. Following a continuous-time gradient flow leads you to that global optimum, following a path that \"looks\" strongly convex as you follow it (so you get linear convergence), without ever switching any of the sign(w_r^T x_i), with high probability.\n\n3. Discrete-time gradient descent, for a small enough step size O(lambda_0 / n^2), does basically the same thing. It's allowed to switch some of the activation patterns, but only a few of them, S_i (or maybe S_i^\\perp, depending on if you go by the definition you give or the way you then use it...). Those ones don't affect the loss too much, and we still have convergence.\n\n\nGiven (1), (2) is maybe not super-surprising: the set of W with the same activation patterns is the intersection of m n linear constraints, and within that set, the objective function is a convex QP. Probably lambda_min(H(0)) is related to lambda_min of the quadratic term in the QP objective, though I couldn't immediately show that. Of course, this doesn't show a result as strong as (2)/(3) without additionally showing you don't happen to break the constraints in following the gradient flow, and and it's circular anyway in that it's not obvious how to show (1) other than through the proof via (2) given here.\n\n\nThe applicability of this approach to deeper networks, then, rests on how realistic the extreme overparameterization here is. Is it still the case that you can avoid switching too many activation patterns in training a deeper network? It would be interesting to track that empirically while training a practical deep net. If switching activation patterns is indeed rare, then this type of approach might be very fruitful for studying deeper nets. Even if not, though, this is an elegant solution to the 1-layer setting.\n\n\nOut of curiosity, I also tried to check numerically what the dependence of lambda_0 is on n for a uniform distribution of inputs. It seems like lambda_0 is about n^{-2} for d=2, n^{-1/2} for d = 5, and n^{-1/4} for d = 10 - https://gist.github.com/dougalsutherland/cc7d8b6d740c6c07d3c6081cfb42d191 . If that's correct, then in 2d the required m is Omega(n^14) (!) while in 10d it's only Omega(n^7), and presumably in very high dimensions it becomes omega(n^6). It might be interesting to try to actually bound lambda_0 in terms of n and d to see if these simulations are accurate. (It might very well be that lambda_0 has a different rate for very large n, with \"very large\" depending on d; I only ran up to n about 3,000 because I only wanted to run for a few minutes on my desktop.)", "title": "some thoughts + a bit of a numerical study of lambda_0"}, "signatures": ["~Dougal_J_Sutherland1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"], "writers": ["~Dougal_J_Sutherland1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311766111, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1eK3i09YQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311766111}}}, {"id": "HygpnikO9X", "original": null, "number": 4, "cdate": 1538943924890, "ddate": null, "tcdate": 1538943924890, "tmdate": 1538944269477, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "S1eK3i09YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Public_Comment", "content": {"comment": "I would like to give a comment on the relation of this paper and certain prior works. The paper by Chizat and Bach proves continuous-time gradient flow can converge to optimal population loss, in the limit of infinite number of neurons, under certain conditions (which include sigmoid activation, and ReLU at a formal level). Mei et al. proves that noisy SGD can optimize to near optimal population loss. In fact, Mei et al. provides a quantitative statement, that the continuous-time flow and the discrete-time one are close already when the number of neurons >> the dimension of the input (i.e. m>>d as in the notation of this paper). As such, these works already suggest that first-order methods can work well on neural nets with a single hidden layer (in terms of population loss), requiring m>>d.\n\nThese two works are briefly mentioned in the paper, but I think it is important to clarify the distinction. The paper, whose analytical approach aligns with many other papers, proves that gradient descent can optimize to optimal empirical loss, for the specific case of ReLU activation. The analysis is nice in its simplicity (and length!), and so I believe many will try to study  this type of analysis. The key finding is that when m>>poly(n) (where n is the number of training samples) and when n is large, many things remain close to initialization at all iterations. As such, random initialization works to our advantage.\n\nInterestingly the aforementioned two works require m>>d, whereas here m>>poly(n). There is no contradiction since the former analyzes SGD, and this paper analyzes (full-batch) gradient descent. Yet this difference raises a question of whether there is an analysis to unify the picture. There is also a question of generalization performance, which is resolved in the aforementioned two works but not in this paper.\n\nI must admit that I have not verified the proof, so it remains to see whether the analysis is correct.\n\nAs a clarifying question, is it crucial that the output weight is initialized uniformly random? The role of random initialization for the output weight is not transparent at first glance.", "title": "comments on relation with prior works"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311766111, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1eK3i09YQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311766111}}}, {"id": "r1el9Jqv5X", "original": null, "number": 3, "cdate": 1538920327819, "ddate": null, "tcdate": 1538920327819, "tmdate": 1538943007535, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "S1eK3i09YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Public_Comment", "content": {"comment": "Interesting results! It seems to me that the definition of H^{\\infty}_{ij} in your main theorems could be simplified as (x_i^T x_j) * arccos(- x_i^T x_j) / (2 * pi) -- am I correct?", "title": "possible simplification of H^{\\infty}"}, "signatures": ["~Hongyi_Zhang1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"], "writers": ["~Hongyi_Zhang1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311766111, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1eK3i09YQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311766111}}}, {"id": "SyemVAXv9Q", "original": null, "number": 2, "cdate": 1538895403077, "ddate": null, "tcdate": 1538895403077, "tmdate": 1538895403077, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "SJeNOPOU9Q", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Public_Comment", "content": {"comment": "Thanks for the author(s) reply.\n\nI've just seen some discussions about this paper on another website and here I wanna seen the official reply from the author(s) w.r.t. the following interesting comments, which is also somewhat the concerns of mine. (I simplily repost those discussions)\n\n1. \"One of the mystery in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks.\"\nHardly a mystery, Cybenko's paper back in 1989 pointed out NN with one hidden layer can approximate any continuous high-dimensional surface without higher degree of smoothness assumption nor being convex, optimization methods like gradient descent is but one of the methods can do the job.\n\n2.\"For an m hidden node shallow neural network with ReLU activation and n training data, we show as long as m is large enough and the data is non-degenerate, randomly initialized gradient descent converges a globally optimal solution with a linear convergence rate for the quadratic loss function.\"\nAnother falsehood, the assumption of surface with positive eigen-values i.e. non-degenerate (in theorem 3.1 and 4.1 for example) implies convexity of the solution landscape. When the data is non-convex, there is no guarantee nor proof that the gradient descent or other more powerful optimization methods can always find the global optimal. Non-convexity problems pose similar challenges like NP-hard problems: solutions stuck in local optimum and there is no way in general to convert locally best solution to global optimal.\n\n3.\"Cybenko's paper back in 1989 pointed out NN with one hidden layer can approximate any continuous high-dimensional surface without higher degree of smoothness assumption nor being convex.\"\nCybenko's paper only says that, for a given continuous function and epsilon, there exists a one-hidden-layer sigmoidal NN with less than epsilon maximum error. It says nothing about the learnability of this NN (nor even the number of neurons in it).\n\n4. \"the assumption of surface with positive eigen-values i.e. non-degenerate (in theorem 3.1 and 4.1 for example) implies convexity of the solution landscape.\"\nThe matrix H\u221e is not the \"solution landscape\". It's a function of the data only, not the parameters. It is not the Hessian of the loss function, as you seem to think.\n\n\n5.\"The key assumption is the least eigenvalue of the matrix H\u221e is strictly positive. Interestingly, various properties of this H\u221e matrix has been thoroughly studied in previous work [Xie et al., 2017, Tsuchida et al., 2017]. In general, unless the data is degenerate, the smallest eigenvalue of H\u221e is strictly positive.\"\nFor example, Xie's paper [1] focus most with spherical data, from section 3. Problem setting and preliminaries.\n\n6.\"We will focus on a special class of data distributions where the input x \u2208 Rd is drawn uniformly from the unit sphere, and assume that |y| \u2264 Y . We consider the following hypothesis class.\"\nMoreover, it also stated:\n\"Typically, gradient descent over L(f) is used to learn all the parameters in f, and a solution with small gradient is returned at the end. However, adjusting the bases {wk} leads to a non-convex optimization problem, and there is no theoretical guarantee that gradient descent can find global optima.\"\n\nIt said nothing how common or not a given data set is convex like the current paper claimed. We suspect not, in general. Xie mentioned nothing about such data being degenerate.\n\nNow to Cybenko's paper:\n\"Cybenko's paper only says that, for a given continuous function and epsilon, there exists a one-hidden-layer sigmoidal NN with less than epsilon maximum error. It says nothing about the learnability of this NN (nor even the number of neurons in it).\"\n\nOnce we know the objective function, and expression of functional form, then the number of hidden layer neurons is a matter of engineering as long as we know \"there exists a one-hidden-layer sigmoidal NN with less than epsilon maximum error.\", that's learnability of one hidden-layer NN.\n\nReferences:\n[1]. Xie, Bo, Yingyu Liang, and Le Song. \"Diverse neural network learns true target functions.\" arXiv preprint arXiv:1611.03131 (2016).\n\n ", "title": "Thanks for Your Reply"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311766111, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1eK3i09YQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311766111}}}, {"id": "SJeNOPOU9Q", "original": null, "number": 1, "cdate": 1538848620179, "ddate": null, "tcdate": 1538848620179, "tmdate": 1538848620179, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "HJei63YVq7", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Official_Comment", "content": {"title": "The Gram matrix is not degenerate. The analysis is simple and novel.", "comment": "We thank for your comments and we are happy to answer your concerns.\n\n1) Adding a linear combination of existing features to the data set leads to a degenerate Gram matrix?\nThis is wrong. Every entry in our Gram matrix is not an inner product between two features, but the result of using a non-linear kernel acting on two features. Please check our definition of the Gram matrix (H^{\\infty}) more carefully (c.f. Theorem 3).\n\nFor data augmentation with a linear combination of other samples, here we provide a counterexample. \nWe have two features (1,0), (0,1) and we add a linear combination (1/\\sqrt{2},1/\\sqrt{2}).\nThe Gram matrix is \n[0.5000         0    0.2652; \n0    0.5000    0.2652; \n0.2652    0.2652    0.5000 ] \nwhich is not degenerate.\nIn general, only if the activation is linear, the Gram matrix becomes degenerate after adding a linear combination of existing features. \n\nIn fact, we can easily prove as long as no two features are parallel, H^\\infty is always non-degenerate. We will add the proof in the revised version.\n\n2) Is this a trivial paper?\nSimplicity is not equivalent to triviality. \n\nOur result is simple: we just prove randomly initialized gradient descent achieves zero training loss for over-parameterized neural networks with a linear convergence rate. However, why randomly initialized first order methods can fit all training data is one of the unsolved open problems in neural network research.\n\nFor the same setting (training two-layer ReLU activated neural networks), there are many previous attempts to answer this question but these results often rely upon strong assumptions on the labels and input distributions or do not imply why randomly initialized first order method can achieve zero training loss. Please see the second paragraph on Page 2 and Section 3 for detailed discussions.\n\nFor technical contributions, we do agree our analysis is simple but we think this is actually an advantage because it will be easier to generalize simple arguments instead of involved ones. Our proof does not require heavy calculations and reveals the intrinsic properties of over-parameterized neural networks and random initialization schemes. Please see Analysis Technique Overview paragraph on page 2.\n\nComparing with [2], except that we use the same property that the patterns do not change by much during training, our analysis is completely different from theirs and is significantly simpler and more transparent. We have devoted a whole paragraph in Section 3 discussing the differences with [2].\n\n3) Experiments\nWe would like to emphasize that this is pure theory paper and the theorem we proved (randomly initialized gradient descent achieves zero training loss) is a well known experimental fact in training neural networks. Nevertheless, we are happy to provide some experimental results in the revised version."}, "signatures": ["ICLR.cc/2019/Conference/Paper729/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617556, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1eK3i09YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper729/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper729/Authors|ICLR.cc/2019/Conference/Paper729/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617556}}}, {"id": "HJei63YVq7", "original": null, "number": 1, "cdate": 1538723011182, "ddate": null, "tcdate": 1538723011182, "tmdate": 1538723011182, "tddate": null, "forum": "S1eK3i09YQ", "replyto": "S1eK3i09YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper729/Public_Comment", "content": {"comment": "The analysis in this paper seems technically sound. However, I have questions w.r.t. this paper: is there any experimental result to support the analysis in this paper? The results are quite simple, and I wish the author(s) could add some experimental validations, even a toy one, to support the theoretical results.\n\nBesides, the assumption on the least eigenvalue of the Gram matrix seems somewhat unreasonable, because if we use some data augmentation tricks, such as mix-up [1] (i.e. if there is a training sample that is the linear combination of other samples), the assumption apparently does not hold in this case in the sense that the least eigenvalue of this gram matrix will become zero. However, the adding of one more data seems have little influence on the training procedure.\n\nAnother concern is that the analysis and conclusion in this paper is somewhat trivial. There are not much technical contributions in this paper. The technical part follows closely to this work [2].\n\n\n[1] Zhang, Hongyi, et al. \"mixup: Beyond Empirical Risk Minimization.\" (2018).\n[2] Li, Yuanzhi, and Yingyu Liang. \"Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data.\" arXiv preprint arXiv:1808.01204 (2018).", "title": "Lack of Experimental Results and Unrealistic Assumptions?"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper729/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.\n\nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.", "keywords": ["theory", "non-convex optimization", "overparameterization", "gradient descent"], "authorids": ["ssdu@cs.cmu.edu", "xiyuzhai@mit.edu", "bapoczos@cs.cmu.edu", "aartisingh@cmu.edu"], "authors": ["Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.", "pdf": "/pdf/669dec7fa785f9039a98728ba2f8c5e11da2aa24.pdf", "paperhash": "du|gradient_descent_provably_optimizes_overparameterized_neural_networks", "_bibtex": "@inproceedings{\ndu2018gradient,\ntitle={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},\nauthor={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1eK3i09YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper729/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311766111, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1eK3i09YQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper729/Authors", "ICLR.cc/2019/Conference/Paper729/Reviewers", "ICLR.cc/2019/Conference/Paper729/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311766111}}}], "count": 43}