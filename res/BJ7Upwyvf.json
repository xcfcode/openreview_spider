{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124460781, "tcdate": 1518464330908, "number": 234, "cdate": 1518464330908, "id": "BJ7Upwyvf", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "BJ7Upwyvf", "signatures": ["~shioya_hiroaki2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Extending Robust Adversarial Reinforcement Learning Considering Adaptation and Diversity", "abstract": "We propose two extensions to Robust Adversarial Reinforcement Learning. (Pinto et al., 2017) One is to add a penalty that brings the training domain closer to the test domain to the objective function of the adversarial agent. The other method trains multiple adversarial agents for one protagonist. We conducted experiments with the physical simulator benchmark task. The results show that our method improves performance in the test domain compared to the baseline.", "paperhash": "shioya|extending_robust_adversarial_reinforcement_learning_considering_adaptation_and_diversity", "_bibtex": "@misc{\n  shioya2018extending,\n  title={Extending Robust Adversarial Reinforcement Learning Considering Adaptation and Diversity},\n  author={Hiroaki Shioya and Yusuke Iwasawa and Yutaka Matsuo},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ7Upwyvf}\n}", "authorids": ["shioya@weblab.t.u-tokyo.ac.jp", "iwasawa@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"], "authors": ["Hiroaki Shioya", "Yusuke Iwasawa", "Yutaka Matsuo"], "keywords": [], "pdf": "/pdf/756060f720c78ee2a78b0fa025a88c4b48120c9b.pdf"}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582885907, "tcdate": 1520530455540, "number": 1, "cdate": 1520530455540, "id": "rJkQNe1YG", "invitation": "ICLR.cc/2018/Workshop/-/Paper234/Official_Review", "forum": "BJ7Upwyvf", "replyto": "BJ7Upwyvf", "signatures": ["ICLR.cc/2018/Workshop/Paper234/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper234/AnonReviewer1"], "content": {"title": "review", "rating": "6: Marginally above acceptance threshold", "review": "This paper proposes two extensions on the robust adversarial reinforcement learning algorithm by Pinto et al., (1) for encouraging the adversary to move closer to the test-domain, and (2) using multiple adversaries.\n\nPros:\n- interesting approach to extending RARL motivated by sim-to-real transfer.\n- promising performance of one of the extensions (penalty for difference from test domain)\n\nCons:\n- the paper has lots of typos and grammatical errors.\n- there are two disjoint improvements that are presented and evaluated separately, rather than a unified algorithm.\n- the paper is motivated from the perspective of sim-to-real, but lacks crucial experimental details for determining if it would be useful for this setting. [e.g. how much test-domain data is used?]\n- the results for the second extension (multiple adversaries) does not show a meaningful improvement.\n\nHere are some questions, comments, and suggestions for improvements:\n1. For the experiment in 3.1, how much data from the test domain was used? Qualitatively, how much did the training and testing domain differ? Both of these are crucial for understanding if the evaluation is representative of the challenge of transferring from simulation to the real world.\n2. The comparison to RARL for the first task doesn't seem particularly fair, since the RARL algorithm wasn't designed for this. Is there another method that would provide a more meaningful comparison?\n3. Learning a model T_t would likely be more difficult in tasks with high-dimensional observation spaces. Is there anyway to mitigate this?\n4. In Figure 1, what do the shaded regions represent? It seems like the proposed algorithm is only marginally (and insignificantly) better than RARL for the hopper and is worse for the walker.\n5. The idea of using multiple adversaries has been previously proposed, e.g. by [1].\n6. One of the most common grammatical mistakes was to remove the space before and after parentheses.\n\n[1] https://arxiv.org/abs/1611.01673\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Extending Robust Adversarial Reinforcement Learning Considering Adaptation and Diversity", "abstract": "We propose two extensions to Robust Adversarial Reinforcement Learning. (Pinto et al., 2017) One is to add a penalty that brings the training domain closer to the test domain to the objective function of the adversarial agent. The other method trains multiple adversarial agents for one protagonist. We conducted experiments with the physical simulator benchmark task. The results show that our method improves performance in the test domain compared to the baseline.", "paperhash": "shioya|extending_robust_adversarial_reinforcement_learning_considering_adaptation_and_diversity", "_bibtex": "@misc{\n  shioya2018extending,\n  title={Extending Robust Adversarial Reinforcement Learning Considering Adaptation and Diversity},\n  author={Hiroaki Shioya and Yusuke Iwasawa and Yutaka Matsuo},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ7Upwyvf}\n}", "authorids": ["shioya@weblab.t.u-tokyo.ac.jp", "iwasawa@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"], "authors": ["Hiroaki Shioya", "Yusuke Iwasawa", "Yutaka Matsuo"], "keywords": [], "pdf": "/pdf/756060f720c78ee2a78b0fa025a88c4b48120c9b.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582885675, "id": "ICLR.cc/2018/Workshop/-/Paper234/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper234/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper234/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper234/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper234/AnonReviewer2"], "reply": {"forum": "BJ7Upwyvf", "replyto": "BJ7Upwyvf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper234/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper234/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582885675}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582798036, "tcdate": 1520624905262, "number": 2, "cdate": 1520624905262, "id": "SyWzHwgKz", "invitation": "ICLR.cc/2018/Workshop/-/Paper234/Official_Review", "forum": "BJ7Upwyvf", "replyto": "BJ7Upwyvf", "signatures": ["ICLR.cc/2018/Workshop/Paper234/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper234/AnonReviewer3"], "content": {"title": "A good exploratory work on improving RARL for test performance, but lacks more thorough empirical comparisons", "rating": "5: Marginally below acceptance threshold", "review": "The paper proposes two modifications to robust adversarial reinforcement learning (RARL): (1) add penalty that penalizes deviation from test domain dynamics, (2) multiple adversarial agents. \n\nA simplest way to implement (2) is to use an ensemble of adversaries. \u201cmean\u201d plots (alpha=0) in Figure 1 roughly correspond to this (if ignore KL penalties), and exhibit most stable performance improvements over the baseline RARL. The \u201csoft\u201d method (alpha=1) does not achieve consistent improvements. The improvement of \u201cmean\u201d over the baseline is not significant, given that it requires more memory and computation. It\u2019s helpful to include comparison without KL penalties, which is the closest to direct application of ensemble to RARL adversary. \n\nThe motivation of (1) is sensible. However, this brings additional assumptions that RARL does not include: collecting samples on test domains during training. Those samples are used for fitting the dynamics model in this paper; however, they can be used in other ways as well: directly use them for protagonist policy update along with RARL learning; use the learned model for additional model-based training. \u201cTRPO-target\u201d in Table 1 is only a limited case of these, and to fully justify the advantage of the proposed approach, alternatives are ideally also compared. The surprisingly good results of \u201cAdaptation\u201d are interesting and may be expanded in results discussion, since this method is quite distinct from RARL: it\u2019s effectively trying to learn interventions to training domains that make the dynamics similar to test domains (based on multi-step errors rather than single-step), and thereby training can generalize. [1] can also be seen as a recent example of this, but depending on how interventions are defined, there are novel directions to pursue.       \n\nOverall, it\u2019s slightly weak for acceptance. The main criticism is that given both proposals involve additional requirements (more computation & access to test domains), it\u2019s helpful to include the simplest variants of RARL that involve those as base comparisons (naive ensemble & include both RARL + test policy gradient during training).  \n\nGiven that this training procedure interacts with test domains during learning, it is also possible to frame in few shot learning setups, as in MAML [Finn et. al., 2017], and derive better extensions for RARL there. \n\nMinor points:\n- Writing needs more attention (citation brackets & spacing, use of generic terms, e.g. \u201ca difficult domain\u201d -> \u201cadversarial domains\u201d, Sec 3.2. second paragraph spacings...).  \n\nPros: \n- clear descriptions of proposed modifications\n\nCons:\n- proposed modifications are relatively straightforward and the improvements are not substantial given additional requirements (more computation; access to test domains). \n\n[1] Konstantinos et. al., 2017. \u201cUsing Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping\u201d. \n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Extending Robust Adversarial Reinforcement Learning Considering Adaptation and Diversity", "abstract": "We propose two extensions to Robust Adversarial Reinforcement Learning. (Pinto et al., 2017) One is to add a penalty that brings the training domain closer to the test domain to the objective function of the adversarial agent. The other method trains multiple adversarial agents for one protagonist. We conducted experiments with the physical simulator benchmark task. The results show that our method improves performance in the test domain compared to the baseline.", "paperhash": "shioya|extending_robust_adversarial_reinforcement_learning_considering_adaptation_and_diversity", "_bibtex": "@misc{\n  shioya2018extending,\n  title={Extending Robust Adversarial Reinforcement Learning Considering Adaptation and Diversity},\n  author={Hiroaki Shioya and Yusuke Iwasawa and Yutaka Matsuo},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ7Upwyvf}\n}", "authorids": ["shioya@weblab.t.u-tokyo.ac.jp", "iwasawa@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"], "authors": ["Hiroaki Shioya", "Yusuke Iwasawa", "Yutaka Matsuo"], "keywords": [], "pdf": "/pdf/756060f720c78ee2a78b0fa025a88c4b48120c9b.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582885675, "id": "ICLR.cc/2018/Workshop/-/Paper234/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper234/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper234/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper234/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper234/AnonReviewer2"], "reply": {"forum": "BJ7Upwyvf", "replyto": "BJ7Upwyvf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper234/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper234/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582885675}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582608206, "tcdate": 1520898779851, "number": 3, "cdate": 1520898779851, "id": "H1N1Q54Yz", "invitation": "ICLR.cc/2018/Workshop/-/Paper234/Official_Review", "forum": "BJ7Upwyvf", "replyto": "BJ7Upwyvf", "signatures": ["ICLR.cc/2018/Workshop/Paper234/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper234/AnonReviewer2"], "content": {"title": "Compelling extensions to RARL to regularize the adversary", "rating": "7: Good paper, accept", "review": "The authors' propose a modification of RARL to learn more robust policies by adding two regularizers. The first, to ensure the adversary doesn't deviate to far from expected behavior the and the second is to train multiple diverse adversaries to randomly choose from.  The overall enhancements are intriguing and the existing experiments validate the approach.  \n\nIt appears the thrust of the first approach is to provide a signal to regularize the amount the adversarial action is allowed to shift the next state.  Using a trained transition model has a couple of interesting side-effects that should be explored 1) when uncertain/beginning training, there will be large error and uncertainty in prediction which will penalize adversarial actions and 2) when in a region that the transition function has converged the penalty enforces small perturbations.\n\nThe second regularizer is geared towards training a diverse set of adversaries and then sampling them weighted by how hard they are. Having numerous advaries that are forced to be diverse may help avoid collapse of learning. The details of this approach were unclear, in particular how the regression was performed, so the author's should clarify this approach.  \n\nAs a meta comment, the two regularization methods appear unrelated and may warrant being discussed in two different publications.\n\nGeneral comments:\nPlease mention how \\lambda affects the performance of the method and what parameter worked best. \nThere may be a notational issue as it appears that Algorithm 1 doesn't update/train \\theta^{adv}.\nRARL uses the adversary and protagonist during rollouts.  You might want to clarify that this is the case in Algorithm 1.\nPlease describe your experiment for the 169 test domains in greater detail including the parameters you are changing and the ranges you are sampling.\nPlease provide details for reproducibility", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Extending Robust Adversarial Reinforcement Learning Considering Adaptation and Diversity", "abstract": "We propose two extensions to Robust Adversarial Reinforcement Learning. (Pinto et al., 2017) One is to add a penalty that brings the training domain closer to the test domain to the objective function of the adversarial agent. The other method trains multiple adversarial agents for one protagonist. We conducted experiments with the physical simulator benchmark task. The results show that our method improves performance in the test domain compared to the baseline.", "paperhash": "shioya|extending_robust_adversarial_reinforcement_learning_considering_adaptation_and_diversity", "_bibtex": "@misc{\n  shioya2018extending,\n  title={Extending Robust Adversarial Reinforcement Learning Considering Adaptation and Diversity},\n  author={Hiroaki Shioya and Yusuke Iwasawa and Yutaka Matsuo},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ7Upwyvf}\n}", "authorids": ["shioya@weblab.t.u-tokyo.ac.jp", "iwasawa@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"], "authors": ["Hiroaki Shioya", "Yusuke Iwasawa", "Yutaka Matsuo"], "keywords": [], "pdf": "/pdf/756060f720c78ee2a78b0fa025a88c4b48120c9b.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582885675, "id": "ICLR.cc/2018/Workshop/-/Paper234/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper234/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper234/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper234/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper234/AnonReviewer2"], "reply": {"forum": "BJ7Upwyvf", "replyto": "BJ7Upwyvf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper234/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper234/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582885675}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573571012, "tcdate": 1521573571012, "number": 123, "cdate": 1521573570669, "id": "ryjTAACKf", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "BJ7Upwyvf", "replyto": "BJ7Upwyvf", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Extending Robust Adversarial Reinforcement Learning Considering Adaptation and Diversity", "abstract": "We propose two extensions to Robust Adversarial Reinforcement Learning. (Pinto et al., 2017) One is to add a penalty that brings the training domain closer to the test domain to the objective function of the adversarial agent. The other method trains multiple adversarial agents for one protagonist. We conducted experiments with the physical simulator benchmark task. The results show that our method improves performance in the test domain compared to the baseline.", "paperhash": "shioya|extending_robust_adversarial_reinforcement_learning_considering_adaptation_and_diversity", "_bibtex": "@misc{\n  shioya2018extending,\n  title={Extending Robust Adversarial Reinforcement Learning Considering Adaptation and Diversity},\n  author={Hiroaki Shioya and Yusuke Iwasawa and Yutaka Matsuo},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ7Upwyvf}\n}", "authorids": ["shioya@weblab.t.u-tokyo.ac.jp", "iwasawa@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"], "authors": ["Hiroaki Shioya", "Yusuke Iwasawa", "Yutaka Matsuo"], "keywords": [], "pdf": "/pdf/756060f720c78ee2a78b0fa025a88c4b48120c9b.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 5}