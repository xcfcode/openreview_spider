{"notes": [{"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458853682426, "tcdate": 1458853682426, "id": "q7WjxMJV4T8LEkD3t7v6", "invitation": "ICLR.cc/2016/workshop/-/paper/140/comment", "forum": "lx9l4r36gU2OVPy8Cv9g", "replyto": "lx9l4r36gU2OVPy8Cv9g", "signatures": ["~Sasha_Targ1"], "readers": ["everyone"], "writers": ["~Sasha_Targ1"], "content": {"title": "Updated paper", "comment": "An updated version of the paper is posted here: https://drive.google.com/file/d/0BxX9BAoclX5fNEJTZFIxTU1qZGs/view?usp=sharing"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Resnet in Resnet: Generalizing Residual Architectures", "abstract": "ResNets have recently achieved state-of-the-art results on challenging computer vision tasks. In this paper, we create a novel architecture that improves ResNets by adding the ability to forget and by making the residuals more expressive, yielding excellent results. ResNet in ResNet outperforms architectures with similar amounts of augmentation on CIFAR-10 and establishes a new state-of-the-art on CIFAR-100.", "pdf": "/pdf/lx9l4r36gU2OVPy8Cv9g.pdf", "paperhash": "targ|resnet_in_resnet_generalizing_residual_architectures", "conflicts": ["enlitic.com", "ucsf.edu"], "authors": ["Sasha Targ", "Diogo Almeida", "Kevin Lyman"], "authorids": ["sasha.targ@gmail.com", "diogo@enlitic.com", "kevin@enlitic.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455827249191, "ddate": null, "super": null, "final": null, "tcdate": 1455827249191, "id": "ICLR.cc/2016/workshop/-/paper/140/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "lx9l4r36gU2OVPy8Cv9g", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/140/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458853572190, "tcdate": 1458853572190, "id": "P7q1N6orrHKvjNORtJZK", "invitation": "ICLR.cc/2016/workshop/-/paper/140/comment", "forum": "lx9l4r36gU2OVPy8Cv9g", "replyto": "0YrwQoRW8FGJ7gK5tROk", "signatures": ["~Sasha_Targ1"], "readers": ["everyone"], "writers": ["~Sasha_Targ1"], "content": {"title": "re: Interesting direction, but missing/incorrect information", "comment": "Thanks to the reviewer for these comments. We have updated the paper to incorporate the feedback, which is posted here: https://drive.google.com/file/d/0BxX9BAoclX5fNEJTZFIxTU1qZGs/view?usp=sharing\n\nWe agree RiR maintains the fixed shallow residual subnetwork size of the ResNet architecture. However, our results with varying numbers of layers per residual block show that the RiR architecture allows training of deeper residuals compared to the original ResNet (Figure 4).\n\nA summary of the changes in response to the review follows:\n- We add a related work section with more information describing similarities and differences to existing work (including LSTM, Grid-LSTM, Highway Networks, and SCRNs)\n- We focus the paper with additional experiments on the generalized residual architecture and RiR and remove forget gate experiments\n- We reword the descriptions stating our networks forget information which may be misleading due to the shared name with forget gates. Our intended meaning is that intermediates could be used without propagating them deeper in the network (in ResNets this can only be done for very shallow intermediates within a block)\n\nRiR architectures demonstrate good results at depths of 30+ layers (in the updated version, we include results at 150+ layers), depths at which the original ResNet already shows advantages over standard CNNs. This implies the benefits may be related to both optimization and generalization, which in our opinion are coupled problems."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Resnet in Resnet: Generalizing Residual Architectures", "abstract": "ResNets have recently achieved state-of-the-art results on challenging computer vision tasks. In this paper, we create a novel architecture that improves ResNets by adding the ability to forget and by making the residuals more expressive, yielding excellent results. ResNet in ResNet outperforms architectures with similar amounts of augmentation on CIFAR-10 and establishes a new state-of-the-art on CIFAR-100.", "pdf": "/pdf/lx9l4r36gU2OVPy8Cv9g.pdf", "paperhash": "targ|resnet_in_resnet_generalizing_residual_architectures", "conflicts": ["enlitic.com", "ucsf.edu"], "authors": ["Sasha Targ", "Diogo Almeida", "Kevin Lyman"], "authorids": ["sasha.targ@gmail.com", "diogo@enlitic.com", "kevin@enlitic.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455827249191, "ddate": null, "super": null, "final": null, "tcdate": 1455827249191, "id": "ICLR.cc/2016/workshop/-/paper/140/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "lx9l4r36gU2OVPy8Cv9g", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/140/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458853519578, "tcdate": 1458853519578, "id": "zvgmpqpvgiM8kw3ZinM9", "invitation": "ICLR.cc/2016/workshop/-/paper/140/comment", "forum": "lx9l4r36gU2OVPy8Cv9g", "replyto": "ANYRpXR5lUNrwlgXCqQ0", "signatures": ["~Sasha_Targ1"], "readers": ["everyone"], "writers": ["~Sasha_Targ1"], "content": {"title": "re: Interesting experiments", "comment": "Thanks to the reviewer for these comments. We have updated the paper to incorporate the feedback, which is posted here: https://drive.google.com/file/d/0BxX9BAoclX5fNEJTZFIxTU1qZGs/view?usp=sharing\n\nA summary of the changes in response to the review follows:\n- We now include an appendix and related work section\n- We update Section 2 to improve clarity, and add a section in appendix with more detailed implementation of the generalized residual block \n- We now include information on hyperparameters including initialization used (MSR init)\n- We now include the exact architectures used and the number of parameters in each in the appendix. ResNet Init and RiR require no additional computation/parameters over CNNs and ResNets, respectively. Difference in number of parameters between CNN and ResNet is due to the projection convolutions when increasing dimensionality.\n- We focus the paper with additional experiments on the generalized residual architecture and RiR and remove forget gate experiments"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Resnet in Resnet: Generalizing Residual Architectures", "abstract": "ResNets have recently achieved state-of-the-art results on challenging computer vision tasks. In this paper, we create a novel architecture that improves ResNets by adding the ability to forget and by making the residuals more expressive, yielding excellent results. ResNet in ResNet outperforms architectures with similar amounts of augmentation on CIFAR-10 and establishes a new state-of-the-art on CIFAR-100.", "pdf": "/pdf/lx9l4r36gU2OVPy8Cv9g.pdf", "paperhash": "targ|resnet_in_resnet_generalizing_residual_architectures", "conflicts": ["enlitic.com", "ucsf.edu"], "authors": ["Sasha Targ", "Diogo Almeida", "Kevin Lyman"], "authorids": ["sasha.targ@gmail.com", "diogo@enlitic.com", "kevin@enlitic.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455827249191, "ddate": null, "super": null, "final": null, "tcdate": 1455827249191, "id": "ICLR.cc/2016/workshop/-/paper/140/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "lx9l4r36gU2OVPy8Cv9g", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/140/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457658389491, "tcdate": 1457658389491, "id": "0YrwQoRW8FGJ7gK5tROk", "invitation": "ICLR.cc/2016/workshop/-/paper/140/review/11", "forum": "lx9l4r36gU2OVPy8Cv9g", "replyto": "lx9l4r36gU2OVPy8Cv9g", "signatures": ["ICLR.cc/2016/workshop/paper/140/reviewer/11"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/140/reviewer/11"], "content": {"title": "Interesting direction, but missing/incorrect information", "rating": "5: Marginally below acceptance threshold", "review": "The authors propose a new way to initialize the weights of a deep feedfoward network based on inspiration from residual networks, then apply it for initialization of layers in a residual network with improved results on CIFAR-10/100.\n\nThe basic motivation for this paper is interesting, but as of now there is a lot of missing information and the report feels rather rushed. In particular:\n\nThe abstract is inaccurate with respect to the experiments actually performed in the paper. An architecture with the ability to 'forget' is only mentioned without detail towards the end of the paper with a single experiment. \n\nIntroduction:\n- 'Residuals must be learned by fixed size shallow subnetworks, despite evidence that deeper networks are more expressive'. \nThe proposed RiR architecture can use a shallower subnetwork but not a deeper one compared to ResNet, so it doesn't fully fix this issue.\n\n- \"even though some features learned at earlier layers of a deep network may no longer provide useful information in later layers. A prior of ...\" \nThe mentioned highway networks and its variants (with/without gate coupling etc.) do have the ability to 'forget', and a stack of highway layers can learn subnetworks of various depths. Why is this not mentioned here (perhaps I misunderstood something)?\nAdditionally the highway networks paper mentions successful training of 'unrolled LSTM' for very deep networks, which are also explicitly used by \"Grid-LSTM\". These have forget gates. Additionally, an unrolled/Grid LSTM also has two streams along depth similar to what is proposed in Section 2, so I'm not sure how original this basic motivation is. It's okay if it's not original, but connections to existing work must be clear.\n\n- \"We propose a novel architecture...\"\nInaccurate currently, similar to abstract.\n\nSection 2:\n-From what I can tell, the proposed generalized 2 stream architecture is never actually used. Instead an initialization is used which lets a usual layer implementation behave like the proposed architecture at the beginning of training. This is an incomplete evaluation of the proposal, and makes hard to say how valuable it is.\n\nSection 3/4:\nOverall, apart from relations to highway networks, unrolled LSTM and Grid LSTM, the presented results seem preliminary even for a workshop contribution. This is because while some consistent improvements are shown for the initialization (wide RiR vs. wide ResNet), it's unclear what the reason for this improvement is.\n\nOne would expect the motivation 'problems' mentioned in the Introduction to lead to difficulties in optimization (not necessarily generalization). But I doubt that the improved results obtained are due to better optimization, since it is likely that all networks were optimized well (these networks are not too deep). Is it just a purely empirical observation then, that this initialization appears to result in better generalization? If so, this should also be stated very clearly.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Resnet in Resnet: Generalizing Residual Architectures", "abstract": "ResNets have recently achieved state-of-the-art results on challenging computer vision tasks. In this paper, we create a novel architecture that improves ResNets by adding the ability to forget and by making the residuals more expressive, yielding excellent results. ResNet in ResNet outperforms architectures with similar amounts of augmentation on CIFAR-10 and establishes a new state-of-the-art on CIFAR-100.", "pdf": "/pdf/lx9l4r36gU2OVPy8Cv9g.pdf", "paperhash": "targ|resnet_in_resnet_generalizing_residual_architectures", "conflicts": ["enlitic.com", "ucsf.edu"], "authors": ["Sasha Targ", "Diogo Almeida", "Kevin Lyman"], "authorids": ["sasha.targ@gmail.com", "diogo@enlitic.com", "kevin@enlitic.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580035578, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580035578, "id": "ICLR.cc/2016/workshop/-/paper/140/review/11", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "lx9l4r36gU2OVPy8Cv9g", "replyto": "lx9l4r36gU2OVPy8Cv9g", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/140/reviewer/11", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457647216676, "tcdate": 1457647216676, "id": "ANYRpXR5lUNrwlgXCqQ0", "invitation": "ICLR.cc/2016/workshop/-/paper/140/review/10", "forum": "lx9l4r36gU2OVPy8Cv9g", "replyto": "lx9l4r36gU2OVPy8Cv9g", "signatures": ["ICLR.cc/2016/workshop/paper/140/reviewer/10"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/140/reviewer/10"], "content": {"title": "Interesting experiments", "rating": "6: Marginally above acceptance threshold", "review": "The authors propose an initialization scheme based on some comparisons to the ResNet architecture. They also replace CONV blocks with the proposed ResNetInit CONV blocks to obtained a Resnet in Resnet (RiR). These experiments are needed, the connections made between the models in the paper are interesting.\n\nThat being said, a few recommendations to the authors:\n\n- The discussion of ResNet Init and RiR is not very clear and I did not understand Section 2 well on my first reading. Please also expand more clearly on the differences between the 4 models in terms of the runtime, or number of parameters, etc. How are the parameters W initialized? This is not mentioned in the paper as far as I can tell, which seems like a very important point - are they drawn from gaussian?\n\n- I would encourage the authors to focus their contribution and not add orthogonal half-baked experiments. For example either develop the forget gate ideas and report on them properly or I recommend not including them at all.\n\n- Are the authors certain that only 5 papers from the entire body of scientific literature is relevant to this work?\n\nI am slightly leaning to accept this work for workshop submission, provided that the paper is clarified, the contribution focused, and that the differences between all the architectures are better compared (e.g. FLOPS? or Wall clock time? or Parameters?)", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Resnet in Resnet: Generalizing Residual Architectures", "abstract": "ResNets have recently achieved state-of-the-art results on challenging computer vision tasks. In this paper, we create a novel architecture that improves ResNets by adding the ability to forget and by making the residuals more expressive, yielding excellent results. ResNet in ResNet outperforms architectures with similar amounts of augmentation on CIFAR-10 and establishes a new state-of-the-art on CIFAR-100.", "pdf": "/pdf/lx9l4r36gU2OVPy8Cv9g.pdf", "paperhash": "targ|resnet_in_resnet_generalizing_residual_architectures", "conflicts": ["enlitic.com", "ucsf.edu"], "authors": ["Sasha Targ", "Diogo Almeida", "Kevin Lyman"], "authorids": ["sasha.targ@gmail.com", "diogo@enlitic.com", "kevin@enlitic.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580035950, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580035950, "id": "ICLR.cc/2016/workshop/-/paper/140/review/10", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "lx9l4r36gU2OVPy8Cv9g", "replyto": "lx9l4r36gU2OVPy8Cv9g", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/140/reviewer/10", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457593813932, "tcdate": 1457593813932, "id": "NL6lGOZw3i0VOPA8ixRW", "invitation": "ICLR.cc/2016/workshop/-/paper/140/comment", "forum": "lx9l4r36gU2OVPy8Cv9g", "replyto": "MwVkZ51oZIqxwkg1t7WW", "signatures": ["~Diogo_Almeida1"], "readers": ["everyone"], "writers": ["~Diogo_Almeida1"], "content": {"title": "Re: Interesting idea, and promising experiments, but have flaws", "comment": "We thank the reviewer for the comments on our paper, which bring up some important points we clarify below\n\n> \"since the ResNet Init architecture was questionable (see \u201cCons\u201d), building RiR with real ResNet (not ResNet Init) would be more reasonable.\"\n\nWe also tried the architecture of a ResNet in Resnet architecture using standard ResNets for both the inner and outer connections, and this didn't perform significantly differently from standard ResNets in the architectures we've tried. Because it seemed intuitive that having another set of identity connections would not cause a large difference in behavior, we did not include this result.\n\n> \"no clear evidence was provided on why the ordinary network can be better than ResNet\"\n\nIt is true we did not find clear evidence that the ResNet Init was always better than a standard ResNet (though it should be noted experiments where the ResNet outperformed were on an architecture tuned for the ResNet). Our intention in presenting the ResNet Init was in the appeal of its architecture of an ordinary CNN which can be used in any existing CNN architecture but still realizes performance benefits. In light of this, we feel a fairer comparison is of the ResNet Init to ordinary CNNs. In our results, ResNet Init consistently outperforms equivalent CNN architectures (Tables 1, 2, 3, 5). \n\nWe conduct the experiments reported in Table 5 to show an example in which the ResNet Init is applied to an existing model, ALL-CNN-C (Springenberg et al., 2014), giving improvements in performance with no changes to the architecture beyond choice of initialization. While it would be interesting to know the performance of other ResNet architectures, unlike for ResNet Init as straightforward a comparison of residual architectures and their ordinary CNN equivalent is not possible for the standard ResNet, which adds depth and certain constraints on how dimensionality reduction occurs.\n\n> \"The good performance on Cifar might attribute to the wide architecture not rather than the proposed method\"\n\nThe insight that the ResNet was sometimes better than the ResNet Init led us to believe that the benefits were not coming from shorter credit assignment paths as many believe to be the source of the benefits of ResNets, but instead from acting as a strong regularizer on the network's activations. This hypothesis on regularization led us to want to try a shallower and wider architecture, which we hypothesized would help emphasize the strengths of the standard ResNet (though this hypothesis was clearly not sufficient since the ResNet Init performed well on this architecture). \n\nWe agree with the reviewer the wide architecture contributes to the good performance on CIFAR. However, as shown by inclusion of the results from equivalent non-residual wide models as baselines for the residual architectures in Tables 2 and 3, wide architectures alone were clearly not sufficient to obtain state of the art performance on these datasets. We thus believe that the RiR architecture led to a significant improvement and in fact the large number of parameters in wide models could highlight the benefit of regularization by residual architectures (see above).\n\n> \"more exploration on the RiR and \u201cforgetting\u201d idea is more promising than selling the ResNet Init.\"\n\nWe appreciate the reviewer's interest in the RiR architecture and our preliminary ideas for forgetting based architectures. We are currently conducting more experiments related to these architectures, which we included to try and understand if the ResNet retaining too much of the input signal is a potential issue with residual architectures. The intended focus of the paper is RiR (hence the title of ResNet in ResNet) and as indicated in the paper we find the best results from this architecture, which uses both shortcut connections and ResNet Init to yield improved performance over the standard ResNet. Because ResNet Init is a necessary part of the RiR architecture, we felt the clearest exposition included the explanation of what the ResNet Init was and we presented results of the ResNet Init compared to both ResNets and standard CNNs in order to provide a thorough analysis of this component in the RiR architecture."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Resnet in Resnet: Generalizing Residual Architectures", "abstract": "ResNets have recently achieved state-of-the-art results on challenging computer vision tasks. In this paper, we create a novel architecture that improves ResNets by adding the ability to forget and by making the residuals more expressive, yielding excellent results. ResNet in ResNet outperforms architectures with similar amounts of augmentation on CIFAR-10 and establishes a new state-of-the-art on CIFAR-100.", "pdf": "/pdf/lx9l4r36gU2OVPy8Cv9g.pdf", "paperhash": "targ|resnet_in_resnet_generalizing_residual_architectures", "conflicts": ["enlitic.com", "ucsf.edu"], "authors": ["Sasha Targ", "Diogo Almeida", "Kevin Lyman"], "authorids": ["sasha.targ@gmail.com", "diogo@enlitic.com", "kevin@enlitic.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455827249191, "ddate": null, "super": null, "final": null, "tcdate": 1455827249191, "id": "ICLR.cc/2016/workshop/-/paper/140/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "lx9l4r36gU2OVPy8Cv9g", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/140/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457549279302, "tcdate": 1457549279302, "id": "MwVkZ51oZIqxwkg1t7WW", "invitation": "ICLR.cc/2016/workshop/-/paper/140/review/12", "forum": "lx9l4r36gU2OVPy8Cv9g", "replyto": "lx9l4r36gU2OVPy8Cv9g", "signatures": ["ICLR.cc/2016/workshop/paper/140/reviewer/12"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/140/reviewer/12"], "content": {"title": "Interesting idea, and promising experiments, but have flaws", "rating": "5: Marginally below acceptance threshold", "review": "Paper summary:\n\nThis paper proposed a generalization of Residual Network (ResNet)(, which turns out to be an ordinary convolutional network. Based on this idea, it proposed the convolutional network initialization method using ResNet, and the ResNet in ResNet architecture. In addition, it also explored the data-depend forgetting gate. \n\nPros:\n\n1. The forgetting gate idea was interesting and substantially different from both the ResNet and ordinary convolutional network. It is worth further explorations. \n2. ResNet in ResNet (RiR) was an interesting idea. However, since the ResNet Init architecture was questionable (see \u201cCons\u201d), building RiR with real ResNet (not ResNet Init) would be more reasonable.\n3. Performance on Cifar-10/100 were quite good. \n\nCons:\n\n1. The proposed architecture was essentially an ordinary convolutional network. Although the insight was interesting, but no clear evidence was provided on why the ordinary network can be better than ResNet. After all, the improved performance of ResNet might be due to the constrained architecture. Making it more flexible as proposed in this paper might be harmful. The underperformance of ResNet Init over ResNet Table 1 demonstrated the concern. \n2. Initializing ordinary neural networks with ResNet was interesting (Table 5 Left). But the performance for ResNet was not reported. If ResNet Init outperforms ResNet Init, ResNet Init will become not very useful.   \n3. The good performance on Cifar might attribute to the wide architecture not rather than the proposed method.\n\n\nOverall:\n\nThis paper provided interesting ideas and insights for ResNet. Some experimental results were promising. But the flaws in the method were somehow significant to me, which hindered me from recommending it for acceptance. I think more exploration on the RiR and \u201cforgetting\u201d idea is more promising than selling the ResNet Init. \n\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Resnet in Resnet: Generalizing Residual Architectures", "abstract": "ResNets have recently achieved state-of-the-art results on challenging computer vision tasks. In this paper, we create a novel architecture that improves ResNets by adding the ability to forget and by making the residuals more expressive, yielding excellent results. ResNet in ResNet outperforms architectures with similar amounts of augmentation on CIFAR-10 and establishes a new state-of-the-art on CIFAR-100.", "pdf": "/pdf/lx9l4r36gU2OVPy8Cv9g.pdf", "paperhash": "targ|resnet_in_resnet_generalizing_residual_architectures", "conflicts": ["enlitic.com", "ucsf.edu"], "authors": ["Sasha Targ", "Diogo Almeida", "Kevin Lyman"], "authorids": ["sasha.targ@gmail.com", "diogo@enlitic.com", "kevin@enlitic.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580034915, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580034915, "id": "ICLR.cc/2016/workshop/-/paper/140/review/12", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "lx9l4r36gU2OVPy8Cv9g", "replyto": "lx9l4r36gU2OVPy8Cv9g", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/140/reviewer/12", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1455827248108, "tcdate": 1455827248108, "id": "lx9l4r36gU2OVPy8Cv9g", "invitation": "ICLR.cc/2016/workshop/-/submission", "forum": "lx9l4r36gU2OVPy8Cv9g", "signatures": ["~Diogo_Almeida1"], "readers": ["everyone"], "writers": ["~Diogo_Almeida1"], "content": {"CMT_id": "", "title": "Resnet in Resnet: Generalizing Residual Architectures", "abstract": "ResNets have recently achieved state-of-the-art results on challenging computer vision tasks. In this paper, we create a novel architecture that improves ResNets by adding the ability to forget and by making the residuals more expressive, yielding excellent results. ResNet in ResNet outperforms architectures with similar amounts of augmentation on CIFAR-10 and establishes a new state-of-the-art on CIFAR-100.", "pdf": "/pdf/lx9l4r36gU2OVPy8Cv9g.pdf", "paperhash": "targ|resnet_in_resnet_generalizing_residual_architectures", "conflicts": ["enlitic.com", "ucsf.edu"], "authors": ["Sasha Targ", "Diogo Almeida", "Kevin Lyman"], "authorids": ["sasha.targ@gmail.com", "diogo@enlitic.com", "kevin@enlitic.com"]}, "nonreaders": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1454464564200, "ddate": null, "super": null, "final": null, "duedate": 1455833700000, "tcdate": 1454464564200, "id": "ICLR.cc/2016/workshop/-/submission", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"order": 4, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv.", "value-regex": "upload|http://arxiv.org/pdf/.+"}, "title": {"order": 3, "description": "Title of paper.", "value-regex": ".{0,500}"}, "abstract": {"order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"order": 1, "description": "Comma separated list of author names, as they appear in the paper.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "author_emails": {"order": 2, "description": "Comma separated list of author email addresses, in the same order as above.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "conflicts": {"order": 100, "description": "Semi-colon separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.).", "value-regex": "^([a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))+(;[a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))*$"}, "CMT_id": {"order": 5, "value-regex": ".*", "description": "If the paper is a resubmission from the ICLR 2016 Conference Track, enter its CMT ID; otherwise, leave blank."}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "expdate": 1463609700000}}}], "count": 8}