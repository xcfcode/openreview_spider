{"notes": [{"id": "S1G_cj05YQ", "original": "Ske2Xyn5t7", "number": 548, "cdate": 1538087824139, "ddate": null, "tcdate": 1538087824139, "tmdate": 1545355402438, "tddate": null, "forum": "S1G_cj05YQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Activity Regularization for Continual Learning", "abstract": "While deep neural networks have achieved remarkable successes, they suffer the well-known catastrophic forgetting issue when switching from existing tasks to tackle a new one. In this paper, we study continual learning with deep neural networks that learn from tasks arriving sequentially. We first propose an approximated multi-task learning framework that unifies a family of popular regularization based continual learning methods. We then analyze the weakness of existing approaches, and propose a novel regularization method named \u201cActivity Regularization\u201d (AR), which alleviates forgetting meanwhile keeping model\u2019s plasticity to acquire new knowledge. Extensive experiments show that our method outperform state-of-the-art methods and effectively overcomes catastrophic forgetting.\n", "keywords": ["continual learning", "regularization"], "authorids": ["hqpham.2017@smu.edu.sg", "chhoi@smu.edu.sg"], "authors": ["Quang H. Pham", "Steven C. H. Hoi"], "TL;DR": "This paper develops a novel regularization for continual learning", "pdf": "/pdf/8a3c59331b09c02b7d70fa9b9901df65d7ee59ca.pdf", "paperhash": "pham|activity_regularization_for_continual_learning", "_bibtex": "@misc{\npham2019activity,\ntitle={Activity Regularization for Continual Learning},\nauthor={Quang H. Pham and Steven C. H. Hoi},\nyear={2019},\nurl={https://openreview.net/forum?id=S1G_cj05YQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rJg_y1mgx4", "original": null, "number": 1, "cdate": 1544724191577, "ddate": null, "tcdate": 1544724191577, "tmdate": 1545354510454, "tddate": null, "forum": "S1G_cj05YQ", "replyto": "S1G_cj05YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper548/Meta_Review", "content": {"metareview": "There is no author response for this paper. The paper presents a multi-task learning framework as a unified view on the previous methods for tackling catastrophic forgetting in continual learning. In light of this framework, the authors propose to minimize the KL-divergence between the predictions of the previous optimal model and the current model using some stored samples from the previous tasks.\n \nThe consensus among all three reviewers and AC is that the paper lacks (1) novelty, as the proposed approach is similar if not identical to Learning without forgetting (LwF)[Li&Hoiem 2017] with the difference that the KL-divergence is computed on samples kept from the previous tasks (and LwF uses samples from the current task). Methodological and experimental comparison to LwF is crucial to assess the benefits and novelty of the proposed approach.\n \nAlso the reviewers address other potential weaknesses and give suggestions for improvement: (2) empirical evaluations can be substantially improved with sensitivity analysis of the hyper-parameters on the validation data (R3), indicating errors and error bars for all results (R3 and R2), using more challenging and realistic experimental setting where the data comes from different domains (R1), justifying the results better -- see R2\u2019s questions; (3) lack of clarity and motivation in Section 3.1 -- see R2\u2019s and R1\u2019s suggestions for how to improve clarity and potentially take advantage of the current task to probably correct the previous models prediction when it was wrong.\n\nAC suggests, in its current state the manuscript is not ready for a publication. We hope the reviews are useful for improving and revising the paper.\n", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Meta-Review"}, "signatures": ["ICLR.cc/2019/Conference/Paper548/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper548/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Activity Regularization for Continual Learning", "abstract": "While deep neural networks have achieved remarkable successes, they suffer the well-known catastrophic forgetting issue when switching from existing tasks to tackle a new one. In this paper, we study continual learning with deep neural networks that learn from tasks arriving sequentially. We first propose an approximated multi-task learning framework that unifies a family of popular regularization based continual learning methods. We then analyze the weakness of existing approaches, and propose a novel regularization method named \u201cActivity Regularization\u201d (AR), which alleviates forgetting meanwhile keeping model\u2019s plasticity to acquire new knowledge. Extensive experiments show that our method outperform state-of-the-art methods and effectively overcomes catastrophic forgetting.\n", "keywords": ["continual learning", "regularization"], "authorids": ["hqpham.2017@smu.edu.sg", "chhoi@smu.edu.sg"], "authors": ["Quang H. Pham", "Steven C. H. Hoi"], "TL;DR": "This paper develops a novel regularization for continual learning", "pdf": "/pdf/8a3c59331b09c02b7d70fa9b9901df65d7ee59ca.pdf", "paperhash": "pham|activity_regularization_for_continual_learning", "_bibtex": "@misc{\npham2019activity,\ntitle={Activity Regularization for Continual Learning},\nauthor={Quang H. Pham and Steven C. H. Hoi},\nyear={2019},\nurl={https://openreview.net/forum?id=S1G_cj05YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper548/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353175646, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1G_cj05YQ", "replyto": "S1G_cj05YQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper548/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper548/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper548/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353175646}}}, {"id": "rkgFqD033X", "original": null, "number": 3, "cdate": 1541363601400, "ddate": null, "tcdate": 1541363601400, "tmdate": 1541533900402, "tddate": null, "forum": "S1G_cj05YQ", "replyto": "S1G_cj05YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper548/Official_Review", "content": {"title": "The authors proposed a new regulariser for continual learning. However, the novelty is not clear and experiments setting needs improvement.", "review": "The authors proposed a new regularizer for continual learning to tackle the catastrophic forgetting problem. The proposed method minimizes the KL-divergence between the prediction of previous models and current models on the stored samples of previous tasks. The idea is straightforward and sounds technical. Experiments show the effectiveness of the methods compared to state-of-the-art. Although the idea sounds interesting and the experiments look promising, the novelty of the paper seems to be limited. In addition, the experiments setting needs to be improved as well. In the following, you have detailed comments. \n\n1. It seems to be an extension of Learning without Forgetting (LwF) Li & Hoiem 2017 with simply on the examples of previous tasks in the memory. LwF only regularizes on the current task. It is not clear what is the difference between the proposed method and LwF except this.\n2. The authors fixed many critical hyper-parameters: temperature(5), learning rate(0.05), epochs(10). The author should report the results for all methods with these hyper-parameters chosen on the validation set.\n3. The authors presented how they split the training and testing data. Please be clear how you split the validation set.\n4. The authors argued that sample quality does not affect the proposed methods. Then the authors should show the variance from different random sampling.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper548/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Activity Regularization for Continual Learning", "abstract": "While deep neural networks have achieved remarkable successes, they suffer the well-known catastrophic forgetting issue when switching from existing tasks to tackle a new one. In this paper, we study continual learning with deep neural networks that learn from tasks arriving sequentially. We first propose an approximated multi-task learning framework that unifies a family of popular regularization based continual learning methods. We then analyze the weakness of existing approaches, and propose a novel regularization method named \u201cActivity Regularization\u201d (AR), which alleviates forgetting meanwhile keeping model\u2019s plasticity to acquire new knowledge. Extensive experiments show that our method outperform state-of-the-art methods and effectively overcomes catastrophic forgetting.\n", "keywords": ["continual learning", "regularization"], "authorids": ["hqpham.2017@smu.edu.sg", "chhoi@smu.edu.sg"], "authors": ["Quang H. Pham", "Steven C. H. Hoi"], "TL;DR": "This paper develops a novel regularization for continual learning", "pdf": "/pdf/8a3c59331b09c02b7d70fa9b9901df65d7ee59ca.pdf", "paperhash": "pham|activity_regularization_for_continual_learning", "_bibtex": "@misc{\npham2019activity,\ntitle={Activity Regularization for Continual Learning},\nauthor={Quang H. Pham and Steven C. H. Hoi},\nyear={2019},\nurl={https://openreview.net/forum?id=S1G_cj05YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper548/Official_Review", "cdate": 1542234436364, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1G_cj05YQ", "replyto": "S1G_cj05YQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper548/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335749150, "tmdate": 1552335749150, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper548/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BkeZupMchQ", "original": null, "number": 2, "cdate": 1541184873213, "ddate": null, "tcdate": 1541184873213, "tmdate": 1541533900051, "tddate": null, "forum": "S1G_cj05YQ", "replyto": "S1G_cj05YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper548/Official_Review", "content": {"title": "Interesting work but not good enough", "review": "The paper addresses the problem of continual learning from a sequence of supervised tasks. The main contribution of the paper are the following. The work:\n* puts the problem in uniform general framework in which many of the state-of-the-art methods fit\n* identifies some drawbacks of some the state of the art methods (namely EWC- Kirkpatrick et al., 2017 and GEM-Lopez-Paz et al., 2017) . \n* proposes two versions of an approach to address these drawbacks \nThe main identified problem in EWC and GEM is that both methods result in a zero penalty when both the previous and current models misclassify a sample, even if they make different mistakes.\nTo solve this problem, the authors propose to keep a memory of randomly sampled data from the previous task, and distil the knowledge from the optima of the previous tasks to the current one using a KL penalty. The first version considers computing this penalty from the whole sequence of tasks, while the second randomly selects one task at each iteration and uses it to compute the penalty, paying some overall accuracy for plasticity.\n\nWhile the paper is clear and well written, I have some concerns about it's quality, originality and significance.\n\nOriginality: The work seems to me to be very related to LwF. The main difference is that while LwF uses only the new data for the KL penalty, this paper keeps a memory of previously seen data to compute this loss. \n\nSignificance: While the way the authors approached the problem seems well structured and motivated, and is based on a sound observation, the authors limited themselves to experiments where the task data have similar structure. I am not sure how significant the improvement of the method would be in the more challenging and realistic setting where the data comes from different domains.  I am more specifically skeptical about the stochastic version of the algorithm in that case.\n\nQuality: \n* The proposed algorithm stores some data from previous tasks along with the outputs of the corresponding optimal model. While knowledge distillation as proposed would result in a non zero penalty when the new model makes different prediction, it still doesn't take advantage of the new information to probably correct the previous models prediction when it is wrong. Why not keeping the ground truth labels instead? This won't increase the memory requirement, and may give better results. It would be interesting to compare against such a method. \n* I think selecting the samples to keep in memory randomly could to be suboptimal.  Other selection methods can be considered. A previous work:  iCaRL: Incremental Classifier and Representation Learning, Rebuffi et al. 2017, gives way to select representative samples. It would be interesting to see the effect of such a selection on the results. \n\nOverall, while the paper doesn't present any significant flaw, it doesn't add much to the continual learning literature either, which explains my rating.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper548/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Activity Regularization for Continual Learning", "abstract": "While deep neural networks have achieved remarkable successes, they suffer the well-known catastrophic forgetting issue when switching from existing tasks to tackle a new one. In this paper, we study continual learning with deep neural networks that learn from tasks arriving sequentially. We first propose an approximated multi-task learning framework that unifies a family of popular regularization based continual learning methods. We then analyze the weakness of existing approaches, and propose a novel regularization method named \u201cActivity Regularization\u201d (AR), which alleviates forgetting meanwhile keeping model\u2019s plasticity to acquire new knowledge. Extensive experiments show that our method outperform state-of-the-art methods and effectively overcomes catastrophic forgetting.\n", "keywords": ["continual learning", "regularization"], "authorids": ["hqpham.2017@smu.edu.sg", "chhoi@smu.edu.sg"], "authors": ["Quang H. Pham", "Steven C. H. Hoi"], "TL;DR": "This paper develops a novel regularization for continual learning", "pdf": "/pdf/8a3c59331b09c02b7d70fa9b9901df65d7ee59ca.pdf", "paperhash": "pham|activity_regularization_for_continual_learning", "_bibtex": "@misc{\npham2019activity,\ntitle={Activity Regularization for Continual Learning},\nauthor={Quang H. Pham and Steven C. H. Hoi},\nyear={2019},\nurl={https://openreview.net/forum?id=S1G_cj05YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper548/Official_Review", "cdate": 1542234436364, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1G_cj05YQ", "replyto": "S1G_cj05YQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper548/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335749150, "tmdate": 1552335749150, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper548/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJlmbJG9hQ", "original": null, "number": 1, "cdate": 1541181178738, "ddate": null, "tcdate": 1541181178738, "tmdate": 1541533899839, "tddate": null, "forum": "S1G_cj05YQ", "replyto": "S1G_cj05YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper548/Official_Review", "content": {"title": "Simple approach, but limited novelty, and needs some improvement in exposition and benchmarking of related work", "review": "This paper proposes an approach to mitigate catastrophic forgetting in supervised learning by regularizing activations. The paper views previous techniques (EWC, SI, and GEM) under a multi-task learning lens, and then proposes an additional loss term to minimise the KL between activations from previous and current models, on previous tasks - this is based on a memory which stores some previous samples and their corresponding activations.\n\nI think it is a simple and intuitive approach and a well-written paper. Unfortunately I have a number of concerns that I think preclude publication in the current state.\n\nFirst, in terms of related work, I believe this is very similar to Learning without forgetting (LwF), with the difference that the KL-divergence is computed on samples kept from the previous tasks. This is briefly mentioned in the paper, but I think it needs to be made more explicit, and LwF should be a baseline in the experiments to clearly indicate the benefit of keeping this data. There is also a relationship to EWC: given the connection between the Fisher information and KL, it can be viewed as minimising the KL divergence in parameter space, rather than in activation space (which is the case here). Also note that EWC uses the true Fisher rather than the empirical, contrary to the derivation in equation (2).\nThere are also a number of papers that haven\u2019t been cited in the related work [1][2][3][4].\n\nSecond, I think the motivation in Section 3.1 could be more convincing. Most importantly, it\u2019s not clear to me that the decision boundary *shouldn\u2019t* change for previously misclassified examples, as this could be an opportunity for backwards transfer.\nFurther, I don\u2019t think the point in the last paragraph about having a small data portion is relevant, since they are from the same data distribution, and we would expect misclassified samples to be in the same (low) frequency in Fisher estimation as overall. I think the point of this paragraph is just that it is important to consider the entire predictive distribution of previous tasks rather than the probability of the correct class, so this should be stated more clearly and then justified. \n\nFinally, I think the experimental justification could be improved as well. Beyond permuted MNIST (which it has been argued is not as useful as other baselines [4]), only the final performance on split notMNIST / CIFAR-100 is reported. Some comments and questions:\n- The accuracies of EWC (and possibly SI) in the table are worse than reported in previous work (eg. [1]), so I think this needs to be examined.\n- What is the fine-tuning baseline (I don't believe it is actually clearly defined)? How can it be so low in figure 2a but better in 2b?\n- I think plots over time (performance on all tasks) would be much more useful than the final performance in Table 2 and Fig 2.\n- Errors and error bars would be beneficial for all results.\n- Table 1 should also include the references provided.\n\nSome other comments and questions:\n- Compared to eqn (2), eqn (6) is missing the \u00bd constant.\n- Typos in section 5.3: \"SI performs better than SI\", and VAR instead of SAR.\n- Section 2, unclear of meaning of \"coined with the likelihood\" (should this be \u201ccoincide\u201d?)\n- The first line should be \u201cHumans have the ability to learn...\u201d In general, I think the introduction could use another proofread for grammar and readability as I saw a few minor things.\n\n[1] Nguyen, Cuong V., et al. \"Variational Continual Learning.\" ICLR, 2018.\n[2] Schwarz, Jonathan, et al. \"Progress & Compress: A scalable framework for continual learning.\" ICML, 2018.\n[3] Shin, Hanul, et al. \"Continual learning with deep generative replay.\" NIPS, 2017.\n[4] Farquhar, Sebastian, and Yarin Gal. \"Towards Robust Evaluations of Continual Learning.\" arXiv, 2018.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper548/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Activity Regularization for Continual Learning", "abstract": "While deep neural networks have achieved remarkable successes, they suffer the well-known catastrophic forgetting issue when switching from existing tasks to tackle a new one. In this paper, we study continual learning with deep neural networks that learn from tasks arriving sequentially. We first propose an approximated multi-task learning framework that unifies a family of popular regularization based continual learning methods. We then analyze the weakness of existing approaches, and propose a novel regularization method named \u201cActivity Regularization\u201d (AR), which alleviates forgetting meanwhile keeping model\u2019s plasticity to acquire new knowledge. Extensive experiments show that our method outperform state-of-the-art methods and effectively overcomes catastrophic forgetting.\n", "keywords": ["continual learning", "regularization"], "authorids": ["hqpham.2017@smu.edu.sg", "chhoi@smu.edu.sg"], "authors": ["Quang H. Pham", "Steven C. H. Hoi"], "TL;DR": "This paper develops a novel regularization for continual learning", "pdf": "/pdf/8a3c59331b09c02b7d70fa9b9901df65d7ee59ca.pdf", "paperhash": "pham|activity_regularization_for_continual_learning", "_bibtex": "@misc{\npham2019activity,\ntitle={Activity Regularization for Continual Learning},\nauthor={Quang H. Pham and Steven C. H. Hoi},\nyear={2019},\nurl={https://openreview.net/forum?id=S1G_cj05YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper548/Official_Review", "cdate": 1542234436364, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1G_cj05YQ", "replyto": "S1G_cj05YQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper548/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335749150, "tmdate": 1552335749150, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper548/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}