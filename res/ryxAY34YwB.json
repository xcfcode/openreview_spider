{"notes": [{"id": "ryxAY34YwB", "original": "rygko9_pIS", "number": 97, "cdate": 1569438853607, "ddate": null, "tcdate": 1569438853607, "tmdate": 1577168230003, "tddate": null, "forum": "ryxAY34YwB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["chezhu@microsoft.com", "zy99@stanford.edu", "rogmyr@microsoft.com", "nzeng@microsoft.com", "xdh@microsoft.com"], "title": "Make Lead Bias in Your Favor: A Simple and Effective Method for News Summarization", "authors": ["Chenguang Zhu", "Ziyi Yang", "Robert Gmyr", "Michael Zeng", "Xuedong Huang"], "pdf": "/pdf/e3dff444dd04f47f69ddb67445f421206d9599d9.pdf", "TL;DR": "A method to leverage lead bias in large-scale pretraining for abstractive news summarization", "abstract": "Lead bias is a common phenomenon in news summarization, where early parts of an article often contain the most salient information. While many algorithms exploit this fact in summary generation, it has a detrimental effect on teaching the model to discriminate and extract important information. We propose that the lead bias can be leveraged in a simple and effective way in our favor to pretrain abstractive news summarization models on large-scale unlabelled corpus: predicting the leading sentences using the rest of an article. Via careful data cleaning and filtering, our transformer-based pretrained model without any finetuning achieves remarkable results over various news summarization tasks. With further finetuning, our model outperforms many competitive baseline models. For example, the pretrained model without finetuning outperforms pointer-generator network on CNN/DailyMail dataset. The finetuned model obtains 3.2% higher ROUGE-1, 1.6% higher ROUGE-2 and 2.1% higher ROUGE-L scores than the best baseline model on XSum dataset.", "code": "https://www.dropbox.com/s/3qbcpfwtzfowzmo/PretrainAbsSum.zip?dl=0", "keywords": ["Summarization", "Pretraining"], "paperhash": "zhu|make_lead_bias_in_your_favor_a_simple_and_effective_method_for_news_summarization", "original_pdf": "/attachment/22ab27f4eaaea2ab0302660b4df8c335a2d75cd5.pdf", "_bibtex": "@misc{\nzhu2020make,\ntitle={Make Lead Bias in Your Favor: A Simple and Effective Method for News Summarization},\nauthor={Chenguang Zhu and Ziyi Yang and Robert Gmyr and Michael Zeng and Xuedong Huang},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxAY34YwB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "s-Ujxf_4W", "original": null, "number": 1, "cdate": 1576798687373, "ddate": null, "tcdate": 1576798687373, "tmdate": 1576800947750, "tddate": null, "forum": "ryxAY34YwB", "replyto": "ryxAY34YwB", "invitation": "ICLR.cc/2020/Conference/Paper97/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes a method to leverage the Lead (i.e., first sentence of an article) in training a model for abstractive news summarization. \n\nReviewers' initial recommendations were weak reject to weak accept, pointing out the limitations of the paper including 1) little novelty in modeling, 2) weak evaluation, and 3) lack of deep analysis. After the author rebuttal and revised paper, one of the reviewers increased the score and were leaning toward weak accept. \n\nHowever, reviewers noted that there was significant overlap with another submission, and we discussed that it would be best to accept one of the two, incorporating the contributions of both papers. Hence, I recommend that this paper not be accepted, and perhaps some of the non-overlapping contents of this paper can be included in the other, accepted paper.\n\nThank you for submitting this paper. I enjoyed reading it.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["chezhu@microsoft.com", "zy99@stanford.edu", "rogmyr@microsoft.com", "nzeng@microsoft.com", "xdh@microsoft.com"], "title": "Make Lead Bias in Your Favor: A Simple and Effective Method for News Summarization", "authors": ["Chenguang Zhu", "Ziyi Yang", "Robert Gmyr", "Michael Zeng", "Xuedong Huang"], "pdf": "/pdf/e3dff444dd04f47f69ddb67445f421206d9599d9.pdf", "TL;DR": "A method to leverage lead bias in large-scale pretraining for abstractive news summarization", "abstract": "Lead bias is a common phenomenon in news summarization, where early parts of an article often contain the most salient information. While many algorithms exploit this fact in summary generation, it has a detrimental effect on teaching the model to discriminate and extract important information. We propose that the lead bias can be leveraged in a simple and effective way in our favor to pretrain abstractive news summarization models on large-scale unlabelled corpus: predicting the leading sentences using the rest of an article. Via careful data cleaning and filtering, our transformer-based pretrained model without any finetuning achieves remarkable results over various news summarization tasks. With further finetuning, our model outperforms many competitive baseline models. For example, the pretrained model without finetuning outperforms pointer-generator network on CNN/DailyMail dataset. The finetuned model obtains 3.2% higher ROUGE-1, 1.6% higher ROUGE-2 and 2.1% higher ROUGE-L scores than the best baseline model on XSum dataset.", "code": "https://www.dropbox.com/s/3qbcpfwtzfowzmo/PretrainAbsSum.zip?dl=0", "keywords": ["Summarization", "Pretraining"], "paperhash": "zhu|make_lead_bias_in_your_favor_a_simple_and_effective_method_for_news_summarization", "original_pdf": "/attachment/22ab27f4eaaea2ab0302660b4df8c335a2d75cd5.pdf", "_bibtex": "@misc{\nzhu2020make,\ntitle={Make Lead Bias in Your Favor: A Simple and Effective Method for News Summarization},\nauthor={Chenguang Zhu and Ziyi Yang and Robert Gmyr and Michael Zeng and Xuedong Huang},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxAY34YwB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "ryxAY34YwB", "replyto": "ryxAY34YwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795713898, "tmdate": 1576800263621, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper97/-/Decision"}}}, {"id": "HJgO4kMOFB", "original": null, "number": 1, "cdate": 1571458864383, "ddate": null, "tcdate": 1571458864383, "tmdate": 1574222558981, "tddate": null, "forum": "ryxAY34YwB", "replyto": "ryxAY34YwB", "invitation": "ICLR.cc/2020/Conference/Paper97/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "This paper proposed an interesting idea on how we can leverage the lead bias in summarization datasets to pretrain abstractive news summarization models on large-scale unlabelled corpus in simple and effective way. \n\nFor pre-training, they collected three years of online news articles data. Then, they take the top 3 sentences of the article as summary and the rest of the article as input document. For better choosing such article-summary pairs, they employ effective data cleaning and filtering process. Overall, they collected 21.4M articles for the pretraining. \n \nOverall, the pretrained model does decent on three summarization datasets without any fine-tuning. After fine-tuning the respective datasets, the gains seem significant. Especially on the XSum dataset, the improvements are remarkable. \n\nI believe that the idea is interesting but the experiments are incomplete and more investigation is required to make this paper stronger. Therefore I suggest to reject this paper. \n\nArguments:\n1) The important experiments that are missing in this paper are evaluating the proposed method on better human written summarization datasets -- DUC. The real world summarizations resemble more like the ones in the DUC dataset and it would be interesting to see if the transfer results of the pretrained model on the DUC datasets. The important question is to understand whether the pretrained model which took advantage of lead-bias could achieve good summaries on real summarization samples. This would also answer whether the pretrained just took advantage of the lead-bias issue of many large summarization datasets or does it really learn good summarization model. \n\n2) This paper has good idea but mainly missing ablation studies. For example, how does the proposed model do compared with GPT-2 in the fine-tuning setting, and how do these two models perform on the DUC datasets. \n\n3) During the dataset filtering/collection, a check on the quality of the filtering process by doing a small human study would have been a great addition. Also, instead of showing the output examples (which can go in the supplementary), human study comparing the quality of the pretrained model with fine-tuning and a baseline (can be from previous work) would have been better. \n\nOther minor questions\n1) \u201cwe only keep articles with 10-150 words in the top three sentences and 150-1200 words in the rest\u201d -- is there any reason on fixing to these numbers? How did you make this decision ?\n\n2) Even though the performance gains look visibly significant, I would suggest to report the statistical significance scores.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper97/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper97/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["chezhu@microsoft.com", "zy99@stanford.edu", "rogmyr@microsoft.com", "nzeng@microsoft.com", "xdh@microsoft.com"], "title": "Make Lead Bias in Your Favor: A Simple and Effective Method for News Summarization", "authors": ["Chenguang Zhu", "Ziyi Yang", "Robert Gmyr", "Michael Zeng", "Xuedong Huang"], "pdf": "/pdf/e3dff444dd04f47f69ddb67445f421206d9599d9.pdf", "TL;DR": "A method to leverage lead bias in large-scale pretraining for abstractive news summarization", "abstract": "Lead bias is a common phenomenon in news summarization, where early parts of an article often contain the most salient information. While many algorithms exploit this fact in summary generation, it has a detrimental effect on teaching the model to discriminate and extract important information. We propose that the lead bias can be leveraged in a simple and effective way in our favor to pretrain abstractive news summarization models on large-scale unlabelled corpus: predicting the leading sentences using the rest of an article. Via careful data cleaning and filtering, our transformer-based pretrained model without any finetuning achieves remarkable results over various news summarization tasks. With further finetuning, our model outperforms many competitive baseline models. For example, the pretrained model without finetuning outperforms pointer-generator network on CNN/DailyMail dataset. The finetuned model obtains 3.2% higher ROUGE-1, 1.6% higher ROUGE-2 and 2.1% higher ROUGE-L scores than the best baseline model on XSum dataset.", "code": "https://www.dropbox.com/s/3qbcpfwtzfowzmo/PretrainAbsSum.zip?dl=0", "keywords": ["Summarization", "Pretraining"], "paperhash": "zhu|make_lead_bias_in_your_favor_a_simple_and_effective_method_for_news_summarization", "original_pdf": "/attachment/22ab27f4eaaea2ab0302660b4df8c335a2d75cd5.pdf", "_bibtex": "@misc{\nzhu2020make,\ntitle={Make Lead Bias in Your Favor: A Simple and Effective Method for News Summarization},\nauthor={Chenguang Zhu and Ziyi Yang and Robert Gmyr and Michael Zeng and Xuedong Huang},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxAY34YwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ryxAY34YwB", "replyto": "ryxAY34YwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper97/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper97/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575604647798, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper97/Reviewers"], "noninvitees": [], "tcdate": 1570237757120, "tmdate": 1575604647810, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper97/-/Official_Review"}}}, {"id": "Hye5SLEjiB", "original": null, "number": 7, "cdate": 1573762625681, "ddate": null, "tcdate": 1573762625681, "tmdate": 1573762625681, "tddate": null, "forum": "ryxAY34YwB", "replyto": "H1gjXOYqoH", "invitation": "ICLR.cc/2020/Conference/Paper97/-/Official_Comment", "content": {"title": "Thanks for your feedback", "comment": "For your questions:\n\n1. Usually ROUGE-F1 is used for summarization. However, on NYT dataset, we align with BERTSUM paper to use limited-ROUGE-recall, which caps the generated summary at the golden summary's length. In this case, ROUGE-recall makes more sense. Similarly, previous papers on DUC2003/2004 cap generated summary at 75 characters (required by DUC competition) and ROUGE-recall has been used for the evaluation in previous literature.\n\n2. Yes, GPT-2 is not specifically trained for summarization. But the GPT paper reports their performance on summarization (CNN/DM) so we conduct a comparison against GPT-2.\n\n3. For inter-annotator agreement, we add the kappa statistics in Section 5.7, which is 0.34.\n   We added in Appendix the evaluation guidelines of scoring criteria we gave to the labelers.\n\n4. Due to time constraint, we could not finish the BERT training on our data at this time, since the data we used (21.4M articles) contains more than double the number of tokens used in BERT. But this is a great idea and we will try it in later studies.\n   On the other hand, BERTSUM method requires summarization-specific linear layers to be added and trained after masked LM is trained. In comparison, our model is ready to use as soon as the pretraining finished."}, "signatures": ["ICLR.cc/2020/Conference/Paper97/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper97/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["chezhu@microsoft.com", "zy99@stanford.edu", "rogmyr@microsoft.com", "nzeng@microsoft.com", "xdh@microsoft.com"], "title": "Make Lead Bias in Your Favor: A Simple and Effective Method for News Summarization", "authors": ["Chenguang Zhu", "Ziyi Yang", "Robert Gmyr", "Michael Zeng", "Xuedong Huang"], "pdf": "/pdf/e3dff444dd04f47f69ddb67445f421206d9599d9.pdf", "TL;DR": "A method to leverage lead bias in large-scale pretraining for abstractive news summarization", "abstract": "Lead bias is a common phenomenon in news summarization, where early parts of an article often contain the most salient information. While many algorithms exploit this fact in summary generation, it has a detrimental effect on teaching the model to discriminate and extract important information. We propose that the lead bias can be leveraged in a simple and effective way in our favor to pretrain abstractive news summarization models on large-scale unlabelled corpus: predicting the leading sentences using the rest of an article. Via careful data cleaning and filtering, our transformer-based pretrained model without any finetuning achieves remarkable results over various news summarization tasks. With further finetuning, our model outperforms many competitive baseline models. For example, the pretrained model without finetuning outperforms pointer-generator network on CNN/DailyMail dataset. The finetuned model obtains 3.2% higher ROUGE-1, 1.6% higher ROUGE-2 and 2.1% higher ROUGE-L scores than the best baseline model on XSum dataset.", "code": "https://www.dropbox.com/s/3qbcpfwtzfowzmo/PretrainAbsSum.zip?dl=0", "keywords": ["Summarization", "Pretraining"], "paperhash": "zhu|make_lead_bias_in_your_favor_a_simple_and_effective_method_for_news_summarization", "original_pdf": "/attachment/22ab27f4eaaea2ab0302660b4df8c335a2d75cd5.pdf", "_bibtex": "@misc{\nzhu2020make,\ntitle={Make Lead Bias in Your Favor: A Simple and Effective Method for News Summarization},\nauthor={Chenguang Zhu and Ziyi Yang and Robert Gmyr and Michael Zeng and Xuedong Huang},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxAY34YwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryxAY34YwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper97/Authors", "ICLR.cc/2020/Conference/Paper97/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper97/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper97/Reviewers", "ICLR.cc/2020/Conference/Paper97/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper97/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper97/Authors|ICLR.cc/2020/Conference/Paper97/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176445, "tmdate": 1576860555206, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper97/Authors", "ICLR.cc/2020/Conference/Paper97/Reviewers", "ICLR.cc/2020/Conference/Paper97/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper97/-/Official_Comment"}}}, {"id": "H1gjXOYqoH", "original": null, "number": 5, "cdate": 1573718051484, "ddate": null, "tcdate": 1573718051484, "tmdate": 1573718051484, "tddate": null, "forum": "ryxAY34YwB", "replyto": "rkeJYDRKsH", "invitation": "ICLR.cc/2020/Conference/Paper97/-/Official_Comment", "content": {"title": "Human evaluation - weak analysis", "comment": "re point 1), about Recall vs F1. Can you explain why different scores are used for different corpora? I assume this is due to summaries of different length, but an explanation would be helpful. Just reporting the same measures as previous without critical analysis opens the door to many bad things.\n\nThe additional experiments on DUC are always good. However, I don't think that too much should be read into the bad performance of GPT-2, as this was not adapted in any way for summarization.\n\nThe human evaluation is done in a very shallow way, and as it stands does not bring much to make the claims stronger.\n - what is the inter-annotator agreement? Without any filtering you risk of running into spammers\n - could you report the exact phrasing the annotators saw in the Appendix? Standard practice is to evaluate different aspects separately, as you merged it all together it is not obvious what is being evaluated.\n\nI believe this is a valuable paper with a good insight for the summarization community. It is a bit light still, and unfortunately the most interesting question for me (is pre-training for summarization better than pre-training for masked LM, controlling for the data) is not yet answered."}, "signatures": ["ICLR.cc/2020/Conference/Paper97/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper97/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["chezhu@microsoft.com", "zy99@stanford.edu", "rogmyr@microsoft.com", "nzeng@microsoft.com", "xdh@microsoft.com"], "title": "Make Lead Bias in Your Favor: A Simple and Effective Method for News Summarization", "authors": ["Chenguang Zhu", "Ziyi Yang", "Robert Gmyr", "Michael Zeng", "Xuedong Huang"], "pdf": "/pdf/e3dff444dd04f47f69ddb67445f421206d9599d9.pdf", "TL;DR": "A method to leverage lead bias in large-scale pretraining for abstractive news summarization", "abstract": "Lead bias is a common phenomenon in news summarization, where early parts of an article often contain the most salient information. While many algorithms exploit this fact in summary generation, it has a detrimental effect on teaching the model to discriminate and extract important information. We propose that the lead bias can be leveraged in a simple and effective way in our favor to pretrain abstractive news summarization models on large-scale unlabelled corpus: predicting the leading sentences using the rest of an article. Via careful data cleaning and filtering, our transformer-based pretrained model without any finetuning achieves remarkable results over various news summarization tasks. With further finetuning, our model outperforms many competitive baseline models. For example, the pretrained model without finetuning outperforms pointer-generator network on CNN/DailyMail dataset. The finetuned model obtains 3.2% higher ROUGE-1, 1.6% higher ROUGE-2 and 2.1% higher ROUGE-L scores than the best baseline model on XSum dataset.", "code": "https://www.dropbox.com/s/3qbcpfwtzfowzmo/PretrainAbsSum.zip?dl=0", "keywords": ["Summarization", "Pretraining"], "paperhash": "zhu|make_lead_bias_in_your_favor_a_simple_and_effective_method_for_news_summarization", "original_pdf": "/attachment/22ab27f4eaaea2ab0302660b4df8c335a2d75cd5.pdf", "_bibtex": "@misc{\nzhu2020make,\ntitle={Make Lead Bias in Your Favor: A Simple and Effective Method for News Summarization},\nauthor={Chenguang Zhu and Ziyi Yang and Robert Gmyr and Michael Zeng and Xuedong Huang},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxAY34YwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryxAY34YwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper97/Authors", "ICLR.cc/2020/Conference/Paper97/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper97/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper97/Reviewers", "ICLR.cc/2020/Conference/Paper97/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper97/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper97/Authors|ICLR.cc/2020/Conference/Paper97/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176445, "tmdate": 1576860555206, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper97/Authors", "ICLR.cc/2020/Conference/Paper97/Reviewers", "ICLR.cc/2020/Conference/Paper97/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper97/-/Official_Comment"}}}, {"id": "rkeJYDRKsH", "original": null, "number": 2, "cdate": 1573672822771, "ddate": null, "tcdate": 1573672822771, "tmdate": 1573698016668, "tddate": null, "forum": "ryxAY34YwB", "replyto": "HJxhRlhaKS", "invitation": "ICLR.cc/2020/Conference/Paper97/-/Official_Comment", "content": {"title": "Thanks for your comments!", "comment": "For where our result leads, we argue that taking advantage of positional bias or any structural information can help with model pretraining. This has a lot of importance as nowadays large pretrained models have proved to be very effective in NLP. Furthermore, our idea offers a different angle from the masked language model, which is more artificially created. \n\nThe idea of pretraining BERT on our large news corpus then using BERTSUM is very good. Due to time limit, we could not finish this experiment before the rebuttal deadline. But we will definitely follow this idea and conduct experiments. In accordance with your idea, we add in results from the GPT-2 model for CNN/DailyMail and DUC2003/2004 (Table 3&4), and it is outperformed by our pretrained-only method PL-NoFT. As both models are pretrained on unlabeled data, this result shows the effectiveness of our approach.\n\nFor your questions,\n1) We use recall on NYT and F1 in XSum/CNN exactly following BERTSUM (https://arxiv.org/pdf/1908.08345.pdf) for fair comparison.\n\n2) The hyper-parameters are tuned on the validation data using the pre-trained model. Then the finetuned model also takes the same set of hyper-parameters. The result is not very sensitive to these parameters.\nFor example, in XSUM, the ROUGE-1 on validation data with different beam-width: \n1\t23.355 (* we choose this)\n2\t22.91\n3\t23.011\n4\t22.391\n5\t22.318\n\n3) The last but one sentence means trigram blocking: if the next generated word triggers a duplicate trigram, we do not use it. Like \"A A B A A\", if \"B\" is the next top word candidate, we ignore it as it will triggle a duplicate trigram AAB.\n   The last sentence means we compute the average cross entropy per word as criterion, which is a popular standard in generation. If the sum is used,\nshorter sentences are favored."}, "signatures": ["ICLR.cc/2020/Conference/Paper97/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper97/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["chezhu@microsoft.com", "zy99@stanford.edu", "rogmyr@microsoft.com", "nzeng@microsoft.com", "xdh@microsoft.com"], "title": "Make Lead Bias in Your Favor: A Simple and Effective Method for News Summarization", "authors": ["Chenguang Zhu", "Ziyi Yang", "Robert Gmyr", "Michael Zeng", "Xuedong Huang"], "pdf": "/pdf/e3dff444dd04f47f69ddb67445f421206d9599d9.pdf", "TL;DR": "A method to leverage lead bias in large-scale pretraining for abstractive news summarization", "abstract": "Lead bias is a common phenomenon in news summarization, where early parts of an article often contain the most salient information. While many algorithms exploit this fact in summary generation, it has a detrimental effect on teaching the model to discriminate and extract important information. We propose that the lead bias can be leveraged in a simple and effective way in our favor to pretrain abstractive news summarization models on large-scale unlabelled corpus: predicting the leading sentences using the rest of an article. Via careful data cleaning and filtering, our transformer-based pretrained model without any finetuning achieves remarkable results over various news summarization tasks. With further finetuning, our model outperforms many competitive baseline models. For example, the pretrained model without finetuning outperforms pointer-generator network on CNN/DailyMail dataset. The finetuned model obtains 3.2% higher ROUGE-1, 1.6% higher ROUGE-2 and 2.1% higher ROUGE-L scores than the best baseline model on XSum dataset.", "code": "https://www.dropbox.com/s/3qbcpfwtzfowzmo/PretrainAbsSum.zip?dl=0", "keywords": ["Summarization", "Pretraining"], "paperhash": "zhu|make_lead_bias_in_your_favor_a_simple_and_effective_method_for_news_summarization", "original_pdf": "/attachment/22ab27f4eaaea2ab0302660b4df8c335a2d75cd5.pdf", "_bibtex": "@misc{\nzhu2020make,\ntitle={Make Lead Bias in Your Favor: A Simple and Effective Method for News Summarization},\nauthor={Chenguang Zhu and Ziyi Yang and Robert Gmyr and Michael Zeng and Xuedong Huang},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxAY34YwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryxAY34YwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper97/Authors", "ICLR.cc/2020/Conference/Paper97/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper97/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper97/Reviewers", "ICLR.cc/2020/Conference/Paper97/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper97/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper97/Authors|ICLR.cc/2020/Conference/Paper97/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176445, "tmdate": 1576860555206, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper97/Authors", "ICLR.cc/2020/Conference/Paper97/Reviewers", "ICLR.cc/2020/Conference/Paper97/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper97/-/Official_Comment"}}}, {"id": "BketsD0YiB", "original": null, "number": 4, "cdate": 1573672865112, "ddate": null, "tcdate": 1573672865112, "tmdate": 1573672865112, "tddate": null, "forum": "ryxAY34YwB", "replyto": "HJgO4kMOFB", "invitation": "ICLR.cc/2020/Conference/Paper97/-/Official_Comment", "content": {"title": "Thanks for your comments!", "comment": "For your questions:\n1. We add experiments on DUC-2003 and DUC-2004, using the dataset as test (Table 4). Our pretrained model achieves state-of-the-art results among all unsupervised models. \n\n2. We add GPT-2's result for CNN/DailyMail and DUC-2003/2004 datasets (Table 3,4). GPT-2 is outperformed by our pretrained-only model PL-NoFT. This is a fair comparison since these two models are both trained only on unlabeled corpus, although GPT-2 has general purposes. Thus, we argue that our pretraining strategy works better in news summarization.\n\n3. We conduct human evaluation on 100 randomly chosen articles/summaries in CNN/DailyMail dataset and show the results in Section 5.7. Our model outperforms the pointer-generator network and the result is statistically significant.\n\nAnswers to minor questions:\n1. There are a few articles with excessively long content, and we filter them mainly to reduce memory consumption. Also some leading sentences are very short (like \"What?\") and we filter them as they contain little information and are unlikely to be a good summary. As the pretraining task is very time-consuming, we did not try other settings. We add these information in Section 5.2.\n\n2. We conduct statistical test on the ROUGE-scores and update all tables. Most of our results are statistically significant with p-value < 0.05, compared with previous best result."}, "signatures": ["ICLR.cc/2020/Conference/Paper97/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper97/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["chezhu@microsoft.com", "zy99@stanford.edu", "rogmyr@microsoft.com", "nzeng@microsoft.com", "xdh@microsoft.com"], "title": "Make Lead Bias in Your Favor: A Simple and Effective Method for News Summarization", "authors": ["Chenguang Zhu", "Ziyi Yang", "Robert Gmyr", "Michael Zeng", "Xuedong Huang"], "pdf": "/pdf/e3dff444dd04f47f69ddb67445f421206d9599d9.pdf", "TL;DR": "A method to leverage lead bias in large-scale pretraining for abstractive news summarization", "abstract": "Lead bias is a common phenomenon in news summarization, where early parts of an article often contain the most salient information. While many algorithms exploit this fact in summary generation, it has a detrimental effect on teaching the model to discriminate and extract important information. We propose that the lead bias can be leveraged in a simple and effective way in our favor to pretrain abstractive news summarization models on large-scale unlabelled corpus: predicting the leading sentences using the rest of an article. Via careful data cleaning and filtering, our transformer-based pretrained model without any finetuning achieves remarkable results over various news summarization tasks. With further finetuning, our model outperforms many competitive baseline models. For example, the pretrained model without finetuning outperforms pointer-generator network on CNN/DailyMail dataset. The finetuned model obtains 3.2% higher ROUGE-1, 1.6% higher ROUGE-2 and 2.1% higher ROUGE-L scores than the best baseline model on XSum dataset.", "code": "https://www.dropbox.com/s/3qbcpfwtzfowzmo/PretrainAbsSum.zip?dl=0", "keywords": ["Summarization", "Pretraining"], "paperhash": "zhu|make_lead_bias_in_your_favor_a_simple_and_effective_method_for_news_summarization", "original_pdf": "/attachment/22ab27f4eaaea2ab0302660b4df8c335a2d75cd5.pdf", "_bibtex": "@misc{\nzhu2020make,\ntitle={Make Lead Bias in Your Favor: A Simple and Effective Method for News Summarization},\nauthor={Chenguang Zhu and Ziyi Yang and Robert Gmyr and Michael Zeng and Xuedong Huang},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxAY34YwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryxAY34YwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper97/Authors", "ICLR.cc/2020/Conference/Paper97/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper97/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper97/Reviewers", "ICLR.cc/2020/Conference/Paper97/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper97/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper97/Authors|ICLR.cc/2020/Conference/Paper97/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176445, "tmdate": 1576860555206, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper97/Authors", "ICLR.cc/2020/Conference/Paper97/Reviewers", "ICLR.cc/2020/Conference/Paper97/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper97/-/Official_Comment"}}}, {"id": "SyeFcDAYiB", "original": null, "number": 3, "cdate": 1573672848948, "ddate": null, "tcdate": 1573672848948, "tmdate": 1573672848948, "tddate": null, "forum": "ryxAY34YwB", "replyto": "HkgUIDhoFH", "invitation": "ICLR.cc/2020/Conference/Paper97/-/Official_Comment", "content": {"title": "Thanks for your comments!", "comment": "For your questions:\n1. Our idea is novel in that we use the structural bias in our favor to pretrain a large-scale news summarization model. For XSUM dataset, according to its paper, it uses the accompanying summary which is the first sentence in BOLD font in the article. It is specially editted by editors to summarize an article. So it's a special format of BBC articles which facilitates fast reading, not just the first sentence of the article. Therefore, the LEAD-1 baseline, which is also used in XSUM and BERTSUM papers, is a valid leading part of the article.\n\n2. The filtering is based on non-stopping words, and the overlapping ratio implies the amount of carried-over information. As an evidence, we compute the overlapping ratio of non-stopping words between golden summary and the article in CNN/DailyMail dataset and the median is 0.87 (which is not surprising due to lead bias). Then we compute the same ratio between the first 3 sentences and the rest of the article in CNN/DailyMail, and the median is 0.77. Thus, a high overlapping ratio is typical for summaries written by human. We add this information in Section 5.2.\n\n3. We conduct human evaluation on 100 randomly chosen articles/summaries in CNN/DailyMail dataset and show the results in Section 5.7. Our model outperforms the pointer-generator network and the result is statistically significant.\n\n4. Positional bias helps with fast news reading, and it also eases the creation of news summarization datasets. However, the positional bias lowers the bar for model to comprehend the article for summary generation. Positional bias is not present in many tasks other than news, like document or dialogue transcript summarization. Therefore, we propose our method to take advantage of lead bias and train a model that could summarize based more on the content, instead of the position. "}, "signatures": ["ICLR.cc/2020/Conference/Paper97/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper97/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["chezhu@microsoft.com", "zy99@stanford.edu", "rogmyr@microsoft.com", "nzeng@microsoft.com", "xdh@microsoft.com"], "title": "Make Lead Bias in Your Favor: A Simple and Effective Method for News Summarization", "authors": ["Chenguang Zhu", "Ziyi Yang", "Robert Gmyr", "Michael Zeng", "Xuedong Huang"], "pdf": "/pdf/e3dff444dd04f47f69ddb67445f421206d9599d9.pdf", "TL;DR": "A method to leverage lead bias in large-scale pretraining for abstractive news summarization", "abstract": "Lead bias is a common phenomenon in news summarization, where early parts of an article often contain the most salient information. While many algorithms exploit this fact in summary generation, it has a detrimental effect on teaching the model to discriminate and extract important information. We propose that the lead bias can be leveraged in a simple and effective way in our favor to pretrain abstractive news summarization models on large-scale unlabelled corpus: predicting the leading sentences using the rest of an article. Via careful data cleaning and filtering, our transformer-based pretrained model without any finetuning achieves remarkable results over various news summarization tasks. With further finetuning, our model outperforms many competitive baseline models. For example, the pretrained model without finetuning outperforms pointer-generator network on CNN/DailyMail dataset. The finetuned model obtains 3.2% higher ROUGE-1, 1.6% higher ROUGE-2 and 2.1% higher ROUGE-L scores than the best baseline model on XSum dataset.", "code": "https://www.dropbox.com/s/3qbcpfwtzfowzmo/PretrainAbsSum.zip?dl=0", "keywords": ["Summarization", "Pretraining"], "paperhash": "zhu|make_lead_bias_in_your_favor_a_simple_and_effective_method_for_news_summarization", "original_pdf": "/attachment/22ab27f4eaaea2ab0302660b4df8c335a2d75cd5.pdf", "_bibtex": "@misc{\nzhu2020make,\ntitle={Make Lead Bias in Your Favor: A Simple and Effective Method for News Summarization},\nauthor={Chenguang Zhu and Ziyi Yang and Robert Gmyr and Michael Zeng and Xuedong Huang},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxAY34YwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryxAY34YwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper97/Authors", "ICLR.cc/2020/Conference/Paper97/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper97/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper97/Reviewers", "ICLR.cc/2020/Conference/Paper97/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper97/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper97/Authors|ICLR.cc/2020/Conference/Paper97/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176445, "tmdate": 1576860555206, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper97/Authors", "ICLR.cc/2020/Conference/Paper97/Reviewers", "ICLR.cc/2020/Conference/Paper97/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper97/-/Official_Comment"}}}, {"id": "HyeUIvRFsr", "original": null, "number": 1, "cdate": 1573672782240, "ddate": null, "tcdate": 1573672782240, "tmdate": 1573672782240, "tddate": null, "forum": "ryxAY34YwB", "replyto": "ryxAY34YwB", "invitation": "ICLR.cc/2020/Conference/Paper97/-/Official_Comment", "content": {"title": "Revised paper", "comment": "Dear reviewers, we have added the following updates into the revised version of our paper:\n1. We add in human evaluation results in Section 5.7.\n2. We add experiments on DUC-2003 and DUC-2004 (Table 4).\n3. We remove introductory details of transformers from Section 4.\n4. We explain our choice of pretraining hyper-parameters in Section 5.2.\n5. We add statistical test for all of our results (Table 1-5, **: p-value<0.01, *: p-value<0.05)"}, "signatures": ["ICLR.cc/2020/Conference/Paper97/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper97/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["chezhu@microsoft.com", "zy99@stanford.edu", "rogmyr@microsoft.com", "nzeng@microsoft.com", "xdh@microsoft.com"], "title": "Make Lead Bias in Your Favor: A Simple and Effective Method for News Summarization", "authors": ["Chenguang Zhu", "Ziyi Yang", "Robert Gmyr", "Michael Zeng", "Xuedong Huang"], "pdf": "/pdf/e3dff444dd04f47f69ddb67445f421206d9599d9.pdf", "TL;DR": "A method to leverage lead bias in large-scale pretraining for abstractive news summarization", "abstract": "Lead bias is a common phenomenon in news summarization, where early parts of an article often contain the most salient information. While many algorithms exploit this fact in summary generation, it has a detrimental effect on teaching the model to discriminate and extract important information. We propose that the lead bias can be leveraged in a simple and effective way in our favor to pretrain abstractive news summarization models on large-scale unlabelled corpus: predicting the leading sentences using the rest of an article. Via careful data cleaning and filtering, our transformer-based pretrained model without any finetuning achieves remarkable results over various news summarization tasks. With further finetuning, our model outperforms many competitive baseline models. For example, the pretrained model without finetuning outperforms pointer-generator network on CNN/DailyMail dataset. The finetuned model obtains 3.2% higher ROUGE-1, 1.6% higher ROUGE-2 and 2.1% higher ROUGE-L scores than the best baseline model on XSum dataset.", "code": "https://www.dropbox.com/s/3qbcpfwtzfowzmo/PretrainAbsSum.zip?dl=0", "keywords": ["Summarization", "Pretraining"], "paperhash": "zhu|make_lead_bias_in_your_favor_a_simple_and_effective_method_for_news_summarization", "original_pdf": "/attachment/22ab27f4eaaea2ab0302660b4df8c335a2d75cd5.pdf", "_bibtex": "@misc{\nzhu2020make,\ntitle={Make Lead Bias in Your Favor: A Simple and Effective Method for News Summarization},\nauthor={Chenguang Zhu and Ziyi Yang and Robert Gmyr and Michael Zeng and Xuedong Huang},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxAY34YwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryxAY34YwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper97/Authors", "ICLR.cc/2020/Conference/Paper97/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper97/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper97/Reviewers", "ICLR.cc/2020/Conference/Paper97/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper97/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper97/Authors|ICLR.cc/2020/Conference/Paper97/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176445, "tmdate": 1576860555206, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper97/Authors", "ICLR.cc/2020/Conference/Paper97/Reviewers", "ICLR.cc/2020/Conference/Paper97/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper97/-/Official_Comment"}}}, {"id": "HkgUIDhoFH", "original": null, "number": 2, "cdate": 1571698510131, "ddate": null, "tcdate": 1571698510131, "tmdate": 1572972639008, "tddate": null, "forum": "ryxAY34YwB", "replyto": "ryxAY34YwB", "invitation": "ICLR.cc/2020/Conference/Paper97/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper suggests generating a large news summarization dataset by taking advantage of the fact that in news articles it is often the case that first few sentences contain the most important information. I have the following criticisms of this paper:\n- the idea is not novel. The XSUM dataset cited had used this to create a large dataset based on BBC articles as the editorial guidelines are such that the first sentence is a summary of the article. The lead1 baseline doesn't make sense, as it is the actual reference of the dataset. As implemented, it actually picks the second sentence of the original article, and unsurprisingly works worse than the lead-X for the other two datasets.\n- the filtering based on word overlap between the initial sentences and the rest of the document means that the training dataset will encourage models copying words; good summaries don't have high word overlap necessarily.\n- no human evaluation is not conducted; ROUGE indicates small differences, but it can't be trusted without confirmation by human evaluation\n- I don't agree that using positional information is bad for the models. The point is that we need to do better than that, but we should still take it into account"}, "signatures": ["ICLR.cc/2020/Conference/Paper97/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper97/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["chezhu@microsoft.com", "zy99@stanford.edu", "rogmyr@microsoft.com", "nzeng@microsoft.com", "xdh@microsoft.com"], "title": "Make Lead Bias in Your Favor: A Simple and Effective Method for News Summarization", "authors": ["Chenguang Zhu", "Ziyi Yang", "Robert Gmyr", "Michael Zeng", "Xuedong Huang"], "pdf": "/pdf/e3dff444dd04f47f69ddb67445f421206d9599d9.pdf", "TL;DR": "A method to leverage lead bias in large-scale pretraining for abstractive news summarization", "abstract": "Lead bias is a common phenomenon in news summarization, where early parts of an article often contain the most salient information. While many algorithms exploit this fact in summary generation, it has a detrimental effect on teaching the model to discriminate and extract important information. We propose that the lead bias can be leveraged in a simple and effective way in our favor to pretrain abstractive news summarization models on large-scale unlabelled corpus: predicting the leading sentences using the rest of an article. Via careful data cleaning and filtering, our transformer-based pretrained model without any finetuning achieves remarkable results over various news summarization tasks. With further finetuning, our model outperforms many competitive baseline models. For example, the pretrained model without finetuning outperforms pointer-generator network on CNN/DailyMail dataset. The finetuned model obtains 3.2% higher ROUGE-1, 1.6% higher ROUGE-2 and 2.1% higher ROUGE-L scores than the best baseline model on XSum dataset.", "code": "https://www.dropbox.com/s/3qbcpfwtzfowzmo/PretrainAbsSum.zip?dl=0", "keywords": ["Summarization", "Pretraining"], "paperhash": "zhu|make_lead_bias_in_your_favor_a_simple_and_effective_method_for_news_summarization", "original_pdf": "/attachment/22ab27f4eaaea2ab0302660b4df8c335a2d75cd5.pdf", "_bibtex": "@misc{\nzhu2020make,\ntitle={Make Lead Bias in Your Favor: A Simple and Effective Method for News Summarization},\nauthor={Chenguang Zhu and Ziyi Yang and Robert Gmyr and Michael Zeng and Xuedong Huang},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxAY34YwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ryxAY34YwB", "replyto": "ryxAY34YwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper97/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper97/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575604647798, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper97/Reviewers"], "noninvitees": [], "tcdate": 1570237757120, "tmdate": 1575604647810, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper97/-/Official_Review"}}}, {"id": "HJxhRlhaKS", "original": null, "number": 3, "cdate": 1571827923824, "ddate": null, "tcdate": 1571827923824, "tmdate": 1572972638973, "tddate": null, "forum": "ryxAY34YwB", "replyto": "ryxAY34YwB", "invitation": "ICLR.cc/2020/Conference/Paper97/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "For news article it has been know since long that the LEAD baseline is a tough-to-beat competitor. This paper proposes to use this knowledge as self-supervision for training summarization models.\nFor this the author download and clean 3 years of news articles and use this to (pre-)train a Tranformer model. This alone already provides a competitive baseline, which is greatly improved by fine-tuning it on 3 different data-sets. While the data-set can probably not be released, it would be very helpful to have the model available for reproductivity and benchmarking.\n\nThe paper is clear and well-written. Section 4 I believe is very redundant for an ICLR audience and could be moved to the appendix, making space for a more detailed analysis. One criticism is that the paper is light: the author show that a simple idea works (this is a compliment), but I would have expected to have used the remaining space for ablation studies or a discussion on where this leads.\nOne important point which I would like to see before recommending acceptance is a comparison to know if what is helping is just more data, or the summarization objective. Using lots of more data beats all those numbers (see BERTSUM paper, Liu & Lapata 2019). The comparison I am missing is training BERT on your crawled data-set, and use that for BERTSUM (the code is available). If that helps as much as the summarization pre-training then it would be disappointing but a nice result in favor of language modeling. If not, then it is a strong support for your idea. \n\nTwo other points which should at least be discussed, as it gives the impression of cherry-picking results instead: \n1/ Table 1 is recall; Table 2&3 F1. Why?\n2/ The parameters of fine-tuning of the appendix vary wildly depending on the data-set (in particular, the difference in the width of the beam search is striking). Was this optimized on test-data? What is the sensitivity of the summaries to this?\n\nI do not understand the last two sentences of Sect 4 (\"A candidate word leading...). Could you explain?"}, "signatures": ["ICLR.cc/2020/Conference/Paper97/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper97/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["chezhu@microsoft.com", "zy99@stanford.edu", "rogmyr@microsoft.com", "nzeng@microsoft.com", "xdh@microsoft.com"], "title": "Make Lead Bias in Your Favor: A Simple and Effective Method for News Summarization", "authors": ["Chenguang Zhu", "Ziyi Yang", "Robert Gmyr", "Michael Zeng", "Xuedong Huang"], "pdf": "/pdf/e3dff444dd04f47f69ddb67445f421206d9599d9.pdf", "TL;DR": "A method to leverage lead bias in large-scale pretraining for abstractive news summarization", "abstract": "Lead bias is a common phenomenon in news summarization, where early parts of an article often contain the most salient information. While many algorithms exploit this fact in summary generation, it has a detrimental effect on teaching the model to discriminate and extract important information. We propose that the lead bias can be leveraged in a simple and effective way in our favor to pretrain abstractive news summarization models on large-scale unlabelled corpus: predicting the leading sentences using the rest of an article. Via careful data cleaning and filtering, our transformer-based pretrained model without any finetuning achieves remarkable results over various news summarization tasks. With further finetuning, our model outperforms many competitive baseline models. For example, the pretrained model without finetuning outperforms pointer-generator network on CNN/DailyMail dataset. The finetuned model obtains 3.2% higher ROUGE-1, 1.6% higher ROUGE-2 and 2.1% higher ROUGE-L scores than the best baseline model on XSum dataset.", "code": "https://www.dropbox.com/s/3qbcpfwtzfowzmo/PretrainAbsSum.zip?dl=0", "keywords": ["Summarization", "Pretraining"], "paperhash": "zhu|make_lead_bias_in_your_favor_a_simple_and_effective_method_for_news_summarization", "original_pdf": "/attachment/22ab27f4eaaea2ab0302660b4df8c335a2d75cd5.pdf", "_bibtex": "@misc{\nzhu2020make,\ntitle={Make Lead Bias in Your Favor: A Simple and Effective Method for News Summarization},\nauthor={Chenguang Zhu and Ziyi Yang and Robert Gmyr and Michael Zeng and Xuedong Huang},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxAY34YwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ryxAY34YwB", "replyto": "ryxAY34YwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper97/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper97/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575604647798, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper97/Reviewers"], "noninvitees": [], "tcdate": 1570237757120, "tmdate": 1575604647810, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper97/-/Official_Review"}}}], "count": 11}