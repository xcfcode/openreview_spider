{"notes": [{"id": "B1fpDsAqt7", "original": "B1gJqb3FK7", "number": 305, "cdate": 1538087780906, "ddate": null, "tcdate": 1538087780906, "tmdate": 1548440441160, "tddate": null, "forum": "B1fpDsAqt7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Visual Reasoning by Progressive Module Networks", "abstract": "Humans learn to solve tasks of increasing complexity by building on top of previously acquired knowledge. Typically, there exists a natural progression in the tasks that we learn \u2013 most do not require completely independent solutions, but can be broken down into simpler subtasks. We propose to represent a solver for each task as a neural module that calls existing modules (solvers for simpler tasks) in a functional program-like manner. Lower modules are a black box to the calling module, and communicate only via a query and an output. Thus, a module for a new task learns to query existing modules and composes their outputs in order to produce its own output. Our model effectively combines previous skill-sets, does not suffer from forgetting, and is fully differentiable. We test our model in learning a set of visual reasoning tasks, and demonstrate improved performances in all tasks by learning progressively. By evaluating the reasoning process using human judges, we show that our model is more interpretable than an attention-based baseline.\n", "keywords": [], "authorids": ["seung@cs.toronto.edu", "makarand@cs.toronto.edu", "fidler@cs.toronto.edu"], "authors": ["Seung Wook Kim", "Makarand Tapaswi", "Sanja Fidler"], "pdf": "/pdf/7ba81b43cb43e5d88e3829f767d714f8de03f675.pdf", "paperhash": "kim|visual_reasoning_by_progressive_module_networks", "_bibtex": "@inproceedings{\nkim2018visual,\ntitle={Visual Reasoning by Progressive Module Networks},\nauthor={Seung Wook Kim and Makarand Tapaswi and Sanja Fidler},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1fpDsAqt7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 16, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "SJgH-q_egE", "original": null, "number": 1, "cdate": 1544747516767, "ddate": null, "tcdate": 1544747516767, "tmdate": 1545354515811, "tddate": null, "forum": "B1fpDsAqt7", "replyto": "B1fpDsAqt7", "invitation": "ICLR.cc/2019/Conference/-/Paper305/Meta_Review", "content": {"metareview": "Important problem (modular & interpretable approaches for VQA and visual reasoning); well-written manuscript, sensible approach. Paper was reviewed by three experts. Initially there were some concerns but after the author response and reviewer discussion, all three unanimously recommend acceptance. ", "confidence": "5: The area chair is absolutely certain", "recommendation": "Accept (Poster)", "title": "meta-review"}, "signatures": ["ICLR.cc/2019/Conference/Paper305/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper305/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visual Reasoning by Progressive Module Networks", "abstract": "Humans learn to solve tasks of increasing complexity by building on top of previously acquired knowledge. Typically, there exists a natural progression in the tasks that we learn \u2013 most do not require completely independent solutions, but can be broken down into simpler subtasks. We propose to represent a solver for each task as a neural module that calls existing modules (solvers for simpler tasks) in a functional program-like manner. Lower modules are a black box to the calling module, and communicate only via a query and an output. Thus, a module for a new task learns to query existing modules and composes their outputs in order to produce its own output. Our model effectively combines previous skill-sets, does not suffer from forgetting, and is fully differentiable. We test our model in learning a set of visual reasoning tasks, and demonstrate improved performances in all tasks by learning progressively. By evaluating the reasoning process using human judges, we show that our model is more interpretable than an attention-based baseline.\n", "keywords": [], "authorids": ["seung@cs.toronto.edu", "makarand@cs.toronto.edu", "fidler@cs.toronto.edu"], "authors": ["Seung Wook Kim", "Makarand Tapaswi", "Sanja Fidler"], "pdf": "/pdf/7ba81b43cb43e5d88e3829f767d714f8de03f675.pdf", "paperhash": "kim|visual_reasoning_by_progressive_module_networks", "_bibtex": "@inproceedings{\nkim2018visual,\ntitle={Visual Reasoning by Progressive Module Networks},\nauthor={Seung Wook Kim and Makarand Tapaswi and Sanja Fidler},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1fpDsAqt7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper305/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353263142, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1fpDsAqt7", "replyto": "B1fpDsAqt7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper305/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper305/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper305/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353263142}}}, {"id": "SkeIsnWJlN", "original": null, "number": 16, "cdate": 1544653982157, "ddate": null, "tcdate": 1544653982157, "tmdate": 1544653982157, "tddate": null, "forum": "B1fpDsAqt7", "replyto": "HygakjZyl4", "invitation": "ICLR.cc/2019/Conference/-/Paper305/Official_Comment", "content": {"title": "Thank you.", "comment": "We have added the GT captions experiment in the 'plug-and-play architecture' paragraph in Section 4.1.\n\nThank you again for your great suggestion!"}, "signatures": ["ICLR.cc/2019/Conference/Paper305/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper305/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper305/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visual Reasoning by Progressive Module Networks", "abstract": "Humans learn to solve tasks of increasing complexity by building on top of previously acquired knowledge. Typically, there exists a natural progression in the tasks that we learn \u2013 most do not require completely independent solutions, but can be broken down into simpler subtasks. We propose to represent a solver for each task as a neural module that calls existing modules (solvers for simpler tasks) in a functional program-like manner. Lower modules are a black box to the calling module, and communicate only via a query and an output. Thus, a module for a new task learns to query existing modules and composes their outputs in order to produce its own output. Our model effectively combines previous skill-sets, does not suffer from forgetting, and is fully differentiable. We test our model in learning a set of visual reasoning tasks, and demonstrate improved performances in all tasks by learning progressively. By evaluating the reasoning process using human judges, we show that our model is more interpretable than an attention-based baseline.\n", "keywords": [], "authorids": ["seung@cs.toronto.edu", "makarand@cs.toronto.edu", "fidler@cs.toronto.edu"], "authors": ["Seung Wook Kim", "Makarand Tapaswi", "Sanja Fidler"], "pdf": "/pdf/7ba81b43cb43e5d88e3829f767d714f8de03f675.pdf", "paperhash": "kim|visual_reasoning_by_progressive_module_networks", "_bibtex": "@inproceedings{\nkim2018visual,\ntitle={Visual Reasoning by Progressive Module Networks},\nauthor={Seung Wook Kim and Makarand Tapaswi and Sanja Fidler},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1fpDsAqt7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper305/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615268, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1fpDsAqt7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper305/Authors", "ICLR.cc/2019/Conference/Paper305/Reviewers", "ICLR.cc/2019/Conference/Paper305/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper305/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper305/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper305/Authors|ICLR.cc/2019/Conference/Paper305/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper305/Reviewers", "ICLR.cc/2019/Conference/Paper305/Authors", "ICLR.cc/2019/Conference/Paper305/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615268}}}, {"id": "HygakjZyl4", "original": null, "number": 15, "cdate": 1544653541495, "ddate": null, "tcdate": 1544653541495, "tmdate": 1544653541495, "tddate": null, "forum": "B1fpDsAqt7", "replyto": "SJlY63MSpX", "invitation": "ICLR.cc/2019/Conference/-/Paper305/Official_Comment", "content": {"title": "Thanks for the response", "comment": "Thanks for the response! It is interesting that the GT captions can help improve the VQA performance, please incorporate the results and update the manuscripts accordingly.\n\nAgain, I think this is a good paper and will not change my rating."}, "signatures": ["ICLR.cc/2019/Conference/Paper305/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper305/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper305/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visual Reasoning by Progressive Module Networks", "abstract": "Humans learn to solve tasks of increasing complexity by building on top of previously acquired knowledge. Typically, there exists a natural progression in the tasks that we learn \u2013 most do not require completely independent solutions, but can be broken down into simpler subtasks. We propose to represent a solver for each task as a neural module that calls existing modules (solvers for simpler tasks) in a functional program-like manner. Lower modules are a black box to the calling module, and communicate only via a query and an output. Thus, a module for a new task learns to query existing modules and composes their outputs in order to produce its own output. Our model effectively combines previous skill-sets, does not suffer from forgetting, and is fully differentiable. We test our model in learning a set of visual reasoning tasks, and demonstrate improved performances in all tasks by learning progressively. By evaluating the reasoning process using human judges, we show that our model is more interpretable than an attention-based baseline.\n", "keywords": [], "authorids": ["seung@cs.toronto.edu", "makarand@cs.toronto.edu", "fidler@cs.toronto.edu"], "authors": ["Seung Wook Kim", "Makarand Tapaswi", "Sanja Fidler"], "pdf": "/pdf/7ba81b43cb43e5d88e3829f767d714f8de03f675.pdf", "paperhash": "kim|visual_reasoning_by_progressive_module_networks", "_bibtex": "@inproceedings{\nkim2018visual,\ntitle={Visual Reasoning by Progressive Module Networks},\nauthor={Seung Wook Kim and Makarand Tapaswi and Sanja Fidler},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1fpDsAqt7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper305/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615268, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1fpDsAqt7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper305/Authors", "ICLR.cc/2019/Conference/Paper305/Reviewers", "ICLR.cc/2019/Conference/Paper305/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper305/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper305/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper305/Authors|ICLR.cc/2019/Conference/Paper305/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper305/Reviewers", "ICLR.cc/2019/Conference/Paper305/Authors", "ICLR.cc/2019/Conference/Paper305/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615268}}}, {"id": "SJgyyPTv3m", "original": null, "number": 1, "cdate": 1541031639392, "ddate": null, "tcdate": 1541031639392, "tmdate": 1543315317124, "tddate": null, "forum": "B1fpDsAqt7", "replyto": "B1fpDsAqt7", "invitation": "ICLR.cc/2019/Conference/-/Paper305/Official_Review", "content": {"title": "Official review.", "review": "Summary:\nThe authors propose a network for VQA incorporating hand-crafted modules and their hierarchy, each of which is a network for a high-level vision task. Some modules may share the same sub-modules at a different level in the module hierarchy. Each module is individually (not end-to-end) trained with a dataset containing a dedicated annotation for their high-level tasks. The proposed model shows comparable scores to the existing models.\n\nPresentation and clarity:\nThe paper is well written and easy to follow and contains reasonable experiments for understanding the proposed method.\n\nOriginality and significance:\nI mainly do not agree that this work generalizes NMN. Instead, I believe that this work is a special case of NMN where the modules and their hierarchy are manually defined based on the authors' intuition. Meanwhile, the proposed network architecture is static, and thus the main idea of having multiple modules in a network is not novel as other approaches using static network architectures such as [A] also facilitate multiple modules for different sub-procedures (e.g., RNN for questions and CNN for image) and sometimes share modules in multiple stages too. The main difference between this and previous works is that the modules in this work deal with high-level tasks chosen by the authors. I am not convinced that designing the modules with high-level tasks is a better choice over designing modules that are less task-specific. Rather, I see more drawbacks as the proposed method requires multiple datasets with diverse task-specific annotation. Also, the modules and their connectivity are less scalable and extendable as they are not learned.\n\nConsidering all the model and dataset complexities, the improvements over black-box models are mostly marginal. The main benefits we get from all these complexities are the interpretability. However, for many modules, the interpretability comes from indirect signals that are often not clear how to interpret for the question answering. On the other hand, the manually designed sub-tasks may cause error propagation in the network as these modules are not directly optimized for the final objective.\n\nSome questions and comments:\nI do not understand why it is necessary to have the image captioning module as it does not directly relate to the question answering. Moreover, the caption itself is generated without conditioning on the question.\n\n[A] Yang, Zichao, et al. \"Stacked attention networks for image question answering.\" CVPR 2016.\n\n\n== After discussion phase\nBased on the rebuttal and additional experiments that clarified and resolved my questions, I change my initial rating.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper305/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Visual Reasoning by Progressive Module Networks", "abstract": "Humans learn to solve tasks of increasing complexity by building on top of previously acquired knowledge. Typically, there exists a natural progression in the tasks that we learn \u2013 most do not require completely independent solutions, but can be broken down into simpler subtasks. We propose to represent a solver for each task as a neural module that calls existing modules (solvers for simpler tasks) in a functional program-like manner. Lower modules are a black box to the calling module, and communicate only via a query and an output. Thus, a module for a new task learns to query existing modules and composes their outputs in order to produce its own output. Our model effectively combines previous skill-sets, does not suffer from forgetting, and is fully differentiable. We test our model in learning a set of visual reasoning tasks, and demonstrate improved performances in all tasks by learning progressively. By evaluating the reasoning process using human judges, we show that our model is more interpretable than an attention-based baseline.\n", "keywords": [], "authorids": ["seung@cs.toronto.edu", "makarand@cs.toronto.edu", "fidler@cs.toronto.edu"], "authors": ["Seung Wook Kim", "Makarand Tapaswi", "Sanja Fidler"], "pdf": "/pdf/7ba81b43cb43e5d88e3829f767d714f8de03f675.pdf", "paperhash": "kim|visual_reasoning_by_progressive_module_networks", "_bibtex": "@inproceedings{\nkim2018visual,\ntitle={Visual Reasoning by Progressive Module Networks},\nauthor={Seung Wook Kim and Makarand Tapaswi and Sanja Fidler},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1fpDsAqt7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper305/Official_Review", "cdate": 1542234492156, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1fpDsAqt7", "replyto": "B1fpDsAqt7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper305/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335694480, "tmdate": 1552335694480, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper305/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SkeHNjEqRQ", "original": null, "number": 10, "cdate": 1543289644766, "ddate": null, "tcdate": 1543289644766, "tmdate": 1543289644766, "tddate": null, "forum": "B1fpDsAqt7", "replyto": "BkgO1UGq07", "invitation": "ICLR.cc/2019/Conference/-/Paper305/Official_Comment", "content": {"title": "Thank you for your suggestions.", "comment": "What you suggested above is also a good way of looking at it.\nWe tested it out:\n\n                             ||   Mobj used            |        Mobj not used . |            Total\n----------------------------------------------------------------------------------------------------------\nMobj correct       ||  A. 15005  (95%)   |         C. 777 (5%)          |       15782 (100%)  \nMobj incorrect    ||  B.  7764  (58%)    |         D. 5513 (42%)     |       13277 (100%)\n\nWhen Mobj's output is equal to the ground-truth output (the first row), it is almost always used. When its output is not correct, it is less likely to be used. Note that B. may seem high because the questions studied (questions starting with 'what') likely need some information from objects, and also Mobj is doing a 1600-way classification, therefore it is not always easy for Mobj to be correct (e.g. if the ground truth answer is 'monitor' and Mobj outputs 'tv', it would be considered as incorrect).\n\n\nWe thank the reviewer for all the interesting suggestions. It seems that the concerns have been addressed, and we hope it will reflect in the reviewer's final rating."}, "signatures": ["ICLR.cc/2019/Conference/Paper305/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper305/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper305/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visual Reasoning by Progressive Module Networks", "abstract": "Humans learn to solve tasks of increasing complexity by building on top of previously acquired knowledge. Typically, there exists a natural progression in the tasks that we learn \u2013 most do not require completely independent solutions, but can be broken down into simpler subtasks. We propose to represent a solver for each task as a neural module that calls existing modules (solvers for simpler tasks) in a functional program-like manner. Lower modules are a black box to the calling module, and communicate only via a query and an output. Thus, a module for a new task learns to query existing modules and composes their outputs in order to produce its own output. Our model effectively combines previous skill-sets, does not suffer from forgetting, and is fully differentiable. We test our model in learning a set of visual reasoning tasks, and demonstrate improved performances in all tasks by learning progressively. By evaluating the reasoning process using human judges, we show that our model is more interpretable than an attention-based baseline.\n", "keywords": [], "authorids": ["seung@cs.toronto.edu", "makarand@cs.toronto.edu", "fidler@cs.toronto.edu"], "authors": ["Seung Wook Kim", "Makarand Tapaswi", "Sanja Fidler"], "pdf": "/pdf/7ba81b43cb43e5d88e3829f767d714f8de03f675.pdf", "paperhash": "kim|visual_reasoning_by_progressive_module_networks", "_bibtex": "@inproceedings{\nkim2018visual,\ntitle={Visual Reasoning by Progressive Module Networks},\nauthor={Seung Wook Kim and Makarand Tapaswi and Sanja Fidler},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1fpDsAqt7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper305/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615268, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1fpDsAqt7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper305/Authors", "ICLR.cc/2019/Conference/Paper305/Reviewers", "ICLR.cc/2019/Conference/Paper305/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper305/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper305/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper305/Authors|ICLR.cc/2019/Conference/Paper305/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper305/Reviewers", "ICLR.cc/2019/Conference/Paper305/Authors", "ICLR.cc/2019/Conference/Paper305/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615268}}}, {"id": "BkgO1UGq07", "original": null, "number": 9, "cdate": 1543280096005, "ddate": null, "tcdate": 1543280096005, "tmdate": 1543280096005, "tddate": null, "forum": "B1fpDsAqt7", "replyto": "Hygs-DpF0X", "invitation": "ICLR.cc/2019/Conference/-/Paper305/Official_Comment", "content": {"title": "Thank you for all your efforts", "comment": "I deeply appreciate all your efforts for the extra experiments. Those tables also show meaningful statistics. However, I think you can simply measure what portion of questions are answered using M_obj (probably with soft weight thresholded) when M_obj outputs correct/incorrect answers. I think this can simply show if the model actually learns to ignore modules when they produce incorrect outputs.\n\nAgain, I thank the authors for these additional experiments and I also want to point out that the authors resolved my questions and concerns."}, "signatures": ["ICLR.cc/2019/Conference/Paper305/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper305/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper305/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visual Reasoning by Progressive Module Networks", "abstract": "Humans learn to solve tasks of increasing complexity by building on top of previously acquired knowledge. Typically, there exists a natural progression in the tasks that we learn \u2013 most do not require completely independent solutions, but can be broken down into simpler subtasks. We propose to represent a solver for each task as a neural module that calls existing modules (solvers for simpler tasks) in a functional program-like manner. Lower modules are a black box to the calling module, and communicate only via a query and an output. Thus, a module for a new task learns to query existing modules and composes their outputs in order to produce its own output. Our model effectively combines previous skill-sets, does not suffer from forgetting, and is fully differentiable. We test our model in learning a set of visual reasoning tasks, and demonstrate improved performances in all tasks by learning progressively. By evaluating the reasoning process using human judges, we show that our model is more interpretable than an attention-based baseline.\n", "keywords": [], "authorids": ["seung@cs.toronto.edu", "makarand@cs.toronto.edu", "fidler@cs.toronto.edu"], "authors": ["Seung Wook Kim", "Makarand Tapaswi", "Sanja Fidler"], "pdf": "/pdf/7ba81b43cb43e5d88e3829f767d714f8de03f675.pdf", "paperhash": "kim|visual_reasoning_by_progressive_module_networks", "_bibtex": "@inproceedings{\nkim2018visual,\ntitle={Visual Reasoning by Progressive Module Networks},\nauthor={Seung Wook Kim and Makarand Tapaswi and Sanja Fidler},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1fpDsAqt7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper305/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615268, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1fpDsAqt7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper305/Authors", "ICLR.cc/2019/Conference/Paper305/Reviewers", "ICLR.cc/2019/Conference/Paper305/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper305/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper305/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper305/Authors|ICLR.cc/2019/Conference/Paper305/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper305/Reviewers", "ICLR.cc/2019/Conference/Paper305/Authors", "ICLR.cc/2019/Conference/Paper305/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615268}}}, {"id": "Hygs-DpF0X", "original": null, "number": 8, "cdate": 1543259907127, "ddate": null, "tcdate": 1543259907127, "tmdate": 1543259907127, "tddate": null, "forum": "B1fpDsAqt7", "replyto": "S1enb1hOCm", "invitation": "ICLR.cc/2019/Conference/-/Paper305/Official_Comment", "content": {"title": "Additional experiment", "comment": "We agree that inspecting module outputs is beneficial. It is hard to directly identify intermediate erroneous outputs as we do not have ground truth labels, so we conducted a new experiment that indirectly measures how erroneous outputs affect the model performance. \n\nBefore that, we would like to clarify PMN chooses which modules to use, not which outputs to use (see line 5 in Algorithm 1). In the previous response, we stated \u201cPMN combines information from the lower modules through importance scores. For a given set of questions, if a module produces erroneous outputs, PMN can learn to ignore such outputs and rely on other modules or it\u2019s own residual.\u201d. This might have caused the confusion, and we apologize. We meant to say - if PMN learns that certain modules\u2019 outputs are not useful for solving a particular type of questions, it can learn to ignore those modules (with low importance scores) for that type of questions.\n\nAs there are more than 200K questions in the val set, we focus on questions starting with \u2018what\u2019 and divide questions into 10 types. This amounts to 74K questions. For this experiment, we choose one submodule Mobj (the object classifier) of Mvqa and analyze 1) the type of questions when this module is used/not used, and 2) how Mobj\u2019s output affects the final performance.\n\nNote: we say Mobj is \u2018used\u2019 if its importance score is higher than any other module.\n\n1) We show the number of questions Mobj is used/not used depending on the question types:\n\n             Q types      ||\n(eg. what+\u2018time\u2019) || #Qs Mobj used    | #Qs Mobj not used \n---------------------------------------------------------------------------------------\ncolor                       ||            102              |            23628 \nkind                        ||          8264              |             1760\ntime                       ||             15               |             1731\nsport                      ||          1435             |               5 \nanimal                   ||          1093             |              46 \nis the number      ||            326             | .           1727\nis on, in the          ||           3903            |              14\nbrand                    ||             896            |              43\nis, are                    ||          24025           |             931\nroom                     ||             931            |               0\n\n\nThis shows PMN correctly learns to use/not use Mobj for certain types of questions (e.g. kind, sport, animal uses Mobj while color, time, number does not).\n\n2) To see the effect of erroneous outputs, we select 23K questions out of the 74K questions whose ground-truth answer is in the vocabulary of Mobj and Mobj is used. That is, it is more likely that Mobj\u2019s output is useful to infer the final answer.\nLet gt_ans be the ground-truth answer and obj_ans be the output label of Mobj.\n\n                                           ||  final answer correct  |  final answer incorrect |  Total\n------------------------------------------------------------------------------------------------------------------------\n#Qs Obj_ans == gt_ans  ||   A.    12315  (82%)       |  B.   2690  (18%)             |  15005 (100%)\n#Qs Obj_ans != gt_ans   ||   C.    3233    (42%)       |  D.   4531  (58%)             |  7764 (100%)\n\nThe numbers in the above table correspond to the number of questions. For example, A is the number of questions where Mobj\u2019s output is equal to the ground-truth answer, leading to the correct final answer. \n\nFor a large number of questions (A. 54%), PMN behaves as expected with correct Mobj\u2019s outputs leading to correct Mvqa\u2019s outputs. As A > B and C < D, we can see that if Mobj produces the correct answer, it is more likely that the final answer of Mvqa is correct, and if Mobj produces an erroneous output, it is more likely that the final answer is incorrect. We would like to stress that this is a weakness of not only our model but any other deep learning model. If some part of a model\u2019s computation path is erroneous, it is more likely that the model performance suffers.\n\n\nB and C show cases where even though Mobj\u2019s output was correct/incorrect, the final answer is incorrect/correct. One reason could be other contributing modules. Since PMN\u2019s selection process is \u2018soft\u2019 using softmax of importance scores, other modules could confuse Mvqa. This could be moderated in future works by employing a hard selection process with gumbel softmax or reinforcement learning. It is also interesting to see that A (82%) is much greater than B (18%) and the difference between C (42%) and D (58%) is not as large. It suggests that other contributing modules might be helpful in situations where Mobj\u2019s output is incorrect.\n\nIt is not easy to directly quantify the effect of erroneous outputs without ground-truth labels. We hope you find these experiments helpful, and if you have another experiment in mind to show this more clearly, we would be happy to do it.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper305/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper305/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper305/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visual Reasoning by Progressive Module Networks", "abstract": "Humans learn to solve tasks of increasing complexity by building on top of previously acquired knowledge. Typically, there exists a natural progression in the tasks that we learn \u2013 most do not require completely independent solutions, but can be broken down into simpler subtasks. We propose to represent a solver for each task as a neural module that calls existing modules (solvers for simpler tasks) in a functional program-like manner. Lower modules are a black box to the calling module, and communicate only via a query and an output. Thus, a module for a new task learns to query existing modules and composes their outputs in order to produce its own output. Our model effectively combines previous skill-sets, does not suffer from forgetting, and is fully differentiable. We test our model in learning a set of visual reasoning tasks, and demonstrate improved performances in all tasks by learning progressively. By evaluating the reasoning process using human judges, we show that our model is more interpretable than an attention-based baseline.\n", "keywords": [], "authorids": ["seung@cs.toronto.edu", "makarand@cs.toronto.edu", "fidler@cs.toronto.edu"], "authors": ["Seung Wook Kim", "Makarand Tapaswi", "Sanja Fidler"], "pdf": "/pdf/7ba81b43cb43e5d88e3829f767d714f8de03f675.pdf", "paperhash": "kim|visual_reasoning_by_progressive_module_networks", "_bibtex": "@inproceedings{\nkim2018visual,\ntitle={Visual Reasoning by Progressive Module Networks},\nauthor={Seung Wook Kim and Makarand Tapaswi and Sanja Fidler},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1fpDsAqt7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper305/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615268, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1fpDsAqt7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper305/Authors", "ICLR.cc/2019/Conference/Paper305/Reviewers", "ICLR.cc/2019/Conference/Paper305/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper305/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper305/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper305/Authors|ICLR.cc/2019/Conference/Paper305/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper305/Reviewers", "ICLR.cc/2019/Conference/Paper305/Authors", "ICLR.cc/2019/Conference/Paper305/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615268}}}, {"id": "S1enb1hOCm", "original": null, "number": 7, "cdate": 1543188228096, "ddate": null, "tcdate": 1543188228096, "tmdate": 1543188228096, "tddate": null, "forum": "B1fpDsAqt7", "replyto": "BylkHTN_RX", "invitation": "ICLR.cc/2019/Conference/-/Paper305/Official_Comment", "content": {"title": "Thanks for the response", "comment": "1. I missed those results at the time of my response and now agree that the progression property of PMNs actually improves the performances of higher-level tasks. In addition to that, I also accept that the plug-and-play architecture is beneficial for further improvement in the future.\n\n2. As the authors argue, the human evaluation involves the quality of the intermediate outputs. But this measure involves many other factors and is hard to understand the effects of the erroneous outputs. I believe including this kind of experiments would be still beneficial."}, "signatures": ["ICLR.cc/2019/Conference/Paper305/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper305/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper305/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visual Reasoning by Progressive Module Networks", "abstract": "Humans learn to solve tasks of increasing complexity by building on top of previously acquired knowledge. Typically, there exists a natural progression in the tasks that we learn \u2013 most do not require completely independent solutions, but can be broken down into simpler subtasks. We propose to represent a solver for each task as a neural module that calls existing modules (solvers for simpler tasks) in a functional program-like manner. Lower modules are a black box to the calling module, and communicate only via a query and an output. Thus, a module for a new task learns to query existing modules and composes their outputs in order to produce its own output. Our model effectively combines previous skill-sets, does not suffer from forgetting, and is fully differentiable. We test our model in learning a set of visual reasoning tasks, and demonstrate improved performances in all tasks by learning progressively. By evaluating the reasoning process using human judges, we show that our model is more interpretable than an attention-based baseline.\n", "keywords": [], "authorids": ["seung@cs.toronto.edu", "makarand@cs.toronto.edu", "fidler@cs.toronto.edu"], "authors": ["Seung Wook Kim", "Makarand Tapaswi", "Sanja Fidler"], "pdf": "/pdf/7ba81b43cb43e5d88e3829f767d714f8de03f675.pdf", "paperhash": "kim|visual_reasoning_by_progressive_module_networks", "_bibtex": "@inproceedings{\nkim2018visual,\ntitle={Visual Reasoning by Progressive Module Networks},\nauthor={Seung Wook Kim and Makarand Tapaswi and Sanja Fidler},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1fpDsAqt7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper305/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615268, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1fpDsAqt7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper305/Authors", "ICLR.cc/2019/Conference/Paper305/Reviewers", "ICLR.cc/2019/Conference/Paper305/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper305/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper305/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper305/Authors|ICLR.cc/2019/Conference/Paper305/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper305/Reviewers", "ICLR.cc/2019/Conference/Paper305/Authors", "ICLR.cc/2019/Conference/Paper305/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615268}}}, {"id": "BylkHTN_RX", "original": null, "number": 6, "cdate": 1543159094535, "ddate": null, "tcdate": 1543159094535, "tmdate": 1543159094535, "tddate": null, "forum": "B1fpDsAqt7", "replyto": "rygVoa6DRQ", "invitation": "ICLR.cc/2019/Conference/-/Paper305/Official_Comment", "content": {"title": "Response to the concerns", "comment": "Thank you for the response. We address your concerns below.\n\n\n1. Performance \n\nWe believe the experiments section (Section 4.1), in particular Table 1 (relationship detection), 2 (counting) and 3 (visual question answering), shows how tasks benefit from utilizing submodules and learning progressively. Also, PMN for the VQA task achieves 64.68% validation accuracy which is an increase of more than 2.5% over the baseline (Table 3). This is without exploiting the additional questions from Visual Genome (which most other state-of-the-art models use, e.g. see Teney et al. 2018) and we do not employ additional data augmentation (Jiang et al. 2018).  Since PMN is a general framework, we also do not use advanced VQA-specific techniques such as bilinear attention (Kim et al. 2018) or ensembling different architectures (Jiang et al. 2018).\n\n\n\n\n2. Erroneous outputs\n\nWe agree that PMN might not be perfect in avoiding erroneous outputs, just like how children make mistakes in reasoning leading to incorrect conclusions. The human evaluation (Section 4.2) measures the quality of intermediate reasoning process as PMN with incorrect explanations are penalized by human workers. As shown in Table 5, PMN gets more part marks than the baseline even when its final output is incorrect. This shows intermediate outputs are good.  Moreover, as stated in the previous response, PMN can be naturally used as a plug-and-play model (see the \"plug-and-play architecture\" added in the experiments (Section 4.1)). Therefore, PMN has a very promising way of improving itself by utilizing better and less erroneous submodules, unlike other models. The query-and-answer level communication within PMN also makes it possible to have human feedback.\n\n\n\n- Teney et al. Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge,  2018\n- Kim et al. Bilinear attention networks, 2018\n- Jiang et al. Pythia v0. 1: the winning entry to the VQA challenge 2018, 2018 \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper305/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper305/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper305/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visual Reasoning by Progressive Module Networks", "abstract": "Humans learn to solve tasks of increasing complexity by building on top of previously acquired knowledge. Typically, there exists a natural progression in the tasks that we learn \u2013 most do not require completely independent solutions, but can be broken down into simpler subtasks. We propose to represent a solver for each task as a neural module that calls existing modules (solvers for simpler tasks) in a functional program-like manner. Lower modules are a black box to the calling module, and communicate only via a query and an output. Thus, a module for a new task learns to query existing modules and composes their outputs in order to produce its own output. Our model effectively combines previous skill-sets, does not suffer from forgetting, and is fully differentiable. We test our model in learning a set of visual reasoning tasks, and demonstrate improved performances in all tasks by learning progressively. By evaluating the reasoning process using human judges, we show that our model is more interpretable than an attention-based baseline.\n", "keywords": [], "authorids": ["seung@cs.toronto.edu", "makarand@cs.toronto.edu", "fidler@cs.toronto.edu"], "authors": ["Seung Wook Kim", "Makarand Tapaswi", "Sanja Fidler"], "pdf": "/pdf/7ba81b43cb43e5d88e3829f767d714f8de03f675.pdf", "paperhash": "kim|visual_reasoning_by_progressive_module_networks", "_bibtex": "@inproceedings{\nkim2018visual,\ntitle={Visual Reasoning by Progressive Module Networks},\nauthor={Seung Wook Kim and Makarand Tapaswi and Sanja Fidler},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1fpDsAqt7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper305/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615268, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1fpDsAqt7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper305/Authors", "ICLR.cc/2019/Conference/Paper305/Reviewers", "ICLR.cc/2019/Conference/Paper305/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper305/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper305/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper305/Authors|ICLR.cc/2019/Conference/Paper305/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper305/Reviewers", "ICLR.cc/2019/Conference/Paper305/Authors", "ICLR.cc/2019/Conference/Paper305/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615268}}}, {"id": "rygVoa6DRQ", "original": null, "number": 5, "cdate": 1543130524071, "ddate": null, "tcdate": 1543130524071, "tmdate": 1543130593047, "tddate": null, "forum": "B1fpDsAqt7", "replyto": "SyeoH17BTQ", "invitation": "ICLR.cc/2019/Conference/-/Paper305/Official_Comment", "content": {"title": "Thank you for your response", "comment": "I appreciate the response from the reviewer.\n\nI accept the authors' arguments that PMNs are different from NMNs and static models, although I believe NMNs are proposed more generally and can still be designed with progression property. And, the explicit architecture design with the progression and the experiments on it should still be counted as the authors' contribution. Also, I also agree that the interpretability of the model is improved compared to previous methods.\n\nHowever, there are several things I still do not agree. To argue that it is beneficial to build a module for a task on top of other modules for lower-level tasks, the higher-level modules should show significantly improved performance compared to the other approaches. Otherwise, it can be thought that having an end-to-end black box model should be enough without the progression even though the concept of progression seems to be similar to the human learning process. To confirm this, I believe the improvements for intermediate tasks should be also measured since the learning process is \"progressive\".\n\nI agree that it is possible that the model has the capability to learn to avoid utilizing erroneous intermediate outputs. However, I am not sure if the model can correctly identify the erroneous outputs through unsupervised attention model. It is especially doubtful as the erroneous outputs are usually produced from hard examples. So the argument of model's capability for not utilizing erroneous outputs by the attention process should be experimentally verified.\n\nOverall, I agree that I underestimated some of the paper's contributions and thus want to raise my score. But, at the same time, I still see some weak points in the arguments that may be resolved by more experiments."}, "signatures": ["ICLR.cc/2019/Conference/Paper305/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper305/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper305/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visual Reasoning by Progressive Module Networks", "abstract": "Humans learn to solve tasks of increasing complexity by building on top of previously acquired knowledge. Typically, there exists a natural progression in the tasks that we learn \u2013 most do not require completely independent solutions, but can be broken down into simpler subtasks. We propose to represent a solver for each task as a neural module that calls existing modules (solvers for simpler tasks) in a functional program-like manner. Lower modules are a black box to the calling module, and communicate only via a query and an output. Thus, a module for a new task learns to query existing modules and composes their outputs in order to produce its own output. Our model effectively combines previous skill-sets, does not suffer from forgetting, and is fully differentiable. We test our model in learning a set of visual reasoning tasks, and demonstrate improved performances in all tasks by learning progressively. By evaluating the reasoning process using human judges, we show that our model is more interpretable than an attention-based baseline.\n", "keywords": [], "authorids": ["seung@cs.toronto.edu", "makarand@cs.toronto.edu", "fidler@cs.toronto.edu"], "authors": ["Seung Wook Kim", "Makarand Tapaswi", "Sanja Fidler"], "pdf": "/pdf/7ba81b43cb43e5d88e3829f767d714f8de03f675.pdf", "paperhash": "kim|visual_reasoning_by_progressive_module_networks", "_bibtex": "@inproceedings{\nkim2018visual,\ntitle={Visual Reasoning by Progressive Module Networks},\nauthor={Seung Wook Kim and Makarand Tapaswi and Sanja Fidler},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1fpDsAqt7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper305/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615268, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1fpDsAqt7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper305/Authors", "ICLR.cc/2019/Conference/Paper305/Reviewers", "ICLR.cc/2019/Conference/Paper305/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper305/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper305/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper305/Authors|ICLR.cc/2019/Conference/Paper305/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper305/Reviewers", "ICLR.cc/2019/Conference/Paper305/Authors", "ICLR.cc/2019/Conference/Paper305/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615268}}}, {"id": "H1xozmAE0m", "original": null, "number": 4, "cdate": 1542935314924, "ddate": null, "tcdate": 1542935314924, "tmdate": 1542935314924, "tddate": null, "forum": "B1fpDsAqt7", "replyto": "B1fpDsAqt7", "invitation": "ICLR.cc/2019/Conference/-/Paper305/Official_Comment", "content": {"title": "Revision", "comment": "\n1. We clarified some notational ambiguities pointed out by Reviewer 3.\n\n2. We added an experiment demonstrating the plug-and-play nature of PMN as suggested by Reviewer 2.\n\n3. As we do not claim PMN is a generalization of Neural Module Networks, we edited the paper to remove misunderstanding our wording may have caused. \n\nWe thank all reviewers for their valuable feedbacks."}, "signatures": ["ICLR.cc/2019/Conference/Paper305/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper305/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper305/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visual Reasoning by Progressive Module Networks", "abstract": "Humans learn to solve tasks of increasing complexity by building on top of previously acquired knowledge. Typically, there exists a natural progression in the tasks that we learn \u2013 most do not require completely independent solutions, but can be broken down into simpler subtasks. We propose to represent a solver for each task as a neural module that calls existing modules (solvers for simpler tasks) in a functional program-like manner. Lower modules are a black box to the calling module, and communicate only via a query and an output. Thus, a module for a new task learns to query existing modules and composes their outputs in order to produce its own output. Our model effectively combines previous skill-sets, does not suffer from forgetting, and is fully differentiable. We test our model in learning a set of visual reasoning tasks, and demonstrate improved performances in all tasks by learning progressively. By evaluating the reasoning process using human judges, we show that our model is more interpretable than an attention-based baseline.\n", "keywords": [], "authorids": ["seung@cs.toronto.edu", "makarand@cs.toronto.edu", "fidler@cs.toronto.edu"], "authors": ["Seung Wook Kim", "Makarand Tapaswi", "Sanja Fidler"], "pdf": "/pdf/7ba81b43cb43e5d88e3829f767d714f8de03f675.pdf", "paperhash": "kim|visual_reasoning_by_progressive_module_networks", "_bibtex": "@inproceedings{\nkim2018visual,\ntitle={Visual Reasoning by Progressive Module Networks},\nauthor={Seung Wook Kim and Makarand Tapaswi and Sanja Fidler},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1fpDsAqt7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper305/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615268, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1fpDsAqt7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper305/Authors", "ICLR.cc/2019/Conference/Paper305/Reviewers", "ICLR.cc/2019/Conference/Paper305/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper305/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper305/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper305/Authors|ICLR.cc/2019/Conference/Paper305/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper305/Reviewers", "ICLR.cc/2019/Conference/Paper305/Authors", "ICLR.cc/2019/Conference/Paper305/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615268}}}, {"id": "SyeoH17BTQ", "original": null, "number": 3, "cdate": 1541906242976, "ddate": null, "tcdate": 1541906242976, "tmdate": 1541906242976, "tddate": null, "forum": "B1fpDsAqt7", "replyto": "SJgyyPTv3m", "invitation": "ICLR.cc/2019/Conference/-/Paper305/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "Thanks for the feedback. We hope to convince you that PMN is a framework to learn continuously from previous knowledge, and not just a solution to VQA.\n\n\n1. PMN vs. NMN\n- We agree that PMN is not a generalization of NMN. However, we argue that it is not a special case of NMN either. We highlight two significant differences:\n\na) Progression: PMN is a framework that learns to do (visual) reasoning by starting with simpler tasks (object labels) and building up to more complex tasks (VQA). This is an important step towards building intelligent agents that continuously learn new tasks by using the tasks they are already good at. There is no sense of progression in NMN and everything is learned from scratch. From the experiments related to the low data regime (Table 6), we see that PMN can make efficient use of available data to learn to communicate with experts and solve the task.\n\nb) Task Modules: Communication in PMN is at the *query-answer level*. Since module outputs are answers to other (human-designed) tasks, the process is easier to interpret (more human-readable). On the other hand, NMN\u2019s modules, as showcased in their paper, contained one or two conv. or linear layers and solve sub-functions such as attention or classification.\n\nWe edited the paper to remove misunderstanding our wording may have caused. \n\n\n2. PMN vs. Static models\n- In addition to the above differences to NMN, PMN has three more significant differences to static models ([A]).\n\na) Dynamic choice of modules: PMN\u2019s state and importance function choose which modules to consider. This can go even further, and using a threshold, we may not execute some modules at all (during inference). Static models always go through the same steps.\n\nb) Information propagates in a tree-like fashion: A high-level module asks for some information from a lower module, that further produces queries for its own lower modules (see Fig. 1). For example, VQA calls counting which calls relationship detection.\n\nc) Direct querying of lower modules: PMN produces explicit queries for lower tasks using the query transmitter Q (see Fig. 2, step 3). Based on the current state, it can choose to ask information about a specific query that may be helpful to answer the question.\n\nThe inter-module communication and the computation graph are all learned.\n\n\n3. Modules with high-level tasks, Multiple datasets\n- Task-specific models are the default practice in machine learning. However, to have an intelligent agent that can learn a host of tasks over time it is beneficial to have the tasks build on top of each other (See 1. (a)). This is similar to the human learning process where kids first learn object names and attributes, followed by increasingly harder tasks such as counting. Datasets in the community are typically focused on one specific task, and thus we are forced to use multiple datasets and annotations to progressively learn visual reasoning abilities. \n\n\n4. Minor improvements over black-box models, Interpretability\n-  We encourage the reviewer to look at our paper in a more holistic view. Our main aim is to mimic challenging real scenarios in which we want to train agents to learn many tasks, increasing in their complexity, rather than squeezing numbers for one particular dataset. The VQA dataset has a strong bias that is exploited by black-box models [B]. This is one of the reasons why PMN performs much better than the other models in the low data regime (Table 6) - the gap gets smaller with more data as black-box models learn to exploit dataset bias. The paper showcases how to learn tasks by progression and modularity. We hope this is interesting to readers beyond just the numbers. The fact that the performance also improves is a nice bonus.\n \nWith respect to interpretability, the query-answer communication within PMN is more human-readable than other models (See 1. (b)). For example, as shown in Fig. 2, it produces queries for the relationship module (bird, \u2018on top of\u2019) and the relationship module returns the box corresponding to 'bench'. Other examples such as Fig. 3, App. C&D, and the human evaluation concretely support the fact that generated outputs are much more interpretable than standard attention maps. \n\n\n5. Error propagation\n- PMN combines information from the lower modules through importance scores. For a given set of questions, if a module produces erroneous outputs, PMN can learn to ignore such outputs and rely on other modules or it\u2019s own residual.\n\n\n6. Captioning for VQA\n- In captioning, one describes the most salient aspects of the picture. For example, a caption \u201ca married couple walking on the beach\u201d, provides answers to several questions ('are they married?', 'where are they?', etc). If the actual question relates to these, then the VQA module can simply leverage the information. In response to R2, we evaluated how well VQA can leverage ground-truth captions and see a large 2.0% improvement.\n\n\n[B] Agrawal, et al. Overcoming Priors for VQA. 2018\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper305/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper305/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper305/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visual Reasoning by Progressive Module Networks", "abstract": "Humans learn to solve tasks of increasing complexity by building on top of previously acquired knowledge. Typically, there exists a natural progression in the tasks that we learn \u2013 most do not require completely independent solutions, but can be broken down into simpler subtasks. We propose to represent a solver for each task as a neural module that calls existing modules (solvers for simpler tasks) in a functional program-like manner. Lower modules are a black box to the calling module, and communicate only via a query and an output. Thus, a module for a new task learns to query existing modules and composes their outputs in order to produce its own output. Our model effectively combines previous skill-sets, does not suffer from forgetting, and is fully differentiable. We test our model in learning a set of visual reasoning tasks, and demonstrate improved performances in all tasks by learning progressively. By evaluating the reasoning process using human judges, we show that our model is more interpretable than an attention-based baseline.\n", "keywords": [], "authorids": ["seung@cs.toronto.edu", "makarand@cs.toronto.edu", "fidler@cs.toronto.edu"], "authors": ["Seung Wook Kim", "Makarand Tapaswi", "Sanja Fidler"], "pdf": "/pdf/7ba81b43cb43e5d88e3829f767d714f8de03f675.pdf", "paperhash": "kim|visual_reasoning_by_progressive_module_networks", "_bibtex": "@inproceedings{\nkim2018visual,\ntitle={Visual Reasoning by Progressive Module Networks},\nauthor={Seung Wook Kim and Makarand Tapaswi and Sanja Fidler},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1fpDsAqt7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper305/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615268, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1fpDsAqt7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper305/Authors", "ICLR.cc/2019/Conference/Paper305/Reviewers", "ICLR.cc/2019/Conference/Paper305/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper305/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper305/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper305/Authors|ICLR.cc/2019/Conference/Paper305/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper305/Reviewers", "ICLR.cc/2019/Conference/Paper305/Authors", "ICLR.cc/2019/Conference/Paper305/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615268}}}, {"id": "SJlY63MSpX", "original": null, "number": 2, "cdate": 1541905601101, "ddate": null, "tcdate": 1541905601101, "tmdate": 1541905601101, "tddate": null, "forum": "B1fpDsAqt7", "replyto": "r1eaptK5hm", "invitation": "ICLR.cc/2019/Conference/-/Paper305/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "We thank the reviewer for the comments and feedback. We will also include the suggested experiment that shows the plug-and-play nature of PMN.\n\n1. Residual modules\n- Residual modules are small neural networks (e.g., an MLP for Mvqa, Sec. 3.4, (4)) that a task module may use when other lower level modules are incapable of providing a solution to a given query. For example, consider the question \u201cis this person going to be happy?\u201d on an image of a person opening a present. Lower level modules of Mvqa may not be sufficient to solve the question. Therefore, Mvqa would make use of its residual module, which would essentially learn to \u201cpick up\u201d all queries that lower level modules cannot answer. \n\n2. Effect of fine-tuning\n- While it might be beneficial to fine-tune the modules for a specific parent task we want each module to be an expert for their own task as it facilitates a plug-and-play architecture. Fine-tuning may push the modules towards blindly improving parent module\u2019s performance but (i) badly affect interpretability of inputs and outputs; and (ii) may also reduce the lower module\u2019s performance on its own task. Most importantly, it would not scale with the number of tasks, as for each task the agent would need to keep several fine-tuned modules of the lower tasks in memory.\n\n3. Feeding in the ground-truth\n- Thanks for this great suggestion. We performed an experiment where we evaluate the benefits that the VQA model may achieve by using ground-truth captions instead of captions generated by the caption module. Our preliminary experiments show a gain of about 2.0% which is a relatively high gain for VQA. \nThis points to important properties of the PMN allowing human-in-the-loop type of continual learning, where a human teacher can pinpoint flaws in the reasoning process and potentially help the model to fix them.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper305/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper305/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper305/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visual Reasoning by Progressive Module Networks", "abstract": "Humans learn to solve tasks of increasing complexity by building on top of previously acquired knowledge. Typically, there exists a natural progression in the tasks that we learn \u2013 most do not require completely independent solutions, but can be broken down into simpler subtasks. We propose to represent a solver for each task as a neural module that calls existing modules (solvers for simpler tasks) in a functional program-like manner. Lower modules are a black box to the calling module, and communicate only via a query and an output. Thus, a module for a new task learns to query existing modules and composes their outputs in order to produce its own output. Our model effectively combines previous skill-sets, does not suffer from forgetting, and is fully differentiable. We test our model in learning a set of visual reasoning tasks, and demonstrate improved performances in all tasks by learning progressively. By evaluating the reasoning process using human judges, we show that our model is more interpretable than an attention-based baseline.\n", "keywords": [], "authorids": ["seung@cs.toronto.edu", "makarand@cs.toronto.edu", "fidler@cs.toronto.edu"], "authors": ["Seung Wook Kim", "Makarand Tapaswi", "Sanja Fidler"], "pdf": "/pdf/7ba81b43cb43e5d88e3829f767d714f8de03f675.pdf", "paperhash": "kim|visual_reasoning_by_progressive_module_networks", "_bibtex": "@inproceedings{\nkim2018visual,\ntitle={Visual Reasoning by Progressive Module Networks},\nauthor={Seung Wook Kim and Makarand Tapaswi and Sanja Fidler},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1fpDsAqt7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper305/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615268, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1fpDsAqt7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper305/Authors", "ICLR.cc/2019/Conference/Paper305/Reviewers", "ICLR.cc/2019/Conference/Paper305/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper305/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper305/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper305/Authors|ICLR.cc/2019/Conference/Paper305/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper305/Reviewers", "ICLR.cc/2019/Conference/Paper305/Authors", "ICLR.cc/2019/Conference/Paper305/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615268}}}, {"id": "Sklfd2fSpX", "original": null, "number": 1, "cdate": 1541905514341, "ddate": null, "tcdate": 1541905514341, "tmdate": 1541905514341, "tddate": null, "forum": "B1fpDsAqt7", "replyto": "H1gUmqkh3Q", "invitation": "ICLR.cc/2019/Conference/-/Paper305/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "We thank the reviewer for the comments and feedback. We will certainly clarify them in the final paper.\n\n1. Title of the paper\n- We agree that the main highest-level task that we show is VQA, even though our method is more general. Our title aimed to convey that we showcase PMN on a host of increasingly complex visual reasoning tasks such as relationship detection, counting, and captioning, as well as VQA. Our focus is on VQA as it happens to be one of the most complex visual reasoning tasks that can leverage each of the (relatively) simpler tasks.\n\n2. Description of variables\n- Thanks for the feedback. Epsilon means the environment, some of the definitions are written in Section 3, but we agree that it can be somewhat challenging to interpret as there are many variables. We edited the text to address variables more gently and to explain the arrow sign. \n\n3. Query for the relationship module\n- The relationship module is fed an N-dimensional (corresponding to N image regions) one-hot vector as input during training. When it is called by other task modules (such as counting), an N-dimensional probability vector is computed using softmax on image regions (see A.4, point 3) and not using the importance scores. This acts as a soft version of the one-hot sampled vector so that we can backpropagate gradients.\n\n4. CIDEr score of captioning \n- That may be true to some extent. However, we think that explicit label information might still be useful since the visual features (environment) are from Faster RCNN and contain diverse information such as edges, background, color, and size.\n\n5 and 6. Comparison with SOTA models for counting and relationship detection\n- To the best of our knowledge, Zhang et al. (2018) is the SOTA method on counting in the context of visual question answering. Our counting module leverages that but achieves higher performance on the number questions - 54.39% with ensembling and 52.12% without vs. 51.62% of Zhang et al. (2018). Note that 51.62% of Zhang et al. (2018) is from a single highly regularized model that provides small gains from ensembling. This shows that additional modules help. Kim et al. (2018) which is concurrent to our work shows similar performance. For the relationship detection task, other works such as Lu et al. (2016) unfortunately have a different setup which makes direct comparison difficult.\n\n7. Table 4, accuracies are from Zhang et al. 2018\n- Yes, the numbers are from their paper. One possible explanation for this could be their use of high regularization for a single model instead of ensembling. Thus, the performance improvement from training on the train set (evaluating on validation) to training on train+val (evaluating on test-dev) is smaller.\n\n(Zhang et al. 2018) Learning to Count Objects in Natural Images for Visual Question Answering\n(Kim et al. 2018) Bilinear Attention Networks\n(Lu et al. 2016) Visual Relationship Detection with Language Priors\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper305/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper305/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper305/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visual Reasoning by Progressive Module Networks", "abstract": "Humans learn to solve tasks of increasing complexity by building on top of previously acquired knowledge. Typically, there exists a natural progression in the tasks that we learn \u2013 most do not require completely independent solutions, but can be broken down into simpler subtasks. We propose to represent a solver for each task as a neural module that calls existing modules (solvers for simpler tasks) in a functional program-like manner. Lower modules are a black box to the calling module, and communicate only via a query and an output. Thus, a module for a new task learns to query existing modules and composes their outputs in order to produce its own output. Our model effectively combines previous skill-sets, does not suffer from forgetting, and is fully differentiable. We test our model in learning a set of visual reasoning tasks, and demonstrate improved performances in all tasks by learning progressively. By evaluating the reasoning process using human judges, we show that our model is more interpretable than an attention-based baseline.\n", "keywords": [], "authorids": ["seung@cs.toronto.edu", "makarand@cs.toronto.edu", "fidler@cs.toronto.edu"], "authors": ["Seung Wook Kim", "Makarand Tapaswi", "Sanja Fidler"], "pdf": "/pdf/7ba81b43cb43e5d88e3829f767d714f8de03f675.pdf", "paperhash": "kim|visual_reasoning_by_progressive_module_networks", "_bibtex": "@inproceedings{\nkim2018visual,\ntitle={Visual Reasoning by Progressive Module Networks},\nauthor={Seung Wook Kim and Makarand Tapaswi and Sanja Fidler},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1fpDsAqt7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper305/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615268, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1fpDsAqt7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper305/Authors", "ICLR.cc/2019/Conference/Paper305/Reviewers", "ICLR.cc/2019/Conference/Paper305/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper305/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper305/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper305/Authors|ICLR.cc/2019/Conference/Paper305/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper305/Reviewers", "ICLR.cc/2019/Conference/Paper305/Authors", "ICLR.cc/2019/Conference/Paper305/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615268}}}, {"id": "H1gUmqkh3Q", "original": null, "number": 3, "cdate": 1541302813559, "ddate": null, "tcdate": 1541302813559, "tmdate": 1541534108273, "tddate": null, "forum": "B1fpDsAqt7", "replyto": "B1fpDsAqt7", "invitation": "ICLR.cc/2019/Conference/-/Paper305/Official_Review", "content": {"title": "very interesting work, but a lot of the details are not clear. ", "review": "[Summary]\nThis paper presents a multi-task learning approach for VQA that represent a solver for each task as a neural module that calls existing modules in a program manner. The authors manually design the task hierarchy and propose a progressive module network to recursive calls the lower modules and gather the information by soft-attention. The final prediction uses all the states and question to infer the final answers. The authors verify the effectiveness of the proposed method on the performance of different tasks and modules. Experiment on VQA shows the proposed model benefits from utilizing different modules. The authors also qualitatively show the model's reasoning process and human study on judging answering quality.\n\n[Strength]\n1. The proposed method is novel and explores to use the existing modules as a black box for visual question answering.  This is different from most existing work. \n\n2: By examing different modules, the proposed method is more interpretable compare to canonical methods. \n\n3: The experiment results are good, especially for the counting problem. \n\n[Weakness] \n1. The title of the paper is \"visual reasoning by progressive module networks.\" The title may be a little overstated since the major task is focused on visual question answering (VQA).  \n\n2. Annotation is not clear in this paper. For example, on page 3, Query transmitter and receiver, \"the output o_k = M_k(q_k) received from M_k is modified using receiver function as v_k = R_{k->n}(s^t, o_k). \" There are multiple new variables in this paragraph, without specifying the dimension and meaning for each attribute, it's really hard to understand. On page 4, State update function, what is the meaning of variable \"Epsilon\" in the equation? From the supplementary, it seems Epsilon means the environment? \n\n3. On the object counting task, the query transmitter needs to produce a query for a relationship module. The authors mentioned that this is softly calculated by softmax on the importance score. Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case? \n\n4. The cider score of image captioning is 109 compared to the baseline 108. The explanation is the COCO dataset has a fixed set of 80 object categories and does not benefit from training the diverse data. Since the input visual feature is the same, the only difference is the proposed model has additional label embedding as input. My assumption is the visual feature already contains the label information for image captioning. \n\n5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results. \n\n6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods? \n\n7. In Table 4, the accuracy of number on Zhang et.al is 49.39, which is higher than other methods, while on test-dev, the accuracy is 51.62, which is lower than others. Is the number right? ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper305/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visual Reasoning by Progressive Module Networks", "abstract": "Humans learn to solve tasks of increasing complexity by building on top of previously acquired knowledge. Typically, there exists a natural progression in the tasks that we learn \u2013 most do not require completely independent solutions, but can be broken down into simpler subtasks. We propose to represent a solver for each task as a neural module that calls existing modules (solvers for simpler tasks) in a functional program-like manner. Lower modules are a black box to the calling module, and communicate only via a query and an output. Thus, a module for a new task learns to query existing modules and composes their outputs in order to produce its own output. Our model effectively combines previous skill-sets, does not suffer from forgetting, and is fully differentiable. We test our model in learning a set of visual reasoning tasks, and demonstrate improved performances in all tasks by learning progressively. By evaluating the reasoning process using human judges, we show that our model is more interpretable than an attention-based baseline.\n", "keywords": [], "authorids": ["seung@cs.toronto.edu", "makarand@cs.toronto.edu", "fidler@cs.toronto.edu"], "authors": ["Seung Wook Kim", "Makarand Tapaswi", "Sanja Fidler"], "pdf": "/pdf/7ba81b43cb43e5d88e3829f767d714f8de03f675.pdf", "paperhash": "kim|visual_reasoning_by_progressive_module_networks", "_bibtex": "@inproceedings{\nkim2018visual,\ntitle={Visual Reasoning by Progressive Module Networks},\nauthor={Seung Wook Kim and Makarand Tapaswi and Sanja Fidler},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1fpDsAqt7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper305/Official_Review", "cdate": 1542234492156, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1fpDsAqt7", "replyto": "B1fpDsAqt7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper305/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335694480, "tmdate": 1552335694480, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper305/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "r1eaptK5hm", "original": null, "number": 2, "cdate": 1541212612938, "ddate": null, "tcdate": 1541212612938, "tmdate": 1541534108026, "tddate": null, "forum": "B1fpDsAqt7", "replyto": "B1fpDsAqt7", "invitation": "ICLR.cc/2019/Conference/-/Paper305/Official_Review", "content": {"title": "The paper proposes to combine different task-level modules in a progressive way for the task for VQA. The model achieved state-of-the-art performance.", "review": "The paper proposes to learn task-level modules progressively to perform the task of VQA. Such task-level modules include object/attribute prediction, image captioning, relationship detection, object counting, and finally VQA model. The benefit of using modules for reasoning allows one to visualize the reasoning process more easily to understand the model better. The results are mainly shown on VQA 2.0 set, with a good amount of analysis.\n\n- I think overall this is a good paper, with clear organization, detailed description of the approach, solid analysis of the approach and cool visualization. I especially appreciate that analysis is done taking into consideration of extra computation cost of the large model; the extra data used for visual relationship detection. I do not have major comments about the paper itself, although I did not check the technical details super carefully.\n\n- One thing I am confused about is the residual model, which seems quite important for the pipeline but I cannot find details describing it and much analysis on this component. \n\n- I am in general curious to see if it will be beneficial to fine-tune the modules themselves can further improve performance. It maybe hard to do it entirely end-to-end, but maybe it is fine to fine-tune just a few top layers (like what Jiang et al did)? \n\n- One great benefit of having a module-based model is feed in the *ground truth* output for some of the modules. For example, what benefit we can get if we have perfect object detection? Where can we get if we have perfect relationships? This can help us not only better understand the models, but also the dataset (VQA) and the task in general. ", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper305/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visual Reasoning by Progressive Module Networks", "abstract": "Humans learn to solve tasks of increasing complexity by building on top of previously acquired knowledge. Typically, there exists a natural progression in the tasks that we learn \u2013 most do not require completely independent solutions, but can be broken down into simpler subtasks. We propose to represent a solver for each task as a neural module that calls existing modules (solvers for simpler tasks) in a functional program-like manner. Lower modules are a black box to the calling module, and communicate only via a query and an output. Thus, a module for a new task learns to query existing modules and composes their outputs in order to produce its own output. Our model effectively combines previous skill-sets, does not suffer from forgetting, and is fully differentiable. We test our model in learning a set of visual reasoning tasks, and demonstrate improved performances in all tasks by learning progressively. By evaluating the reasoning process using human judges, we show that our model is more interpretable than an attention-based baseline.\n", "keywords": [], "authorids": ["seung@cs.toronto.edu", "makarand@cs.toronto.edu", "fidler@cs.toronto.edu"], "authors": ["Seung Wook Kim", "Makarand Tapaswi", "Sanja Fidler"], "pdf": "/pdf/7ba81b43cb43e5d88e3829f767d714f8de03f675.pdf", "paperhash": "kim|visual_reasoning_by_progressive_module_networks", "_bibtex": "@inproceedings{\nkim2018visual,\ntitle={Visual Reasoning by Progressive Module Networks},\nauthor={Seung Wook Kim and Makarand Tapaswi and Sanja Fidler},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1fpDsAqt7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper305/Official_Review", "cdate": 1542234492156, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1fpDsAqt7", "replyto": "B1fpDsAqt7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper305/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335694480, "tmdate": 1552335694480, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper305/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 17}