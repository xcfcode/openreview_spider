{"notes": [{"id": "hTUPgfEobsm", "original": "GuHTBU3ylO", "number": 1128, "cdate": 1601308126828, "ddate": null, "tcdate": 1601308126828, "tmdate": 1614985685148, "tddate": null, "forum": "hTUPgfEobsm", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "ADIS-GAN: Affine Disentangled GAN", "authorids": ["~Letao_Liu1", "martin.saerbeck@tuvsud.com", "jdauwels@ntu.edu.sg"], "authors": ["Letao Liu", "Martin Saerbeck", "Justin Dauwels"], "keywords": ["Deep Learning", "Disentangled Representation", "Generative Adversarial Network", "Computer Vision"], "abstract": "This paper proposes Affine Disentangled GAN (ADIS-GAN), which is a Generative Adversarial Network that can explicitly disentangle affine transformations in a self-supervised and rigorous manner.  The objective is inspired by InfoGAN, where an additional affine regularizer acts as the inductive bias.  The affine regularizer is rooted in the affine transformation properties of images, changing some properties of the underlying images, while leaving all other properties invariant. We derive the affine regularizer by decomposing the affine matrix into separate transformation matrices and inferring the transformation parameters by maximum-likelihood estimation.  Unlike the disentangled representations learned by existing approaches, the features learned by ADIS-GAN are axis-aligned and scalable, where transformations such as rotation, horizontal and vertical zoom, horizontal and vertical skew,  horizontal and vertical translation can be explicitly selected and learned. ADIS-GAN successfully disentangles these features on the MNIST, CelebA, and dSprites datasets.", "one-sentence_summary": "A Generative Adversarial Network that can explicitly disentangle affine transformations in a self-supervised and rigorous manner.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|adisgan_affine_disentangled_gan", "supplementary_material": "/attachment/916bf85f58b3dbccfaef4f8f57f6f25ac1eddec1.zip", "pdf": "/pdf/7fdabffbb7873150d6097dac467869b53b2e118c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=jN5K6PG7n-", "_bibtex": "@misc{\nliu2021adisgan,\ntitle={{\\{}ADIS{\\}}-{\\{}GAN{\\}}: Affine Disentangled {\\{}GAN{\\}}},\nauthor={Letao Liu and Martin Saerbeck and Justin Dauwels},\nyear={2021},\nurl={https://openreview.net/forum?id=hTUPgfEobsm}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "9Z-Kd5_r6D0", "original": null, "number": 1, "cdate": 1610040466475, "ddate": null, "tcdate": 1610040466475, "tmdate": 1610474070087, "tddate": null, "forum": "hTUPgfEobsm", "replyto": "hTUPgfEobsm", "invitation": "ICLR.cc/2021/Conference/Paper1128/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "Most of the reviewers had serious problems with clarity to start out. \nThe authors have addressed some, but not all of these problems. \n\nMore importantly, there were issues of significance and experimental evaluation.\nI concur with r4 on the experimental evaluation. \nI think if you're going to explicitly specialize toward disentangling affine transform parameters, \nthat's fine, but then you're in application-paper land, and I think there needs to be more of an attempt to show\nthat it will work \"in the wild\". \nFor this reason, and for the general reason that reviewers unanimously voted to reject, I am recommending rejection."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ADIS-GAN: Affine Disentangled GAN", "authorids": ["~Letao_Liu1", "martin.saerbeck@tuvsud.com", "jdauwels@ntu.edu.sg"], "authors": ["Letao Liu", "Martin Saerbeck", "Justin Dauwels"], "keywords": ["Deep Learning", "Disentangled Representation", "Generative Adversarial Network", "Computer Vision"], "abstract": "This paper proposes Affine Disentangled GAN (ADIS-GAN), which is a Generative Adversarial Network that can explicitly disentangle affine transformations in a self-supervised and rigorous manner.  The objective is inspired by InfoGAN, where an additional affine regularizer acts as the inductive bias.  The affine regularizer is rooted in the affine transformation properties of images, changing some properties of the underlying images, while leaving all other properties invariant. We derive the affine regularizer by decomposing the affine matrix into separate transformation matrices and inferring the transformation parameters by maximum-likelihood estimation.  Unlike the disentangled representations learned by existing approaches, the features learned by ADIS-GAN are axis-aligned and scalable, where transformations such as rotation, horizontal and vertical zoom, horizontal and vertical skew,  horizontal and vertical translation can be explicitly selected and learned. ADIS-GAN successfully disentangles these features on the MNIST, CelebA, and dSprites datasets.", "one-sentence_summary": "A Generative Adversarial Network that can explicitly disentangle affine transformations in a self-supervised and rigorous manner.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|adisgan_affine_disentangled_gan", "supplementary_material": "/attachment/916bf85f58b3dbccfaef4f8f57f6f25ac1eddec1.zip", "pdf": "/pdf/7fdabffbb7873150d6097dac467869b53b2e118c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=jN5K6PG7n-", "_bibtex": "@misc{\nliu2021adisgan,\ntitle={{\\{}ADIS{\\}}-{\\{}GAN{\\}}: Affine Disentangled {\\{}GAN{\\}}},\nauthor={Letao Liu and Martin Saerbeck and Justin Dauwels},\nyear={2021},\nurl={https://openreview.net/forum?id=hTUPgfEobsm}\n}"}, "tags": [], "invitation": {"reply": {"forum": "hTUPgfEobsm", "replyto": "hTUPgfEobsm", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040466462, "tmdate": 1610474070072, "id": "ICLR.cc/2021/Conference/Paper1128/-/Decision"}}}, {"id": "Ql1nkM67HfK", "original": null, "number": 1, "cdate": 1603945183111, "ddate": null, "tcdate": 1603945183111, "tmdate": 1606967826996, "tddate": null, "forum": "hTUPgfEobsm", "replyto": "hTUPgfEobsm", "invitation": "ICLR.cc/2021/Conference/Paper1128/-/Official_Review", "content": {"title": "Some interesting results, but the benefits of the explicit parameterization are not clear ", "review": "This paper proposes a method that can explicitly learn the affine transformations via disentangled representations in a self-supervised manner. The proposed ADIS-GAN learns to extract affine parameters by adding an affine regularizer on the top of InfoGAN. It seems to me that the main contribution of the paper is to learn the explicit parameterization of the affine transformation, such as rotation, zoom, and translation, by maximum likelihood estimation.  Experiments show that the proposed method is successful in estimating individual parameters of the affine transformation. \n\n**Strengths**\n+ The paper represents the first method that can learn explicit parameters of the affine transformation in a self-supervised manner.\n+ The experiments show that the proposed method can learn a disentangled representation with explicit affine parameters. \n\n**Weakness** \n- Experiments are not convincing enough. The paper only contains experiments on simple/synthetic datasets. The benefits of the proposed method are not completely clear. For the dSprites dataset, the proposed method does not outperform InfoGAN-CR, while InfoGAN-CR can perform disentanglement beyond affine transformations. For scalability, the paper only presents one example for zoom. It is also unclear how to merge zoom parameters into one. It is also helpful to give real applications that can benefit from the explicit disentanglement. \n- Writing of the paper can be improved. For example, Section 4.1 needs improvements. The notations are not completely clear, and their roles in the whole algorithm are not clear. It would be better to strengthen the connection of Equation (7) to the affine regularizer loss defined at the end of Section 4. \n- The factorization of the affine matrix and derivation of the maximum likelihood estimator are straightforward, making the paper's technical contribution less significant. \n\n**Minor issues**\n- The paper emphasizes scalability, but it is not completely clear what scalability the proposed method achieves and how it is achieved. For example, is it possible to merge the translation parameters into a single one? If so, how to do it, and how well the combined parameter performs?\n- It is unclear whether the proposed method can factor out skews. Figure 1 shows examples for skews, but the paper does not present how the skew parameters are estimated. \n- In Appendix A, the paper assumes the noise statistics for all parameters A_{ij}. It seems unrealistic. \n- The citations are correctly formatted. The parentheses are missing for citations.\n\n**Post-rebuttal**\n\nThank the authors for the rebuttal. It addresses parts of the raised issues. However, my rating keeps the same after reading the rebuttal and other reviews because\n1. the contribution and utility of the proposed method are not significant;\n2. the writing needs improvement; and\n3. the experiments are not convincing enough, and its advantages over previous methods are not clear. ", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper1128/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1128/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ADIS-GAN: Affine Disentangled GAN", "authorids": ["~Letao_Liu1", "martin.saerbeck@tuvsud.com", "jdauwels@ntu.edu.sg"], "authors": ["Letao Liu", "Martin Saerbeck", "Justin Dauwels"], "keywords": ["Deep Learning", "Disentangled Representation", "Generative Adversarial Network", "Computer Vision"], "abstract": "This paper proposes Affine Disentangled GAN (ADIS-GAN), which is a Generative Adversarial Network that can explicitly disentangle affine transformations in a self-supervised and rigorous manner.  The objective is inspired by InfoGAN, where an additional affine regularizer acts as the inductive bias.  The affine regularizer is rooted in the affine transformation properties of images, changing some properties of the underlying images, while leaving all other properties invariant. We derive the affine regularizer by decomposing the affine matrix into separate transformation matrices and inferring the transformation parameters by maximum-likelihood estimation.  Unlike the disentangled representations learned by existing approaches, the features learned by ADIS-GAN are axis-aligned and scalable, where transformations such as rotation, horizontal and vertical zoom, horizontal and vertical skew,  horizontal and vertical translation can be explicitly selected and learned. ADIS-GAN successfully disentangles these features on the MNIST, CelebA, and dSprites datasets.", "one-sentence_summary": "A Generative Adversarial Network that can explicitly disentangle affine transformations in a self-supervised and rigorous manner.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|adisgan_affine_disentangled_gan", "supplementary_material": "/attachment/916bf85f58b3dbccfaef4f8f57f6f25ac1eddec1.zip", "pdf": "/pdf/7fdabffbb7873150d6097dac467869b53b2e118c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=jN5K6PG7n-", "_bibtex": "@misc{\nliu2021adisgan,\ntitle={{\\{}ADIS{\\}}-{\\{}GAN{\\}}: Affine Disentangled {\\{}GAN{\\}}},\nauthor={Letao Liu and Martin Saerbeck and Justin Dauwels},\nyear={2021},\nurl={https://openreview.net/forum?id=hTUPgfEobsm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "hTUPgfEobsm", "replyto": "hTUPgfEobsm", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1128/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538126180, "tmdate": 1606915791085, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1128/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1128/-/Official_Review"}}}, {"id": "3Ls9hiZt6R", "original": null, "number": 5, "cdate": 1605867168525, "ddate": null, "tcdate": 1605867168525, "tmdate": 1605867168525, "tddate": null, "forum": "hTUPgfEobsm", "replyto": "Ql1nkM67HfK", "invitation": "ICLR.cc/2021/Conference/Paper1128/-/Official_Comment", "content": {"title": "Thank you for your detailed comments, we have added many details in the revised paper (especially Section 4, affine regularizer).", "comment": "We would like to thank the reviewer for the detailed and informative comments. We really appreciate this learning opportunity. The responses to the comments are as follows:\n\nA quick summary of the revised paper:\n-\tWe have revised the whole Section 4, which adds many details to the working principle of the affine regularizer.\no\tWe have added Figure 3 and 4 to better illustrate the working principle of the affine regularizer.\no\tWe have added a new section 4.1 which describes conversion between latent vectors and affine matrix.\n-\tWe have added translation examples in Figure 9 to illustrate the scalability property.\n-\tWe have renamed some of the notations (e.g., W_real to x_real) to make the paper more consistent.\n-\tWe have added Appendix G that provides information about the network structure.\n-\tWe have added Appendix H that shows different combinations of affine matrix parameters.\n-\tWe have added Appendix I to demonstrate how the proposed system achieves scalability.\n-\tWe have amended some of the citations with \\citep{}.\n\n\nQ1: Experiments ... datasets. \n\nAnswer: Thank you for your comments. For real-world applicability, perhaps the CelebA dataset can be an indication since it is an RGB dataset with real-world images. We will include more real-world datasets in future work.\n\nQ2: The benefits ... completely clear. \n\nAnswer: There are three major contributions: \n-\tADIS-GAN is the first algorithm that can disentangle an entire affine transformation. One of the use cases is the medical image, as discussed in \u201cExplicitly disentangling image content from translation and rotation with spatial-VAE\u201d in Neurips 2019. Their algorithm can only disentangle rotation and translation, where ADIS-GAN can disentangle an entire affine transformation.\n-\tAxis-alignment. The latent vectors in previous work do not guarantee the axis-alignment, where the attributes learned may be assigned to different latent vectors for each trial. For instance, if we train on the MNIST dataset with existing algorithms with 3 latent vectors, each trial may assign the rotation attribute to any of the 3 latent vectors. In contrast, in ADIS-GAN we can assign the rotation attribute to the exact latent vector for every trial by predefining how to form the affine matrix using the latent vectors.\n-\tScalability. Previous approaches cannot leverage the compactness and expressivity of the learned attributes. You may refer to Appendix H for more combinations of affine parameters.\n\nQ3: For the ... transformations. \n\nAnswer: Thank you for the detailed observations. InfoGAN-CR does not outperform on the MIG score, where our score is 0.46 and InfoGAN-CR is 0.37. For the rest of the score, ADIS-GAN is comparable to the InfoGAN-CR and outperforms other previous approaches. We do not apply model centrality which is a hyperparameter tuning method since our focus is on axis-alignment and scalability.\n\n\nQ4: For scalability ... into one. \n\nAnswer: Thank you for the advice. The translation example is added in the revised paper Section 5.1. We have provided more details on how to merge zoom parameters into one in appendix I.\n\nQ5: Writing of ... Section 4.\n\nAnswer: Thank you very much for your advice. We have added many details for Section 4 and strengthen the connections.\n\nQ6: The factorization ... significant.\n\nAnswer: \n-\tYes, a more complicated assumption might improve the performance and are part of future work (e.g., covariance matrix). \n-\tMore variations of affine matrix and derivation of the maximum likelihood are added in appendix H and I. We believe the affine matrix family can improve the completeness.\n-\tAnother technical contribution is the relative affine transformation (see Section 4.2 and Equation 9). We have never seen such a relative transformation in other works.\n\nQ7:  The paper...  single one? \n\nAnswer: Yes, it is possible to merge the translation parameters into a single one. We have provided a translation example in the revised paper (Figure 9). You may also refer to Appendix H for more affine matrices combination. \n\nQ8: If so ... performs?\n\nAnswer: We have provided a detailed example in Appendix I. \n\nQ9: It is ... estimated.\n\nAnswer: Yes, we can factor out skew. The methodology is similar to the way presented in the paper. You may refer to Appendix H.4 to H.7 that includes the skew in the affine matrix.\n\nQ10: In Appendix A ... unrealistic.\n\nAnswer: Thank you for your detailed comments. We agree that more complicated assumptions can be made (e.g., the covariance matrix for the noise). Since this is the first work that attempts to solve the affine transformation in a disentangled manner, we would like to make a relatively simpler assumption and verify the idea. We will definitely make more rigorous assumptions in the future.\n\nQ11: The citations ... citations.\nAnswer: Thank you for the advice. We have amended it in the revised paper.\n\n\n\n\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1128/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1128/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ADIS-GAN: Affine Disentangled GAN", "authorids": ["~Letao_Liu1", "martin.saerbeck@tuvsud.com", "jdauwels@ntu.edu.sg"], "authors": ["Letao Liu", "Martin Saerbeck", "Justin Dauwels"], "keywords": ["Deep Learning", "Disentangled Representation", "Generative Adversarial Network", "Computer Vision"], "abstract": "This paper proposes Affine Disentangled GAN (ADIS-GAN), which is a Generative Adversarial Network that can explicitly disentangle affine transformations in a self-supervised and rigorous manner.  The objective is inspired by InfoGAN, where an additional affine regularizer acts as the inductive bias.  The affine regularizer is rooted in the affine transformation properties of images, changing some properties of the underlying images, while leaving all other properties invariant. We derive the affine regularizer by decomposing the affine matrix into separate transformation matrices and inferring the transformation parameters by maximum-likelihood estimation.  Unlike the disentangled representations learned by existing approaches, the features learned by ADIS-GAN are axis-aligned and scalable, where transformations such as rotation, horizontal and vertical zoom, horizontal and vertical skew,  horizontal and vertical translation can be explicitly selected and learned. ADIS-GAN successfully disentangles these features on the MNIST, CelebA, and dSprites datasets.", "one-sentence_summary": "A Generative Adversarial Network that can explicitly disentangle affine transformations in a self-supervised and rigorous manner.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|adisgan_affine_disentangled_gan", "supplementary_material": "/attachment/916bf85f58b3dbccfaef4f8f57f6f25ac1eddec1.zip", "pdf": "/pdf/7fdabffbb7873150d6097dac467869b53b2e118c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=jN5K6PG7n-", "_bibtex": "@misc{\nliu2021adisgan,\ntitle={{\\{}ADIS{\\}}-{\\{}GAN{\\}}: Affine Disentangled {\\{}GAN{\\}}},\nauthor={Letao Liu and Martin Saerbeck and Justin Dauwels},\nyear={2021},\nurl={https://openreview.net/forum?id=hTUPgfEobsm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "hTUPgfEobsm", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1128/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1128/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1128/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1128/Authors|ICLR.cc/2021/Conference/Paper1128/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1128/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863330, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1128/-/Official_Comment"}}}, {"id": "-FsGaNfyddX", "original": null, "number": 4, "cdate": 1605866011015, "ddate": null, "tcdate": 1605866011015, "tmdate": 1605866055479, "tddate": null, "forum": "hTUPgfEobsm", "replyto": "MBlHWV98toi", "invitation": "ICLR.cc/2021/Conference/Paper1128/-/Official_Comment", "content": {"title": "Thank you for your detailed comments, we have added many details in the revised paper (especially Section 4, affine regularizer).", "comment": "We would like to thank the reviewer for the detailed and informative comments. We really appreciate this learning opportunity. The responses to the comments are as follows:\n\nA quick summary of the revised paper:\n-\tWe have revised the whole Section 4, which adds many details to the working principle of the affine regularizer.\no\tWe have added Figure 3 and 4 to better illustrate the working principle of the affine regularizer.\no\tWe have added a new section 4.1 which describes conversion between latent vectors and affine matrix.\n-\tWe have added translation examples in Figure 9 to illustrate the scalability property.\n-\tWe have renamed some of the notations (e.g., W_real to x_real) to make the paper more consistent.\n-\tWe have added Appendix G that provides information about the network structure.\n-\tWe have added Appendix H that shows different combinations of affine matrix parameters.\n-\tWe have added Appendix I to demonstrate how the proposed system achieves scalability.\n-\tWe have amended some of the citations with \\citep{}.\n\n\nQ1: The paper ... latent factors?\n\nAnswer: ADIS-GAN is a self-supervised method since there are no manual labels required for each image. Yes, the affine transformation matrix is known beforehand, which is similar to many PDE based methods (e.g., physics informed deep learning) where the PDE equations are known beforehand. The ADIS-GAN is also capable of discovering unknown latent factors, where we can define additional latent vectors besides the affine transformation latent factors, those latent factors will learn the unknown latent factors in a completely unsupervised manner just like InfoGAN.\n\nQ2: The paper ... previous works? \n\nAnswer: Yes, the latent vectors in previous work do not guarantee the axis-alignment property, which means the attributes learned by the algorithm may be assigned to different latent vectors for each trial. For instance, if we train on the MNIST dataset with existing algorithms and define 3 latent vectors, each trial may assign the rotation attribute to any of the 3 latent vectors. In ADIS-GAN we can assign the rotation attribute to the exact latent vector by predefining how to form the affine matrix using the latent vectors. The rotation attribute will be assigned to the fixed latent vector for every trial.\n\n\nQ3: Isn't the scalability... from data?\n\nAnswer: ADIS-GAN provides an inductive bias that makes it possible to learn the data in a scalable way. After training, ADIS-GAN does not rigidly remember the affine transformation itself but the way to conduct the affine transformation and keep the transformed images look realistic as well. \nPrevious approaches cannot leverage the compactness and expressivity of the learned attributes explicitly. You may refer to Appendix H and I for how to derive the scalability.\n\nQ4: There seems ... is described.\n\nAnswer: Sorry for the inconvenience. We have renamed all the notations that are not consistent in the revised paper. Specifically, all the images are renamed as $x$ instead of $W$. we have also modified the main network diagram (see Figure 2) to make it more comprehensive.\n\nQ5: Why is M 3x3?\n\nAnswer: Indeed, if we do not include translation in the affine matrix, it can be 2x2. We have listed more forms of affine matrix in Appendix H.\n\n\nQ6: Where do ...  6 A_{ij}'s.\n\nAnswer: Sorry for the confusion, we have it more clear in the revised paper.  A_{ij}\u2019s is the multiplication of the three affine matrices on the right.  If we multiply those three matrices on the right, we can have a 3x3 matrix, the value of the bottom row is constant \u201c0,0,1\u201d, the rest six parameters are A_{ij}\u2019s.\n\nQ7: There seems ... are.\n\nAnswer: Yes, we have put more details in the revised paper. You may refer to Section 4.1.2 for the maximum likelihood estimation. The calculation of A_{ij}\u2019s is mentioned in Section 4.1.1. Figure 3 also describes the complete affine regularization. \n\nQ8: The purpose ... applied?\n\nAnswer: That is correct. It is described in algorithm 1 \u201cLatent to affine normalization (LA)\u201d and \u201cAffine to latent normalization (AL)\u201d. We have mentioned more details in the revised paper. Norm_LA is mentioned in Section 4.1.1 and Norm_AL is mentioned in Section 4.1.2. \n\nQ9: What is ... latent variables?\n\nAnswer: Sorry for the confusion, we have mentioned more details in the revised paper. Encoder in Algorithm 1 does the same thing in InfoGAN, where the input is an image and output is a distribution over the latent variables. You may refer to Figure 3 for the complete process.  \n\nQ10: There is ... affine loss?\n\nAnswer: Yes, the categorical codes do not contribute to the affine loss.\n\nQ11: There are ...\"InfoGAN(modified)\"?\n\nAnswer: Sorry for the incompleteness. We have included those references in the revised version. HFVAE and ChyVAE in Section 2, and InfoGAN (modified) in Table 2. \n\nQ12: Please  ... a sentence.\n\nAnswer: Thank you for the advice. We have corrected those citations in the revised version. \n\n\n\n\n\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1128/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1128/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ADIS-GAN: Affine Disentangled GAN", "authorids": ["~Letao_Liu1", "martin.saerbeck@tuvsud.com", "jdauwels@ntu.edu.sg"], "authors": ["Letao Liu", "Martin Saerbeck", "Justin Dauwels"], "keywords": ["Deep Learning", "Disentangled Representation", "Generative Adversarial Network", "Computer Vision"], "abstract": "This paper proposes Affine Disentangled GAN (ADIS-GAN), which is a Generative Adversarial Network that can explicitly disentangle affine transformations in a self-supervised and rigorous manner.  The objective is inspired by InfoGAN, where an additional affine regularizer acts as the inductive bias.  The affine regularizer is rooted in the affine transformation properties of images, changing some properties of the underlying images, while leaving all other properties invariant. We derive the affine regularizer by decomposing the affine matrix into separate transformation matrices and inferring the transformation parameters by maximum-likelihood estimation.  Unlike the disentangled representations learned by existing approaches, the features learned by ADIS-GAN are axis-aligned and scalable, where transformations such as rotation, horizontal and vertical zoom, horizontal and vertical skew,  horizontal and vertical translation can be explicitly selected and learned. ADIS-GAN successfully disentangles these features on the MNIST, CelebA, and dSprites datasets.", "one-sentence_summary": "A Generative Adversarial Network that can explicitly disentangle affine transformations in a self-supervised and rigorous manner.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|adisgan_affine_disentangled_gan", "supplementary_material": "/attachment/916bf85f58b3dbccfaef4f8f57f6f25ac1eddec1.zip", "pdf": "/pdf/7fdabffbb7873150d6097dac467869b53b2e118c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=jN5K6PG7n-", "_bibtex": "@misc{\nliu2021adisgan,\ntitle={{\\{}ADIS{\\}}-{\\{}GAN{\\}}: Affine Disentangled {\\{}GAN{\\}}},\nauthor={Letao Liu and Martin Saerbeck and Justin Dauwels},\nyear={2021},\nurl={https://openreview.net/forum?id=hTUPgfEobsm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "hTUPgfEobsm", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1128/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1128/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1128/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1128/Authors|ICLR.cc/2021/Conference/Paper1128/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1128/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863330, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1128/-/Official_Comment"}}}, {"id": "m6526NRkU2s", "original": null, "number": 3, "cdate": 1605864944015, "ddate": null, "tcdate": 1605864944015, "tmdate": 1605865023803, "tddate": null, "forum": "hTUPgfEobsm", "replyto": "V-MeUVO9kr", "invitation": "ICLR.cc/2021/Conference/Paper1128/-/Official_Comment", "content": {"title": "Thank you for your detailed comments, we have added many details in the revised paper (especially Section 4, affine regularizer).", "comment": "We would like to thank the reviewer for the detailed and informative comments. We really appreciate this learning opportunity. The responses to the comments are as follows:\n\nA quick summary of the revised paper:\n-\tWe have revised the whole Section 4, which adds many details to the working principle of the affine regularizer.\no\tWe have added Figure 3 and 4 to better illustrate the working principle of the affine regularizer.\no\tWe have added a new section 4.1 which describes conversion between latent vectors and affine matrix.\n-\tWe have added translation examples in Figure 9 to illustrate the scalability property.\n-\tWe have renamed some of the notations (e.g., W_real to x_real) to make the paper more consistent.\n-\tWe have added Appendix G that provides information about the network structure.\n-\tWe have added Appendix H that shows a different combination of affine matrix parameters.\n-\tWe have added Appendix I to demonstrate how the proposed system achieves scalability.\n-\tWe have amended some of the citations with \\citep{}.\n\n\n1.\tHow is ... variability encoded?\n\nSub-question a: How is the latent vector, C, used?\n\nAnswer: For the latent vector $c$ sampled from a uniform distribution [-1,1], it is used in two ways: \nUsage 1: it is used to generate fake images and calculate the mutual information loss, which is the same as InfoGAN. \nUsage 2: it is used to generate the designed affine transformation matrix $M^\\text{real}_\\text{transform}$, and calculate the affine loss $L_\\text{affine}$. You may refer to Figure 3 of the revised paper\n\n\n\nSub-question b: How does it interact with W_basis and W_real?\n\nAnswer: You may refer to Figure 3 of the revised paper. \no\tWe rename $W_\\text{basis}$ to $x_\\text{basis}$ and $W_\\text{real}$ to $x_\\text{real}$ for consistency.\no\tWe can encode $x_\\text{real}$ to $c_\\text{real}^\\text{basis}$ through the encoder.\no\t$c_\\text{real}^\\text{basis}$ can be transformed to $M_\\text{real}^\\text{basis}$ through Flow 1.\no\tBy the assumption that every image can be represented as the multiplication between an affine transformation and $x_\\text{basis}$, we have $x_\\text{real}  = M_\\text{real}^\\text{basis} \\times x_\\text{basis}$.\n\n\nSub-question c: Must it be 5-d to match with rotation, zoom, and translation. How are additional factors of variability encoded?\n\nAnswer:  No, we can use different dimensions. You may refer to Appendix H for more details.\n\n2.\tHow is $W_\\text{basis}$ learned?\n\nAnswer: We have renamed $W_\\text{basis}$ to $x_\\text{basis}$ for consistency in the revised paper. $x_\\text{basis}$ is purely learned from data and does not refer to a particular image in the training dataset. For visualization, we can set all the latent vector $c$ to zero and generate $x_\\text{basis}$ for each category.\n\n3.\tWhat is Norm_AL\n\nAnswer: We have revised the paper for this part. You may refer to Figure 4 for more details. \n\n\n4.\tWhat are the encoder ... the text.\n\nAnswer: We have revised the paper for this part. You may refer to Figure 2, where E stands for the encoder, G stands for generator and D stands for the discriminator. The encoder in algorithm 1 can be found in both Figure 2 and Figure 3.\n\n\n5.\tHow do you ... zoom transformation.\n\nAnswer: Thank you for your detailed observation. We have also observed similar phenomena in other algorithms. For example, in InfoGAN, the presence of sunglass changes with hairstyle (Figure 4 in appendix). In beta-VAE and TC-beta-VAE (Figure 5 in appendix), the lighting condition changes with the baldness attribute, and the hairstyle changes with the face width. \nTo some degree, it might also help to improve the realism of the images. For example, in Figure 7 left image, where the length of the face is shortened, the algorithm not only compresses the length of the face but knows to add the neck to make the image more natural.\nThe affine transformations should be the main attributes that the latent vectors try to encode, but not rigidly perform the affine transformation to the image. It might be a good thing that the disentanglement is correlated to the realism of the generated images.\n\nThings that would improve my score:\n\n1.\tFully explain ...parameters.\n\nAnswer: Thank you very much for the comments. We have amended those issues in the revised paper as follows:\n-\tWe have explained the method in a more detailed and consistent way (see Section 4 in the revised paper). \n-\tWe have included a more complete neural network architecture in appendix G.\n-\tWe have included the training parameters at the beginning of Section 5 (e.g., learning rate, batch, regularization weight).\n\n2.\tExplain how affine ... the image.\n\nAnswer: Thank you very much for the comments. We have added a lot of details in the revised paper regarding that. You may refer to Section 4, Figure 3 and 4 for a detailed description. \n\n\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1128/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1128/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ADIS-GAN: Affine Disentangled GAN", "authorids": ["~Letao_Liu1", "martin.saerbeck@tuvsud.com", "jdauwels@ntu.edu.sg"], "authors": ["Letao Liu", "Martin Saerbeck", "Justin Dauwels"], "keywords": ["Deep Learning", "Disentangled Representation", "Generative Adversarial Network", "Computer Vision"], "abstract": "This paper proposes Affine Disentangled GAN (ADIS-GAN), which is a Generative Adversarial Network that can explicitly disentangle affine transformations in a self-supervised and rigorous manner.  The objective is inspired by InfoGAN, where an additional affine regularizer acts as the inductive bias.  The affine regularizer is rooted in the affine transformation properties of images, changing some properties of the underlying images, while leaving all other properties invariant. We derive the affine regularizer by decomposing the affine matrix into separate transformation matrices and inferring the transformation parameters by maximum-likelihood estimation.  Unlike the disentangled representations learned by existing approaches, the features learned by ADIS-GAN are axis-aligned and scalable, where transformations such as rotation, horizontal and vertical zoom, horizontal and vertical skew,  horizontal and vertical translation can be explicitly selected and learned. ADIS-GAN successfully disentangles these features on the MNIST, CelebA, and dSprites datasets.", "one-sentence_summary": "A Generative Adversarial Network that can explicitly disentangle affine transformations in a self-supervised and rigorous manner.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|adisgan_affine_disentangled_gan", "supplementary_material": "/attachment/916bf85f58b3dbccfaef4f8f57f6f25ac1eddec1.zip", "pdf": "/pdf/7fdabffbb7873150d6097dac467869b53b2e118c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=jN5K6PG7n-", "_bibtex": "@misc{\nliu2021adisgan,\ntitle={{\\{}ADIS{\\}}-{\\{}GAN{\\}}: Affine Disentangled {\\{}GAN{\\}}},\nauthor={Letao Liu and Martin Saerbeck and Justin Dauwels},\nyear={2021},\nurl={https://openreview.net/forum?id=hTUPgfEobsm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "hTUPgfEobsm", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1128/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1128/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1128/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1128/Authors|ICLR.cc/2021/Conference/Paper1128/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1128/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863330, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1128/-/Official_Comment"}}}, {"id": "MBlHWV98toi", "original": null, "number": 2, "cdate": 1604000490648, "ddate": null, "tcdate": 1604000490648, "tmdate": 1605024523218, "tddate": null, "forum": "hTUPgfEobsm", "replyto": "hTUPgfEobsm", "invitation": "ICLR.cc/2021/Conference/Paper1128/-/Official_Review", "content": {"title": "review", "review": "Paper summary:\n\nThis paper seems to be about aligning latent variables in a GAN with certain affine transformations of the image. This is done by adding an additional \"affine regularization\" on top of the InfoGAN formulation. This affine regularization seems to be the L2 distance between the randomly sampled c (interpretable latent variables?) and some transformed c'. How the transformed c' is computed was not understood by the reviewer, but it seems to depend on a set of explicitly chosen affine transformations. The affine transformations specified are rotation, horizontal and vertical zoom, horizontal and vertical translation. The authors experiment on MNIST, dSprites, four shapes, and CelebA data. Quantitative metrics are reported for dSprites.\n\nStrong points:\n\nNot many papers tackling disentanglement in GANs.\n\nThe paper seems to show nice visual results and seems to be working at recovering the selected affine transformations of images.\n\nWeak points:\n\n(My overall concerns are about the additional supervision required by the method and the clarity of the writing, in particular the method itself.)\n\nThe paper mentions this is a self-supervised approach, but it seems closer to a supervised approach where the affine loss can only help recover specific latent factors that must be known beforehand. Would ADIS-GAN be capable of discovering unknown latent factors?\n\nThe paper repeatedly claims in the text and in Table 1 that previous works do not have axis-alignment or scalability. Does ADIS-GAN guarantee alignment in some way that is more reliable than in previous works? Isn't the scalability aspect (modeling horizontal and vertical zoom) explicitly encoded by ADIS-GAN, whereas previous works attempt to learn this from data?\n\nI could not follow the exposition in the paper. For instance:\n  - There seems to be a complete change in notation from Sections 1-3 to Section 4. No mention of the previously defined x_real, x_fake, z, or c are used in Section 4, where the core contribution is described. \n  - Many sentences end in \"see Appendix\", but I have looked at the Appendix and still come away confused..\n  - Why is M 3x3?\n  - Where do the A_{ij}'s come from in equation 9? I guessed it might be elements of c, but there are 5 elements in c and 6 A_{ij}'s.\n  - There seems to be derivations of a maximum likelihood estimate for constructing c', but these depend on knowing what the A_{ij}'s are.\n  - The purpose of Norm_{AL/LA} is never explicitly mentioned, though I imagine it's for scaling the interval [-1, 1] to specific bounds so that M can be applied?\n  - What is Encoder in Algorithm 1 and why does it return a linear operator? (Is this different from the encoder in InfoGAN which outputs a distribution over the latent variables?)\n\nAdditional feedback:\n\n - There is mention that ADIS-GAN used categorical codes as well. I assume these do not contribute to the affine loss?\n\n - There are multiple rows in Table 2 that reference models not explained in the text. e.g. references for HFVAE and ChyVAE, and what is \"InfoGAN(modified)\"?\n\n - Please use \\citep to include brackets around your citations, when not explicitly using them in a sentence.", "rating": "4: Ok but not good enough - rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper1128/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1128/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ADIS-GAN: Affine Disentangled GAN", "authorids": ["~Letao_Liu1", "martin.saerbeck@tuvsud.com", "jdauwels@ntu.edu.sg"], "authors": ["Letao Liu", "Martin Saerbeck", "Justin Dauwels"], "keywords": ["Deep Learning", "Disentangled Representation", "Generative Adversarial Network", "Computer Vision"], "abstract": "This paper proposes Affine Disentangled GAN (ADIS-GAN), which is a Generative Adversarial Network that can explicitly disentangle affine transformations in a self-supervised and rigorous manner.  The objective is inspired by InfoGAN, where an additional affine regularizer acts as the inductive bias.  The affine regularizer is rooted in the affine transformation properties of images, changing some properties of the underlying images, while leaving all other properties invariant. We derive the affine regularizer by decomposing the affine matrix into separate transformation matrices and inferring the transformation parameters by maximum-likelihood estimation.  Unlike the disentangled representations learned by existing approaches, the features learned by ADIS-GAN are axis-aligned and scalable, where transformations such as rotation, horizontal and vertical zoom, horizontal and vertical skew,  horizontal and vertical translation can be explicitly selected and learned. ADIS-GAN successfully disentangles these features on the MNIST, CelebA, and dSprites datasets.", "one-sentence_summary": "A Generative Adversarial Network that can explicitly disentangle affine transformations in a self-supervised and rigorous manner.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|adisgan_affine_disentangled_gan", "supplementary_material": "/attachment/916bf85f58b3dbccfaef4f8f57f6f25ac1eddec1.zip", "pdf": "/pdf/7fdabffbb7873150d6097dac467869b53b2e118c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=jN5K6PG7n-", "_bibtex": "@misc{\nliu2021adisgan,\ntitle={{\\{}ADIS{\\}}-{\\{}GAN{\\}}: Affine Disentangled {\\{}GAN{\\}}},\nauthor={Letao Liu and Martin Saerbeck and Justin Dauwels},\nyear={2021},\nurl={https://openreview.net/forum?id=hTUPgfEobsm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "hTUPgfEobsm", "replyto": "hTUPgfEobsm", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1128/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538126180, "tmdate": 1606915791085, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1128/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1128/-/Official_Review"}}}, {"id": "V-MeUVO9kr", "original": null, "number": 3, "cdate": 1604994618325, "ddate": null, "tcdate": 1604994618325, "tmdate": 1605024523156, "tddate": null, "forum": "hTUPgfEobsm", "replyto": "hTUPgfEobsm", "invitation": "ICLR.cc/2021/Conference/Paper1128/-/Official_Review", "content": {"title": "Description of method incomplete", "review": "This manuscript presents ADIS-GAN (affine disentangled GAN), a method for learning disentangled affine transformations of images such as rotation, zoom, and translation. The authors enforce this disentanglement using a regularizer that penalizes the difference between affine transformations on latent representations. The method is difficult to follow with many critical details left out. Without a complete description, it\u2019s impossible to fully understand and evaluate this work. Some specific comments and questions follow below.\n1.\tHow is the latent vector, C, used? How does it interact with W_basis and W_real? Must it be 5-d to match with rotation, zoom, and translation? How are additional factors of variability encoded?\n2.\tHow is W_basis learned?\n3.\tWhat is Norm_AL\n4.\tWhat are the encoder/generator/adversary model architectures and how were they trained? For that matter, what is the encoder listed in algorithm 1? I can\u2019t find any reference to it in the text.\n5.\tHow do you ensure that affine transformations of the latent vector actual produce affine transformations of the image? In Figure 5, it looks like the faces aren\u2019t simply being transformed by zoom, because other attributes clear change. In column 5 from the right, the top face has short hair, an exposed ear, and visible teeth. The bottom face, in contrast, has long hair, covered ear, and no visible teeth. This does not appear to be a simple zoom transformation.\n\nThings that would improve my score:\n1.\tFully explain the method, neural network architectures, and training parameters.\n2.\tExplain how affine transformations of the latent vector are forced to produce matching affine transformations of the image.\n", "rating": "3: Clear rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper1128/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1128/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ADIS-GAN: Affine Disentangled GAN", "authorids": ["~Letao_Liu1", "martin.saerbeck@tuvsud.com", "jdauwels@ntu.edu.sg"], "authors": ["Letao Liu", "Martin Saerbeck", "Justin Dauwels"], "keywords": ["Deep Learning", "Disentangled Representation", "Generative Adversarial Network", "Computer Vision"], "abstract": "This paper proposes Affine Disentangled GAN (ADIS-GAN), which is a Generative Adversarial Network that can explicitly disentangle affine transformations in a self-supervised and rigorous manner.  The objective is inspired by InfoGAN, where an additional affine regularizer acts as the inductive bias.  The affine regularizer is rooted in the affine transformation properties of images, changing some properties of the underlying images, while leaving all other properties invariant. We derive the affine regularizer by decomposing the affine matrix into separate transformation matrices and inferring the transformation parameters by maximum-likelihood estimation.  Unlike the disentangled representations learned by existing approaches, the features learned by ADIS-GAN are axis-aligned and scalable, where transformations such as rotation, horizontal and vertical zoom, horizontal and vertical skew,  horizontal and vertical translation can be explicitly selected and learned. ADIS-GAN successfully disentangles these features on the MNIST, CelebA, and dSprites datasets.", "one-sentence_summary": "A Generative Adversarial Network that can explicitly disentangle affine transformations in a self-supervised and rigorous manner.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|adisgan_affine_disentangled_gan", "supplementary_material": "/attachment/916bf85f58b3dbccfaef4f8f57f6f25ac1eddec1.zip", "pdf": "/pdf/7fdabffbb7873150d6097dac467869b53b2e118c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=jN5K6PG7n-", "_bibtex": "@misc{\nliu2021adisgan,\ntitle={{\\{}ADIS{\\}}-{\\{}GAN{\\}}: Affine Disentangled {\\{}GAN{\\}}},\nauthor={Letao Liu and Martin Saerbeck and Justin Dauwels},\nyear={2021},\nurl={https://openreview.net/forum?id=hTUPgfEobsm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "hTUPgfEobsm", "replyto": "hTUPgfEobsm", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1128/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538126180, "tmdate": 1606915791085, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1128/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1128/-/Official_Review"}}}], "count": 8}