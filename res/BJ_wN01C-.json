{"notes": [{"id": "Syl6Y4Dwrm", "original": null, "number": 4, "cdate": 1533666437195, "ddate": null, "tcdate": 1533666437195, "tmdate": 1533666507919, "tddate": null, "forum": "BJ_wN01C-", "replyto": "BJ_wN01C-", "invitation": "ICLR.cc/2018/Conference/-/Paper189/Official_Comment", "content": {"title": "Erratum: Corrections in the appendix", "comment": "In the section \"Methods\" and sub-section \"Initialization of connectivity matrices\" of the appendix:\n\nThe fifth paragraph should read:\n\"For CIFAR-10 [...] The numbers of parameters per connectivity matrices were therefore 5k, 102k, 885k, 74k and 2k from input to output. The connectivity matrices were initialized with connectivity $1, 4p_0, 0.4p_0, 4p_0,$ and $1$.\"\n(before the number of parameters in the fourth connection matrix was mistaken as 738K and the connectivities  were $1, 8p_0, 0.8p_0, 8p_0,$ and $1$)\n\nThe sixth paragraph should read:\n\"- For TIMIT, [..] Each of these three connectivity matrices were initialized with a connectivity of $1.8 p_0, 0.6 p_0$, and $ 6 p_0$.\"\n(before these numbers were mistaken as $3 p_0, p_0$, and $ 10 p_0$)\n"}, "signatures": ["ICLR.cc/2018/Conference/Paper189/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2018/Conference/Paper189/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Rewiring: Training very sparse deep networks", "abstract": "Neuromorphic hardware tends to pose limits on the connectivity of deep networks that one can run on them. But also generic hardware and software implementations of deep learning run more efficiently for sparse networks. Several methods exist for pruning connections of a neural network after it was trained without connectivity constraints. We present an algorithm, DEEP R, that enables us to train directly a sparsely connected neural network. DEEP R automatically rewires the network during supervised training so that connections are there where they are most needed for the task, while its total number is all the time strictly bounded. We demonstrate that DEEP R can be used to train very sparse feedforward and recurrent neural networks on standard benchmark tasks with just a minor loss in performance. DEEP R is based on a rigorous theoretical foundation that views rewiring as stochastic sampling of network configurations from a posterior.", "pdf": "/pdf/3e24f9f67b6738385d15c0f0edd85ab4110fd8a3.pdf", "TL;DR": "The paper presents Deep Rewiring, an algorithm that can be used to train deep neural networks when the network connectivity is severely constrained during training.", "paperhash": "bellec|deep_rewiring_training_very_sparse_deep_networks", "_bibtex": "@inproceedings{\nbellec2018deep,\ntitle={Deep Rewiring: Training very sparse deep networks},\nauthor={Guillaume Bellec and David Kappel and Wolfgang Maass and Robert Legenstein},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=BJ_wN01C-},\n}", "keywords": ["deep learning", "pruning", "LSTM", "convolutional networks", "recurrent neural network", "sparse networks", "neuromorphic hardware", "energy efficient computing", "low memory hardware", "stochastic differential equation", "fokker-planck equation"], "authors": ["Guillaume Bellec", "David Kappel", "Wolfgang Maass", "Robert Legenstein"], "authorids": ["bellec@igi.tugraz.at", "kappel@igi.tugraz.at", "maass@igi.tugraz.at", "legenstein@igi.tugraz.at"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825737827, "id": "ICLR.cc/2018/Conference/-/Paper189/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "BJ_wN01C-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper189/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper189/Authors|ICLR.cc/2018/Conference/Paper189/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper189/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper189/Authors|ICLR.cc/2018/Conference/Paper189/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper189/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper189/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper189/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper189/Reviewers", "ICLR.cc/2018/Conference/Paper189/Authors", "ICLR.cc/2018/Conference/Paper189/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825737827}}}, {"tddate": null, "ddate": null, "tmdate": 1518730185986, "tcdate": 1509053535943, "number": 189, "cdate": 1518730185975, "id": "BJ_wN01C-", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "BJ_wN01C-", "original": "ryDDN0k0Z", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Deep Rewiring: Training very sparse deep networks", "abstract": "Neuromorphic hardware tends to pose limits on the connectivity of deep networks that one can run on them. But also generic hardware and software implementations of deep learning run more efficiently for sparse networks. Several methods exist for pruning connections of a neural network after it was trained without connectivity constraints. We present an algorithm, DEEP R, that enables us to train directly a sparsely connected neural network. DEEP R automatically rewires the network during supervised training so that connections are there where they are most needed for the task, while its total number is all the time strictly bounded. We demonstrate that DEEP R can be used to train very sparse feedforward and recurrent neural networks on standard benchmark tasks with just a minor loss in performance. DEEP R is based on a rigorous theoretical foundation that views rewiring as stochastic sampling of network configurations from a posterior.", "pdf": "/pdf/3e24f9f67b6738385d15c0f0edd85ab4110fd8a3.pdf", "TL;DR": "The paper presents Deep Rewiring, an algorithm that can be used to train deep neural networks when the network connectivity is severely constrained during training.", "paperhash": "bellec|deep_rewiring_training_very_sparse_deep_networks", "_bibtex": "@inproceedings{\nbellec2018deep,\ntitle={Deep Rewiring: Training very sparse deep networks},\nauthor={Guillaume Bellec and David Kappel and Wolfgang Maass and Robert Legenstein},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=BJ_wN01C-},\n}", "keywords": ["deep learning", "pruning", "LSTM", "convolutional networks", "recurrent neural network", "sparse networks", "neuromorphic hardware", "energy efficient computing", "low memory hardware", "stochastic differential equation", "fokker-planck equation"], "authors": ["Guillaume Bellec", "David Kappel", "Wolfgang Maass", "Robert Legenstein"], "authorids": ["bellec@igi.tugraz.at", "kappel@igi.tugraz.at", "maass@igi.tugraz.at", "legenstein@igi.tugraz.at"]}, "nonreaders": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260096982, "tcdate": 1517249387372, "number": 169, "cdate": 1517249387355, "id": "H1Qd7kpBM", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "BJ_wN01C-", "replyto": "BJ_wN01C-", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"title": "ICLR 2018 Conference Acceptance Decision", "comment": "Clearly explained, well motivated and empirically supported algorithm for training deep networks while simultaneously learning their sparse connectivity.\nThe approach is similar to previous work (in particular Welling et al., Bayesian Learning via Stochastic Gradient Langevin Dynamics, ICML 2011) but is novel in that it satisfies a hard constraint on the network sparsity, which could be an advantage to match neuromorphic hardware limitations.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Rewiring: Training very sparse deep networks", "abstract": "Neuromorphic hardware tends to pose limits on the connectivity of deep networks that one can run on them. But also generic hardware and software implementations of deep learning run more efficiently for sparse networks. Several methods exist for pruning connections of a neural network after it was trained without connectivity constraints. We present an algorithm, DEEP R, that enables us to train directly a sparsely connected neural network. DEEP R automatically rewires the network during supervised training so that connections are there where they are most needed for the task, while its total number is all the time strictly bounded. We demonstrate that DEEP R can be used to train very sparse feedforward and recurrent neural networks on standard benchmark tasks with just a minor loss in performance. DEEP R is based on a rigorous theoretical foundation that views rewiring as stochastic sampling of network configurations from a posterior.", "pdf": "/pdf/3e24f9f67b6738385d15c0f0edd85ab4110fd8a3.pdf", "TL;DR": "The paper presents Deep Rewiring, an algorithm that can be used to train deep neural networks when the network connectivity is severely constrained during training.", "paperhash": "bellec|deep_rewiring_training_very_sparse_deep_networks", "_bibtex": "@inproceedings{\nbellec2018deep,\ntitle={Deep Rewiring: Training very sparse deep networks},\nauthor={Guillaume Bellec and David Kappel and Wolfgang Maass and Robert Legenstein},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=BJ_wN01C-},\n}", "keywords": ["deep learning", "pruning", "LSTM", "convolutional networks", "recurrent neural network", "sparse networks", "neuromorphic hardware", "energy efficient computing", "low memory hardware", "stochastic differential equation", "fokker-planck equation"], "authors": ["Guillaume Bellec", "David Kappel", "Wolfgang Maass", "Robert Legenstein"], "authorids": ["bellec@igi.tugraz.at", "kappel@igi.tugraz.at", "maass@igi.tugraz.at", "legenstein@igi.tugraz.at"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642406223, "tcdate": 1511821873274, "number": 1, "cdate": 1511821873274, "id": "Syx4zM9xM", "invitation": "ICLR.cc/2018/Conference/-/Paper189/Official_Review", "forum": "BJ_wN01C-", "replyto": "BJ_wN01C-", "signatures": ["ICLR.cc/2018/Conference/Paper189/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "Review of \"DEEP R\"", "rating": "8: Top 50% of accepted papers, clear accept", "review": "In this paper, the authors present an approach to implement deep learning directly on sparsely connected graphs. Previous approaches have focused on transferring trained deep networks to a sparse graph for fast or efficient utilization; using this approach, sparse networks can be trained efficiently online, allowing for fast and flexible learning. Further investigation is necessary to understand the full implications of the two main conceptual changes introduced here (signed connections that can disappear and random walk in parameter space), but the initial results are quite promising.\n\nIt would also be interesting to understand more fully how performance scales to larger networks. If the target connectivity could be pushed to a very sparse limit, where only a fixed number of connections were added with each additional neuron, then this could significantly shape how these networks are trained at very large scales. Perhaps the heuristics for initializing the connectivity matrices will be insufficient, but could these be improved in further work?\n\nAs a last minor comment, the authors should specify explicitly what the shaded areas are in Fig. 4b,c.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Rewiring: Training very sparse deep networks", "abstract": "Neuromorphic hardware tends to pose limits on the connectivity of deep networks that one can run on them. But also generic hardware and software implementations of deep learning run more efficiently for sparse networks. Several methods exist for pruning connections of a neural network after it was trained without connectivity constraints. We present an algorithm, DEEP R, that enables us to train directly a sparsely connected neural network. DEEP R automatically rewires the network during supervised training so that connections are there where they are most needed for the task, while its total number is all the time strictly bounded. We demonstrate that DEEP R can be used to train very sparse feedforward and recurrent neural networks on standard benchmark tasks with just a minor loss in performance. DEEP R is based on a rigorous theoretical foundation that views rewiring as stochastic sampling of network configurations from a posterior.", "pdf": "/pdf/3e24f9f67b6738385d15c0f0edd85ab4110fd8a3.pdf", "TL;DR": "The paper presents Deep Rewiring, an algorithm that can be used to train deep neural networks when the network connectivity is severely constrained during training.", "paperhash": "bellec|deep_rewiring_training_very_sparse_deep_networks", "_bibtex": "@inproceedings{\nbellec2018deep,\ntitle={Deep Rewiring: Training very sparse deep networks},\nauthor={Guillaume Bellec and David Kappel and Wolfgang Maass and Robert Legenstein},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=BJ_wN01C-},\n}", "keywords": ["deep learning", "pruning", "LSTM", "convolutional networks", "recurrent neural network", "sparse networks", "neuromorphic hardware", "energy efficient computing", "low memory hardware", "stochastic differential equation", "fokker-planck equation"], "authors": ["Guillaume Bellec", "David Kappel", "Wolfgang Maass", "Robert Legenstein"], "authorids": ["bellec@igi.tugraz.at", "kappel@igi.tugraz.at", "maass@igi.tugraz.at", "legenstein@igi.tugraz.at"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642406117, "id": "ICLR.cc/2018/Conference/-/Paper189/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper189/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper189/AnonReviewer3", "ICLR.cc/2018/Conference/Paper189/AnonReviewer1", "ICLR.cc/2018/Conference/Paper189/AnonReviewer2"], "reply": {"forum": "BJ_wN01C-", "replyto": "BJ_wN01C-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper189/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642406117}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642406186, "tcdate": 1512086324714, "number": 2, "cdate": 1512086324714, "id": "H1aEoGAgG", "invitation": "ICLR.cc/2018/Conference/-/Paper189/Official_Review", "forum": "BJ_wN01C-", "replyto": "BJ_wN01C-", "signatures": ["ICLR.cc/2018/Conference/Paper189/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Interesting algorithm to training with limited memory, but needs some additional relationships to existing work.", "rating": "5: Marginally below acceptance threshold", "review": "The authors provide a novel, interesting, and simple algorithm capable of training with limited memory.  The algorithm is well-motivated and clearly explained, and empirical evidence suggests that the algorithm works well.  However, the paper needs additional examination in how the algorithm can deal with larger data inputs and outputs.  Second, the relationship to existing work needs to be explained better.\n\nPro:\nThe algorithm is clearly explained, well-motivated, and empirically supported.\n\nCon:\nThe relationship to stochastic gradient markov chain monte carlo needs to be explained better.  In particular, the update form was first introduced in [1], the annealing scheme was analyzed in [2], and the reflection step was introduced in [3].  These relationships need to be explained clearly.\nThe evidence is presented on very small input data.  With something like natural images, the parameterization is much larger and with more data, the number of total parameters is much larger.  Is there any evidence that the proposed algorithm could continue performing comparatively as the total number of parameters in state-of-the-art networks increases? This would require a smaller ratio of included parameters.\n\n[1] Welling, M. and Teh, Y.W., 2011. Bayesian learning via stochastic gradient Langevin dynamics. In Proceedings of the 28th International Conference on Machine Learning (ICML-11)(pp. 681-688).\n\n[2] Chen, C., Carlson, D., Gan, Z., Li, C. and Carin, L., 2016, May. Bridging the gap between stochastic gradient MCMC and stochastic optimization. In Artificial Intelligence and Statistics(pp. 1051-1060).\n\n[3] Patterson, S. and Teh, Y.W., 2013. Stochastic gradient Riemannian Langevin dynamics on the probability simplex. In Advances in Neural Information Processing Systems (pp. 3102-3110).\n \n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Rewiring: Training very sparse deep networks", "abstract": "Neuromorphic hardware tends to pose limits on the connectivity of deep networks that one can run on them. But also generic hardware and software implementations of deep learning run more efficiently for sparse networks. Several methods exist for pruning connections of a neural network after it was trained without connectivity constraints. We present an algorithm, DEEP R, that enables us to train directly a sparsely connected neural network. DEEP R automatically rewires the network during supervised training so that connections are there where they are most needed for the task, while its total number is all the time strictly bounded. We demonstrate that DEEP R can be used to train very sparse feedforward and recurrent neural networks on standard benchmark tasks with just a minor loss in performance. DEEP R is based on a rigorous theoretical foundation that views rewiring as stochastic sampling of network configurations from a posterior.", "pdf": "/pdf/3e24f9f67b6738385d15c0f0edd85ab4110fd8a3.pdf", "TL;DR": "The paper presents Deep Rewiring, an algorithm that can be used to train deep neural networks when the network connectivity is severely constrained during training.", "paperhash": "bellec|deep_rewiring_training_very_sparse_deep_networks", "_bibtex": "@inproceedings{\nbellec2018deep,\ntitle={Deep Rewiring: Training very sparse deep networks},\nauthor={Guillaume Bellec and David Kappel and Wolfgang Maass and Robert Legenstein},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=BJ_wN01C-},\n}", "keywords": ["deep learning", "pruning", "LSTM", "convolutional networks", "recurrent neural network", "sparse networks", "neuromorphic hardware", "energy efficient computing", "low memory hardware", "stochastic differential equation", "fokker-planck equation"], "authors": ["Guillaume Bellec", "David Kappel", "Wolfgang Maass", "Robert Legenstein"], "authorids": ["bellec@igi.tugraz.at", "kappel@igi.tugraz.at", "maass@igi.tugraz.at", "legenstein@igi.tugraz.at"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642406117, "id": "ICLR.cc/2018/Conference/-/Paper189/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper189/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper189/AnonReviewer3", "ICLR.cc/2018/Conference/Paper189/AnonReviewer1", "ICLR.cc/2018/Conference/Paper189/AnonReviewer2"], "reply": {"forum": "BJ_wN01C-", "replyto": "BJ_wN01C-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper189/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642406117}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642406136, "tcdate": 1512250990065, "number": 3, "cdate": 1512250990065, "id": "r1UOC9lbf", "invitation": "ICLR.cc/2018/Conference/-/Paper189/Official_Review", "forum": "BJ_wN01C-", "replyto": "BJ_wN01C-", "signatures": ["ICLR.cc/2018/Conference/Paper189/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "Promising approach for network compression on hardware with limited resources, however important references to previous work are missing", "rating": "6: Marginally above acceptance threshold", "review": "This paper presents an iterative approach to sparsify a network already during training. During the training process, the amount of connections in the network is guaranteed to stay under a specific threshold. This is a big advantage when training is performed on hardware with computational limitations, in comparison to \"post-hoc\" sparsification methods, that compress the network after training.\nThe method is derived by considering the \"rewiring\" of an (artificial) neural network as a stochastic process. This perspective is based on a recent model in computational biology but also can be interpreted as a (sequential) monte carlo sampling based stochastic gradient descent approach. References to previous work in this area are missing, e.g.\n\n[1] de Freitas et al., Sequential Monte Carlo Methods to Train Neural Network\nModels, Neural Computation 2000\n[2] Welling et al., Bayesian Learning via Stochastic Gradient Langevin Dynamics, ICML 2011\n\nEspecially the stochastic gradient method in [2] is strongly related to the existing approach.\n\nPositive aspects\n\n- The presented approach is well grounded in the theory of stochastic processes. The authors provide proofs of convergence by showing that the iterative updates converge to a fixpoint of the stochastic process\n\n- By keeping the temperature parameter of the stochastic process high, it can be directly applied to online transfer learning.\n\n- The method is specifically designed for online learning with limited hardware ressources.\n\nNegative aspects\n\n- The presented approach is outperformed for moderate compression levels (by Han's pruning method for >5% connectivity on MNIST, Fig. 3 A, and by l1-shrinkage for >40% connectivity on CIFAR-10 and TIMIT, Fig. 3 B&C). Especially the results on MNIST suggest that this method is most advantageous for very high compression levels. However in these cases the overall classification accuracy has already dropped significantly which could limit the practical applicability.\n\n- A detailled discussion of the relation to previously existing very similar work is missing (see above)\n\n\nTechnical Remarks\n\nFig. 1, 2 and 3 are referenced on the pages following the page containing the figure. Readibility could be slightly increased by putting the figures on the respective pages.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Deep Rewiring: Training very sparse deep networks", "abstract": "Neuromorphic hardware tends to pose limits on the connectivity of deep networks that one can run on them. But also generic hardware and software implementations of deep learning run more efficiently for sparse networks. Several methods exist for pruning connections of a neural network after it was trained without connectivity constraints. We present an algorithm, DEEP R, that enables us to train directly a sparsely connected neural network. DEEP R automatically rewires the network during supervised training so that connections are there where they are most needed for the task, while its total number is all the time strictly bounded. We demonstrate that DEEP R can be used to train very sparse feedforward and recurrent neural networks on standard benchmark tasks with just a minor loss in performance. DEEP R is based on a rigorous theoretical foundation that views rewiring as stochastic sampling of network configurations from a posterior.", "pdf": "/pdf/3e24f9f67b6738385d15c0f0edd85ab4110fd8a3.pdf", "TL;DR": "The paper presents Deep Rewiring, an algorithm that can be used to train deep neural networks when the network connectivity is severely constrained during training.", "paperhash": "bellec|deep_rewiring_training_very_sparse_deep_networks", "_bibtex": "@inproceedings{\nbellec2018deep,\ntitle={Deep Rewiring: Training very sparse deep networks},\nauthor={Guillaume Bellec and David Kappel and Wolfgang Maass and Robert Legenstein},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=BJ_wN01C-},\n}", "keywords": ["deep learning", "pruning", "LSTM", "convolutional networks", "recurrent neural network", "sparse networks", "neuromorphic hardware", "energy efficient computing", "low memory hardware", "stochastic differential equation", "fokker-planck equation"], "authors": ["Guillaume Bellec", "David Kappel", "Wolfgang Maass", "Robert Legenstein"], "authorids": ["bellec@igi.tugraz.at", "kappel@igi.tugraz.at", "maass@igi.tugraz.at", "legenstein@igi.tugraz.at"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642406117, "id": "ICLR.cc/2018/Conference/-/Paper189/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper189/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper189/AnonReviewer3", "ICLR.cc/2018/Conference/Paper189/AnonReviewer1", "ICLR.cc/2018/Conference/Paper189/AnonReviewer2"], "reply": {"forum": "BJ_wN01C-", "replyto": "BJ_wN01C-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper189/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642406117}}}], "count": 6}