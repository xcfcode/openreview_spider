{"notes": [{"id": "HJgb7lSFwS", "original": "BklVh7xFvS", "number": 2200, "cdate": 1569439769131, "ddate": null, "tcdate": 1569439769131, "tmdate": 1577168290191, "tddate": null, "forum": "HJgb7lSFwS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["graham.spinks@cs.kuleuven.be", "sien.moens@cs.kuleuven.be"], "title": "Distance-based Composable Representations with Neural Networks", "authors": ["Graham Spinks", "Marie-Francine Moens"], "pdf": "/pdf/d21c65a73518e618cabb980e9daa65e3ff68a383.pdf", "abstract": "We introduce a new deep learning technique that builds individual and class representations based on distance estimates to randomly generated contextual dimensions for different modalities. Recent works have demonstrated advantages to creating representations from probability distributions over their contexts rather than single points in a low-dimensional Euclidean vector space. These methods, however, rely on pre-existing features and are limited to textual information. In this work, we obtain generic template representations that are vectors containing the average distance of a class to randomly generated contextual information. These representations have the benefit of being both interpretable and composable. They are initially learned by estimating the Wasserstein distance for different data subsets with deep neural networks. Individual samples or instances can then be compared to the generic class representations, which we call templates, to determine their similarity and thus class membership. We show that this technique, which we call WDVec, delivers good results for multi-label image classification. Additionally, we illustrate the benefit of templates and their composability by performing retrieval with complex queries where we modify the information content in the representations. Our method can be used in conjunction with any existing neural network and create theoretically infinitely large feature maps.", "keywords": ["Representation learning", "Wasserstein distance", "Composability", "Templates"], "paperhash": "spinks|distancebased_composable_representations_with_neural_networks", "original_pdf": "/attachment/ef760f6458066b2b349f2f38627cd3081619d330.pdf", "_bibtex": "@misc{\nspinks2020distancebased,\ntitle={Distance-based Composable Representations with Neural Networks},\nauthor={Graham Spinks and Marie-Francine Moens},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgb7lSFwS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "b7BMFDzfOF", "original": null, "number": 1, "cdate": 1576798743041, "ddate": null, "tcdate": 1576798743041, "tmdate": 1576800893160, "tddate": null, "forum": "HJgb7lSFwS", "replyto": "HJgb7lSFwS", "invitation": "ICLR.cc/2020/Conference/Paper2200/-/Decision", "content": {"decision": "Reject", "comment": "The paper proposes an approach for learning class-level and individual-level (token-level) representations based on Wasserstein distances between data subsets.  The idea is appealing and seems to have applicability to multiple tasks.  The reviewers voiced significant concerns with the unclear writing of the paper and with the limited experiments.  The authors have improved the paper, but to my mind it still needs a good amount of work on both of these aspects.  The choice of wording in many places is imprecise.  The tasks are non-standard ones so they don't have existing published numbers to compare against; in such a situation I would expect to see more baselines, such as alternative class/instance representations that would show the benefit specifically of the Wasserstein distance-based approach.  I cannot tell from the paper in its current form whether or when I would want to use the proposed approach.  In short, despite a very interesting initial idea, I believe the paper is too preliminary for publication.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["graham.spinks@cs.kuleuven.be", "sien.moens@cs.kuleuven.be"], "title": "Distance-based Composable Representations with Neural Networks", "authors": ["Graham Spinks", "Marie-Francine Moens"], "pdf": "/pdf/d21c65a73518e618cabb980e9daa65e3ff68a383.pdf", "abstract": "We introduce a new deep learning technique that builds individual and class representations based on distance estimates to randomly generated contextual dimensions for different modalities. Recent works have demonstrated advantages to creating representations from probability distributions over their contexts rather than single points in a low-dimensional Euclidean vector space. These methods, however, rely on pre-existing features and are limited to textual information. In this work, we obtain generic template representations that are vectors containing the average distance of a class to randomly generated contextual information. These representations have the benefit of being both interpretable and composable. They are initially learned by estimating the Wasserstein distance for different data subsets with deep neural networks. Individual samples or instances can then be compared to the generic class representations, which we call templates, to determine their similarity and thus class membership. We show that this technique, which we call WDVec, delivers good results for multi-label image classification. Additionally, we illustrate the benefit of templates and their composability by performing retrieval with complex queries where we modify the information content in the representations. Our method can be used in conjunction with any existing neural network and create theoretically infinitely large feature maps.", "keywords": ["Representation learning", "Wasserstein distance", "Composability", "Templates"], "paperhash": "spinks|distancebased_composable_representations_with_neural_networks", "original_pdf": "/attachment/ef760f6458066b2b349f2f38627cd3081619d330.pdf", "_bibtex": "@misc{\nspinks2020distancebased,\ntitle={Distance-based Composable Representations with Neural Networks},\nauthor={Graham Spinks and Marie-Francine Moens},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgb7lSFwS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJgb7lSFwS", "replyto": "HJgb7lSFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795715753, "tmdate": 1576800265735, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2200/-/Decision"}}}, {"id": "rkxVztJjsB", "original": null, "number": 4, "cdate": 1573742860186, "ddate": null, "tcdate": 1573742860186, "tmdate": 1573742860186, "tddate": null, "forum": "HJgb7lSFwS", "replyto": "HJgb7lSFwS", "invitation": "ICLR.cc/2020/Conference/Paper2200/-/Official_Comment", "content": {"title": "Reviewers, any comments on author response?", "comment": "Dear Reviewers, thanks for your thoughtful input on this submission!  The authors have now responded to your comments.  Please be sure to go through their replies and revisions.  If you have additional feedback or questions, it would be great to get them this week while the authors still have the opportunity to respond/revise further.  Thanks!\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2200/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2200/Area_Chair1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["graham.spinks@cs.kuleuven.be", "sien.moens@cs.kuleuven.be"], "title": "Distance-based Composable Representations with Neural Networks", "authors": ["Graham Spinks", "Marie-Francine Moens"], "pdf": "/pdf/d21c65a73518e618cabb980e9daa65e3ff68a383.pdf", "abstract": "We introduce a new deep learning technique that builds individual and class representations based on distance estimates to randomly generated contextual dimensions for different modalities. Recent works have demonstrated advantages to creating representations from probability distributions over their contexts rather than single points in a low-dimensional Euclidean vector space. These methods, however, rely on pre-existing features and are limited to textual information. In this work, we obtain generic template representations that are vectors containing the average distance of a class to randomly generated contextual information. These representations have the benefit of being both interpretable and composable. They are initially learned by estimating the Wasserstein distance for different data subsets with deep neural networks. Individual samples or instances can then be compared to the generic class representations, which we call templates, to determine their similarity and thus class membership. We show that this technique, which we call WDVec, delivers good results for multi-label image classification. Additionally, we illustrate the benefit of templates and their composability by performing retrieval with complex queries where we modify the information content in the representations. Our method can be used in conjunction with any existing neural network and create theoretically infinitely large feature maps.", "keywords": ["Representation learning", "Wasserstein distance", "Composability", "Templates"], "paperhash": "spinks|distancebased_composable_representations_with_neural_networks", "original_pdf": "/attachment/ef760f6458066b2b349f2f38627cd3081619d330.pdf", "_bibtex": "@misc{\nspinks2020distancebased,\ntitle={Distance-based Composable Representations with Neural Networks},\nauthor={Graham Spinks and Marie-Francine Moens},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgb7lSFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJgb7lSFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2200/Authors", "ICLR.cc/2020/Conference/Paper2200/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2200/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2200/Reviewers", "ICLR.cc/2020/Conference/Paper2200/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2200/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2200/Authors|ICLR.cc/2020/Conference/Paper2200/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144869, "tmdate": 1576860531247, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2200/Authors", "ICLR.cc/2020/Conference/Paper2200/Reviewers", "ICLR.cc/2020/Conference/Paper2200/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2200/-/Official_Comment"}}}, {"id": "rJg7R9qcsS", "original": null, "number": 3, "cdate": 1573722827154, "ddate": null, "tcdate": 1573722827154, "tmdate": 1573722827154, "tddate": null, "forum": "HJgb7lSFwS", "replyto": "Skx0fuGctH", "invitation": "ICLR.cc/2020/Conference/Paper2200/-/Official_Comment", "content": {"title": "Reply to reviewer #2", "comment": "\u201cThe general idea of measuring the distribution divergence for a set of classes is interesting and seems to be novel.\u201d\nThank you for your review and the effort you put into it. \n\n\u201c[...]\n- A set of divergences doesn't contain any pixel-level information, only divergences to some predefined classes\n- As a consequence, this representation will not be able to discover information that is not covered by the labels\nBecause of these limitations, it seems that this particular representation may be less useful for some applications than others.\u201d\nWe would argue that, to the contrary, using labeled information is useful and many applications require such representations. Our approach offers an easy method to integrate continuous (images) and discrete (labels) information into one representation which, for instance, offers many possibilities given the current state of object recognizers in computer vision. In particular, using labeled data to explore both global and local similarities over different classes can be useful in many specialized or critical applications where wrong classification results are highly undesirable (medical diagnosis, robotic automation,...) \n\n\u201cI don't follow why the paper proposes to use 'environments' -- random combinations of classes. It seems that a square matrix (n_c x n_c) with all classes should do the same job.\u201d\nNote that the experiments that are detailed in section 4.2 with plot in appendix A.1 experimentally confirm the validity of using random combinations. Results for $R$=1 are indicative of the outcome when using a square matrix, yet the results improve significantly for larger $R$ values. We also explicitly tried using a square matrix which did not work as well. We have clarified why we use environments in the revised submission (sections 2 and 4.2). In short, we follow the findings of the work of [1] that apply the theory of Random Feature Approximations [2] which has shown to lead to beneficial shift-invariant properties. \n\n\u201cThe experimentation is very weak and does very little to support the claims. The paper considers only one substantial task to test the representation. This task is image retrieval by image query. The paper doesn't provide any comparison to existing methods or simple baselines.\u201d\nWith all due respect, this is not correct on two accounts: first, we perform not only retrieval experiments (section 4.3), but also classification experiments on the obtained representation (4.2). Additionally, for both experiments we provide a comparison to baselines. For the classification task we provide baselines with three state-of-the-art classification models that employ binary cross-entropy (see table 1, section 4.2), for the retrieval based on modified representations we provide a baseline on the basis of CNN features (see table 2, section 4.3).\n\n\u201cThe second contribution that the representations are interpretable and composable is not addressed. I seems that it should be hard to interpret a large vector of distances to randomly chosen subsets of classes. There is no experiment demonstrating interpretability of the proposed approach. The compositionality is not addressed either. [...]\u201d \nWe respectfully disagree on this account as well. The retrieval experiment in section 4.3 is specifically designed to illustrate the interpretability and composability following the method described in section 3.5. In that experiment we  compose new representations from existing representations by exploiting the structure of the representations that are interpretable over rows and columns. We subsequently retrieve images that are similar to the modified representations, which we quantitatively have evaluated in table 2 and qualitatively in figure A2. Additionally, the \u2018SIM\u2019 retrieval method for this experiment relies on the interpretability of representations over different classes. \n\n\u201cThe paper is generally well written and it is easy to follow. The literature review can be improved by [...]\u201d\nThank you, we have improved this section in the revised version by adding several relevant works.\n\n\u201c- The proposed technique can be used with any task, but the paper is clearly limited to the retrieval task\n- The environments are too vaguely described and can be misinterpreted in the introduction\u201d\nAgain we note that we implemented both a classification task and retrieval task. We have rephrased and detailed certain parts of the paper.\n\n\u201cI recommend using term divergence instead of distance when it is not symmetrical.\u201d\nCould you specifically refer to a relevant instance? All distance estimates employed in the paper are intended as approximations or estimates of the Wasserstein distance which is a distance between distributions. The IPM formulation is found to be symmetric and satisfies the triangular inequality when Lipschitz continuous [3].\n\n[1] https://arxiv.org/abs/1811.01713\n[2] http://papers.nips.cc/paper/3182-random-features-for-large-scale-kernel-machines.pdf\n[3] https://arxiv.org/abs/1701.07875"}, "signatures": ["ICLR.cc/2020/Conference/Paper2200/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2200/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["graham.spinks@cs.kuleuven.be", "sien.moens@cs.kuleuven.be"], "title": "Distance-based Composable Representations with Neural Networks", "authors": ["Graham Spinks", "Marie-Francine Moens"], "pdf": "/pdf/d21c65a73518e618cabb980e9daa65e3ff68a383.pdf", "abstract": "We introduce a new deep learning technique that builds individual and class representations based on distance estimates to randomly generated contextual dimensions for different modalities. Recent works have demonstrated advantages to creating representations from probability distributions over their contexts rather than single points in a low-dimensional Euclidean vector space. These methods, however, rely on pre-existing features and are limited to textual information. In this work, we obtain generic template representations that are vectors containing the average distance of a class to randomly generated contextual information. These representations have the benefit of being both interpretable and composable. They are initially learned by estimating the Wasserstein distance for different data subsets with deep neural networks. Individual samples or instances can then be compared to the generic class representations, which we call templates, to determine their similarity and thus class membership. We show that this technique, which we call WDVec, delivers good results for multi-label image classification. Additionally, we illustrate the benefit of templates and their composability by performing retrieval with complex queries where we modify the information content in the representations. Our method can be used in conjunction with any existing neural network and create theoretically infinitely large feature maps.", "keywords": ["Representation learning", "Wasserstein distance", "Composability", "Templates"], "paperhash": "spinks|distancebased_composable_representations_with_neural_networks", "original_pdf": "/attachment/ef760f6458066b2b349f2f38627cd3081619d330.pdf", "_bibtex": "@misc{\nspinks2020distancebased,\ntitle={Distance-based Composable Representations with Neural Networks},\nauthor={Graham Spinks and Marie-Francine Moens},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgb7lSFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJgb7lSFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2200/Authors", "ICLR.cc/2020/Conference/Paper2200/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2200/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2200/Reviewers", "ICLR.cc/2020/Conference/Paper2200/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2200/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2200/Authors|ICLR.cc/2020/Conference/Paper2200/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144869, "tmdate": 1576860531247, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2200/Authors", "ICLR.cc/2020/Conference/Paper2200/Reviewers", "ICLR.cc/2020/Conference/Paper2200/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2200/-/Official_Comment"}}}, {"id": "Hkejtt59jB", "original": null, "number": 2, "cdate": 1573722498600, "ddate": null, "tcdate": 1573722498600, "tmdate": 1573722498600, "tddate": null, "forum": "HJgb7lSFwS", "replyto": "HygNqe2ntB", "invitation": "ICLR.cc/2020/Conference/Paper2200/-/Official_Comment", "content": {"title": "Reply to reviewer #3", "comment": "We thank reviewer #3 for the elaborate and very constructive review. We appreciate that you find it an excellent idea, yet acknowledge that the exposition had some issues. We have tried to address your concerns and have thoroughly improved the formulation in the revised version (especially section 3). \n\n\u201cHowever, the exposition could be greatly improved by using the standard language of probability theory. The discussion in 3.1 was particularly painful to read. What is the difference between \"existing in an environment\" and \"conditioning on a measurable event\"? Phrases like \"belonging to any random subset of the dataset\" suggest a non-deterministic method of selecting an element of the power set of the training data, but it is unclear what to do if more training data arrives in this case.\u201d\nIndeed, these phrases lacked clarity and we have modified these sentences throughout the whole text in the revised version and especially in section 3.1. \n\n\u201cThroughout the entire paper the word \"random\" is apparently used in the colloquial sense of \"arbitrary\". *Correct every instance of this.* If you actually are referring to generating samples from a distribution, be explicit about the generative process.\u201d\nWe have corrected our use of the word \u201crandom\u201d in the paper. Our intention is to convey the following: Environments are randomly generated in the following manner: the size is uniformly selected from the range given by [1,$R$] and the attributes that make up the environments are uniformly selected without replacement from the set of all attributes.\n\n\u201cSection 3.5 was more confusing than enlightening. In general I understand that environments can be leveraged for intelligibility and admit manipulation for information retrieval. The exact strategy remains somewhat opaque. If you are under space constraints refer to an appendix with more explicit details.\u201d\nWe have rephrased and detailed section 3.5 to improve clarity. \n\n\u201cIn the experiments section phrases like \"environments consist of random combinations of classes\" is also not helpful. Do you mean something like \"uniformly selected from the set of all class pairs?\" Or something like \"uniformly selected from the power set of all classes?\" \nWe improved this formulation (as well as other phrases) as you have suggested. These improvements can be found throughout the text and especially in the introduction and section 3.\n\n\u201cHow volatile are the experimental results with respect to the non-deterministic choice of environments?\u201d\nAs can be seen from the standard deviations in table 1, the F1 scores in the classification task are not impacted much for different non-deterministic choices. Intuitively, the sensitivity depends on the values of the hyperparameters $n_e$ and $R$ , the amount and maximum size of the environments respectively. We thus added additional sensitivity analyses in appendix A1 (tables A1 and A2) of the revised version, illustrating that the sensitivity remains low for all values of $R$ or $n_e$.\n\n\u201cThe technique bears some resemblance to Wasserstein Discriminant Analysis.[1] [...] That is ok since the representation is designed to be used for a variety of tasks (modulo section 4.2), but it does leave open the question \"what if the matrix of estimated Wasserstein distances isn't informative, e.g., due to poor choice of environments?\" There is no attempt to assess the representation except via utility in downstream tasks.\u201d \nThank you for this reference, we have added it in section 2 (background \u2013 recent work). As mentioned above, we added figures that show a small variance for classification outcomes for different values of $R$ or $n_e$, which suggests the representations are quite robust with respect to the choice of environments. \n\n\u201cThe common representation was justified computationally, but I suspect is beneficial statistically. [...]\u201d\nThese are interesting points. From our composition experiments we found that, for the given values of $R$ and $n_e$ , the spectrum was very non-flat which suggests indeed that the representation could be further compressed to a large degree.  We believe that your suggestion of a diagnostic to guard against insufficient capacity is very interesting and could be part of further future work, for example by evaluating the evolution of the spectrum of the representations as training progresses.\n\n\u201cI am curious what the results in appendix A.1. look like relative to the spectral norm or the smallest eigenvalue of the estimated WD matrix (smallest eigenvalue assuming number of environments < number of classes, otherwise the k-th eigenvalue where k = number of classes).\u201d\nWe have added tables A.3 and A.4 in the appendix that show average values of the spectral norm of 100 representations for different values of $R$ and $n_e$."}, "signatures": ["ICLR.cc/2020/Conference/Paper2200/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2200/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["graham.spinks@cs.kuleuven.be", "sien.moens@cs.kuleuven.be"], "title": "Distance-based Composable Representations with Neural Networks", "authors": ["Graham Spinks", "Marie-Francine Moens"], "pdf": "/pdf/d21c65a73518e618cabb980e9daa65e3ff68a383.pdf", "abstract": "We introduce a new deep learning technique that builds individual and class representations based on distance estimates to randomly generated contextual dimensions for different modalities. Recent works have demonstrated advantages to creating representations from probability distributions over their contexts rather than single points in a low-dimensional Euclidean vector space. These methods, however, rely on pre-existing features and are limited to textual information. In this work, we obtain generic template representations that are vectors containing the average distance of a class to randomly generated contextual information. These representations have the benefit of being both interpretable and composable. They are initially learned by estimating the Wasserstein distance for different data subsets with deep neural networks. Individual samples or instances can then be compared to the generic class representations, which we call templates, to determine their similarity and thus class membership. We show that this technique, which we call WDVec, delivers good results for multi-label image classification. Additionally, we illustrate the benefit of templates and their composability by performing retrieval with complex queries where we modify the information content in the representations. Our method can be used in conjunction with any existing neural network and create theoretically infinitely large feature maps.", "keywords": ["Representation learning", "Wasserstein distance", "Composability", "Templates"], "paperhash": "spinks|distancebased_composable_representations_with_neural_networks", "original_pdf": "/attachment/ef760f6458066b2b349f2f38627cd3081619d330.pdf", "_bibtex": "@misc{\nspinks2020distancebased,\ntitle={Distance-based Composable Representations with Neural Networks},\nauthor={Graham Spinks and Marie-Francine Moens},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgb7lSFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJgb7lSFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2200/Authors", "ICLR.cc/2020/Conference/Paper2200/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2200/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2200/Reviewers", "ICLR.cc/2020/Conference/Paper2200/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2200/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2200/Authors|ICLR.cc/2020/Conference/Paper2200/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144869, "tmdate": 1576860531247, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2200/Authors", "ICLR.cc/2020/Conference/Paper2200/Reviewers", "ICLR.cc/2020/Conference/Paper2200/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2200/-/Official_Comment"}}}, {"id": "rJxIZt9qsS", "original": null, "number": 1, "cdate": 1573722366168, "ddate": null, "tcdate": 1573722366168, "tmdate": 1573722366168, "tddate": null, "forum": "HJgb7lSFwS", "replyto": "rkxEKlP0tr", "invitation": "ICLR.cc/2020/Conference/Paper2200/-/Official_Comment", "content": {"title": "Reply to reviewer #1", "comment": "Thank you for your constructive review. Below we address your concerns. \n\n\u201c- Since the environments are taken randomly in the experiments, it is not investigated how sensitive the method is with respect to the choices of environments. \u201d\nWe partially addressed this concern in the results of table 1 (page 8) that shows the average F1 scores for the classification task over several runs, where each run has a different randomly selected choice of environments. From this table it is clear that the standard deviation in F1 scores is low and in line with the standard deviations of the baselines. This suggests that for sufficiently large $n_e$ and $R$ (the parameters that determine the amount of environments and the maximum amount of attributes per environment) the method is not sensitive with respect to the choices of environments. We also added this sensitivity for different choices of $n_e$ and $R$ to the appendix of the revised version (see appendix A1, tables A1 and A2), where it becomes clear that even for small values, the sensitivity is low. \n\n\u201cAlso, does it make any sense to design environments to include related (and not random) classes?\u201d\nThe rationale for not designing (handmade) environments is that they require knowledge about what would lead to distinguishing features.  Note that the randomly selected features will lead to many environments that are related to any class, thus ensuring a good choice over any set of features. The idea is inspired by the Random Features approximation as developed in the work of [1] and [2], we have clarified this in the revision (\u2018Recent work\u2019 in section 2).\n\n\u201c- It seems necessary to include some experiments to assess sensitivity of the interpretation with regard to the small perturbations that are not changing the class label.\u201d\nOur interpretation of your question is as follows, please correct us if necessary: \u201cWhat amount of perturbation is needed before the interpretation of the class label of the representation changes?\u201d. As part of the retrieval method we evaluated the size of the factor $q$ (the factor that determines how much the representation is modified for the retrieval experiment in section 4.3) and its influence on the change in class. This gives an estimate of the sensitivity as it shows that small perturbations to representations don\u2019t easily modify class membership. We have added the results in section 4.3.\n\n[1] https://arxiv.org/abs/1811.01713\n[2] http://papers.nips.cc/paper/3182-random-features-for-large-scale-kernel-machines.pdf"}, "signatures": ["ICLR.cc/2020/Conference/Paper2200/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2200/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["graham.spinks@cs.kuleuven.be", "sien.moens@cs.kuleuven.be"], "title": "Distance-based Composable Representations with Neural Networks", "authors": ["Graham Spinks", "Marie-Francine Moens"], "pdf": "/pdf/d21c65a73518e618cabb980e9daa65e3ff68a383.pdf", "abstract": "We introduce a new deep learning technique that builds individual and class representations based on distance estimates to randomly generated contextual dimensions for different modalities. Recent works have demonstrated advantages to creating representations from probability distributions over their contexts rather than single points in a low-dimensional Euclidean vector space. These methods, however, rely on pre-existing features and are limited to textual information. In this work, we obtain generic template representations that are vectors containing the average distance of a class to randomly generated contextual information. These representations have the benefit of being both interpretable and composable. They are initially learned by estimating the Wasserstein distance for different data subsets with deep neural networks. Individual samples or instances can then be compared to the generic class representations, which we call templates, to determine their similarity and thus class membership. We show that this technique, which we call WDVec, delivers good results for multi-label image classification. Additionally, we illustrate the benefit of templates and their composability by performing retrieval with complex queries where we modify the information content in the representations. Our method can be used in conjunction with any existing neural network and create theoretically infinitely large feature maps.", "keywords": ["Representation learning", "Wasserstein distance", "Composability", "Templates"], "paperhash": "spinks|distancebased_composable_representations_with_neural_networks", "original_pdf": "/attachment/ef760f6458066b2b349f2f38627cd3081619d330.pdf", "_bibtex": "@misc{\nspinks2020distancebased,\ntitle={Distance-based Composable Representations with Neural Networks},\nauthor={Graham Spinks and Marie-Francine Moens},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgb7lSFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJgb7lSFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2200/Authors", "ICLR.cc/2020/Conference/Paper2200/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2200/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2200/Reviewers", "ICLR.cc/2020/Conference/Paper2200/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2200/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2200/Authors|ICLR.cc/2020/Conference/Paper2200/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144869, "tmdate": 1576860531247, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2200/Authors", "ICLR.cc/2020/Conference/Paper2200/Reviewers", "ICLR.cc/2020/Conference/Paper2200/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2200/-/Official_Comment"}}}, {"id": "Skx0fuGctH", "original": null, "number": 1, "cdate": 1571592214192, "ddate": null, "tcdate": 1571592214192, "tmdate": 1572972370059, "tddate": null, "forum": "HJgb7lSFwS", "replyto": "HJgb7lSFwS", "invitation": "ICLR.cc/2020/Conference/Paper2200/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Paper contributions\n=================\n- This paper proposes a method for constructing representations using a matrix of Wasserstein distances. These distances measure the discrepancy between each class and each environment, that is a random combination of some classes.\n- The paper evaluates this approach on a task of image retrieval.\n\nGeneral notes\n============\nThe general idea of measuring the distribution divergence for a set of classes is interesting and seems to be novel. But I argue that this representation can be limiting:\n- A set of divergences doesn't contain any pixel-level information, only divergences to some predefined classes\n- As a consequence, this representation will not be able to discover information that is not covered by the labels\nBecause of these limitations, it seems that this particular representation may be less useful for some applications than others.\n\nI don't follow why the paper proposes to use 'environments'  -- random combinations of classes. It seems that a square matrix (n_c x n_c) with all classes should do the same job.\n\nThe experimentation is very weak and does very little to support the claims. The paper considers only one substantial task to test the representation. This task is image retrieval by image query. The paper doesn't provide any comparison to existing methods or simple baselines.\n\nThe second contribution that the representations are interpretable and composable is not addressed.  I seems that it should be hard to interpret a large vector of distances to randomly chosen subsets of classes. There is no experiment demonstrating interpretability of the proposed approach. The compositionality is not addressed either. The samples provided in the appendix are not convincing.\n\nThe paper is generally well written and it is easy to follow. The literature review can be improved by providing prior work where \"approaches use hidden state vector of LSTM\" and \"features extracted from CNNs\" instead of generic references. \n\nSome of the claims are vague and excessively broad:\n- The proposed technique can be used with any task, but the paper is clearly limited to the retrieval task\n- The environments are too vaguely described and can be misinterpreted in the introduction\n\nConclusion\n=========\n\nI recommend to reject on the basis that \n- the approach is more limited than the paper advocates\n- the experimentation is weak\n- some claims are not addressed\n\nOther notes\n==========\nI recommend using term divergence instead of distance when it is not symmetrical."}, "signatures": ["ICLR.cc/2020/Conference/Paper2200/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2200/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["graham.spinks@cs.kuleuven.be", "sien.moens@cs.kuleuven.be"], "title": "Distance-based Composable Representations with Neural Networks", "authors": ["Graham Spinks", "Marie-Francine Moens"], "pdf": "/pdf/d21c65a73518e618cabb980e9daa65e3ff68a383.pdf", "abstract": "We introduce a new deep learning technique that builds individual and class representations based on distance estimates to randomly generated contextual dimensions for different modalities. Recent works have demonstrated advantages to creating representations from probability distributions over their contexts rather than single points in a low-dimensional Euclidean vector space. These methods, however, rely on pre-existing features and are limited to textual information. In this work, we obtain generic template representations that are vectors containing the average distance of a class to randomly generated contextual information. These representations have the benefit of being both interpretable and composable. They are initially learned by estimating the Wasserstein distance for different data subsets with deep neural networks. Individual samples or instances can then be compared to the generic class representations, which we call templates, to determine their similarity and thus class membership. We show that this technique, which we call WDVec, delivers good results for multi-label image classification. Additionally, we illustrate the benefit of templates and their composability by performing retrieval with complex queries where we modify the information content in the representations. Our method can be used in conjunction with any existing neural network and create theoretically infinitely large feature maps.", "keywords": ["Representation learning", "Wasserstein distance", "Composability", "Templates"], "paperhash": "spinks|distancebased_composable_representations_with_neural_networks", "original_pdf": "/attachment/ef760f6458066b2b349f2f38627cd3081619d330.pdf", "_bibtex": "@misc{\nspinks2020distancebased,\ntitle={Distance-based Composable Representations with Neural Networks},\nauthor={Graham Spinks and Marie-Francine Moens},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgb7lSFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJgb7lSFwS", "replyto": "HJgb7lSFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2200/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2200/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575821543832, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2200/Reviewers"], "noninvitees": [], "tcdate": 1570237726270, "tmdate": 1575821543851, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2200/-/Official_Review"}}}, {"id": "HygNqe2ntB", "original": null, "number": 2, "cdate": 1571762316009, "ddate": null, "tcdate": 1571762316009, "tmdate": 1572972370009, "tddate": null, "forum": "HJgb7lSFwS", "replyto": "HJgb7lSFwS", "invitation": "ICLR.cc/2020/Conference/Paper2200/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper defines a representation learning strategy based upon estimation\nof a matrix of Wasserstein distances.\n\nThe idea is excellent.  The ability to \"solve\" IPMs reliably is a recent\ndevelopment in deep learning whose ramifications are still being explored.\nIntuitively this line of research could plausibly result in general\nmethods which are theoretically intelligible and broadly applicable.\nIndexing at least one side of the matrix of estimated WDs with events\n(rather than classes) has interpretability properties useful for\ninformation retrieval and also conveys benefits reminiscent of learning\nwith privileged information.\n\nHowever, the exposition could be greatly improved by using the\nstandard language of probability theory.  The discussion in 3.1\nwas particularly painful to read.  What is the difference between\n\"existing in an environment\" and \"conditioning on a measurable event\"?\nPhrases like \"belonging to any random subset of the dataset\" suggest\na non-deterministic method of selecting an element of the power set of\nthe training data, but it is unclear what to do if more training data\narrives in this case.  \n\nThroughout the entire paper the word \"random\" is apparently used in the \ncolloquial sense of \"arbitrary\".  *Correct every instance of this.*\nIf you actually are referring to generating samples from a distribution,\nbe explicit about the generative process.\n\nSection 3.5 was more confusing than enlightening.  In general I understand \nthat environments can be leveraged for intelligibility and admit manipulation \nfor information retrieval.  The exact strategy remains somewhat opaque.  If \nyou are under space constraints refer to an appendix with more explicit details.\n\nIn the experiments section phrases like \"environments consist of random \ncombinations of classes\" is also not helpful.  Do you mean something like \n\"uniformly selected from the set of all class pairs?\"  Or something like \n\"uniformly selected from the power set of all classes?\"  How volatile\nare the experimental results with respect to the non-deterministic choice \nof environments? \n\nI want to accept this paper if the exposition is improved, which I think\nis possible during the response period.\n\nMy other comments are not blocking issues, but would either improve the\ncurrent paper or inform future directions of research.\n\nThe technique bears some resemblance to Wasserstein Discriminant\nAnalysis.[1]  That paper seeks a projection that maximizes the ratio of \nWasserstein distance between classes vs. within classes.  Here, \nalthough the common representation is a nonlinear mapping \nanalogous to a projection, we merely try to estimate all the\nWasserstein distances rather than maximize them, so it is not trained\nto be discriminative per se.   That is ok since the representation is\ndesigned to be used for a variety of tasks (modulo section 4.2), but it \ndoes leave open the question \"what if the matrix of estimated \nWasserstein distances isn't informative, e.g., due to poor choice of \nenvironments?\"  There is no attempt to assess the representation \nexcept via utility in downstream tasks.\n\nThe common representation was justified computationally, but I suspect\nis beneficial statistically.  It might facilitate safely including a\nlarge number of environments and then spectrally compressing (i.e., SVD)\nthe resulting matrix without overfitting the data.  However clearly if\nthe capacity of this layer is too small, then all estimated WDs will\nbe close to zero.  If we posit a low Bayes error classifier for the\nmulti-class problem associated with the dataset, that might imply there\nis some conditioning of the input under which the matrix of (actual) WDs\nhas rank equal to the number of classes, which would in turn provide a\nuseful diagnostic to guard against an insufficiently discriminative choice\nof environments or insufficient capacity in the common representation. If\nthe matrix is full rank with a flat spectrum, however, that might indicate\nthe choice of environments is too granular and overfitting has occurred,\nit's not immediately obvious to me how to guard against this.  \n\nI am curious what the results in appendix A.1. look like relative to the spectral \nnorm or the smallest eigenvalue of the estimated WD matrix (smallest \neigenvalue assuming number of environments < number of classes,\notherwise the k-th eigenvalue where k = number of classes).\n\n[1] https://arxiv.org/abs/1608.08063\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2200/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2200/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["graham.spinks@cs.kuleuven.be", "sien.moens@cs.kuleuven.be"], "title": "Distance-based Composable Representations with Neural Networks", "authors": ["Graham Spinks", "Marie-Francine Moens"], "pdf": "/pdf/d21c65a73518e618cabb980e9daa65e3ff68a383.pdf", "abstract": "We introduce a new deep learning technique that builds individual and class representations based on distance estimates to randomly generated contextual dimensions for different modalities. Recent works have demonstrated advantages to creating representations from probability distributions over their contexts rather than single points in a low-dimensional Euclidean vector space. These methods, however, rely on pre-existing features and are limited to textual information. In this work, we obtain generic template representations that are vectors containing the average distance of a class to randomly generated contextual information. These representations have the benefit of being both interpretable and composable. They are initially learned by estimating the Wasserstein distance for different data subsets with deep neural networks. Individual samples or instances can then be compared to the generic class representations, which we call templates, to determine their similarity and thus class membership. We show that this technique, which we call WDVec, delivers good results for multi-label image classification. Additionally, we illustrate the benefit of templates and their composability by performing retrieval with complex queries where we modify the information content in the representations. Our method can be used in conjunction with any existing neural network and create theoretically infinitely large feature maps.", "keywords": ["Representation learning", "Wasserstein distance", "Composability", "Templates"], "paperhash": "spinks|distancebased_composable_representations_with_neural_networks", "original_pdf": "/attachment/ef760f6458066b2b349f2f38627cd3081619d330.pdf", "_bibtex": "@misc{\nspinks2020distancebased,\ntitle={Distance-based Composable Representations with Neural Networks},\nauthor={Graham Spinks and Marie-Francine Moens},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgb7lSFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJgb7lSFwS", "replyto": "HJgb7lSFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2200/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2200/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575821543832, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2200/Reviewers"], "noninvitees": [], "tcdate": 1570237726270, "tmdate": 1575821543851, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2200/-/Official_Review"}}}, {"id": "rkxEKlP0tr", "original": null, "number": 3, "cdate": 1571872891698, "ddate": null, "tcdate": 1571872891698, "tmdate": 1572972369965, "tddate": null, "forum": "HJgb7lSFwS", "replyto": "HJgb7lSFwS", "invitation": "ICLR.cc/2020/Conference/Paper2200/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors proposed a template-based interpretable representation that works based on the earth mover's distance of each class to a number of \"environments\", which could be taken as union of a few random classes. To achieve this, they train several critics based on Fisher GAN. The method is evaluated based on classification and retrieval tasks. \nThe representation, by construction, is aimed towards interpretation and is specially useful in multi-class classification tasks. \nHere are my concerns:\n- Since the environments are taken randomly in the experiments, it is not investigated how sensitive the method is with respect to the choices of environments. Also, does it make any sense to design environments to include related (and not random) classes?\n- It seems necessary to include some experiments to assess sensitivity of the interpretation with regard to the small perturbations that are not changing the class label. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2200/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2200/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["graham.spinks@cs.kuleuven.be", "sien.moens@cs.kuleuven.be"], "title": "Distance-based Composable Representations with Neural Networks", "authors": ["Graham Spinks", "Marie-Francine Moens"], "pdf": "/pdf/d21c65a73518e618cabb980e9daa65e3ff68a383.pdf", "abstract": "We introduce a new deep learning technique that builds individual and class representations based on distance estimates to randomly generated contextual dimensions for different modalities. Recent works have demonstrated advantages to creating representations from probability distributions over their contexts rather than single points in a low-dimensional Euclidean vector space. These methods, however, rely on pre-existing features and are limited to textual information. In this work, we obtain generic template representations that are vectors containing the average distance of a class to randomly generated contextual information. These representations have the benefit of being both interpretable and composable. They are initially learned by estimating the Wasserstein distance for different data subsets with deep neural networks. Individual samples or instances can then be compared to the generic class representations, which we call templates, to determine their similarity and thus class membership. We show that this technique, which we call WDVec, delivers good results for multi-label image classification. Additionally, we illustrate the benefit of templates and their composability by performing retrieval with complex queries where we modify the information content in the representations. Our method can be used in conjunction with any existing neural network and create theoretically infinitely large feature maps.", "keywords": ["Representation learning", "Wasserstein distance", "Composability", "Templates"], "paperhash": "spinks|distancebased_composable_representations_with_neural_networks", "original_pdf": "/attachment/ef760f6458066b2b349f2f38627cd3081619d330.pdf", "_bibtex": "@misc{\nspinks2020distancebased,\ntitle={Distance-based Composable Representations with Neural Networks},\nauthor={Graham Spinks and Marie-Francine Moens},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgb7lSFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJgb7lSFwS", "replyto": "HJgb7lSFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2200/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2200/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575821543832, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2200/Reviewers"], "noninvitees": [], "tcdate": 1570237726270, "tmdate": 1575821543851, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2200/-/Official_Review"}}}], "count": 9}