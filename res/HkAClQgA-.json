{"notes": [{"tddate": null, "ddate": null, "tmdate": 1519322843545, "tcdate": 1509073109922, "number": 228, "cdate": 1518730184682, "id": "HkAClQgA-", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "HkAClQgA-", "original": "HJTAgQgRW", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "A Deep Reinforced Model for Abstractive Summarization", "abstract": "Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL). \nModels trained only with supervised learning often exhibit \"exposure bias\" - they assume ground truth is provided at each step during training.\nHowever, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable.\nWe evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries.", "pdf": "/pdf/ae4dce902a6df56754a36bcc69366eca1f6e502f.pdf", "TL;DR": "A summarization model combining a new intra-attention and reinforcement learning method to increase summary ROUGE scores and quality for long sequences.", "paperhash": "paulus|a_deep_reinforced_model_for_abstractive_summarization", "_bibtex": "@inproceedings{\npaulus2018a,\ntitle={A Deep Reinforced Model for Abstractive Summarization},\nauthor={Romain Paulus and Caiming Xiong and Richard Socher},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HkAClQgA-},\n}", "keywords": ["deep learning", "natural language processing", "reinforcement learning", "text summarization", "sequence generation"], "authors": ["Romain Paulus", "Caiming Xiong", "Richard Socher"], "authorids": ["rpaulus@salesforce.com", "cxiong@salesforce.com", "richard@socher.org"]}, "nonreaders": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260098665, "tcdate": 1517249313299, "number": 106, "cdate": 1517249313285, "id": "r1FmQJpHG", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "HkAClQgA-", "replyto": "HkAClQgA-", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"title": "ICLR 2018 Conference Acceptance Decision", "comment": "This work extends upon recent ideas to build a complete summarization system using clever attention, copying, and RL training. Reviewers like the work but have some criticisms. Particularly in terms of its originality and potential significance  noting \"It is a good incremental research, but the downside of this paper is lack of innovations since most of the methods proposed in this paper are not new to us.\". Still reviewers note the experimental results are of high quality performing excellent on several datasets and building \"a strong summarization model.\" Furthermore the model is extensively tested including in \"human readability and relevance assessments \".  The work itself is well written and clear.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Deep Reinforced Model for Abstractive Summarization", "abstract": "Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL). \nModels trained only with supervised learning often exhibit \"exposure bias\" - they assume ground truth is provided at each step during training.\nHowever, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable.\nWe evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries.", "pdf": "/pdf/ae4dce902a6df56754a36bcc69366eca1f6e502f.pdf", "TL;DR": "A summarization model combining a new intra-attention and reinforcement learning method to increase summary ROUGE scores and quality for long sequences.", "paperhash": "paulus|a_deep_reinforced_model_for_abstractive_summarization", "_bibtex": "@inproceedings{\npaulus2018a,\ntitle={A Deep Reinforced Model for Abstractive Summarization},\nauthor={Romain Paulus and Caiming Xiong and Richard Socher},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HkAClQgA-},\n}", "keywords": ["deep learning", "natural language processing", "reinforcement learning", "text summarization", "sequence generation"], "authors": ["Romain Paulus", "Caiming Xiong", "Richard Socher"], "authorids": ["rpaulus@salesforce.com", "cxiong@salesforce.com", "richard@socher.org"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642413323, "tcdate": 1511806455643, "number": 1, "cdate": 1511806455643, "id": "ryxZURtlf", "invitation": "ICLR.cc/2018/Conference/-/Paper228/Official_Review", "forum": "HkAClQgA-", "replyto": "HkAClQgA-", "signatures": ["ICLR.cc/2018/Conference/Paper228/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Strong empirical contribution", "rating": "8: Top 50% of accepted papers, clear accept", "review": "The paper proposes a model for abstractive document summarization using a self-critical policy gradient training algorithm, which is mixed with maximum likelihood objective. The Seq2seq architecture incorporates both intra-temporal and intra-decoder attention, and a pointer copying mechanism. A hard constraint is imposed during decoding to avoid trigram repetition. Most of the modelling ideas already exists, but this paper show how they can be applied as a strong summarization model.\n\nThe approach obtains strong results on the CNN/Daily Mail and NYT datasets. Results show that intra-attention improves performance for only one of the datasets. RL results are reported with only the best-performing attention setup for each dataset. My concern with that is that the authors might be using the test set for model selection; It is not a priori clear that the setup that works better for ML should also be better for RL, especially as it is not the same across datasets. So I suggest that results for RL should be reported with and without intra-attention on both datasets, at least on the validation set.\n\nIt is shown that intra-decoder attention decoder improves performance on longer sentences. It would be interesting to see more analysis on this, especially analyzing what the mechanism is attending to, as it is less clear what its interpretation should be than for intra-temporal attention. Further ablations such as the effect of the trigram repetition constraint will also help to analyse the contribution of different modelling choices to the performance. \n\nFor the mixed decoding objective, how is the mixing weight chosen and what is its effect on performance? If it is purely a scaling factor, how is the scale quantified? It is claimed that readability correlates with perplexity, so it would be interesting to see perplexity results for the models. The lack of correlation between automatic and human evaluation raises interesting questions about the evaluation of abstractive summarization that should be investigated further in future work.\n\nThis is a strong paper that presents a significant improvement in document summarization.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Deep Reinforced Model for Abstractive Summarization", "abstract": "Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL). \nModels trained only with supervised learning often exhibit \"exposure bias\" - they assume ground truth is provided at each step during training.\nHowever, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable.\nWe evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries.", "pdf": "/pdf/ae4dce902a6df56754a36bcc69366eca1f6e502f.pdf", "TL;DR": "A summarization model combining a new intra-attention and reinforcement learning method to increase summary ROUGE scores and quality for long sequences.", "paperhash": "paulus|a_deep_reinforced_model_for_abstractive_summarization", "_bibtex": "@inproceedings{\npaulus2018a,\ntitle={A Deep Reinforced Model for Abstractive Summarization},\nauthor={Romain Paulus and Caiming Xiong and Richard Socher},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HkAClQgA-},\n}", "keywords": ["deep learning", "natural language processing", "reinforcement learning", "text summarization", "sequence generation"], "authors": ["Romain Paulus", "Caiming Xiong", "Richard Socher"], "authorids": ["rpaulus@salesforce.com", "cxiong@salesforce.com", "richard@socher.org"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642413162, "id": "ICLR.cc/2018/Conference/-/Paper228/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper228/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper228/AnonReviewer1", "ICLR.cc/2018/Conference/Paper228/AnonReviewer2", "ICLR.cc/2018/Conference/Paper228/AnonReviewer3"], "reply": {"forum": "HkAClQgA-", "replyto": "HkAClQgA-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper228/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642413162}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642413254, "tcdate": 1511819289608, "number": 2, "cdate": 1511819289608, "id": "HyzQdZqez", "invitation": "ICLR.cc/2018/Conference/-/Paper228/Official_Review", "forum": "HkAClQgA-", "replyto": "HkAClQgA-", "signatures": ["ICLR.cc/2018/Conference/Paper228/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "Well-motivated, with clear experimental results and thorough evaluations.", "rating": "7: Good paper, accept", "review": "This is a very clearly written paper, and a pleasure to read.\n\nIt combines some mechanisms known from previous work for summarization (intra-temporal attention; pointing mechanism with a switch) with novel architecture design components (intra-decoder attention), as well as a new training objective drawn from work from reinforcement learning, which directly optimizes ROUGE-L. The model is trained by a policy gradient algorithm. \n\nWhile the new mechanisms are simple variants of what is taken from existing work, the entire combination is well tested in the experiments. ROUGE results are reported for the full hybrid RL+ML model, as well as various versions that drop each of the new components (RL training; intra-attention). The best method finally outperforms the lea-3d baseline for summarization. What makes this paper more compelling is that they compared against a recent extractive method (Durret et al., 2016), and the fact that they also performed human readability and relevance assessments to demonstrate that their ML+RL model doesn't merely over-optimize on ROUGE. It was a nice result that only optimizing ROUGE directly leads to lower human evaluation scores, despite the fact that that model achieves the best ROUGE-1 and ROUGE-L performance on CNN/Daily Mail.\n\nSome minor points that I wonder about:\n - The heuristic against repeating trigrams seems quite crude. Is there a more sophisticated method that can avoid redundancy without this heuristic?\n - What about a reward based on a general language model, rather than one that relies on L_{ml} in Equation (14)? If the LM part really is to model grammaticality and coherence, a general LM might be suitable as well.\n - Why does ROUGE-L seem to work better than ROUGE-1 or ROUGE-2 as the reward? Do you have any insights are speculations regarding this?", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Deep Reinforced Model for Abstractive Summarization", "abstract": "Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL). \nModels trained only with supervised learning often exhibit \"exposure bias\" - they assume ground truth is provided at each step during training.\nHowever, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable.\nWe evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries.", "pdf": "/pdf/ae4dce902a6df56754a36bcc69366eca1f6e502f.pdf", "TL;DR": "A summarization model combining a new intra-attention and reinforcement learning method to increase summary ROUGE scores and quality for long sequences.", "paperhash": "paulus|a_deep_reinforced_model_for_abstractive_summarization", "_bibtex": "@inproceedings{\npaulus2018a,\ntitle={A Deep Reinforced Model for Abstractive Summarization},\nauthor={Romain Paulus and Caiming Xiong and Richard Socher},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HkAClQgA-},\n}", "keywords": ["deep learning", "natural language processing", "reinforcement learning", "text summarization", "sequence generation"], "authors": ["Romain Paulus", "Caiming Xiong", "Richard Socher"], "authorids": ["rpaulus@salesforce.com", "cxiong@salesforce.com", "richard@socher.org"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642413162, "id": "ICLR.cc/2018/Conference/-/Paper228/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper228/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper228/AnonReviewer1", "ICLR.cc/2018/Conference/Paper228/AnonReviewer2", "ICLR.cc/2018/Conference/Paper228/AnonReviewer3"], "reply": {"forum": "HkAClQgA-", "replyto": "HkAClQgA-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper228/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642413162}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642413186, "tcdate": 1511833547528, "number": 3, "cdate": 1511833547528, "id": "BkQAkH5lM", "invitation": "ICLR.cc/2018/Conference/-/Paper228/Official_Review", "forum": "HkAClQgA-", "replyto": "HkAClQgA-", "signatures": ["ICLR.cc/2018/Conference/Paper228/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "Good incremental research but not exciting", "rating": "6: Marginally above acceptance threshold", "review": "The paper is generally well-written and the intuition is very clear. It combines the advanced attention mechanism, pointer networks and REINFORCE learning signal to train a sequence-to-sequence model for text summarization. The experimental results show that the model is able to achieve the state-of-the-art performance on CNN/Daily Main and New York Times datasets. It is a good incremental research, but the downside of this paper is lack of innovations since most of the methods proposed in this paper are not new to us.\n\nI would like to see the model ablation w.r.t. repetition avoidance trick by muting the second trigram at test time. Intuitively, if the repetition issue is prominent to having decent summarization performance, it might affect our judgement on the significance of using intra-attention or combined RL signal.\nAnother thought on this: is it possible to integrate the trigram occurrence with summarization reward? so that the recurrent neural networks with attention could capture the learning signal to avoid the repetition issue and the heuristic function in the test time can be removed. \n\nIn addition, as the encoder-decoder structure gradually becomes the standard choice of sequence prediction, I would suggest the authors to add the sum of parameters into model ablation for reference.\n\nSuggested References:\nBahdanau et al. (2016) An Actor-critic Algorithm for Sequence Prediction. (actor-critic on machine translation)\nMiao & Blunsom (2016) Language as a Latent Variable: Discrete Generative Models for Sentence Compression. (mixture pointer mechanism + REINFORCE)\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Deep Reinforced Model for Abstractive Summarization", "abstract": "Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL). \nModels trained only with supervised learning often exhibit \"exposure bias\" - they assume ground truth is provided at each step during training.\nHowever, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable.\nWe evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries.", "pdf": "/pdf/ae4dce902a6df56754a36bcc69366eca1f6e502f.pdf", "TL;DR": "A summarization model combining a new intra-attention and reinforcement learning method to increase summary ROUGE scores and quality for long sequences.", "paperhash": "paulus|a_deep_reinforced_model_for_abstractive_summarization", "_bibtex": "@inproceedings{\npaulus2018a,\ntitle={A Deep Reinforced Model for Abstractive Summarization},\nauthor={Romain Paulus and Caiming Xiong and Richard Socher},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HkAClQgA-},\n}", "keywords": ["deep learning", "natural language processing", "reinforcement learning", "text summarization", "sequence generation"], "authors": ["Romain Paulus", "Caiming Xiong", "Richard Socher"], "authorids": ["rpaulus@salesforce.com", "cxiong@salesforce.com", "richard@socher.org"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642413162, "id": "ICLR.cc/2018/Conference/-/Paper228/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper228/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper228/AnonReviewer1", "ICLR.cc/2018/Conference/Paper228/AnonReviewer2", "ICLR.cc/2018/Conference/Paper228/AnonReviewer3"], "reply": {"forum": "HkAClQgA-", "replyto": "HkAClQgA-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper228/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642413162}}}, {"tddate": null, "ddate": null, "tmdate": 1515104099650, "tcdate": 1515104099650, "number": 5, "cdate": 1515104099650, "id": "S1nwDXnQM", "invitation": "ICLR.cc/2018/Conference/-/Paper228/Official_Comment", "forum": "HkAClQgA-", "replyto": "HkAClQgA-", "signatures": ["ICLR.cc/2018/Conference/Paper228/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper228/Authors"], "content": {"title": "Revisions and updated PDF", "comment": "Following the helpful comments and feedback from all reviewers, we updated our paper submission with the following changes:\n- Add the number of parameters of our model\n- Add model ablation results with respect to the trigram avoidance trick on the CNN/Daily Mail and New York Times datasets\n- Add perplexity scores and compare them with human evaluation results\n- Add Bahdanau et al. (2016) and Miao & Blunsom (2016) citations\n- Other minor fixes in citations"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Deep Reinforced Model for Abstractive Summarization", "abstract": "Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL). \nModels trained only with supervised learning often exhibit \"exposure bias\" - they assume ground truth is provided at each step during training.\nHowever, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable.\nWe evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries.", "pdf": "/pdf/ae4dce902a6df56754a36bcc69366eca1f6e502f.pdf", "TL;DR": "A summarization model combining a new intra-attention and reinforcement learning method to increase summary ROUGE scores and quality for long sequences.", "paperhash": "paulus|a_deep_reinforced_model_for_abstractive_summarization", "_bibtex": "@inproceedings{\npaulus2018a,\ntitle={A Deep Reinforced Model for Abstractive Summarization},\nauthor={Romain Paulus and Caiming Xiong and Richard Socher},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HkAClQgA-},\n}", "keywords": ["deep learning", "natural language processing", "reinforcement learning", "text summarization", "sequence generation"], "authors": ["Romain Paulus", "Caiming Xiong", "Richard Socher"], "authorids": ["rpaulus@salesforce.com", "cxiong@salesforce.com", "richard@socher.org"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825737205, "id": "ICLR.cc/2018/Conference/-/Paper228/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HkAClQgA-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper228/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper228/Authors|ICLR.cc/2018/Conference/Paper228/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper228/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper228/Authors|ICLR.cc/2018/Conference/Paper228/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper228/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper228/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper228/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper228/Reviewers", "ICLR.cc/2018/Conference/Paper228/Authors", "ICLR.cc/2018/Conference/Paper228/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825737205}}}, {"tddate": null, "ddate": null, "tmdate": 1514417330424, "tcdate": 1514417330424, "number": 1, "cdate": 1514417330424, "id": "B1c32sZmM", "invitation": "ICLR.cc/2018/Conference/-/Paper228/Official_Comment", "forum": "HkAClQgA-", "replyto": "HkAClQgA-", "signatures": ["ICLR.cc/2018/Conference/Paper228/Area_Chair"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper228/Area_Chair"], "content": {"title": "Discussion", "comment": "Authors,\n\nPlease respond to the reviewers if you have any rebuttal points. While scores are positive, it is helpful to have these points resolved. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Deep Reinforced Model for Abstractive Summarization", "abstract": "Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL). \nModels trained only with supervised learning often exhibit \"exposure bias\" - they assume ground truth is provided at each step during training.\nHowever, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable.\nWe evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries.", "pdf": "/pdf/ae4dce902a6df56754a36bcc69366eca1f6e502f.pdf", "TL;DR": "A summarization model combining a new intra-attention and reinforcement learning method to increase summary ROUGE scores and quality for long sequences.", "paperhash": "paulus|a_deep_reinforced_model_for_abstractive_summarization", "_bibtex": "@inproceedings{\npaulus2018a,\ntitle={A Deep Reinforced Model for Abstractive Summarization},\nauthor={Romain Paulus and Caiming Xiong and Richard Socher},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HkAClQgA-},\n}", "keywords": ["deep learning", "natural language processing", "reinforcement learning", "text summarization", "sequence generation"], "authors": ["Romain Paulus", "Caiming Xiong", "Richard Socher"], "authorids": ["rpaulus@salesforce.com", "cxiong@salesforce.com", "richard@socher.org"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825737205, "id": "ICLR.cc/2018/Conference/-/Paper228/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HkAClQgA-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper228/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper228/Authors|ICLR.cc/2018/Conference/Paper228/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper228/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper228/Authors|ICLR.cc/2018/Conference/Paper228/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper228/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper228/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper228/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper228/Reviewers", "ICLR.cc/2018/Conference/Paper228/Authors", "ICLR.cc/2018/Conference/Paper228/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825737205}}}, {"tddate": null, "ddate": null, "tmdate": 1512651505607, "tcdate": 1512651505607, "number": 1, "cdate": 1512651505607, "id": "SyqgjnIWM", "invitation": "ICLR.cc/2018/Conference/-/Paper228/Public_Comment", "forum": "HkAClQgA-", "replyto": "HkAClQgA-", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Request for code", "comment": "Hi! I think your paper is very very instructive. Can you share the code with me? Email Address:zhu1qingqing@gmail.com                  Thank you!\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Deep Reinforced Model for Abstractive Summarization", "abstract": "Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL). \nModels trained only with supervised learning often exhibit \"exposure bias\" - they assume ground truth is provided at each step during training.\nHowever, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable.\nWe evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries.", "pdf": "/pdf/ae4dce902a6df56754a36bcc69366eca1f6e502f.pdf", "TL;DR": "A summarization model combining a new intra-attention and reinforcement learning method to increase summary ROUGE scores and quality for long sequences.", "paperhash": "paulus|a_deep_reinforced_model_for_abstractive_summarization", "_bibtex": "@inproceedings{\npaulus2018a,\ntitle={A Deep Reinforced Model for Abstractive Summarization},\nauthor={Romain Paulus and Caiming Xiong and Richard Socher},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HkAClQgA-},\n}", "keywords": ["deep learning", "natural language processing", "reinforcement learning", "text summarization", "sequence generation"], "authors": ["Romain Paulus", "Caiming Xiong", "Richard Socher"], "authorids": ["rpaulus@salesforce.com", "cxiong@salesforce.com", "richard@socher.org"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791690551, "id": "ICLR.cc/2018/Conference/-/Paper228/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "HkAClQgA-", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper228/Authors", "ICLR.cc/2018/Conference/Paper228/Reviewers", "ICLR.cc/2018/Conference/Paper228/Area_Chair"], "cdate": 1512791690551}}}], "count": 8}