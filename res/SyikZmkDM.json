{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124478739, "tcdate": 1518444771350, "number": 121, "cdate": 1518444771350, "id": "SyikZmkDM", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "SyikZmkDM", "signatures": ["~Thomas_Wolf1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Meta-Learning a Dynamical Language Model", "abstract": "We consider the task of word-level language modeling and study the possibility of combining hidden-states-based short-term representations with medium-term representations encoded in dynamical weights of a language model. Our work extends recent experiments on language models with dynamically evolving weights by casting the language modeling problem into an online learning-to-learn framework in which a meta-learner is trained by gradient-descent to continuously update a language model weights.", "paperhash": "wolf|metalearning_a_dynamical_language_model", "keywords": ["language models", "hierarchical representations", "meta-learning", "nonstationarity", "catastrophic forgetting", "recurrent neural network"], "_bibtex": "@misc{\n  wolf2018meta-learning,\n  title={Meta-Learning a Dynamical Language Model},\n  author={Thomas Wolf and Julien Chaumond and Clement Delangue},\n  year={2018},\n  url={https://openreview.net/forum?id=SyikZmkDM}\n}", "authorids": ["thomas@huggingface.co", "julien@huggingface.co", "clement@huggingface.co"], "authors": ["Thomas Wolf", "Julien Chaumond", "Clement Delangue"], "TL;DR": "Language modeling with dynamical weights as an instance of continuous meta-learning", "pdf": "/pdf/b54bbd277a19c0b3965bace613d7f3a24ab38519.pdf"}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582820570, "tcdate": 1520611950691, "number": 1, "cdate": 1520611950691, "id": "H1PuMNgYG", "invitation": "ICLR.cc/2018/Workshop/-/Paper121/Official_Review", "forum": "SyikZmkDM", "replyto": "SyikZmkDM", "signatures": ["ICLR.cc/2018/Workshop/Paper121/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper121/AnonReviewer3"], "content": {"title": "Meta-learning but for what?", "rating": "6: Marginally above acceptance threshold", "review": "The paper explores how to incorporate test time information into a trained language model.\n\nPrevious works include the Neural Cache and Dynamic Evaluation. The latter keeps taking small gradient descent steps while working through the test set. The authors recast Dynamic Evaluation as Meta-Learning where the weights of a pretrained RNN are updated by a meta model which is another RNN. The meta model has access to the loss and to the gradients of the loss with respect to the weights, plus the pretrained weights.\n\nHaving the same information available as the cited Krause et al. 2017 paper, they proceed to get the same results too, but in a meta learning formulation.\n\nPros:\n- The formulation of the meta learning model is interesting.\n- The view of the overall model as having memories of three different time scales is intriguing.\n- Gorgeous and informative visualization of the contribution of the meta model.\n\nCons:\n- It's not very novel: it's a reformulation with meta learning (which was proposed before).\n- The meta learning doesn't seem to contribute anything.\n\nMaybe this could be better presented as a negative result?\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning a Dynamical Language Model", "abstract": "We consider the task of word-level language modeling and study the possibility of combining hidden-states-based short-term representations with medium-term representations encoded in dynamical weights of a language model. Our work extends recent experiments on language models with dynamically evolving weights by casting the language modeling problem into an online learning-to-learn framework in which a meta-learner is trained by gradient-descent to continuously update a language model weights.", "paperhash": "wolf|metalearning_a_dynamical_language_model", "keywords": ["language models", "hierarchical representations", "meta-learning", "nonstationarity", "catastrophic forgetting", "recurrent neural network"], "_bibtex": "@misc{\n  wolf2018meta-learning,\n  title={Meta-Learning a Dynamical Language Model},\n  author={Thomas Wolf and Julien Chaumond and Clement Delangue},\n  year={2018},\n  url={https://openreview.net/forum?id=SyikZmkDM}\n}", "authorids": ["thomas@huggingface.co", "julien@huggingface.co", "clement@huggingface.co"], "authors": ["Thomas Wolf", "Julien Chaumond", "Clement Delangue"], "TL;DR": "Language modeling with dynamical weights as an instance of continuous meta-learning", "pdf": "/pdf/b54bbd277a19c0b3965bace613d7f3a24ab38519.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582820387, "id": "ICLR.cc/2018/Workshop/-/Paper121/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper121/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper121/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper121/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper121/AnonReviewer1"], "reply": {"forum": "SyikZmkDM", "replyto": "SyikZmkDM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper121/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper121/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582820387}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582676121, "tcdate": 1520735157635, "number": 2, "cdate": 1520735157635, "id": "HyRhmMGKM", "invitation": "ICLR.cc/2018/Workshop/-/Paper121/Official_Review", "forum": "SyikZmkDM", "replyto": "SyikZmkDM", "signatures": ["ICLR.cc/2018/Workshop/Paper121/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper121/AnonReviewer2"], "content": {"title": "REVIEW", "rating": "5: Marginally below acceptance threshold", "review": "== summary ==\nThe authors describe a new technique to perform dynamic evaluation of recurrent neural network language models. The proposed method is inspired by \"fast weights\", more specifically, using an additional network to update the weights of the models to adapt to change in the data. Here, the authors propose to use the method introduced by Ravi and Larochelle: each new weight is a linear combination of the previous weight, the gradient, and the weight of the static model. The weights of this linear combination are computed by the additional network. This method can be seen as an extension of stochastic gradient descent, where the learning rate for each weight is computed by a network. The authors evaluate the method on the WikiText-2 dataset.\n\n== assessment ==\nThis paper is clearly written and easy to follow (and explain connection between dynamic evaluation and meta learning). However, I believe that the contributions are a bit incremental. The main idea of the paper is to combine dynamic evaluation of RNNLM (where SGD is used to update the model) and the meta learning approach of Ravi and Larochelle. The experimental results are a bit disappointing, since the proposed method obtains similar results as SGD (SGD: 44.3, here 46.9), with SGD being a special case of the model.\n\n== pros and cons ==\n+ well written paper, connection between dynamic eval and meta learning.\n- a bit incremental\n- weak experimental results", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning a Dynamical Language Model", "abstract": "We consider the task of word-level language modeling and study the possibility of combining hidden-states-based short-term representations with medium-term representations encoded in dynamical weights of a language model. Our work extends recent experiments on language models with dynamically evolving weights by casting the language modeling problem into an online learning-to-learn framework in which a meta-learner is trained by gradient-descent to continuously update a language model weights.", "paperhash": "wolf|metalearning_a_dynamical_language_model", "keywords": ["language models", "hierarchical representations", "meta-learning", "nonstationarity", "catastrophic forgetting", "recurrent neural network"], "_bibtex": "@misc{\n  wolf2018meta-learning,\n  title={Meta-Learning a Dynamical Language Model},\n  author={Thomas Wolf and Julien Chaumond and Clement Delangue},\n  year={2018},\n  url={https://openreview.net/forum?id=SyikZmkDM}\n}", "authorids": ["thomas@huggingface.co", "julien@huggingface.co", "clement@huggingface.co"], "authors": ["Thomas Wolf", "Julien Chaumond", "Clement Delangue"], "TL;DR": "Language modeling with dynamical weights as an instance of continuous meta-learning", "pdf": "/pdf/b54bbd277a19c0b3965bace613d7f3a24ab38519.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582820387, "id": "ICLR.cc/2018/Workshop/-/Paper121/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper121/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper121/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper121/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper121/AnonReviewer1"], "reply": {"forum": "SyikZmkDM", "replyto": "SyikZmkDM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper121/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper121/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582820387}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582606993, "tcdate": 1520902388177, "number": 3, "cdate": 1520902388177, "id": "ry3lbiNtz", "invitation": "ICLR.cc/2018/Workshop/-/Paper121/Official_Review", "forum": "SyikZmkDM", "replyto": "SyikZmkDM", "signatures": ["ICLR.cc/2018/Workshop/Paper121/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper121/AnonReviewer1"], "content": {"title": "Meta learning for dynamic language models", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper uses meta learning to learn medium time-scale representations in dynamic language models, which is potentially useful for language modeling on long texts where contextual information transmitted over long distances can be beneficial. \n\nOn language modeling, they perform at par with previous work in dynamic language models, and their qualitative analysis suggest that their dynamic language model is able to learn medium-term representations. \n\nThis is an interesting paper and a promising line of work. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning a Dynamical Language Model", "abstract": "We consider the task of word-level language modeling and study the possibility of combining hidden-states-based short-term representations with medium-term representations encoded in dynamical weights of a language model. Our work extends recent experiments on language models with dynamically evolving weights by casting the language modeling problem into an online learning-to-learn framework in which a meta-learner is trained by gradient-descent to continuously update a language model weights.", "paperhash": "wolf|metalearning_a_dynamical_language_model", "keywords": ["language models", "hierarchical representations", "meta-learning", "nonstationarity", "catastrophic forgetting", "recurrent neural network"], "_bibtex": "@misc{\n  wolf2018meta-learning,\n  title={Meta-Learning a Dynamical Language Model},\n  author={Thomas Wolf and Julien Chaumond and Clement Delangue},\n  year={2018},\n  url={https://openreview.net/forum?id=SyikZmkDM}\n}", "authorids": ["thomas@huggingface.co", "julien@huggingface.co", "clement@huggingface.co"], "authors": ["Thomas Wolf", "Julien Chaumond", "Clement Delangue"], "TL;DR": "Language modeling with dynamical weights as an instance of continuous meta-learning", "pdf": "/pdf/b54bbd277a19c0b3965bace613d7f3a24ab38519.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582820387, "id": "ICLR.cc/2018/Workshop/-/Paper121/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper121/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper121/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper121/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper121/AnonReviewer1"], "reply": {"forum": "SyikZmkDM", "replyto": "SyikZmkDM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper121/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper121/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582820387}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573564440, "tcdate": 1521573564440, "number": 94, "cdate": 1521573564106, "id": "SJN6AC0tf", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "SyikZmkDM", "replyto": "SyikZmkDM", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning a Dynamical Language Model", "abstract": "We consider the task of word-level language modeling and study the possibility of combining hidden-states-based short-term representations with medium-term representations encoded in dynamical weights of a language model. Our work extends recent experiments on language models with dynamically evolving weights by casting the language modeling problem into an online learning-to-learn framework in which a meta-learner is trained by gradient-descent to continuously update a language model weights.", "paperhash": "wolf|metalearning_a_dynamical_language_model", "keywords": ["language models", "hierarchical representations", "meta-learning", "nonstationarity", "catastrophic forgetting", "recurrent neural network"], "_bibtex": "@misc{\n  wolf2018meta-learning,\n  title={Meta-Learning a Dynamical Language Model},\n  author={Thomas Wolf and Julien Chaumond and Clement Delangue},\n  year={2018},\n  url={https://openreview.net/forum?id=SyikZmkDM}\n}", "authorids": ["thomas@huggingface.co", "julien@huggingface.co", "clement@huggingface.co"], "authors": ["Thomas Wolf", "Julien Chaumond", "Clement Delangue"], "TL;DR": "Language modeling with dynamical weights as an instance of continuous meta-learning", "pdf": "/pdf/b54bbd277a19c0b3965bace613d7f3a24ab38519.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 5}