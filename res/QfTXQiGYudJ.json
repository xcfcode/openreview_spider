{"notes": [{"id": "QfTXQiGYudJ", "original": "LJXk5h2t-9w", "number": 44, "cdate": 1601308013927, "ddate": null, "tcdate": 1601308013927, "tmdate": 1615258136695, "tddate": null, "forum": "QfTXQiGYudJ", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Stabilized Medical Image Attacks", "authorids": ["~Gege_Qi1", "~Lijun_GONG2", "~Yibing_Song1", "~Kai_Ma2", "~Yefeng_Zheng2"], "authors": ["Gege Qi", "Lijun GONG", "Yibing Song", "Kai Ma", "Yefeng Zheng"], "keywords": ["Healthcare", "Biometrics"], "abstract": "Convolutional Neural Networks (CNNs) have advanced existing medical systems for automatic disease diagnosis. However, a threat to these systems arises that adversarial attacks make CNNs vulnerable. Inaccurate diagnosis results make a negative influence on human healthcare. There is a need to investigate potential adversarial attacks to robustify deep medical diagnosis systems. On the other side, there are several modalities of medical images (e.g., CT, fundus, and endoscopic image) of which each type is significantly different from others. It is more challenging to generate adversarial perturbations for different types of medical images. In this paper, we propose an image-based medical adversarial attack method to consistently produce adversarial perturbations on medical images. The objective function of our method consists of a loss deviation term and a loss stabilization term. The loss deviation term increases the divergence between the CNN prediction of an adversarial example and its ground truth label. Meanwhile, the loss stabilization term ensures similar CNN predictions of this example and its smoothed input. From the perspective of the whole iterations for perturbation generation, the proposed loss stabilization term exhaustively searches the perturbation space to smooth the single spot for local optimum escape. We further analyze the KL-divergence of the proposed loss function and find that the loss stabilization term makes the perturbations updated towards a fixed objective spot while deviating from the ground truth. This stabilization ensures the proposed medical attack effective for different types of medical images while producing perturbations in small variance. Experiments on several medical image analysis benchmarks including the recent COVID-19 dataset show the stability of the proposed method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qi|stabilized_medical_image_attacks", "one-sentence_summary": "We propose a stabilized adversarial attack method for medical image analysis.", "supplementary_material": "", "pdf": "/pdf/063b8cff90b57f7bc22aacce8251a0b2f7e8e110.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nqi2021stabilized,\ntitle={Stabilized Medical Image Attacks},\nauthor={Gege Qi and Lijun GONG and Yibing Song and Kai Ma and Yefeng Zheng},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=QfTXQiGYudJ}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "tcwc5eY54I2", "original": null, "number": 1, "cdate": 1610040359237, "ddate": null, "tcdate": 1610040359237, "tmdate": 1610473949264, "tddate": null, "forum": "QfTXQiGYudJ", "replyto": "QfTXQiGYudJ", "invitation": "ICLR.cc/2021/Conference/Paper44/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Spotlight)", "comment": " The paper proposes to use a regularization term for stabilizing the perturbation trajectories in generating adversarial examples for medical image tasks. The authors tested the effectiveness of their proposal on different medical image datasets obtained by different modalities, and the experimental results are generally encouraging.\nAll the reviewers see the value of the paper and give positive comments. At the same time, they also point out some aspects for further improvement, including\n1)\tThe datasets used are relatively small\n2)\tThe title is a little misleading since the paper only tackles the image attacks (but the title is stabilized medical attacks).\n3)\tCase studies and visualization are needed to help people better understand the paper\n\nThe authors have done a good job in their rebuttal and paper revision, by adding experiments on larger datasets, changing the title to \u201cstabilized medical image attacks\u201d, and adding some geometric figures for better illustration. These have largely addressed the concerns of the reviewers, and we see no problem with accepting the paper.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stabilized Medical Image Attacks", "authorids": ["~Gege_Qi1", "~Lijun_GONG2", "~Yibing_Song1", "~Kai_Ma2", "~Yefeng_Zheng2"], "authors": ["Gege Qi", "Lijun GONG", "Yibing Song", "Kai Ma", "Yefeng Zheng"], "keywords": ["Healthcare", "Biometrics"], "abstract": "Convolutional Neural Networks (CNNs) have advanced existing medical systems for automatic disease diagnosis. However, a threat to these systems arises that adversarial attacks make CNNs vulnerable. Inaccurate diagnosis results make a negative influence on human healthcare. There is a need to investigate potential adversarial attacks to robustify deep medical diagnosis systems. On the other side, there are several modalities of medical images (e.g., CT, fundus, and endoscopic image) of which each type is significantly different from others. It is more challenging to generate adversarial perturbations for different types of medical images. In this paper, we propose an image-based medical adversarial attack method to consistently produce adversarial perturbations on medical images. The objective function of our method consists of a loss deviation term and a loss stabilization term. The loss deviation term increases the divergence between the CNN prediction of an adversarial example and its ground truth label. Meanwhile, the loss stabilization term ensures similar CNN predictions of this example and its smoothed input. From the perspective of the whole iterations for perturbation generation, the proposed loss stabilization term exhaustively searches the perturbation space to smooth the single spot for local optimum escape. We further analyze the KL-divergence of the proposed loss function and find that the loss stabilization term makes the perturbations updated towards a fixed objective spot while deviating from the ground truth. This stabilization ensures the proposed medical attack effective for different types of medical images while producing perturbations in small variance. Experiments on several medical image analysis benchmarks including the recent COVID-19 dataset show the stability of the proposed method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qi|stabilized_medical_image_attacks", "one-sentence_summary": "We propose a stabilized adversarial attack method for medical image analysis.", "supplementary_material": "", "pdf": "/pdf/063b8cff90b57f7bc22aacce8251a0b2f7e8e110.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nqi2021stabilized,\ntitle={Stabilized Medical Image Attacks},\nauthor={Gege Qi and Lijun GONG and Yibing Song and Kai Ma and Yefeng Zheng},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=QfTXQiGYudJ}\n}"}, "tags": [], "invitation": {"reply": {"forum": "QfTXQiGYudJ", "replyto": "QfTXQiGYudJ", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040359222, "tmdate": 1610473949245, "id": "ICLR.cc/2021/Conference/Paper44/-/Decision"}}}, {"id": "ZCZ3TQekh3y", "original": null, "number": 3, "cdate": 1603991480198, "ddate": null, "tcdate": 1603991480198, "tmdate": 1606518193382, "tddate": null, "forum": "QfTXQiGYudJ", "replyto": "QfTXQiGYudJ", "invitation": "ICLR.cc/2021/Conference/Paper44/-/Official_Review", "content": {"title": "proposes to use a regularization term for stabilizing the perturbation trajectories in generating adversarial examples for medical image tasks", "review": "The paper proposes to use a regularization term for stabilizing the perturbation trajectories in generating adversarial examples for medical image tasks. More specifically, they introduce a loss stabilization term which forces perturbed inputs to be close to smoothed perturbed inputs in the CNN output space. The authors give a shallow analysis which demonstrates that this regularizer forces convergence of softmax output to a uniform distribution (i.e. the maximum entropy distribution). In addition, their theoretical analysis yields a practical implementation for their loss term. The authors provide one visualization to demonstrate the differences in variance of the perturbation and variance in the directions of perturbation. Finally, they demonstrate the effectiveness of their method on three medical image datasets for separate computer vision tasks by comparing with the state of the art methods for adversarial attacks. I like the idea of using regularization in the generation of adversarial examples. In particular, the theoretical motivation to control the perturbation variance and output distribution is thought provoking. I believe more generalization and deeper analysis would be beneficial to future deep learning research, and for this primary reason do I vote to accept. My primary concern for the paper is the clarity of the presentation, which I explain in the cons section.\nPros:\u00a0\n\n1. The paper presents a theoretically founded, and (somewhat) experimentally validated application of regularization for generating adversarial examples for multi-modal tasks and data for medical images.\n\u00a0\n2. The proposed method is simple and elegant. It is theoretically well-founded and easily implemented. \n\u00a0\n3. The paper provides good initial results, showing that to some degree their method is generalizable. The ablation study provides a good basis for demonstrating the effectiveness of the proposed regularization. \nCons:\u00a0\n\u00a0\n1. The datasets used are rather small and not well known within medical imaging community. I suggest expanding this study to include larger datasets (10,000+ images) and include other well-founded datasets for medical tasks (see literature cited by the paper). This would demonstrate greater generalization of the proposed method, especially in the case that this method generalizes to large, varied datasets.\n\u00a0\n2. Because the real danger of adversarial attacks in medical images concerns false positives, the generation of adversarial examples that yield false positives would be helpful. Experiments regarding this would be even better.\n\u00a0\n3. Could include visuals for multiple datasets and multiple tasks. \n\n4. Should include information on how the hyper-parameters were determined and the results of such validation. Particularly, epsilon, alpha, and stopping iteration. In addition, why was the second iteration chosen to introduce the regularization? \n\n5. On page 4, the authors claim \u201cthe limitations that are brought by huge network and data variance are effectively solved via our stabilized medical attack\u201d. Why is this the case? While the method does reduce the variance in the perturbation and the direction of the perturbations, can you explain in the paper why this is inherent to deep networks and a variety of data modalities? The introduction could also touch on previous work demonstrating the difficulties adversarial examples have for differing data modalities (i.e. why, as is claimed, do previous methods not generalize away from natural images?)\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper44/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper44/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stabilized Medical Image Attacks", "authorids": ["~Gege_Qi1", "~Lijun_GONG2", "~Yibing_Song1", "~Kai_Ma2", "~Yefeng_Zheng2"], "authors": ["Gege Qi", "Lijun GONG", "Yibing Song", "Kai Ma", "Yefeng Zheng"], "keywords": ["Healthcare", "Biometrics"], "abstract": "Convolutional Neural Networks (CNNs) have advanced existing medical systems for automatic disease diagnosis. However, a threat to these systems arises that adversarial attacks make CNNs vulnerable. Inaccurate diagnosis results make a negative influence on human healthcare. There is a need to investigate potential adversarial attacks to robustify deep medical diagnosis systems. On the other side, there are several modalities of medical images (e.g., CT, fundus, and endoscopic image) of which each type is significantly different from others. It is more challenging to generate adversarial perturbations for different types of medical images. In this paper, we propose an image-based medical adversarial attack method to consistently produce adversarial perturbations on medical images. The objective function of our method consists of a loss deviation term and a loss stabilization term. The loss deviation term increases the divergence between the CNN prediction of an adversarial example and its ground truth label. Meanwhile, the loss stabilization term ensures similar CNN predictions of this example and its smoothed input. From the perspective of the whole iterations for perturbation generation, the proposed loss stabilization term exhaustively searches the perturbation space to smooth the single spot for local optimum escape. We further analyze the KL-divergence of the proposed loss function and find that the loss stabilization term makes the perturbations updated towards a fixed objective spot while deviating from the ground truth. This stabilization ensures the proposed medical attack effective for different types of medical images while producing perturbations in small variance. Experiments on several medical image analysis benchmarks including the recent COVID-19 dataset show the stability of the proposed method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qi|stabilized_medical_image_attacks", "one-sentence_summary": "We propose a stabilized adversarial attack method for medical image analysis.", "supplementary_material": "", "pdf": "/pdf/063b8cff90b57f7bc22aacce8251a0b2f7e8e110.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nqi2021stabilized,\ntitle={Stabilized Medical Image Attacks},\nauthor={Gege Qi and Lijun GONG and Yibing Song and Kai Ma and Yefeng Zheng},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=QfTXQiGYudJ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "QfTXQiGYudJ", "replyto": "QfTXQiGYudJ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper44/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538151220, "tmdate": 1606915760989, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper44/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper44/-/Official_Review"}}}, {"id": "du1EufUuAIp", "original": null, "number": 1, "cdate": 1603802240004, "ddate": null, "tcdate": 1603802240004, "tmdate": 1605024772582, "tddate": null, "forum": "QfTXQiGYudJ", "replyto": "QfTXQiGYudJ", "invitation": "ICLR.cc/2021/Conference/Paper44/-/Official_Review", "content": {"title": "A stabilized adversarial attack method for various types of medical data.", "review": "The authors present a universal medical attack method that can consistently produce adversarial examples across several medical imaging domains. The authors achieve this by developing a novel objective function that includes two terms, which they refer to as stabilized medical attack (SMA). The first term is the loss deviation term, inspired by the conventional fast gradient sign method, which enlarges the difference between CNN predictions and ground truth labels. The second term, a regularizer, is the loss stabilization term that enforces consistent predictions between the adversarial image and the Gaussian smoothed version of the adversarial image. The authors then provide an insightful interpretation of their SMA loss via KL divergence. The derivation demonstrates that perturbations consistently move towards a fixed location in the SAM objective landscape during successive iterations of gradient ascent. This method increases perturbation robustness by overcoming huge variations that result from different types of medical imaging data. The authors provide an illustrative figure (Fig.2) to demonstrate that both the variance and direction of the adversarial perturbation remain stable and consistent across multiple iterations, compared to using the deviation loss alone. The authors then perform an ablation study to demonstrate the DEV + STA loss results in a significantly greater reduction in model performance across medical imaging datasets compared to DEV loss alone. Finally, compared to the state-of-the-art adversarial methods, SMA results in the greatest reduction in performance for all datasets.\n\nPros:\nThis is an excellent paper. Succinct and clear, addressing a known problem with any CAD system. They provide good justification of the technique. They provide sufficient mathematical detail to follow the KL divergence derivation. Both empirical studies are sound and demonstrate the expected results. Additional examples are provided in the supplementary materials. The methods are correct. The empirical methodology is correct. The paper is generally clear.\n\nCons:\nIt would be helpful for the reader/audience to have an associated figure that graphically demonstrated the point made in 3.2. There is a compelling geometric intuition behind the idea expressed in the text above Sec 3.3 that would help present the contribution of adding the STA term.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper44/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper44/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stabilized Medical Image Attacks", "authorids": ["~Gege_Qi1", "~Lijun_GONG2", "~Yibing_Song1", "~Kai_Ma2", "~Yefeng_Zheng2"], "authors": ["Gege Qi", "Lijun GONG", "Yibing Song", "Kai Ma", "Yefeng Zheng"], "keywords": ["Healthcare", "Biometrics"], "abstract": "Convolutional Neural Networks (CNNs) have advanced existing medical systems for automatic disease diagnosis. However, a threat to these systems arises that adversarial attacks make CNNs vulnerable. Inaccurate diagnosis results make a negative influence on human healthcare. There is a need to investigate potential adversarial attacks to robustify deep medical diagnosis systems. On the other side, there are several modalities of medical images (e.g., CT, fundus, and endoscopic image) of which each type is significantly different from others. It is more challenging to generate adversarial perturbations for different types of medical images. In this paper, we propose an image-based medical adversarial attack method to consistently produce adversarial perturbations on medical images. The objective function of our method consists of a loss deviation term and a loss stabilization term. The loss deviation term increases the divergence between the CNN prediction of an adversarial example and its ground truth label. Meanwhile, the loss stabilization term ensures similar CNN predictions of this example and its smoothed input. From the perspective of the whole iterations for perturbation generation, the proposed loss stabilization term exhaustively searches the perturbation space to smooth the single spot for local optimum escape. We further analyze the KL-divergence of the proposed loss function and find that the loss stabilization term makes the perturbations updated towards a fixed objective spot while deviating from the ground truth. This stabilization ensures the proposed medical attack effective for different types of medical images while producing perturbations in small variance. Experiments on several medical image analysis benchmarks including the recent COVID-19 dataset show the stability of the proposed method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qi|stabilized_medical_image_attacks", "one-sentence_summary": "We propose a stabilized adversarial attack method for medical image analysis.", "supplementary_material": "", "pdf": "/pdf/063b8cff90b57f7bc22aacce8251a0b2f7e8e110.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nqi2021stabilized,\ntitle={Stabilized Medical Image Attacks},\nauthor={Gege Qi and Lijun GONG and Yibing Song and Kai Ma and Yefeng Zheng},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=QfTXQiGYudJ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "QfTXQiGYudJ", "replyto": "QfTXQiGYudJ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper44/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538151220, "tmdate": 1606915760989, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper44/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper44/-/Official_Review"}}}, {"id": "38PL-rUgfC2", "original": null, "number": 2, "cdate": 1603845551641, "ddate": null, "tcdate": 1603845551641, "tmdate": 1605024772447, "tddate": null, "forum": "QfTXQiGYudJ", "replyto": "QfTXQiGYudJ", "invitation": "ICLR.cc/2021/Conference/Paper44/-/Official_Review", "content": {"title": "Title needs to be more specific", "review": "The authors proposed to introduce a combination of a loss deviation term and a loss stabilization term to generate more consistent adversarial perturbations on medical images. The loss deviation term increases the divergence between the CNN prediction of an adversarial example and its ground truth label. At the same time, the loss stabilization term ensures similar CNN predictions of this example and its smoothed input. The authors tested against 3 different medical image datasets obtained by different modalities. The proposed strategy seems straightforward and the benefits are clearly demonstrated with these three datasets.\n\nOverall, I vote for good paper, accept, after minor revise. The proposed strategy seems straightforward and the benefits are clearly demonstrated with these three datasets. This approach may contribute to further improve CNN-based medical image diagnosis systems.\n\nMinor points:\n\nSince the authors demonstrated only with medical image dataset, the title Stabilized \u201cMedical Attacks\u201d seems misleading and inaccurate. There are many other data types in medical field such as text, wave form, numerical values, etc, thus the title should be more specific, at least \u201cMedical Image Attacks\u201d. Also the term \u201cuniversal\u201d is used several times in the main text, and it has the same issue since the authors tested only three medical image datasets while there are many more different modalities exist in the real medical field even if limit the data type to an image.\nConversely, the strategy, introducing a stabilization loss, seems not quite specific to the characteristics of medical image, rather potentially more general. Thus it is quite curious to see how this strategy can be generalized toward image datasets in other fields.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper44/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper44/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stabilized Medical Image Attacks", "authorids": ["~Gege_Qi1", "~Lijun_GONG2", "~Yibing_Song1", "~Kai_Ma2", "~Yefeng_Zheng2"], "authors": ["Gege Qi", "Lijun GONG", "Yibing Song", "Kai Ma", "Yefeng Zheng"], "keywords": ["Healthcare", "Biometrics"], "abstract": "Convolutional Neural Networks (CNNs) have advanced existing medical systems for automatic disease diagnosis. However, a threat to these systems arises that adversarial attacks make CNNs vulnerable. Inaccurate diagnosis results make a negative influence on human healthcare. There is a need to investigate potential adversarial attacks to robustify deep medical diagnosis systems. On the other side, there are several modalities of medical images (e.g., CT, fundus, and endoscopic image) of which each type is significantly different from others. It is more challenging to generate adversarial perturbations for different types of medical images. In this paper, we propose an image-based medical adversarial attack method to consistently produce adversarial perturbations on medical images. The objective function of our method consists of a loss deviation term and a loss stabilization term. The loss deviation term increases the divergence between the CNN prediction of an adversarial example and its ground truth label. Meanwhile, the loss stabilization term ensures similar CNN predictions of this example and its smoothed input. From the perspective of the whole iterations for perturbation generation, the proposed loss stabilization term exhaustively searches the perturbation space to smooth the single spot for local optimum escape. We further analyze the KL-divergence of the proposed loss function and find that the loss stabilization term makes the perturbations updated towards a fixed objective spot while deviating from the ground truth. This stabilization ensures the proposed medical attack effective for different types of medical images while producing perturbations in small variance. Experiments on several medical image analysis benchmarks including the recent COVID-19 dataset show the stability of the proposed method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qi|stabilized_medical_image_attacks", "one-sentence_summary": "We propose a stabilized adversarial attack method for medical image analysis.", "supplementary_material": "", "pdf": "/pdf/063b8cff90b57f7bc22aacce8251a0b2f7e8e110.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nqi2021stabilized,\ntitle={Stabilized Medical Image Attacks},\nauthor={Gege Qi and Lijun GONG and Yibing Song and Kai Ma and Yefeng Zheng},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=QfTXQiGYudJ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "QfTXQiGYudJ", "replyto": "QfTXQiGYudJ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper44/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538151220, "tmdate": 1606915760989, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper44/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper44/-/Official_Review"}}}], "count": 5}