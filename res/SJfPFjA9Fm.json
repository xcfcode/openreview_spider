{"notes": [{"id": "SJfPFjA9Fm", "original": "H1gizWicFX", "number": 455, "cdate": 1538087807316, "ddate": null, "tcdate": 1538087807316, "tmdate": 1551910250271, "tddate": null, "forum": "SJfPFjA9Fm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "ACCELERATING NONCONVEX LEARNING VIA REPLICA EXCHANGE LANGEVIN DIFFUSION", "abstract": "Langevin diffusion is a powerful method for nonconvex optimization, which enables the escape from local minima by injecting noise into the gradient. In particular, the temperature parameter controlling the noise level gives rise to a tradeoff between ``global exploration'' and ``local exploitation'', which correspond to high and low temperatures. To attain the advantages of both regimes, we propose to use replica exchange, which swaps between two Langevin diffusions with different temperatures. We theoretically analyze the acceleration effect of replica exchange from two perspectives: (i) the convergence in $\\chi^2$-divergence, and (ii) the large deviation principle. Such an acceleration effect allows us to faster approach the global minima. Furthermore, by discretizing the replica exchange Langevin diffusion, we obtain a discrete-time algorithm. For such an algorithm, we quantify its discretization error in theory and demonstrate its acceleration effect in practice. ", "keywords": [], "authorids": ["yichen2016@u.northwestern.edu", "jinglinc@illinois.edu", "jd2736@columbia.edu", "jianpeng@illinois.edu", "zhaoranwang@gmail.com"], "authors": ["Yi Chen", "Jinglin Chen", "Jing Dong", "Jian Peng", "Zhaoran Wang"], "pdf": "/pdf/61073f72577a50f075e4f466d2ec08b720670c92.pdf", "paperhash": "chen|accelerating_nonconvex_learning_via_replica_exchange_langevin_diffusion", "_bibtex": "@inproceedings{\nchen2018accelerating,\ntitle={{ACCELERATING} {NONCONVEX} {LEARNING} {VIA} {REPLICA} {EXCHANGE} {LANGEVIN} {DIFFUSION}},\nauthor={Yi Chen and Jinglin Chen and Jing Dong and Jian Peng and Zhaoran Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJfPFjA9Fm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "r1xHTF-ElV", "original": null, "number": 1, "cdate": 1544980924595, "ddate": null, "tcdate": 1544980924595, "tmdate": 1545354488572, "tddate": null, "forum": "SJfPFjA9Fm", "replyto": "SJfPFjA9Fm", "invitation": "ICLR.cc/2019/Conference/-/Paper455/Meta_Review", "content": {"metareview": "The main criticisms were around novelty: that the analysis is rather standard. Given that all the reviewers agreed the paper is well written, I'm inclined to think the paper will be a useful contribution to the literature. The authors also highlight the analysis of the discretization, which seems to be missed by the most critical reviewer. I would suggest to the reviewers that they use the criticisms to rework the paper's introduction, to better explain which parts of the work are novel and which parts are standard. I would also suggest that standard background be moved to the appendix so that it is there for the nonexpert, while making the body of the work more focused on the novel aspects.", "confidence": "3: The area chair is somewhat confident", "recommendation": "Accept (Poster)", "title": "Interesting contribution, even if the analysis is mostly standard"}, "signatures": ["ICLR.cc/2019/Conference/Paper455/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper455/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ACCELERATING NONCONVEX LEARNING VIA REPLICA EXCHANGE LANGEVIN DIFFUSION", "abstract": "Langevin diffusion is a powerful method for nonconvex optimization, which enables the escape from local minima by injecting noise into the gradient. In particular, the temperature parameter controlling the noise level gives rise to a tradeoff between ``global exploration'' and ``local exploitation'', which correspond to high and low temperatures. To attain the advantages of both regimes, we propose to use replica exchange, which swaps between two Langevin diffusions with different temperatures. We theoretically analyze the acceleration effect of replica exchange from two perspectives: (i) the convergence in $\\chi^2$-divergence, and (ii) the large deviation principle. Such an acceleration effect allows us to faster approach the global minima. Furthermore, by discretizing the replica exchange Langevin diffusion, we obtain a discrete-time algorithm. For such an algorithm, we quantify its discretization error in theory and demonstrate its acceleration effect in practice. ", "keywords": [], "authorids": ["yichen2016@u.northwestern.edu", "jinglinc@illinois.edu", "jd2736@columbia.edu", "jianpeng@illinois.edu", "zhaoranwang@gmail.com"], "authors": ["Yi Chen", "Jinglin Chen", "Jing Dong", "Jian Peng", "Zhaoran Wang"], "pdf": "/pdf/61073f72577a50f075e4f466d2ec08b720670c92.pdf", "paperhash": "chen|accelerating_nonconvex_learning_via_replica_exchange_langevin_diffusion", "_bibtex": "@inproceedings{\nchen2018accelerating,\ntitle={{ACCELERATING} {NONCONVEX} {LEARNING} {VIA} {REPLICA} {EXCHANGE} {LANGEVIN} {DIFFUSION}},\nauthor={Yi Chen and Jinglin Chen and Jing Dong and Jian Peng and Zhaoran Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJfPFjA9Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper455/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353212008, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJfPFjA9Fm", "replyto": "SJfPFjA9Fm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper455/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper455/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper455/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353212008}}}, {"id": "rJlr5Z-t0Q", "original": null, "number": 6, "cdate": 1543209356947, "ddate": null, "tcdate": 1543209356947, "tmdate": 1543253673093, "tddate": null, "forum": "SJfPFjA9Fm", "replyto": "Sygv6hHya7", "invitation": "ICLR.cc/2019/Conference/-/Paper455/Official_Comment", "content": {"title": "Standard analysis but nontrivial results.", "comment": "We appreciate your valuable comments. As you have said, these methods are popular in practice and achieve good performance. However, most of them are done in the setting of MCMC, and people rarely use them in nonconvex optimization. One contribution of this paper is to apply these techniques to optimization problems. \n\nOur paper also tries to understand the acceleration effect of replica exchange. We quantify it in both LDP and the convergence of chi^2 divergence. Although in Dupuis\u2019s work, he also quantifies the acceleration effect via LDP, the LDP theory we use in this paper is different from that. Specifically, his approach is based on the LDP variational theory, and ours is based on the theory of Donsker-Varadhan. As a result, our rate function has different form from his. In our paper, we also analyze the acceleration in the convergence of chi^2 divergence. It is a new perspective and not discussed by Dupuis. We emphasize that LDP and chi^2 divergence are two different approached to quantify convergence. They have different meanings. The first one characterizes the decay rate of the probability that the empirical measures deviate from the stationary measure and the second one characterizes the decay rate of the discrepancy between the transit distributions and limiting distribution.  Although the theory of LDP and convergence of chi^2 divergence for a general Markov process are well established and standard, to the best of our knowledge, our paper is the first to apply these tools in this specific problem. \n\nIn our paper, one contribution is that we demonstrate the acceleration effect of replica exchange mathematically. We first show that the LDP rate function is boosted by replica exchange. Dupuis\u2019s work includes similar results but in a different form. We also show that the derivative of chi^2 divergence is boosted. Specifically, we demonstrate that a strict positive term caused by the replica exchange is added, if the density ratio between current distribution and limiting distribution is not symmetric. We say that a function is symmetric if we swap the positions of variables, the function value does not change. In this case, the derivative of chi^2 divergence is strictly boosted, and hence, the convergence is accelerated strictly. It reflects the benefits of replica exchange. To the best of our knowledge, this phenomenon has never been observed by previous literature, including Dupuis\u2019s paper. We think it is interesting and useful. \n\nAnother contribution of our paper is the discretization algorithm. In practice, it is impossible to simulate the continuous process directly, and discretization is necessary. To the best of our knowledge, no one has discussed the discretization of replica exchange Langevin diffusion before. Our paper is the first one to analyze the discretization theoretically. In this paper, we establish the linear convergence rate for the discretization error, which is highly trivial since the process has state-dependent jumps. This result, combined with the acceleration effect, justifies the empirical success of the replica exchange Langevin diffusion in practice.  \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper455/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper455/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper455/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ACCELERATING NONCONVEX LEARNING VIA REPLICA EXCHANGE LANGEVIN DIFFUSION", "abstract": "Langevin diffusion is a powerful method for nonconvex optimization, which enables the escape from local minima by injecting noise into the gradient. In particular, the temperature parameter controlling the noise level gives rise to a tradeoff between ``global exploration'' and ``local exploitation'', which correspond to high and low temperatures. To attain the advantages of both regimes, we propose to use replica exchange, which swaps between two Langevin diffusions with different temperatures. We theoretically analyze the acceleration effect of replica exchange from two perspectives: (i) the convergence in $\\chi^2$-divergence, and (ii) the large deviation principle. Such an acceleration effect allows us to faster approach the global minima. Furthermore, by discretizing the replica exchange Langevin diffusion, we obtain a discrete-time algorithm. For such an algorithm, we quantify its discretization error in theory and demonstrate its acceleration effect in practice. ", "keywords": [], "authorids": ["yichen2016@u.northwestern.edu", "jinglinc@illinois.edu", "jd2736@columbia.edu", "jianpeng@illinois.edu", "zhaoranwang@gmail.com"], "authors": ["Yi Chen", "Jinglin Chen", "Jing Dong", "Jian Peng", "Zhaoran Wang"], "pdf": "/pdf/61073f72577a50f075e4f466d2ec08b720670c92.pdf", "paperhash": "chen|accelerating_nonconvex_learning_via_replica_exchange_langevin_diffusion", "_bibtex": "@inproceedings{\nchen2018accelerating,\ntitle={{ACCELERATING} {NONCONVEX} {LEARNING} {VIA} {REPLICA} {EXCHANGE} {LANGEVIN} {DIFFUSION}},\nauthor={Yi Chen and Jinglin Chen and Jing Dong and Jian Peng and Zhaoran Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJfPFjA9Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper455/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621623514, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJfPFjA9Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper455/Authors", "ICLR.cc/2019/Conference/Paper455/Reviewers", "ICLR.cc/2019/Conference/Paper455/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper455/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper455/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper455/Authors|ICLR.cc/2019/Conference/Paper455/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper455/Reviewers", "ICLR.cc/2019/Conference/Paper455/Authors", "ICLR.cc/2019/Conference/Paper455/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621623514}}}, {"id": "Syx9fKn7RX", "original": null, "number": 3, "cdate": 1542863121948, "ddate": null, "tcdate": 1542863121948, "tmdate": 1543215857475, "tddate": null, "forum": "SJfPFjA9Fm", "replyto": "S1eEnYWC3Q", "invitation": "ICLR.cc/2019/Conference/-/Paper455/Official_Comment", "content": {"title": "Infinity swapping is not a trivial extension. ", "comment": "We really appreciate your comments. Replica exchange Langevin diffusion is widely used in classic MCMC over the years. Our work also uses this methodology in the setting of nonconvex optimization problem, which arises in many machine learning applications such as training neural networks. There are also many interesting questions in this direction, for example, how to choose the best temperature based on the structure of specific problems. That is why we still submit it for ICLR.\nAs for the comments on math side. First, when a->infty, the exchange process should be defined in another way. Our current definition, which swapping particles with some rate, is only valid for finite a. This extension is not totally trivial and in Dupuis&et. al's work, some results are established. In our paper, we only discuss finite swap rate. This brings convenience for the discussion of discretization error. Otherwise, we need to use a different approach to analyze. Moreover, we point out that in discretization, the swapping intensity a should be smaller than the step size. This also reflects the nontrivial connection between infinity swapping and discretization. \nSecond, the kappa in (3.10) is related to the Poincare inequality and it is also a lower estimate of the spectral gap of Markov process. Kappa is can be defined as the solution of a variational problem involved Dirichlet form. However, although our result shows that swapping boosts the Dirichlet form, we still cannot obtain an analytical formula of kappa depending on a, since the variational problem makes this relation extremely complicated. Even in the field of pure math, it is still very hard to obtain an explicit formula of kappa for a general Markov process. However, for this special case, we will keep trying to solve it in the future.    "}, "signatures": ["ICLR.cc/2019/Conference/Paper455/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper455/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper455/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ACCELERATING NONCONVEX LEARNING VIA REPLICA EXCHANGE LANGEVIN DIFFUSION", "abstract": "Langevin diffusion is a powerful method for nonconvex optimization, which enables the escape from local minima by injecting noise into the gradient. In particular, the temperature parameter controlling the noise level gives rise to a tradeoff between ``global exploration'' and ``local exploitation'', which correspond to high and low temperatures. To attain the advantages of both regimes, we propose to use replica exchange, which swaps between two Langevin diffusions with different temperatures. We theoretically analyze the acceleration effect of replica exchange from two perspectives: (i) the convergence in $\\chi^2$-divergence, and (ii) the large deviation principle. Such an acceleration effect allows us to faster approach the global minima. Furthermore, by discretizing the replica exchange Langevin diffusion, we obtain a discrete-time algorithm. For such an algorithm, we quantify its discretization error in theory and demonstrate its acceleration effect in practice. ", "keywords": [], "authorids": ["yichen2016@u.northwestern.edu", "jinglinc@illinois.edu", "jd2736@columbia.edu", "jianpeng@illinois.edu", "zhaoranwang@gmail.com"], "authors": ["Yi Chen", "Jinglin Chen", "Jing Dong", "Jian Peng", "Zhaoran Wang"], "pdf": "/pdf/61073f72577a50f075e4f466d2ec08b720670c92.pdf", "paperhash": "chen|accelerating_nonconvex_learning_via_replica_exchange_langevin_diffusion", "_bibtex": "@inproceedings{\nchen2018accelerating,\ntitle={{ACCELERATING} {NONCONVEX} {LEARNING} {VIA} {REPLICA} {EXCHANGE} {LANGEVIN} {DIFFUSION}},\nauthor={Yi Chen and Jinglin Chen and Jing Dong and Jian Peng and Zhaoran Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJfPFjA9Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper455/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621623514, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJfPFjA9Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper455/Authors", "ICLR.cc/2019/Conference/Paper455/Reviewers", "ICLR.cc/2019/Conference/Paper455/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper455/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper455/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper455/Authors|ICLR.cc/2019/Conference/Paper455/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper455/Reviewers", "ICLR.cc/2019/Conference/Paper455/Authors", "ICLR.cc/2019/Conference/Paper455/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621623514}}}, {"id": "BJxtOEzrR7", "original": null, "number": 5, "cdate": 1542952048556, "ddate": null, "tcdate": 1542952048556, "tmdate": 1543215848548, "tddate": null, "forum": "SJfPFjA9Fm", "replyto": "BJgu9kFX0Q", "invitation": "ICLR.cc/2019/Conference/-/Paper455/Official_Comment", "content": {"title": "We will adjust the the structure of our paper.", "comment": "Thank your comments. Yes, our paper incorporates some material similar to the existing papers with proper citations in order to be self-contained (mainly the LDP part, which we takes a different Donsker-Varadhan approach from Dupuis\u2019s work). Restricted to space, we have to spend less space on the discretization analysis, which is an innovative contribution of our paper. We will adjust the structure of our paper accordingly and emphasize this part.  "}, "signatures": ["ICLR.cc/2019/Conference/Paper455/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper455/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper455/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ACCELERATING NONCONVEX LEARNING VIA REPLICA EXCHANGE LANGEVIN DIFFUSION", "abstract": "Langevin diffusion is a powerful method for nonconvex optimization, which enables the escape from local minima by injecting noise into the gradient. In particular, the temperature parameter controlling the noise level gives rise to a tradeoff between ``global exploration'' and ``local exploitation'', which correspond to high and low temperatures. To attain the advantages of both regimes, we propose to use replica exchange, which swaps between two Langevin diffusions with different temperatures. We theoretically analyze the acceleration effect of replica exchange from two perspectives: (i) the convergence in $\\chi^2$-divergence, and (ii) the large deviation principle. Such an acceleration effect allows us to faster approach the global minima. Furthermore, by discretizing the replica exchange Langevin diffusion, we obtain a discrete-time algorithm. For such an algorithm, we quantify its discretization error in theory and demonstrate its acceleration effect in practice. ", "keywords": [], "authorids": ["yichen2016@u.northwestern.edu", "jinglinc@illinois.edu", "jd2736@columbia.edu", "jianpeng@illinois.edu", "zhaoranwang@gmail.com"], "authors": ["Yi Chen", "Jinglin Chen", "Jing Dong", "Jian Peng", "Zhaoran Wang"], "pdf": "/pdf/61073f72577a50f075e4f466d2ec08b720670c92.pdf", "paperhash": "chen|accelerating_nonconvex_learning_via_replica_exchange_langevin_diffusion", "_bibtex": "@inproceedings{\nchen2018accelerating,\ntitle={{ACCELERATING} {NONCONVEX} {LEARNING} {VIA} {REPLICA} {EXCHANGE} {LANGEVIN} {DIFFUSION}},\nauthor={Yi Chen and Jinglin Chen and Jing Dong and Jian Peng and Zhaoran Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJfPFjA9Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper455/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621623514, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJfPFjA9Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper455/Authors", "ICLR.cc/2019/Conference/Paper455/Reviewers", "ICLR.cc/2019/Conference/Paper455/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper455/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper455/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper455/Authors|ICLR.cc/2019/Conference/Paper455/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper455/Reviewers", "ICLR.cc/2019/Conference/Paper455/Authors", "ICLR.cc/2019/Conference/Paper455/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621623514}}}, {"id": "SJgu9sxfAQ", "original": null, "number": 1, "cdate": 1542749072332, "ddate": null, "tcdate": 1542749072332, "tmdate": 1542749147927, "tddate": null, "forum": "SJfPFjA9Fm", "replyto": "SkxMd1K32Q", "invitation": "ICLR.cc/2019/Conference/-/Paper455/Official_Comment", "content": {"title": "Our emphasis is an alternative approach and discretization analysis.", "comment": "We really appreciate your comments. The main purpose of this paper is to introduce a new method to solve global optimization problem via replica exchange Langevin diffusion. We quantify the acceleration effect from the viewpoint of continuous time process. Although this work is inspired from Dupuis's work, their setting is MCMC and they only investigate by large deviation. We quantify the acceleration effect by both large deviation and chi^2 divergence. Besides, the large deviation rate function in our paper is different with that of Dupuis's since we use an alternative approach. We choose such a form of rate function because it is connected to the Dirichlet form, and hence, the convergence of chi^2 divergence. We acknowledge that our analysis tools is standard and not fancy in mathematics. However, this is not a mathematics conference after all. One of our contribution is applying standard mathematical tools to a specific machine learning problem. Finally, another contribution is that we propose a discretized algorithm. Although Dupuis& et.al's work establishes beautiful and complicated mathematical theory for replica exchange Langevin diffusions, they does not consider the discretization at all. In practice, we can only use the discretized one instead of the ideal continuous process to solve problems. Our theory quantifies the discretization error and convergence rate and hence, ensures the validity to use the discretized algorithm. "}, "signatures": ["ICLR.cc/2019/Conference/Paper455/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper455/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper455/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ACCELERATING NONCONVEX LEARNING VIA REPLICA EXCHANGE LANGEVIN DIFFUSION", "abstract": "Langevin diffusion is a powerful method for nonconvex optimization, which enables the escape from local minima by injecting noise into the gradient. In particular, the temperature parameter controlling the noise level gives rise to a tradeoff between ``global exploration'' and ``local exploitation'', which correspond to high and low temperatures. To attain the advantages of both regimes, we propose to use replica exchange, which swaps between two Langevin diffusions with different temperatures. We theoretically analyze the acceleration effect of replica exchange from two perspectives: (i) the convergence in $\\chi^2$-divergence, and (ii) the large deviation principle. Such an acceleration effect allows us to faster approach the global minima. Furthermore, by discretizing the replica exchange Langevin diffusion, we obtain a discrete-time algorithm. For such an algorithm, we quantify its discretization error in theory and demonstrate its acceleration effect in practice. ", "keywords": [], "authorids": ["yichen2016@u.northwestern.edu", "jinglinc@illinois.edu", "jd2736@columbia.edu", "jianpeng@illinois.edu", "zhaoranwang@gmail.com"], "authors": ["Yi Chen", "Jinglin Chen", "Jing Dong", "Jian Peng", "Zhaoran Wang"], "pdf": "/pdf/61073f72577a50f075e4f466d2ec08b720670c92.pdf", "paperhash": "chen|accelerating_nonconvex_learning_via_replica_exchange_langevin_diffusion", "_bibtex": "@inproceedings{\nchen2018accelerating,\ntitle={{ACCELERATING} {NONCONVEX} {LEARNING} {VIA} {REPLICA} {EXCHANGE} {LANGEVIN} {DIFFUSION}},\nauthor={Yi Chen and Jinglin Chen and Jing Dong and Jian Peng and Zhaoran Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJfPFjA9Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper455/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621623514, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJfPFjA9Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper455/Authors", "ICLR.cc/2019/Conference/Paper455/Reviewers", "ICLR.cc/2019/Conference/Paper455/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper455/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper455/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper455/Authors|ICLR.cc/2019/Conference/Paper455/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper455/Reviewers", "ICLR.cc/2019/Conference/Paper455/Authors", "ICLR.cc/2019/Conference/Paper455/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621623514}}}, {"id": "Sygv6hHya7", "original": null, "number": 3, "cdate": 1541524671325, "ddate": null, "tcdate": 1541524671325, "tmdate": 1541533981327, "tddate": null, "forum": "SJfPFjA9Fm", "replyto": "SJfPFjA9Fm", "invitation": "ICLR.cc/2019/Conference/-/Paper455/Official_Review", "content": {"title": "well written but not weak results", "review": "The paper considers 'replica exchange' Langevin dynamics. These methods are very popular among practitioners, and developing some theory backing the empirical successes is an important goal.\nUnfortunately this paper offers only weak results. \n- The first 6 pages set up the general formalism. This is textbook material adapted to the current problem.\n- Page 7 offers a result (expression for the Dirichlet form), which is hardly more than an exercise for anybody familiar with Markov Chains theory. \n- Page 8 gives a Poincare inequality. Again, this follows from known results. More importantly: (1) It does not show any advantage of replica exchange over standard dynamics; (2) It does not provide any quantitative insight for high-dimensional problems.\n- Similar comments hold for the following pages. They are an exercise in applying standard formalism to this problem, without really showing any significative advantage of replica exchange.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper455/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ACCELERATING NONCONVEX LEARNING VIA REPLICA EXCHANGE LANGEVIN DIFFUSION", "abstract": "Langevin diffusion is a powerful method for nonconvex optimization, which enables the escape from local minima by injecting noise into the gradient. In particular, the temperature parameter controlling the noise level gives rise to a tradeoff between ``global exploration'' and ``local exploitation'', which correspond to high and low temperatures. To attain the advantages of both regimes, we propose to use replica exchange, which swaps between two Langevin diffusions with different temperatures. We theoretically analyze the acceleration effect of replica exchange from two perspectives: (i) the convergence in $\\chi^2$-divergence, and (ii) the large deviation principle. Such an acceleration effect allows us to faster approach the global minima. Furthermore, by discretizing the replica exchange Langevin diffusion, we obtain a discrete-time algorithm. For such an algorithm, we quantify its discretization error in theory and demonstrate its acceleration effect in practice. ", "keywords": [], "authorids": ["yichen2016@u.northwestern.edu", "jinglinc@illinois.edu", "jd2736@columbia.edu", "jianpeng@illinois.edu", "zhaoranwang@gmail.com"], "authors": ["Yi Chen", "Jinglin Chen", "Jing Dong", "Jian Peng", "Zhaoran Wang"], "pdf": "/pdf/61073f72577a50f075e4f466d2ec08b720670c92.pdf", "paperhash": "chen|accelerating_nonconvex_learning_via_replica_exchange_langevin_diffusion", "_bibtex": "@inproceedings{\nchen2018accelerating,\ntitle={{ACCELERATING} {NONCONVEX} {LEARNING} {VIA} {REPLICA} {EXCHANGE} {LANGEVIN} {DIFFUSION}},\nauthor={Yi Chen and Jinglin Chen and Jing Dong and Jian Peng and Zhaoran Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJfPFjA9Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper455/Official_Review", "cdate": 1542234457683, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SJfPFjA9Fm", "replyto": "SJfPFjA9Fm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper455/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335728038, "tmdate": 1552335728038, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper455/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1eEnYWC3Q", "original": null, "number": 2, "cdate": 1541441963886, "ddate": null, "tcdate": 1541441963886, "tmdate": 1541533981117, "tddate": null, "forum": "SJfPFjA9Fm", "replyto": "SJfPFjA9Fm", "invitation": "ICLR.cc/2019/Conference/-/Paper455/Official_Review", "content": {"title": "Proof that replica exchange accelerates convergence in Langevin dynamics", "review": "This paper gives a theoretical analysis of an interesting statistical physics technique known as replica exchange. The basic idea is that Langevin dynamics at low temperature is slow to converge, and that one could potentially boost the convergence by alternating between low and high temperature. At the extreme one could imagine running in parallel a random search and a gradient descent, and ``teleporting\" the gradient descent algorithm whenever the random search algorithm finds a point with better value. This makes a lot of sense and it is nice to see a theoretical analysis of this. The mathematics are sound, but I do not know whether it is an appropriate submission for ICLR.\n\nOne comment from the math side: it would be interesting (albeit probably difficult) to study kappa in (3.10) as a function of a. In particular at face value it looks like one only benefits from taking a larger, so why not study the limiting behavior of a->infty? What is the limiting value of kappa? Can you perform those calculations in the convex case at least?", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper455/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ACCELERATING NONCONVEX LEARNING VIA REPLICA EXCHANGE LANGEVIN DIFFUSION", "abstract": "Langevin diffusion is a powerful method for nonconvex optimization, which enables the escape from local minima by injecting noise into the gradient. In particular, the temperature parameter controlling the noise level gives rise to a tradeoff between ``global exploration'' and ``local exploitation'', which correspond to high and low temperatures. To attain the advantages of both regimes, we propose to use replica exchange, which swaps between two Langevin diffusions with different temperatures. We theoretically analyze the acceleration effect of replica exchange from two perspectives: (i) the convergence in $\\chi^2$-divergence, and (ii) the large deviation principle. Such an acceleration effect allows us to faster approach the global minima. Furthermore, by discretizing the replica exchange Langevin diffusion, we obtain a discrete-time algorithm. For such an algorithm, we quantify its discretization error in theory and demonstrate its acceleration effect in practice. ", "keywords": [], "authorids": ["yichen2016@u.northwestern.edu", "jinglinc@illinois.edu", "jd2736@columbia.edu", "jianpeng@illinois.edu", "zhaoranwang@gmail.com"], "authors": ["Yi Chen", "Jinglin Chen", "Jing Dong", "Jian Peng", "Zhaoran Wang"], "pdf": "/pdf/61073f72577a50f075e4f466d2ec08b720670c92.pdf", "paperhash": "chen|accelerating_nonconvex_learning_via_replica_exchange_langevin_diffusion", "_bibtex": "@inproceedings{\nchen2018accelerating,\ntitle={{ACCELERATING} {NONCONVEX} {LEARNING} {VIA} {REPLICA} {EXCHANGE} {LANGEVIN} {DIFFUSION}},\nauthor={Yi Chen and Jinglin Chen and Jing Dong and Jian Peng and Zhaoran Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJfPFjA9Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper455/Official_Review", "cdate": 1542234457683, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SJfPFjA9Fm", "replyto": "SJfPFjA9Fm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper455/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335728038, "tmdate": 1552335728038, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper455/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SkxMd1K32Q", "original": null, "number": 1, "cdate": 1541341034468, "ddate": null, "tcdate": 1541341034468, "tmdate": 1541533980914, "tddate": null, "forum": "SJfPFjA9Fm", "replyto": "SJfPFjA9Fm", "invitation": "ICLR.cc/2019/Conference/-/Paper455/Official_Review", "content": {"title": "well written.", "review": "PROS:\n- The text is very well written, with a good balance between mathematical details and intuitions.\n- I really like the high-level description of the algorithms and proof techniques\n\nCONS:\nto be completely honest, I am not sure I have learnt anything new from the paper. \n1) the proof techniques are very standard\n2) although there must be some small innovations, I thought that all the results had more or less been proven by Dupuis and co-authors:\na. large deviation principles\nb. the larger the swapping rate, the better (which motivated Dupuis & al to consider the infinite swapping limit.)\n\nand\nc. Bakri & al methodology to prove convergence relying on the carre du champ is by now very standard and the proofs of the paper are only minor adaptations.\n\nI must probably be missing something, and I encourage the authors to clarify what the main novelties are when compared to the several papers by Dupuis & al. \n\nREMARKS:\n1) I do not really understand the emphasis on optimisation while all the proofs are related to the convergence to the stationary distributions.\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper455/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ACCELERATING NONCONVEX LEARNING VIA REPLICA EXCHANGE LANGEVIN DIFFUSION", "abstract": "Langevin diffusion is a powerful method for nonconvex optimization, which enables the escape from local minima by injecting noise into the gradient. In particular, the temperature parameter controlling the noise level gives rise to a tradeoff between ``global exploration'' and ``local exploitation'', which correspond to high and low temperatures. To attain the advantages of both regimes, we propose to use replica exchange, which swaps between two Langevin diffusions with different temperatures. We theoretically analyze the acceleration effect of replica exchange from two perspectives: (i) the convergence in $\\chi^2$-divergence, and (ii) the large deviation principle. Such an acceleration effect allows us to faster approach the global minima. Furthermore, by discretizing the replica exchange Langevin diffusion, we obtain a discrete-time algorithm. For such an algorithm, we quantify its discretization error in theory and demonstrate its acceleration effect in practice. ", "keywords": [], "authorids": ["yichen2016@u.northwestern.edu", "jinglinc@illinois.edu", "jd2736@columbia.edu", "jianpeng@illinois.edu", "zhaoranwang@gmail.com"], "authors": ["Yi Chen", "Jinglin Chen", "Jing Dong", "Jian Peng", "Zhaoran Wang"], "pdf": "/pdf/61073f72577a50f075e4f466d2ec08b720670c92.pdf", "paperhash": "chen|accelerating_nonconvex_learning_via_replica_exchange_langevin_diffusion", "_bibtex": "@inproceedings{\nchen2018accelerating,\ntitle={{ACCELERATING} {NONCONVEX} {LEARNING} {VIA} {REPLICA} {EXCHANGE} {LANGEVIN} {DIFFUSION}},\nauthor={Yi Chen and Jinglin Chen and Jing Dong and Jian Peng and Zhaoran Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJfPFjA9Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper455/Official_Review", "cdate": 1542234457683, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SJfPFjA9Fm", "replyto": "SJfPFjA9Fm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper455/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335728038, "tmdate": 1552335728038, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper455/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 9}