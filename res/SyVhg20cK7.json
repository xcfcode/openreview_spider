{"notes": [{"id": "SyVhg20cK7", "original": "S1e0Tnnctm", "number": 1116, "cdate": 1538087924334, "ddate": null, "tcdate": 1538087924334, "tmdate": 1545355398422, "tddate": null, "forum": "SyVhg20cK7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Inducing Cooperation via Learning to reshape rewards in semi-cooperative multi-agent reinforcement learning", "abstract": "We propose a deep reinforcement learning algorithm for semi-cooperative multi-agent tasks, where agents are equipped with their separate reward functions, yet with willingness to cooperate. Under these semi-cooperative scenarios, popular methods of centralized training with decentralized execution for inducing cooperation and removing the non-stationarity problem do not work well due to lack of a common shared reward as well as inscalability in centralized training. Our algorithm, called Peer-Evaluation based Dual DQN (PED-DQN), proposes to give peer evaluation signals to observed agents, which quantifies how they feel about a certain transition. This exchange of peer evaluation over time turns out to render agents to gradually reshape their reward functions so that their action choices from the myopic best-response tend to result in the good joint action with high cooperation. This evaluation-based method also allows flexible and scalable training by not assuming knowledge of the number of other agents and their observation and action spaces. We provide the performance evaluation of PED-DQN for the scenarios ranging from a simple two-person prisoner's dilemma to more complex semi-cooperative multi-agent tasks. In special cases where agents share a common reward function as in the centralized training methods, we show that inter-agent\nevaluation leads to better performance\n", "keywords": ["multi-agent reinforcement learning", "deep reinforcement learning", "multi-agent systems"], "authorids": ["ddhostallero@kaist.ac.kr", "kdw2139@gmail.com", "khson@lanada.kaist.ac.kr", "yiyung@kaist.edu"], "authors": ["David Earl Hostallero", "Daewoo Kim", "Kyunghwan Son", "Yung Yi"], "TL;DR": "We use an peer evaluation mechanism to make semi-cooperative agents learn collaborative strategies in multiagent reinforcement learning settings", "pdf": "/pdf/c0d82610150172220b6d0f9634e5fc3585f07791.pdf", "paperhash": "hostallero|inducing_cooperation_via_learning_to_reshape_rewards_in_semicooperative_multiagent_reinforcement_learning", "_bibtex": "@misc{\nhostallero2019inducing,\ntitle={Inducing Cooperation via Learning to reshape rewards in semi-cooperative multi-agent reinforcement learning},\nauthor={David Earl Hostallero and Daewoo Kim and Kyunghwan Son and Yung Yi},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVhg20cK7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "S1xFZ3BlxV", "original": null, "number": 1, "cdate": 1544735745425, "ddate": null, "tcdate": 1544735745425, "tmdate": 1545354513697, "tddate": null, "forum": "SyVhg20cK7", "replyto": "SyVhg20cK7", "invitation": "ICLR.cc/2019/Conference/-/Paper1116/Meta_Review", "content": {"metareview": "This work introduces a reward-shaping scheme for multi-agent settings based on the TD-error of other agents. \n\nOverall, reviewers were positive about the direction and the presentation but had a variety of concerns and questions and felt more experiments were necessary to validate the claims of flexibility and scalability, with results more comparable to the scale of the contemporary multi-agent literature. One note in particular: a feed-forward Q network is used in a partially observable environment, which the authors seemed to dismiss in their rebuttal. I agree with the reviewer that this is an important consideration when comparing to baselines which were developed with recurrent networks in mind.\n\nA revised manuscript addressed concerns with the presentation but did not introduce new results or plots, and reviewers were not convinced to alter their evaluation. There is agreement that this is an interesting paper, so I recommend that the authors conduct a more thorough empirical evaluation and submit to another venue.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Intriguing work, not yet ready for publication."}, "signatures": ["ICLR.cc/2019/Conference/Paper1116/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1116/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inducing Cooperation via Learning to reshape rewards in semi-cooperative multi-agent reinforcement learning", "abstract": "We propose a deep reinforcement learning algorithm for semi-cooperative multi-agent tasks, where agents are equipped with their separate reward functions, yet with willingness to cooperate. Under these semi-cooperative scenarios, popular methods of centralized training with decentralized execution for inducing cooperation and removing the non-stationarity problem do not work well due to lack of a common shared reward as well as inscalability in centralized training. Our algorithm, called Peer-Evaluation based Dual DQN (PED-DQN), proposes to give peer evaluation signals to observed agents, which quantifies how they feel about a certain transition. This exchange of peer evaluation over time turns out to render agents to gradually reshape their reward functions so that their action choices from the myopic best-response tend to result in the good joint action with high cooperation. This evaluation-based method also allows flexible and scalable training by not assuming knowledge of the number of other agents and their observation and action spaces. We provide the performance evaluation of PED-DQN for the scenarios ranging from a simple two-person prisoner's dilemma to more complex semi-cooperative multi-agent tasks. In special cases where agents share a common reward function as in the centralized training methods, we show that inter-agent\nevaluation leads to better performance\n", "keywords": ["multi-agent reinforcement learning", "deep reinforcement learning", "multi-agent systems"], "authorids": ["ddhostallero@kaist.ac.kr", "kdw2139@gmail.com", "khson@lanada.kaist.ac.kr", "yiyung@kaist.edu"], "authors": ["David Earl Hostallero", "Daewoo Kim", "Kyunghwan Son", "Yung Yi"], "TL;DR": "We use an peer evaluation mechanism to make semi-cooperative agents learn collaborative strategies in multiagent reinforcement learning settings", "pdf": "/pdf/c0d82610150172220b6d0f9634e5fc3585f07791.pdf", "paperhash": "hostallero|inducing_cooperation_via_learning_to_reshape_rewards_in_semicooperative_multiagent_reinforcement_learning", "_bibtex": "@misc{\nhostallero2019inducing,\ntitle={Inducing Cooperation via Learning to reshape rewards in semi-cooperative multi-agent reinforcement learning},\nauthor={David Earl Hostallero and Daewoo Kim and Kyunghwan Son and Yung Yi},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVhg20cK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1116/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352960394, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyVhg20cK7", "replyto": "SyVhg20cK7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1116/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1116/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1116/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352960394}}}, {"id": "BkgEMMW937", "original": null, "number": 3, "cdate": 1541177867896, "ddate": null, "tcdate": 1541177867896, "tmdate": 1543532985317, "tddate": null, "forum": "SyVhg20cK7", "replyto": "SyVhg20cK7", "invitation": "ICLR.cc/2019/Conference/-/Paper1116/Official_Review", "content": {"title": "This paper addresses this challenge by introducing a reward-shaping mechanism and incorporating a second DQN technique which is responsible for evaluating other agents performance. No discussion about convergence.", "review": "This work is well-written, but the quality of some sections can be improved significantly as suggested in the comments. I have a few main concerns that I explain in detailed comments. Among those, the paper argues that the algorithms converge without discussing why. Also, the amount of overestimation of the Q-values are one of my big concerns and not intuitive for me in the game of Prisoner's Dilemma that needs to be justified. For these reasons, I am voting for a weak reject now and conditional on the authors' rebuttal, I might increase my score later.\n\n1) I have a series of questions about Prisoner's Dilemma example. I am curious to see what are the Q-values for t=100000 in PD. Is table 1h shows the converged values? What I am expecting to see is that the Q-values should converge to some values slightly larger than 3, but the values are  ~ 30. It is important to quantify how much bias you add to the optimal solution by reward shaping, and why this difference in the Q-values are observed.\n\n2) One thing that is totally missing is the discussion of convergence of the proposed method. In section 3.4, you say that the Q-values converge, but it is not discussed why we expect convergence. The only place in the paper which I can make a conjecture about the convergence is in figure 4c which implicitly implies the convergence of the Mission DQN, but for the other one, I don't see such an observation. Is it possible to formalize the proposed method in the tabular case and discuss whether the Q-values should converge or not? Also, I would like to see the comparison of the Q-values plots in the experiments for both networks.\n\n3) The intuition behind (2) should be better clarified. An example will be informative. I don't understand what |Z_a| is doing in this formula.\n\n4) One of the main contributions of the paper is proposing the reward shaping mechanism. When I read section 3.3, I was expecting to see some result for policy gradient algorithms as well, but this paper does not analyze these algorithms. That would be very nice to see its performance in PG algorithms though. In such case that you are not going to implement these algorithms, I would suggest moving this section to the end of the paper and add it to a section named discussion and conclusion.\n\n5) Is it possible to change the order of parts where you define $\\hat{r}$ with the next part where you define $z_a$? I think that the clarity of this section should be improved. This is just a suggestion to explore. I was confused at the first reading when I saw $z_a$, \"evaluation of transition\" and then (2) without knowing how you define evaluation and why.\n\n6) Is there any reason that ablation testing is only done for trio case? or you choose it randomly. Does the same behavior hold for other cases too?\n\n7) Why in figure 4a, random is always around zero?\n\n8) What will happen if you pass the location of the agent in addition to its observation? In this way, it is possible to have one  Dual-Q-network shared for all agents. This experiment might be added to the baselines in future revisions.\n\nMinor: \n* afore-mentioned -> aforementioned\nsection 4.2: I-DQN is used before definition\n* Is it R_a in (4)?\n* I assume that the table 1f-1h are not for the case of using independent Q-learning. Introducing these tables for the first time right after saying \"This is observed when we use independent Q-learning\" means that these values are coming from independent Q-learning, while they are not as far as I understand. Please make sure that this is correct.\nsection 4.1: * who's -> whose\n* This work is also trying to answer a similar question to yours and should be referenced: \"Learning Policy Representations in Multiagent Systems, by Grover et al. 2018\"\n* Visual illustrations of the game would be helpful in understanding the details of the experiment. Preparing a video of the learned policies also would informative.\n-----------------------------------------------\nAfter rebuttal: after reading the answers, I got answers to most of my questions. Some parts of the paper are vague that I see that other reviewers had the same questions. Given the amount of change required to address these modifications, I am not sure about the quality of the final work, so I keep my score the same.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1116/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Inducing Cooperation via Learning to reshape rewards in semi-cooperative multi-agent reinforcement learning", "abstract": "We propose a deep reinforcement learning algorithm for semi-cooperative multi-agent tasks, where agents are equipped with their separate reward functions, yet with willingness to cooperate. Under these semi-cooperative scenarios, popular methods of centralized training with decentralized execution for inducing cooperation and removing the non-stationarity problem do not work well due to lack of a common shared reward as well as inscalability in centralized training. Our algorithm, called Peer-Evaluation based Dual DQN (PED-DQN), proposes to give peer evaluation signals to observed agents, which quantifies how they feel about a certain transition. This exchange of peer evaluation over time turns out to render agents to gradually reshape their reward functions so that their action choices from the myopic best-response tend to result in the good joint action with high cooperation. This evaluation-based method also allows flexible and scalable training by not assuming knowledge of the number of other agents and their observation and action spaces. We provide the performance evaluation of PED-DQN for the scenarios ranging from a simple two-person prisoner's dilemma to more complex semi-cooperative multi-agent tasks. In special cases where agents share a common reward function as in the centralized training methods, we show that inter-agent\nevaluation leads to better performance\n", "keywords": ["multi-agent reinforcement learning", "deep reinforcement learning", "multi-agent systems"], "authorids": ["ddhostallero@kaist.ac.kr", "kdw2139@gmail.com", "khson@lanada.kaist.ac.kr", "yiyung@kaist.edu"], "authors": ["David Earl Hostallero", "Daewoo Kim", "Kyunghwan Son", "Yung Yi"], "TL;DR": "We use an peer evaluation mechanism to make semi-cooperative agents learn collaborative strategies in multiagent reinforcement learning settings", "pdf": "/pdf/c0d82610150172220b6d0f9634e5fc3585f07791.pdf", "paperhash": "hostallero|inducing_cooperation_via_learning_to_reshape_rewards_in_semicooperative_multiagent_reinforcement_learning", "_bibtex": "@misc{\nhostallero2019inducing,\ntitle={Inducing Cooperation via Learning to reshape rewards in semi-cooperative multi-agent reinforcement learning},\nauthor={David Earl Hostallero and Daewoo Kim and Kyunghwan Son and Yung Yi},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVhg20cK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1116/Official_Review", "cdate": 1542234302472, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SyVhg20cK7", "replyto": "SyVhg20cK7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1116/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335876145, "tmdate": 1552335876145, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1116/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SkgTcbF-07", "original": null, "number": 7, "cdate": 1542717844921, "ddate": null, "tcdate": 1542717844921, "tmdate": 1542717844921, "tddate": null, "forum": "SyVhg20cK7", "replyto": "SyVhg20cK7", "invitation": "ICLR.cc/2019/Conference/-/Paper1116/Official_Comment", "content": {"title": "Revisions", "comment": "Dear reviewers,\n\nThank you for the valuable insights. Significant updates have been made in the methods section to make our framework clearer. We have also added some illustrations in the Prisoner's dilemma game so as to see the convergence of the Q-tables. Individual responses have been commented on your own posts.  "}, "signatures": ["ICLR.cc/2019/Conference/Paper1116/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1116/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1116/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inducing Cooperation via Learning to reshape rewards in semi-cooperative multi-agent reinforcement learning", "abstract": "We propose a deep reinforcement learning algorithm for semi-cooperative multi-agent tasks, where agents are equipped with their separate reward functions, yet with willingness to cooperate. Under these semi-cooperative scenarios, popular methods of centralized training with decentralized execution for inducing cooperation and removing the non-stationarity problem do not work well due to lack of a common shared reward as well as inscalability in centralized training. Our algorithm, called Peer-Evaluation based Dual DQN (PED-DQN), proposes to give peer evaluation signals to observed agents, which quantifies how they feel about a certain transition. This exchange of peer evaluation over time turns out to render agents to gradually reshape their reward functions so that their action choices from the myopic best-response tend to result in the good joint action with high cooperation. This evaluation-based method also allows flexible and scalable training by not assuming knowledge of the number of other agents and their observation and action spaces. We provide the performance evaluation of PED-DQN for the scenarios ranging from a simple two-person prisoner's dilemma to more complex semi-cooperative multi-agent tasks. In special cases where agents share a common reward function as in the centralized training methods, we show that inter-agent\nevaluation leads to better performance\n", "keywords": ["multi-agent reinforcement learning", "deep reinforcement learning", "multi-agent systems"], "authorids": ["ddhostallero@kaist.ac.kr", "kdw2139@gmail.com", "khson@lanada.kaist.ac.kr", "yiyung@kaist.edu"], "authors": ["David Earl Hostallero", "Daewoo Kim", "Kyunghwan Son", "Yung Yi"], "TL;DR": "We use an peer evaluation mechanism to make semi-cooperative agents learn collaborative strategies in multiagent reinforcement learning settings", "pdf": "/pdf/c0d82610150172220b6d0f9634e5fc3585f07791.pdf", "paperhash": "hostallero|inducing_cooperation_via_learning_to_reshape_rewards_in_semicooperative_multiagent_reinforcement_learning", "_bibtex": "@misc{\nhostallero2019inducing,\ntitle={Inducing Cooperation via Learning to reshape rewards in semi-cooperative multi-agent reinforcement learning},\nauthor={David Earl Hostallero and Daewoo Kim and Kyunghwan Son and Yung Yi},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVhg20cK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1116/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617230, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyVhg20cK7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1116/Authors", "ICLR.cc/2019/Conference/Paper1116/Reviewers", "ICLR.cc/2019/Conference/Paper1116/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1116/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1116/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1116/Authors|ICLR.cc/2019/Conference/Paper1116/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1116/Reviewers", "ICLR.cc/2019/Conference/Paper1116/Authors", "ICLR.cc/2019/Conference/Paper1116/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617230}}}, {"id": "BklUQlK-A7", "original": null, "number": 6, "cdate": 1542717470500, "ddate": null, "tcdate": 1542717470500, "tmdate": 1542717470500, "tddate": null, "forum": "SyVhg20cK7", "replyto": "BkgEMMW937", "invitation": "ICLR.cc/2019/Conference/-/Paper1116/Official_Comment", "content": {"title": "Convergence, ablation tests, and sharing the neural network", "comment": "Prisoner\u2019s Dilemma\n\nWe made some modifications to the experiments to see the convergence. We extended the steps to 500k but the epsilon goes down from 1.0 to 0.05 only in the first 100k steps. \n\nSince this is a repeated game, the update rule of Q-table is Q[u] = Q[u] + 0.9*maxQ[u]. Given the rewards, it should naturally converge to values around 30. We replaced the tables with a better illustration for the evolution of the Q-values.\n\nConvergence of the method\n\nSince the Mission-DQN is just a regular DQN, the convergence property of that part is the same, which has no theoretical guarantees. If M-DQN converges, then the TD will be consistent with some granularity. When that happens, the hat{r} will also be consistent. We can then treat the A-DQN as a regular DQN with the same convergence property. The experience replay of the M-DQN has to be large enough to accommodate different policies of the other agents. Otherwise, the M-DQN will not be stable (because it is trying to converge to the recent policies) and thus less likely to converge.\n\nIntuition for (2)\n\nDue to the page limit, we could not fit in an example. First, |Z_a| is there for instances where the magnitude of the received evaluation is smaller than the given evaluation. For example, when an agent receives no evaluation at all, then \\hat{r} should just be the base reward. On the other hand, if the agent a and other observable agents a\u2019 disagree on the value of transition (i.e. sgn(Z_a) != sgn(z_a)), the agent\u2019s adjustment would be less drastic. However, if they agree on the value of transition (i.e. sgn(Z_a) == sgn(z_a)), \\hat{r} could be too large giving the action-Q a large update, only to be decreased again on the next time the transition is observed. This is because mission-Q is also learning. In many cases, the TD error is lower the next time they observe the transition. The formulation \\hat{r}_a = r_a + Z_a will also work.\n\nAblation Testing\n\nYes, the trio capture was selected randomly. We expect to observe the same trend for the other cases. \n\nRegarding Figure 4(a), we believe you are referring to reward, instead of random. In reward, we give the agents reward as a peer evaluation. This means that every time agent a relocates, other agents that can observe will also receive -0.1. The agents learned to avoid other agents, to avoid sharing this additional -0.1 from the other agents. \n\nSharing the neural network\nOne way of sharing the neural network is to concatenate the agent ID to the observation. However, when we experimented on this, the NN learned to ignore the agent IDs. Although the location information is a different information, this may still lead to the agents having the same policy. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1116/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1116/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1116/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inducing Cooperation via Learning to reshape rewards in semi-cooperative multi-agent reinforcement learning", "abstract": "We propose a deep reinforcement learning algorithm for semi-cooperative multi-agent tasks, where agents are equipped with their separate reward functions, yet with willingness to cooperate. Under these semi-cooperative scenarios, popular methods of centralized training with decentralized execution for inducing cooperation and removing the non-stationarity problem do not work well due to lack of a common shared reward as well as inscalability in centralized training. Our algorithm, called Peer-Evaluation based Dual DQN (PED-DQN), proposes to give peer evaluation signals to observed agents, which quantifies how they feel about a certain transition. This exchange of peer evaluation over time turns out to render agents to gradually reshape their reward functions so that their action choices from the myopic best-response tend to result in the good joint action with high cooperation. This evaluation-based method also allows flexible and scalable training by not assuming knowledge of the number of other agents and their observation and action spaces. We provide the performance evaluation of PED-DQN for the scenarios ranging from a simple two-person prisoner's dilemma to more complex semi-cooperative multi-agent tasks. In special cases where agents share a common reward function as in the centralized training methods, we show that inter-agent\nevaluation leads to better performance\n", "keywords": ["multi-agent reinforcement learning", "deep reinforcement learning", "multi-agent systems"], "authorids": ["ddhostallero@kaist.ac.kr", "kdw2139@gmail.com", "khson@lanada.kaist.ac.kr", "yiyung@kaist.edu"], "authors": ["David Earl Hostallero", "Daewoo Kim", "Kyunghwan Son", "Yung Yi"], "TL;DR": "We use an peer evaluation mechanism to make semi-cooperative agents learn collaborative strategies in multiagent reinforcement learning settings", "pdf": "/pdf/c0d82610150172220b6d0f9634e5fc3585f07791.pdf", "paperhash": "hostallero|inducing_cooperation_via_learning_to_reshape_rewards_in_semicooperative_multiagent_reinforcement_learning", "_bibtex": "@misc{\nhostallero2019inducing,\ntitle={Inducing Cooperation via Learning to reshape rewards in semi-cooperative multi-agent reinforcement learning},\nauthor={David Earl Hostallero and Daewoo Kim and Kyunghwan Son and Yung Yi},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVhg20cK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1116/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617230, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyVhg20cK7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1116/Authors", "ICLR.cc/2019/Conference/Paper1116/Reviewers", "ICLR.cc/2019/Conference/Paper1116/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1116/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1116/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1116/Authors|ICLR.cc/2019/Conference/Paper1116/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1116/Reviewers", "ICLR.cc/2019/Conference/Paper1116/Authors", "ICLR.cc/2019/Conference/Paper1116/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617230}}}, {"id": "BJgggeFW07", "original": null, "number": 5, "cdate": 1542717416178, "ddate": null, "tcdate": 1542717416178, "tmdate": 1542717416178, "tddate": null, "forum": "SyVhg20cK7", "replyto": "SygVYCxF27", "invitation": "ICLR.cc/2019/Conference/-/Paper1116/Official_Comment", "content": {"title": "Equation (2)'s effect and reward assumptions", "comment": "Reward shaping in (2)\n\nThe sgn(Z_a) in (2) is there to keep the original sign of the aggregate peer evaluation Z_a which may have changed in the minimization term due to the absolute value operation. We can also clip the peer evaluation values, but it may be difficult to select the correct clipping parameters. The term that actually reduces the magnitude is the minimization term. \n\nThe minimization term in (2) takes tries not to overestimate the value of the transition. For example, if agent a thinks that the transition is \u2018good\u2019 (z_a > 0) and then it is also incentivized by other agents (Z_a > 0), then agent a only needs a little push because the value update is already going to that direction. On the other hand, if Z_a < 0 and z_a > 0, then it tries to prevent a sudden change in the direction of the update. Of course the change of direction in extreme cases (e.g. Z_a << z_a, Z_a >> z_a) may be inevitable, and we may need to clip the rewards. Without the minimization term, the algorithm still works but the fluctuation of value functions are more apparent. \n\nReward assumption and observation claim\n\nThe statement was under the assumption that we are using a neural network and that each agent has a fixed input index in each other\u2019s NN for their shared message (e.g. action, state, signal). In this case, if agent 1 observed state o_t, and the same observation o_{t+k}, but with different agents involved, the two experiences are treated as different. This can be observed in MADDPG\u2019s critic networks. We have modified the TD as Peer evaluation part to make this clear..\n\nYou are correct. Having knowledge of the actions of the other agents may be better. However, even if the reward is a function of both states and actions, the agents can still give a good peer evaluation since the evaluation is a function of the reward. A lower/higher than expected base reward will result in an agent giving a penalty/incentive even if they don\u2019t have explicit knowledge of the actions.\n\nOther Things\n\nThank you for pointing out the parameter sharing approaches. And yes, we are also claiming that this approach works on heterogeneous agents.\n\nAs for the table 1, we added a more comprehensive illustration and discussion of the evolution of the mission-q and action-q.\n\nWe also updated figure 4(a) to include independent DQN (zero evaluation). In contrast to what you said, random does not perform so much better than zero evaluation. If you were asking why random is much better than sharing the reward to observable agents (\u2018reward\u2019 in the plot), it is because agents learned to avoid other agents. Since agents are given a reward of -0.1 for relocating, the observers also get -0.1. We added this in the discussion.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1116/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1116/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1116/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inducing Cooperation via Learning to reshape rewards in semi-cooperative multi-agent reinforcement learning", "abstract": "We propose a deep reinforcement learning algorithm for semi-cooperative multi-agent tasks, where agents are equipped with their separate reward functions, yet with willingness to cooperate. Under these semi-cooperative scenarios, popular methods of centralized training with decentralized execution for inducing cooperation and removing the non-stationarity problem do not work well due to lack of a common shared reward as well as inscalability in centralized training. Our algorithm, called Peer-Evaluation based Dual DQN (PED-DQN), proposes to give peer evaluation signals to observed agents, which quantifies how they feel about a certain transition. This exchange of peer evaluation over time turns out to render agents to gradually reshape their reward functions so that their action choices from the myopic best-response tend to result in the good joint action with high cooperation. This evaluation-based method also allows flexible and scalable training by not assuming knowledge of the number of other agents and their observation and action spaces. We provide the performance evaluation of PED-DQN for the scenarios ranging from a simple two-person prisoner's dilemma to more complex semi-cooperative multi-agent tasks. In special cases where agents share a common reward function as in the centralized training methods, we show that inter-agent\nevaluation leads to better performance\n", "keywords": ["multi-agent reinforcement learning", "deep reinforcement learning", "multi-agent systems"], "authorids": ["ddhostallero@kaist.ac.kr", "kdw2139@gmail.com", "khson@lanada.kaist.ac.kr", "yiyung@kaist.edu"], "authors": ["David Earl Hostallero", "Daewoo Kim", "Kyunghwan Son", "Yung Yi"], "TL;DR": "We use an peer evaluation mechanism to make semi-cooperative agents learn collaborative strategies in multiagent reinforcement learning settings", "pdf": "/pdf/c0d82610150172220b6d0f9634e5fc3585f07791.pdf", "paperhash": "hostallero|inducing_cooperation_via_learning_to_reshape_rewards_in_semicooperative_multiagent_reinforcement_learning", "_bibtex": "@misc{\nhostallero2019inducing,\ntitle={Inducing Cooperation via Learning to reshape rewards in semi-cooperative multi-agent reinforcement learning},\nauthor={David Earl Hostallero and Daewoo Kim and Kyunghwan Son and Yung Yi},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVhg20cK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1116/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617230, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyVhg20cK7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1116/Authors", "ICLR.cc/2019/Conference/Paper1116/Reviewers", "ICLR.cc/2019/Conference/Paper1116/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1116/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1116/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1116/Authors|ICLR.cc/2019/Conference/Paper1116/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1116/Reviewers", "ICLR.cc/2019/Conference/Paper1116/Authors", "ICLR.cc/2019/Conference/Paper1116/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617230}}}, {"id": "rkeR41FW0m", "original": null, "number": 4, "cdate": 1542717237613, "ddate": null, "tcdate": 1542717237613, "tmdate": 1542717237613, "tddate": null, "forum": "SyVhg20cK7", "replyto": "r1eC3Qh_hQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1116/Official_Comment", "content": {"title": "Reward Shaping, Prisoner's Dilemma, and other details", "comment": "Reward shaping in (2) and (3)\n\nThe initial design of the reward shaping is similar to your suggested simple formulation \\hat{r}_a = r_a + Z_a. The reason we used a subset K_a in (3) is so that the \u201cchange\u201d in the reward can be associated with the agents. We think that receiving peer evaluation from unobservable agents is just noise, although this may not be the case for agents with different observation ranges. \n\nThe minimization term in (2) takes tries not to overestimate the value of the transition. For example, if agent a thinks that the transition is \u2018good\u2019 (z_a > 0) and then it is also incentivized by other agents (Z_a > 0), then agent a only needs a little push because the value update is already going to that direction. On the other hand, if Z_a < 0 and z_a > 0, then it tries to prevent a sudden change in the direction of the update. Of course the change of direction in extreme cases (e.g. Z_a << z_a, Z_a >> z_a) may be inevitable, and we may need to clip the rewards. Without the minimization term, the algorithm still works but the fluctuation of value functions are more apparent. \n\nAs of the comment about VDN, in a way, our method looks like the reverse process of VDN but starting with individual rewards instead of a global one. Thus, the values are already decomposed. However, we don\u2019t have any proof that this reduces to VDN.\n\nPrisoner\u2019s Dilemma\n\nAlthough the last action cannot be observed, the base rewards can imply the action of the opponent. Also, we designed the PD so that the agents do not condition their Q-values on more than 1 state, and thus easier to analyze.\n\nThe goal is to reshape the rewards so that the perceived reward of the agents become as cooperative as they can. The willingness to cooperate, \\beta, governs this. In the prisoner\u2019s dilemma, we had to use a beta of 1.4 so that they could read the global optimum (3,3). \n\n\nCentral-V, QMIX, I-DQN\n\nThank you for pointing out the error in centralized neural networks. We have updated the introduction appropriately. As for using feed-forward networks, we did not focus on the neural network structure. Similarly, QMIX\u2019s main contribution is the value function factorization. Of course, RNN can be used as an alternative when necessary. For the experiments that involved I-DQN we only used recent buffer [1] for all the algorithms. Of crouse, the M-DQN in our framework used a replay buffer.\n\n[1]Joel Z Leibo, Vinicius Zambaldi, Marc Lanctot, Janusz Marecki, and Thore Graepel.  Multi-agent reinforcement learning in sequential social dilemmas. InProceedings of International Conference on Autonomous Agents and MultiAgent Systems, pp. 464\u2013473, 2017.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1116/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1116/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1116/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inducing Cooperation via Learning to reshape rewards in semi-cooperative multi-agent reinforcement learning", "abstract": "We propose a deep reinforcement learning algorithm for semi-cooperative multi-agent tasks, where agents are equipped with their separate reward functions, yet with willingness to cooperate. Under these semi-cooperative scenarios, popular methods of centralized training with decentralized execution for inducing cooperation and removing the non-stationarity problem do not work well due to lack of a common shared reward as well as inscalability in centralized training. Our algorithm, called Peer-Evaluation based Dual DQN (PED-DQN), proposes to give peer evaluation signals to observed agents, which quantifies how they feel about a certain transition. This exchange of peer evaluation over time turns out to render agents to gradually reshape their reward functions so that their action choices from the myopic best-response tend to result in the good joint action with high cooperation. This evaluation-based method also allows flexible and scalable training by not assuming knowledge of the number of other agents and their observation and action spaces. We provide the performance evaluation of PED-DQN for the scenarios ranging from a simple two-person prisoner's dilemma to more complex semi-cooperative multi-agent tasks. In special cases where agents share a common reward function as in the centralized training methods, we show that inter-agent\nevaluation leads to better performance\n", "keywords": ["multi-agent reinforcement learning", "deep reinforcement learning", "multi-agent systems"], "authorids": ["ddhostallero@kaist.ac.kr", "kdw2139@gmail.com", "khson@lanada.kaist.ac.kr", "yiyung@kaist.edu"], "authors": ["David Earl Hostallero", "Daewoo Kim", "Kyunghwan Son", "Yung Yi"], "TL;DR": "We use an peer evaluation mechanism to make semi-cooperative agents learn collaborative strategies in multiagent reinforcement learning settings", "pdf": "/pdf/c0d82610150172220b6d0f9634e5fc3585f07791.pdf", "paperhash": "hostallero|inducing_cooperation_via_learning_to_reshape_rewards_in_semicooperative_multiagent_reinforcement_learning", "_bibtex": "@misc{\nhostallero2019inducing,\ntitle={Inducing Cooperation via Learning to reshape rewards in semi-cooperative multi-agent reinforcement learning},\nauthor={David Earl Hostallero and Daewoo Kim and Kyunghwan Son and Yung Yi},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVhg20cK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1116/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617230, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyVhg20cK7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1116/Authors", "ICLR.cc/2019/Conference/Paper1116/Reviewers", "ICLR.cc/2019/Conference/Paper1116/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1116/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1116/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1116/Authors|ICLR.cc/2019/Conference/Paper1116/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1116/Reviewers", "ICLR.cc/2019/Conference/Paper1116/Authors", "ICLR.cc/2019/Conference/Paper1116/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617230}}}, {"id": "r1eC3Qh_hQ", "original": null, "number": 1, "cdate": 1541092278384, "ddate": null, "tcdate": 1541092278384, "tmdate": 1541533408280, "tddate": null, "forum": "SyVhg20cK7", "replyto": "SyVhg20cK7", "invitation": "ICLR.cc/2019/Conference/-/Paper1116/Official_Review", "content": {"title": "Potentially an intriguing paper, however some key choices are poorly motivated and a few results miss leading. ", "review": "The authors suggest a reward shaping algorithm for multi-agent settings that adds a shaping term based on the TD-error of other agents to the reward. In order to implement this, each agent needs to keep tack of two different value estimates through different DQN networks, one for the unshaped reward and one for the shaped reward. \n\nPoints of improvement and questions: \n-Can you please motivate the form of the reward shaping suggested in (2) and (3)? It looks very similar to simply taking \\hat{r}_a = r_a + sum_{a' not a} z_{'a}. Did you compare against this simple formulation? I think this will basically reduce the method to Value Decomposition Networks (Sunehag \u200e2017) \n-The results on the prisoners dilemma seem miss-leading: The \"peer review\" signal effectively changes the game from being self-interested to optimising a joint reward. It's not at all surprising that agents get higher rewards in a single shot dilemma when optimising the joint reward. The same holds for the \"Selfish Quota-based Pursuit\" - changing the reward function clearly will change the outcome here. Eg. there is a trivial adjustment that adds all other agents rewards to the reward for agent i that will will also resolve any social dilemma.\n-What's the point of playing an iterated prisoners dilemma when the last action can't be observed? That seems like a confounding factor. Also, using gamma of 0.9 means the agents' horizon is effectively limited to around 10 steps, making 50k games even more unnecessary. \n-\"The input for the centralized neural network involves the concatenation of the observations and actions, and optionally, the full state\": This is not true. For example, the Central-V baseline in COMA can be implemented by feeding the central state along (without any actions or local observations) into the value-function. It is thus scalable to large numbers of agents. \n-The model seems to use a feed-forward policy in a partially observable multi-agent setting. Can you please provide a justification for this choice? Some of the baseline methods you compare against, eg. QMIX, were developed and tested on recurrent policies. Furthermore, independent Q-learning is known to be less stable when using feedfoward networks due to the non-stationarity issues arising (see eg. \"Stabilising Experience Replay\", ICML 2017, Foerster et al). In it's current form the concerns mentioned outweigh the contributions of the paper.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1116/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inducing Cooperation via Learning to reshape rewards in semi-cooperative multi-agent reinforcement learning", "abstract": "We propose a deep reinforcement learning algorithm for semi-cooperative multi-agent tasks, where agents are equipped with their separate reward functions, yet with willingness to cooperate. Under these semi-cooperative scenarios, popular methods of centralized training with decentralized execution for inducing cooperation and removing the non-stationarity problem do not work well due to lack of a common shared reward as well as inscalability in centralized training. Our algorithm, called Peer-Evaluation based Dual DQN (PED-DQN), proposes to give peer evaluation signals to observed agents, which quantifies how they feel about a certain transition. This exchange of peer evaluation over time turns out to render agents to gradually reshape their reward functions so that their action choices from the myopic best-response tend to result in the good joint action with high cooperation. This evaluation-based method also allows flexible and scalable training by not assuming knowledge of the number of other agents and their observation and action spaces. We provide the performance evaluation of PED-DQN for the scenarios ranging from a simple two-person prisoner's dilemma to more complex semi-cooperative multi-agent tasks. In special cases where agents share a common reward function as in the centralized training methods, we show that inter-agent\nevaluation leads to better performance\n", "keywords": ["multi-agent reinforcement learning", "deep reinforcement learning", "multi-agent systems"], "authorids": ["ddhostallero@kaist.ac.kr", "kdw2139@gmail.com", "khson@lanada.kaist.ac.kr", "yiyung@kaist.edu"], "authors": ["David Earl Hostallero", "Daewoo Kim", "Kyunghwan Son", "Yung Yi"], "TL;DR": "We use an peer evaluation mechanism to make semi-cooperative agents learn collaborative strategies in multiagent reinforcement learning settings", "pdf": "/pdf/c0d82610150172220b6d0f9634e5fc3585f07791.pdf", "paperhash": "hostallero|inducing_cooperation_via_learning_to_reshape_rewards_in_semicooperative_multiagent_reinforcement_learning", "_bibtex": "@misc{\nhostallero2019inducing,\ntitle={Inducing Cooperation via Learning to reshape rewards in semi-cooperative multi-agent reinforcement learning},\nauthor={David Earl Hostallero and Daewoo Kim and Kyunghwan Son and Yung Yi},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVhg20cK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1116/Official_Review", "cdate": 1542234302472, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SyVhg20cK7", "replyto": "SyVhg20cK7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1116/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335876145, "tmdate": 1552335876145, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1116/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SygVYCxF27", "original": null, "number": 2, "cdate": 1541111419970, "ddate": null, "tcdate": 1541111419970, "tmdate": 1541533408070, "tddate": null, "forum": "SyVhg20cK7", "replyto": "SyVhg20cK7", "invitation": "ICLR.cc/2019/Conference/-/Paper1116/Official_Review", "content": {"title": "Interesting connections to study of social dilemma and role of peer evaluation; experiments not enough to make the scalability claim  ", "review": "The paper introduces a DQN based, hierarchical, peer-evaluation scheme for reward design that induces cooperation in semi-cooperative multi-agent RL systems. The key feature of this approach is its scalability since only local \u201ccommunication\u201d is required -- the number of agents is impertinent; no states and actions are shared between the agents. Moreover this \u201ccommunication\u201d is bound to be low dimensional since only scalar values are shared and has interesting connections to sociology. Interesting metaphor of \u201cfeel\u201d about a transition.\n\nRegarding sgn(Z_a) in Eq2, often DQN based approaches clip their rewards to be between say -1 and 1. The paper says this helps reduce magnitude, but is it just an optimization artifact, or it\u2019s necessary for the reward shaping to work, is slightly unclear. \n\nI agree with the paper\u2019s claim that it\u2019s important for an agent to learn from it\u2019s local observation than to depend on joint actions. However, the sentence \u201cThis is because similar partially-observed transitions involving different subsets of agents will require different samples when we assume that agents share some state or action information.\u201d is unclear to me. Is the paper trying to just say that it\u2019s more efficient because what we care about is the value of the transition and different joint actions might have the same transition value because the same change in state occured. However, it seems that paper is making an implicit assumption about how rewards look like. If the rewards are a function of both states and actions, r(s,a) ignoring actions might lead to incorrect approximations.\n\nIn Sec 3.2, under scalability and flexibility, I agree with the paper that neural networks are weird and increasing the number of parameters doesn\u2019t necessarily make the task more complex. However the last sentence ignores parameter sharing approaches as in [1], whose input size doesn\u2019t necessarily increase as the number of agents grows. I understand that the authors want to claim that the introduced approach works in non homogeneous settings as well.\n\nI get the point being made, but Table 1 is unclear to me. In my understanding of the notations, Q_a should refer to Action Q-table. But the top row seems to be showing the perceived reward matrix. How does it relate to Mission Q-table and Action Q-table is not obviously clear.\n\nGiven all the setup and focus on flexibility and scalability, as I reach the experiment section, I am expecting some bigger experiments compared to a lot of recent MARL papers which often don\u2019t have more two agents. From that perspective the experiments are a bit disappointing. Even if the focus is on pedagogy and therefore pursuit-evasion domain, not only are the maps quite small, the number of agents is not that large (maximum being 5). So it\u2019s hard to confirm whether the scalability claim necessarily make sense here. I would also prefer to see some discussion/intuitions for why the random peer evaluation works as well as it did in Fig 4(a). It doesn\u2019t seem like the problem is that of \\beta being too small. But then how is random evaluation able to do so much better than zero evaluation?\n\nOverall it\u2019s definitely an interesting paper. However it needs more experiments to confirm some of its claims about scalability and flexibility.\n\nMinor points\nI think the section on application to actor critic is unnecessary and without experiments, hard to say it would actually work that well, given there\u2019s a policy to be learned and the value function being learned is more about variance reduction than actual actions.\nIn Supplementary, Table 2: map size says 8x7. Which one is correct?\n\n[1]: https://link.springer.com/chapter/10.1007/978-3-319-71682-4_5\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1116/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inducing Cooperation via Learning to reshape rewards in semi-cooperative multi-agent reinforcement learning", "abstract": "We propose a deep reinforcement learning algorithm for semi-cooperative multi-agent tasks, where agents are equipped with their separate reward functions, yet with willingness to cooperate. Under these semi-cooperative scenarios, popular methods of centralized training with decentralized execution for inducing cooperation and removing the non-stationarity problem do not work well due to lack of a common shared reward as well as inscalability in centralized training. Our algorithm, called Peer-Evaluation based Dual DQN (PED-DQN), proposes to give peer evaluation signals to observed agents, which quantifies how they feel about a certain transition. This exchange of peer evaluation over time turns out to render agents to gradually reshape their reward functions so that their action choices from the myopic best-response tend to result in the good joint action with high cooperation. This evaluation-based method also allows flexible and scalable training by not assuming knowledge of the number of other agents and their observation and action spaces. We provide the performance evaluation of PED-DQN for the scenarios ranging from a simple two-person prisoner's dilemma to more complex semi-cooperative multi-agent tasks. In special cases where agents share a common reward function as in the centralized training methods, we show that inter-agent\nevaluation leads to better performance\n", "keywords": ["multi-agent reinforcement learning", "deep reinforcement learning", "multi-agent systems"], "authorids": ["ddhostallero@kaist.ac.kr", "kdw2139@gmail.com", "khson@lanada.kaist.ac.kr", "yiyung@kaist.edu"], "authors": ["David Earl Hostallero", "Daewoo Kim", "Kyunghwan Son", "Yung Yi"], "TL;DR": "We use an peer evaluation mechanism to make semi-cooperative agents learn collaborative strategies in multiagent reinforcement learning settings", "pdf": "/pdf/c0d82610150172220b6d0f9634e5fc3585f07791.pdf", "paperhash": "hostallero|inducing_cooperation_via_learning_to_reshape_rewards_in_semicooperative_multiagent_reinforcement_learning", "_bibtex": "@misc{\nhostallero2019inducing,\ntitle={Inducing Cooperation via Learning to reshape rewards in semi-cooperative multi-agent reinforcement learning},\nauthor={David Earl Hostallero and Daewoo Kim and Kyunghwan Son and Yung Yi},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVhg20cK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1116/Official_Review", "cdate": 1542234302472, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SyVhg20cK7", "replyto": "SyVhg20cK7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1116/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335876145, "tmdate": 1552335876145, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1116/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 9}