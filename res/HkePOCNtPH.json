{"notes": [{"id": "HkePOCNtPH", "original": "HJl2hpwuPS", "number": 1215, "cdate": 1569439343247, "ddate": null, "tcdate": 1569439343247, "tmdate": 1577168291017, "tddate": null, "forum": "HkePOCNtPH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Non-Sequential Melody Generation", "authors": ["Mitchell Billard", "Robert Bishop", "Moustafa Elsisy", "Laura Graves", "Antonina Kolokolova", "Vineel Nagisetty", "Zachary Northcott", "Heather Patey"], "authorids": ["mlb238@mun.ca", "r.bishop@mun.ca", "mmatelsisy@mun.ca", "cmgraves@mun.ca", "kol@mun.ca", "vnagisetty@mun.ca", "zmnorthcott@mun.ca", "hpatey@gmail.com"], "keywords": ["melody generation", "DCGAN", "dilated convolutions"], "TL;DR": "Representing melodies as images with semantic units aligned we can generate them using a DCGAN without any recurrent components.", "abstract": "In this paper we present a method for algorithmic melody generation using a generative adversarial network without recurrent components. Music generation has been successfully done using recurrent neural networks, where the model learns sequence information that can help create authentic sounding melodies.  Here, we use DCGAN architecture with dilated convolutions and towers to capture sequential information as spatial image information, and learn long-range dependencies in fixed-length melody forms such as Irish traditional reel. ", "code": "https://github.com/gan-music-generation/gan_music_generation", "pdf": "/pdf/150f6f66fb5f8ef007444d4fa905636c8fa9bc82.pdf", "paperhash": "billard|nonsequential_melody_generation", "original_pdf": "/attachment/150f6f66fb5f8ef007444d4fa905636c8fa9bc82.pdf", "_bibtex": "@misc{\nbillard2020nonsequential,\ntitle={Non-Sequential Melody Generation},\nauthor={Mitchell Billard and Robert Bishop and Moustafa Elsisy and Laura Graves and Antonina Kolokolova and Vineel Nagisetty and Zachary Northcott and Heather Patey},\nyear={2020},\nurl={https://openreview.net/forum?id=HkePOCNtPH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "B5KfLgT1_z", "original": null, "number": 1, "cdate": 1576798717668, "ddate": null, "tcdate": 1576798717668, "tmdate": 1576800918897, "tddate": null, "forum": "HkePOCNtPH", "replyto": "HkePOCNtPH", "invitation": "ICLR.cc/2020/Conference/Paper1215/-/Decision", "content": {"decision": "Reject", "comment": "All the reviewers pointed out issues with the experiments, which the rebuttal did not address. The paper seems interesting, and the authors are encouraged to improve it.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Non-Sequential Melody Generation", "authors": ["Mitchell Billard", "Robert Bishop", "Moustafa Elsisy", "Laura Graves", "Antonina Kolokolova", "Vineel Nagisetty", "Zachary Northcott", "Heather Patey"], "authorids": ["mlb238@mun.ca", "r.bishop@mun.ca", "mmatelsisy@mun.ca", "cmgraves@mun.ca", "kol@mun.ca", "vnagisetty@mun.ca", "zmnorthcott@mun.ca", "hpatey@gmail.com"], "keywords": ["melody generation", "DCGAN", "dilated convolutions"], "TL;DR": "Representing melodies as images with semantic units aligned we can generate them using a DCGAN without any recurrent components.", "abstract": "In this paper we present a method for algorithmic melody generation using a generative adversarial network without recurrent components. Music generation has been successfully done using recurrent neural networks, where the model learns sequence information that can help create authentic sounding melodies.  Here, we use DCGAN architecture with dilated convolutions and towers to capture sequential information as spatial image information, and learn long-range dependencies in fixed-length melody forms such as Irish traditional reel. ", "code": "https://github.com/gan-music-generation/gan_music_generation", "pdf": "/pdf/150f6f66fb5f8ef007444d4fa905636c8fa9bc82.pdf", "paperhash": "billard|nonsequential_melody_generation", "original_pdf": "/attachment/150f6f66fb5f8ef007444d4fa905636c8fa9bc82.pdf", "_bibtex": "@misc{\nbillard2020nonsequential,\ntitle={Non-Sequential Melody Generation},\nauthor={Mitchell Billard and Robert Bishop and Moustafa Elsisy and Laura Graves and Antonina Kolokolova and Vineel Nagisetty and Zachary Northcott and Heather Patey},\nyear={2020},\nurl={https://openreview.net/forum?id=HkePOCNtPH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HkePOCNtPH", "replyto": "HkePOCNtPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795727415, "tmdate": 1576800279642, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1215/-/Decision"}}}, {"id": "HJenBTmnor", "original": null, "number": 3, "cdate": 1573825860456, "ddate": null, "tcdate": 1573825860456, "tmdate": 1573825860456, "tddate": null, "forum": "HkePOCNtPH", "replyto": "SkxwB_uvKB", "invitation": "ICLR.cc/2020/Conference/Paper1215/-/Official_Comment", "content": {"title": "Thank you", "comment": "Thank you for your feedback. We appreciate the meticulousness you put into reading the paper, and will use add those changes in the revision.\n\nWe were also wondering if you had any ideas of what metrics might be better than the note distribution? We opted to use this, as in the beginning of the project, the generated music very obviously fell outside what would be typically expected, and this metric helped quickly asses large amounts of data without listening to many songs."}, "signatures": ["ICLR.cc/2020/Conference/Paper1215/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1215/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Non-Sequential Melody Generation", "authors": ["Mitchell Billard", "Robert Bishop", "Moustafa Elsisy", "Laura Graves", "Antonina Kolokolova", "Vineel Nagisetty", "Zachary Northcott", "Heather Patey"], "authorids": ["mlb238@mun.ca", "r.bishop@mun.ca", "mmatelsisy@mun.ca", "cmgraves@mun.ca", "kol@mun.ca", "vnagisetty@mun.ca", "zmnorthcott@mun.ca", "hpatey@gmail.com"], "keywords": ["melody generation", "DCGAN", "dilated convolutions"], "TL;DR": "Representing melodies as images with semantic units aligned we can generate them using a DCGAN without any recurrent components.", "abstract": "In this paper we present a method for algorithmic melody generation using a generative adversarial network without recurrent components. Music generation has been successfully done using recurrent neural networks, where the model learns sequence information that can help create authentic sounding melodies.  Here, we use DCGAN architecture with dilated convolutions and towers to capture sequential information as spatial image information, and learn long-range dependencies in fixed-length melody forms such as Irish traditional reel. ", "code": "https://github.com/gan-music-generation/gan_music_generation", "pdf": "/pdf/150f6f66fb5f8ef007444d4fa905636c8fa9bc82.pdf", "paperhash": "billard|nonsequential_melody_generation", "original_pdf": "/attachment/150f6f66fb5f8ef007444d4fa905636c8fa9bc82.pdf", "_bibtex": "@misc{\nbillard2020nonsequential,\ntitle={Non-Sequential Melody Generation},\nauthor={Mitchell Billard and Robert Bishop and Moustafa Elsisy and Laura Graves and Antonina Kolokolova and Vineel Nagisetty and Zachary Northcott and Heather Patey},\nyear={2020},\nurl={https://openreview.net/forum?id=HkePOCNtPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkePOCNtPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1215/Authors", "ICLR.cc/2020/Conference/Paper1215/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1215/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1215/Reviewers", "ICLR.cc/2020/Conference/Paper1215/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1215/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1215/Authors|ICLR.cc/2020/Conference/Paper1215/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504159456, "tmdate": 1576860530881, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1215/Authors", "ICLR.cc/2020/Conference/Paper1215/Reviewers", "ICLR.cc/2020/Conference/Paper1215/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1215/-/Official_Comment"}}}, {"id": "rJlnf6XhjB", "original": null, "number": 2, "cdate": 1573825811848, "ddate": null, "tcdate": 1573825811848, "tmdate": 1573825811848, "tddate": null, "forum": "HkePOCNtPH", "replyto": "S1eEwK92Fr", "invitation": "ICLR.cc/2020/Conference/Paper1215/-/Official_Comment", "content": {"title": "Thank you", "comment": "Thank you for your feedback. We particularly like the suggestion of comparing the different iterations of the models we tried.\n\nAs well, when it comes to the T-SNE metric we used, do you have something specific in mind that you feel would be better than it?"}, "signatures": ["ICLR.cc/2020/Conference/Paper1215/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1215/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Non-Sequential Melody Generation", "authors": ["Mitchell Billard", "Robert Bishop", "Moustafa Elsisy", "Laura Graves", "Antonina Kolokolova", "Vineel Nagisetty", "Zachary Northcott", "Heather Patey"], "authorids": ["mlb238@mun.ca", "r.bishop@mun.ca", "mmatelsisy@mun.ca", "cmgraves@mun.ca", "kol@mun.ca", "vnagisetty@mun.ca", "zmnorthcott@mun.ca", "hpatey@gmail.com"], "keywords": ["melody generation", "DCGAN", "dilated convolutions"], "TL;DR": "Representing melodies as images with semantic units aligned we can generate them using a DCGAN without any recurrent components.", "abstract": "In this paper we present a method for algorithmic melody generation using a generative adversarial network without recurrent components. Music generation has been successfully done using recurrent neural networks, where the model learns sequence information that can help create authentic sounding melodies.  Here, we use DCGAN architecture with dilated convolutions and towers to capture sequential information as spatial image information, and learn long-range dependencies in fixed-length melody forms such as Irish traditional reel. ", "code": "https://github.com/gan-music-generation/gan_music_generation", "pdf": "/pdf/150f6f66fb5f8ef007444d4fa905636c8fa9bc82.pdf", "paperhash": "billard|nonsequential_melody_generation", "original_pdf": "/attachment/150f6f66fb5f8ef007444d4fa905636c8fa9bc82.pdf", "_bibtex": "@misc{\nbillard2020nonsequential,\ntitle={Non-Sequential Melody Generation},\nauthor={Mitchell Billard and Robert Bishop and Moustafa Elsisy and Laura Graves and Antonina Kolokolova and Vineel Nagisetty and Zachary Northcott and Heather Patey},\nyear={2020},\nurl={https://openreview.net/forum?id=HkePOCNtPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkePOCNtPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1215/Authors", "ICLR.cc/2020/Conference/Paper1215/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1215/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1215/Reviewers", "ICLR.cc/2020/Conference/Paper1215/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1215/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1215/Authors|ICLR.cc/2020/Conference/Paper1215/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504159456, "tmdate": 1576860530881, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1215/Authors", "ICLR.cc/2020/Conference/Paper1215/Reviewers", "ICLR.cc/2020/Conference/Paper1215/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1215/-/Official_Comment"}}}, {"id": "S1eWe673sB", "original": null, "number": 1, "cdate": 1573825769332, "ddate": null, "tcdate": 1573825769332, "tmdate": 1573825769332, "tddate": null, "forum": "HkePOCNtPH", "replyto": "HygOPyIfqr", "invitation": "ICLR.cc/2020/Conference/Paper1215/-/Official_Comment", "content": {"title": "Thank you for your feedback", "comment": "Thank you very much for your thorough feedback and pointers to many useful references.  We will incorporate your suggestions into the next version of the paper, in particular including a more detailed explanation and evaluation of hyperparameters, more comparisons, and a further evaluation with respect to both given and other metrics.  We are especially grateful for your suggestions on the analysis of mode dropping, data augmentation and variants of the architecture to consider. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1215/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1215/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Non-Sequential Melody Generation", "authors": ["Mitchell Billard", "Robert Bishop", "Moustafa Elsisy", "Laura Graves", "Antonina Kolokolova", "Vineel Nagisetty", "Zachary Northcott", "Heather Patey"], "authorids": ["mlb238@mun.ca", "r.bishop@mun.ca", "mmatelsisy@mun.ca", "cmgraves@mun.ca", "kol@mun.ca", "vnagisetty@mun.ca", "zmnorthcott@mun.ca", "hpatey@gmail.com"], "keywords": ["melody generation", "DCGAN", "dilated convolutions"], "TL;DR": "Representing melodies as images with semantic units aligned we can generate them using a DCGAN without any recurrent components.", "abstract": "In this paper we present a method for algorithmic melody generation using a generative adversarial network without recurrent components. Music generation has been successfully done using recurrent neural networks, where the model learns sequence information that can help create authentic sounding melodies.  Here, we use DCGAN architecture with dilated convolutions and towers to capture sequential information as spatial image information, and learn long-range dependencies in fixed-length melody forms such as Irish traditional reel. ", "code": "https://github.com/gan-music-generation/gan_music_generation", "pdf": "/pdf/150f6f66fb5f8ef007444d4fa905636c8fa9bc82.pdf", "paperhash": "billard|nonsequential_melody_generation", "original_pdf": "/attachment/150f6f66fb5f8ef007444d4fa905636c8fa9bc82.pdf", "_bibtex": "@misc{\nbillard2020nonsequential,\ntitle={Non-Sequential Melody Generation},\nauthor={Mitchell Billard and Robert Bishop and Moustafa Elsisy and Laura Graves and Antonina Kolokolova and Vineel Nagisetty and Zachary Northcott and Heather Patey},\nyear={2020},\nurl={https://openreview.net/forum?id=HkePOCNtPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkePOCNtPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1215/Authors", "ICLR.cc/2020/Conference/Paper1215/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1215/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1215/Reviewers", "ICLR.cc/2020/Conference/Paper1215/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1215/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1215/Authors|ICLR.cc/2020/Conference/Paper1215/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504159456, "tmdate": 1576860530881, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1215/Authors", "ICLR.cc/2020/Conference/Paper1215/Reviewers", "ICLR.cc/2020/Conference/Paper1215/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1215/-/Official_Comment"}}}, {"id": "SkxwB_uvKB", "original": null, "number": 1, "cdate": 1571420223295, "ddate": null, "tcdate": 1571420223295, "tmdate": 1572972497813, "tddate": null, "forum": "HkePOCNtPH", "replyto": "HkePOCNtPH", "invitation": "ICLR.cc/2020/Conference/Paper1215/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper presents a DCGAN for generating Irish folk music. The fixed length of Irish reels (in terms of bars) means that a piece can be represented as a 2-D image with fixed height and fixed width. This study represents each performance as a 4x64 image, where each successive row represents a phrase and pixels in neighbouring rows represent corresponding notes in successive phrases. A DCGAN architecture is then used to generate novel melodies using this representation. Each pixel has a normalised value between [-1,1] which correspond to the notes A-G, a-g in the vocabulary. \n\nThis study to me appears incomplete and at an early stage and requires further work before it can be accepted as a conference publication. Firstly, I believe the title is misleading. The title encourages the reader to expect a general model for generating melodies with GANs however the application is specifically to generate monophonic Irish folk music only, with a fixed number of bars. Secondly, the main novelty in the paper, the DCGAN architecture has not been described in enough detail, for instance the optimiser and hyper parameters are not mentioned, making it difficult for this work to be reproduced. \n\nThe authors claim 3 major motivations for  the DCGAN; 1) Dilated convolutions for introducing music related inductive priors, however the model description does not provide any intuition or insight for why the exact filters used were chosen. 2) The authors claim the GAN also yields a discriminator in addition to a melody generator, but they do not provide any explanation for why a discriminator might be useful. 3) They claim using random noise instead of a musically meaningful seed is better, but they do not describe the input noise distribution that the proposed generator is conditioned on.\n\nFor the evaluations, the Frechet distance is introduced without a reference/citation and the \u201cmodels in Magenta\u201d are referenced without a citation.  One downside of using the GAN is that there is no way to calculate the log-likelihood of the generated samples under the model distribution, which makes evaluating/comparing models a difficult problem. I think the distribution of notes in the generated samples is not a very informative metric for comparing the quality of generated samples. A few audio examples would have been extremely useful in getting a sense of how this model performs. \n\nI think this work should be resubmitted with major revisions. \n\nMinor Comments\n\n1. Section 2 does a good job of providing background for Irish folk music and the ABC notation. \n2. \u201cnormalised MIDI value\u201d should be elaborated. I inferred it to mean the MIDI values are normalised to the range [-1,1] after reading the whole paper, however this is not clear in Section 2.1 \n3. \u201cresulting vectors of 256 values as a 64x4 image\u201d, I think the image is 4x64 (Figure 1). \n4. \u201c..to use bidirectional RNNs with an activation mechanism\u201d, this should be attention mechanism. \n5. There should be some comment on the motivation behind the specific choice of filters in the generator and discriminator. \n6. There should be some details about the optimisation algorithm, batch sizes, learning rate schedules etc used to train the network. \n7. The magenta model requires a citation as it is not obvious what is being referred to. \n8. The Frechet distance should be introduced with a citation. \n\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1215/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1215/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Non-Sequential Melody Generation", "authors": ["Mitchell Billard", "Robert Bishop", "Moustafa Elsisy", "Laura Graves", "Antonina Kolokolova", "Vineel Nagisetty", "Zachary Northcott", "Heather Patey"], "authorids": ["mlb238@mun.ca", "r.bishop@mun.ca", "mmatelsisy@mun.ca", "cmgraves@mun.ca", "kol@mun.ca", "vnagisetty@mun.ca", "zmnorthcott@mun.ca", "hpatey@gmail.com"], "keywords": ["melody generation", "DCGAN", "dilated convolutions"], "TL;DR": "Representing melodies as images with semantic units aligned we can generate them using a DCGAN without any recurrent components.", "abstract": "In this paper we present a method for algorithmic melody generation using a generative adversarial network without recurrent components. Music generation has been successfully done using recurrent neural networks, where the model learns sequence information that can help create authentic sounding melodies.  Here, we use DCGAN architecture with dilated convolutions and towers to capture sequential information as spatial image information, and learn long-range dependencies in fixed-length melody forms such as Irish traditional reel. ", "code": "https://github.com/gan-music-generation/gan_music_generation", "pdf": "/pdf/150f6f66fb5f8ef007444d4fa905636c8fa9bc82.pdf", "paperhash": "billard|nonsequential_melody_generation", "original_pdf": "/attachment/150f6f66fb5f8ef007444d4fa905636c8fa9bc82.pdf", "_bibtex": "@misc{\nbillard2020nonsequential,\ntitle={Non-Sequential Melody Generation},\nauthor={Mitchell Billard and Robert Bishop and Moustafa Elsisy and Laura Graves and Antonina Kolokolova and Vineel Nagisetty and Zachary Northcott and Heather Patey},\nyear={2020},\nurl={https://openreview.net/forum?id=HkePOCNtPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkePOCNtPH", "replyto": "HkePOCNtPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1215/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1215/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575910906955, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1215/Reviewers"], "noninvitees": [], "tcdate": 1570237740633, "tmdate": 1575910906966, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1215/-/Official_Review"}}}, {"id": "S1eEwK92Fr", "original": null, "number": 2, "cdate": 1571756379796, "ddate": null, "tcdate": 1571756379796, "tmdate": 1572972497776, "tddate": null, "forum": "HkePOCNtPH", "replyto": "HkePOCNtPH", "invitation": "ICLR.cc/2020/Conference/Paper1215/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper describes a (DC)GAN architecture for modeling folk song melodies (Irish reels).\nThe main idea of this work is to exploit the rigid form of this type of music --- bar structure and repetition --- to enable 2-dimensional convolutional modeling rather than the purely sequential modeling that is commonly used in recent (melodic) music generation architectures.\nThe ideas in this paper seem sound, though they primarily consist of recombining known techniques for a specific application.\nThe main weaknesses of this paper are in the evaluation (see below), and while I understand that evaluating generate models for creative applications is difficult and fraught territory, I don't think the efforts taken here are sufficiently convincing.\n\n\n\nStrengths:\n\n- The paper is clearly written, and the authors have taken great care to describe the unique structures of the data they are modeling.\n- The proposed architecture seems well motivated, and matches to the structure of the data.\n\n\nWeaknesses:\n\nThere are three components to the evaluation, and each of them are problematic:\n\n- The first evaluation (Fig 2) compares the average Frechet distance between phrases generated by different models, and within the original dataset.  Some brief argument is given for why Frechet is a good choice here, but it still seems quite tenuous: what does this distance intuitively mean in terms of the data?  How should the scale of these distances be interpreted / what's a meaningfully large difference?  How concentrated are these average distances (ie, please show error bars, variance estimates, or some notion of spread)?\n\n- The second evaluation (Fig 3) uses t-SNE to embed the generated melodies into a 2D space to allow visual inspection of the differences between distributions produced by each model.  While this might be a reasonable qualitative gut-check, t-SNE is by no means an appropriate tool for quantitative evaluation.  The authors at least did multiple runs of t-SNE, but this hardly amounts to compelling evidence.  Moreover, combining all data sources into one sample prior to running t-SNE induces dependencies between the point-wise neighbor selection distributions, which seems undesirable if the eventual goal is to determine how similar each model's distribution is to the source data.  A better approach might be to create independent plots for each model's output (with the original data), but I'd generally advise against using t-SNE for this kind of analysis altogether.\n\n- The third evaluation (Fig 4) measures the amount of divergence from the key (D) in terms of note unigrams.  This evaluation is done qualitatively, and the histogram is difficult to read --- it may be easier to read if the octave content was collapsed out to produce pitch classes rather than pitches.  If, however, the goal is to actually measure distance from the target key, one could do this quantitatively by comparing histograms to a probe tone profile (or otherwise constructed unigram note model) to more clearly characterise the behaviors of the various models in question.\n\n\nAt a higher level, there is no error analysis provided for the model, nor any ablation study to measure the impact of the various design choices taken here (eg dilation patterns in Figure 1). \nThe authors seem to argue that these choices are the main contribution of this work, so they should be explicitly evaluated in a controlled setting.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1215/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1215/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Non-Sequential Melody Generation", "authors": ["Mitchell Billard", "Robert Bishop", "Moustafa Elsisy", "Laura Graves", "Antonina Kolokolova", "Vineel Nagisetty", "Zachary Northcott", "Heather Patey"], "authorids": ["mlb238@mun.ca", "r.bishop@mun.ca", "mmatelsisy@mun.ca", "cmgraves@mun.ca", "kol@mun.ca", "vnagisetty@mun.ca", "zmnorthcott@mun.ca", "hpatey@gmail.com"], "keywords": ["melody generation", "DCGAN", "dilated convolutions"], "TL;DR": "Representing melodies as images with semantic units aligned we can generate them using a DCGAN without any recurrent components.", "abstract": "In this paper we present a method for algorithmic melody generation using a generative adversarial network without recurrent components. Music generation has been successfully done using recurrent neural networks, where the model learns sequence information that can help create authentic sounding melodies.  Here, we use DCGAN architecture with dilated convolutions and towers to capture sequential information as spatial image information, and learn long-range dependencies in fixed-length melody forms such as Irish traditional reel. ", "code": "https://github.com/gan-music-generation/gan_music_generation", "pdf": "/pdf/150f6f66fb5f8ef007444d4fa905636c8fa9bc82.pdf", "paperhash": "billard|nonsequential_melody_generation", "original_pdf": "/attachment/150f6f66fb5f8ef007444d4fa905636c8fa9bc82.pdf", "_bibtex": "@misc{\nbillard2020nonsequential,\ntitle={Non-Sequential Melody Generation},\nauthor={Mitchell Billard and Robert Bishop and Moustafa Elsisy and Laura Graves and Antonina Kolokolova and Vineel Nagisetty and Zachary Northcott and Heather Patey},\nyear={2020},\nurl={https://openreview.net/forum?id=HkePOCNtPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkePOCNtPH", "replyto": "HkePOCNtPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1215/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1215/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575910906955, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1215/Reviewers"], "noninvitees": [], "tcdate": 1570237740633, "tmdate": 1575910906966, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1215/-/Official_Review"}}}, {"id": "HygOPyIfqr", "original": null, "number": 3, "cdate": 1572130655636, "ddate": null, "tcdate": 1572130655636, "tmdate": 1572972497728, "tddate": null, "forum": "HkePOCNtPH", "replyto": "HkePOCNtPH", "invitation": "ICLR.cc/2020/Conference/Paper1215/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper is interested in music generation, leveraging a 2D representation of the music data. \n\nThe idea of using a 2D representation is quite good, though here very specific of the considered type of music. The authors might want to discuss related approaches , considering 2D general representations of music [1, 2]. The originality of the proposed approach is to use dilated convolutions to capture the long range dependencies in a GAN framework. \n\nThe paper however looks premature for publication at ICLR, for several reasons:\n\n* The thorough discussion of the dataset might be put in supplementary material. Instead, the reader would like to know whether the authors considered data augmentation of their relatively small dataset (820 tunes). Likewise, the description of the architecture could be put in a supplementary section, or on github for reproducibility.\n\n* The authors did not justify the choice of the specifics of the architecture, such as the number of filters, layers or the presence / absence of batch normalization (part 3.1). The reviewer would like to see how the results vary with respect to those parameters. It would also be interesting to see what the generated images look like before the tanh (part 3.1.2).\n\n* The assessment of the results is hard to interpret; the reasons why the Frechet distance varies depending on the models and the phrases combinations should be discussed.\n\n* The gold standard for evaluating music is based on the human assessment of the generated music (involving naive, advanced and expert people), as done in [3], cited; having a human being evaluate and compare the tunes would help to gain insight into the generation. In the same perspective, it would be appreciated to put the results on a website for the reader to assess the quality of the generated music. \n\n* The contraction of the support (as displayed in Fig. 3) suggests that there might be some mode dropping with the GAN; this should be studied in depth. \n\n* The authors consider Magenta and FolkRNN as baselines; the reviewer suggests to also consider e.g. [4]  (although not not exploiting the specifics of the style), to comparatively assess the proposed approach.\n\n[1] \"Onsets and Frames: Dual-Objective Piano Transcription\", Hawthorne et al., 2017.\n[2] \"TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer\", Huang et al., 2018.\n[3] \"DeepBach: a Steerable Model for Bach Chorales Generation\", Hadjeres et al., 2017\n[4] \"Music Transformer\", Huang et al., 2018."}, "signatures": ["ICLR.cc/2020/Conference/Paper1215/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1215/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Non-Sequential Melody Generation", "authors": ["Mitchell Billard", "Robert Bishop", "Moustafa Elsisy", "Laura Graves", "Antonina Kolokolova", "Vineel Nagisetty", "Zachary Northcott", "Heather Patey"], "authorids": ["mlb238@mun.ca", "r.bishop@mun.ca", "mmatelsisy@mun.ca", "cmgraves@mun.ca", "kol@mun.ca", "vnagisetty@mun.ca", "zmnorthcott@mun.ca", "hpatey@gmail.com"], "keywords": ["melody generation", "DCGAN", "dilated convolutions"], "TL;DR": "Representing melodies as images with semantic units aligned we can generate them using a DCGAN without any recurrent components.", "abstract": "In this paper we present a method for algorithmic melody generation using a generative adversarial network without recurrent components. Music generation has been successfully done using recurrent neural networks, where the model learns sequence information that can help create authentic sounding melodies.  Here, we use DCGAN architecture with dilated convolutions and towers to capture sequential information as spatial image information, and learn long-range dependencies in fixed-length melody forms such as Irish traditional reel. ", "code": "https://github.com/gan-music-generation/gan_music_generation", "pdf": "/pdf/150f6f66fb5f8ef007444d4fa905636c8fa9bc82.pdf", "paperhash": "billard|nonsequential_melody_generation", "original_pdf": "/attachment/150f6f66fb5f8ef007444d4fa905636c8fa9bc82.pdf", "_bibtex": "@misc{\nbillard2020nonsequential,\ntitle={Non-Sequential Melody Generation},\nauthor={Mitchell Billard and Robert Bishop and Moustafa Elsisy and Laura Graves and Antonina Kolokolova and Vineel Nagisetty and Zachary Northcott and Heather Patey},\nyear={2020},\nurl={https://openreview.net/forum?id=HkePOCNtPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkePOCNtPH", "replyto": "HkePOCNtPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1215/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1215/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575910906955, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1215/Reviewers"], "noninvitees": [], "tcdate": 1570237740633, "tmdate": 1575910906966, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1215/-/Official_Review"}}}], "count": 8}