{"notes": [{"id": "HJlRFlHFPS", "original": "rklGD0gtDH", "number": 2454, "cdate": 1569439878112, "ddate": null, "tcdate": 1569439878112, "tmdate": 1577168283007, "tddate": null, "forum": "HJlRFlHFPS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Unsupervised Distillation of Syntactic Information from Contextualized Word Representations", "authors": ["Shauli Ravfogel", "Yanai Elazar", "Jacob Goldberger", "Yoav Goldberg"], "authorids": ["shauli.ravfogel@gmail.com", "yanaiela@gmail.com", "jacob.goldberger@biu.ac.il", "yogo@cs.biu.ac.il"], "keywords": ["dismantlement", "contextualized word representations", "language models", "representation learning"], "TL;DR": "We distill language models representations for syntax by unsupervised metric learning", "abstract": "Contextualized word representations, such as ELMo and BERT, were shown to perform well on a various of semantic and structural (syntactic) task. In this work, we tackle the task of unsupervised disentanglement between semantics and structure in neural language representations: we aim to learn a transformation of the contextualized vectors, that discards the lexical semantics, but keeps the structural information. To this end, we automatically generate groups of sentences which are structurally similar but semantically different, and use metric-learning approach to learn a transformation that emphasizes the structural component that is encoded in the vectors. We demonstrate that our transformation clusters vectors in space by structural properties, rather than by lexical semantics. Finally, we demonstrate the utility of our distilled representations by showing that they outperform the original contextualized representations in few-shot parsing setting.", "pdf": "/pdf/8cf5ded7ea1b4017eac4e64c2f85195738eb726b.pdf", "code": "https://drive.google.com/file/d/1tGoYmNCOSTgE7T5RjRv_bV7o3JUD160t/view", "paperhash": "ravfogel|unsupervised_distillation_of_syntactic_information_from_contextualized_word_representations", "original_pdf": "/attachment/231581bc4b9cde5ab2e00c4c8ef1aa387a03104c.pdf", "_bibtex": "@misc{\nravfogel2020unsupervised,\ntitle={Unsupervised Distillation of Syntactic Information from Contextualized Word Representations},\nauthor={Shauli Ravfogel and Yanai Elazar and Jacob Goldberger and Yoav Goldberg},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlRFlHFPS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 17, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "IWMijNANQw", "original": null, "number": 1, "cdate": 1576798749507, "ddate": null, "tcdate": 1576798749507, "tmdate": 1576800886399, "tddate": null, "forum": "HJlRFlHFPS", "replyto": "HJlRFlHFPS", "invitation": "ICLR.cc/2020/Conference/Paper2454/-/Decision", "content": {"decision": "Reject", "comment": "This paper aims to disentangle semantics and syntax inside of popular contextualized word embedding models. They use the model to generate sentences which are structurally similar but semantically different. \n\nThis paper generated a lot of discussion. The reviewers do like the method for generating structurally similar sentences, and the triplet loss.  They felt the evaluation methods were clever.  However, one reviewer raised several issues.  First, they thought the idea of syntax had not been well defined. They also thought the evaluation did not support the claims.  The reviewer also argued very hard for the need to compare performance to SOTA models.  The authors argued that beating SOTA is not the goal of their work, rather it is to understand what SOTA models are doing.  The reviewers also argue that nearest neighbors is not a good method for evaluating the syntactic information in the representations.  \n\nI hope all of the comments of the reviewers will help improve the paper as it is revised for a future submission.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Distillation of Syntactic Information from Contextualized Word Representations", "authors": ["Shauli Ravfogel", "Yanai Elazar", "Jacob Goldberger", "Yoav Goldberg"], "authorids": ["shauli.ravfogel@gmail.com", "yanaiela@gmail.com", "jacob.goldberger@biu.ac.il", "yogo@cs.biu.ac.il"], "keywords": ["dismantlement", "contextualized word representations", "language models", "representation learning"], "TL;DR": "We distill language models representations for syntax by unsupervised metric learning", "abstract": "Contextualized word representations, such as ELMo and BERT, were shown to perform well on a various of semantic and structural (syntactic) task. In this work, we tackle the task of unsupervised disentanglement between semantics and structure in neural language representations: we aim to learn a transformation of the contextualized vectors, that discards the lexical semantics, but keeps the structural information. To this end, we automatically generate groups of sentences which are structurally similar but semantically different, and use metric-learning approach to learn a transformation that emphasizes the structural component that is encoded in the vectors. We demonstrate that our transformation clusters vectors in space by structural properties, rather than by lexical semantics. Finally, we demonstrate the utility of our distilled representations by showing that they outperform the original contextualized representations in few-shot parsing setting.", "pdf": "/pdf/8cf5ded7ea1b4017eac4e64c2f85195738eb726b.pdf", "code": "https://drive.google.com/file/d/1tGoYmNCOSTgE7T5RjRv_bV7o3JUD160t/view", "paperhash": "ravfogel|unsupervised_distillation_of_syntactic_information_from_contextualized_word_representations", "original_pdf": "/attachment/231581bc4b9cde5ab2e00c4c8ef1aa387a03104c.pdf", "_bibtex": "@misc{\nravfogel2020unsupervised,\ntitle={Unsupervised Distillation of Syntactic Information from Contextualized Word Representations},\nauthor={Shauli Ravfogel and Yanai Elazar and Jacob Goldberger and Yoav Goldberg},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlRFlHFPS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJlRFlHFPS", "replyto": "HJlRFlHFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795723706, "tmdate": 1576800275230, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2454/-/Decision"}}}, {"id": "SJgn9R5hsS", "original": null, "number": 17, "cdate": 1573854867525, "ddate": null, "tcdate": 1573854867525, "tmdate": 1573854867525, "tddate": null, "forum": "HJlRFlHFPS", "replyto": "SyekktO3jB", "invitation": "ICLR.cc/2020/Conference/Paper2454/-/Official_Comment", "content": {"title": "Response to Reply to response", "comment": "Thanks for taking the time to carefully re-read our paper. We regret that you still do not \"get\" what we were trying to achieve in this work. Most notably, we were *not* aiming at beating any other system. That is simply not the intention of the works. Our intention was to distill the structural representation encoded in contextualized vectors, in an unsupervised manner. That is, to produce a representation that captures as much as possible of the structure of the sentence and as little as possible of its lexical semantics. \n\nOur experiments are intended to measure how well this goal (to preserve structural properties) was achieved. As no other work that we are aware of tackled this goal before, the claims of \"unfair comparison\" (points 3a,b,c) seem irrelevant. \n\nResponses to specific comments:\n2) as we are not trying to beat anyone, we also see no reason to explore hyper-parameters. We chose an initial setup (indeed, heuristically) that worked fine for our purposes, and left it at that.\n3a) while we do not see it as a baseline, we did agree that the delexicalized setup is interesting. As we wrote in our response, we will perform these experiments for the final version of the paper (we didn't have time to properly do this during the response period, as the author responsible for the parsing experiments was travelling. But this will happen for camera ready).\n3b) we are not trying to beat ELMo (or BERT), we are trying to figure out what is captured by them.\n3c) The 1M wikipedia sentences are not weak supervision, they are used for evaluation: we perform clustering and search for nearest neighbours in over this space. While this setup is not standard in the literature, our goal is also non standard. We are not trying to \"win\" a parsing context, but to distill syntactic knowledge from a pre-trained LM.  \n3c.2) We did have 150,000 sentences that were POS-tagged by spaCy and used for deriving the training instances. As mentioned in our response to other reviewers, after submission we also experimented with a version that did not use the POS-tag information, getting similar results.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2454/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2454/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Distillation of Syntactic Information from Contextualized Word Representations", "authors": ["Shauli Ravfogel", "Yanai Elazar", "Jacob Goldberger", "Yoav Goldberg"], "authorids": ["shauli.ravfogel@gmail.com", "yanaiela@gmail.com", "jacob.goldberger@biu.ac.il", "yogo@cs.biu.ac.il"], "keywords": ["dismantlement", "contextualized word representations", "language models", "representation learning"], "TL;DR": "We distill language models representations for syntax by unsupervised metric learning", "abstract": "Contextualized word representations, such as ELMo and BERT, were shown to perform well on a various of semantic and structural (syntactic) task. In this work, we tackle the task of unsupervised disentanglement between semantics and structure in neural language representations: we aim to learn a transformation of the contextualized vectors, that discards the lexical semantics, but keeps the structural information. To this end, we automatically generate groups of sentences which are structurally similar but semantically different, and use metric-learning approach to learn a transformation that emphasizes the structural component that is encoded in the vectors. We demonstrate that our transformation clusters vectors in space by structural properties, rather than by lexical semantics. Finally, we demonstrate the utility of our distilled representations by showing that they outperform the original contextualized representations in few-shot parsing setting.", "pdf": "/pdf/8cf5ded7ea1b4017eac4e64c2f85195738eb726b.pdf", "code": "https://drive.google.com/file/d/1tGoYmNCOSTgE7T5RjRv_bV7o3JUD160t/view", "paperhash": "ravfogel|unsupervised_distillation_of_syntactic_information_from_contextualized_word_representations", "original_pdf": "/attachment/231581bc4b9cde5ab2e00c4c8ef1aa387a03104c.pdf", "_bibtex": "@misc{\nravfogel2020unsupervised,\ntitle={Unsupervised Distillation of Syntactic Information from Contextualized Word Representations},\nauthor={Shauli Ravfogel and Yanai Elazar and Jacob Goldberger and Yoav Goldberg},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlRFlHFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlRFlHFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2454/Authors", "ICLR.cc/2020/Conference/Paper2454/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2454/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2454/Reviewers", "ICLR.cc/2020/Conference/Paper2454/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2454/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2454/Authors|ICLR.cc/2020/Conference/Paper2454/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141110, "tmdate": 1576860534258, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2454/Authors", "ICLR.cc/2020/Conference/Paper2454/Reviewers", "ICLR.cc/2020/Conference/Paper2454/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2454/-/Official_Comment"}}}, {"id": "SyekktO3jB", "original": null, "number": 16, "cdate": 1573845206700, "ddate": null, "tcdate": 1573845206700, "tmdate": 1573845206700, "tddate": null, "forum": "HJlRFlHFPS", "replyto": "BJesp3WXsB", "invitation": "ICLR.cc/2020/Conference/Paper2454/-/Official_Comment", "content": {"title": "Reply to response", "comment": "Thank you for responding. After a second *very close* reading of the updated paper and the authors' reply, I maintain that this paper is far from ICLR standard and will keep my score at 1.\n\n1. The authors propose to distill syntactic knowledge from the contextualized representation. However, the authors do not formalize the notion of \"syntactic knowledge,\" and it is unclear what exactly they hope to disentangle from these representations.\n2. The authors approached this task by generating \"syntactically similar\" (also vaguely defined) sentence pairs using heuristics on BERT representations (Section 3.1). Given that this process is heuristic-drive, one would expect an analysis of the hyperparameters selected to achieve the goal. However, the authors performed no such analysis, and the hyperparameters of this generation process (k=6, top-30, ...) appear to be selected at random.\n3. The authors fail to convince me that their method has any practical utility (Section 4.3):\n    a. Lack of a standard baseline. The authors do not address my concerns regarding the delexicalized parser baseline in the updated paper.\n    b. Unfair comparisons. In Figure 4, the author's proposed method \"Syntax\" uses BERT during training (for generating sentence pairs). It is unfair that their baselines only have access to ELMo embeddings. A standard fine-tuned BERT should be the minimum comparison.\n    c. Lack of standard datasets + automatically generated golden labels. The authors did not perform experiments on standard parsing datasets, and they fail to describe their parsing corpus in detail. According to Section 4 [Corpus], they used off-the-shelf spaCy parser to generate golden labels for 1M Wikipedia sentences. This practice is non-existent in literature. If weak-supervision (spaCy's output) is taken as golden labels, the authors should at least provide detailed statistics on their data, as well human-evaluation of the quality of those labels."}, "signatures": ["ICLR.cc/2020/Conference/Paper2454/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2454/AnonReviewer4", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Distillation of Syntactic Information from Contextualized Word Representations", "authors": ["Shauli Ravfogel", "Yanai Elazar", "Jacob Goldberger", "Yoav Goldberg"], "authorids": ["shauli.ravfogel@gmail.com", "yanaiela@gmail.com", "jacob.goldberger@biu.ac.il", "yogo@cs.biu.ac.il"], "keywords": ["dismantlement", "contextualized word representations", "language models", "representation learning"], "TL;DR": "We distill language models representations for syntax by unsupervised metric learning", "abstract": "Contextualized word representations, such as ELMo and BERT, were shown to perform well on a various of semantic and structural (syntactic) task. In this work, we tackle the task of unsupervised disentanglement between semantics and structure in neural language representations: we aim to learn a transformation of the contextualized vectors, that discards the lexical semantics, but keeps the structural information. To this end, we automatically generate groups of sentences which are structurally similar but semantically different, and use metric-learning approach to learn a transformation that emphasizes the structural component that is encoded in the vectors. We demonstrate that our transformation clusters vectors in space by structural properties, rather than by lexical semantics. Finally, we demonstrate the utility of our distilled representations by showing that they outperform the original contextualized representations in few-shot parsing setting.", "pdf": "/pdf/8cf5ded7ea1b4017eac4e64c2f85195738eb726b.pdf", "code": "https://drive.google.com/file/d/1tGoYmNCOSTgE7T5RjRv_bV7o3JUD160t/view", "paperhash": "ravfogel|unsupervised_distillation_of_syntactic_information_from_contextualized_word_representations", "original_pdf": "/attachment/231581bc4b9cde5ab2e00c4c8ef1aa387a03104c.pdf", "_bibtex": "@misc{\nravfogel2020unsupervised,\ntitle={Unsupervised Distillation of Syntactic Information from Contextualized Word Representations},\nauthor={Shauli Ravfogel and Yanai Elazar and Jacob Goldberger and Yoav Goldberg},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlRFlHFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlRFlHFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2454/Authors", "ICLR.cc/2020/Conference/Paper2454/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2454/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2454/Reviewers", "ICLR.cc/2020/Conference/Paper2454/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2454/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2454/Authors|ICLR.cc/2020/Conference/Paper2454/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141110, "tmdate": 1576860534258, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2454/Authors", "ICLR.cc/2020/Conference/Paper2454/Reviewers", "ICLR.cc/2020/Conference/Paper2454/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2454/-/Official_Comment"}}}, {"id": "SJgOLrZhiB", "original": null, "number": 15, "cdate": 1573815631591, "ddate": null, "tcdate": 1573815631591, "tmdate": 1573815631591, "tddate": null, "forum": "HJlRFlHFPS", "replyto": "HyxJmJuUjB", "invitation": "ICLR.cc/2020/Conference/Paper2454/-/Official_Comment", "content": {"title": "Response", "comment": "Thank you for the meaningful comments!"}, "signatures": ["ICLR.cc/2020/Conference/Paper2454/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2454/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Distillation of Syntactic Information from Contextualized Word Representations", "authors": ["Shauli Ravfogel", "Yanai Elazar", "Jacob Goldberger", "Yoav Goldberg"], "authorids": ["shauli.ravfogel@gmail.com", "yanaiela@gmail.com", "jacob.goldberger@biu.ac.il", "yogo@cs.biu.ac.il"], "keywords": ["dismantlement", "contextualized word representations", "language models", "representation learning"], "TL;DR": "We distill language models representations for syntax by unsupervised metric learning", "abstract": "Contextualized word representations, such as ELMo and BERT, were shown to perform well on a various of semantic and structural (syntactic) task. In this work, we tackle the task of unsupervised disentanglement between semantics and structure in neural language representations: we aim to learn a transformation of the contextualized vectors, that discards the lexical semantics, but keeps the structural information. To this end, we automatically generate groups of sentences which are structurally similar but semantically different, and use metric-learning approach to learn a transformation that emphasizes the structural component that is encoded in the vectors. We demonstrate that our transformation clusters vectors in space by structural properties, rather than by lexical semantics. Finally, we demonstrate the utility of our distilled representations by showing that they outperform the original contextualized representations in few-shot parsing setting.", "pdf": "/pdf/8cf5ded7ea1b4017eac4e64c2f85195738eb726b.pdf", "code": "https://drive.google.com/file/d/1tGoYmNCOSTgE7T5RjRv_bV7o3JUD160t/view", "paperhash": "ravfogel|unsupervised_distillation_of_syntactic_information_from_contextualized_word_representations", "original_pdf": "/attachment/231581bc4b9cde5ab2e00c4c8ef1aa387a03104c.pdf", "_bibtex": "@misc{\nravfogel2020unsupervised,\ntitle={Unsupervised Distillation of Syntactic Information from Contextualized Word Representations},\nauthor={Shauli Ravfogel and Yanai Elazar and Jacob Goldberger and Yoav Goldberg},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlRFlHFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlRFlHFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2454/Authors", "ICLR.cc/2020/Conference/Paper2454/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2454/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2454/Reviewers", "ICLR.cc/2020/Conference/Paper2454/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2454/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2454/Authors|ICLR.cc/2020/Conference/Paper2454/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141110, "tmdate": 1576860534258, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2454/Authors", "ICLR.cc/2020/Conference/Paper2454/Reviewers", "ICLR.cc/2020/Conference/Paper2454/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2454/-/Official_Comment"}}}, {"id": "rJg-gS-2iS", "original": null, "number": 14, "cdate": 1573815528554, "ddate": null, "tcdate": 1573815528554, "tmdate": 1573815528554, "tddate": null, "forum": "HJlRFlHFPS", "replyto": "H1lgdg3siH", "invitation": "ICLR.cc/2020/Conference/Paper2454/-/Official_Comment", "content": {"title": "Response", "comment": "Thank you for your constructive review and appreciation of our work!"}, "signatures": ["ICLR.cc/2020/Conference/Paper2454/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2454/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Distillation of Syntactic Information from Contextualized Word Representations", "authors": ["Shauli Ravfogel", "Yanai Elazar", "Jacob Goldberger", "Yoav Goldberg"], "authorids": ["shauli.ravfogel@gmail.com", "yanaiela@gmail.com", "jacob.goldberger@biu.ac.il", "yogo@cs.biu.ac.il"], "keywords": ["dismantlement", "contextualized word representations", "language models", "representation learning"], "TL;DR": "We distill language models representations for syntax by unsupervised metric learning", "abstract": "Contextualized word representations, such as ELMo and BERT, were shown to perform well on a various of semantic and structural (syntactic) task. In this work, we tackle the task of unsupervised disentanglement between semantics and structure in neural language representations: we aim to learn a transformation of the contextualized vectors, that discards the lexical semantics, but keeps the structural information. To this end, we automatically generate groups of sentences which are structurally similar but semantically different, and use metric-learning approach to learn a transformation that emphasizes the structural component that is encoded in the vectors. We demonstrate that our transformation clusters vectors in space by structural properties, rather than by lexical semantics. Finally, we demonstrate the utility of our distilled representations by showing that they outperform the original contextualized representations in few-shot parsing setting.", "pdf": "/pdf/8cf5ded7ea1b4017eac4e64c2f85195738eb726b.pdf", "code": "https://drive.google.com/file/d/1tGoYmNCOSTgE7T5RjRv_bV7o3JUD160t/view", "paperhash": "ravfogel|unsupervised_distillation_of_syntactic_information_from_contextualized_word_representations", "original_pdf": "/attachment/231581bc4b9cde5ab2e00c4c8ef1aa387a03104c.pdf", "_bibtex": "@misc{\nravfogel2020unsupervised,\ntitle={Unsupervised Distillation of Syntactic Information from Contextualized Word Representations},\nauthor={Shauli Ravfogel and Yanai Elazar and Jacob Goldberger and Yoav Goldberg},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlRFlHFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlRFlHFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2454/Authors", "ICLR.cc/2020/Conference/Paper2454/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2454/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2454/Reviewers", "ICLR.cc/2020/Conference/Paper2454/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2454/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2454/Authors|ICLR.cc/2020/Conference/Paper2454/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141110, "tmdate": 1576860534258, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2454/Authors", "ICLR.cc/2020/Conference/Paper2454/Reviewers", "ICLR.cc/2020/Conference/Paper2454/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2454/-/Official_Comment"}}}, {"id": "H1lgdg3siH", "original": null, "number": 13, "cdate": 1573793895945, "ddate": null, "tcdate": 1573793895945, "tmdate": 1573793895945, "tddate": null, "forum": "HJlRFlHFPS", "replyto": "rygVr0-7oS", "invitation": "ICLR.cc/2020/Conference/Paper2454/-/Official_Comment", "content": {"title": "AnonReviewer3 Response", "comment": "I am satisfied with the responses to my review and the others. I am raising my rating to 8: Accept."}, "signatures": ["ICLR.cc/2020/Conference/Paper2454/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2454/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Distillation of Syntactic Information from Contextualized Word Representations", "authors": ["Shauli Ravfogel", "Yanai Elazar", "Jacob Goldberger", "Yoav Goldberg"], "authorids": ["shauli.ravfogel@gmail.com", "yanaiela@gmail.com", "jacob.goldberger@biu.ac.il", "yogo@cs.biu.ac.il"], "keywords": ["dismantlement", "contextualized word representations", "language models", "representation learning"], "TL;DR": "We distill language models representations for syntax by unsupervised metric learning", "abstract": "Contextualized word representations, such as ELMo and BERT, were shown to perform well on a various of semantic and structural (syntactic) task. In this work, we tackle the task of unsupervised disentanglement between semantics and structure in neural language representations: we aim to learn a transformation of the contextualized vectors, that discards the lexical semantics, but keeps the structural information. To this end, we automatically generate groups of sentences which are structurally similar but semantically different, and use metric-learning approach to learn a transformation that emphasizes the structural component that is encoded in the vectors. We demonstrate that our transformation clusters vectors in space by structural properties, rather than by lexical semantics. Finally, we demonstrate the utility of our distilled representations by showing that they outperform the original contextualized representations in few-shot parsing setting.", "pdf": "/pdf/8cf5ded7ea1b4017eac4e64c2f85195738eb726b.pdf", "code": "https://drive.google.com/file/d/1tGoYmNCOSTgE7T5RjRv_bV7o3JUD160t/view", "paperhash": "ravfogel|unsupervised_distillation_of_syntactic_information_from_contextualized_word_representations", "original_pdf": "/attachment/231581bc4b9cde5ab2e00c4c8ef1aa387a03104c.pdf", "_bibtex": "@misc{\nravfogel2020unsupervised,\ntitle={Unsupervised Distillation of Syntactic Information from Contextualized Word Representations},\nauthor={Shauli Ravfogel and Yanai Elazar and Jacob Goldberger and Yoav Goldberg},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlRFlHFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlRFlHFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2454/Authors", "ICLR.cc/2020/Conference/Paper2454/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2454/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2454/Reviewers", "ICLR.cc/2020/Conference/Paper2454/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2454/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2454/Authors|ICLR.cc/2020/Conference/Paper2454/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141110, "tmdate": 1576860534258, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2454/Authors", "ICLR.cc/2020/Conference/Paper2454/Reviewers", "ICLR.cc/2020/Conference/Paper2454/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2454/-/Official_Comment"}}}, {"id": "HyxJmJuUjB", "original": null, "number": 10, "cdate": 1573449495017, "ddate": null, "tcdate": 1573449495017, "tmdate": 1573449495017, "tddate": null, "forum": "HJlRFlHFPS", "replyto": "rJxPTT-IYH", "invitation": "ICLR.cc/2020/Conference/Paper2454/-/Official_Comment", "content": {"title": "Thank you for your detailed response", "comment": "I appreciate the detailed response to my review. I think the revised paper is improved in terms of background and motivation, and more complete experiments. It's also good to see the quantitative clustering purity results. \nWhile I don't think getting very high parsing results is a must for this work, I agree with reviewer 1 that comparing with a POS-based baseline is in order. \nI hope to see the paper accepted and at this point will keep my current evaluation. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2454/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2454/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Distillation of Syntactic Information from Contextualized Word Representations", "authors": ["Shauli Ravfogel", "Yanai Elazar", "Jacob Goldberger", "Yoav Goldberg"], "authorids": ["shauli.ravfogel@gmail.com", "yanaiela@gmail.com", "jacob.goldberger@biu.ac.il", "yogo@cs.biu.ac.il"], "keywords": ["dismantlement", "contextualized word representations", "language models", "representation learning"], "TL;DR": "We distill language models representations for syntax by unsupervised metric learning", "abstract": "Contextualized word representations, such as ELMo and BERT, were shown to perform well on a various of semantic and structural (syntactic) task. In this work, we tackle the task of unsupervised disentanglement between semantics and structure in neural language representations: we aim to learn a transformation of the contextualized vectors, that discards the lexical semantics, but keeps the structural information. To this end, we automatically generate groups of sentences which are structurally similar but semantically different, and use metric-learning approach to learn a transformation that emphasizes the structural component that is encoded in the vectors. We demonstrate that our transformation clusters vectors in space by structural properties, rather than by lexical semantics. Finally, we demonstrate the utility of our distilled representations by showing that they outperform the original contextualized representations in few-shot parsing setting.", "pdf": "/pdf/8cf5ded7ea1b4017eac4e64c2f85195738eb726b.pdf", "code": "https://drive.google.com/file/d/1tGoYmNCOSTgE7T5RjRv_bV7o3JUD160t/view", "paperhash": "ravfogel|unsupervised_distillation_of_syntactic_information_from_contextualized_word_representations", "original_pdf": "/attachment/231581bc4b9cde5ab2e00c4c8ef1aa387a03104c.pdf", "_bibtex": "@misc{\nravfogel2020unsupervised,\ntitle={Unsupervised Distillation of Syntactic Information from Contextualized Word Representations},\nauthor={Shauli Ravfogel and Yanai Elazar and Jacob Goldberger and Yoav Goldberg},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlRFlHFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlRFlHFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2454/Authors", "ICLR.cc/2020/Conference/Paper2454/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2454/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2454/Reviewers", "ICLR.cc/2020/Conference/Paper2454/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2454/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2454/Authors|ICLR.cc/2020/Conference/Paper2454/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141110, "tmdate": 1576860534258, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2454/Authors", "ICLR.cc/2020/Conference/Paper2454/Reviewers", "ICLR.cc/2020/Conference/Paper2454/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2454/-/Official_Comment"}}}, {"id": "S1xa5Zz7sH", "original": null, "number": 9, "cdate": 1573228948637, "ddate": null, "tcdate": 1573228948637, "tmdate": 1573228948637, "tddate": null, "forum": "HJlRFlHFPS", "replyto": "rJxPTT-IYH", "invitation": "ICLR.cc/2020/Conference/Paper2454/-/Official_Comment", "content": {"title": "Response to \"Main Comments\"", "comment": "# Responses to \u201cMain comments\u201d:\n\n1. Thank you for pointing this out, we strengthened the introduction on the motivation. As we see it, disentanglement is interesting for several reasons. From a purely scientific view, once disentanglement is achieved, one can better control for confounding factors and analyze the knowledge the model acquires, e.g.  attributing the predictions of the model to one factor of variation while controlling for the other. In addition to explaining model predictions, such disentanglement can be useful for the comparison of the representations the model acquires to linguistic knowledge, e.g. by showing the model is or is not able to learn specific linguistic abstractions, or by contrasting the way certain phenomena are represented in the model with their representation in syntactic schemes defined by linguists.The latter option can be especially illuminating, as the right way to describe certain syntactic phenomena is still in dispute among linguistics.\n\nFrom a more practical perspective, disentanglement can be a first step toward controlled generation/paraphrasing that considers only aspects of the structure, akin to the style-transfer works in computer vision. For example, one can imagine creating variants of a sentence that ignore syntax while preserving semantics, or that mimic the syntactic structures favoured by an authors. But we leave this to future research.\n\n2. Thanks for pointing out to this, we tried to make the argument clearer this time. \n\n3. We have considered alternatives, such as element-wise absolute value of the difference, element-wise multiplication, and an average. We did not observe qualitative differences between the different ways to represent pairs, and chose the difference because its simplicity and because of the known literature on arithmetic of word vectors, to which you have referred. \n\n4. We experimented also with the last BERT layer alone, which performed worse. We added the full BERT results on the closest-word evaluation in the appendix. We agree that techniques such as a learned weighted average would probably yield better results and outperform ELMO. We did not try such methods due to time constraints. However, as we see the main contribution of the paper in the proposed method and in the demonstration of a proof-of-concept for unsupervised distillation of syntax, we think that the exact scores we get on, e.g., the parsing tasks, are relatively less important. \n\n5. We expanded the existing background material into a separate related work section, in order to better situate this research with respect to previous works. \n\n6. We added the exact numbers in the parsing experiments in an appendix. We agree that the differences are relatively small. However, with enough data, we cannot expect our representation to outperform ELMO, as our extraction process does not change the ELMO encoder but only transforms its output into a lower dimension, potentially discarding information. With enough direct supervision, the parser can probably directly extract the relevant information from ELMO vectors. We see the relatively significant LAS differences in very low data regime (50-100) as support for this hypothesis. We are not sure why the differences in the unlabeled setting are significantly smaller.  \n\n7. Thanks for pointing this out. We focus on lexical semantics, and we make it more explicit in the text.  \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2454/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2454/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Distillation of Syntactic Information from Contextualized Word Representations", "authors": ["Shauli Ravfogel", "Yanai Elazar", "Jacob Goldberger", "Yoav Goldberg"], "authorids": ["shauli.ravfogel@gmail.com", "yanaiela@gmail.com", "jacob.goldberger@biu.ac.il", "yogo@cs.biu.ac.il"], "keywords": ["dismantlement", "contextualized word representations", "language models", "representation learning"], "TL;DR": "We distill language models representations for syntax by unsupervised metric learning", "abstract": "Contextualized word representations, such as ELMo and BERT, were shown to perform well on a various of semantic and structural (syntactic) task. In this work, we tackle the task of unsupervised disentanglement between semantics and structure in neural language representations: we aim to learn a transformation of the contextualized vectors, that discards the lexical semantics, but keeps the structural information. To this end, we automatically generate groups of sentences which are structurally similar but semantically different, and use metric-learning approach to learn a transformation that emphasizes the structural component that is encoded in the vectors. We demonstrate that our transformation clusters vectors in space by structural properties, rather than by lexical semantics. Finally, we demonstrate the utility of our distilled representations by showing that they outperform the original contextualized representations in few-shot parsing setting.", "pdf": "/pdf/8cf5ded7ea1b4017eac4e64c2f85195738eb726b.pdf", "code": "https://drive.google.com/file/d/1tGoYmNCOSTgE7T5RjRv_bV7o3JUD160t/view", "paperhash": "ravfogel|unsupervised_distillation_of_syntactic_information_from_contextualized_word_representations", "original_pdf": "/attachment/231581bc4b9cde5ab2e00c4c8ef1aa387a03104c.pdf", "_bibtex": "@misc{\nravfogel2020unsupervised,\ntitle={Unsupervised Distillation of Syntactic Information from Contextualized Word Representations},\nauthor={Shauli Ravfogel and Yanai Elazar and Jacob Goldberger and Yoav Goldberg},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlRFlHFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlRFlHFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2454/Authors", "ICLR.cc/2020/Conference/Paper2454/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2454/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2454/Reviewers", "ICLR.cc/2020/Conference/Paper2454/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2454/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2454/Authors|ICLR.cc/2020/Conference/Paper2454/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141110, "tmdate": 1576860534258, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2454/Authors", "ICLR.cc/2020/Conference/Paper2454/Reviewers", "ICLR.cc/2020/Conference/Paper2454/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2454/-/Official_Comment"}}}, {"id": "rJg2I-fQjr", "original": null, "number": 8, "cdate": 1573228884220, "ddate": null, "tcdate": 1573228884220, "tmdate": 1573228884220, "tddate": null, "forum": "HJlRFlHFPS", "replyto": "rJxPTT-IYH", "invitation": "ICLR.cc/2020/Conference/Paper2454/-/Official_Comment", "content": {"title": "Response to \"Other Comments\"", "comment": "# Responses to \u201cOther comments\u201d:\n\n1. We agree that for many purposes is it beneficial not to discard the semantic information altogether. Ideally, we would want to allow \u201ctuning\u201d the representations on the semantics-syntax axis, changing the saliency of one factor or another according to the task at hand. However, as a first-order approximation, in this work we did aim to discard lexical semantics. The triplet objective does not explicitly do that, although insofar as the the \u201cequivalent\u201d sentences are indeed lexically diverse, we think that the hard-negatives mining should discard lexical semantics (as if vectors are embedded in space according to lexical information, the hard negatives would be semantically similar to the anchor vector, increasing the loss). But indeed, in practice we do not succeed in discarding all semantic information.\n\n2. You are right, and we added a short discussion to the paper. We indeed observe that our method tends to conflate local syntactic distinctions like adjective-noun differences in environments where both can occur, in favor of more global distinctions (in this case, noun modifiers vs. other functions).\n \n3. The dimensionality of the transformed vector was chosen according to development set performance. As for the value of K, it is not very important, as in practice we sample only 10 pairs from each group of sentences. This information was indeed missing in the paper. We added a clarification. \n\n4. This is an important point, which we will make more salient in the text: we did not finetune the contextualized representations. We limited ourselves to extracting information that is already encoded in the vectors. This is consistent with our focus on disentangling existing representations rather than merely creating strong syntactic models.\n\n5. The current description in the paper is indeed unclear. We will rephrase those parts. The number of evaluation sentences in section 2.2 refers to the training of the model with the triplet loss: we did not use them for the closest-word query. The 1 million sentences that are mentioned in section 3 are another (different) section of wikipedia, which we used for the closest-word test. From this set we sampled 400,000 sentences as mentioned in 3.2, and evaluated the percentage of (query, value) pair that share the different properties. \n\n6. Thanks for the suggestion, we now added the purity measure to the section that presents the tsne results.\n\n7. We added this comparison. \n\n8. We added the closest-word results for BERT in an appendix.\n\n9. Thanks for the references! We will definitely expand the discussion on related work. We will look further into the style transfer literature. If you are aware of references that are especially relevant, we would appreciate it if you share them. \n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2454/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2454/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Distillation of Syntactic Information from Contextualized Word Representations", "authors": ["Shauli Ravfogel", "Yanai Elazar", "Jacob Goldberger", "Yoav Goldberg"], "authorids": ["shauli.ravfogel@gmail.com", "yanaiela@gmail.com", "jacob.goldberger@biu.ac.il", "yogo@cs.biu.ac.il"], "keywords": ["dismantlement", "contextualized word representations", "language models", "representation learning"], "TL;DR": "We distill language models representations for syntax by unsupervised metric learning", "abstract": "Contextualized word representations, such as ELMo and BERT, were shown to perform well on a various of semantic and structural (syntactic) task. In this work, we tackle the task of unsupervised disentanglement between semantics and structure in neural language representations: we aim to learn a transformation of the contextualized vectors, that discards the lexical semantics, but keeps the structural information. To this end, we automatically generate groups of sentences which are structurally similar but semantically different, and use metric-learning approach to learn a transformation that emphasizes the structural component that is encoded in the vectors. We demonstrate that our transformation clusters vectors in space by structural properties, rather than by lexical semantics. Finally, we demonstrate the utility of our distilled representations by showing that they outperform the original contextualized representations in few-shot parsing setting.", "pdf": "/pdf/8cf5ded7ea1b4017eac4e64c2f85195738eb726b.pdf", "code": "https://drive.google.com/file/d/1tGoYmNCOSTgE7T5RjRv_bV7o3JUD160t/view", "paperhash": "ravfogel|unsupervised_distillation_of_syntactic_information_from_contextualized_word_representations", "original_pdf": "/attachment/231581bc4b9cde5ab2e00c4c8ef1aa387a03104c.pdf", "_bibtex": "@misc{\nravfogel2020unsupervised,\ntitle={Unsupervised Distillation of Syntactic Information from Contextualized Word Representations},\nauthor={Shauli Ravfogel and Yanai Elazar and Jacob Goldberger and Yoav Goldberg},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlRFlHFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlRFlHFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2454/Authors", "ICLR.cc/2020/Conference/Paper2454/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2454/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2454/Reviewers", "ICLR.cc/2020/Conference/Paper2454/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2454/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2454/Authors|ICLR.cc/2020/Conference/Paper2454/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141110, "tmdate": 1576860534258, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2454/Authors", "ICLR.cc/2020/Conference/Paper2454/Reviewers", "ICLR.cc/2020/Conference/Paper2454/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2454/-/Official_Comment"}}}, {"id": "ryx6n0b7iH", "original": null, "number": 6, "cdate": 1573228212612, "ddate": null, "tcdate": 1573228212612, "tmdate": 1573228212612, "tddate": null, "forum": "HJlRFlHFPS", "replyto": "SygRa5LTKS", "invitation": "ICLR.cc/2020/Conference/Paper2454/-/Official_Comment", "content": {"title": "Response", "comment": "Thank you for you comments! \n\nWe are only aware of the work [1] which demonstrated the existence of a semantics-syntax tradeoff in word vectors that capture different orders of similarity. They projected pre-trained word vectors to different orders by a parameter-free transformation which derives from the similarity matrix, and measured the performance on semantic and syntactic tasks.  We now mention this work in the revised paper. We would appreciate pointers to other works in this direction, if any of the reviewers are aware of them.\n\nThere are additional works which learn from scratch word embeddings that are tailored for syntax, some of them are mentioned by reviewer 2. These works are somewhat less relevant for the current study, as we aim to extract existing information from contextualized representations and make it more salient, rather than learning from scratch representations that capture syntax. Yet, we now note them in the new related work section.\n\n[1] Artetxe, Mikel, et al. \"Uncovering divergent linguistic information in word embeddings with lessons for intrinsic and extrinsic evaluation.\" arXiv preprint arXiv:1809.02094 (2018). APA\t\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2454/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2454/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Distillation of Syntactic Information from Contextualized Word Representations", "authors": ["Shauli Ravfogel", "Yanai Elazar", "Jacob Goldberger", "Yoav Goldberg"], "authorids": ["shauli.ravfogel@gmail.com", "yanaiela@gmail.com", "jacob.goldberger@biu.ac.il", "yogo@cs.biu.ac.il"], "keywords": ["dismantlement", "contextualized word representations", "language models", "representation learning"], "TL;DR": "We distill language models representations for syntax by unsupervised metric learning", "abstract": "Contextualized word representations, such as ELMo and BERT, were shown to perform well on a various of semantic and structural (syntactic) task. In this work, we tackle the task of unsupervised disentanglement between semantics and structure in neural language representations: we aim to learn a transformation of the contextualized vectors, that discards the lexical semantics, but keeps the structural information. To this end, we automatically generate groups of sentences which are structurally similar but semantically different, and use metric-learning approach to learn a transformation that emphasizes the structural component that is encoded in the vectors. We demonstrate that our transformation clusters vectors in space by structural properties, rather than by lexical semantics. Finally, we demonstrate the utility of our distilled representations by showing that they outperform the original contextualized representations in few-shot parsing setting.", "pdf": "/pdf/8cf5ded7ea1b4017eac4e64c2f85195738eb726b.pdf", "code": "https://drive.google.com/file/d/1tGoYmNCOSTgE7T5RjRv_bV7o3JUD160t/view", "paperhash": "ravfogel|unsupervised_distillation_of_syntactic_information_from_contextualized_word_representations", "original_pdf": "/attachment/231581bc4b9cde5ab2e00c4c8ef1aa387a03104c.pdf", "_bibtex": "@misc{\nravfogel2020unsupervised,\ntitle={Unsupervised Distillation of Syntactic Information from Contextualized Word Representations},\nauthor={Shauli Ravfogel and Yanai Elazar and Jacob Goldberger and Yoav Goldberg},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlRFlHFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlRFlHFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2454/Authors", "ICLR.cc/2020/Conference/Paper2454/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2454/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2454/Reviewers", "ICLR.cc/2020/Conference/Paper2454/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2454/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2454/Authors|ICLR.cc/2020/Conference/Paper2454/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141110, "tmdate": 1576860534258, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2454/Authors", "ICLR.cc/2020/Conference/Paper2454/Reviewers", "ICLR.cc/2020/Conference/Paper2454/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2454/-/Official_Comment"}}}, {"id": "rygVr0-7oS", "original": null, "number": 5, "cdate": 1573228092146, "ddate": null, "tcdate": 1573228092146, "tmdate": 1573228092146, "tddate": null, "forum": "HJlRFlHFPS", "replyto": "SklmVR-fqH", "invitation": "ICLR.cc/2020/Conference/Paper2454/-/Official_Comment", "content": {"title": "Response", "comment": "We appreciate your constructive and detailed review!\n\nYour are right in noting that subtle differences in the surface level can mask substantial differences in the deep argument structure of the sentence, and that verbs are particularly sensitive in this respect. This is a limitation of the current approach, which we now acknowledge in the paper. Thanks for pointing this out.  Indeed, in general we can expect the replacement process to yield a grammatical sentence with equivalent structures only to the extent that BERT implicitly encodes the grammatical restrictions that apply to the masked word (i.e., we can only capture raising vs control distinction to the extent BERT-like LM can captures them). While BERT is a powerful LM -- and that is the reason we used it rather than simple POS-based replacement -- it may at times violates some of those restrictions. As you point out, this reasoning behind the substitution process and the premises we made were not clearly stated in the paper, and made it clearer in the revisioned version. However, we note that the average sentences we generate seem grammatical, and do not diverge much from the structure of the original sentence; we therefore think this method does at least approximate our end goal of generating grammatical sentences of the same structure. \nMoreover, we remind that our method attempts to uncover the structural information that is encoded in the neural LMs. Thus, we find it reasonable to not capture structural distinctions that are not reflected in current state-of-the-art neural LMs. \n\nThank you for pointing out to the works on vector-space arithmetic. This was our motivation for representing pairs as the difference between the corresponding word vectors, and we will explicitly mention that in the paper.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2454/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2454/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Distillation of Syntactic Information from Contextualized Word Representations", "authors": ["Shauli Ravfogel", "Yanai Elazar", "Jacob Goldberger", "Yoav Goldberg"], "authorids": ["shauli.ravfogel@gmail.com", "yanaiela@gmail.com", "jacob.goldberger@biu.ac.il", "yogo@cs.biu.ac.il"], "keywords": ["dismantlement", "contextualized word representations", "language models", "representation learning"], "TL;DR": "We distill language models representations for syntax by unsupervised metric learning", "abstract": "Contextualized word representations, such as ELMo and BERT, were shown to perform well on a various of semantic and structural (syntactic) task. In this work, we tackle the task of unsupervised disentanglement between semantics and structure in neural language representations: we aim to learn a transformation of the contextualized vectors, that discards the lexical semantics, but keeps the structural information. To this end, we automatically generate groups of sentences which are structurally similar but semantically different, and use metric-learning approach to learn a transformation that emphasizes the structural component that is encoded in the vectors. We demonstrate that our transformation clusters vectors in space by structural properties, rather than by lexical semantics. Finally, we demonstrate the utility of our distilled representations by showing that they outperform the original contextualized representations in few-shot parsing setting.", "pdf": "/pdf/8cf5ded7ea1b4017eac4e64c2f85195738eb726b.pdf", "code": "https://drive.google.com/file/d/1tGoYmNCOSTgE7T5RjRv_bV7o3JUD160t/view", "paperhash": "ravfogel|unsupervised_distillation_of_syntactic_information_from_contextualized_word_representations", "original_pdf": "/attachment/231581bc4b9cde5ab2e00c4c8ef1aa387a03104c.pdf", "_bibtex": "@misc{\nravfogel2020unsupervised,\ntitle={Unsupervised Distillation of Syntactic Information from Contextualized Word Representations},\nauthor={Shauli Ravfogel and Yanai Elazar and Jacob Goldberger and Yoav Goldberg},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlRFlHFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlRFlHFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2454/Authors", "ICLR.cc/2020/Conference/Paper2454/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2454/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2454/Reviewers", "ICLR.cc/2020/Conference/Paper2454/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2454/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2454/Authors|ICLR.cc/2020/Conference/Paper2454/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141110, "tmdate": 1576860534258, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2454/Authors", "ICLR.cc/2020/Conference/Paper2454/Reviewers", "ICLR.cc/2020/Conference/Paper2454/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2454/-/Official_Comment"}}}, {"id": "BJesp3WXsB", "original": null, "number": 4, "cdate": 1573227715046, "ddate": null, "tcdate": 1573227715046, "tmdate": 1573227715046, "tddate": null, "forum": "HJlRFlHFPS", "replyto": "Byey-Ljz5H", "invitation": "ICLR.cc/2020/Conference/Paper2454/-/Official_Comment", "content": {"title": "Response", "comment": "Thank you for your comments.\n\n>>in the method, the authors construct a dataset where each group of the sentence share similar syntactic structures (having the same POS tag). It seems there that the structural information just means POS tags.\n\nOur method definitely captures more than the POS tag of the words. For example, it clearly differentiates nouns in subject position from nouns in object position from nouns in relative clauses, and also differentiates nouns in passive constructions from those in active voice. This holds also for multiple usages of the same word, e.g the word \"dog\" will get different neighbours in \"the dog barked\" and \"he heard the dog bark\", even though both are nouns. Similarly for verbs, adjectives, and other classes.\n\nAs for the use of POS tags in the process of sentence generation, we agree this injects some syntactic bias to the model. Following the submission, we have performed experiments on datasets that are constructed without this using POS information, and got similar results, suggesting that the POS information is not necessary for our method. We need to work more to get detailed and robust results we can report, but we are in the process of doing so and will include these results in the camera ready version.  \n\n>>what do you mean by structural information without a clear definition?\n\nBy \u201cStructural information\u201d we refer to properties that linguists would identify as \u201csyntax\u201d. We did not want to pre-specify the characteristics of this structural representation (e.g. by identifying it with a certain type of dependency or constituency representation scheme), as many competing frameworks exist, and the \u201cright\u201d ways to represent different phenomena are still in dispute among linguists. As discussed in the paper, defining the problem in an implicit manner (structure A is similar to B and C is similar to D, without specifying what constitutes this similarity) allows us to not rely on any specific annotation scheme, but rather extract the structural representations in an unsupervised way. However, in evaluation, we do compare our representations with linguistic notions of syntax, and show that to a large degree our representations capture those properties, although they were not trained to explicitly achieve this objective.\n \n>>In Figure 4, the authors should compare with delexicalized dependency parsing, which performs pretty well in los-resource setting.\n\nThank you for the suggestion. In this experiment, we have tried to cautionaly use controls: performing PCA on the ELMO vectors, and projecting them to a lower dimension using a learned linear transformation. The rationale behind these controls is to encourage the parser to discard irrelevant information from the ELMO vectors. We agree that a comparison to a POS-based parser is of interest, and can help test the claim we capture information beyond the POS level. We plan to perform this experiment for the camera ready version. \n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2454/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2454/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Distillation of Syntactic Information from Contextualized Word Representations", "authors": ["Shauli Ravfogel", "Yanai Elazar", "Jacob Goldberger", "Yoav Goldberg"], "authorids": ["shauli.ravfogel@gmail.com", "yanaiela@gmail.com", "jacob.goldberger@biu.ac.il", "yogo@cs.biu.ac.il"], "keywords": ["dismantlement", "contextualized word representations", "language models", "representation learning"], "TL;DR": "We distill language models representations for syntax by unsupervised metric learning", "abstract": "Contextualized word representations, such as ELMo and BERT, were shown to perform well on a various of semantic and structural (syntactic) task. In this work, we tackle the task of unsupervised disentanglement between semantics and structure in neural language representations: we aim to learn a transformation of the contextualized vectors, that discards the lexical semantics, but keeps the structural information. To this end, we automatically generate groups of sentences which are structurally similar but semantically different, and use metric-learning approach to learn a transformation that emphasizes the structural component that is encoded in the vectors. We demonstrate that our transformation clusters vectors in space by structural properties, rather than by lexical semantics. Finally, we demonstrate the utility of our distilled representations by showing that they outperform the original contextualized representations in few-shot parsing setting.", "pdf": "/pdf/8cf5ded7ea1b4017eac4e64c2f85195738eb726b.pdf", "code": "https://drive.google.com/file/d/1tGoYmNCOSTgE7T5RjRv_bV7o3JUD160t/view", "paperhash": "ravfogel|unsupervised_distillation_of_syntactic_information_from_contextualized_word_representations", "original_pdf": "/attachment/231581bc4b9cde5ab2e00c4c8ef1aa387a03104c.pdf", "_bibtex": "@misc{\nravfogel2020unsupervised,\ntitle={Unsupervised Distillation of Syntactic Information from Contextualized Word Representations},\nauthor={Shauli Ravfogel and Yanai Elazar and Jacob Goldberger and Yoav Goldberg},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlRFlHFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlRFlHFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2454/Authors", "ICLR.cc/2020/Conference/Paper2454/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2454/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2454/Reviewers", "ICLR.cc/2020/Conference/Paper2454/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2454/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2454/Authors|ICLR.cc/2020/Conference/Paper2454/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141110, "tmdate": 1576860534258, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2454/Authors", "ICLR.cc/2020/Conference/Paper2454/Reviewers", "ICLR.cc/2020/Conference/Paper2454/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2454/-/Official_Comment"}}}, {"id": "SJeeN3WmoS", "original": null, "number": 3, "cdate": 1573227560056, "ddate": null, "tcdate": 1573227560056, "tmdate": 1573227560056, "tddate": null, "forum": "HJlRFlHFPS", "replyto": "HJlRFlHFPS", "invitation": "ICLR.cc/2020/Conference/Paper2454/-/Official_Comment", "content": {"title": "General response.", "comment": "We thank all reviewers for their insightful comments. We updated the paper to account for some of them. Other comments will require more time to properly address, but we are working towards that as well.\n\nWe address individual reviewers comments in the responses to their reviews."}, "signatures": ["ICLR.cc/2020/Conference/Paper2454/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2454/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Distillation of Syntactic Information from Contextualized Word Representations", "authors": ["Shauli Ravfogel", "Yanai Elazar", "Jacob Goldberger", "Yoav Goldberg"], "authorids": ["shauli.ravfogel@gmail.com", "yanaiela@gmail.com", "jacob.goldberger@biu.ac.il", "yogo@cs.biu.ac.il"], "keywords": ["dismantlement", "contextualized word representations", "language models", "representation learning"], "TL;DR": "We distill language models representations for syntax by unsupervised metric learning", "abstract": "Contextualized word representations, such as ELMo and BERT, were shown to perform well on a various of semantic and structural (syntactic) task. In this work, we tackle the task of unsupervised disentanglement between semantics and structure in neural language representations: we aim to learn a transformation of the contextualized vectors, that discards the lexical semantics, but keeps the structural information. To this end, we automatically generate groups of sentences which are structurally similar but semantically different, and use metric-learning approach to learn a transformation that emphasizes the structural component that is encoded in the vectors. We demonstrate that our transformation clusters vectors in space by structural properties, rather than by lexical semantics. Finally, we demonstrate the utility of our distilled representations by showing that they outperform the original contextualized representations in few-shot parsing setting.", "pdf": "/pdf/8cf5ded7ea1b4017eac4e64c2f85195738eb726b.pdf", "code": "https://drive.google.com/file/d/1tGoYmNCOSTgE7T5RjRv_bV7o3JUD160t/view", "paperhash": "ravfogel|unsupervised_distillation_of_syntactic_information_from_contextualized_word_representations", "original_pdf": "/attachment/231581bc4b9cde5ab2e00c4c8ef1aa387a03104c.pdf", "_bibtex": "@misc{\nravfogel2020unsupervised,\ntitle={Unsupervised Distillation of Syntactic Information from Contextualized Word Representations},\nauthor={Shauli Ravfogel and Yanai Elazar and Jacob Goldberger and Yoav Goldberg},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlRFlHFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlRFlHFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2454/Authors", "ICLR.cc/2020/Conference/Paper2454/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2454/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2454/Reviewers", "ICLR.cc/2020/Conference/Paper2454/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2454/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2454/Authors|ICLR.cc/2020/Conference/Paper2454/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141110, "tmdate": 1576860534258, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2454/Authors", "ICLR.cc/2020/Conference/Paper2454/Reviewers", "ICLR.cc/2020/Conference/Paper2454/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2454/-/Official_Comment"}}}, {"id": "rJxPTT-IYH", "original": null, "number": 1, "cdate": 1571327423278, "ddate": null, "tcdate": 1571327423278, "tmdate": 1572972336107, "tddate": null, "forum": "HJlRFlHFPS", "replyto": "HJlRFlHFPS", "invitation": "ICLR.cc/2020/Conference/Paper2454/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary:\n=========\nThis paper aims to disentangle semantics and syntax in contextualized word representations. The main idea is to learn a transformation of the contexualized representations that will make two word representations to be more similar if they appear in the same syntactic context, but less similar if they appear in different syntactic contexts. The efficacy of this transformation is evaluated through cluster analysis, showing that words better organize in syntactic clusters after the transformation, and through low-resource dependency parsing. \n\nThe paper presents a simple approach to transform word representations and expose their syntactic information. The experiments are mostly convincing. I would like to see better motivation, more engagement with a wider range of related work, and more thorough quantitative evaluations. Another important question to address is also what kind of semantic/syntactic types of information are targeted, and how to handle the tradeoff between them, for instance for different purposes. \n\n\nMain comments:\n==============\n1. Motivation: I found the motivation for the problem understudied a bit lacking. The main motivation seems to be to disentangle semantic and syntactic information. But why should we care about that? Beyond reference to disentangling in computer vision, some more motivation would be good. The few-shot parsing is a good such motivation, although the results are a bit disappointing (see more on this below). Another possible motivation is potential applications of disentanglement in language generation. There is a line of work on style transfer also in language generation, and it seems plausible that the methodology could be applied to such tasks. \n2. The present work is well-differentiated from work on extracting syntactic information from word representations via supervised ways, as the current work does so in an unsupervised way. I don't quite get the terminological differentiation between \"mapping\" and \"extracting\" in the introduction, but the idea is clear. \n3. Have you considered alternative representations of word pairs besides the different of their transformations f(x)-f(y)? \n4. I found it interesting that the word representation from BERT is the concatenation of layer 16 with the mean of all the other layers. This is motivated by Hewitt and Manning's findings, and [5] found similar results. However, the different between layer 16 and others is not that large as to warrant emphasizing it so much. Perhaps a scalar mix with fine-tuning may work better, as in [5], or another method. Have you tried other word representations? I also wonder whether it makes sense to use different layers for different parts of the triplet loss, depending on whether to emphasize syntactic vs. semantic similarity. \n5. The introduction lays out connections to some related work, but leaves several relevant pieces missing. See examples below. \n6. The results in 3.3 are limited but useful. The comparison with a PCA-ed and reduced representation is well thought of, because of the risk with low-resource and high dimensionality. That said, I found the gap between the proposed syntax model and the ELMo-reduced disappointingly small. Even in the LAS, it seems like the difference is very small, ~0.5, although it's hard to tell from the figure. Providing the actual numbers and a measure of statistical significance would be helpful here. \n7. Some care should be taken to define what kind of semantics is targeted here. In several cases this is \"lexical semantics\", but then we have \"meaning\" in parentheses sometimes (end of intro). Obviously, there's much more to semantics and meaning that the lexical semantics, so a short discussion of how the work views other, say compositional semantics, would be good. \n\n\nOther comments:\n===============\n1. The introduction seeks a representation that will ignore the similarity between \"syrup\" in (2) and (4). I wonder if \"ignoring\" is too strong. One may not want to lose all lexical semantic information. Moreover, the proposed triplet loss does not guarantee that information is ignored (and justly so, in my opinion). \n2. In the example, \"maple\" and \"neural\" are said to be syntactically similar, although \"maple syrup\" is a noun compound while \"neural networks\" is an adjective-noun. Shouldn't they be treated differently then? Unless the notion of syntax is more narrow and just looks at unlabeled dependency arcs. \n3. Some experimental choices are left unexplained, such as k=6 (section 2.1) or mapping to 27 dims (section 2.3); these two seem potentially important. \n4. Section 2.3: do you also back-prop back into the BERT/ELMo model weights? \n5. The dataset statistics in section 3 do not match those in section 2.2. Please clarify. \n6. The qualitative cluster analysis via t-SNE (3.1) is compelling. It could be made stronger by reporting quantitative clustering statistics such as cluster purity before and after transformation. \n7. In the examples showin in 3.1, it would be good to give also the nearest neighbor before the transformation for comparison. \n8. The quantitative results in 3.2 convey the point convincingly. It's good to see also the lexical match measure going down. The random baseline is also a good sanity check to have. It would be good to provide full results with BERT, at least in the appendix and at least for section 3.2, maybe also for 3.3.\n9. More related work: \n+ Work that injects syntactic information into word representations in a supervised way, such as [1,2]\n+ Work that shows that word embeddings contain different kinds of information (syntactic/semantic), and propose simle linear transformations to uncover them. \n+ Engaging with the literature on style transfer in language generation would be good, as mentioned above for motivation, but also to situate this work w.r.t to related style transfer work. \n+ Another line of work that may be mentioned is the variety of papers trying to extract syntactic information from contextualized word representations, such as constructing trees from attention weights. There were a few such papers in BlackboxNLP 2018 and 2019. \n\nTypos, phrasing, formatting, etc.:\n============================\n- Abstract: a various of semantic... task -> various semantic... tasks; use metric-learning approach -> use a metric-learning approach; in few-shot parsing setting -> in a few-shot parsing setting\n- Wilcox et al. does not have a year\n- Introduction: few-shots parsing -> few-shot parsing\n- Method: extract vectors -> extracts vectors; Operativly -> Operatively \n- Section 3: should encourages -> should encourage; a few-shots settings -> a few-shot setting\n- 3.2: -- was not rendered properly\n- 3.3: matrix that reduce -> reduces \n\n\nReferences\n==========\n[1] Levy and Goldberg. 2014. Dependency-Based Word Embeddings\n[2] Bansal et al. 2014. Tailoring Continuous Word Representations for Dependency Parsing\n[3] Artetxe et al. 2018. Uncovering divergent linguistic information in word embeddings with lessons for intrinsic and extrinsic evaluation\n[4] Tenney et al. 2019. BERT Rediscovers the Classical NLP Pipeline\n[5] Liu et al. 2019. Linguistic Knowledge and Transferability of Contextual Representations"}, "signatures": ["ICLR.cc/2020/Conference/Paper2454/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2454/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Distillation of Syntactic Information from Contextualized Word Representations", "authors": ["Shauli Ravfogel", "Yanai Elazar", "Jacob Goldberger", "Yoav Goldberg"], "authorids": ["shauli.ravfogel@gmail.com", "yanaiela@gmail.com", "jacob.goldberger@biu.ac.il", "yogo@cs.biu.ac.il"], "keywords": ["dismantlement", "contextualized word representations", "language models", "representation learning"], "TL;DR": "We distill language models representations for syntax by unsupervised metric learning", "abstract": "Contextualized word representations, such as ELMo and BERT, were shown to perform well on a various of semantic and structural (syntactic) task. In this work, we tackle the task of unsupervised disentanglement between semantics and structure in neural language representations: we aim to learn a transformation of the contextualized vectors, that discards the lexical semantics, but keeps the structural information. To this end, we automatically generate groups of sentences which are structurally similar but semantically different, and use metric-learning approach to learn a transformation that emphasizes the structural component that is encoded in the vectors. We demonstrate that our transformation clusters vectors in space by structural properties, rather than by lexical semantics. Finally, we demonstrate the utility of our distilled representations by showing that they outperform the original contextualized representations in few-shot parsing setting.", "pdf": "/pdf/8cf5ded7ea1b4017eac4e64c2f85195738eb726b.pdf", "code": "https://drive.google.com/file/d/1tGoYmNCOSTgE7T5RjRv_bV7o3JUD160t/view", "paperhash": "ravfogel|unsupervised_distillation_of_syntactic_information_from_contextualized_word_representations", "original_pdf": "/attachment/231581bc4b9cde5ab2e00c4c8ef1aa387a03104c.pdf", "_bibtex": "@misc{\nravfogel2020unsupervised,\ntitle={Unsupervised Distillation of Syntactic Information from Contextualized Word Representations},\nauthor={Shauli Ravfogel and Yanai Elazar and Jacob Goldberger and Yoav Goldberg},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlRFlHFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJlRFlHFPS", "replyto": "HJlRFlHFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2454/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2454/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575842116681, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2454/Reviewers"], "noninvitees": [], "tcdate": 1570237722585, "tmdate": 1575842116694, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2454/-/Official_Review"}}}, {"id": "SygRa5LTKS", "original": null, "number": 2, "cdate": 1571805893695, "ddate": null, "tcdate": 1571805893695, "tmdate": 1572972336068, "tddate": null, "forum": "HJlRFlHFPS", "replyto": "HJlRFlHFPS", "invitation": "ICLR.cc/2020/Conference/Paper2454/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The authors state a clear hypothesis: it is possible to extract syntactic information from contextualized word vectors in an unsupervised manner. The method of creating syntactically equivalent (but semantically different) sentences is indeed interesting on its own. Experiments do support the main hypothesis -- the distilled embeddings are stronger in syntactic tasks than the default contextualized vectors. The authors provide the code for ease of reproducibility which is nice.\n\nThere is a short literature review, but I am wondering if something similar was done for static word embeddings. I understand that they are obsolete these days, but on the other hand, they are better researched, so were there any attempts to disentangle syntax and semantics in the classical static word vectors?\n\nOverall, I have no major concerns with the paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper2454/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2454/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Distillation of Syntactic Information from Contextualized Word Representations", "authors": ["Shauli Ravfogel", "Yanai Elazar", "Jacob Goldberger", "Yoav Goldberg"], "authorids": ["shauli.ravfogel@gmail.com", "yanaiela@gmail.com", "jacob.goldberger@biu.ac.il", "yogo@cs.biu.ac.il"], "keywords": ["dismantlement", "contextualized word representations", "language models", "representation learning"], "TL;DR": "We distill language models representations for syntax by unsupervised metric learning", "abstract": "Contextualized word representations, such as ELMo and BERT, were shown to perform well on a various of semantic and structural (syntactic) task. In this work, we tackle the task of unsupervised disentanglement between semantics and structure in neural language representations: we aim to learn a transformation of the contextualized vectors, that discards the lexical semantics, but keeps the structural information. To this end, we automatically generate groups of sentences which are structurally similar but semantically different, and use metric-learning approach to learn a transformation that emphasizes the structural component that is encoded in the vectors. We demonstrate that our transformation clusters vectors in space by structural properties, rather than by lexical semantics. Finally, we demonstrate the utility of our distilled representations by showing that they outperform the original contextualized representations in few-shot parsing setting.", "pdf": "/pdf/8cf5ded7ea1b4017eac4e64c2f85195738eb726b.pdf", "code": "https://drive.google.com/file/d/1tGoYmNCOSTgE7T5RjRv_bV7o3JUD160t/view", "paperhash": "ravfogel|unsupervised_distillation_of_syntactic_information_from_contextualized_word_representations", "original_pdf": "/attachment/231581bc4b9cde5ab2e00c4c8ef1aa387a03104c.pdf", "_bibtex": "@misc{\nravfogel2020unsupervised,\ntitle={Unsupervised Distillation of Syntactic Information from Contextualized Word Representations},\nauthor={Shauli Ravfogel and Yanai Elazar and Jacob Goldberger and Yoav Goldberg},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlRFlHFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJlRFlHFPS", "replyto": "HJlRFlHFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2454/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2454/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575842116681, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2454/Reviewers"], "noninvitees": [], "tcdate": 1570237722585, "tmdate": 1575842116694, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2454/-/Official_Review"}}}, {"id": "SklmVR-fqH", "original": null, "number": 3, "cdate": 1572113963167, "ddate": null, "tcdate": 1572113963167, "tmdate": 1572972336017, "tddate": null, "forum": "HJlRFlHFPS", "replyto": "HJlRFlHFPS", "invitation": "ICLR.cc/2020/Conference/Paper2454/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "CONTRIBUTIONS:\nTopic: Disentangling syntax from semantics in contextualized word representations\nC1. A method for generating \u2018structurally equivalent\u2019 sentences is proposed, based only on the assumption that maintaining function words, and replacing one content word of a source sentence with another to produce a new grammatical sentence, yields a target sentence that is equivalent to the source sentence. \nC2. The \u2018structural relation\u2019 between two words in a sentence is modeled as the difference between their vector embeddings.\nC3a. The structural relation between a pair of content words in one sentence is assumed to be the same as that between the corresponding pair in an equivalent sentence. \nC3b. The structural relation between any pair of content words in one sentence is assumed to be different from the structural relation between any pair of content words in an inequivalent sentence. \nC4. Given a selected word in a source sentence, to generate an alternative \u2018corresponding\u2019 content word for an equivalent target sentence, BERT is used to predict the source word when it is masked, given the remaining words in the source sentence. The alternative corresponding word is randomly selected from among the top (30) candidates predicted by BERT. Given a source sentence, the set of target sentences formed by cumulatively replacing content words one at a time in randomly selected positions defines an \u2018equivalence set\u2019 in which words in different sentences with the same left-to-right index are corresponding words. (To promote the formation of grammatical target sentences, a word is only replaced by another word with the same POS.) A pre-defined set of equivalence sets is used for training.\nC5. A metric learning paradigm with triplet loss is used to find a function f for mapping ELMo or BERT word embeddings to a new vector space of \u2018transformed word representations\u2019. Implementing C2 and C3a, given the indices i and i\u2019 of two content words, the triplet loss rewards closeness of the difference D between the transformed embeddings of the pair of words with these indices in sentence S and the corresponding difference D\u2019 for an equivalent sentence S\u2019. Implementing C3b, the triplet loss penalizes closeness between D and D\u201d, where D\u201d is the difference between transformed word embeddings of a pair of content words in a sentence S\u201d that is inequivalent to S. (Eq. 4).\nC6. (Implementing C5.) To form a mini-batch for minimizing the triplet loss, a set of (500) sentences S is selected, and for each a pair of indices of content words is chosen. Training will use the difference in the transformed embeddings of the words in S with these indices: call this D, and call the set of these (500) D vectors B. For each sentence S in B, a \u2018positive pair\u2019 (D, D\u2019) is generated, where D\u2019 is the corresponding difference for S\u2019, a selected sentence in the equivalence set of S. Closeness of D and D\u2019 is rewarded by the triplet loss, implementing C3a. To implement C3b, a \u2018negative pair\u2019 (D, D\u201d), for which closeness is penalized by the loss, is formed as follows. D\u201d is the closest vector in B to D that is derived from a sentence S\u201d that is not equivalent to S. \nC7. 2-D t-SNE plots (seem to) show that relative to the original ELMo embeddings, the transformed embeddings cluster better by POS (Fig. 3). (No quantitative measure of this is provided, and the two plots are not easy to distinguish.)\nC8. Pairs of closest ELMo vectors share syntactic (dependency parse) properties to a greater degree after transformation than before (Table 1). To check that this goes beyond merely POS-based closeness, the syntactic relations that least determine POS are examined separately, and the result remains. Furthermore, the proportion of pairs of closest vectors that are embeddings of the same word (in different contexts) drops from 77.6% to 27.4%, showing that the transformation reduces the influence of lexical-semantic similarity. Similar results hold for BERT embeddings, but to a lesser degree, so the paper focusses on ELMo. \nC9. Few-shot parsing. Two dependency parsers are trained, one on ELMo embeddings, the other on their transformations (under the proposed method). In the small-data regime (less than 200 training examples), the transformed embeddings yield higher parser performance, even when the encoding size of the ELMo embeddings is reduced (from 2048 to 75) to match that of the transformed embeddings by either PCA or a learned linear mapping. (Fig. 4) \nRATING: Weak accept\nREASONS FOR RATING (SUMMARY). Using deep learning to create an encoding of syntactic structure with minimal supervision is an important goal and the paper proposes a clever way of doing this. The only \u2018supervision\u2019 here comes from (i) the function/content-word distinction (C1 above): two grammatical sentences are structurally equivalent if [but not only if] one can be derived from the other by replacing one content word with another; and (ii) filtering candidate replacement words to match the POS of the replaced word. BERT\u2019s ability to guess a masked word is put to good use in providing suitable content word substitutions. The experimental results are rather convincing.\nREVIEW (beyond the summary above)\nC1. This assumption is famously not deemed to be true in linguistics, where the structural difference between \u2018control\u2019 and \u2018raising\u2019 verbs is basic Ling 101 material: see https://en.wikipedia.org/wiki/Control_(linguistics)#Control_vs._raising. This particular structural contrast illustrates how verbs can differ in their argument structure, without there being function words to signal the difference. So substituting *verbs* in particular may be non-ideal for the purposes of this work. Even the third example given by the authors in Sec. 3.1 illustrates a related  point, where function words do signal the contrast:  while the meaning of \u2018let\u2019 and \u2018allow\u2019 may be very similar, their argument structures differ, so that replacing \u2018lets\u2019 with \u2018allows\u2019 in the first sentence, or the reverse in the second sentence, produces ungrammatical results: \n*their first project is software that *allows* players connect the company \u2019s controller to their device\n*the city offers a route-finding website that *lets* users to map personalized bike routes\nTherefore, contrary to the paper, relative to linguistic syntactic structure, it is not a good result that \u2018lets\u2019 in the original version of the first sentence is the closest neighbor in transformed embedding space to \u2018allows\u2019 in the second. Rather, it is probably meaning, not structure, that makes \u2018let\u2019 and \u2018allow\u2019 similar.\nIt would improve the paper to make note of this general concern with C1 and to provide a response.\nOn another point, an important premise of the proposed method (C2 above) is that differences in vector space embeddings encode relations; this has been used by a number of previous authors since the famous Mikolov, Yih & Zweig NAACL2013, and that work should be cited and discussed."}, "signatures": ["ICLR.cc/2020/Conference/Paper2454/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2454/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Distillation of Syntactic Information from Contextualized Word Representations", "authors": ["Shauli Ravfogel", "Yanai Elazar", "Jacob Goldberger", "Yoav Goldberg"], "authorids": ["shauli.ravfogel@gmail.com", "yanaiela@gmail.com", "jacob.goldberger@biu.ac.il", "yogo@cs.biu.ac.il"], "keywords": ["dismantlement", "contextualized word representations", "language models", "representation learning"], "TL;DR": "We distill language models representations for syntax by unsupervised metric learning", "abstract": "Contextualized word representations, such as ELMo and BERT, were shown to perform well on a various of semantic and structural (syntactic) task. In this work, we tackle the task of unsupervised disentanglement between semantics and structure in neural language representations: we aim to learn a transformation of the contextualized vectors, that discards the lexical semantics, but keeps the structural information. To this end, we automatically generate groups of sentences which are structurally similar but semantically different, and use metric-learning approach to learn a transformation that emphasizes the structural component that is encoded in the vectors. We demonstrate that our transformation clusters vectors in space by structural properties, rather than by lexical semantics. Finally, we demonstrate the utility of our distilled representations by showing that they outperform the original contextualized representations in few-shot parsing setting.", "pdf": "/pdf/8cf5ded7ea1b4017eac4e64c2f85195738eb726b.pdf", "code": "https://drive.google.com/file/d/1tGoYmNCOSTgE7T5RjRv_bV7o3JUD160t/view", "paperhash": "ravfogel|unsupervised_distillation_of_syntactic_information_from_contextualized_word_representations", "original_pdf": "/attachment/231581bc4b9cde5ab2e00c4c8ef1aa387a03104c.pdf", "_bibtex": "@misc{\nravfogel2020unsupervised,\ntitle={Unsupervised Distillation of Syntactic Information from Contextualized Word Representations},\nauthor={Shauli Ravfogel and Yanai Elazar and Jacob Goldberger and Yoav Goldberg},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlRFlHFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJlRFlHFPS", "replyto": "HJlRFlHFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2454/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2454/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575842116681, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2454/Reviewers"], "noninvitees": [], "tcdate": 1570237722585, "tmdate": 1575842116694, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2454/-/Official_Review"}}}, {"id": "Byey-Ljz5H", "original": null, "number": 4, "cdate": 1572152822583, "ddate": null, "tcdate": 1572152822583, "tmdate": 1572972335968, "tddate": null, "forum": "HJlRFlHFPS", "replyto": "HJlRFlHFPS", "invitation": "ICLR.cc/2020/Conference/Paper2454/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary\nThe authors proposed to disentangle syntactic information and semantic information from pre-trained contextualized word representations.\n\nThey use BERT to generate groups of sentences that are structurally similar (have the same POS tag for each word) but semantically different. Then they use a metric-learning approach to learn a linear transformation that encourages sentences from the same group to have closer distance. Specifically, they defined a triplet loss (Eq4) and uses negative sampling.\n\nThey use 150,000 sentences from Wikipedia to train the transformation. POS tags are obtained from spaCy. To evaluate the learned representations, they provided a tSNE visualization of the original and transformed representations (groups by dependency label); evaluate whether the nearest neighbor shares the same syntactic role; low-resource parsing.\n\nReasons of rejection:\n1. I don't agree with the authors' argument, \"we aim to extract the structural information encoded in the network in an unsupervised manner, without pre-supposing an existing syntactic annotation scheme\".  First, what do you mean by structural information without a clear definition? Also, in the method, the authors construct a dataset where each group of the sentence share similar syntactic structures (having the same POS tag). It seems there that the structural information just means POS tags.\n\n2. The author failed to convince me that the learned representation is more powerful than just combining POS tags with the original representations. Since POS tags are assumed to be available during training. I think a reasonable baseline in all experiments would be the performance based on POS tags. For example, in Figure 3, although the original EMLo representation does not correlates with the dependency label very much, the POS tags may do. In Figure 4, the authors should compare with delexicalized dependency parsing, which performs pretty well in los-resource setting.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2454/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2454/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Distillation of Syntactic Information from Contextualized Word Representations", "authors": ["Shauli Ravfogel", "Yanai Elazar", "Jacob Goldberger", "Yoav Goldberg"], "authorids": ["shauli.ravfogel@gmail.com", "yanaiela@gmail.com", "jacob.goldberger@biu.ac.il", "yogo@cs.biu.ac.il"], "keywords": ["dismantlement", "contextualized word representations", "language models", "representation learning"], "TL;DR": "We distill language models representations for syntax by unsupervised metric learning", "abstract": "Contextualized word representations, such as ELMo and BERT, were shown to perform well on a various of semantic and structural (syntactic) task. In this work, we tackle the task of unsupervised disentanglement between semantics and structure in neural language representations: we aim to learn a transformation of the contextualized vectors, that discards the lexical semantics, but keeps the structural information. To this end, we automatically generate groups of sentences which are structurally similar but semantically different, and use metric-learning approach to learn a transformation that emphasizes the structural component that is encoded in the vectors. We demonstrate that our transformation clusters vectors in space by structural properties, rather than by lexical semantics. Finally, we demonstrate the utility of our distilled representations by showing that they outperform the original contextualized representations in few-shot parsing setting.", "pdf": "/pdf/8cf5ded7ea1b4017eac4e64c2f85195738eb726b.pdf", "code": "https://drive.google.com/file/d/1tGoYmNCOSTgE7T5RjRv_bV7o3JUD160t/view", "paperhash": "ravfogel|unsupervised_distillation_of_syntactic_information_from_contextualized_word_representations", "original_pdf": "/attachment/231581bc4b9cde5ab2e00c4c8ef1aa387a03104c.pdf", "_bibtex": "@misc{\nravfogel2020unsupervised,\ntitle={Unsupervised Distillation of Syntactic Information from Contextualized Word Representations},\nauthor={Shauli Ravfogel and Yanai Elazar and Jacob Goldberger and Yoav Goldberg},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlRFlHFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJlRFlHFPS", "replyto": "HJlRFlHFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2454/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2454/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575842116681, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2454/Reviewers"], "noninvitees": [], "tcdate": 1570237722585, "tmdate": 1575842116694, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2454/-/Official_Review"}}}], "count": 18}