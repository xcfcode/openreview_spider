{"notes": [{"id": "Byx0PREtDH", "original": "SJx7AcwOwS", "number": 1194, "cdate": 1569439334043, "ddate": null, "tcdate": 1569439334043, "tmdate": 1577168290266, "tddate": null, "forum": "Byx0PREtDH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "BEYOND SUPERVISED LEARNING: RECOGNIZING UNSEEN ATTRIBUTE-OBJECT PAIRS WITH VISION-LANGUAGE FUSION AND ATTRACTOR NETWORKS", "authors": ["Hui Chen", "Zhixiong Nan", "Nanning Zheng"], "authorids": ["chenhui0622@stu.xjtu.edu.cn", "nanzhixiong@stu.xjtu.edu.cn", "nnzheng@mail.xjtu.edu.cn"], "keywords": ["image understanding"], "abstract": "This paper handles a challenging problem, unseen attribute-object pair recognition, which asks a model to simultaneously recognize the attribute type and the object type of a given image while this attribute-object pair is not included in the training set. In the past years, the conventional classifier-based methods, which recognize unseen attribute-object pairs by composing separately-trained attribute classifiers and object classifiers, are strongly frustrated. Different from conventional methods, we propose a generative model with a visual pathway and a linguistic pathway. In each pathway, the attractor network is involved to learn the intrinsic feature representation to explore the inner relationship between the attribute and the object. With the learned features in both pathways, the unseen attribute-object pair is recognized by finding out the pair whose linguistic feature closely matches the visual feature of the given image. On two public datasets, our model achieves impressive experiment results, notably outperforming the state-of-the-art methods.", "pdf": "/pdf/c0367eacb3829d6df0b60e0df205ec355e425d1a.pdf", "paperhash": "chen|beyond_supervised_learning_recognizing_unseen_attributeobject_pairs_with_visionlanguage_fusion_and_attractor_networks", "original_pdf": "/attachment/c0367eacb3829d6df0b60e0df205ec355e425d1a.pdf", "_bibtex": "@misc{\nchen2020beyond,\ntitle={{\\{}BEYOND{\\}} {\\{}SUPERVISED{\\}} {\\{}LEARNING{\\}}: {\\{}RECOGNIZING{\\}} {\\{}UNSEEN{\\}} {\\{}ATTRIBUTE{\\}}-{\\{}OBJECT{\\}} {\\{}PAIRS{\\}} {\\{}WITH{\\}} {\\{}VISION{\\}}-{\\{}LANGUAGE{\\}} {\\{}FUSION{\\}} {\\{}AND{\\}} {\\{}ATTRACTOR{\\}} {\\{}NETWORKS{\\}}},\nauthor={Hui Chen and Zhixiong Nan and Nanning Zheng},\nyear={2020},\nurl={https://openreview.net/forum?id=Byx0PREtDH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "KjPnxd3BFj", "original": null, "number": 1, "cdate": 1576798717021, "ddate": null, "tcdate": 1576798717021, "tmdate": 1576800919507, "tddate": null, "forum": "Byx0PREtDH", "replyto": "Byx0PREtDH", "invitation": "ICLR.cc/2020/Conference/Paper1194/-/Decision", "content": {"decision": "Reject", "comment": "The paper focuses on attribute-object pairs image recognition, leveraging some novel \"attractor network\".\n\nAt this stage, all reviewers agree the paper needs a lot of improvements in the writing. There are also concerns regarding (i) novelty: the proposed approach being two encoder-decoder networks; (ii) lack of motivation for such architecture (iii) possible flow in the approach (are the authors using test labels?) and (iv) weak experiments.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "BEYOND SUPERVISED LEARNING: RECOGNIZING UNSEEN ATTRIBUTE-OBJECT PAIRS WITH VISION-LANGUAGE FUSION AND ATTRACTOR NETWORKS", "authors": ["Hui Chen", "Zhixiong Nan", "Nanning Zheng"], "authorids": ["chenhui0622@stu.xjtu.edu.cn", "nanzhixiong@stu.xjtu.edu.cn", "nnzheng@mail.xjtu.edu.cn"], "keywords": ["image understanding"], "abstract": "This paper handles a challenging problem, unseen attribute-object pair recognition, which asks a model to simultaneously recognize the attribute type and the object type of a given image while this attribute-object pair is not included in the training set. In the past years, the conventional classifier-based methods, which recognize unseen attribute-object pairs by composing separately-trained attribute classifiers and object classifiers, are strongly frustrated. Different from conventional methods, we propose a generative model with a visual pathway and a linguistic pathway. In each pathway, the attractor network is involved to learn the intrinsic feature representation to explore the inner relationship between the attribute and the object. With the learned features in both pathways, the unseen attribute-object pair is recognized by finding out the pair whose linguistic feature closely matches the visual feature of the given image. On two public datasets, our model achieves impressive experiment results, notably outperforming the state-of-the-art methods.", "pdf": "/pdf/c0367eacb3829d6df0b60e0df205ec355e425d1a.pdf", "paperhash": "chen|beyond_supervised_learning_recognizing_unseen_attributeobject_pairs_with_visionlanguage_fusion_and_attractor_networks", "original_pdf": "/attachment/c0367eacb3829d6df0b60e0df205ec355e425d1a.pdf", "_bibtex": "@misc{\nchen2020beyond,\ntitle={{\\{}BEYOND{\\}} {\\{}SUPERVISED{\\}} {\\{}LEARNING{\\}}: {\\{}RECOGNIZING{\\}} {\\{}UNSEEN{\\}} {\\{}ATTRIBUTE{\\}}-{\\{}OBJECT{\\}} {\\{}PAIRS{\\}} {\\{}WITH{\\}} {\\{}VISION{\\}}-{\\{}LANGUAGE{\\}} {\\{}FUSION{\\}} {\\{}AND{\\}} {\\{}ATTRACTOR{\\}} {\\{}NETWORKS{\\}}},\nauthor={Hui Chen and Zhixiong Nan and Nanning Zheng},\nyear={2020},\nurl={https://openreview.net/forum?id=Byx0PREtDH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Byx0PREtDH", "replyto": "Byx0PREtDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795727489, "tmdate": 1576800279739, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1194/-/Decision"}}}, {"id": "SylMYma6YH", "original": null, "number": 1, "cdate": 1571832697831, "ddate": null, "tcdate": 1571832697831, "tmdate": 1572972500501, "tddate": null, "forum": "Byx0PREtDH", "replyto": "Byx0PREtDH", "invitation": "ICLR.cc/2020/Conference/Paper1194/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper attempts to tackle unseen object-attribute recognition in still images. This follows a line of works that addresses such a problem using embedding (with operators), classification and generative approaches. The paper makes use of the visual attractor network, extracting different visual representations for objects and attributes. A collection of losses (some regularisation and some distance in embedding space) are proposed. I believe the paper (except A below) is technically correct.\n\nThis paper was particularly tricky to read. While convincing at parts, there are two worrying statements. One I currently believe (pending the authors' response) is a scientific flaw (A), the other causes the results to be potentially incomparable to published works (B). I detail these next.\n\n(A) Scientific Flaw: Quoting from the paper, at inference (i.e. for a test image), \"we propose a voting inference method. For an input image, its visual attractor feature AV is computed, ... By computing the L2 distance ... the pair that corresponds to the minimum distance is taken as the initial attribute-object pair recognition for the input image. For the images belonging to the same pair, we use the recognitions for all images to vote a pair label to be the final attribute-object pair recognition.\"\nIn my interpretation, the authors are taking all test images that are labeled using the same pair (e.g. 'sliced-apple' - assuming this is an unseen pair). They then use all these to make a final decision for all these images. \nI believe my interpretation is correct because their statement \"images belonging to the same pair\" can only exist in testing for their target scenario (unseen object-attribute pair).\nIf my interpretation is correct, this is A MAJOR FLAW in the approach. How do the authors know that a collection of test images \"belong to the same pair\"?? This knowledge can only be acquired from labels of the test set and cannot be used as part of any algorithm.\nI would very much like for my interpretation to be naive, but I could not find a different explanation for this statement in page 6. The authors note in the conclusion that this voting approach is part of their contribution.\n\n(B) In reporting the results, the authors state that: \"Using the grid-search method, we set parameters \\alpha\u000b; ... [5 parameters] \f in Eq.17 to be 1:0; 2:0; 6:0; 3:0; 2:0 for the MIT-States dataset and 1:0; 3:0; 6:0; 1:0; 2:0 for the UTZappos50K\ndataset. Attractor parameters cv; cl; Tv; Tl are set as 0:1; 0:2; 20; 10 for the MIT-States dataset and 0:08; 0:16; 25; 15 for the UT-Zappos50K dataset.\"\nThis assumes that the authors are grid-searching all parameters, and evaluating the test set performance for each case, then reporting the maximum possible test performance per dataset. The authors do not reference a validation set they use to set these parameters instead. Importantly, the values are SIGNIFICANTLY different for each test set and there is no explanation of why the parameters vary largely. It is not clear how the performance on an new dataset would be, \nUp to my knowledge, the other methods that they compare to in Table 1, have not reported a grid-search over the parameter space for maximum performance of the test set. This IMO makes the comparative evaluation, which the authors deem to be 'impressive' quite unfair. It is not possible to assess the method's performance using these results."}, "signatures": ["ICLR.cc/2020/Conference/Paper1194/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1194/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "BEYOND SUPERVISED LEARNING: RECOGNIZING UNSEEN ATTRIBUTE-OBJECT PAIRS WITH VISION-LANGUAGE FUSION AND ATTRACTOR NETWORKS", "authors": ["Hui Chen", "Zhixiong Nan", "Nanning Zheng"], "authorids": ["chenhui0622@stu.xjtu.edu.cn", "nanzhixiong@stu.xjtu.edu.cn", "nnzheng@mail.xjtu.edu.cn"], "keywords": ["image understanding"], "abstract": "This paper handles a challenging problem, unseen attribute-object pair recognition, which asks a model to simultaneously recognize the attribute type and the object type of a given image while this attribute-object pair is not included in the training set. In the past years, the conventional classifier-based methods, which recognize unseen attribute-object pairs by composing separately-trained attribute classifiers and object classifiers, are strongly frustrated. Different from conventional methods, we propose a generative model with a visual pathway and a linguistic pathway. In each pathway, the attractor network is involved to learn the intrinsic feature representation to explore the inner relationship between the attribute and the object. With the learned features in both pathways, the unseen attribute-object pair is recognized by finding out the pair whose linguistic feature closely matches the visual feature of the given image. On two public datasets, our model achieves impressive experiment results, notably outperforming the state-of-the-art methods.", "pdf": "/pdf/c0367eacb3829d6df0b60e0df205ec355e425d1a.pdf", "paperhash": "chen|beyond_supervised_learning_recognizing_unseen_attributeobject_pairs_with_visionlanguage_fusion_and_attractor_networks", "original_pdf": "/attachment/c0367eacb3829d6df0b60e0df205ec355e425d1a.pdf", "_bibtex": "@misc{\nchen2020beyond,\ntitle={{\\{}BEYOND{\\}} {\\{}SUPERVISED{\\}} {\\{}LEARNING{\\}}: {\\{}RECOGNIZING{\\}} {\\{}UNSEEN{\\}} {\\{}ATTRIBUTE{\\}}-{\\{}OBJECT{\\}} {\\{}PAIRS{\\}} {\\{}WITH{\\}} {\\{}VISION{\\}}-{\\{}LANGUAGE{\\}} {\\{}FUSION{\\}} {\\{}AND{\\}} {\\{}ATTRACTOR{\\}} {\\{}NETWORKS{\\}}},\nauthor={Hui Chen and Zhixiong Nan and Nanning Zheng},\nyear={2020},\nurl={https://openreview.net/forum?id=Byx0PREtDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Byx0PREtDH", "replyto": "Byx0PREtDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1194/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1194/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575660268968, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1194/Reviewers"], "noninvitees": [], "tcdate": 1570237740959, "tmdate": 1575660268983, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1194/-/Official_Review"}}}, {"id": "Bkl7Aytg5S", "original": null, "number": 2, "cdate": 1572011978889, "ddate": null, "tcdate": 1572011978889, "tmdate": 1572972500445, "tddate": null, "forum": "Byx0PREtDH", "replyto": "Byx0PREtDH", "invitation": "ICLR.cc/2020/Conference/Paper1194/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies the so-called \u201cunseen attribute-object pair recognition\u201d,\nwhich asks a model to simultaneously recognize the attribute type and the\nobject type of a given image. \n\n1) Despite the authours claimed this is a novel task, it has been thoroughly studied long time ago. For example, NEIL [1], and comparative attributes [2]. So it would be advisable to thoroughly review those related literature.\n\n2) the whole framework is a two pathway encoder-decoder networks. The framework is well explained. It would be great if there are more words about the motivations why the network is designed in such a manner. In general, there is lack of discussion why the attractor networks are novel, and significant. \n\n3) In term of network structure, the loss functions introduced are pretty straight-forward. Please claim novelty about loss functins, if there is anything special.\n\n4) The experiments are pretty weak: Only on two small datasets. It would be great if there are more experiments on larger, or challenging datasets. Considering it\u2019s almost ten pages; I would prefer more persuasive experiments to validate the novelties (but please better summarize the novelty again).\n\n\n\n[1] NEIL: Extracting Visual Knowledge from Web Data. 2012\n\n[2] Constrained Semi-Supervised Learning using Attributes and Comparative Attributes. ECCV 2012\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1194/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1194/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "BEYOND SUPERVISED LEARNING: RECOGNIZING UNSEEN ATTRIBUTE-OBJECT PAIRS WITH VISION-LANGUAGE FUSION AND ATTRACTOR NETWORKS", "authors": ["Hui Chen", "Zhixiong Nan", "Nanning Zheng"], "authorids": ["chenhui0622@stu.xjtu.edu.cn", "nanzhixiong@stu.xjtu.edu.cn", "nnzheng@mail.xjtu.edu.cn"], "keywords": ["image understanding"], "abstract": "This paper handles a challenging problem, unseen attribute-object pair recognition, which asks a model to simultaneously recognize the attribute type and the object type of a given image while this attribute-object pair is not included in the training set. In the past years, the conventional classifier-based methods, which recognize unseen attribute-object pairs by composing separately-trained attribute classifiers and object classifiers, are strongly frustrated. Different from conventional methods, we propose a generative model with a visual pathway and a linguistic pathway. In each pathway, the attractor network is involved to learn the intrinsic feature representation to explore the inner relationship between the attribute and the object. With the learned features in both pathways, the unseen attribute-object pair is recognized by finding out the pair whose linguistic feature closely matches the visual feature of the given image. On two public datasets, our model achieves impressive experiment results, notably outperforming the state-of-the-art methods.", "pdf": "/pdf/c0367eacb3829d6df0b60e0df205ec355e425d1a.pdf", "paperhash": "chen|beyond_supervised_learning_recognizing_unseen_attributeobject_pairs_with_visionlanguage_fusion_and_attractor_networks", "original_pdf": "/attachment/c0367eacb3829d6df0b60e0df205ec355e425d1a.pdf", "_bibtex": "@misc{\nchen2020beyond,\ntitle={{\\{}BEYOND{\\}} {\\{}SUPERVISED{\\}} {\\{}LEARNING{\\}}: {\\{}RECOGNIZING{\\}} {\\{}UNSEEN{\\}} {\\{}ATTRIBUTE{\\}}-{\\{}OBJECT{\\}} {\\{}PAIRS{\\}} {\\{}WITH{\\}} {\\{}VISION{\\}}-{\\{}LANGUAGE{\\}} {\\{}FUSION{\\}} {\\{}AND{\\}} {\\{}ATTRACTOR{\\}} {\\{}NETWORKS{\\}}},\nauthor={Hui Chen and Zhixiong Nan and Nanning Zheng},\nyear={2020},\nurl={https://openreview.net/forum?id=Byx0PREtDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Byx0PREtDH", "replyto": "Byx0PREtDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1194/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1194/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575660268968, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1194/Reviewers"], "noninvitees": [], "tcdate": 1570237740959, "tmdate": 1575660268983, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1194/-/Official_Review"}}}, {"id": "SylOEzmTqr", "original": null, "number": 3, "cdate": 1572839984248, "ddate": null, "tcdate": 1572839984248, "tmdate": 1572972500403, "tddate": null, "forum": "Byx0PREtDH", "replyto": "Byx0PREtDH", "invitation": "ICLR.cc/2020/Conference/Paper1194/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper tries to handle the unseen attribute-object pairs recognition, which asks a model to simultaneously recognize the attribute type and the object type of a given image while this attribute-object pair is not included in the training set. The claimed contribution includes: (1) they for the first time introduce the attractor networks to recognize unseen attribute-object pairs; (2) their method exhibits much better performance than conventional methods and state-of-the-art methods on two challenging public datasets.  \n\nI found this paper poorly written. First, the introduction of attractor networks lacks intuition, explanation, and experiment support. The only support of attractor network is Table2, which show marginal improvement on MIT-States and significant improvement on UT-Zappos. Even for the two picked dataset, the improvement is not consistent, this is not enough to show the effectiveness of attractor network. The authors could design some visualization/metrics to show what the attractor networks have learned, i.e., the visualization of the nodes. The authors should also provide more ablation studies on the attractor network, i.e., the number of the nodes, the time-tick Tv, etc. After all, the experiment results in this paper shows little evidence of how the attractor network works, and the insight of how it works if so.\n\nSecond, the ablation study and conclusion are confusing. Combining Table 1,2&3, we can see that the decoding loss is the magic. Therefore, I should say that the decoding loss is the main reason for the ''much better performance'' in this paper, instead of the attractor network. However, the baseline method GENERATE already has the reconstruction/decoder loss. Why is its number low? \n\nI did not get the meaning of this sentence in Section 3.4. \"For the images belonging to the same pair, we\nuse the recognitions for all images to vote a pair label to be the final attribute-object pair recognition.\""}, "signatures": ["ICLR.cc/2020/Conference/Paper1194/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1194/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "BEYOND SUPERVISED LEARNING: RECOGNIZING UNSEEN ATTRIBUTE-OBJECT PAIRS WITH VISION-LANGUAGE FUSION AND ATTRACTOR NETWORKS", "authors": ["Hui Chen", "Zhixiong Nan", "Nanning Zheng"], "authorids": ["chenhui0622@stu.xjtu.edu.cn", "nanzhixiong@stu.xjtu.edu.cn", "nnzheng@mail.xjtu.edu.cn"], "keywords": ["image understanding"], "abstract": "This paper handles a challenging problem, unseen attribute-object pair recognition, which asks a model to simultaneously recognize the attribute type and the object type of a given image while this attribute-object pair is not included in the training set. In the past years, the conventional classifier-based methods, which recognize unseen attribute-object pairs by composing separately-trained attribute classifiers and object classifiers, are strongly frustrated. Different from conventional methods, we propose a generative model with a visual pathway and a linguistic pathway. In each pathway, the attractor network is involved to learn the intrinsic feature representation to explore the inner relationship between the attribute and the object. With the learned features in both pathways, the unseen attribute-object pair is recognized by finding out the pair whose linguistic feature closely matches the visual feature of the given image. On two public datasets, our model achieves impressive experiment results, notably outperforming the state-of-the-art methods.", "pdf": "/pdf/c0367eacb3829d6df0b60e0df205ec355e425d1a.pdf", "paperhash": "chen|beyond_supervised_learning_recognizing_unseen_attributeobject_pairs_with_visionlanguage_fusion_and_attractor_networks", "original_pdf": "/attachment/c0367eacb3829d6df0b60e0df205ec355e425d1a.pdf", "_bibtex": "@misc{\nchen2020beyond,\ntitle={{\\{}BEYOND{\\}} {\\{}SUPERVISED{\\}} {\\{}LEARNING{\\}}: {\\{}RECOGNIZING{\\}} {\\{}UNSEEN{\\}} {\\{}ATTRIBUTE{\\}}-{\\{}OBJECT{\\}} {\\{}PAIRS{\\}} {\\{}WITH{\\}} {\\{}VISION{\\}}-{\\{}LANGUAGE{\\}} {\\{}FUSION{\\}} {\\{}AND{\\}} {\\{}ATTRACTOR{\\}} {\\{}NETWORKS{\\}}},\nauthor={Hui Chen and Zhixiong Nan and Nanning Zheng},\nyear={2020},\nurl={https://openreview.net/forum?id=Byx0PREtDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Byx0PREtDH", "replyto": "Byx0PREtDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1194/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1194/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575660268968, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1194/Reviewers"], "noninvitees": [], "tcdate": 1570237740959, "tmdate": 1575660268983, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1194/-/Official_Review"}}}], "count": 5}