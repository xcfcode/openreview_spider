{"notes": [{"id": "S1g8K1BFwS", "original": "SyxxmLRuvB", "number": 1841, "cdate": 1569439613605, "ddate": null, "tcdate": 1569439613605, "tmdate": 1583912050876, "tddate": null, "forum": "S1g8K1BFwS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["tabacof@gmail.com", "luca.costabello@accenture.com"], "title": "Probability Calibration for Knowledge Graph Embedding Models", "authors": ["Pedro Tabacof", "Luca Costabello"], "pdf": "/pdf/2124ce9449505128b4a25bddac7a0c984b70c4a6.pdf", "TL;DR": "We propose a novel method to calibrate knowledge graph embedding models without the need of negative examples.", "abstract": "Knowledge graph embedding research has overlooked the problem of probability calibration. We show popular embedding models are indeed uncalibrated. That means probability estimates associated to predicted triples are unreliable. We present a novel method to calibrate a model when ground truth negatives are not available, which is the usual case in knowledge graphs. We propose to use Platt scaling and isotonic regression alongside our method. Experiments on three datasets with ground truth negatives show our contribution leads to well calibrated models when compared to the gold standard of using negatives. We get significantly better results than the uncalibrated models from all calibration methods. We show isotonic regression offers the best the performance overall, not without trade-offs. We also show that calibrated models reach state-of-the-art accuracy without the need to define relation-specific decision thresholds.", "keywords": ["knowledge graph embeddings", "probability calibration", "calibration", "graph representation learning", "knowledge graphs"], "paperhash": "tabacof|probability_calibration_for_knowledge_graph_embedding_models", "_bibtex": "@inproceedings{\nTabacof2020Probability,\ntitle={Probability Calibration for Knowledge Graph Embedding Models},\nauthor={Pedro Tabacof and Luca Costabello},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g8K1BFwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ed2d5653d860e524905339d2adcfcc0a9996bc65.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "qEuv1CdS4S", "original": null, "number": 1, "cdate": 1576798733821, "ddate": null, "tcdate": 1576798733821, "tmdate": 1576800902596, "tddate": null, "forum": "S1g8K1BFwS", "replyto": "S1g8K1BFwS", "invitation": "ICLR.cc/2020/Conference/Paper1841/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "The paper proposes a novel method to calibrate a knowledge graph embedding method when ground truth negatives are not available. Essentially, the method relies on generating corrupted triples as negative examples to be used by known approaches (Platt scaling and isotonic regression). \n\nThis is claimed as the first approach of probability calibration for knowledge graph embedding models, which is considered to be very relevant for practitioners working on knowledge graph embedding (although this is a narrow audience). The paper does not propose a wholly novel method for probability calibration. Instead, the value in experimental insights provided.\n\nSome reviewers would have liked to see a more in-depth analysis, but reviewers appreciated the thoroughness of the results in the clear articulation of the findings and the fact that multiple datasets and models are studied. \n\nThere was an animated discussion about this paper, but the paper seems a useful contribution to the ICLR community and I would like to recommend acceptance. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tabacof@gmail.com", "luca.costabello@accenture.com"], "title": "Probability Calibration for Knowledge Graph Embedding Models", "authors": ["Pedro Tabacof", "Luca Costabello"], "pdf": "/pdf/2124ce9449505128b4a25bddac7a0c984b70c4a6.pdf", "TL;DR": "We propose a novel method to calibrate knowledge graph embedding models without the need of negative examples.", "abstract": "Knowledge graph embedding research has overlooked the problem of probability calibration. We show popular embedding models are indeed uncalibrated. That means probability estimates associated to predicted triples are unreliable. We present a novel method to calibrate a model when ground truth negatives are not available, which is the usual case in knowledge graphs. We propose to use Platt scaling and isotonic regression alongside our method. Experiments on three datasets with ground truth negatives show our contribution leads to well calibrated models when compared to the gold standard of using negatives. We get significantly better results than the uncalibrated models from all calibration methods. We show isotonic regression offers the best the performance overall, not without trade-offs. We also show that calibrated models reach state-of-the-art accuracy without the need to define relation-specific decision thresholds.", "keywords": ["knowledge graph embeddings", "probability calibration", "calibration", "graph representation learning", "knowledge graphs"], "paperhash": "tabacof|probability_calibration_for_knowledge_graph_embedding_models", "_bibtex": "@inproceedings{\nTabacof2020Probability,\ntitle={Probability Calibration for Knowledge Graph Embedding Models},\nauthor={Pedro Tabacof and Luca Costabello},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g8K1BFwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ed2d5653d860e524905339d2adcfcc0a9996bc65.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "S1g8K1BFwS", "replyto": "S1g8K1BFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795709658, "tmdate": 1576800258470, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1841/-/Decision"}}}, {"id": "HkeLm0hpqH", "original": null, "number": 4, "cdate": 1572879901890, "ddate": null, "tcdate": 1572879901890, "tmdate": 1574430626224, "tddate": null, "forum": "S1g8K1BFwS", "replyto": "S1g8K1BFwS", "invitation": "ICLR.cc/2020/Conference/Paper1841/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #6", "review": "This is the first work that studies probability calibration for knowledge graph embedding models. In the case where ground-truth negatives are available the authors directly use off-the-shelf established calibration techniques (Platt scaling, isotonic regression). When ground-truth negatives are not available they propose to synthetically generate corrupted triples as negatives and use sample weights to guarantee that the frequencies adhere to the base rate.\n\nIn general the paper is well-written and easy to follow. Given that the paper's major contribution is experimental insight, and there are no major technical contributions, I would have liked to see a more in-depth analysis of how some of the key hyper-parameters influence the calibration of a model beyond the type of the loss, and beyond the correlation with embedding quality. Overall, I would be willing to increase the score if the authors perform a more comprehensive experimental analysis.\n\nSuggestions to improve the paper:\n1) I would expect that especially the negatives per positive ratio \\eta, and the dimensionality of the embeddings have a significant impact on model calibration. It would be valuable to experimentally quantify the impact of these key hyper-parameters.\n2) It is currently difficult to judge how well-calibrated are the models from the reliability diagrams/calibration plots since the total counts are not shown (e.g. total number of instances with mean predicted value between 0.4 and 0.5). That is, it could be that deviation from identity is due to small sample effects, i.e. we are estimating the fraction of positives from a handful of instances. Showing the total counts for each bin will help the reader better understand the calibration of the models.\n3) Several questions can be clarified regarding the sample weights:\n3.1) How essential is the proposed weighting scheme? How do the calibration techniques perform when using synthetic negatives with uniform sample weights?\n3.2) How does the proposed weighting scheme relate to the the general problem of calibrating models that have class imbalance?\n4.1) Can we observe significant difference in terms of calibration between translational distance models and semantic matching models, i.e. using distance-based scoring functions vs. using similarity-based scoring functions. If so is there any reason for that? To help answer this question the authors could compare additional models from each group (beyond the three models used in the paper). \n4.2) Are methods that represent entities as random variable to capture uncertainties (e.g. KG2E) better calibrated?\n5) Platt scaling assumes that per-class probabilities are normally distributed, while isotonic regression makes no assumption about the input probabilities. Given that Platt scaling performs worse in the experiments it would be interesting to investigate whether this can be (partly) explained by a deviation from the above assumption.\n6) Results reported in Table 3 are for WN11. It would be valuable to report similar results for the other datasets in the appendix.\n7) it would be beneficial to explore the different procedures proposed in the literature for generating synthetic negatives and their impact on the calibration. \n\nSuggestions to improve the paper that did not impact the score:\n1) On the triple classification task in Table 4, there is a significant gap between the literature results and the reproduced results on FB13 and YAGO39K. Is there an explanation for this? Furthermore, it would be interesting to investigate how much do the per-relation \\tau_i's deviate from 0.5 when they are learned using both non-calibrated and calibrated probabilities.\n2) In Eq. 6 after the second equality shouldn't there be \"N/(w- + N)\" instead of \"N/(w_{-} + PN)\"?  Is the additional P a typo?\n3) It would be nice to make the figures more readable (e.g. when printed in black and white) by using different markers for each line. \n\nEdit: Rating updated to 6 after rebuttal.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1841/AnonReviewer6"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1841/AnonReviewer6"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tabacof@gmail.com", "luca.costabello@accenture.com"], "title": "Probability Calibration for Knowledge Graph Embedding Models", "authors": ["Pedro Tabacof", "Luca Costabello"], "pdf": "/pdf/2124ce9449505128b4a25bddac7a0c984b70c4a6.pdf", "TL;DR": "We propose a novel method to calibrate knowledge graph embedding models without the need of negative examples.", "abstract": "Knowledge graph embedding research has overlooked the problem of probability calibration. We show popular embedding models are indeed uncalibrated. That means probability estimates associated to predicted triples are unreliable. We present a novel method to calibrate a model when ground truth negatives are not available, which is the usual case in knowledge graphs. We propose to use Platt scaling and isotonic regression alongside our method. Experiments on three datasets with ground truth negatives show our contribution leads to well calibrated models when compared to the gold standard of using negatives. We get significantly better results than the uncalibrated models from all calibration methods. We show isotonic regression offers the best the performance overall, not without trade-offs. We also show that calibrated models reach state-of-the-art accuracy without the need to define relation-specific decision thresholds.", "keywords": ["knowledge graph embeddings", "probability calibration", "calibration", "graph representation learning", "knowledge graphs"], "paperhash": "tabacof|probability_calibration_for_knowledge_graph_embedding_models", "_bibtex": "@inproceedings{\nTabacof2020Probability,\ntitle={Probability Calibration for Knowledge Graph Embedding Models},\nauthor={Pedro Tabacof and Luca Costabello},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g8K1BFwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ed2d5653d860e524905339d2adcfcc0a9996bc65.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1g8K1BFwS", "replyto": "S1g8K1BFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1841/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1841/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575570574423, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1841/Reviewers"], "noninvitees": [], "tcdate": 1570237731533, "tmdate": 1575570574436, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1841/-/Official_Review"}}}, {"id": "BkgKWAKc9S", "original": null, "number": 3, "cdate": 1572670977506, "ddate": null, "tcdate": 1572670977506, "tmdate": 1573924178000, "tddate": null, "forum": "S1g8K1BFwS", "replyto": "S1g8K1BFwS", "invitation": "ICLR.cc/2020/Conference/Paper1841/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #5", "review": "\n1. Summary\nThe paper studies probability calibration for three different knowledge graph embedding methods, with a focus on TransE evaluated on the task of knowledge graph triple classification. It studies Brier and log loss performance of Platt scaling and isotonic regression probability calibration on WN11 for TransE and claims that better calibration yields better performance as measured by mean reciprocal rank. Calibration plots for other datasets are also included as evidence. Furthermore, evidence is presented that probability calibration can lead to better performance for the task of triple classification. The main contributions of the paper also include the adaption of sampling techniques introduced by Bordes et al. (2013) adapted for estimating negatives for probability calibrations.\n\n2. Decision (See the updated decision in the comment below)\n\nProbability calibration is a very relevant issue, particularly in industry and when combining knowledge graph embedding models as external data in other models. Thus I see this work as a valuable contribution to the literature. In particular, I like the analysis from multiple views: Calibration plots, calibration metrics, and model performance. However, there is currently not enough evidence in the paper to make recommendations or judgments about when researchers and practitioners may want to use probability calibration. I also believe the datasets and models are not well tied into the literature, for example, in 2018/2019 I can find 3 papers for triple classification and 9 papers for link prediction as triple/entity ranking and from the data, it is not clear how probability calibration affects the latter. In the current state of the work, I recommend rejecting this work.\n\n3. Further supporting arguments\n\nAs a researcher and practitioner in this area, I know very well that predictions of most knowledge graph embedding models usually live near the decision boundary so that there is little difference between probability or score of a true positive and false positive. Also talking with people in industry, I heard that word embedding models are currently not that useful practically because they make too many useless predictions. This shows me that probability calibration is an important topic and I see this study as an important contribution to the field which is often mindlessly following evaluation metrics. \n\nHowever, from experience I also know that evaluation of knowledge graph embedding methods is not very reliable, that is people often get widely varying results and replication is difficult. Thus it is difficult to trust results if they are not well tied into the literature and compared against multiple datasets and models. This work focuses on three models TranseE, DistMult and ComplEx. DistMult and ComplEx have become models that are viewed as quite reliable to compare against. However, their performance is mostly studied on a learning-to-rank objective on datasets such as FB15k-237 and WN18RR. The authors report Brier score for their synthetic calibration method these datasets, but do not report any modeling results. Inclusions of results on these datasets would greatly improve this work. \n\nThe authors also currently focus on establishing that probability calibration improves the performance of the models. They claim that low Brier score or log loss are tied to good performance, but Pairwise and Multiclass-NLL loses achieve similar Brier/log loss performance while the MRR is double for Multiclass-NLL compared to the Pairwise loss. NLL and Multiclass NLL losses have similar MRR but very different Brier/log loss performance. As such I do not think this claim is sufficiently substantiated. I do not believe it is necessary to establish that better probability calibration is correlated with better model performance. I view the careful study of probability calibration and its effects per se as more useful.\nAs mentioned above, I also believe the results on WN11, FB13, and YAGO39k to not be sufficient to evaluate the effect of probability calibration.\n\n4. Additional feedback\n\nI really like this work. I think adding more results would make this paper great and I would be happy to change my acceptance decision.\n\nAs mentioned above I believe including results on FB15k-237 and WN18RR would make the results easier to interpret. Please also add more results to the table (no need to rerun those experiments, take them from other papers). I really like the analysis of Brier Score/Log loss and MRR. I think if you would extend this it would give very valuable insights into how probability calibration relates to performance.\n\nOne additional experiment which I do not deem critical, but which would improve your work further would be to tie probability calibration into a more practical setting. A setting that is also very interesting to researchers is if probability calibration would affect the results in tasks where you use word embedding models as an external \"knowledge source\". I really like  Kumar et al., 2019[1] since their word embedding model integrated into an entity linking model beats a strong BERT baseline. But I think a study of any task/model of your choice that integrates a knowledge embedding model would be a valuable addition to your work.\n\nAgain as I mentioned above, I do not believe it is critical to show improve performance on these tasks, a study of the effects of probability calibration is valuable in its own right. You might want to slightly pivot into this direction if you have sufficient evidence to make judgments about the effects of probability calibration.\n\nFurther small details: In the introduction, you make specific claims and justify them by citing a survey paper (Nickel et al., 2016). It would be easier for the reader to look up these claims in the source rather than in the survey paper. I believe there is a typo in your derivation in equation (6): the denominator of the second term should be just w-N + N or N(w- + 1).\n\n[1] Zero-shot Word Sense Disambiguation using Sense Definition Embeddings: https://www.aclweb.org/anthology/P19-1568/", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1841/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1841/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tabacof@gmail.com", "luca.costabello@accenture.com"], "title": "Probability Calibration for Knowledge Graph Embedding Models", "authors": ["Pedro Tabacof", "Luca Costabello"], "pdf": "/pdf/2124ce9449505128b4a25bddac7a0c984b70c4a6.pdf", "TL;DR": "We propose a novel method to calibrate knowledge graph embedding models without the need of negative examples.", "abstract": "Knowledge graph embedding research has overlooked the problem of probability calibration. We show popular embedding models are indeed uncalibrated. That means probability estimates associated to predicted triples are unreliable. We present a novel method to calibrate a model when ground truth negatives are not available, which is the usual case in knowledge graphs. We propose to use Platt scaling and isotonic regression alongside our method. Experiments on three datasets with ground truth negatives show our contribution leads to well calibrated models when compared to the gold standard of using negatives. We get significantly better results than the uncalibrated models from all calibration methods. We show isotonic regression offers the best the performance overall, not without trade-offs. We also show that calibrated models reach state-of-the-art accuracy without the need to define relation-specific decision thresholds.", "keywords": ["knowledge graph embeddings", "probability calibration", "calibration", "graph representation learning", "knowledge graphs"], "paperhash": "tabacof|probability_calibration_for_knowledge_graph_embedding_models", "_bibtex": "@inproceedings{\nTabacof2020Probability,\ntitle={Probability Calibration for Knowledge Graph Embedding Models},\nauthor={Pedro Tabacof and Luca Costabello},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g8K1BFwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ed2d5653d860e524905339d2adcfcc0a9996bc65.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1g8K1BFwS", "replyto": "S1g8K1BFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1841/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1841/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575570574423, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1841/Reviewers"], "noninvitees": [], "tcdate": 1570237731533, "tmdate": 1575570574436, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1841/-/Official_Review"}}}, {"id": "ryg1q3W3jr", "original": null, "number": 8, "cdate": 1573817479010, "ddate": null, "tcdate": 1573817479010, "tmdate": 1573817479010, "tddate": null, "forum": "S1g8K1BFwS", "replyto": "S1g8K1BFwS", "invitation": "ICLR.cc/2020/Conference/Paper1841/-/Official_Comment", "content": {"title": "Insightful reviews, thanks. Some general comments.", "comment": "Thank you for the comments. We have posted a personal reply to each reviewer. Let us also add a few general comments here:\n\n* [Usefulness and importance of calibration]: Calibration will not impact rank-order metrics for link prediction, such as MRR. The usefulness of calibration lies on being able to trust the output of knowledge graph embedding models and even quantify this trust. \nThis has great importance in practice when discovering new links in biological networks: better calibrated probabilities help human experts (biologists) validating discoveries and make a ML pipeline based on graph embeddings more trustworthy. \nAs shown in the paper, an additional minor application is in the task of triple classification, where calibrated probabilities replace the need to learn arbitrary per-relation decision thresholds.\n\n* [Novelty] We believe the paper is novel because it proposes the first framework for calibrating knowledge graph embedding models, something that to the best of our knowledge has never systematically been done before. It is certainly true that we rely on well-established calibration techniques, but we would like to point out that i) this is the first paper that covers this topic as first-class-citizen, and ii) the proposal of calibrating without positives is novel, as well as the use of sample weights to correct the distributional distortions created by the corruption generation procedure.\n\n\n* We have made the following changes to the paper:\n\nA) We expanded Table 3 results (impact of loss functions) in Sec 5.1 to include the other two datasets (FB13 and YAGO39K): We thank the reviewers for suggesting this additional experiment. In fact, we notice no correlation between calibration results and MRR. In other words, configurations that lead to the best predictive power are not necessary the best calibrated.\n\nB) We have clarified in 5.1 that better or worse calibration has no impact on ranking metrics such as MRR. We only evaluate the hypothesis of embedding quality being a common cause of both MRR and calibration quality.\n\nC) We added results and comments for two new experiments in appendix A.3, the impact of \\eta and the embedding size on calibration: results show among all that the embedding size has higher impact than \\eta.\n\nD) We added the histograms that show the total count of instances for each bin used in the calibration plots. Figures and comment are in appendix A.2.\n\nE) We have edited parts of the preliminary and related work, expanding the related work and condensing the preliminaries, as suggested. For example, now we mention KG2E (Gaussian embeddings) and many more additional recent papers.\n\nF) We have fixed the typo on equation 6, thanks.\n\nG) In the appendix, we added a table with MRR, MR, Hits@10 for all the datasets used. As pointed out in (B) above, we do not claim any causal relation of calibration on such task metrics.\n\nH) Added per-relation decision thresholds in appendix A.5.\n\nI) Move calibration-related preliminaries to appendix A.1.\n\nJ) All images are now black&white printout friendly.open"}, "signatures": ["ICLR.cc/2020/Conference/Paper1841/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1841/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tabacof@gmail.com", "luca.costabello@accenture.com"], "title": "Probability Calibration for Knowledge Graph Embedding Models", "authors": ["Pedro Tabacof", "Luca Costabello"], "pdf": "/pdf/2124ce9449505128b4a25bddac7a0c984b70c4a6.pdf", "TL;DR": "We propose a novel method to calibrate knowledge graph embedding models without the need of negative examples.", "abstract": "Knowledge graph embedding research has overlooked the problem of probability calibration. We show popular embedding models are indeed uncalibrated. That means probability estimates associated to predicted triples are unreliable. We present a novel method to calibrate a model when ground truth negatives are not available, which is the usual case in knowledge graphs. We propose to use Platt scaling and isotonic regression alongside our method. Experiments on three datasets with ground truth negatives show our contribution leads to well calibrated models when compared to the gold standard of using negatives. We get significantly better results than the uncalibrated models from all calibration methods. We show isotonic regression offers the best the performance overall, not without trade-offs. We also show that calibrated models reach state-of-the-art accuracy without the need to define relation-specific decision thresholds.", "keywords": ["knowledge graph embeddings", "probability calibration", "calibration", "graph representation learning", "knowledge graphs"], "paperhash": "tabacof|probability_calibration_for_knowledge_graph_embedding_models", "_bibtex": "@inproceedings{\nTabacof2020Probability,\ntitle={Probability Calibration for Knowledge Graph Embedding Models},\nauthor={Pedro Tabacof and Luca Costabello},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g8K1BFwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ed2d5653d860e524905339d2adcfcc0a9996bc65.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1g8K1BFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1841/Authors", "ICLR.cc/2020/Conference/Paper1841/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1841/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1841/Reviewers", "ICLR.cc/2020/Conference/Paper1841/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1841/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1841/Authors|ICLR.cc/2020/Conference/Paper1841/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504150122, "tmdate": 1576860535130, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1841/Authors", "ICLR.cc/2020/Conference/Paper1841/Reviewers", "ICLR.cc/2020/Conference/Paper1841/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1841/-/Official_Comment"}}}, {"id": "rJeJL3ZhsB", "original": null, "number": 7, "cdate": 1573817415213, "ddate": null, "tcdate": 1573817415213, "tmdate": 1573817415213, "tddate": null, "forum": "S1g8K1BFwS", "replyto": "BJgfSVlTKB", "invitation": "ICLR.cc/2020/Conference/Paper1841/-/Official_Comment", "content": {"title": "Thanks for your comments", "comment": "Thank you for the review. We have improved the preliminary section as suggested."}, "signatures": ["ICLR.cc/2020/Conference/Paper1841/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1841/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tabacof@gmail.com", "luca.costabello@accenture.com"], "title": "Probability Calibration for Knowledge Graph Embedding Models", "authors": ["Pedro Tabacof", "Luca Costabello"], "pdf": "/pdf/2124ce9449505128b4a25bddac7a0c984b70c4a6.pdf", "TL;DR": "We propose a novel method to calibrate knowledge graph embedding models without the need of negative examples.", "abstract": "Knowledge graph embedding research has overlooked the problem of probability calibration. We show popular embedding models are indeed uncalibrated. That means probability estimates associated to predicted triples are unreliable. We present a novel method to calibrate a model when ground truth negatives are not available, which is the usual case in knowledge graphs. We propose to use Platt scaling and isotonic regression alongside our method. Experiments on three datasets with ground truth negatives show our contribution leads to well calibrated models when compared to the gold standard of using negatives. We get significantly better results than the uncalibrated models from all calibration methods. We show isotonic regression offers the best the performance overall, not without trade-offs. We also show that calibrated models reach state-of-the-art accuracy without the need to define relation-specific decision thresholds.", "keywords": ["knowledge graph embeddings", "probability calibration", "calibration", "graph representation learning", "knowledge graphs"], "paperhash": "tabacof|probability_calibration_for_knowledge_graph_embedding_models", "_bibtex": "@inproceedings{\nTabacof2020Probability,\ntitle={Probability Calibration for Knowledge Graph Embedding Models},\nauthor={Pedro Tabacof and Luca Costabello},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g8K1BFwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ed2d5653d860e524905339d2adcfcc0a9996bc65.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1g8K1BFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1841/Authors", "ICLR.cc/2020/Conference/Paper1841/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1841/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1841/Reviewers", "ICLR.cc/2020/Conference/Paper1841/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1841/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1841/Authors|ICLR.cc/2020/Conference/Paper1841/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504150122, "tmdate": 1576860535130, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1841/Authors", "ICLR.cc/2020/Conference/Paper1841/Reviewers", "ICLR.cc/2020/Conference/Paper1841/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1841/-/Official_Comment"}}}, {"id": "HJxEVhZ3sB", "original": null, "number": 6, "cdate": 1573817388253, "ddate": null, "tcdate": 1573817388253, "tmdate": 1573817388253, "tddate": null, "forum": "S1g8K1BFwS", "replyto": "HJxwSZbY9r", "invitation": "ICLR.cc/2020/Conference/Paper1841/-/Official_Comment", "content": {"title": "Our paper novelty and other comments", "comment": "Major:\n\n1 [Novelty]. The paper is novel because it proposes the first framework for calibrating knowledge graph embedding models, something that to the best of our knowledge has never been done before, besides en passant comments in one workshop paper (Krompa\u00df and Tresp, 2015). The proposal of calibrating without positives is certainly a novel contribution as well, plus the use of sample weights to correct the distributional distortions created by the corruption generation procedure.\n\n2. [Shorten preliminaries]: We shortened the preliminaries as suggested, and moved calibration-related background to appendix A.1. Our main goal is first of all raise awareness on the problem of calibration in the graph representation learning community, and we believe some preliminaries are useful for better engagement with the reader, given that our community has not addressed the issue until now.\n\n3. [insufficient literature review] As pointed out at the beginning of Section 2, a comprehensive survey is out of the scope of this work. Nevertheless, we added many extra references in Section 2.\n\n\nMinor: \n\n[Table2, analysis of results]: yes, experiments show that in general Isotonic regression is better than Platt scaling. We are not sure to understand what reviewer #4 means by \"the results are again the conclusion. Is this because of the optimization issue?\". The message we want to send in Table2 is that our calibration works across  different datasets and models. We always obtain better Brier score and log loss. We also show that our heuristic based on synthetic negatives always obtains better calibration scores, regardless of the dataset and model adopted. We hope to have answered your doubts.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1841/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1841/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tabacof@gmail.com", "luca.costabello@accenture.com"], "title": "Probability Calibration for Knowledge Graph Embedding Models", "authors": ["Pedro Tabacof", "Luca Costabello"], "pdf": "/pdf/2124ce9449505128b4a25bddac7a0c984b70c4a6.pdf", "TL;DR": "We propose a novel method to calibrate knowledge graph embedding models without the need of negative examples.", "abstract": "Knowledge graph embedding research has overlooked the problem of probability calibration. We show popular embedding models are indeed uncalibrated. That means probability estimates associated to predicted triples are unreliable. We present a novel method to calibrate a model when ground truth negatives are not available, which is the usual case in knowledge graphs. We propose to use Platt scaling and isotonic regression alongside our method. Experiments on three datasets with ground truth negatives show our contribution leads to well calibrated models when compared to the gold standard of using negatives. We get significantly better results than the uncalibrated models from all calibration methods. We show isotonic regression offers the best the performance overall, not without trade-offs. We also show that calibrated models reach state-of-the-art accuracy without the need to define relation-specific decision thresholds.", "keywords": ["knowledge graph embeddings", "probability calibration", "calibration", "graph representation learning", "knowledge graphs"], "paperhash": "tabacof|probability_calibration_for_knowledge_graph_embedding_models", "_bibtex": "@inproceedings{\nTabacof2020Probability,\ntitle={Probability Calibration for Knowledge Graph Embedding Models},\nauthor={Pedro Tabacof and Luca Costabello},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g8K1BFwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ed2d5653d860e524905339d2adcfcc0a9996bc65.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1g8K1BFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1841/Authors", "ICLR.cc/2020/Conference/Paper1841/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1841/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1841/Reviewers", "ICLR.cc/2020/Conference/Paper1841/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1841/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1841/Authors|ICLR.cc/2020/Conference/Paper1841/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504150122, "tmdate": 1576860535130, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1841/Authors", "ICLR.cc/2020/Conference/Paper1841/Reviewers", "ICLR.cc/2020/Conference/Paper1841/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1841/-/Official_Comment"}}}, {"id": "rygfk2bhoS", "original": null, "number": 5, "cdate": 1573817305581, "ddate": null, "tcdate": 1573817305581, "tmdate": 1573817305581, "tddate": null, "forum": "S1g8K1BFwS", "replyto": "BkgKWAKc9S", "invitation": "ICLR.cc/2020/Conference/Paper1841/-/Official_Comment", "content": {"title": "Our reply to your suggestions", "comment": "\n- [Unsubstantiated claim: impact of calibration on link prediction] There is a small misunderstanding: we did not claim there is a causal effect between calibration and rank metrics such as MR, MRR or Hits@K (in fact, calibration does not change the rank-order of the link prediction results). Our initial submission limited to hint at a possible correlation between calibration scores and MRR (Table3 caption). \nIn fact, as suggested by reviewer #4, we carried out additional experiments which are included in the latest revision attached, and our updated results in Table 3 suggest that there is no correlation between calibration results and MRR, i.e. \"better\" embeddings (i.e. embeddings that lead to higher link prediction MRR) are not necessarily easier to calibrate. \nWe have clarified this in the main text (Sect 5.1 and Table3 caption). \n\nOn the other hand, calibration does affect triple classification. More precisely, it affects the way we choose the decision threshold \\tau. We show that with calibrated probabilities you only need one natural decision threshold \\tau=0.5 to maximize accuracy, while other methods require arbitrary per-relation thresholds.\n\n\n- [no evidence of real-world use of calibration] As pointed out in the introduction, the usefulness of calibration lies on being able to trust the output of knowledge graph embedding models and even quantify this trust. This has great importance when discovery new links in biological networks: better calibrated probabilities help human experts (biologists) validating discoveries and make a ML pipeline based on graph embeddings more trustworthy. \nMoreover, a minor application is triple classification, where calibrated probabilities replace per-relation thresholds (Section 5.1, Table 4).\nWe improved the part in the introduction where we talk about real-world examples of why calibration is important.\n\n- [insufficient literature review] As pointed out at the beginning of Section 2, a comprehensive survey is out of the scope of this work. However, we have added extra references and enriched the prior art section.\n\n- [Main claim of the paper is calibration study]: We would like to point out again that this is the only goal of our work. We do not aim at showing causality between calibration and predictive power - as stated above. We have made clearer this point in the text (Sect 5.1). \n\n- [Add link prediction metrics results] In the appendix, we added a table with MRR, MR, Hits@10 for all the datasets used. As pointed out in A.1) above, we do not claim any causal relation of calibration on such task metrics.\n\nMinor comments:\n\n[Typo in Equation 6] Fixed, thanks.\n\n[Reference to Nickel et al. 2016 in introduction] We only refer to this work to point the reader to the first paper to suggest the use of a sigmoid function to turn scores into probabilities.\n\n[Additional experiments, knowledge injection]: in fact, calibration does not affect the embeddings per-se, as it consists in a downstream operation carried out after training. If Platt scaling is adopted, then new weights are learned, but these are separate from the embeddings, which are not touched at this stage. That means calibration will not have any impact on such experiment.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1841/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1841/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tabacof@gmail.com", "luca.costabello@accenture.com"], "title": "Probability Calibration for Knowledge Graph Embedding Models", "authors": ["Pedro Tabacof", "Luca Costabello"], "pdf": "/pdf/2124ce9449505128b4a25bddac7a0c984b70c4a6.pdf", "TL;DR": "We propose a novel method to calibrate knowledge graph embedding models without the need of negative examples.", "abstract": "Knowledge graph embedding research has overlooked the problem of probability calibration. We show popular embedding models are indeed uncalibrated. That means probability estimates associated to predicted triples are unreliable. We present a novel method to calibrate a model when ground truth negatives are not available, which is the usual case in knowledge graphs. We propose to use Platt scaling and isotonic regression alongside our method. Experiments on three datasets with ground truth negatives show our contribution leads to well calibrated models when compared to the gold standard of using negatives. We get significantly better results than the uncalibrated models from all calibration methods. We show isotonic regression offers the best the performance overall, not without trade-offs. We also show that calibrated models reach state-of-the-art accuracy without the need to define relation-specific decision thresholds.", "keywords": ["knowledge graph embeddings", "probability calibration", "calibration", "graph representation learning", "knowledge graphs"], "paperhash": "tabacof|probability_calibration_for_knowledge_graph_embedding_models", "_bibtex": "@inproceedings{\nTabacof2020Probability,\ntitle={Probability Calibration for Knowledge Graph Embedding Models},\nauthor={Pedro Tabacof and Luca Costabello},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g8K1BFwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ed2d5653d860e524905339d2adcfcc0a9996bc65.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1g8K1BFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1841/Authors", "ICLR.cc/2020/Conference/Paper1841/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1841/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1841/Reviewers", "ICLR.cc/2020/Conference/Paper1841/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1841/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1841/Authors|ICLR.cc/2020/Conference/Paper1841/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504150122, "tmdate": 1576860535130, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1841/Authors", "ICLR.cc/2020/Conference/Paper1841/Reviewers", "ICLR.cc/2020/Conference/Paper1841/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1841/-/Official_Comment"}}}, {"id": "HygS2oZnjr", "original": null, "number": 4, "cdate": 1573817261350, "ddate": null, "tcdate": 1573817261350, "tmdate": 1573817261350, "tddate": null, "forum": "S1g8K1BFwS", "replyto": "HkeLm0hpqH", "invitation": "ICLR.cc/2020/Conference/Paper1841/-/Official_Comment", "content": {"title": "Our reply to your suggestions", "comment": "Main points:\n\n1. We ran experiments to assess the impact of the embedding size and negatives/positive ratio \\eta. We added such additional results to appendix A.3. Results show that the embedding size has higher impact than the negative/positive ratio \\eta. We observe that calibrated and uncalibrated low-dimensional embeddings have worse Brier score. Results also show that any k>50 does not improve calibration anymore. The negative/positive ratio \\eta follows a similar pattern: choosing \\eta>10 does not have any effect on the calibration score.\n\n2. In appendix A.2 We added histograms that show the total count of instances for each bin used in the calibration plots. As expected, calibration considerably helps spreading out instances across bins, whereas in uncalibrated scenarios instances are squeezed in the first or last bins.\n\n3.1. Without sample weights, the base rate will be determined implicitly by the negatives/positive ratio eta used for calibration. For example, if we use eta=3 for calibration, this implies a positive base rate \\alpha=25%. As this base rate will most likely be wrong, calibration without sample weights leads to meaningless results. \n\n3.2. Sample weights allow the user to balance the positives and negatives in a way he or she sees fit for the problem, independent from choices such as the calibration eta (when using corruptions for the calibration). This is indeed similar to dealing with imbalanced datasets, especially in the case where the training dataset distribution does not match the expected test / deployment distribution.\n\n4.1. We added extra experiments with HolE, another model implemented in the library we used for the experiments. We can certainly add experiments on other models, but as such implementations do not belong to the same codebase, we fear we will most likely end up with unfair comparisons (as you know this is a well-known problem in this community). All in all, the set of models we used is quite diverse (translation-based, tensor-decomposition-based, with different scoring functions), and it is well representative of models actually used in the wild by practitioners, even outside the boundaries of our community.\n\n4.2. That is an interesting direction for future work. While KG2E proposes to use Gaussian distributed embeddings to account for the uncertainty, their model does not provide the probability of a triple being true, so KG2E would also benefit from the output calibration procedure we propose here. It is an open question how to design embedding methods that naturally lead to well-calibrated probabilities. \n\n5. Platt scaling was developed originally to calibrate SVMs (Platt et al., 1999), where the output is not a probability, but a continuous score, which is similar to the output scores in knowledge graph embeddings. We could not find a reference that asserts the need of normally distributed class probabilities. Perhaps it was meant that the logits need to be normally distributed, given the connection between Platt scaling and logistic regression. If so, that indeed points to a new direction to investigate the limits of our proposed framework.\n\n6. We have added two extra tables in 5.1 with additional results for FB13 and YAGO39k.\n\n7. We have tried two variations of corruption generation, all entities and per-batch entities, without any significant changes to the results. We have clarified this in the main text (Sec 4, footnote). We also pointed out that future experiments will experiment with techniques proposed by  (Kotnis and Nastase 2017).\n\nExtra points:\n\n1. Note that our goal was focusing on calibration, and not on achieving better predictive power. We have tried minimal random search on the hyperparameters without significant effect on triple classification results and we still could not reproduce the SOTA for FB13 and YAGO39K. Thus, we did not change the results of Table 4, besides adding the new HolE model. Some results in Table 4 incidentally achieve SOTA results, but we would rather leave the problem of achieving better predictive power aside.\n\n1.1 Table 4 caption states that \"for all calibration methods there is one single threshold \\tau=0.5\". We have added this to the header of Table 4 as well, for clarity. We added the per-relation decision thresholds in the appendix A.5. Note that the thresholds reported in A.5 are not probabilities, as they have been applied to the raw scores returned by the model-dependent scoring functions.\n\n2. Fixed, thanks.\n\n3. We made the figures more readable and printout friendly, as requested."}, "signatures": ["ICLR.cc/2020/Conference/Paper1841/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1841/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tabacof@gmail.com", "luca.costabello@accenture.com"], "title": "Probability Calibration for Knowledge Graph Embedding Models", "authors": ["Pedro Tabacof", "Luca Costabello"], "pdf": "/pdf/2124ce9449505128b4a25bddac7a0c984b70c4a6.pdf", "TL;DR": "We propose a novel method to calibrate knowledge graph embedding models without the need of negative examples.", "abstract": "Knowledge graph embedding research has overlooked the problem of probability calibration. We show popular embedding models are indeed uncalibrated. That means probability estimates associated to predicted triples are unreliable. We present a novel method to calibrate a model when ground truth negatives are not available, which is the usual case in knowledge graphs. We propose to use Platt scaling and isotonic regression alongside our method. Experiments on three datasets with ground truth negatives show our contribution leads to well calibrated models when compared to the gold standard of using negatives. We get significantly better results than the uncalibrated models from all calibration methods. We show isotonic regression offers the best the performance overall, not without trade-offs. We also show that calibrated models reach state-of-the-art accuracy without the need to define relation-specific decision thresholds.", "keywords": ["knowledge graph embeddings", "probability calibration", "calibration", "graph representation learning", "knowledge graphs"], "paperhash": "tabacof|probability_calibration_for_knowledge_graph_embedding_models", "_bibtex": "@inproceedings{\nTabacof2020Probability,\ntitle={Probability Calibration for Knowledge Graph Embedding Models},\nauthor={Pedro Tabacof and Luca Costabello},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g8K1BFwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ed2d5653d860e524905339d2adcfcc0a9996bc65.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1g8K1BFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1841/Authors", "ICLR.cc/2020/Conference/Paper1841/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1841/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1841/Reviewers", "ICLR.cc/2020/Conference/Paper1841/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1841/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1841/Authors|ICLR.cc/2020/Conference/Paper1841/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504150122, "tmdate": 1576860535130, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1841/Authors", "ICLR.cc/2020/Conference/Paper1841/Reviewers", "ICLR.cc/2020/Conference/Paper1841/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1841/-/Official_Comment"}}}, {"id": "BJgfSVlTKB", "original": null, "number": 1, "cdate": 1571779641628, "ddate": null, "tcdate": 1571779641628, "tmdate": 1572972416744, "tddate": null, "forum": "S1g8K1BFwS", "replyto": "S1g8K1BFwS", "invitation": "ICLR.cc/2020/Conference/Paper1841/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, the authors deal with the calibration problem in graph embedding models. They used Platt scaling and isotonic regression in the situation when there is ground truth negatives. They also work when there is no ground truth negatives. In this situation, they proposed a calibration heuristics for synthetically generated negatives. Overall, the approach is not very innovative, but the problem they tackled is under studied. The presentation of the whole paper is ok, although it falls onto preliminary side. Since I did not identify any technical problem so far, I will vote for a weak acceptance, unless I observe more technical issues during the discussion.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1841/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1841/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tabacof@gmail.com", "luca.costabello@accenture.com"], "title": "Probability Calibration for Knowledge Graph Embedding Models", "authors": ["Pedro Tabacof", "Luca Costabello"], "pdf": "/pdf/2124ce9449505128b4a25bddac7a0c984b70c4a6.pdf", "TL;DR": "We propose a novel method to calibrate knowledge graph embedding models without the need of negative examples.", "abstract": "Knowledge graph embedding research has overlooked the problem of probability calibration. We show popular embedding models are indeed uncalibrated. That means probability estimates associated to predicted triples are unreliable. We present a novel method to calibrate a model when ground truth negatives are not available, which is the usual case in knowledge graphs. We propose to use Platt scaling and isotonic regression alongside our method. Experiments on three datasets with ground truth negatives show our contribution leads to well calibrated models when compared to the gold standard of using negatives. We get significantly better results than the uncalibrated models from all calibration methods. We show isotonic regression offers the best the performance overall, not without trade-offs. We also show that calibrated models reach state-of-the-art accuracy without the need to define relation-specific decision thresholds.", "keywords": ["knowledge graph embeddings", "probability calibration", "calibration", "graph representation learning", "knowledge graphs"], "paperhash": "tabacof|probability_calibration_for_knowledge_graph_embedding_models", "_bibtex": "@inproceedings{\nTabacof2020Probability,\ntitle={Probability Calibration for Knowledge Graph Embedding Models},\nauthor={Pedro Tabacof and Luca Costabello},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g8K1BFwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ed2d5653d860e524905339d2adcfcc0a9996bc65.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1g8K1BFwS", "replyto": "S1g8K1BFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1841/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1841/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575570574423, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1841/Reviewers"], "noninvitees": [], "tcdate": 1570237731533, "tmdate": 1575570574436, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1841/-/Official_Review"}}}, {"id": "HJxwSZbY9r", "original": null, "number": 2, "cdate": 1572569406885, "ddate": null, "tcdate": 1572569406885, "tmdate": 1572972416700, "tddate": null, "forum": "S1g8K1BFwS", "replyto": "S1g8K1BFwS", "invitation": "ICLR.cc/2020/Conference/Paper1841/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper focuses on the calibration of the knowledge graph embedding task with Platt scaling and isotonic regression. This paper is well-written, well-motivated and well-organized. However, my major concern is the novelty of this paper or the contribution.\n\nMajor Concerns:\n\n1. This paper lacks novelty. In this paper, the authors only apply the existing techniques (e.g. Platt Scaling, Isotonic Regression) to tackle the calibration issue, which makes a minor contribution. I suggest that the authors could provide their own method specified to knowledge graph tasks rather than leverage the off-shelf methods.\n\n2. The related work could be enhanced, while the preliminaries could be reduced. Actually, in the area of knowledge graph or natural language processing, the preliminary of this paper is a bit trivial. \n\nMinor Concerns:\n\n1. In Table 2, we can conclude that Iso will be better than Platt in general. However, in the case of FB13 (ComplEx) and YAGO (TransE), the results are again the conclusion. Is this because of the optimization issue? I suggest the authors clearly state the experimental analysis."}, "signatures": ["ICLR.cc/2020/Conference/Paper1841/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1841/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tabacof@gmail.com", "luca.costabello@accenture.com"], "title": "Probability Calibration for Knowledge Graph Embedding Models", "authors": ["Pedro Tabacof", "Luca Costabello"], "pdf": "/pdf/2124ce9449505128b4a25bddac7a0c984b70c4a6.pdf", "TL;DR": "We propose a novel method to calibrate knowledge graph embedding models without the need of negative examples.", "abstract": "Knowledge graph embedding research has overlooked the problem of probability calibration. We show popular embedding models are indeed uncalibrated. That means probability estimates associated to predicted triples are unreliable. We present a novel method to calibrate a model when ground truth negatives are not available, which is the usual case in knowledge graphs. We propose to use Platt scaling and isotonic regression alongside our method. Experiments on three datasets with ground truth negatives show our contribution leads to well calibrated models when compared to the gold standard of using negatives. We get significantly better results than the uncalibrated models from all calibration methods. We show isotonic regression offers the best the performance overall, not without trade-offs. We also show that calibrated models reach state-of-the-art accuracy without the need to define relation-specific decision thresholds.", "keywords": ["knowledge graph embeddings", "probability calibration", "calibration", "graph representation learning", "knowledge graphs"], "paperhash": "tabacof|probability_calibration_for_knowledge_graph_embedding_models", "_bibtex": "@inproceedings{\nTabacof2020Probability,\ntitle={Probability Calibration for Knowledge Graph Embedding Models},\nauthor={Pedro Tabacof and Luca Costabello},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g8K1BFwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ed2d5653d860e524905339d2adcfcc0a9996bc65.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1g8K1BFwS", "replyto": "S1g8K1BFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1841/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1841/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575570574423, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1841/Reviewers"], "noninvitees": [], "tcdate": 1570237731533, "tmdate": 1575570574436, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1841/-/Official_Review"}}}], "count": 11}