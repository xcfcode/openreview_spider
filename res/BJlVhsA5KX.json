{"notes": [{"id": "BJlVhsA5KX", "original": "BylINMa5Ym", "number": 701, "cdate": 1538087851879, "ddate": null, "tcdate": 1538087851879, "tmdate": 1545355401291, "tddate": null, "forum": "BJlVhsA5KX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Sequenced-Replacement Sampling for Deep Learning", "abstract": "We propose sequenced-replacement sampling (SRS) for training deep neural networks. The basic idea is to assign a fixed sequence index to each sample in the dataset. Once a mini-batch is randomly drawn in each training iteration, we refill the original dataset by successively adding samples according to their sequence index. Thus we carry out replacement sampling but in a batched and sequenced way. In a sense, SRS could be viewed as a way of performing \"mini-batch augmentation\". It is particularly useful for a task where we have a relatively small images-per-class such as CIFAR-100. Together with a longer period of initial large learning rate, it significantly improves the classification accuracy in CIFAR-100 over the current state-of-the-art results. Our experiments indicate that training deeper networks with SRS is less prone to over-fitting. In the best case, we achieve an error rate as low as 10.10%.", "keywords": ["deep neural networks", "stochastic gradient descent", "sequenced-replacement sampling"], "authorids": ["chiuman100@gmail.com", "pdhvip@gmail.com", "wei.yang2@huawei.com", "yichang@acm.org"], "authors": ["Chiu Man Ho", "Dae Hoon Park", "Wei Yang", "Yi Chang"], "TL;DR": "Proposed a novel way (without adding new parameters) of training deep neural network in order to improve generalization, especially for the case where we have relatively small images-per-class.", "pdf": "/pdf/d5e0c948a797ea18533ba50e7c7f8e167e9b71f1.pdf", "paperhash": "ho|sequencedreplacement_sampling_for_deep_learning", "_bibtex": "@misc{\nho2019sequencedreplacement,\ntitle={Sequenced-Replacement Sampling for Deep Learning},\nauthor={Chiu Man Ho and Dae Hoon Park and Wei Yang and Yi Chang},\nyear={2019},\nurl={https://openreview.net/forum?id=BJlVhsA5KX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rylYjfNlg4", "original": null, "number": 1, "cdate": 1544729249333, "ddate": null, "tcdate": 1544729249333, "tmdate": 1545354511430, "tddate": null, "forum": "BJlVhsA5KX", "replyto": "BJlVhsA5KX", "invitation": "ICLR.cc/2019/Conference/-/Paper701/Meta_Review", "content": {"metareview": "This paper proposes a new batching strategy for training deep nets. The idea is to have the properties of sampling with replacement while reducing the chance of not touching an example in a given epoch. Experimental results show that this can improve performance on one of the tasks considered. However the reviewers consistently agree that the experimental validation of this work is much too limited. Furthermore the motivation for the approach should be more clearly established.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Meta-review"}, "signatures": ["ICLR.cc/2019/Conference/Paper701/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper701/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sequenced-Replacement Sampling for Deep Learning", "abstract": "We propose sequenced-replacement sampling (SRS) for training deep neural networks. The basic idea is to assign a fixed sequence index to each sample in the dataset. Once a mini-batch is randomly drawn in each training iteration, we refill the original dataset by successively adding samples according to their sequence index. Thus we carry out replacement sampling but in a batched and sequenced way. In a sense, SRS could be viewed as a way of performing \"mini-batch augmentation\". It is particularly useful for a task where we have a relatively small images-per-class such as CIFAR-100. Together with a longer period of initial large learning rate, it significantly improves the classification accuracy in CIFAR-100 over the current state-of-the-art results. Our experiments indicate that training deeper networks with SRS is less prone to over-fitting. In the best case, we achieve an error rate as low as 10.10%.", "keywords": ["deep neural networks", "stochastic gradient descent", "sequenced-replacement sampling"], "authorids": ["chiuman100@gmail.com", "pdhvip@gmail.com", "wei.yang2@huawei.com", "yichang@acm.org"], "authors": ["Chiu Man Ho", "Dae Hoon Park", "Wei Yang", "Yi Chang"], "TL;DR": "Proposed a novel way (without adding new parameters) of training deep neural network in order to improve generalization, especially for the case where we have relatively small images-per-class.", "pdf": "/pdf/d5e0c948a797ea18533ba50e7c7f8e167e9b71f1.pdf", "paperhash": "ho|sequencedreplacement_sampling_for_deep_learning", "_bibtex": "@misc{\nho2019sequencedreplacement,\ntitle={Sequenced-Replacement Sampling for Deep Learning},\nauthor={Chiu Man Ho and Dae Hoon Park and Wei Yang and Yi Chang},\nyear={2019},\nurl={https://openreview.net/forum?id=BJlVhsA5KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper701/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353119901, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJlVhsA5KX", "replyto": "BJlVhsA5KX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper701/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper701/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper701/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353119901}}}, {"id": "H1egDgd2hm", "original": null, "number": 3, "cdate": 1541337176245, "ddate": null, "tcdate": 1541337176245, "tmdate": 1541533761498, "tddate": null, "forum": "BJlVhsA5KX", "replyto": "BJlVhsA5KX", "invitation": "ICLR.cc/2019/Conference/-/Paper701/Official_Review", "content": {"title": "More work is needed", "review": "The paper suggests a new way of sampling mini-batches for training deep neural nets. The idea is to first index the samples then select the batches during training in a sequential way. The proposed method is tested on the CIFAR dataset and some improvement on the classification accuracy is reported.\n\nI find the idea interesting but feel that much more is needed in order to have a better understanding of how the proposed method works, when it works and when it doesn't work. Some theoretical insight, or at least a more systematic experimental study, is needed to justify the proposed method.  ", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper701/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sequenced-Replacement Sampling for Deep Learning", "abstract": "We propose sequenced-replacement sampling (SRS) for training deep neural networks. The basic idea is to assign a fixed sequence index to each sample in the dataset. Once a mini-batch is randomly drawn in each training iteration, we refill the original dataset by successively adding samples according to their sequence index. Thus we carry out replacement sampling but in a batched and sequenced way. In a sense, SRS could be viewed as a way of performing \"mini-batch augmentation\". It is particularly useful for a task where we have a relatively small images-per-class such as CIFAR-100. Together with a longer period of initial large learning rate, it significantly improves the classification accuracy in CIFAR-100 over the current state-of-the-art results. Our experiments indicate that training deeper networks with SRS is less prone to over-fitting. In the best case, we achieve an error rate as low as 10.10%.", "keywords": ["deep neural networks", "stochastic gradient descent", "sequenced-replacement sampling"], "authorids": ["chiuman100@gmail.com", "pdhvip@gmail.com", "wei.yang2@huawei.com", "yichang@acm.org"], "authors": ["Chiu Man Ho", "Dae Hoon Park", "Wei Yang", "Yi Chang"], "TL;DR": "Proposed a novel way (without adding new parameters) of training deep neural network in order to improve generalization, especially for the case where we have relatively small images-per-class.", "pdf": "/pdf/d5e0c948a797ea18533ba50e7c7f8e167e9b71f1.pdf", "paperhash": "ho|sequencedreplacement_sampling_for_deep_learning", "_bibtex": "@misc{\nho2019sequencedreplacement,\ntitle={Sequenced-Replacement Sampling for Deep Learning},\nauthor={Chiu Man Ho and Dae Hoon Park and Wei Yang and Yi Chang},\nyear={2019},\nurl={https://openreview.net/forum?id=BJlVhsA5KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper701/Official_Review", "cdate": 1542234399521, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJlVhsA5KX", "replyto": "BJlVhsA5KX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper701/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335783878, "tmdate": 1552335783878, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper701/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1lYj1Hcn7", "original": null, "number": 2, "cdate": 1541193632582, "ddate": null, "tcdate": 1541193632582, "tmdate": 1541533761294, "tddate": null, "forum": "BJlVhsA5KX", "replyto": "BJlVhsA5KX", "invitation": "ICLR.cc/2019/Conference/-/Paper701/Official_Review", "content": {"title": "The approach requires more validation", "review": "This paper constructs a very simple, new scheme for sampling mini-batches. It aims to (i) achieve the noise properties of sampling with replacement while (ii) reduce the probability of not touching a sample in a given epoch. The result is a biased sampling scheme called \u201csequenced-replacement sampling (SRS)\u201d. Experimental results show that this scheme performs significantly better than a standard baseline on CIFAR-100 with minor improvements on CIFAR-10. \n\nThis is a highly empirical paper that presents a simple and sound method for mini-batch sampling with impressive results on CIFAR-100. It however needs more thorough analysis or experiments that validate the ideas as also experiments on harder, large-scale datasets.\n\nDetailed comments:\n\n1. The authors are motivated by the exploration properties of sampling with replacement which I find quite vague. For instance, https://arxiv.org/abs/1710.11029 , https://arxiv.org/abs/1705.07562 etc. show that sampling mini-batches with replacement has a large variance than sampling without replacement and consequently SGD has better regularization properties. Also, for mini-batches sampled with replacement, the probability of not sampling a given sample across an epoch is very small.\n\n2. I believe the sampling scheme is unnecessarily complicated. Why not draw samples with replacement with a probability p and draw samples without replacement with a probability 1-p? Do the experimental results remain consistent with this more natural sampling scheme which also aligns with the motivations of the authors?\n\n3. To validate the claim that SRS works well when there are fewer examples per class, can you do ablation experiments on CIFAR-10 or ImageNet/restricted subset of it?", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper701/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sequenced-Replacement Sampling for Deep Learning", "abstract": "We propose sequenced-replacement sampling (SRS) for training deep neural networks. The basic idea is to assign a fixed sequence index to each sample in the dataset. Once a mini-batch is randomly drawn in each training iteration, we refill the original dataset by successively adding samples according to their sequence index. Thus we carry out replacement sampling but in a batched and sequenced way. In a sense, SRS could be viewed as a way of performing \"mini-batch augmentation\". It is particularly useful for a task where we have a relatively small images-per-class such as CIFAR-100. Together with a longer period of initial large learning rate, it significantly improves the classification accuracy in CIFAR-100 over the current state-of-the-art results. Our experiments indicate that training deeper networks with SRS is less prone to over-fitting. In the best case, we achieve an error rate as low as 10.10%.", "keywords": ["deep neural networks", "stochastic gradient descent", "sequenced-replacement sampling"], "authorids": ["chiuman100@gmail.com", "pdhvip@gmail.com", "wei.yang2@huawei.com", "yichang@acm.org"], "authors": ["Chiu Man Ho", "Dae Hoon Park", "Wei Yang", "Yi Chang"], "TL;DR": "Proposed a novel way (without adding new parameters) of training deep neural network in order to improve generalization, especially for the case where we have relatively small images-per-class.", "pdf": "/pdf/d5e0c948a797ea18533ba50e7c7f8e167e9b71f1.pdf", "paperhash": "ho|sequencedreplacement_sampling_for_deep_learning", "_bibtex": "@misc{\nho2019sequencedreplacement,\ntitle={Sequenced-Replacement Sampling for Deep Learning},\nauthor={Chiu Man Ho and Dae Hoon Park and Wei Yang and Yi Chang},\nyear={2019},\nurl={https://openreview.net/forum?id=BJlVhsA5KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper701/Official_Review", "cdate": 1542234399521, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJlVhsA5KX", "replyto": "BJlVhsA5KX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper701/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335783878, "tmdate": 1552335783878, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper701/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "r1lhSo5w3m", "original": null, "number": 1, "cdate": 1541020484477, "ddate": null, "tcdate": 1541020484477, "tmdate": 1541533761086, "tddate": null, "forum": "BJlVhsA5KX", "replyto": "BJlVhsA5KX", "invitation": "ICLR.cc/2019/Conference/-/Paper701/Official_Review", "content": {"title": "Needs More Work", "review": "In this paper, the authors introduce a sampling strategy that aims to combine the benefits of with- and without-replacement SGD. With-replacement strategies add more randomness to the process, which the authors claim helps convergence, while the without-replacement strategies ensure equal usage of all datapoints. The authors present numerical results showing better convergence and improved final accuracy. While I found the idea appealing, I felt that the paper needs more work before it can be published. I detail some of my primary concerns below:\n\n- The entire motivation of the paper is predicated on the hypothesis that more randomness is better for training. This is not generally true. Past work has shown that specific kinds of random noise aid convergence through exploration/saddle point avoidance/escaping spurious minima while others either make no change, or hurt. Noise from sampling tends to be a structured noise that aids exploration/convergence over batch gradient descent, but it is not immediately clear to me why the choice between with- and without-replacement should imply exploration. \n\n- Maybe it's obvious but I'm not grasping why, mathematically, the number of accessible configurations for SRS is the same as original replacement sampling (4th paragraph on page 3).\n\n- Given that the central motivation of the work was to enable with-replacement strategies while still ensuring equal usage, I recommend that the authors include a histogram of datapoint usage for three strategies (with, without, hybrid). This should help convince the reader that the SRS indeed improves upon the usage statistics of with replacement. \n\n- If one were to create a hybrid sampling strategy, one that is natural is doing 50-50 sampling with and without replacement. In other words, for a batch size of 64, say, 32 are sampled with replacement and 32 without. By changing the ratio, you can also control what end of the sampling spectrum you want to be on. Did you try such a strategy? \n\n- For the numerical experiments, as I see it, there are 3 differences between the SRS setup and the baseline: location of batch normalization, learning rate, and batch size. The authors show (at the bottom of Page 6) that the performance boost does not come from learning rate or mini-batch size, but what about the placement of the BN layer? Seems like that still remains as a confounding factor?\n\n- \"SRS leads to much more fluctuations, and hence significantly more covariate shift\". How do the authors define covariate shift? Can the authors substantiate this claim theoretically/empirically?\n\n- The authors claim that the method works better when the dataset size is low compared to number of classes. Again, can the authors substantiate this claim theoretically/empirically? Maybe you can try running a sub-sampled version of CIFAR-10/100 with the baselines?\n\n- The writing in the paper needs improving. A few sample phrases that need editing: \"smaller mini-batch means a larger approximation\", \"more accessible configurations of mini-batches\", \"hence more exploration-induction\", \"less optimal local minimum\"\n\n- Minor comment: why is the queue filled with repeated samples? In Figure 1, why not have the system initialized with 1 2 3 in the pool and 4 5 in the queue? Seems like by repeating, there is an unnecessary bias towards those datapoints.  ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper701/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sequenced-Replacement Sampling for Deep Learning", "abstract": "We propose sequenced-replacement sampling (SRS) for training deep neural networks. The basic idea is to assign a fixed sequence index to each sample in the dataset. Once a mini-batch is randomly drawn in each training iteration, we refill the original dataset by successively adding samples according to their sequence index. Thus we carry out replacement sampling but in a batched and sequenced way. In a sense, SRS could be viewed as a way of performing \"mini-batch augmentation\". It is particularly useful for a task where we have a relatively small images-per-class such as CIFAR-100. Together with a longer period of initial large learning rate, it significantly improves the classification accuracy in CIFAR-100 over the current state-of-the-art results. Our experiments indicate that training deeper networks with SRS is less prone to over-fitting. In the best case, we achieve an error rate as low as 10.10%.", "keywords": ["deep neural networks", "stochastic gradient descent", "sequenced-replacement sampling"], "authorids": ["chiuman100@gmail.com", "pdhvip@gmail.com", "wei.yang2@huawei.com", "yichang@acm.org"], "authors": ["Chiu Man Ho", "Dae Hoon Park", "Wei Yang", "Yi Chang"], "TL;DR": "Proposed a novel way (without adding new parameters) of training deep neural network in order to improve generalization, especially for the case where we have relatively small images-per-class.", "pdf": "/pdf/d5e0c948a797ea18533ba50e7c7f8e167e9b71f1.pdf", "paperhash": "ho|sequencedreplacement_sampling_for_deep_learning", "_bibtex": "@misc{\nho2019sequencedreplacement,\ntitle={Sequenced-Replacement Sampling for Deep Learning},\nauthor={Chiu Man Ho and Dae Hoon Park and Wei Yang and Yi Chang},\nyear={2019},\nurl={https://openreview.net/forum?id=BJlVhsA5KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper701/Official_Review", "cdate": 1542234399521, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJlVhsA5KX", "replyto": "BJlVhsA5KX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper701/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335783878, "tmdate": 1552335783878, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper701/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}