{"notes": [{"id": "0pxiMpCyBtr", "original": "bXNYXBtFBj", "number": 1044, "cdate": 1601308117754, "ddate": null, "tcdate": 1601308117754, "tmdate": 1611607624278, "tddate": null, "forum": "0pxiMpCyBtr", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Monotonic Kronecker-Factored Lattice", "authorids": ["~William_Taylor_Bakst1", "~Nobuyuki_Morioka1", "~Erez_Louidor1"], "authors": ["William Taylor Bakst", "Nobuyuki Morioka", "Erez Louidor"], "keywords": ["Theory", "Regularization", "Algorithms", "Classification", "Regression", "Matrix and Tensor Factorization", "Fairness", "Evaluation", "Efficiency", "Machine Learning"], "abstract": "It is computationally challenging to learn flexible monotonic functions that guarantee model behavior and provide interpretability beyond a few input features, and in a time where minimizing resource use is increasingly important, we must be able to learn such models that are still efficient. In this paper we show how to effectively and efficiently learn such functions using Kronecker-Factored Lattice ($\\mathrm{KFL}$), an efficient reparameterization of flexible monotonic lattice regression via Kronecker product. Both computational and storage costs scale linearly in the number of input features, which is a significant improvement over existing methods that grow exponentially. We also show that we can still properly enforce monotonicity and other shape constraints. The $\\mathrm{KFL}$ function class consists of products of piecewise-linear functions, and the size of the function class can be further increased through ensembling. We prove that the function class of an ensemble of $M$ base $\\mathrm{KFL}$ models strictly increases as $M$ increases up to a certain threshold. Beyond this threshold, every multilinear interpolated lattice function can be expressed. Our experimental results demonstrate that $\\mathrm{KFL}$ trains faster with fewer parameters while still achieving accuracy and evaluation speeds comparable to or better than the baseline methods and preserving monotonicity guarantees on the learned model.", "one-sentence_summary": "We show how to effectively and efficiently learn flexible and interpretable monotonic functions using Kronecker-Factored Lattice, an efficient reparameterization of flexible monotonic lattice regression via Kronecker product.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bakst|monotonic_kroneckerfactored_lattice", "pdf": "/pdf/86fd2e9cf39fac3f9c7f37e71683f60eb7e3e575.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nbakst2021monotonic,\ntitle={Monotonic Kronecker-Factored Lattice},\nauthor={William Taylor Bakst and Nobuyuki Morioka and Erez Louidor},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=0pxiMpCyBtr}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "9iyVScrX3uv", "original": null, "number": 1, "cdate": 1611190996094, "ddate": null, "tcdate": 1611190996094, "tmdate": 1611190996094, "tddate": null, "forum": "0pxiMpCyBtr", "replyto": "0pxiMpCyBtr", "invitation": "ICLR.cc/2021/Conference/Paper1044/-/Comment", "content": {"title": "Camera ready version uploaded.", "comment": "Thank you so much to the reviewers and everyone involved for taking the time to review our paper thoroughly and provide good feedback. We have uploaded our camera ready submission. If you see anything that needs to be fixed that we missed, please say so and we'll update it."}, "signatures": ["ICLR.cc/2021/Conference/Paper1044/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1044/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Monotonic Kronecker-Factored Lattice", "authorids": ["~William_Taylor_Bakst1", "~Nobuyuki_Morioka1", "~Erez_Louidor1"], "authors": ["William Taylor Bakst", "Nobuyuki Morioka", "Erez Louidor"], "keywords": ["Theory", "Regularization", "Algorithms", "Classification", "Regression", "Matrix and Tensor Factorization", "Fairness", "Evaluation", "Efficiency", "Machine Learning"], "abstract": "It is computationally challenging to learn flexible monotonic functions that guarantee model behavior and provide interpretability beyond a few input features, and in a time where minimizing resource use is increasingly important, we must be able to learn such models that are still efficient. In this paper we show how to effectively and efficiently learn such functions using Kronecker-Factored Lattice ($\\mathrm{KFL}$), an efficient reparameterization of flexible monotonic lattice regression via Kronecker product. Both computational and storage costs scale linearly in the number of input features, which is a significant improvement over existing methods that grow exponentially. We also show that we can still properly enforce monotonicity and other shape constraints. The $\\mathrm{KFL}$ function class consists of products of piecewise-linear functions, and the size of the function class can be further increased through ensembling. We prove that the function class of an ensemble of $M$ base $\\mathrm{KFL}$ models strictly increases as $M$ increases up to a certain threshold. Beyond this threshold, every multilinear interpolated lattice function can be expressed. Our experimental results demonstrate that $\\mathrm{KFL}$ trains faster with fewer parameters while still achieving accuracy and evaluation speeds comparable to or better than the baseline methods and preserving monotonicity guarantees on the learned model.", "one-sentence_summary": "We show how to effectively and efficiently learn flexible and interpretable monotonic functions using Kronecker-Factored Lattice, an efficient reparameterization of flexible monotonic lattice regression via Kronecker product.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bakst|monotonic_kroneckerfactored_lattice", "pdf": "/pdf/86fd2e9cf39fac3f9c7f37e71683f60eb7e3e575.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nbakst2021monotonic,\ntitle={Monotonic Kronecker-Factored Lattice},\nauthor={William Taylor Bakst and Nobuyuki Morioka and Erez Louidor},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=0pxiMpCyBtr}\n}"}, "tags": [], "invitation": {"reply": {"forum": "0pxiMpCyBtr", "readers": {"values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2021/Conference/Paper1044/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1044/Authors|ICLR.cc/2021/Conference/Paper1044/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs"}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}}, "multiReply": true, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["everyone"], "tcdate": 1610649465120, "tmdate": 1610649465120, "id": "ICLR.cc/2021/Conference/Paper1044/-/Comment"}}}, {"id": "a_IPxf163Q", "original": null, "number": 1, "cdate": 1610040491811, "ddate": null, "tcdate": 1610040491811, "tmdate": 1610474097796, "tddate": null, "forum": "0pxiMpCyBtr", "replyto": "0pxiMpCyBtr", "invitation": "ICLR.cc/2021/Conference/Paper1044/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "The focus of the submission is shape-constrained regression, particularly the goal is to learn monotonic, 'reasonably rich' functions. In order to tackle this task, the authors extend the monotonic regression framework (Gupta et al., 2016) which scales less benignly in the input dimension. They propose to use lattice functions with parameters having Kronecker product structure (, and their ensembles). The resulting function class can be (i) stored and evaluated in linear time (Proposition 1), (ii) characterized / checked from monotonicity perspective (Proposition 2). The efficiency of the approach is demonstrated in three real-world examples.\n\nShape-constrained regression is a central topic in machine learning and statistics. The authors propose a parametric family to learn monotonically constrained functions. The storage and the evaluation of the resulting functions are both fast (linear), and the numerical experiments are encouraging. The submission can be of definite interest to the ICLR community. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Monotonic Kronecker-Factored Lattice", "authorids": ["~William_Taylor_Bakst1", "~Nobuyuki_Morioka1", "~Erez_Louidor1"], "authors": ["William Taylor Bakst", "Nobuyuki Morioka", "Erez Louidor"], "keywords": ["Theory", "Regularization", "Algorithms", "Classification", "Regression", "Matrix and Tensor Factorization", "Fairness", "Evaluation", "Efficiency", "Machine Learning"], "abstract": "It is computationally challenging to learn flexible monotonic functions that guarantee model behavior and provide interpretability beyond a few input features, and in a time where minimizing resource use is increasingly important, we must be able to learn such models that are still efficient. In this paper we show how to effectively and efficiently learn such functions using Kronecker-Factored Lattice ($\\mathrm{KFL}$), an efficient reparameterization of flexible monotonic lattice regression via Kronecker product. Both computational and storage costs scale linearly in the number of input features, which is a significant improvement over existing methods that grow exponentially. We also show that we can still properly enforce monotonicity and other shape constraints. The $\\mathrm{KFL}$ function class consists of products of piecewise-linear functions, and the size of the function class can be further increased through ensembling. We prove that the function class of an ensemble of $M$ base $\\mathrm{KFL}$ models strictly increases as $M$ increases up to a certain threshold. Beyond this threshold, every multilinear interpolated lattice function can be expressed. Our experimental results demonstrate that $\\mathrm{KFL}$ trains faster with fewer parameters while still achieving accuracy and evaluation speeds comparable to or better than the baseline methods and preserving monotonicity guarantees on the learned model.", "one-sentence_summary": "We show how to effectively and efficiently learn flexible and interpretable monotonic functions using Kronecker-Factored Lattice, an efficient reparameterization of flexible monotonic lattice regression via Kronecker product.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bakst|monotonic_kroneckerfactored_lattice", "pdf": "/pdf/86fd2e9cf39fac3f9c7f37e71683f60eb7e3e575.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nbakst2021monotonic,\ntitle={Monotonic Kronecker-Factored Lattice},\nauthor={William Taylor Bakst and Nobuyuki Morioka and Erez Louidor},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=0pxiMpCyBtr}\n}"}, "tags": [], "invitation": {"reply": {"forum": "0pxiMpCyBtr", "replyto": "0pxiMpCyBtr", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040491797, "tmdate": 1610474097780, "id": "ICLR.cc/2021/Conference/Paper1044/-/Decision"}}}, {"id": "p27g10Gz16", "original": null, "number": 1, "cdate": 1603809554208, "ddate": null, "tcdate": 1603809554208, "tmdate": 1606581760203, "tddate": null, "forum": "0pxiMpCyBtr", "replyto": "0pxiMpCyBtr", "invitation": "ICLR.cc/2021/Conference/Paper1044/-/Official_Review", "content": {"title": "Review Comment #1", "review": "(Added on 11/29/2020) **Post Rebuttal Comment**\n\nI thank the authors for sincerely replying my review comments. I think the authors answered my questions. \n\nAddtional Comments\n\n- Section 3.4: $(c_1(x[1]),...,c_D(x[D])$ \u2192$(c_1(x[1]),...,c_D(x[D]))$ (Add a right parenthesis)\n\n---------------------------------------------------------\n\n**Review Summary**\n\nTheoretical claims about the condition about single KFL's monotonicity and the expressive power of the ensemble of KFL's are correct. Also, the ensemble of KFL demonstrates empirical performance comparable with existing methods with fewer computational complexities up to 24-dimensional feature vectors. In the experiments, this paper learns the monotonic function using the ensemble of KFL, whose weak learners are monotonic. However, I wonder whether it is theoretically justified (see \"Soundness of the claims\" section).\n\n**Summary of the Paper**\n\nThis paper proposed Kronecker-Factored Lattice (KFL) for learning monotonic functions, which is computationally efficient than the existing method in terms of input dimension. This paper derived a necessary and sufficient condition that KFL is monotonic, which is easy to check. This paper also proposed an ensemble of KFL and showed that its expressive power is sufficiently strong that any lattice-point-interpolating function when the number of base learners is sufficiently large. This paper applied the ensemble of KFL for learning monotonic function using three datasets and confirmed that the proposed methods performed comparably with the existing methods with few computational resources and time.\n\n**Claim**\n\nIf I understand correctly, the main claims of this paper are as follows. I assume them and evaluate the paper based on whether it supports them.\n- Claim 1: The computational and storage cost of the proposed method is efficient. (Contribution 1, 4)\n- Claim 2: We can use KFL to learn monotonic function (Contribution 2)\n- Claim 3: The ensemble of KFL is sufficiently expressive (Contribution 3)\n- Claim 4: The proposed method empirically performs well (Contribution 4)\n\n**Soundness of the claims**\n\nCan theory support the claim?\n- Claim 1 is supported by the discussion in the last paragraph of Section 2. The proposed method is $O(D)$ evaluation time and $O(\\sum_d \\mathcal{V}[d])$  parameters, which is typically linear in $D$.\n- Regarding Claim 2, the author proposed a training method (Section 3.3, Paragraph 3) of KFL that is justified by Proposition 2.\n- Claim 3 is supported by Proposition 3. This paper showed that the expressive power of the ensemble of $T$ KFL is strictly increasing in terms of $T$ up to some $T_0$. This paper also showed that the $T$ ensemble of KFL can represent any lattice-point-interpolating functino for any $T\\geq T_0$.\n- In the experiments, this paper learned monotonic functions using the ensemble of KFL by imposing monotonicity to base learners (i.e., KFL). However, if I understand correctly, the expressive power of the ensemble of monotonic KFL was not studied. It is true that any function in $\\mathcal{L}(\\mathcal{V})$ can represent the sum of KFLs, which are not necessarily monotonic. However, it is not known we can express the monotonic function using monotonic KFL's only.\n\nCan empirical evaluation support the claim?\n- Claim 1 is supported by the fact that the training time in Tables 3, 4, and 5 is reduced. Regarding storage cost, the number of parameters is reduced.\n- Claim 2, 3 do not have corresponding empirical support due to their theoretical nature.\n- Claim 4 is supported by the test accuracy in Tables 3, 4, and 5. At least we can observe clear improvement from multilinear and simplex methods by Gupta et al. (2016) when appropriately configuring  and $M$. However, from Figure 4--6, the ensemble of KFL does not perform well when $V$ and $M$ is small.\n\n**Significance and novelty**\n\nRelation to previous work (What is different from previous work)?\n- Gupta et al. (2016) also proposed a method for learning monotonic function using a function that interpolates lattice points linearly. They discussed the difference between the proposed method and Gupta et al. (2016). Specifically, Gupta et al. (2016) takes $O(2^D)$ time to evaluate a $D$-variate function and has $O(\\prod_d \\mathcal{V}[d])$ parameters, which is typically exponential in $D$. On the other hand, those of the proposed method are typically linear (Claim 1).\n\nNovelty\n- The idea of the proposed method is reasonable to me in that we employ Kronecker Factorization to reduce the computational and storage complexities and compensate for the reduced expressive power by ensembling. The idea of ensembling is also found in Canini et al. (2016); however, this paper's strength is that it backbones this idea by theoretical results (Proposition 2, 3), which are novel.\n\n**Correctness and Clarity**\n\nIs the theorem correct?\n- The proofs of propositions are correct, so far as I check.\n\nIs the experimental evaluation correct?\n- I do not find any inappropriate points in experimental settings.\n\nIs the experiment reproducible?\n- The implementation of the proposed method is made public. This paper used three datasets in experiments. One dataset (Adult Income) is made public, and the other two datasets are proprietary. So, it is hard for us to reproduce the same experiments using private datasets.\n\n**Clarity**\n\nCan I understand the main point of the paper easily?\n- Yes, I can understand the backgrounds of the proposed method explained in Section 2. The mathematical descriptions of this paper are clear.\n\nIs the organization of paper well?\n- Yes. I did not find any problem with the organization of the paper.\n\nAre figures and tables appropriately made?\n- Yes\n\n**Additional feedback**\n\n- Section 4, Paragraph 1: This paper says that for each monotonic feature $d$, $w^{(m)}_d$ is projected to satisfy the monotonicity constraints of Proposition 2 Condition 3. I think this projection operation is not obvious per se. So, I want this paper to clarify it.\n- Proposition 3: Is there a case in which $M$ is strictly smaller than the upepr bound $|\\mathcal{M}_\\mathcal{V}| /  \\max_i \\mathcal{V}_i$ ? It is merely my intuition but since $T$ can take an arbitrary tensor of size $\\mathcal{V}$, there might not be such a case.", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper1044/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1044/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Monotonic Kronecker-Factored Lattice", "authorids": ["~William_Taylor_Bakst1", "~Nobuyuki_Morioka1", "~Erez_Louidor1"], "authors": ["William Taylor Bakst", "Nobuyuki Morioka", "Erez Louidor"], "keywords": ["Theory", "Regularization", "Algorithms", "Classification", "Regression", "Matrix and Tensor Factorization", "Fairness", "Evaluation", "Efficiency", "Machine Learning"], "abstract": "It is computationally challenging to learn flexible monotonic functions that guarantee model behavior and provide interpretability beyond a few input features, and in a time where minimizing resource use is increasingly important, we must be able to learn such models that are still efficient. In this paper we show how to effectively and efficiently learn such functions using Kronecker-Factored Lattice ($\\mathrm{KFL}$), an efficient reparameterization of flexible monotonic lattice regression via Kronecker product. Both computational and storage costs scale linearly in the number of input features, which is a significant improvement over existing methods that grow exponentially. We also show that we can still properly enforce monotonicity and other shape constraints. The $\\mathrm{KFL}$ function class consists of products of piecewise-linear functions, and the size of the function class can be further increased through ensembling. We prove that the function class of an ensemble of $M$ base $\\mathrm{KFL}$ models strictly increases as $M$ increases up to a certain threshold. Beyond this threshold, every multilinear interpolated lattice function can be expressed. Our experimental results demonstrate that $\\mathrm{KFL}$ trains faster with fewer parameters while still achieving accuracy and evaluation speeds comparable to or better than the baseline methods and preserving monotonicity guarantees on the learned model.", "one-sentence_summary": "We show how to effectively and efficiently learn flexible and interpretable monotonic functions using Kronecker-Factored Lattice, an efficient reparameterization of flexible monotonic lattice regression via Kronecker product.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bakst|monotonic_kroneckerfactored_lattice", "pdf": "/pdf/86fd2e9cf39fac3f9c7f37e71683f60eb7e3e575.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nbakst2021monotonic,\ntitle={Monotonic Kronecker-Factored Lattice},\nauthor={William Taylor Bakst and Nobuyuki Morioka and Erez Louidor},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=0pxiMpCyBtr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "0pxiMpCyBtr", "replyto": "0pxiMpCyBtr", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1044/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538128436, "tmdate": 1606915797935, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1044/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1044/-/Official_Review"}}}, {"id": "ztKN3kNDc2U", "original": null, "number": 4, "cdate": 1604655865540, "ddate": null, "tcdate": 1604655865540, "tmdate": 1606117941238, "tddate": null, "forum": "0pxiMpCyBtr", "replyto": "0pxiMpCyBtr", "invitation": "ICLR.cc/2021/Conference/Paper1044/-/Official_Review", "content": {"title": "The paper provides a computationally efficient method for the modeling of monotonic functions. However, the authors should investigate various shape constraints, such as convexity, positivity, etc.", "review": "In this paper, the authors study statistical models based on the Kronecker-factored lattice (KFL) and investigated the condition that the monotonicity holds. The KFL provides efficient and flexible modeling for monotonic shape constraints. Furthermore, the ensemble of KFL is proposed, and its approximation ability is theoretically investigated. Some numerical experiments indicate that the KFL trains faster with fewer parameters with comparable prediction accuracy to existing methods. \n\nThis paper was assigned as an emergency review. So I did not have sufficient time to understand the content of this paper deeply. \nThe paper provides a computationally efficient method for the modeling of monotonic functions. However, the authors should investigate more variety of shape constraints such as convexity, positivity, etc.\n\nProposition 2 is an interesting result. However, more detailed studies on the shape would be necessary for the publication. For example, is it possible to derive the condition that the function is increasing in a variable and convex in the other variable?\n\nProposition 3 seems relatively straightforward. Supplementing untrivial statements would be good for readers to understand the significance of the proposition. \n\nIn the numerical experiments, the authors found that the hyperparameter V is important to tune the capacity of the proposed statistical model. Is there some theoretical support for that finding? Showing a clear insight would be excellent.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1044/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1044/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Monotonic Kronecker-Factored Lattice", "authorids": ["~William_Taylor_Bakst1", "~Nobuyuki_Morioka1", "~Erez_Louidor1"], "authors": ["William Taylor Bakst", "Nobuyuki Morioka", "Erez Louidor"], "keywords": ["Theory", "Regularization", "Algorithms", "Classification", "Regression", "Matrix and Tensor Factorization", "Fairness", "Evaluation", "Efficiency", "Machine Learning"], "abstract": "It is computationally challenging to learn flexible monotonic functions that guarantee model behavior and provide interpretability beyond a few input features, and in a time where minimizing resource use is increasingly important, we must be able to learn such models that are still efficient. In this paper we show how to effectively and efficiently learn such functions using Kronecker-Factored Lattice ($\\mathrm{KFL}$), an efficient reparameterization of flexible monotonic lattice regression via Kronecker product. Both computational and storage costs scale linearly in the number of input features, which is a significant improvement over existing methods that grow exponentially. We also show that we can still properly enforce monotonicity and other shape constraints. The $\\mathrm{KFL}$ function class consists of products of piecewise-linear functions, and the size of the function class can be further increased through ensembling. We prove that the function class of an ensemble of $M$ base $\\mathrm{KFL}$ models strictly increases as $M$ increases up to a certain threshold. Beyond this threshold, every multilinear interpolated lattice function can be expressed. Our experimental results demonstrate that $\\mathrm{KFL}$ trains faster with fewer parameters while still achieving accuracy and evaluation speeds comparable to or better than the baseline methods and preserving monotonicity guarantees on the learned model.", "one-sentence_summary": "We show how to effectively and efficiently learn flexible and interpretable monotonic functions using Kronecker-Factored Lattice, an efficient reparameterization of flexible monotonic lattice regression via Kronecker product.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bakst|monotonic_kroneckerfactored_lattice", "pdf": "/pdf/86fd2e9cf39fac3f9c7f37e71683f60eb7e3e575.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nbakst2021monotonic,\ntitle={Monotonic Kronecker-Factored Lattice},\nauthor={William Taylor Bakst and Nobuyuki Morioka and Erez Louidor},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=0pxiMpCyBtr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "0pxiMpCyBtr", "replyto": "0pxiMpCyBtr", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1044/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538128436, "tmdate": 1606915797935, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1044/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1044/-/Official_Review"}}}, {"id": "jD81mF2M9eL", "original": null, "number": 13, "cdate": 1606117915683, "ddate": null, "tcdate": 1606117915683, "tmdate": 1606117915683, "tddate": null, "forum": "0pxiMpCyBtr", "replyto": "YBTm2OYys7n", "invitation": "ICLR.cc/2021/Conference/Paper1044/-/Official_Comment", "content": {"title": "response to the revision", "comment": "Thanks for the thoughtful response.\n\n1. I found that the authors addressed my concern in the revision.\n2. The wording of my comment was not sufficient. I did not think the past proposition 3 was trivial, but that the result sounded quite natural. I also understand a mathematically rigorous proof of such a result sometimes requires a somewhat involved argument. My concern was that the significance of their result was not very clear in the previous version. The revised expression of Proposition 5 is good for me. \n3. Thanks for the revision.\n\nThe response to 1 is satisfactory for me, so I raise the score. \n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1044/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1044/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Monotonic Kronecker-Factored Lattice", "authorids": ["~William_Taylor_Bakst1", "~Nobuyuki_Morioka1", "~Erez_Louidor1"], "authors": ["William Taylor Bakst", "Nobuyuki Morioka", "Erez Louidor"], "keywords": ["Theory", "Regularization", "Algorithms", "Classification", "Regression", "Matrix and Tensor Factorization", "Fairness", "Evaluation", "Efficiency", "Machine Learning"], "abstract": "It is computationally challenging to learn flexible monotonic functions that guarantee model behavior and provide interpretability beyond a few input features, and in a time where minimizing resource use is increasingly important, we must be able to learn such models that are still efficient. In this paper we show how to effectively and efficiently learn such functions using Kronecker-Factored Lattice ($\\mathrm{KFL}$), an efficient reparameterization of flexible monotonic lattice regression via Kronecker product. Both computational and storage costs scale linearly in the number of input features, which is a significant improvement over existing methods that grow exponentially. We also show that we can still properly enforce monotonicity and other shape constraints. The $\\mathrm{KFL}$ function class consists of products of piecewise-linear functions, and the size of the function class can be further increased through ensembling. We prove that the function class of an ensemble of $M$ base $\\mathrm{KFL}$ models strictly increases as $M$ increases up to a certain threshold. Beyond this threshold, every multilinear interpolated lattice function can be expressed. Our experimental results demonstrate that $\\mathrm{KFL}$ trains faster with fewer parameters while still achieving accuracy and evaluation speeds comparable to or better than the baseline methods and preserving monotonicity guarantees on the learned model.", "one-sentence_summary": "We show how to effectively and efficiently learn flexible and interpretable monotonic functions using Kronecker-Factored Lattice, an efficient reparameterization of flexible monotonic lattice regression via Kronecker product.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bakst|monotonic_kroneckerfactored_lattice", "pdf": "/pdf/86fd2e9cf39fac3f9c7f37e71683f60eb7e3e575.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nbakst2021monotonic,\ntitle={Monotonic Kronecker-Factored Lattice},\nauthor={William Taylor Bakst and Nobuyuki Morioka and Erez Louidor},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=0pxiMpCyBtr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0pxiMpCyBtr", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1044/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1044/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1044/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1044/Authors|ICLR.cc/2021/Conference/Paper1044/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1044/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864325, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1044/-/Official_Comment"}}}, {"id": "hCwMX38Dd7n", "original": null, "number": 12, "cdate": 1605852304772, "ddate": null, "tcdate": 1605852304772, "tmdate": 1605852304772, "tddate": null, "forum": "0pxiMpCyBtr", "replyto": "V2I07EuNwYX", "invitation": "ICLR.cc/2021/Conference/Paper1044/-/Official_Comment", "content": {"title": "Reply to authors' response", "comment": "I thank the authors to consider my review comment seriously and answer my questions. I understand the authors' questions. Let me take time to check the updated version of the paper. I will ask additional questions if I have them."}, "signatures": ["ICLR.cc/2021/Conference/Paper1044/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1044/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Monotonic Kronecker-Factored Lattice", "authorids": ["~William_Taylor_Bakst1", "~Nobuyuki_Morioka1", "~Erez_Louidor1"], "authors": ["William Taylor Bakst", "Nobuyuki Morioka", "Erez Louidor"], "keywords": ["Theory", "Regularization", "Algorithms", "Classification", "Regression", "Matrix and Tensor Factorization", "Fairness", "Evaluation", "Efficiency", "Machine Learning"], "abstract": "It is computationally challenging to learn flexible monotonic functions that guarantee model behavior and provide interpretability beyond a few input features, and in a time where minimizing resource use is increasingly important, we must be able to learn such models that are still efficient. In this paper we show how to effectively and efficiently learn such functions using Kronecker-Factored Lattice ($\\mathrm{KFL}$), an efficient reparameterization of flexible monotonic lattice regression via Kronecker product. Both computational and storage costs scale linearly in the number of input features, which is a significant improvement over existing methods that grow exponentially. We also show that we can still properly enforce monotonicity and other shape constraints. The $\\mathrm{KFL}$ function class consists of products of piecewise-linear functions, and the size of the function class can be further increased through ensembling. We prove that the function class of an ensemble of $M$ base $\\mathrm{KFL}$ models strictly increases as $M$ increases up to a certain threshold. Beyond this threshold, every multilinear interpolated lattice function can be expressed. Our experimental results demonstrate that $\\mathrm{KFL}$ trains faster with fewer parameters while still achieving accuracy and evaluation speeds comparable to or better than the baseline methods and preserving monotonicity guarantees on the learned model.", "one-sentence_summary": "We show how to effectively and efficiently learn flexible and interpretable monotonic functions using Kronecker-Factored Lattice, an efficient reparameterization of flexible monotonic lattice regression via Kronecker product.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bakst|monotonic_kroneckerfactored_lattice", "pdf": "/pdf/86fd2e9cf39fac3f9c7f37e71683f60eb7e3e575.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nbakst2021monotonic,\ntitle={Monotonic Kronecker-Factored Lattice},\nauthor={William Taylor Bakst and Nobuyuki Morioka and Erez Louidor},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=0pxiMpCyBtr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0pxiMpCyBtr", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1044/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1044/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1044/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1044/Authors|ICLR.cc/2021/Conference/Paper1044/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1044/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864325, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1044/-/Official_Comment"}}}, {"id": "V2I07EuNwYX", "original": null, "number": 11, "cdate": 1605636357527, "ddate": null, "tcdate": 1605636357527, "tmdate": 1605636357527, "tddate": null, "forum": "0pxiMpCyBtr", "replyto": "p27g10Gz16", "invitation": "ICLR.cc/2021/Conference/Paper1044/-/Official_Comment", "content": {"title": "Response", "comment": "We would like to thank the reviewer for their thoughtful feedback!\n\n1. It may well be that some monotonic functions in L(V) cannot be expressed as a sum of monotonic KFL functions. In fact, the same caveat may also be true for the technique of constraining monotonicity of ensembles of conventional multilinear lattices described in Canini et al. (2016), where each base lattice is constrained to be monotonic. In practice, though, we often find that ensembles are expressive enough for most applications, which is further demonstrated by our empirical results. Thank you very much for pointing this out.\n\n2. We updated the paper to show the exact algorithm used (see Section 3.5 and Appendix B). Thank you for pointing this out.\n \n3. Thank you for the good question! No, unfortunately, the bound is not tight. For example, for $2 \\times 2 \\times 2$ real tensors, the maximum rank (over the real field) is 3 not 4 (see https://arxiv.org/pdf/math/0607647.pdf, Section 7), and for general n x n x n real tensors, the maximum rank (again, over the real field) is at most $n(n+1)/2$ (see https://link.springer.com/article/10.1007/s10463-010-0294-5 , Theorems 1, and 3). As far as we know, the problem of finding the maximum rank for a general tensor size is open.  "}, "signatures": ["ICLR.cc/2021/Conference/Paper1044/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1044/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Monotonic Kronecker-Factored Lattice", "authorids": ["~William_Taylor_Bakst1", "~Nobuyuki_Morioka1", "~Erez_Louidor1"], "authors": ["William Taylor Bakst", "Nobuyuki Morioka", "Erez Louidor"], "keywords": ["Theory", "Regularization", "Algorithms", "Classification", "Regression", "Matrix and Tensor Factorization", "Fairness", "Evaluation", "Efficiency", "Machine Learning"], "abstract": "It is computationally challenging to learn flexible monotonic functions that guarantee model behavior and provide interpretability beyond a few input features, and in a time where minimizing resource use is increasingly important, we must be able to learn such models that are still efficient. In this paper we show how to effectively and efficiently learn such functions using Kronecker-Factored Lattice ($\\mathrm{KFL}$), an efficient reparameterization of flexible monotonic lattice regression via Kronecker product. Both computational and storage costs scale linearly in the number of input features, which is a significant improvement over existing methods that grow exponentially. We also show that we can still properly enforce monotonicity and other shape constraints. The $\\mathrm{KFL}$ function class consists of products of piecewise-linear functions, and the size of the function class can be further increased through ensembling. We prove that the function class of an ensemble of $M$ base $\\mathrm{KFL}$ models strictly increases as $M$ increases up to a certain threshold. Beyond this threshold, every multilinear interpolated lattice function can be expressed. Our experimental results demonstrate that $\\mathrm{KFL}$ trains faster with fewer parameters while still achieving accuracy and evaluation speeds comparable to or better than the baseline methods and preserving monotonicity guarantees on the learned model.", "one-sentence_summary": "We show how to effectively and efficiently learn flexible and interpretable monotonic functions using Kronecker-Factored Lattice, an efficient reparameterization of flexible monotonic lattice regression via Kronecker product.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bakst|monotonic_kroneckerfactored_lattice", "pdf": "/pdf/86fd2e9cf39fac3f9c7f37e71683f60eb7e3e575.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nbakst2021monotonic,\ntitle={Monotonic Kronecker-Factored Lattice},\nauthor={William Taylor Bakst and Nobuyuki Morioka and Erez Louidor},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=0pxiMpCyBtr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0pxiMpCyBtr", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1044/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1044/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1044/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1044/Authors|ICLR.cc/2021/Conference/Paper1044/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1044/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864325, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1044/-/Official_Comment"}}}, {"id": "SS-O-gpXNEG", "original": null, "number": 10, "cdate": 1605636221077, "ddate": null, "tcdate": 1605636221077, "tmdate": 1605636221077, "tddate": null, "forum": "0pxiMpCyBtr", "replyto": "ouomYiRD1JN", "invitation": "ICLR.cc/2021/Conference/Paper1044/-/Official_Comment", "content": {"title": "Response", "comment": "We thank the reviewer for their thoughtful feedback!\n\nIn regards to the train/eval time comparison against Simplex, we would like to first reiterate that while Simplex\u2019s space complexity is $O(V^D)$, the time complexity to evaluate one example is $O(Dlog(D))$. This is because Simplex partitions the unit hypercube into $D!$ simplices (for $V=2$), and finds the simplex containing the example during inference. Hence, for the first two  datasets with 14 features, the evaluation times are roughly similar for KFL and Simplex. However, during training, examples in a mini-batch do not necessarily have the same containing simplex, and training thus requires access to many simplices resulting in expensive matrix operations for each gradient update step. We show KFL is up to $2.5x$ faster to train than Simplex and, as mentioned by the reviewer, we also expect the training time gap to widen as the number of features grows. In fact, for the User Query Intent dataset, a single Simplex lattice with all 24 features takes ~2hrs to train for a single epoch. We estimate this to be $400x$ slower than KFL with $M=100$. In the reported experiment, however, Simplex used a random tiny lattice ensemble of 100 lattices with each seeing a random subset of 24 features as described in Canini et al. (2016) instead to achieve efficient training. This restricts the model capacity as it cannot capture full non-linear feature interaction, so our KFL model with $M=100$, $V=8$ is shown to be significantly more accurate, yet faster and more compact."}, "signatures": ["ICLR.cc/2021/Conference/Paper1044/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1044/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Monotonic Kronecker-Factored Lattice", "authorids": ["~William_Taylor_Bakst1", "~Nobuyuki_Morioka1", "~Erez_Louidor1"], "authors": ["William Taylor Bakst", "Nobuyuki Morioka", "Erez Louidor"], "keywords": ["Theory", "Regularization", "Algorithms", "Classification", "Regression", "Matrix and Tensor Factorization", "Fairness", "Evaluation", "Efficiency", "Machine Learning"], "abstract": "It is computationally challenging to learn flexible monotonic functions that guarantee model behavior and provide interpretability beyond a few input features, and in a time where minimizing resource use is increasingly important, we must be able to learn such models that are still efficient. In this paper we show how to effectively and efficiently learn such functions using Kronecker-Factored Lattice ($\\mathrm{KFL}$), an efficient reparameterization of flexible monotonic lattice regression via Kronecker product. Both computational and storage costs scale linearly in the number of input features, which is a significant improvement over existing methods that grow exponentially. We also show that we can still properly enforce monotonicity and other shape constraints. The $\\mathrm{KFL}$ function class consists of products of piecewise-linear functions, and the size of the function class can be further increased through ensembling. We prove that the function class of an ensemble of $M$ base $\\mathrm{KFL}$ models strictly increases as $M$ increases up to a certain threshold. Beyond this threshold, every multilinear interpolated lattice function can be expressed. Our experimental results demonstrate that $\\mathrm{KFL}$ trains faster with fewer parameters while still achieving accuracy and evaluation speeds comparable to or better than the baseline methods and preserving monotonicity guarantees on the learned model.", "one-sentence_summary": "We show how to effectively and efficiently learn flexible and interpretable monotonic functions using Kronecker-Factored Lattice, an efficient reparameterization of flexible monotonic lattice regression via Kronecker product.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bakst|monotonic_kroneckerfactored_lattice", "pdf": "/pdf/86fd2e9cf39fac3f9c7f37e71683f60eb7e3e575.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nbakst2021monotonic,\ntitle={Monotonic Kronecker-Factored Lattice},\nauthor={William Taylor Bakst and Nobuyuki Morioka and Erez Louidor},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=0pxiMpCyBtr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0pxiMpCyBtr", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1044/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1044/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1044/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1044/Authors|ICLR.cc/2021/Conference/Paper1044/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1044/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864325, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1044/-/Official_Comment"}}}, {"id": "PNGJT_W90mv", "original": null, "number": 9, "cdate": 1605636103611, "ddate": null, "tcdate": 1605636103611, "tmdate": 1605636103611, "tddate": null, "forum": "0pxiMpCyBtr", "replyto": "cK5wirYeKPy", "invitation": "ICLR.cc/2021/Conference/Paper1044/-/Official_Comment", "content": {"title": "Response", "comment": "We would like to thank the reviewer for their thoughtful comments and address the motivations behind monotonic functions and their significance.\n\nConsider a bank that wants to create a model that decides whether or not to approve a loan for a client where one of the features is the client\u2019s credit score. It follows that if all other features are frozen, increasing credit score should always correspond to an increased chance of approval; however, an unconstrained model may learn a function that does not properly learn this relationship. This is an example of what makes monotonicity constraints significant -- we can impose this real-world knowledge constraint on the model from the beginning, which often results in better generalization, interpretability, and confidence that the model will perform as expected especially when the distribution of data used during inference differs from the training data distribution. In this loan approval case, it may actually be against regulations for the bank to have a model that is not properly constrained (i.e. higher credit score corresponding to lower chance of approval). This is just one of many examples that show that it is significant to be able to impose not just monotonicity but any constraints known/required prior to training. Such techniques can also be useful when one can reasonably assume (rather than know for certain) that a feature should be constrained in a certain way."}, "signatures": ["ICLR.cc/2021/Conference/Paper1044/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1044/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Monotonic Kronecker-Factored Lattice", "authorids": ["~William_Taylor_Bakst1", "~Nobuyuki_Morioka1", "~Erez_Louidor1"], "authors": ["William Taylor Bakst", "Nobuyuki Morioka", "Erez Louidor"], "keywords": ["Theory", "Regularization", "Algorithms", "Classification", "Regression", "Matrix and Tensor Factorization", "Fairness", "Evaluation", "Efficiency", "Machine Learning"], "abstract": "It is computationally challenging to learn flexible monotonic functions that guarantee model behavior and provide interpretability beyond a few input features, and in a time where minimizing resource use is increasingly important, we must be able to learn such models that are still efficient. In this paper we show how to effectively and efficiently learn such functions using Kronecker-Factored Lattice ($\\mathrm{KFL}$), an efficient reparameterization of flexible monotonic lattice regression via Kronecker product. Both computational and storage costs scale linearly in the number of input features, which is a significant improvement over existing methods that grow exponentially. We also show that we can still properly enforce monotonicity and other shape constraints. The $\\mathrm{KFL}$ function class consists of products of piecewise-linear functions, and the size of the function class can be further increased through ensembling. We prove that the function class of an ensemble of $M$ base $\\mathrm{KFL}$ models strictly increases as $M$ increases up to a certain threshold. Beyond this threshold, every multilinear interpolated lattice function can be expressed. Our experimental results demonstrate that $\\mathrm{KFL}$ trains faster with fewer parameters while still achieving accuracy and evaluation speeds comparable to or better than the baseline methods and preserving monotonicity guarantees on the learned model.", "one-sentence_summary": "We show how to effectively and efficiently learn flexible and interpretable monotonic functions using Kronecker-Factored Lattice, an efficient reparameterization of flexible monotonic lattice regression via Kronecker product.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bakst|monotonic_kroneckerfactored_lattice", "pdf": "/pdf/86fd2e9cf39fac3f9c7f37e71683f60eb7e3e575.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nbakst2021monotonic,\ntitle={Monotonic Kronecker-Factored Lattice},\nauthor={William Taylor Bakst and Nobuyuki Morioka and Erez Louidor},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=0pxiMpCyBtr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0pxiMpCyBtr", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1044/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1044/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1044/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1044/Authors|ICLR.cc/2021/Conference/Paper1044/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1044/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864325, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1044/-/Official_Comment"}}}, {"id": "YBTm2OYys7n", "original": null, "number": 8, "cdate": 1605636060583, "ddate": null, "tcdate": 1605636060583, "tmdate": 1605636060583, "tddate": null, "forum": "0pxiMpCyBtr", "replyto": "ztKN3kNDc2U", "invitation": "ICLR.cc/2021/Conference/Paper1044/-/Official_Comment", "content": {"title": "Response", "comment": "We would like to thank the reviewer for their suggestions to improve our paper.\n\n1. We have added proofs for both convexity and (what we believe is) positivity to the paper (see Section 3.2 and Appendix A). We also note that one can impose any number of these constraints on any dimensions so long as the conditions for each hold. However, we would like to clarify what the reviewer meant by the positivity shape constraint. If this is simply a nonnegative (or strictly positive) function, then it is straightforward to impose on KFL by enforcing nonnegativity (or positivity) on all parameters. We are happy to further revise the paper to update our definition if not correct. We believe that any more shape constraints beyond these are out of the scope of this paper, but we would love to explore even more shape constraints in future work, especially higher dimensional ones.\n\n2. Can the reviewer clarify why they claim Proposition 3 is trivial? We note that the same proposition is not true when dealing with multilinear lattices. Namely, since $\\mathcal{L}(\\mathcal{V})$ is closed under addition, any function that is expressible as a sum of conventional multilinear lattices (with the same dimension and inputs) is also expressible as a single multilinear lattice; however, Proposition 5 (previously 3) implies that in general there are functions expressible as a sum of KFL functions which are not expressible using a single such lattice. We rephrased the proposition to make this detail more explicit. \n\n3. We have added a section and corresponding proof to the paper showing how increasing $\\mathcal{V}$ affects the model capacity (see Section 3.4 and Appendix A). We have also added more discussion using this proof as reasoning.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1044/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1044/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Monotonic Kronecker-Factored Lattice", "authorids": ["~William_Taylor_Bakst1", "~Nobuyuki_Morioka1", "~Erez_Louidor1"], "authors": ["William Taylor Bakst", "Nobuyuki Morioka", "Erez Louidor"], "keywords": ["Theory", "Regularization", "Algorithms", "Classification", "Regression", "Matrix and Tensor Factorization", "Fairness", "Evaluation", "Efficiency", "Machine Learning"], "abstract": "It is computationally challenging to learn flexible monotonic functions that guarantee model behavior and provide interpretability beyond a few input features, and in a time where minimizing resource use is increasingly important, we must be able to learn such models that are still efficient. In this paper we show how to effectively and efficiently learn such functions using Kronecker-Factored Lattice ($\\mathrm{KFL}$), an efficient reparameterization of flexible monotonic lattice regression via Kronecker product. Both computational and storage costs scale linearly in the number of input features, which is a significant improvement over existing methods that grow exponentially. We also show that we can still properly enforce monotonicity and other shape constraints. The $\\mathrm{KFL}$ function class consists of products of piecewise-linear functions, and the size of the function class can be further increased through ensembling. We prove that the function class of an ensemble of $M$ base $\\mathrm{KFL}$ models strictly increases as $M$ increases up to a certain threshold. Beyond this threshold, every multilinear interpolated lattice function can be expressed. Our experimental results demonstrate that $\\mathrm{KFL}$ trains faster with fewer parameters while still achieving accuracy and evaluation speeds comparable to or better than the baseline methods and preserving monotonicity guarantees on the learned model.", "one-sentence_summary": "We show how to effectively and efficiently learn flexible and interpretable monotonic functions using Kronecker-Factored Lattice, an efficient reparameterization of flexible monotonic lattice regression via Kronecker product.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bakst|monotonic_kroneckerfactored_lattice", "pdf": "/pdf/86fd2e9cf39fac3f9c7f37e71683f60eb7e3e575.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nbakst2021monotonic,\ntitle={Monotonic Kronecker-Factored Lattice},\nauthor={William Taylor Bakst and Nobuyuki Morioka and Erez Louidor},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=0pxiMpCyBtr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0pxiMpCyBtr", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1044/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1044/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1044/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1044/Authors|ICLR.cc/2021/Conference/Paper1044/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1044/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864325, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1044/-/Official_Comment"}}}, {"id": "wWyDe6ZlB6n", "original": null, "number": 7, "cdate": 1605635946116, "ddate": null, "tcdate": 1605635946116, "tmdate": 1605635946116, "tddate": null, "forum": "0pxiMpCyBtr", "replyto": "0pxiMpCyBtr", "invitation": "ICLR.cc/2021/Conference/Paper1044/-/Official_Comment", "content": {"title": "Paper has been revised.", "comment": "Thank you to all four reviewers for taking the time to review our work. We really appreciate it! We responded to each reviewer individually and uploaded a revised copy of the paper incorporating the feedback we received."}, "signatures": ["ICLR.cc/2021/Conference/Paper1044/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1044/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Monotonic Kronecker-Factored Lattice", "authorids": ["~William_Taylor_Bakst1", "~Nobuyuki_Morioka1", "~Erez_Louidor1"], "authors": ["William Taylor Bakst", "Nobuyuki Morioka", "Erez Louidor"], "keywords": ["Theory", "Regularization", "Algorithms", "Classification", "Regression", "Matrix and Tensor Factorization", "Fairness", "Evaluation", "Efficiency", "Machine Learning"], "abstract": "It is computationally challenging to learn flexible monotonic functions that guarantee model behavior and provide interpretability beyond a few input features, and in a time where minimizing resource use is increasingly important, we must be able to learn such models that are still efficient. In this paper we show how to effectively and efficiently learn such functions using Kronecker-Factored Lattice ($\\mathrm{KFL}$), an efficient reparameterization of flexible monotonic lattice regression via Kronecker product. Both computational and storage costs scale linearly in the number of input features, which is a significant improvement over existing methods that grow exponentially. We also show that we can still properly enforce monotonicity and other shape constraints. The $\\mathrm{KFL}$ function class consists of products of piecewise-linear functions, and the size of the function class can be further increased through ensembling. We prove that the function class of an ensemble of $M$ base $\\mathrm{KFL}$ models strictly increases as $M$ increases up to a certain threshold. Beyond this threshold, every multilinear interpolated lattice function can be expressed. Our experimental results demonstrate that $\\mathrm{KFL}$ trains faster with fewer parameters while still achieving accuracy and evaluation speeds comparable to or better than the baseline methods and preserving monotonicity guarantees on the learned model.", "one-sentence_summary": "We show how to effectively and efficiently learn flexible and interpretable monotonic functions using Kronecker-Factored Lattice, an efficient reparameterization of flexible monotonic lattice regression via Kronecker product.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bakst|monotonic_kroneckerfactored_lattice", "pdf": "/pdf/86fd2e9cf39fac3f9c7f37e71683f60eb7e3e575.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nbakst2021monotonic,\ntitle={Monotonic Kronecker-Factored Lattice},\nauthor={William Taylor Bakst and Nobuyuki Morioka and Erez Louidor},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=0pxiMpCyBtr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0pxiMpCyBtr", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1044/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1044/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1044/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1044/Authors|ICLR.cc/2021/Conference/Paper1044/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1044/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864325, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1044/-/Official_Comment"}}}, {"id": "cK5wirYeKPy", "original": null, "number": 3, "cdate": 1603950118293, "ddate": null, "tcdate": 1603950118293, "tmdate": 1605024544291, "tddate": null, "forum": "0pxiMpCyBtr", "replyto": "0pxiMpCyBtr", "invitation": "ICLR.cc/2021/Conference/Paper1044/-/Official_Review", "content": {"title": "Faster model with the factorization", "review": "The paper proposes to use Kronecker factorization on a lattice for a monotonic lattice. Thanks to the factorization, the computational cost is improved to linear compared to exponential. The paper went on to extend to use an ensemble of KFLs due to each factorized lattice is too restricted. Compared with the baseline, monotonic KFL achieves good accuracy with much faster run times on the experiments.\n\nThe approach seems to strike a good balance between computation and representation of power by choosing an ensemble of factorized functions.\n\nI am not familiar with the motivation for monotonic functions and why the datasets/tasks such as query result matching requires the resulting function to satisfy monotonicity. For this reason, I am not sure about the significance of the work.", "rating": "6: Marginally above acceptance threshold", "confidence": "1: The reviewer's evaluation is an educated guess"}, "signatures": ["ICLR.cc/2021/Conference/Paper1044/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1044/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Monotonic Kronecker-Factored Lattice", "authorids": ["~William_Taylor_Bakst1", "~Nobuyuki_Morioka1", "~Erez_Louidor1"], "authors": ["William Taylor Bakst", "Nobuyuki Morioka", "Erez Louidor"], "keywords": ["Theory", "Regularization", "Algorithms", "Classification", "Regression", "Matrix and Tensor Factorization", "Fairness", "Evaluation", "Efficiency", "Machine Learning"], "abstract": "It is computationally challenging to learn flexible monotonic functions that guarantee model behavior and provide interpretability beyond a few input features, and in a time where minimizing resource use is increasingly important, we must be able to learn such models that are still efficient. In this paper we show how to effectively and efficiently learn such functions using Kronecker-Factored Lattice ($\\mathrm{KFL}$), an efficient reparameterization of flexible monotonic lattice regression via Kronecker product. Both computational and storage costs scale linearly in the number of input features, which is a significant improvement over existing methods that grow exponentially. We also show that we can still properly enforce monotonicity and other shape constraints. The $\\mathrm{KFL}$ function class consists of products of piecewise-linear functions, and the size of the function class can be further increased through ensembling. We prove that the function class of an ensemble of $M$ base $\\mathrm{KFL}$ models strictly increases as $M$ increases up to a certain threshold. Beyond this threshold, every multilinear interpolated lattice function can be expressed. Our experimental results demonstrate that $\\mathrm{KFL}$ trains faster with fewer parameters while still achieving accuracy and evaluation speeds comparable to or better than the baseline methods and preserving monotonicity guarantees on the learned model.", "one-sentence_summary": "We show how to effectively and efficiently learn flexible and interpretable monotonic functions using Kronecker-Factored Lattice, an efficient reparameterization of flexible monotonic lattice regression via Kronecker product.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bakst|monotonic_kroneckerfactored_lattice", "pdf": "/pdf/86fd2e9cf39fac3f9c7f37e71683f60eb7e3e575.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nbakst2021monotonic,\ntitle={Monotonic Kronecker-Factored Lattice},\nauthor={William Taylor Bakst and Nobuyuki Morioka and Erez Louidor},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=0pxiMpCyBtr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "0pxiMpCyBtr", "replyto": "0pxiMpCyBtr", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1044/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538128436, "tmdate": 1606915797935, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1044/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1044/-/Official_Review"}}}, {"id": "ouomYiRD1JN", "original": null, "number": 2, "cdate": 1603879740297, "ddate": null, "tcdate": 1603879740297, "tmdate": 1605024544221, "tddate": null, "forum": "0pxiMpCyBtr", "replyto": "0pxiMpCyBtr", "invitation": "ICLR.cc/2021/Conference/Paper1044/-/Official_Review", "content": {"title": "Novel reparametrization of monotonic lattice regression which shows significant empirical gains. ", "review": "**Summary**\nThe authors propose KFL, an efficient reparametrization of monotonic lattice regression using Kronecker factorization. The goal is to achieve efficiency both in terms of computations and in terms of the number of parameters. The authors show that the proposed KFL has storage and computational costs that scale linearly in the number of input features. \nExperimental results show that KFL has better speed and storage space. The authors also provide necessary and sufficient conditions for a KFL model to be monotonic with respect to some features. \n\n**+ves**\n+ This appears to be a new parametrization of monotonic lattice regression with parametric and computational advanages\n+ Theoretical results show necessary and sufficient conditions for KFL to be monotonic, and this allows the design of training algorithms; there are also theoretical results on the capacity\n+ Experiments on public and proprietary datasets show that KFL maintains error rate while needing less time to train and fewer parameters. \n\n**Concerns**\n- The gains in training and eval time (especially compared to simplex) seem modest. However the number of parameters is significantly reduced. I wonder if a larger dataset/more complex task would demonstrate the benefits more clearly. \n\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1044/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1044/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Monotonic Kronecker-Factored Lattice", "authorids": ["~William_Taylor_Bakst1", "~Nobuyuki_Morioka1", "~Erez_Louidor1"], "authors": ["William Taylor Bakst", "Nobuyuki Morioka", "Erez Louidor"], "keywords": ["Theory", "Regularization", "Algorithms", "Classification", "Regression", "Matrix and Tensor Factorization", "Fairness", "Evaluation", "Efficiency", "Machine Learning"], "abstract": "It is computationally challenging to learn flexible monotonic functions that guarantee model behavior and provide interpretability beyond a few input features, and in a time where minimizing resource use is increasingly important, we must be able to learn such models that are still efficient. In this paper we show how to effectively and efficiently learn such functions using Kronecker-Factored Lattice ($\\mathrm{KFL}$), an efficient reparameterization of flexible monotonic lattice regression via Kronecker product. Both computational and storage costs scale linearly in the number of input features, which is a significant improvement over existing methods that grow exponentially. We also show that we can still properly enforce monotonicity and other shape constraints. The $\\mathrm{KFL}$ function class consists of products of piecewise-linear functions, and the size of the function class can be further increased through ensembling. We prove that the function class of an ensemble of $M$ base $\\mathrm{KFL}$ models strictly increases as $M$ increases up to a certain threshold. Beyond this threshold, every multilinear interpolated lattice function can be expressed. Our experimental results demonstrate that $\\mathrm{KFL}$ trains faster with fewer parameters while still achieving accuracy and evaluation speeds comparable to or better than the baseline methods and preserving monotonicity guarantees on the learned model.", "one-sentence_summary": "We show how to effectively and efficiently learn flexible and interpretable monotonic functions using Kronecker-Factored Lattice, an efficient reparameterization of flexible monotonic lattice regression via Kronecker product.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bakst|monotonic_kroneckerfactored_lattice", "pdf": "/pdf/86fd2e9cf39fac3f9c7f37e71683f60eb7e3e575.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nbakst2021monotonic,\ntitle={Monotonic Kronecker-Factored Lattice},\nauthor={William Taylor Bakst and Nobuyuki Morioka and Erez Louidor},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=0pxiMpCyBtr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "0pxiMpCyBtr", "replyto": "0pxiMpCyBtr", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1044/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538128436, "tmdate": 1606915797935, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1044/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1044/-/Official_Review"}}}], "count": 14}