{"notes": [{"id": "SkxlElBYDS", "original": "S1lh2BxtwS", "number": 2234, "cdate": 1569439783580, "ddate": null, "tcdate": 1569439783580, "tmdate": 1577168216632, "tddate": null, "forum": "SkxlElBYDS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Continual Learning via Principal Components Projection", "authors": ["Gyuhak Kim", "Bing Liu"], "authorids": ["gkim87@uic.edu", "liub@uic.edu"], "keywords": ["Neural network", "continual learning", "catastrophic forgetting", "lifelong learning"], "abstract": "Continual learning in neural networks (NN) often suffers from catastrophic forgetting. That is, when learning a sequence of tasks on an NN, the learning of a new task will cause weight changes that may destroy the learned knowledge embedded in the weights for previous tasks. Without solving this problem, it is difficult to use an NN to perform continual or lifelong learning. Although researchers have attempted to solve the problem in many ways, it remains to be challenging. In this paper, we propose a new approach, called principal components projection (PCP). The idea is that in learning a new task, if we can ensure that the gradient updates will only occur in the orthogonal directions to the input vectors of the previous tasks, then the weight updates for learning the new task will not affect the previous tasks. We propose to compute the principal components of the input vectors and use them to transform the input and to project the gradient updates for learning each new task. PCP does not need to store any sampled data from previous tasks or to generate pseudo data of previous tasks and use them to help learn a new task. Empirical evaluation shows that the proposed method PCP markedly outperforms the state-of-the-art baseline methods.", "pdf": "/pdf/64112be7509aa860d2df96c60743896de5c1cc26.pdf", "paperhash": "kim|continual_learning_via_principal_components_projection", "original_pdf": "/attachment/64112be7509aa860d2df96c60743896de5c1cc26.pdf", "_bibtex": "@misc{\nkim2020continual,\ntitle={Continual Learning via Principal Components Projection},\nauthor={Gyuhak Kim and Bing Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxlElBYDS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "dXOs7kkw2", "original": null, "number": 1, "cdate": 1576798743884, "ddate": null, "tcdate": 1576798743884, "tmdate": 1576800892291, "tddate": null, "forum": "SkxlElBYDS", "replyto": "SkxlElBYDS", "invitation": "ICLR.cc/2020/Conference/Paper2234/-/Decision", "content": {"decision": "Reject", "comment": "There is no author response for this paper. The paper addresses the issue of catastrophic forgetting in continual learning. The authors build upon the idea from [Zheng,2019], namely finding gradient updates in the space perpendicular to the input vectors of the previous tasks resulting in less forgetting, and propose an improvement, namely to use principal component analysis to enable learning new tasks without restricting their solution space as in [Zheng,2019]. \nWhile the reviewers acknowledge the importance to study continual learning, they raised several concerns that were viewed by the AC as critical issues: (1) convincing experimental evaluation -- an analysis that clearly shows how and when the proposed method can solve the issue that [Zheng,2019] faces with (task similarity/dissimilarity scenario) would substantially strengthen the evaluation and would allow to assess the scope and contributions of this work; also see R3\u2019s detailed concerns and questions on empirical evaluation, R2\u2019s suggestion to follow the standard protocols, and R1\u2019s suggestion to use PackNet and HAT as baselines for comparison;  (2) lack of presentation clarity -- see R2\u2019s concerns how to improve, and R1\u2019s suggestions on how to better position the paper. \nA general consensus among reviewers and AC suggests, in its current state the manuscript is not ready for a publication. It needs clarifications, more empirical studies and polish to achieve the desired goal.\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continual Learning via Principal Components Projection", "authors": ["Gyuhak Kim", "Bing Liu"], "authorids": ["gkim87@uic.edu", "liub@uic.edu"], "keywords": ["Neural network", "continual learning", "catastrophic forgetting", "lifelong learning"], "abstract": "Continual learning in neural networks (NN) often suffers from catastrophic forgetting. That is, when learning a sequence of tasks on an NN, the learning of a new task will cause weight changes that may destroy the learned knowledge embedded in the weights for previous tasks. Without solving this problem, it is difficult to use an NN to perform continual or lifelong learning. Although researchers have attempted to solve the problem in many ways, it remains to be challenging. In this paper, we propose a new approach, called principal components projection (PCP). The idea is that in learning a new task, if we can ensure that the gradient updates will only occur in the orthogonal directions to the input vectors of the previous tasks, then the weight updates for learning the new task will not affect the previous tasks. We propose to compute the principal components of the input vectors and use them to transform the input and to project the gradient updates for learning each new task. PCP does not need to store any sampled data from previous tasks or to generate pseudo data of previous tasks and use them to help learn a new task. Empirical evaluation shows that the proposed method PCP markedly outperforms the state-of-the-art baseline methods.", "pdf": "/pdf/64112be7509aa860d2df96c60743896de5c1cc26.pdf", "paperhash": "kim|continual_learning_via_principal_components_projection", "original_pdf": "/attachment/64112be7509aa860d2df96c60743896de5c1cc26.pdf", "_bibtex": "@misc{\nkim2020continual,\ntitle={Continual Learning via Principal Components Projection},\nauthor={Gyuhak Kim and Bing Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxlElBYDS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SkxlElBYDS", "replyto": "SkxlElBYDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795725233, "tmdate": 1576800277034, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2234/-/Decision"}}}, {"id": "r1lB7pU2FS", "original": null, "number": 1, "cdate": 1571740957352, "ddate": null, "tcdate": 1571740957352, "tmdate": 1572972365550, "tddate": null, "forum": "SkxlElBYDS", "replyto": "SkxlElBYDS", "invitation": "ICLR.cc/2020/Conference/Paper2234/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The method proposes a method for continual learning. The method is an extension of recent work, called orthogonal weights modification (OWM) [Zheng,2019]. This method aims to find gradient updates which are perpendicular to the input vectors of previous tasks (resulting in less forgetting). However, the authors argue, that the learning of new tasks is happening in the solution space of the previous tasks, which might severely limit the ability to adapt to new tasks. The authors propose a \u2018principal component\u2019-based solution to this problem. The method is considering the \u2018task continual learning\u2019 scenario (also known as task-aware) which means that the task label is given at inference time.\n\nConclusion:\n\n1. The paper is not well-positioned in related works. I think the work is more related to works with \u2018parameter isolation methods\u2019 such as Piggyback, Packnet, HAT. These methods reserve part of the capacity of the network for tasks. I think the authors should relate their work with these methods, and provide an argument of the problem with these previous methods, which is addressed by their approach. I can see that rather than freezing weights (PackNet) or features (HAT) , the method freezes linear combinations of features. But it is for me not directly clear that that is desirable. In HAT the backpropagated vector is projected on the mask vector which coincides with the neurons (activations). \n\n2. The experimental verification of the paper is too weak, and only comparison to EWC and OWM (not well known) are provided. At least a comparison with the more related works PackNet and HAT should be included. For more recent method for task-aware CL see also \u2018Continual learning: A comparative study on how to defy forgetting in classification tasks\u2019. Also results seem bad. For example on CIFAR10, 5 tasks in TCL setting is two-class problem per task; I would expect better results. \n\n3. The authors claim that OWM is effective if tasks are similar, but not when dissimilar. And the proposed PCP solves this problem. However, all experiments are on similar tasks, and no cross domain tasks are considered, e.g. going from MNIST (task1) to EMNIST-26 (task2) etc. This would empirically support the claim. Also, the authors expect the difference between PCP and OWM to be even larger then. \n\n4. Some more analysis of the success of PCA in representing the distribution would be appreciated, e.g. the percentage of total energy which is captured (sum of selected eigenvalues divided by sum of all eigenvalues). Such an analysis of P_l^k as a function of the tasks (and for several layers) would be interesting to see, for example for EMNIST-47(10 tasks). \n\n5. Novelty with respect to OWM is rather small.\n\n6. The authors should mention that the method is pretrained on ImageNet in section 4.3. Given these datasets, I think it makes more sense to train from scratch and I would like to see those results. \n\nMinor remarks:\n- I wonder if you use OWM or PCP you discard the possibility of positive backward transfer. Maybe the authors could comment on that. \n\n- The authors write that \u2018TCL setting the classification results are usually better than those of the CCL\u2019 is that not per definition true ? Anything correctly classified under CCL is correctly classified under TCL but not the other way around. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2234/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2234/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continual Learning via Principal Components Projection", "authors": ["Gyuhak Kim", "Bing Liu"], "authorids": ["gkim87@uic.edu", "liub@uic.edu"], "keywords": ["Neural network", "continual learning", "catastrophic forgetting", "lifelong learning"], "abstract": "Continual learning in neural networks (NN) often suffers from catastrophic forgetting. That is, when learning a sequence of tasks on an NN, the learning of a new task will cause weight changes that may destroy the learned knowledge embedded in the weights for previous tasks. Without solving this problem, it is difficult to use an NN to perform continual or lifelong learning. Although researchers have attempted to solve the problem in many ways, it remains to be challenging. In this paper, we propose a new approach, called principal components projection (PCP). The idea is that in learning a new task, if we can ensure that the gradient updates will only occur in the orthogonal directions to the input vectors of the previous tasks, then the weight updates for learning the new task will not affect the previous tasks. We propose to compute the principal components of the input vectors and use them to transform the input and to project the gradient updates for learning each new task. PCP does not need to store any sampled data from previous tasks or to generate pseudo data of previous tasks and use them to help learn a new task. Empirical evaluation shows that the proposed method PCP markedly outperforms the state-of-the-art baseline methods.", "pdf": "/pdf/64112be7509aa860d2df96c60743896de5c1cc26.pdf", "paperhash": "kim|continual_learning_via_principal_components_projection", "original_pdf": "/attachment/64112be7509aa860d2df96c60743896de5c1cc26.pdf", "_bibtex": "@misc{\nkim2020continual,\ntitle={Continual Learning via Principal Components Projection},\nauthor={Gyuhak Kim and Bing Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxlElBYDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkxlElBYDS", "replyto": "SkxlElBYDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2234/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2234/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575299604775, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2234/Reviewers"], "noninvitees": [], "tcdate": 1570237725779, "tmdate": 1575299604789, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2234/-/Official_Review"}}}, {"id": "S1gezXh3tB", "original": null, "number": 2, "cdate": 1571762952489, "ddate": null, "tcdate": 1571762952489, "tmdate": 1572972365441, "tddate": null, "forum": "SkxlElBYDS", "replyto": "SkxlElBYDS", "invitation": "ICLR.cc/2020/Conference/Paper2234/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper introduces Principal Components Projection, a method that computes the principal components of input vectors, using them to train on a transformed input space and to project gradient updates. Experiments show improved results over OWM (the method that this paper builds on) and EWC.\n\nIf I understand correctly (which I think may not be the case), the principal component vectors are computed after the first forward/backward pass of each task, for the inputs to each layer (C_l^k). These principal components are then fixed, the orthogonal projection matrix P_l^k is then found, and then normal training is iterated until convergence using this C_l^k and P_l^k.\n\nQuestions:\n- Seeing as (especially for the first task), weights are initialised randomly, why does this method provide reasonable principal components for layers after the first layer? \n- I also do not understand why the dxd projection matrix P, which is orthogonal to all previous basis matrices C, has the property span(P^i) \\subset span(P^j) for i < j. Surely as more basis matrices are found, then the orthogonal space restricts in size.\n- I also do not understand Equation 1. What is \\grad{W}? If it is, as defined 2 pages later, 'the backpropagation with respect to X_{k+1}' [or X_k here], then is Equation 1 saying that only one gradient step is used per task?\n\nThe experiments seem reasonable, except that there are no standard deviations on the results. However, as far as I'm aware, these experimental protocols (dataset and model size) are not used in other papers: it would be nice to see experiments which match previous papers' protocols, for example with MNIST and CIFAR-10 at least (other papers use smaller model sizes).\n\nAs it is currently, I am unable to understand the paper despite spending some time trying to understand it. I am therefore giving the paper a weak reject. Hopefully the authors can answer my questions.\n\nFinally, some minor specific suggestions for improving the writing:\n- Immediately after Equation 12, there is \\grad{P^j} instead of \\grad{W^j}{P^{k-2}}\n- The paragraph before Equation 13 uses 't' instead of 'k' sometimes for task index\n- Use `   not ' for open quotation marks"}, "signatures": ["ICLR.cc/2020/Conference/Paper2234/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2234/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continual Learning via Principal Components Projection", "authors": ["Gyuhak Kim", "Bing Liu"], "authorids": ["gkim87@uic.edu", "liub@uic.edu"], "keywords": ["Neural network", "continual learning", "catastrophic forgetting", "lifelong learning"], "abstract": "Continual learning in neural networks (NN) often suffers from catastrophic forgetting. That is, when learning a sequence of tasks on an NN, the learning of a new task will cause weight changes that may destroy the learned knowledge embedded in the weights for previous tasks. Without solving this problem, it is difficult to use an NN to perform continual or lifelong learning. Although researchers have attempted to solve the problem in many ways, it remains to be challenging. In this paper, we propose a new approach, called principal components projection (PCP). The idea is that in learning a new task, if we can ensure that the gradient updates will only occur in the orthogonal directions to the input vectors of the previous tasks, then the weight updates for learning the new task will not affect the previous tasks. We propose to compute the principal components of the input vectors and use them to transform the input and to project the gradient updates for learning each new task. PCP does not need to store any sampled data from previous tasks or to generate pseudo data of previous tasks and use them to help learn a new task. Empirical evaluation shows that the proposed method PCP markedly outperforms the state-of-the-art baseline methods.", "pdf": "/pdf/64112be7509aa860d2df96c60743896de5c1cc26.pdf", "paperhash": "kim|continual_learning_via_principal_components_projection", "original_pdf": "/attachment/64112be7509aa860d2df96c60743896de5c1cc26.pdf", "_bibtex": "@misc{\nkim2020continual,\ntitle={Continual Learning via Principal Components Projection},\nauthor={Gyuhak Kim and Bing Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxlElBYDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkxlElBYDS", "replyto": "SkxlElBYDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2234/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2234/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575299604775, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2234/Reviewers"], "noninvitees": [], "tcdate": 1570237725779, "tmdate": 1575299604789, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2234/-/Official_Review"}}}, {"id": "SyxXQhYAYr", "original": null, "number": 3, "cdate": 1571884059074, "ddate": null, "tcdate": 1571884059074, "tmdate": 1572972365398, "tddate": null, "forum": "SkxlElBYDS", "replyto": "SkxlElBYDS", "invitation": "ICLR.cc/2020/Conference/Paper2234/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper tries to solve the catastrophic forgetting issue in the continual learning problem. The authors propose a method based on principal components projection to tackle this issue.  The authors conduct experiments on image classification tasks to show the performance of the proposed method and compare it with two other baselines EWC and OWM.\n\nStrong points:\n1. This paper tries to solve an important problem.\n2. The intuition of applying principal components projection is straightforward.\n\nWeak points:\n1. The most concerned point about this paper is the experiment. It is not convincing. The authors claim that OWM is one of the strongest baselines, but actually it perform really badly on EMNIST-26 (5 tasks),  EMNIST-47 (5 tasks) and EMNIST-47 (10 tasks). What is the reason? Is it because of insufficient parameter tuning? If different methods perform differently on various datasets, it is really necessary to show more baseline methods to illustrate that the proposed method has universally good performance on different datasets.\n2. It might strengthen the paper if the authors can show the comparison results on more other datasets, e.g., other image classification tasks. It would be better if the authors can show the proposed method can generalize to other tasks.\n3. The authors point out that one key drawback of OWM is that, if the tasks are not quite related, OWM may perform very badly. Is this the case in the experiment?\n4. It is not clear why the proposed method can solve the issue that OWM faces with (bad accuracy when tasks are not quite related).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2234/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2234/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continual Learning via Principal Components Projection", "authors": ["Gyuhak Kim", "Bing Liu"], "authorids": ["gkim87@uic.edu", "liub@uic.edu"], "keywords": ["Neural network", "continual learning", "catastrophic forgetting", "lifelong learning"], "abstract": "Continual learning in neural networks (NN) often suffers from catastrophic forgetting. That is, when learning a sequence of tasks on an NN, the learning of a new task will cause weight changes that may destroy the learned knowledge embedded in the weights for previous tasks. Without solving this problem, it is difficult to use an NN to perform continual or lifelong learning. Although researchers have attempted to solve the problem in many ways, it remains to be challenging. In this paper, we propose a new approach, called principal components projection (PCP). The idea is that in learning a new task, if we can ensure that the gradient updates will only occur in the orthogonal directions to the input vectors of the previous tasks, then the weight updates for learning the new task will not affect the previous tasks. We propose to compute the principal components of the input vectors and use them to transform the input and to project the gradient updates for learning each new task. PCP does not need to store any sampled data from previous tasks or to generate pseudo data of previous tasks and use them to help learn a new task. Empirical evaluation shows that the proposed method PCP markedly outperforms the state-of-the-art baseline methods.", "pdf": "/pdf/64112be7509aa860d2df96c60743896de5c1cc26.pdf", "paperhash": "kim|continual_learning_via_principal_components_projection", "original_pdf": "/attachment/64112be7509aa860d2df96c60743896de5c1cc26.pdf", "_bibtex": "@misc{\nkim2020continual,\ntitle={Continual Learning via Principal Components Projection},\nauthor={Gyuhak Kim and Bing Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxlElBYDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkxlElBYDS", "replyto": "SkxlElBYDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2234/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2234/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575299604775, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2234/Reviewers"], "noninvitees": [], "tcdate": 1570237725779, "tmdate": 1575299604789, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2234/-/Official_Review"}}}], "count": 5}