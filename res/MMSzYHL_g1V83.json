{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1391857440000, "tcdate": 1391857440000, "number": 3, "id": "jyk6ZpLAXVZNe", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "MMSzYHL_g1V83", "replyto": "MMSzYHL_g1V83", "signatures": ["anonymous reviewer a728"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Rate-Distortion Auto-Encoders", "review": "* Brief summary of paper:\r\nThe paper proposes a novel kind of regularization for learning autoencoders that is rooted in rate-distortion theory.\r\nThe regularization term is a kernel-based estimator of entropy (of the reconstructed data) proposed by the authors at last year's ICLR.\r\nExperiments on 2D toy data show that it works as expected.\r\n\r\n* Assessment:\r\n\r\nThe rate-distortion approach to autoencoders is interesting, and I beleive novel. I would however refrain from stating, as written in the paper, that it allows learning autoencoders without an explicit regularization terms: the conditional entropy clearly plays the role of a data-dependent regularization term (similar to most alternative approaches for learning overcomplete autoencders: they also don't define an explicit penalty on the parameters). \r\n\r\nThe kernel-based entropy estimator seems however computationally very expensive, since it requires full eigendecomposition of Gram matrices. \r\n\r\nThe main weakness of the paper is the very limited experimental evaluation of the method (2D toys), and the lack of comparison with any other regularized autoencoder approach, not even a discussion of what this new mathematically sophisticated and computationally heavy approach might offer as benefits. Similarly, the authors only use their own gram-matrix-based entropy estimator: a brief discussion of properly referenced alternative, more classical, nonparametric entropy estimators would have been in order (and ideally with experimental comparison). \r\n\r\nThe paper is mostly well written. There is however a confusing proabably unintended notational shift happening after Eq. 14. In eq 14 and in ghe previous sub-section you use S_alpha, and in what follows you use H(K) which has nowhere been formally defined or related to S_alpha. Also where does the N arise from in eq 16. Please clarify.\r\n\r\nLastly I have a question/remark: conditional entropy H(X|X^) could also be rewritten as \r\nH(X|X^) = H(X) + H(X^|X) - H(X^)\r\nNow since you are concerned with a deterministic autoencoder, X^ is a deterministic function of X, so it would seem that H(X^ | X) is a constant.\r\nSo it seems you might as well penalize only H(X^) rathe than trying to estimate and penalize the actual conditional entropy. Does this reasoning seem valid? \r\n\r\n\r\n* Pros and cons:\r\n\r\nPros:\r\n+ original approach to autoencoder regularization\r\n\r\nCons:\r\n- computationally very heavy approach\r\n- very limited experimental evaluation (toy 2d data)\r\n- lack of comparison (neither in discussion nor experimental) with anything related"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Rate-Distortion Auto-Encoders", "decision": "submitted, no decision", "abstract": "We propose a learning algorithm for auto-encoders based on a rate-distortion objective. Our goal is to minimize the mutual information between the inputs and the outputs of an auto-encoder subject to a fidelity constraint. Minimizing the mutual information acts as a regularization term whereas the fidelity constraint can be understood as a risk functional in the conventional statistical learning setting. The proposed algorithm uses a recently introduced measure of entropy based on infinitely divisible matrices that avoids the plug in estimation of densities. Experiments using over-complete bases show that the auto-encoder learns a regularized input-output map without explicit regularization terms or add-hoc constraints such as tied weights.", "pdf": "https://arxiv.org/abs/1312.7381", "paperhash": "giraldo|ratedistortion_autoencoders", "keywords": [], "conflicts": [], "authors": ["Luis G. Sanchez Giraldo", "Jose C. Principe"], "authorids": ["sanchez@cnel.ufl.edu", "principe@cnel.ufl.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391498460000, "tcdate": 1391498460000, "number": 2, "id": "W06vWEntsKUx4", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "MMSzYHL_g1V83", "replyto": "MMSzYHL_g1V83", "signatures": ["anonymous reviewer f2d6"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Rate-Distortion Auto-Encoders", "review": "This paper proposes a new criterion to train auto-encoders: minimizing the\r\nmutual information (MI) between the input and output distributions, under the\r\nconstraint that the output is close enough to the output. This constraint can\r\nbe seen as the 'risk' we want to keep small, while the minimization of the MI\r\nadds regularization to prevent overfitting (i.e. learning an identity mapping).\r\n\r\nThis seems to be an interesting direction to investigate, however I find the\r\nsubmitted paper to fall short in two important areas:\r\n\r\n1. The motivation and intuition behind this algorithm are not very clear. At\r\nthe end of section 2, we do see that minimizing the MI 'can have the effect of\r\nlowering the entropy of the output variable and thus, we can think of the\r\nmapping f as a contraction' but that does not really explain what kind of\r\nproperties we can expect when minimizing eq. 2. For instance, if we wanted to\r\nlower this entropy, why not just do that directly? Another point that confuses\r\nme is that the problem is initially stated as a constrained optimization one,\r\nbut if I understand correctly, the actual algorithm is performing gradient\r\ndescent on the Lagrangian (eq. 8): this Lagrangian is the sum of two terms, one\r\nbeing the risk to minimize, and one being the regularization, and the parameter\r\nmu gives the trade-off between the two. Now my question is: why not start\r\ndirectly from this criterion (which I personally find more intuitive) instead\r\nof the constrained optimization formulation (whose added value is not obvious\r\nto me)?\r\n\r\n2. The experiments are extremely limited, being run only on two 2D toy datasets \r\nand without any comparison to other popular auto-encoder algorithms. I also\r\nfeel they are not enough to give additional intuition on the algorithm's\r\nbehavior. Here are some examples of topics which could have been investigated:\r\n- How does the output change with mu?\r\n- How does the output change with different kernels used in the entropy \r\n  estimations?\r\n- How does the algorithm behave on real data?\r\n- How does the algorithm behave as dimension increases? (when the data do not \r\n  lie on a low-dimensional manifold, local kernel methods tend to fail)\r\n- How does the algorithm behave compared to the typical auto-encoders mentioned \r\n  in the introduction? (the goal would not necessarily be to show it works\r\n  better, but to gain understanding of the differences between algorithms)\r\n\r\nOverall, an interesting idea, but one that would deserve a more in-depth \r\ntreatment (note that novelty seems limited, since the starting point is a\r\ncriterion already proposed for manifold learning, and the kernel-based entropy\r\nestimation comes from a previous paper by the same authors).\r\n\r\nA few more small points:\r\n- There is a non negligible amount of typos, it could use a proofread pass.\r\n- Notation inconsistencies (or not well explained): D instead of d in intro\r\n  (not defined by the way), using both hat{x} and \tilde{x} to denote the\r\n  reconstructed data (and hat{x} is also used in the intro to denote the noisy\r\n  input in the denoising auto-encoder), multiple P's in eq. 1, not clear if we\r\n  are working in a continuous (eq.1)  or discrete (eq. 3) space, h not defined in\r\n  1st paragraph of 2.1.\r\n- The manifold learning algorithm described in Section 1 is not very clear. Eq.1\r\n  seems to be only part of the cost, and it is not clear if it is minimized\r\n  or maximized. Unfortunately I did not have time to read the corresponding\r\n  reference, but I feel like it may deserver a more thorough description, given\r\n  that it seems to be key to the algorithm presented here.\r\n- Greek letters cannot be seen on some PDF readers (like an iPad), although it\r\n  works under Windows."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Rate-Distortion Auto-Encoders", "decision": "submitted, no decision", "abstract": "We propose a learning algorithm for auto-encoders based on a rate-distortion objective. Our goal is to minimize the mutual information between the inputs and the outputs of an auto-encoder subject to a fidelity constraint. Minimizing the mutual information acts as a regularization term whereas the fidelity constraint can be understood as a risk functional in the conventional statistical learning setting. The proposed algorithm uses a recently introduced measure of entropy based on infinitely divisible matrices that avoids the plug in estimation of densities. Experiments using over-complete bases show that the auto-encoder learns a regularized input-output map without explicit regularization terms or add-hoc constraints such as tied weights.", "pdf": "https://arxiv.org/abs/1312.7381", "paperhash": "giraldo|ratedistortion_autoencoders", "keywords": [], "conflicts": [], "authors": ["Luis G. Sanchez Giraldo", "Jose C. Principe"], "authorids": ["sanchez@cnel.ufl.edu", "principe@cnel.ufl.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391459640000, "tcdate": 1391459640000, "number": 1, "id": "iBE1B8FLXVYTa", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "MMSzYHL_g1V83", "replyto": "MMSzYHL_g1V83", "signatures": ["anonymous reviewer b49b"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Rate-Distortion Auto-Encoders", "review": "This paper proposes to regularize auto-encoders by minimizing the mutual information between input and output. The minimization of mutual information is based on an alternative definition of entropy (Sanchez et al. 2013). Although auto-encoders have been around for more than 20 years, the introduction of deep learning (Hinton et al. 2006,Bengio et al. 2006) has renewed the interest in these models and their regularization schemes (Vincent et al. 2008), as they can be stacked to achieve state-of-the-art performance.\r\n\r\nA first contribution establishes a link between the proposed model (rate-distortion auto-encoders) and PCA in the case of a Gaussian input variable but does not discuss the already established proof that traditional linear auto-encoders are equivalent to PCA (Baldi and Hornik;1989).\r\n\r\nThe authors then derive a gradient training procedure for the rate-distortion auto-encoder and report the result of two experiments in dimension 2, namely they compare the input to the reconstruction :\r\n1 - In the case where the input is a specific Gaussian variable.\r\n2 - In the case where the input is a specific mixture of 3 Gaussian variables.\r\n\r\nfinally the authors  conclude that the reconstruction approximately fits the input distribution.\r\n\r\nAlthough the use of rate distortion theory to regularize an auto-encoder is new, the paper suffers from several issues. First, the authors do not define the problem which they are trying to address. They present a cost function and their goal is to minimize it. To what end ? Accordingly, it is unclear what the authors are trying to prove in their experiments: a regularization property ? Since the end goad is not clearly defined, why is regularization important at all ? Additionally, the experiments are very insufficient as they only consider two very simple artificial datasets: the first with a single predefined Gaussian, and the second with three Gaussians. In these experiments, the proposed method is  not compared to any other model or baseline.\r\n\r\npros:\r\ncons:\r\n - Goal is not defined.\r\n - experiments are in a very low dimensional space (dim=2) which is not very relevant for auto-encoders.\r\n - experiments do not compare the proposed model to a baseline (traditional auto-encoder) or to other forms of regularized auto-encoders (e.g. denoising auto-encoders or contracting auto-encoders).\r\n - experiments do not report any quantitative measure of performance (e.g. log-likelihood or classification accuracy)."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Rate-Distortion Auto-Encoders", "decision": "submitted, no decision", "abstract": "We propose a learning algorithm for auto-encoders based on a rate-distortion objective. Our goal is to minimize the mutual information between the inputs and the outputs of an auto-encoder subject to a fidelity constraint. Minimizing the mutual information acts as a regularization term whereas the fidelity constraint can be understood as a risk functional in the conventional statistical learning setting. The proposed algorithm uses a recently introduced measure of entropy based on infinitely divisible matrices that avoids the plug in estimation of densities. Experiments using over-complete bases show that the auto-encoder learns a regularized input-output map without explicit regularization terms or add-hoc constraints such as tied weights.", "pdf": "https://arxiv.org/abs/1312.7381", "paperhash": "giraldo|ratedistortion_autoencoders", "keywords": [], "conflicts": [], "authors": ["Luis G. Sanchez Giraldo", "Jose C. Principe"], "authorids": ["sanchez@cnel.ufl.edu", "principe@cnel.ufl.edu"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1388471280000, "tcdate": 1388471280000, "number": 62, "id": "MMSzYHL_g1V83", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "MMSzYHL_g1V83", "signatures": ["sanchez@cnel.ufl.edu"], "readers": ["everyone"], "content": {"title": "Rate-Distortion Auto-Encoders", "decision": "submitted, no decision", "abstract": "We propose a learning algorithm for auto-encoders based on a rate-distortion objective. Our goal is to minimize the mutual information between the inputs and the outputs of an auto-encoder subject to a fidelity constraint. Minimizing the mutual information acts as a regularization term whereas the fidelity constraint can be understood as a risk functional in the conventional statistical learning setting. The proposed algorithm uses a recently introduced measure of entropy based on infinitely divisible matrices that avoids the plug in estimation of densities. Experiments using over-complete bases show that the auto-encoder learns a regularized input-output map without explicit regularization terms or add-hoc constraints such as tied weights.", "pdf": "https://arxiv.org/abs/1312.7381", "paperhash": "giraldo|ratedistortion_autoencoders", "keywords": [], "conflicts": [], "authors": ["Luis G. Sanchez Giraldo", "Jose C. Principe"], "authorids": ["sanchez@cnel.ufl.edu", "principe@cnel.ufl.edu"]}, "writers": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 4}