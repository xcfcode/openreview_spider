{"notes": [{"id": "SyegvgHtwr", "original": "SyeNWieFvB", "number": 2348, "cdate": 1569439832236, "ddate": null, "tcdate": 1569439832236, "tmdate": 1577168232617, "tddate": null, "forum": "SyegvgHtwr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["rcornish@robots.ox.ac.uk", "anthony.caterini@stats.ox.ac.uk", "deligian@stats.ox.ac.uk", "doucet@stats.ox.ac.uk"], "title": "Localised Generative Flows", "authors": ["Rob Cornish", "Anthony Caterini", "George Deligiannidis", "Arnaud Doucet"], "pdf": "/pdf/25aae4bf66a9800822f1385cffcd1374214482c4.pdf", "TL;DR": "We use a deep continuous mixture of bijections to improve normalising flows for density estimation.", "abstract": "We argue that flow-based density models based on continuous bijections are limited in their ability to learn target distributions with complicated topologies, and propose localised generative flows (LGFs) to address this problem. LGFs are composed of stacked continuous mixtures of bijections, which enables each bijection to learn a local region of the target rather than its entirety. Our method is a generalisation of existing flow-based methods, which can be used without modification as the basis for an LGF model. Unlike normalising flows, LGFs do not permit exact computation of log likelihoods, but we propose a simple variational scheme that performs well in practice. We show empirically that LGFs yield improved performance across a variety of common density estimation tasks.", "code": "https://github.com/anonsubmission974/lgf", "keywords": ["Deep generative models", "normalizing flows", "variational inference"], "paperhash": "cornish|localised_generative_flows", "original_pdf": "/attachment/23609fcb86259d5db679c20403c0fd6fe2a4e38f.pdf", "_bibtex": "@misc{\ncornish2020localised,\ntitle={Localised Generative Flows},\nauthor={Rob Cornish and Anthony Caterini and George Deligiannidis and Arnaud Doucet},\nyear={2020},\nurl={https://openreview.net/forum?id=SyegvgHtwr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "oJ4lESJm-a", "original": null, "number": 1, "cdate": 1576798746806, "ddate": null, "tcdate": 1576798746806, "tmdate": 1576800889290, "tddate": null, "forum": "SyegvgHtwr", "replyto": "SyegvgHtwr", "invitation": "ICLR.cc/2020/Conference/Paper2348/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes to overcome some fundamental limitations of normalizing flows by introducing auxiliary continuous latent variables. While the problem this paper is trying to address is mathematically legitimate, there is no strong evidence that this is a relevant problem in practice. Moreover, the proposed solution is not entirely novel, converting the flow in a latent-variable model. Overall, I believe this paper will be of minor relevance to the ICLR community.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rcornish@robots.ox.ac.uk", "anthony.caterini@stats.ox.ac.uk", "deligian@stats.ox.ac.uk", "doucet@stats.ox.ac.uk"], "title": "Localised Generative Flows", "authors": ["Rob Cornish", "Anthony Caterini", "George Deligiannidis", "Arnaud Doucet"], "pdf": "/pdf/25aae4bf66a9800822f1385cffcd1374214482c4.pdf", "TL;DR": "We use a deep continuous mixture of bijections to improve normalising flows for density estimation.", "abstract": "We argue that flow-based density models based on continuous bijections are limited in their ability to learn target distributions with complicated topologies, and propose localised generative flows (LGFs) to address this problem. LGFs are composed of stacked continuous mixtures of bijections, which enables each bijection to learn a local region of the target rather than its entirety. Our method is a generalisation of existing flow-based methods, which can be used without modification as the basis for an LGF model. Unlike normalising flows, LGFs do not permit exact computation of log likelihoods, but we propose a simple variational scheme that performs well in practice. We show empirically that LGFs yield improved performance across a variety of common density estimation tasks.", "code": "https://github.com/anonsubmission974/lgf", "keywords": ["Deep generative models", "normalizing flows", "variational inference"], "paperhash": "cornish|localised_generative_flows", "original_pdf": "/attachment/23609fcb86259d5db679c20403c0fd6fe2a4e38f.pdf", "_bibtex": "@misc{\ncornish2020localised,\ntitle={Localised Generative Flows},\nauthor={Rob Cornish and Anthony Caterini and George Deligiannidis and Arnaud Doucet},\nyear={2020},\nurl={https://openreview.net/forum?id=SyegvgHtwr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SyegvgHtwr", "replyto": "SyegvgHtwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795708518, "tmdate": 1576800256963, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2348/-/Decision"}}}, {"id": "r1xPlCLSFS", "original": null, "number": 1, "cdate": 1571282414688, "ddate": null, "tcdate": 1571282414688, "tmdate": 1574137456572, "tddate": null, "forum": "SyegvgHtwr", "replyto": "SyegvgHtwr", "invitation": "ICLR.cc/2020/Conference/Paper2348/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "The paper introduces a straight-forward way to expand the flow models by considering mixture of flow distributions. The idea is not very novel since several previous work have tried the mixture of flow such as the mentioned RAD and Deep Mixture. The paper studies some further improvements such as using the continuous auxiliary variable and stacking multiple mixture layers. \n\nThe major concerns are the following:\n\n1) The paper tries to solve a \u201cproblem\u201d build upon intuition. The paper explains as \u201cthe normalizing flow places global constraint on the bijection\u201d, \u201cit need to match the topology of  X to the topology of Z \u201d, \u201ccontinuous functions necessarily preserve topology\u201d. What kind of topological properties are referred to here? Are all topological properties preserved under continuous function? It needs to be more accurate when using such terminologies. The intuition of the paper is weak and heuristic. The example in Figure 1 can potentially be easily solved with a two component Gaussian mixture of input Z to a vanilla flow model.\n\n2) For the mixture p(X), will the proposed method generate samples concentrated on one or some of the components? Why or why not?\n\n3) Considering there are a plenty of improvements of flow models, it is neccesary for the proposed method to compare with, at least for some methods explained in the Related Work section.\n\n4) Since the proposed methods inevitably lose the advantage of analytic density property of flow methods, it is better to show some advantage over implicit or semi-implicit methods. For example, (https://arxiv.org/abs/1805.11183) also uses a hierarchical model with continuous auxiliary variables and a marginalization similar to Eq.(4) in this paper. How does the proposed method related to or compared to these methods?\n\n5) In experiment section, in Table 1, the proposed method is worse than MAF for 2 datasets out of 4? In Table 2, what are the numbers refer to?\n\n6) On page 4, it should be p(U|Z) not p(U|X)?\n\nIn sum, I think though the paper makes contribution on exploring better flow models but the novelty is relatively weak, the discussion and comparison of related work is insufficient and the experiments are not convincing or have mistakes. I think a modification is necessary before publishing.\n\n################\n\nI have read the author's feedback.\n\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2348/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2348/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rcornish@robots.ox.ac.uk", "anthony.caterini@stats.ox.ac.uk", "deligian@stats.ox.ac.uk", "doucet@stats.ox.ac.uk"], "title": "Localised Generative Flows", "authors": ["Rob Cornish", "Anthony Caterini", "George Deligiannidis", "Arnaud Doucet"], "pdf": "/pdf/25aae4bf66a9800822f1385cffcd1374214482c4.pdf", "TL;DR": "We use a deep continuous mixture of bijections to improve normalising flows for density estimation.", "abstract": "We argue that flow-based density models based on continuous bijections are limited in their ability to learn target distributions with complicated topologies, and propose localised generative flows (LGFs) to address this problem. LGFs are composed of stacked continuous mixtures of bijections, which enables each bijection to learn a local region of the target rather than its entirety. Our method is a generalisation of existing flow-based methods, which can be used without modification as the basis for an LGF model. Unlike normalising flows, LGFs do not permit exact computation of log likelihoods, but we propose a simple variational scheme that performs well in practice. We show empirically that LGFs yield improved performance across a variety of common density estimation tasks.", "code": "https://github.com/anonsubmission974/lgf", "keywords": ["Deep generative models", "normalizing flows", "variational inference"], "paperhash": "cornish|localised_generative_flows", "original_pdf": "/attachment/23609fcb86259d5db679c20403c0fd6fe2a4e38f.pdf", "_bibtex": "@misc{\ncornish2020localised,\ntitle={Localised Generative Flows},\nauthor={Rob Cornish and Anthony Caterini and George Deligiannidis and Arnaud Doucet},\nyear={2020},\nurl={https://openreview.net/forum?id=SyegvgHtwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SyegvgHtwr", "replyto": "SyegvgHtwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2348/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2348/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575829041581, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2348/Reviewers"], "noninvitees": [], "tcdate": 1570237724117, "tmdate": 1575829041593, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2348/-/Official_Review"}}}, {"id": "HyltFrE2jS", "original": null, "number": 2, "cdate": 1573827968869, "ddate": null, "tcdate": 1573827968869, "tmdate": 1573854801215, "tddate": null, "forum": "SyegvgHtwr", "replyto": "SyegvgHtwr", "invitation": "ICLR.cc/2020/Conference/Paper2348/-/Official_Comment", "content": {"title": "Common Response to All Reviewers - Part I", "comment": "We thank the reviewers for their feedback and comments. In this thread, we respond to several points that were common across all reviews. More specific replies are also given in the individual reviewer threads below.\n\n1. Is our paper purely conjectural?\n\nAll reviewers were critical of what they perceive to be a lack of solid theoretical justification for our approach. It is indeed true that our discussion involves some conjecture, which we are careful to make explicit whenever it occurs. However, many of our statements are mathematically precise. For instance, the discussion in the second paragraph of section 2 constitutes a proof of the following proposition:\n\n* Proposition: If the supports of $p^*_X$ and $p_Z$ are not homeomorphic, then no normalising flow model with $p_Z$ as the prior will yield $p_X^*$ exactly.\n\nThis proposition pinpoints a concrete, well-defined problem with normalising flows that, to our knowledge, has not been addressed in the literature to date.\n\nSimilarly, although presented informally, our discussion in section 3.2 of the paper also provides a clear theoretical account of the potential benefits of LGF models compared with standard normalising flows. This discussion can be summarised by the following proposition:\n\n* Proposition: Suppose that $\\text{supp} p_X^*$ is open and $G(\\cdot; u)$ is continuous for each $u$. Suppose further that, for each $x \\in \\text{supp} p_X^*$, the set\n\\[\n    B_x := \\{u : p_Z(G(x; u)) |\\det DG(x; u)| > 0 \\}\n\\]\nhas positive Lebesgue measure. Then there exists $p_{U|Z}$ such that $\\text{supp} p_X = \\text{supp} p_X^*$.\n\nIn other words, a sufficiently expressive $p_{U|Z}$ can correct the problem identified in the earlier proposition, which stems from the fact that homeomorphisms do not exist between sets with different topologies. The condition on $B_x$ is trivially satisfied in the standard case that $p_Z$ has full support and $|\\det DG(x;u)| > 0$ for all $x$ and $u$. The proof of this result is straightforward and proceeds exactly as described in section 3.2 of our paper, with $p_{U|Z}$ constructed so as to downweight regions of mass that fall outside the support of the target.\n\nTo better clarify our discussion in the paper, we have stated both results more formally in an updated version resubmitted above.\n\n[Continued below....]"}, "signatures": ["ICLR.cc/2020/Conference/Paper2348/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2348/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rcornish@robots.ox.ac.uk", "anthony.caterini@stats.ox.ac.uk", "deligian@stats.ox.ac.uk", "doucet@stats.ox.ac.uk"], "title": "Localised Generative Flows", "authors": ["Rob Cornish", "Anthony Caterini", "George Deligiannidis", "Arnaud Doucet"], "pdf": "/pdf/25aae4bf66a9800822f1385cffcd1374214482c4.pdf", "TL;DR": "We use a deep continuous mixture of bijections to improve normalising flows for density estimation.", "abstract": "We argue that flow-based density models based on continuous bijections are limited in their ability to learn target distributions with complicated topologies, and propose localised generative flows (LGFs) to address this problem. LGFs are composed of stacked continuous mixtures of bijections, which enables each bijection to learn a local region of the target rather than its entirety. Our method is a generalisation of existing flow-based methods, which can be used without modification as the basis for an LGF model. Unlike normalising flows, LGFs do not permit exact computation of log likelihoods, but we propose a simple variational scheme that performs well in practice. We show empirically that LGFs yield improved performance across a variety of common density estimation tasks.", "code": "https://github.com/anonsubmission974/lgf", "keywords": ["Deep generative models", "normalizing flows", "variational inference"], "paperhash": "cornish|localised_generative_flows", "original_pdf": "/attachment/23609fcb86259d5db679c20403c0fd6fe2a4e38f.pdf", "_bibtex": "@misc{\ncornish2020localised,\ntitle={Localised Generative Flows},\nauthor={Rob Cornish and Anthony Caterini and George Deligiannidis and Arnaud Doucet},\nyear={2020},\nurl={https://openreview.net/forum?id=SyegvgHtwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyegvgHtwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2348/Authors", "ICLR.cc/2020/Conference/Paper2348/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2348/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2348/Reviewers", "ICLR.cc/2020/Conference/Paper2348/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2348/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2348/Authors|ICLR.cc/2020/Conference/Paper2348/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142702, "tmdate": 1576860554188, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2348/Authors", "ICLR.cc/2020/Conference/Paper2348/Reviewers", "ICLR.cc/2020/Conference/Paper2348/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2348/-/Official_Comment"}}}, {"id": "H1gWPLNhsr", "original": null, "number": 3, "cdate": 1573828185431, "ddate": null, "tcdate": 1573828185431, "tmdate": 1573854569576, "tddate": null, "forum": "SyegvgHtwr", "replyto": "HyltFrE2jS", "invitation": "ICLR.cc/2020/Conference/Paper2348/-/Official_Comment", "content": {"title": "Common Response to All Reviewers - Part II", "comment": "2. Were our experiments comprehensive enough?\n\nAll reviewers argued that our experiments were not comprehensive enough to serve as a justification for our method, suggesting that, while we demonstrate improvement on the baseline models we consider, we may encounter diminishing returns when using LGFs in conjunction with a stronger underlying model.\n\nTo this, we first emphasise that the experiments we report do indeed demonstrate a conclusive and comprehensive benefit of LGFs within the scope considered. For the MAF and RealNVP models that we use, LGFs _uniformly_ improve performance across a variety of interesting density tasks. These tasks vary significantly in size, with the dimensionality of the data ranging over 3 orders of magnitude from fewer than 10 dimensions (for the 2D datasets, GAS, and POWER) to 3072 dimensions (for CIFAR-10). These tasks also vary significantly in terms of structure; in particular, we show that LGFs scale to handle large-scale, fully convolutional, multi-scale density models (i.e. the RealNVP model considered) that exploit the pixel structure of MNIST and CIFAR-10. (Indeed, this has not been demonstrated for several of the benchmarks suggested in the reviews.) While we do not include results for current state-of-the-art density models, we believe the consistent improvement over these baselines alone is compelling.\n\nHowever, we do also agree that more baselines are always better. To this end, we report additional experimental results here. First, we considered the Rational Quadratic Spline from the Neural Spline Flows (NSF) paper (Durkan et al., 2019). Here, LGF yielded improved stability and test scores for a variety of 2D experiments, which can be reproduced using an update that we have pushed to our repository above. (See for example the \"rings\" dataset, which is quite topologically distinct from the standard Gaussian prior and therefore difficult to train on for the baseline.) We also compared NSF with LGF-NSF on the POWER, GAS, and MINIBOONE UCI datasets (the limited time for rebuttal precluded the inclusion of the HEPMASS dataset). For each dataset and model, we report the test-set log likelihood averaged over three random seeds (higher is better). We use the hyperparameter settings provided in the Appendix of the NSF paper for the baseline. On POWER: NSF achieves 0.65, with 3004722 parameters; LGF-NSF achieves 0.66, with 3064418 parameters. On GAS: NSF achieves 12.50, with 3128392 parameters; LGF-NSF achieves 12.29, with 3161104 parameters. On MINIBOONE: NSF achieves -10.80, with 212102 parameters; LGF-NSF achieves -9.51, with 169168 parameters. So, when both models have a number of parameters that is approximately equal, we obtained the same behaviour on POWER, slightly lower likelihoods on GAS, but a significant improvement on MINIBOONE. We also note that we achieved these results with the first hyperparameter settings (namely, the choice of $p$, $q$, and $s$ and $t$ networks) for our model that we attempted, and therefore expect further improvements to be possible here.\n\nWe additionally experimented with a large-scale Glow model (Kingma & Dhariwal, 2018). As a baseline, we used exactly the model used in (Kingma & Dhariwal, 2018), but replaced their act norm layers with batch norm. We made our LGF model shallower, using only 2 multi-scale steps as opposed to the default of 3. We also reduced the number of hidden channels in the coupling networks from 512 (the default) to 256. Both our baseline and LGF here achieve the same 3.40 BPD on CIFAR10 after 1000 epochs. However, our model uses only 15M parameters, as opposed to the 44M required by Glow. Again, we achieved these results with minimal tuning of our method, and we believe it possible to improve performance here even further.\n\nWe also tried SoS (Jaini et al, 2019), but found we could not reproduce the results described in the paper. In particular, we found the method to be unstable -- to the extent that, for the UCI datasets, we did not achieve a non-NaN value for the average test-set log likelihood on any run (i.e. at least one test value was NaN). We believe this is due to the fact that the suggested configuration in Jaini et al. (2019) means the bijection considered involves 9th order polynomials, which can produce extremely large values when given inputs with values outside the range $[-1, 1]$. It was therefore unclear how to incorporate LGFs into this context.\n\n[Continued below....]"}, "signatures": ["ICLR.cc/2020/Conference/Paper2348/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2348/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rcornish@robots.ox.ac.uk", "anthony.caterini@stats.ox.ac.uk", "deligian@stats.ox.ac.uk", "doucet@stats.ox.ac.uk"], "title": "Localised Generative Flows", "authors": ["Rob Cornish", "Anthony Caterini", "George Deligiannidis", "Arnaud Doucet"], "pdf": "/pdf/25aae4bf66a9800822f1385cffcd1374214482c4.pdf", "TL;DR": "We use a deep continuous mixture of bijections to improve normalising flows for density estimation.", "abstract": "We argue that flow-based density models based on continuous bijections are limited in their ability to learn target distributions with complicated topologies, and propose localised generative flows (LGFs) to address this problem. LGFs are composed of stacked continuous mixtures of bijections, which enables each bijection to learn a local region of the target rather than its entirety. Our method is a generalisation of existing flow-based methods, which can be used without modification as the basis for an LGF model. Unlike normalising flows, LGFs do not permit exact computation of log likelihoods, but we propose a simple variational scheme that performs well in practice. We show empirically that LGFs yield improved performance across a variety of common density estimation tasks.", "code": "https://github.com/anonsubmission974/lgf", "keywords": ["Deep generative models", "normalizing flows", "variational inference"], "paperhash": "cornish|localised_generative_flows", "original_pdf": "/attachment/23609fcb86259d5db679c20403c0fd6fe2a4e38f.pdf", "_bibtex": "@misc{\ncornish2020localised,\ntitle={Localised Generative Flows},\nauthor={Rob Cornish and Anthony Caterini and George Deligiannidis and Arnaud Doucet},\nyear={2020},\nurl={https://openreview.net/forum?id=SyegvgHtwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyegvgHtwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2348/Authors", "ICLR.cc/2020/Conference/Paper2348/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2348/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2348/Reviewers", "ICLR.cc/2020/Conference/Paper2348/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2348/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2348/Authors|ICLR.cc/2020/Conference/Paper2348/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142702, "tmdate": 1576860554188, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2348/Authors", "ICLR.cc/2020/Conference/Paper2348/Reviewers", "ICLR.cc/2020/Conference/Paper2348/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2348/-/Official_Comment"}}}, {"id": "ryeRNDV2ir", "original": null, "number": 7, "cdate": 1573828405785, "ddate": null, "tcdate": 1573828405785, "tmdate": 1573854141886, "tddate": null, "forum": "SyegvgHtwr", "replyto": "r1xPlCLSFS", "invitation": "ICLR.cc/2020/Conference/Paper2348/-/Official_Comment", "content": {"title": "Response to Official Blind Reviewer #2", "comment": "Thank you for your review.\n\nWe first address the claim that LGFs lack novelty. Please see above our discussion about RAD. In particular, our use of continuous mixing variables is not simply an incremental design choice, but provides a method for circumventing the computational cost which is exponentially increasing with the depth of a standard deep mixture model (as mentioned in the Appendix and related work section) _without_ introducing discontinuities into the loss function or explicit partitioning schemes. Moreover, continuous mixing variables require a significantly different approach to training (namely a variational scheme) which further distinguishes LGFs from these other methods.\n\n\nPlease see below our responses to your other points:\n\n1) Regarding the claim that our paper is pure intuition, please see the discussion above. As we discuss there, although we did not previously formulate our discussion in terms of concrete mathematical statements, we did take care to be precise where this was possible. In particular, note that when we talk about \"preserving topology\", we mean so in the well-defined mathematical sense that the topology (i.e. the open sets) of a space is preserved under continuous mappings that have continuous inverses. Such functions indeed preserve _every_ topological characteristic, including properties such as connectedness, compactness, genus (i.e. the number of \"holes\" in the space), etc. These are standard mathematical concepts, and we refer you to https://en.wikipedia.org/wiki/Homeomorphism for more information and references.\n\nRegarding the comment that the example in Figure 1 would be fixed by a simple mixture model - we acknowledge this directly in the 4th paragraph of section 2. However, as we argue there, this sort of approach does not scale to complicated datasets (like CIFAR10) where the topology of the target is completely opaque. In these instances we would like a method that can somehow learn the topology of the target on its own.\n\n2) As we discuss in section 2, the maximum likelihood objective corresponds to a mode-covering KL objective. In other words, the loss function is encouraged to ensure the support of our model covers all the modes of the target. This is a standard feature of likelihood-based training and is not specific to our model.\n\n3) Refer to the general comment above for a discussion on testing the improvement LGFs make on other flow models. As for other variational improvements on flow models, those are discussed in the related work section. These methods are either outside of the scope of LGFs (e.g. Das et al., (2019), Gritsenko et al. (2019)), orthogonal improvements that could be combined with LGFs (e.g. Ho et al. (2019)), or RAD (Dinh at al., 2019) which is discussed above.  \n\n4) The linked work (Semi-Implicit Variational Inference (SIVI)) may indeed seem superficially similar, but it is notably different in both its motivation and application. In particular, SIVI is a method which looks to improve the approximate variational distribution in variational inference; LGF is a method which looks to improve the bijection approach in density estimation using normalizing flows. LGFs require a variational scheme to optimize their parameters (for which methods like SIVI could potentially be useful), but this is different from considering LGFs to be a variational method on their own. Plus SIVI has no notion of stacking models to obtain even greater expressiveness - there is only one hierarchical layer. \nGANs are another instance of an implicit model, but they provide no reliable method to approximate log-likelihoods. \n\n5) It is not clear what is referred to here. Note that we report the average test set log-likelihood, for which higher is better. In all cases in Table 1, the log-likelihood is higher for LGF-MAF than for MAF.\n\n6) Thank you for pointing this out. We have corrected this typo.\n\nFinally, we wonder whether you can comment further on the mistakes you perceive in our experiments? Please note point 5) above."}, "signatures": ["ICLR.cc/2020/Conference/Paper2348/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2348/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rcornish@robots.ox.ac.uk", "anthony.caterini@stats.ox.ac.uk", "deligian@stats.ox.ac.uk", "doucet@stats.ox.ac.uk"], "title": "Localised Generative Flows", "authors": ["Rob Cornish", "Anthony Caterini", "George Deligiannidis", "Arnaud Doucet"], "pdf": "/pdf/25aae4bf66a9800822f1385cffcd1374214482c4.pdf", "TL;DR": "We use a deep continuous mixture of bijections to improve normalising flows for density estimation.", "abstract": "We argue that flow-based density models based on continuous bijections are limited in their ability to learn target distributions with complicated topologies, and propose localised generative flows (LGFs) to address this problem. LGFs are composed of stacked continuous mixtures of bijections, which enables each bijection to learn a local region of the target rather than its entirety. Our method is a generalisation of existing flow-based methods, which can be used without modification as the basis for an LGF model. Unlike normalising flows, LGFs do not permit exact computation of log likelihoods, but we propose a simple variational scheme that performs well in practice. We show empirically that LGFs yield improved performance across a variety of common density estimation tasks.", "code": "https://github.com/anonsubmission974/lgf", "keywords": ["Deep generative models", "normalizing flows", "variational inference"], "paperhash": "cornish|localised_generative_flows", "original_pdf": "/attachment/23609fcb86259d5db679c20403c0fd6fe2a4e38f.pdf", "_bibtex": "@misc{\ncornish2020localised,\ntitle={Localised Generative Flows},\nauthor={Rob Cornish and Anthony Caterini and George Deligiannidis and Arnaud Doucet},\nyear={2020},\nurl={https://openreview.net/forum?id=SyegvgHtwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyegvgHtwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2348/Authors", "ICLR.cc/2020/Conference/Paper2348/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2348/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2348/Reviewers", "ICLR.cc/2020/Conference/Paper2348/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2348/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2348/Authors|ICLR.cc/2020/Conference/Paper2348/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142702, "tmdate": 1576860554188, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2348/Authors", "ICLR.cc/2020/Conference/Paper2348/Reviewers", "ICLR.cc/2020/Conference/Paper2348/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2348/-/Official_Comment"}}}, {"id": "S1xw68VhjS", "original": null, "number": 5, "cdate": 1573828287085, "ddate": null, "tcdate": 1573828287085, "tmdate": 1573853707114, "tddate": null, "forum": "SyegvgHtwr", "replyto": "BJlmkymy5S", "invitation": "ICLR.cc/2020/Conference/Paper2348/-/Official_Comment", "content": {"title": "Response to Official Blind Reviewer #1", "comment": "Thank you for your review.\n\nFirst and foremost, we wish to state our objection to certain uncharitable comments made within this review. Regarding your claim that our experiments are oversold -- we respectfully disagree with this. The statement of ours that you quote describes improved performance on a variety of _tasks_. We stand by this claim: compared with the standard flow-based baselines that we consider, we found that LGFs do improve density estimation for 2D densities, real-world tabular data, and high-dimensional image data, all of which have very distinct structures and dimensionalities. Please see our general reply above for further discussion of this point, as well as further empirical results.\n\nIn a similar vein, when describing our contribution, you state that our \"(e)xperiments suggest an improvement over MAF\". We again refer you to our discussion above, and emphasise that, for the models considered, our results demonstrate that our method yields significant and unambiguous benefit. Moreover, in addition to MAF, which we apply to tabular data, we also consider a large-scale RealNVP model that makes use of fully convolutional networks and a multi-scale architecture. We believe these changes yield a density model with a significantly different structure and characteristics to MAF, and it bears emphasising that our method provides benefit within this quite distinct context also.\n\nIn response to your claim that our paper is purely based on intuition, we refer you to our general reply above.\n\nRegarding BSDS300 - although we omitted this benchmark, we did consider Fashion-MNIST and CIFAR10, both of which have far higher dimensionality than this dataset (784 and 3072 as opposed to 63 dimensions). Even with this increase in dimension, LGFs still yielded a performance benefit over the baselines we considered. Please also see above for our results using Glow that we have obtained subsequently.\n\nRegarding RAD - please see above.\n\nFinally, regarding your points about the inexact log-likelihoods: as we argue in section 3.3.1 of the paper, this does not pose a major limitation for our method. When log-likelihoods are required at evaluation time, it is straightforward to obtain an estimate using importance sampling as described. This estimate is consistent in the sense that it is possible to achieve as much accuracy as desired simply by taking more importance samples. This approach is standard, for example, within the VAE literature -- and indeed your comment would seem to apply equally to all of these models as well as ours.\n\nFor implicit models like GANs, no straightforward estimate of the likelihood is available at all. In this setting it is also typically impractical to estimate the latent distribution $p(z|x)$ for a given $x$, which can be useful for downstream tasks. We also mention that various normalising flow models exist that do provide fast sampling as well as density estimation -- for instance, RealNVP, which we also consider in this paper.\n\nFinally, we have uploaded a revision of the paper that is not longer than 8 pages."}, "signatures": ["ICLR.cc/2020/Conference/Paper2348/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2348/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rcornish@robots.ox.ac.uk", "anthony.caterini@stats.ox.ac.uk", "deligian@stats.ox.ac.uk", "doucet@stats.ox.ac.uk"], "title": "Localised Generative Flows", "authors": ["Rob Cornish", "Anthony Caterini", "George Deligiannidis", "Arnaud Doucet"], "pdf": "/pdf/25aae4bf66a9800822f1385cffcd1374214482c4.pdf", "TL;DR": "We use a deep continuous mixture of bijections to improve normalising flows for density estimation.", "abstract": "We argue that flow-based density models based on continuous bijections are limited in their ability to learn target distributions with complicated topologies, and propose localised generative flows (LGFs) to address this problem. LGFs are composed of stacked continuous mixtures of bijections, which enables each bijection to learn a local region of the target rather than its entirety. Our method is a generalisation of existing flow-based methods, which can be used without modification as the basis for an LGF model. Unlike normalising flows, LGFs do not permit exact computation of log likelihoods, but we propose a simple variational scheme that performs well in practice. We show empirically that LGFs yield improved performance across a variety of common density estimation tasks.", "code": "https://github.com/anonsubmission974/lgf", "keywords": ["Deep generative models", "normalizing flows", "variational inference"], "paperhash": "cornish|localised_generative_flows", "original_pdf": "/attachment/23609fcb86259d5db679c20403c0fd6fe2a4e38f.pdf", "_bibtex": "@misc{\ncornish2020localised,\ntitle={Localised Generative Flows},\nauthor={Rob Cornish and Anthony Caterini and George Deligiannidis and Arnaud Doucet},\nyear={2020},\nurl={https://openreview.net/forum?id=SyegvgHtwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyegvgHtwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2348/Authors", "ICLR.cc/2020/Conference/Paper2348/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2348/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2348/Reviewers", "ICLR.cc/2020/Conference/Paper2348/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2348/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2348/Authors|ICLR.cc/2020/Conference/Paper2348/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142702, "tmdate": 1576860554188, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2348/Authors", "ICLR.cc/2020/Conference/Paper2348/Reviewers", "ICLR.cc/2020/Conference/Paper2348/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2348/-/Official_Comment"}}}, {"id": "HylQcUE3oB", "original": null, "number": 4, "cdate": 1573828234962, "ddate": null, "tcdate": 1573828234962, "tmdate": 1573828788447, "tddate": null, "forum": "SyegvgHtwr", "replyto": "H1gWPLNhsr", "invitation": "ICLR.cc/2020/Conference/Paper2348/-/Official_Comment", "content": {"title": "Common Response to All Reviewers - Part III", "comment": "We likewise considered B-NAF (De Cao et al, 2019) model, but also encountered problems. In particular, we note that the default choice of $\\tanh$ nonlinearity suggested in the paper (and which we believe their results are based on) means that their model is not a bijection, since $\\tanh$ is not surjective. The suggested alternative choice of LeakyReLU does fix this, but then introduces a second problem of removing the gradient signal on the Jacobian terms in the loss, which become constant almost everywhere. We sought to address this by considering a soft version of the LeakyReLU defined by $x \\mapsto \\epsilon x + (1 - \\epsilon) \\log(1 + e^x)$, where $\\epsilon \\in (0, 1)$  corresponds to the slope on the negative part of the real line. However, while improving over the \"hard\" LeakyReLU, this did not train successfully even for simple 2D experiments. It was again unclear for us how best to consider an LGF version of B-NAF.\n\nWe have updated our repository with code to reproduce all these results, including for SoS and B-NAF.\n\n3. Why didn't we explicitly compare against RAD?\n\nAll reviewers cited RAD (Dinh et al., 2019) as a particular benchmark that we ought to have compared against. We agree that RAD is a very interesting paper. However, we do not believe that the version of RAD that exists online at present is ready to be used as a benchmark yet.\n\nThe choice of a discrete mixing variable brings significant difficulties that we discuss in section B of our appendix. In particular, naive stacking entails that the cost of evaluating likelihoods grows exponentially in the depth of the model, which quickly becomes intractable as larger models are used e.g. for image datasets.\n\nRAD avoids this by partitioning the input space in a way that achieves a linear cost in the depth of the model. However, this partitioning scheme means that the loss landscape becomes discontinuous. Some guidance is provided in their appendix on how to resolve this problem for the case of a one-dimensional density with three components, but it is not immediately clear how to extend this to higher dimensions or more complicated targets. It therefore fell outside the scope of this paper to establish RAD as a comparable benchmark for the problems we wished to consider. We also note that the RAD paper itself does not consider problems in higher than 2 dimensions."}, "signatures": ["ICLR.cc/2020/Conference/Paper2348/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2348/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rcornish@robots.ox.ac.uk", "anthony.caterini@stats.ox.ac.uk", "deligian@stats.ox.ac.uk", "doucet@stats.ox.ac.uk"], "title": "Localised Generative Flows", "authors": ["Rob Cornish", "Anthony Caterini", "George Deligiannidis", "Arnaud Doucet"], "pdf": "/pdf/25aae4bf66a9800822f1385cffcd1374214482c4.pdf", "TL;DR": "We use a deep continuous mixture of bijections to improve normalising flows for density estimation.", "abstract": "We argue that flow-based density models based on continuous bijections are limited in their ability to learn target distributions with complicated topologies, and propose localised generative flows (LGFs) to address this problem. LGFs are composed of stacked continuous mixtures of bijections, which enables each bijection to learn a local region of the target rather than its entirety. Our method is a generalisation of existing flow-based methods, which can be used without modification as the basis for an LGF model. Unlike normalising flows, LGFs do not permit exact computation of log likelihoods, but we propose a simple variational scheme that performs well in practice. We show empirically that LGFs yield improved performance across a variety of common density estimation tasks.", "code": "https://github.com/anonsubmission974/lgf", "keywords": ["Deep generative models", "normalizing flows", "variational inference"], "paperhash": "cornish|localised_generative_flows", "original_pdf": "/attachment/23609fcb86259d5db679c20403c0fd6fe2a4e38f.pdf", "_bibtex": "@misc{\ncornish2020localised,\ntitle={Localised Generative Flows},\nauthor={Rob Cornish and Anthony Caterini and George Deligiannidis and Arnaud Doucet},\nyear={2020},\nurl={https://openreview.net/forum?id=SyegvgHtwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyegvgHtwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2348/Authors", "ICLR.cc/2020/Conference/Paper2348/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2348/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2348/Reviewers", "ICLR.cc/2020/Conference/Paper2348/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2348/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2348/Authors|ICLR.cc/2020/Conference/Paper2348/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142702, "tmdate": 1576860554188, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2348/Authors", "ICLR.cc/2020/Conference/Paper2348/Reviewers", "ICLR.cc/2020/Conference/Paper2348/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2348/-/Official_Comment"}}}, {"id": "B1gQMDEhiB", "original": null, "number": 6, "cdate": 1573828363157, "ddate": null, "tcdate": 1573828363157, "tmdate": 1573828363157, "tddate": null, "forum": "SyegvgHtwr", "replyto": "HJgjqGLwtH", "invitation": "ICLR.cc/2020/Conference/Paper2348/-/Official_Comment", "content": {"title": "Response to Official Blind Reviewer #3", "comment": "Thank you for your review.\n\nRegarding your points about the theoretical underpinnings and empirical evaluation of our method, please see our general replies in the Common thread above.\n\nThank you very much for your other feedback also. We agree a diagram could help to convey intuition. We have also fixed the typos and clarity issues that you have pointed out in the uploaded version of the paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper2348/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2348/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rcornish@robots.ox.ac.uk", "anthony.caterini@stats.ox.ac.uk", "deligian@stats.ox.ac.uk", "doucet@stats.ox.ac.uk"], "title": "Localised Generative Flows", "authors": ["Rob Cornish", "Anthony Caterini", "George Deligiannidis", "Arnaud Doucet"], "pdf": "/pdf/25aae4bf66a9800822f1385cffcd1374214482c4.pdf", "TL;DR": "We use a deep continuous mixture of bijections to improve normalising flows for density estimation.", "abstract": "We argue that flow-based density models based on continuous bijections are limited in their ability to learn target distributions with complicated topologies, and propose localised generative flows (LGFs) to address this problem. LGFs are composed of stacked continuous mixtures of bijections, which enables each bijection to learn a local region of the target rather than its entirety. Our method is a generalisation of existing flow-based methods, which can be used without modification as the basis for an LGF model. Unlike normalising flows, LGFs do not permit exact computation of log likelihoods, but we propose a simple variational scheme that performs well in practice. We show empirically that LGFs yield improved performance across a variety of common density estimation tasks.", "code": "https://github.com/anonsubmission974/lgf", "keywords": ["Deep generative models", "normalizing flows", "variational inference"], "paperhash": "cornish|localised_generative_flows", "original_pdf": "/attachment/23609fcb86259d5db679c20403c0fd6fe2a4e38f.pdf", "_bibtex": "@misc{\ncornish2020localised,\ntitle={Localised Generative Flows},\nauthor={Rob Cornish and Anthony Caterini and George Deligiannidis and Arnaud Doucet},\nyear={2020},\nurl={https://openreview.net/forum?id=SyegvgHtwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyegvgHtwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2348/Authors", "ICLR.cc/2020/Conference/Paper2348/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2348/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2348/Reviewers", "ICLR.cc/2020/Conference/Paper2348/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2348/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2348/Authors|ICLR.cc/2020/Conference/Paper2348/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142702, "tmdate": 1576860554188, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2348/Authors", "ICLR.cc/2020/Conference/Paper2348/Reviewers", "ICLR.cc/2020/Conference/Paper2348/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2348/-/Official_Comment"}}}, {"id": "HJgjqGLwtH", "original": null, "number": 2, "cdate": 1571410578624, "ddate": null, "tcdate": 1571410578624, "tmdate": 1572972350050, "tddate": null, "forum": "SyegvgHtwr", "replyto": "SyegvgHtwr", "invitation": "ICLR.cc/2020/Conference/Paper2348/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose to extend flow-based density models by replacing a single bijection with a hierarchical mixture of bijections. Each component in the mixture is then required to only push the prior onto a local region; this helps improve the coverage of $\\mathcal{X}$. This is motivated by the conjecture that in many cases the topology of $\\mathcal{X}$ might be overly complicated to be effectively captured by a single bijection. Formally, this is achieved by introducing a conditional random variable $U|Z$. In doing however, the log-likelihood is rendered intractable and a variational approximation must instead be resorted to. A recursive formula for computing the ELBO is introduced in this vain. \n\nOverall, I think this is a generally interesting contribution to the normalizing-flow literature that I expect to spark further research. However, there are some rough edges to this paper. The initial motivation is well-presented and relatively easy to follow, though a diagram would serve to cement the intuition regarding the support mismatch. The issue mentioned in footnote 2 deserves further discussion. At the same time, while well-reasoned, their justifications are nonetheless largely conjectural and further theoretical or empirical evidence would be welcome, both for characterising the pathologies they aim to redress and their proposed solution.\n\nFor the experiment section, I would have liked to have seen comparisons not only to the simplest baseline but also to some of the other methods mentioned in related works. In general, the experiment section is quite short and I didn't get a very good sense of how well this method performs.\n\nThe following should be addressed:\n\n- provide more evidence for the conjectures surrounding the motivation and derivation\n- supply more varied baselines (e.g. RAD model)\n\nMinor comments:\n\n- at the top of page 4, you refer to $p_{U|X}$ several times, but I think you mean $p_{U|Z}$\n- in the paragraph after equation (2), the $\\theta$ superscript seems to be missing from the $p_X^\\theta$\n- in the first sentence of section 3.1, you refer to \"the single $g$ used in equation 2\", but equation 2 mentions no $g$\n- in the first line on page 3, you talk about some region of supp $p_X^\\theta$ being pushed out of supp $p_X^*$, shouldn't this be the other way round since the KL is infinite only if the the support of $p_x^*$ is not contained within the support of $p_X^\\theta$?"}, "signatures": ["ICLR.cc/2020/Conference/Paper2348/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2348/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rcornish@robots.ox.ac.uk", "anthony.caterini@stats.ox.ac.uk", "deligian@stats.ox.ac.uk", "doucet@stats.ox.ac.uk"], "title": "Localised Generative Flows", "authors": ["Rob Cornish", "Anthony Caterini", "George Deligiannidis", "Arnaud Doucet"], "pdf": "/pdf/25aae4bf66a9800822f1385cffcd1374214482c4.pdf", "TL;DR": "We use a deep continuous mixture of bijections to improve normalising flows for density estimation.", "abstract": "We argue that flow-based density models based on continuous bijections are limited in their ability to learn target distributions with complicated topologies, and propose localised generative flows (LGFs) to address this problem. LGFs are composed of stacked continuous mixtures of bijections, which enables each bijection to learn a local region of the target rather than its entirety. Our method is a generalisation of existing flow-based methods, which can be used without modification as the basis for an LGF model. Unlike normalising flows, LGFs do not permit exact computation of log likelihoods, but we propose a simple variational scheme that performs well in practice. We show empirically that LGFs yield improved performance across a variety of common density estimation tasks.", "code": "https://github.com/anonsubmission974/lgf", "keywords": ["Deep generative models", "normalizing flows", "variational inference"], "paperhash": "cornish|localised_generative_flows", "original_pdf": "/attachment/23609fcb86259d5db679c20403c0fd6fe2a4e38f.pdf", "_bibtex": "@misc{\ncornish2020localised,\ntitle={Localised Generative Flows},\nauthor={Rob Cornish and Anthony Caterini and George Deligiannidis and Arnaud Doucet},\nyear={2020},\nurl={https://openreview.net/forum?id=SyegvgHtwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SyegvgHtwr", "replyto": "SyegvgHtwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2348/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2348/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575829041581, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2348/Reviewers"], "noninvitees": [], "tcdate": 1570237724117, "tmdate": 1575829041593, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2348/-/Official_Review"}}}, {"id": "BJlmkymy5S", "original": null, "number": 3, "cdate": 1571921627106, "ddate": null, "tcdate": 1571921627106, "tmdate": 1572972350000, "tddate": null, "forum": "SyegvgHtwr", "replyto": "SyegvgHtwr", "invitation": "ICLR.cc/2020/Conference/Paper2348/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nThis paper conjectures that normalizing flows are fundamentally limited due to the architecture assumption that the generative function g is continuous in x. It is argued that this constraint makes maximum likelihood estimation difficult in general. Localised generative flows are proposed as a solution and consist in modeling the generative model as a continuous mixture of bijections. Experiments suggest an improvement over MAF.\n\nDecision:\nThe observation that continuity imposes a hard constraint on the network is sound, and the proposed solution appears to show some improvement. However, in its current state, this work appears to be quite fragile both from a theoretical and experimental point of view. First, it is only conjectured that this constraint poses actual problems. Second, the experimental evaluation is weak and insufficient. It omits comparisons with more recent generative flows that have shown to be able to model discontinuous densities. For this reason, I do not recommend the paper for acceptance.\n\nFurther arguments:\n- The whole paper rests on intuition without strong theoretical backup. \n- The experiments are quite poor and results frankly oversold. It is said the method \"improves performance across a variety of common density benchmarks\". While we see improvements in Table 1 over MAF, the comparison omits all recent architectures based on Normalizing Flows, such as TAN (Olivia et al, 2018), NAF (Huang et al, 2018), B-NAF (De Cao et al, 2019) or SOS (Jaini et al, 2019). All of those methods have reported better results than those provided in Table 1. They have also been shown empirically to work for discontinuous densities. While I understand that LGF can be combined with any flow architecture, the question remains whether using a continuous mixture translates into significant improvements for those baselines as well. The experimental benchmarks also omit datasets such as BSDS300, for which the higher dimensionality is usually challenging. The same goes for Table 2 which omits recent and better results, such as Glow or FFJORD.\n- Closer to LGF, a proper experimental comparison to RAD (Dinh et al, 2019) would be appreciated.\n- The proposed architecture supposedly enables better generative models. However, this comes at the price that the density can no longer be evaluated exactly and analytically. Since normalizing flows are also typically slow for sampling, this makes the benefits of the proposed architecture quite limited. In particular, it is not clear why generative models that are good at sampling only (e.g., GANs) should not then be preferred?\n- As a result of the point above, the experimental results are reported only in terms of approximated negative log-likelihood. I do not think this is fair, since models like MAF do provide exact values. It also makes the comparison with previous methods more difficult.\n\nFurther feedback:\n- As per ICLR policy, higher standards should be applied to papers with 9 or more pages. I am confident the paper could be written within 8 pages only.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2348/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2348/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rcornish@robots.ox.ac.uk", "anthony.caterini@stats.ox.ac.uk", "deligian@stats.ox.ac.uk", "doucet@stats.ox.ac.uk"], "title": "Localised Generative Flows", "authors": ["Rob Cornish", "Anthony Caterini", "George Deligiannidis", "Arnaud Doucet"], "pdf": "/pdf/25aae4bf66a9800822f1385cffcd1374214482c4.pdf", "TL;DR": "We use a deep continuous mixture of bijections to improve normalising flows for density estimation.", "abstract": "We argue that flow-based density models based on continuous bijections are limited in their ability to learn target distributions with complicated topologies, and propose localised generative flows (LGFs) to address this problem. LGFs are composed of stacked continuous mixtures of bijections, which enables each bijection to learn a local region of the target rather than its entirety. Our method is a generalisation of existing flow-based methods, which can be used without modification as the basis for an LGF model. Unlike normalising flows, LGFs do not permit exact computation of log likelihoods, but we propose a simple variational scheme that performs well in practice. We show empirically that LGFs yield improved performance across a variety of common density estimation tasks.", "code": "https://github.com/anonsubmission974/lgf", "keywords": ["Deep generative models", "normalizing flows", "variational inference"], "paperhash": "cornish|localised_generative_flows", "original_pdf": "/attachment/23609fcb86259d5db679c20403c0fd6fe2a4e38f.pdf", "_bibtex": "@misc{\ncornish2020localised,\ntitle={Localised Generative Flows},\nauthor={Rob Cornish and Anthony Caterini and George Deligiannidis and Arnaud Doucet},\nyear={2020},\nurl={https://openreview.net/forum?id=SyegvgHtwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SyegvgHtwr", "replyto": "SyegvgHtwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2348/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2348/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575829041581, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2348/Reviewers"], "noninvitees": [], "tcdate": 1570237724117, "tmdate": 1575829041593, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2348/-/Official_Review"}}}, {"id": "S1eB7k4Zdr", "original": null, "number": 1, "cdate": 1569959708734, "ddate": null, "tcdate": 1569959708734, "tmdate": 1569959708734, "tddate": null, "forum": "SyegvgHtwr", "replyto": "r1x0meQ2vH", "invitation": "ICLR.cc/2020/Conference/Paper2348/-/Official_Comment", "content": {"comment": "Hi Kevin,\n\nWe are working hard to release our code as quickly as possible, and at this stage plan to do so by October 4th, when reviewers will be assigned to papers.\n\nWe respectfully disagree with your assessment that this practice is unfair, since the facility to upload code past the submission deadline is available to all authors, either via the method we have chosen, or by posting an updated link as a comment to their work.\n\nWe also note that there is nothing in the ICLR call for papers that forbids or even discourages this approach.\n\nWe will be fully transparent about the exact times that we upload our work, which will be timestamped in our git commits. We leave any decisions about how to make use of this information to the discretion of the reviewers.", "title": "Response"}, "signatures": ["ICLR.cc/2020/Conference/Paper2348/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2348/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rcornish@robots.ox.ac.uk", "anthony.caterini@stats.ox.ac.uk", "deligian@stats.ox.ac.uk", "doucet@stats.ox.ac.uk"], "title": "Localised Generative Flows", "authors": ["Rob Cornish", "Anthony Caterini", "George Deligiannidis", "Arnaud Doucet"], "pdf": "/pdf/25aae4bf66a9800822f1385cffcd1374214482c4.pdf", "TL;DR": "We use a deep continuous mixture of bijections to improve normalising flows for density estimation.", "abstract": "We argue that flow-based density models based on continuous bijections are limited in their ability to learn target distributions with complicated topologies, and propose localised generative flows (LGFs) to address this problem. LGFs are composed of stacked continuous mixtures of bijections, which enables each bijection to learn a local region of the target rather than its entirety. Our method is a generalisation of existing flow-based methods, which can be used without modification as the basis for an LGF model. Unlike normalising flows, LGFs do not permit exact computation of log likelihoods, but we propose a simple variational scheme that performs well in practice. We show empirically that LGFs yield improved performance across a variety of common density estimation tasks.", "code": "https://github.com/anonsubmission974/lgf", "keywords": ["Deep generative models", "normalizing flows", "variational inference"], "paperhash": "cornish|localised_generative_flows", "original_pdf": "/attachment/23609fcb86259d5db679c20403c0fd6fe2a4e38f.pdf", "_bibtex": "@misc{\ncornish2020localised,\ntitle={Localised Generative Flows},\nauthor={Rob Cornish and Anthony Caterini and George Deligiannidis and Arnaud Doucet},\nyear={2020},\nurl={https://openreview.net/forum?id=SyegvgHtwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyegvgHtwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2348/Authors", "ICLR.cc/2020/Conference/Paper2348/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2348/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2348/Reviewers", "ICLR.cc/2020/Conference/Paper2348/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2348/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2348/Authors|ICLR.cc/2020/Conference/Paper2348/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142702, "tmdate": 1576860554188, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2348/Authors", "ICLR.cc/2020/Conference/Paper2348/Reviewers", "ICLR.cc/2020/Conference/Paper2348/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2348/-/Official_Comment"}}}, {"id": "r1x0meQ2vH", "original": null, "number": 1, "cdate": 1569628197555, "ddate": null, "tcdate": 1569628197555, "tmdate": 1569628208173, "tddate": null, "forum": "SyegvgHtwr", "replyto": "SyegvgHtwr", "invitation": "ICLR.cc/2020/Conference/Paper2348/-/Public_Comment", "content": {"comment": "Hi,\nAs of close to 56 hours after submission deadline , no code is present in the provided github link. It is not fair to provide a placeholder link for code submissions (which impact the review process) and submit code taking considerable buffer time after submission deadline.", "title": "No code in provided github link even after 56 hours of submission deadline"}, "signatures": ["~Kevin_Zhang2"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Kevin_Zhang2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rcornish@robots.ox.ac.uk", "anthony.caterini@stats.ox.ac.uk", "deligian@stats.ox.ac.uk", "doucet@stats.ox.ac.uk"], "title": "Localised Generative Flows", "authors": ["Rob Cornish", "Anthony Caterini", "George Deligiannidis", "Arnaud Doucet"], "pdf": "/pdf/25aae4bf66a9800822f1385cffcd1374214482c4.pdf", "TL;DR": "We use a deep continuous mixture of bijections to improve normalising flows for density estimation.", "abstract": "We argue that flow-based density models based on continuous bijections are limited in their ability to learn target distributions with complicated topologies, and propose localised generative flows (LGFs) to address this problem. LGFs are composed of stacked continuous mixtures of bijections, which enables each bijection to learn a local region of the target rather than its entirety. Our method is a generalisation of existing flow-based methods, which can be used without modification as the basis for an LGF model. Unlike normalising flows, LGFs do not permit exact computation of log likelihoods, but we propose a simple variational scheme that performs well in practice. We show empirically that LGFs yield improved performance across a variety of common density estimation tasks.", "code": "https://github.com/anonsubmission974/lgf", "keywords": ["Deep generative models", "normalizing flows", "variational inference"], "paperhash": "cornish|localised_generative_flows", "original_pdf": "/attachment/23609fcb86259d5db679c20403c0fd6fe2a4e38f.pdf", "_bibtex": "@misc{\ncornish2020localised,\ntitle={Localised Generative Flows},\nauthor={Rob Cornish and Anthony Caterini and George Deligiannidis and Arnaud Doucet},\nyear={2020},\nurl={https://openreview.net/forum?id=SyegvgHtwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyegvgHtwr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504181585, "tmdate": 1576860587355, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2348/Authors", "ICLR.cc/2020/Conference/Paper2348/Reviewers", "ICLR.cc/2020/Conference/Paper2348/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2348/-/Public_Comment"}}}], "count": 13}