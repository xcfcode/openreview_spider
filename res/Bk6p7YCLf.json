{"notes": [{"tddate": null, "ddate": null, "original": null, "tmdate": 1521582849586, "tcdate": 1520579274595, "number": 1, "cdate": 1520579274595, "id": "rJQAM2JYM", "invitation": "ICLR.cc/2018/Workshop/-/Paper80/Official_Review", "forum": "Bk6p7YCLf", "replyto": "Bk6p7YCLf", "signatures": ["ICLR.cc/2018/Workshop/Paper80/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper80/AnonReviewer3"], "content": {"title": "Interesting look at handling domain shift via meta-learning and exploiting temporal information", "rating": "7: Good paper, accept", "review": "The submission is looking at the problem of one-shot learning under domain shift, e.g. transferring from human to robot actions. The main novelties are the domain-adaptive extension of the MAML algorithm, and the introduction of a temporal adaptation objective.\nFor the inner adaptation objective, actions are not known, so the objective operates only on the policy activations. Therefore, meta-learning is required to provide a strong policy prior, such that the \"fine-tuning\" at inference time will output the right actions.\n\nWhile the submission makes the above incremental contributions and regards a constrained set-up, experimental results are convincing, both numerically and in the provided video, and including ablation studies. In particular, using the proposed temporal loss makes a substantial difference under the presence of domain shift. The proposed approach is still fundamentally imitation learning of similar actions; I suspect that further approach adaptation are required to generalize further.\nThe 4-page workshop submission is unfortunately not as easy to read as the full paper, due to the necessity to condense.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "One-Shot Imitation from Observing Humans via Domain-Adaptive Meta-Learning", "abstract": "Humans and animals are capable of learning a new behavior by observing others perform the skill just once. We consider the problem of allowing a robot to do the same -- learning from a raw video pixels of a human, even when there is substantial domain shift in the perspective, environment, and embodiment between the robot and the observed human. Prior approaches to this problem have hand-specified how human and robot actions correspond and often relied on explicit human pose detection systems. In this work, we present an approach for one-shot learning from a video of a human by using human and robot demonstration data from a variety of previous tasks to build up prior knowledge through meta-learning. Then, combining this prior knowledge and only a single video demonstration from a human, the robot can perform the task that the human demonstrated. We show experiments on a PR2 arm, demonstrating that after meta-learning, the robot can learn to place, push, and pick-and-place new objects using just one video of a human performing the manipulation.", "pdf": "/pdf/728e081d6eedd6ba30e4341acb30a996356143ed.pdf", "TL;DR": "Robot can learn a new behavior from a raw video pixels of a human, even when there is substantial domain shift between the robot and the observed human.", "paperhash": "yu|oneshot_imitation_from_observing_humans_via_domainadaptive_metalearning", "keywords": ["robot learning", "imitation learning", "few-shot learning", "meta-learning", "robot vision"], "authors": ["Tianhe Yu*", "Chelsea Finn*", "Annie Xie", "Sudeep Dasari", "Tianhao Zhang", "Pieter Abbeel", "Sergey Levine"], "authorids": ["tianhe.yu@berkeley.edu", "cbfinn@berkeley.edu", "anniexie@berkeley.edu", "sdasari@berkeley.edu", "tianhao.z@berkeley.edu", "pabbeel@cs.berkeley.edu", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582849359, "id": "ICLR.cc/2018/Workshop/-/Paper80/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper80/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper80/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper80/AnonReviewer1"], "reply": {"forum": "Bk6p7YCLf", "replyto": "Bk6p7YCLf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper80/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper80/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582849359}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582600573, "tcdate": 1520970031775, "number": 2, "cdate": 1520970031775, "id": "BJ_4KjHtG", "invitation": "ICLR.cc/2018/Workshop/-/Paper80/Official_Review", "forum": "Bk6p7YCLf", "replyto": "Bk6p7YCLf", "signatures": ["ICLR.cc/2018/Workshop/Paper80/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper80/AnonReviewer1"], "content": {"title": "Great idea for learning how to imitate", "rating": "10: Top 5% of accepted papers, seminal paper", "review": "The paper presents an algorithm for performing one-shot learning: visually imitate a human performing a task after having observed some pairs of demonstrations and imitations, potentially with different backgrounds, imitators or viewpoints.\n\nPositive points:\nThe problem the paper is attacking is extremely challenging: how can we teach a robot to imitate in new conditions? The authors describe their approach in a clear manner with a good level of detail given the maximum length of the paper, and provide further details in the full version.\nThe authors offer some important insights, like defining the loss in terms of the policy activations instead of the actions (not available for the human demonstration). Or like using a an adaptation objective that exploits multiple frames from the demonstration.\nThe proposed approach is evaluated against two baselines, and ablated in terms of the contribution of the temporal loss. Moreover, to justify that there's indeed a non-trivial domain shift between train and test, the authors show that the performance doesn't degrade too much when observing imitations with a different viewpoint, background and demonstrator.\n\nNegative points:\nThere are many things that could be added to the paper to make it more complete, like discussing more about how big the domain shift can be or how to attack the imitation of more complex actions. However, I think the available space for this workshop submission is well used and those details belong to a full submission of this work ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "One-Shot Imitation from Observing Humans via Domain-Adaptive Meta-Learning", "abstract": "Humans and animals are capable of learning a new behavior by observing others perform the skill just once. We consider the problem of allowing a robot to do the same -- learning from a raw video pixels of a human, even when there is substantial domain shift in the perspective, environment, and embodiment between the robot and the observed human. Prior approaches to this problem have hand-specified how human and robot actions correspond and often relied on explicit human pose detection systems. In this work, we present an approach for one-shot learning from a video of a human by using human and robot demonstration data from a variety of previous tasks to build up prior knowledge through meta-learning. Then, combining this prior knowledge and only a single video demonstration from a human, the robot can perform the task that the human demonstrated. We show experiments on a PR2 arm, demonstrating that after meta-learning, the robot can learn to place, push, and pick-and-place new objects using just one video of a human performing the manipulation.", "pdf": "/pdf/728e081d6eedd6ba30e4341acb30a996356143ed.pdf", "TL;DR": "Robot can learn a new behavior from a raw video pixels of a human, even when there is substantial domain shift between the robot and the observed human.", "paperhash": "yu|oneshot_imitation_from_observing_humans_via_domainadaptive_metalearning", "keywords": ["robot learning", "imitation learning", "few-shot learning", "meta-learning", "robot vision"], "authors": ["Tianhe Yu*", "Chelsea Finn*", "Annie Xie", "Sudeep Dasari", "Tianhao Zhang", "Pieter Abbeel", "Sergey Levine"], "authorids": ["tianhe.yu@berkeley.edu", "cbfinn@berkeley.edu", "anniexie@berkeley.edu", "sdasari@berkeley.edu", "tianhao.z@berkeley.edu", "pabbeel@cs.berkeley.edu", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582849359, "id": "ICLR.cc/2018/Workshop/-/Paper80/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper80/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper80/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper80/AnonReviewer1"], "reply": {"forum": "Bk6p7YCLf", "replyto": "Bk6p7YCLf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper80/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper80/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582849359}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573546956, "tcdate": 1521573546956, "number": 17, "cdate": 1521573546607, "id": "H1X3RCCYf", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "Bk6p7YCLf", "replyto": "Bk6p7YCLf", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "One-Shot Imitation from Observing Humans via Domain-Adaptive Meta-Learning", "abstract": "Humans and animals are capable of learning a new behavior by observing others perform the skill just once. We consider the problem of allowing a robot to do the same -- learning from a raw video pixels of a human, even when there is substantial domain shift in the perspective, environment, and embodiment between the robot and the observed human. Prior approaches to this problem have hand-specified how human and robot actions correspond and often relied on explicit human pose detection systems. In this work, we present an approach for one-shot learning from a video of a human by using human and robot demonstration data from a variety of previous tasks to build up prior knowledge through meta-learning. Then, combining this prior knowledge and only a single video demonstration from a human, the robot can perform the task that the human demonstrated. We show experiments on a PR2 arm, demonstrating that after meta-learning, the robot can learn to place, push, and pick-and-place new objects using just one video of a human performing the manipulation.", "pdf": "/pdf/728e081d6eedd6ba30e4341acb30a996356143ed.pdf", "TL;DR": "Robot can learn a new behavior from a raw video pixels of a human, even when there is substantial domain shift between the robot and the observed human.", "paperhash": "yu|oneshot_imitation_from_observing_humans_via_domainadaptive_metalearning", "keywords": ["robot learning", "imitation learning", "few-shot learning", "meta-learning", "robot vision"], "authors": ["Tianhe Yu*", "Chelsea Finn*", "Annie Xie", "Sudeep Dasari", "Tianhao Zhang", "Pieter Abbeel", "Sergey Levine"], "authorids": ["tianhe.yu@berkeley.edu", "cbfinn@berkeley.edu", "anniexie@berkeley.edu", "sdasari@berkeley.edu", "tianhao.z@berkeley.edu", "pabbeel@cs.berkeley.edu", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1518404848413, "tcdate": 1518404549445, "number": 80, "cdate": 1518404549445, "id": "Bk6p7YCLf", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "Bk6p7YCLf", "signatures": ["~Tianhe_Yu1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "One-Shot Imitation from Observing Humans via Domain-Adaptive Meta-Learning", "abstract": "Humans and animals are capable of learning a new behavior by observing others perform the skill just once. We consider the problem of allowing a robot to do the same -- learning from a raw video pixels of a human, even when there is substantial domain shift in the perspective, environment, and embodiment between the robot and the observed human. Prior approaches to this problem have hand-specified how human and robot actions correspond and often relied on explicit human pose detection systems. In this work, we present an approach for one-shot learning from a video of a human by using human and robot demonstration data from a variety of previous tasks to build up prior knowledge through meta-learning. Then, combining this prior knowledge and only a single video demonstration from a human, the robot can perform the task that the human demonstrated. We show experiments on a PR2 arm, demonstrating that after meta-learning, the robot can learn to place, push, and pick-and-place new objects using just one video of a human performing the manipulation.", "pdf": "/pdf/728e081d6eedd6ba30e4341acb30a996356143ed.pdf", "TL;DR": "Robot can learn a new behavior from a raw video pixels of a human, even when there is substantial domain shift between the robot and the observed human.", "paperhash": "yu|oneshot_imitation_from_observing_humans_via_domainadaptive_metalearning", "keywords": ["robot learning", "imitation learning", "few-shot learning", "meta-learning", "robot vision"], "authors": ["Tianhe Yu*", "Chelsea Finn*", "Annie Xie", "Sudeep Dasari", "Tianhao Zhang", "Pieter Abbeel", "Sergey Levine"], "authorids": ["tianhe.yu@berkeley.edu", "cbfinn@berkeley.edu", "anniexie@berkeley.edu", "sdasari@berkeley.edu", "tianhao.z@berkeley.edu", "pabbeel@cs.berkeley.edu", "svlevine@eecs.berkeley.edu"]}, "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}], "count": 4}