{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1392682260000, "tcdate": 1392682260000, "number": 1, "id": "1uSbuOvDbOKVF", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "kkgljR8O6hjHA", "replyto": "78tW7PNx028b_", "signatures": ["Lorenzo Torresani"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "We thank the reviewer for highlighting the two main contributions of our approach over prior work: computational efficiency and manual annotation parsimony."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "EXMOVES: Classifier-based Features for Scalable Action Recognition", "decision": "submitted, no decision", "abstract": "This paper introduces EXMOVES, learned exemplar-based features for efficient recognition of actions in videos. The entries in our descriptor are produced by evaluating a set of movement classifiers over spatial-temporal volumes of the input sequence. Each movement classifier is a simple exemplar-SVM trained on low-level features, i.e., an SVM learned using a single annotated positive space-time volume and a large number of unannotated videos. Our representation offers two main advantages. First, since our mid-level features are learned from individual video exemplars, they require minimal amount of supervision. Second, we show that simple linear classification models trained on our global video descriptor yield action recognition accuracy approaching the state-of-the-art but at orders of magnitude lower cost, since at test-time no sliding window is necessary and linear models are efficient to train and test. This enables scalable action recognition, i.e., efficient classification of a large number of different actions even in large video databases. We show the generality of our approach by building our mid-level descriptors from two different low-level feature representations. The accuracy and efficiency of the approach are demonstrated on several large-scale action recognition benchmarks.", "pdf": "https://arxiv.org/abs/1312.5785", "paperhash": "tran|exmoves_classifierbased_features_for_scalable_action_recognition", "keywords": [], "conflicts": [], "authors": ["Du Tran", "Lorenzo Torresani"], "authorids": ["trandu@gmail.com", "lorenzo@cs.dartmouth.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392682140000, "tcdate": 1392682140000, "number": 4, "id": "SSBu-4TLuQxWJ", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "kkgljR8O6hjHA", "replyto": "kkgljR8O6hjHA", "signatures": ["Lorenzo Torresani"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We thank the reviewer for the comments and for the useful suggestions to improve the paper. In the final version of our article we will address these issues as follows:\r\n- We will clarify further that having more than one example per basis-activity yields improved accuracy, as shown in the experiments.\r\n- We will stress that tags and similar metadata information can be exploited to select the negative videos. We already hint to it in the first paragraph of section 3.2.\r\n- We will specify the dimensionality of all descriptors in our experiments. In any case these are indeed comparable to the dimensionality of our feature vector: EXMOVES contain 41,172 features, Action Bank has 44,895 entries, while BOWs based on Dense Trajectories have dimensionality equal to 25,000.\r\n\r\nAs for the large-scale experiments involving Action Bank, for UCF50 we used the accuracy number reported by the authors in their paper. We have estimated that extracting Action Bank features for UCF101 (part 2) would take 132 days by using 10 nodes of our cluster exclusively for this computation. For this reason we are unable to carry out such evaluation. Extraction of Action Bank features from Hollywood-2 would take even longer.\r\n\r\nAs mentioned in the conclusions, we will release the software implementing our features and all the data used in our experiments."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "EXMOVES: Classifier-based Features for Scalable Action Recognition", "decision": "submitted, no decision", "abstract": "This paper introduces EXMOVES, learned exemplar-based features for efficient recognition of actions in videos. The entries in our descriptor are produced by evaluating a set of movement classifiers over spatial-temporal volumes of the input sequence. Each movement classifier is a simple exemplar-SVM trained on low-level features, i.e., an SVM learned using a single annotated positive space-time volume and a large number of unannotated videos. Our representation offers two main advantages. First, since our mid-level features are learned from individual video exemplars, they require minimal amount of supervision. Second, we show that simple linear classification models trained on our global video descriptor yield action recognition accuracy approaching the state-of-the-art but at orders of magnitude lower cost, since at test-time no sliding window is necessary and linear models are efficient to train and test. This enables scalable action recognition, i.e., efficient classification of a large number of different actions even in large video databases. We show the generality of our approach by building our mid-level descriptors from two different low-level feature representations. The accuracy and efficiency of the approach are demonstrated on several large-scale action recognition benchmarks.", "pdf": "https://arxiv.org/abs/1312.5785", "paperhash": "tran|exmoves_classifierbased_features_for_scalable_action_recognition", "keywords": [], "conflicts": [], "authors": ["Du Tran", "Lorenzo Torresani"], "authorids": ["trandu@gmail.com", "lorenzo@cs.dartmouth.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392680940000, "tcdate": 1392680940000, "number": 1, "id": "BsgHBqmdHqB4I", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "kkgljR8O6hjHA", "replyto": "7ZJu769dwD72O", "signatures": ["Lorenzo Torresani"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "We thank the reviewer for the thoughtful suggestions. We would like to comments on a few points of the review.\r\n\r\n- We believe that it is quite apparent that our approach is not a straightforward extension of [Malisiewicz et al.,2011]. This prior work has not been applied to videos, just to object detection in still images. A naive application of [Malisiewicz et al.,2011] to videos is simply not feasible because of the prohibitive cost. In our paper we describe how to adapt it to work efficiently on videos so that it can scale to large datasets. This is not a trivial contribution, as partly acknowledged by the reviewer.\r\n\r\n- The mining of hard negatives is a standard strategy in the learning of exemplar SVMs. Having said this, the reviewer makes an excellent suggestion in proposing the use of stochastic gradient descent on the entire negative set. This is definitely an interesting experiment for future work. However, our expectation is that results would be quite similar as those obtained with iterative hard negative mining since only examples violating the margin (i.e., the hard negatives) would contribute to refining the parameters in stochastic gradient descent.\r\n\r\n- We agree with the reviewer that regularized logistic regression may be a sensible alternative to the two-step learning of the SVMs and the sigmoids. Again, we opted for the simple two-step solution as it has been proven to work effectively in several prior systems (e.g., [Malisiewicz et al., 2011; Deng et al., CVPR 2011, Bergamo and Torresani, CVPR 2012]). \r\n\r\n- As suggested by the reviewer, we will make sure to report the feature extraction time also in frames per second in order to make this number more easily interpretable. We recognize that, despite the significant speedup enabled by our approach, feature extraction remains more costly than recognition. However, we note that there are many practical scenarios where a feature extraction time of 5 frames per second (as opposed to the 4 frames per *minute* of Action Bank) would enable application of action recognition in large-scale datasets. For example, consider the motivating application of interactive content-based video search where the user may query a system by providing an example sequence in order to find videos containing the same action in the database. In such scenario the search index (containing the features) can be built offline while the training of the action classifier and the recognition itself must be done at query time. Our system can be directly used in such scenarios, in principle even for YouTube-size datasets, while prior mid-level descriptors are simply too costly to be computed for large databases.\r\n\r\n- We disagree with the final conclusion of the reviewer that this paper 'describes a vision system rather than investigating the learning of representations.' Our entire work centers around the learning of a novel intermediate representation for action recognition. While it is true that it shares similarities with prior high-level descriptors (which we discuss in the paper), it should also be acknowledged that our new representation model introduces significant advantages in terms of computational cost and recognition accuracy over the most closely related prior system."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "EXMOVES: Classifier-based Features for Scalable Action Recognition", "decision": "submitted, no decision", "abstract": "This paper introduces EXMOVES, learned exemplar-based features for efficient recognition of actions in videos. The entries in our descriptor are produced by evaluating a set of movement classifiers over spatial-temporal volumes of the input sequence. Each movement classifier is a simple exemplar-SVM trained on low-level features, i.e., an SVM learned using a single annotated positive space-time volume and a large number of unannotated videos. Our representation offers two main advantages. First, since our mid-level features are learned from individual video exemplars, they require minimal amount of supervision. Second, we show that simple linear classification models trained on our global video descriptor yield action recognition accuracy approaching the state-of-the-art but at orders of magnitude lower cost, since at test-time no sliding window is necessary and linear models are efficient to train and test. This enables scalable action recognition, i.e., efficient classification of a large number of different actions even in large video databases. We show the generality of our approach by building our mid-level descriptors from two different low-level feature representations. The accuracy and efficiency of the approach are demonstrated on several large-scale action recognition benchmarks.", "pdf": "https://arxiv.org/abs/1312.5785", "paperhash": "tran|exmoves_classifierbased_features_for_scalable_action_recognition", "keywords": [], "conflicts": [], "authors": ["Du Tran", "Lorenzo Torresani"], "authorids": ["trandu@gmail.com", "lorenzo@cs.dartmouth.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391902560000, "tcdate": 1391902560000, "number": 3, "id": "bSWoSyUOoLbMr", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "kkgljR8O6hjHA", "replyto": "kkgljR8O6hjHA", "signatures": ["anonymous reviewer 9716"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of EXMOVES: Classifier-based Features for Scalable Action Recognition", "review": "This paper proposes a novel method for human activity recognition in video. The main properties it seeks are: 1) developing invariance to the various types of intra-class variation; 2) efficiency in training - both in terms of complexity and minimizing human involvement, 3) efficiency at test time (e.g. able to handle YouTube-scale collections). The main idea is to train, from typical low-level bag-of-visual-words features, a series of exemplar-SVMs which learn on a single positive example and many negative examples, and use the collective output of the SVMs at multiple scales and positions as a mid-level descriptor. The final stage of the method is a linear SVM classifier. The method performs comparably, or better than other recent methods on three modern datasets: HMDB51, Hollywood2, UCF50/101, moreover it is up to two orders of magnitude more efficient than the other methods.\r\n\r\nThe main contribution is a simple yet effective method that can perform activity recognition on large video databases. It would be easy to re-implement, though I hope the authors release a baseline implementation. As the idea of developing invariance to intra-class variability is a key consideration in the motivation of the work, I'd like to see more discussion on this. To me, the fact that the SVMs are trained on single exemplars is an interesting feature and certainly important when human labeling is expensive. However, this seems at odds with learning invariance. For example, if we only ever see one example of someone performing a tennis swing, will this not make it much more difficult to recognize the tennis swing under different conditions: clothing, body type, scale, etc.?\r\n\r\nPositives\r\n* Paper is well written and the description of the method is clear. In particular, the presentation of Algorithm 1 and its associated description in the text is good.\r\n* As stated earlier, it's a simple and effective method, and I see others using this as a baseline\r\n* Datasets considered are modern and challenging\r\n* Speedup over other methods is impressive: key to this is the use of the integral video, a nice idea\r\n\r\nNegatives\r\n* Paper is very specific to an application and type of dataset, may not be as of wide of interest as some other papers\r\n* Related to above, the representation extracted likely would not be useful to other AI problems (e.g. those with structured output)\r\n* Method still requires first stage of engineered feature extraction pipeline\r\n\r\nOverall, while it may not have the broad appeal across ICLR, I think this is a good paper which clearly presents an effective method for activity recognition.\r\n\r\nQuestions & Comments\r\n====================\r\nEmphasize at the beginning that 'exemplars' do not necessary have to be 1 per activity type. This was the impression I got at the beginning of the paper, but the description of the experiments (e.g. 188 exemplars and 50 actions with multiple exemplars per action) made this clear. \r\n\r\nAs mentioned, there is still some human effort in finding the 'negative' examples. For example, one needs to watch a video to determine for sure that it does not contain any tennis. Perhaps you could use tags or some other means to propose negative videos that you are very confident don't contain a particular activity. In that case, how sensitive would the exemplar-SVM be to label noise (if the negative videos contained volumes semantically very similar to that of the exemplar)?\r\n\r\nThe final dimensionality of the EXMOVE descriptor is 41,172 which is going to benefit the linear SVM. Is the dimensionality of the other methods comparable? You may want to indicate this somewhere, say in Table 1.\r\n\r\nIt would be nice to see Table 1 contain more low-level feature/mid-level descriptor pairs (e.g. the outer product of low-level features and mid-level descriptors). \r\n\r\nWith respect to the comment 'we were unable to test [Action Bank] on the large-scale datasets of Hollywood-2 and UCF101. For Hollywood2, I can understand that it's the case. But would UCF101 not just be roughly 2x the time to train on UCF50? Can you clarify this? I realize it may have just not been a situation of not enough time before the deadline to run the experiment, so could this be reported in the final version of the paper?"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "EXMOVES: Classifier-based Features for Scalable Action Recognition", "decision": "submitted, no decision", "abstract": "This paper introduces EXMOVES, learned exemplar-based features for efficient recognition of actions in videos. The entries in our descriptor are produced by evaluating a set of movement classifiers over spatial-temporal volumes of the input sequence. Each movement classifier is a simple exemplar-SVM trained on low-level features, i.e., an SVM learned using a single annotated positive space-time volume and a large number of unannotated videos. Our representation offers two main advantages. First, since our mid-level features are learned from individual video exemplars, they require minimal amount of supervision. Second, we show that simple linear classification models trained on our global video descriptor yield action recognition accuracy approaching the state-of-the-art but at orders of magnitude lower cost, since at test-time no sliding window is necessary and linear models are efficient to train and test. This enables scalable action recognition, i.e., efficient classification of a large number of different actions even in large video databases. We show the generality of our approach by building our mid-level descriptors from two different low-level feature representations. The accuracy and efficiency of the approach are demonstrated on several large-scale action recognition benchmarks.", "pdf": "https://arxiv.org/abs/1312.5785", "paperhash": "tran|exmoves_classifierbased_features_for_scalable_action_recognition", "keywords": [], "conflicts": [], "authors": ["Du Tran", "Lorenzo Torresani"], "authorids": ["trandu@gmail.com", "lorenzo@cs.dartmouth.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391728680000, "tcdate": 1391728680000, "number": 2, "id": "7ZJu769dwD72O", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "kkgljR8O6hjHA", "replyto": "kkgljR8O6hjHA", "signatures": ["anonymous reviewer 4c79"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of EXMOVES: Classifier-based Features for Scalable Action Recognition", "review": "The paper describes a mid-level representation for videos that can be faster than existing representations and yields similar performance. The idea is to train many SVMs to detect predefined action instances on sub-blocks from the video, and then to aggregate the SVM responses into a representation for the whole video.\r\n\r\nThis work seems like a fairly straightforward extension of previous similar work that was done on images (Malisiewicz et al.), but there are some technical differences like the use of an integral video trick to compute SVM responses fast, which seems nice.\r\n\r\nI don't really understand the mining of negative examples for training the exemplar SVMs. Why is it not possible to train the SVM, say with stochastic gradient descent, on all or many negative examples?\r\n\r\nThe method relies on extra, intermediate labels for training which are not used by bag-of-words. This makes it hard to judge the performance differences between the two and seems to be unfair towards bag-of-words. \r\n\r\n3.3: Rather than learning to scale svm confidences within a sigmoid, why not train a regularized logistic regression classifier in the first place, instead of the svm?\r\n\r\nThe feature extraction time is reported as the total on the whole UT-I dataset. It would be much better to report it in frames per second to make it comparable with other datasets without having to dig up the description of this dataset. If I am not mistaken it amounts to approximately 5 frames (with more or less standard resolution) per second? If correct, this means that despite the improvement over action bank, the bottleneck is really feature exctraction not classification. So the speed up due to the linear classifier will be swamped by the feature extraction and is not really that relevant, unless I'm missing something. \r\n\r\npro:\r\n- Well-written, uses some nice engineering tricks like the integral video for computing SVM responses.\r\n\r\n\r\nneg:\r\n- The paper seems like a slightly strange fit for this conference as it describes a vision system rather than investigating the learning of representations. That intermediate labels are useful on this data is well-known (and unsurprising). The paper does propose a faster way to use them, which is probably worthwhile."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "EXMOVES: Classifier-based Features for Scalable Action Recognition", "decision": "submitted, no decision", "abstract": "This paper introduces EXMOVES, learned exemplar-based features for efficient recognition of actions in videos. The entries in our descriptor are produced by evaluating a set of movement classifiers over spatial-temporal volumes of the input sequence. Each movement classifier is a simple exemplar-SVM trained on low-level features, i.e., an SVM learned using a single annotated positive space-time volume and a large number of unannotated videos. Our representation offers two main advantages. First, since our mid-level features are learned from individual video exemplars, they require minimal amount of supervision. Second, we show that simple linear classification models trained on our global video descriptor yield action recognition accuracy approaching the state-of-the-art but at orders of magnitude lower cost, since at test-time no sliding window is necessary and linear models are efficient to train and test. This enables scalable action recognition, i.e., efficient classification of a large number of different actions even in large video databases. We show the generality of our approach by building our mid-level descriptors from two different low-level feature representations. The accuracy and efficiency of the approach are demonstrated on several large-scale action recognition benchmarks.", "pdf": "https://arxiv.org/abs/1312.5785", "paperhash": "tran|exmoves_classifierbased_features_for_scalable_action_recognition", "keywords": [], "conflicts": [], "authors": ["Du Tran", "Lorenzo Torresani"], "authorids": ["trandu@gmail.com", "lorenzo@cs.dartmouth.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391481300000, "tcdate": 1391481300000, "number": 1, "id": "78tW7PNx028b_", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "kkgljR8O6hjHA", "replyto": "kkgljR8O6hjHA", "signatures": ["anonymous reviewer c3e9"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of EXMOVES: Classifier-based Features for Scalable Action Recognition", "review": "This paper explores a computationally efficient way to learn an intermediate representation for action recognition. The technique is somewhat inspired by the approach of \u2018action bank\u2019 but focuses on a much more computationally efficient way to achieve a similar effect.  The technique takes advantages of \u2018integral videos\u2019 and has the flavor of a modern day Viola & Jones type of technique for recognizing activities in a way similar to the classic technique for face detection. I really like the fact that the authors have taken the issue of computational complexity seriously here. While some might argue that once the community has found techniques that work very well, one could focus on optimizing them for faster run-time performance. However, some choices imply complexity differences that would be very difficult to address in practical working systems and this paper starts by using a technique as a building block that is pretty close to practical right now.\r\n\r\nThe work here is also capable of learning intermediate representations with a particularly small amount of data, i.e. single exeamples of classes of interest and lots of negative examples, due to the use of Malisiewicz et al,\u2019s exemplar-SVM technique.  This could have advantages in a number of practical situations.\r\n\r\nI think this paper is both fairly well presented and executed in terms of the experiment work.  Importantly, it addresses some key practical issues that deserve more attention. The results are not absolutely at the state of the art, but the value added by this work is quite good, especially for those working in the area of action recognition where data processing considerations are particularly challenging."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "EXMOVES: Classifier-based Features for Scalable Action Recognition", "decision": "submitted, no decision", "abstract": "This paper introduces EXMOVES, learned exemplar-based features for efficient recognition of actions in videos. The entries in our descriptor are produced by evaluating a set of movement classifiers over spatial-temporal volumes of the input sequence. Each movement classifier is a simple exemplar-SVM trained on low-level features, i.e., an SVM learned using a single annotated positive space-time volume and a large number of unannotated videos. Our representation offers two main advantages. First, since our mid-level features are learned from individual video exemplars, they require minimal amount of supervision. Second, we show that simple linear classification models trained on our global video descriptor yield action recognition accuracy approaching the state-of-the-art but at orders of magnitude lower cost, since at test-time no sliding window is necessary and linear models are efficient to train and test. This enables scalable action recognition, i.e., efficient classification of a large number of different actions even in large video databases. We show the generality of our approach by building our mid-level descriptors from two different low-level feature representations. The accuracy and efficiency of the approach are demonstrated on several large-scale action recognition benchmarks.", "pdf": "https://arxiv.org/abs/1312.5785", "paperhash": "tran|exmoves_classifierbased_features_for_scalable_action_recognition", "keywords": [], "conflicts": [], "authors": ["Du Tran", "Lorenzo Torresani"], "authorids": ["trandu@gmail.com", "lorenzo@cs.dartmouth.edu"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387781820000, "tcdate": 1387781820000, "number": 28, "id": "kkgljR8O6hjHA", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "kkgljR8O6hjHA", "signatures": ["trandu@gmail.com"], "readers": ["everyone"], "content": {"title": "EXMOVES: Classifier-based Features for Scalable Action Recognition", "decision": "submitted, no decision", "abstract": "This paper introduces EXMOVES, learned exemplar-based features for efficient recognition of actions in videos. The entries in our descriptor are produced by evaluating a set of movement classifiers over spatial-temporal volumes of the input sequence. Each movement classifier is a simple exemplar-SVM trained on low-level features, i.e., an SVM learned using a single annotated positive space-time volume and a large number of unannotated videos. Our representation offers two main advantages. First, since our mid-level features are learned from individual video exemplars, they require minimal amount of supervision. Second, we show that simple linear classification models trained on our global video descriptor yield action recognition accuracy approaching the state-of-the-art but at orders of magnitude lower cost, since at test-time no sliding window is necessary and linear models are efficient to train and test. This enables scalable action recognition, i.e., efficient classification of a large number of different actions even in large video databases. We show the generality of our approach by building our mid-level descriptors from two different low-level feature representations. The accuracy and efficiency of the approach are demonstrated on several large-scale action recognition benchmarks.", "pdf": "https://arxiv.org/abs/1312.5785", "paperhash": "tran|exmoves_classifierbased_features_for_scalable_action_recognition", "keywords": [], "conflicts": [], "authors": ["Du Tran", "Lorenzo Torresani"], "authorids": ["trandu@gmail.com", "lorenzo@cs.dartmouth.edu"]}, "writers": [], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 7}