{"notes": [{"id": "S1lEX04tPr", "original": "B1gwqOS_vr", "number": 1035, "cdate": 1569439260438, "ddate": null, "tcdate": 1569439260438, "tmdate": 1583912048168, "tddate": null, "forum": "S1lEX04tPr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning", "authors": ["Jiachen Yang", "Alireza Nakhaei", "David Isele", "Kikuo Fujimura", "Hongyuan Zha"], "authorids": ["yjiachen@gmail.com", "anakhaei@honda-ri.com", "disele@honda-ri.com", "kfujimura@honda-ri.com", "zha@cc.gatech.edu"], "keywords": ["multi-agent reinforcement learning"], "TL;DR": "A modular method for fully cooperative multi-goal multi-agent reinforcement learning, based on curriculum learning for efficient exploration and credit assignment for action-goal interactions.", "abstract": "A variety of cooperative multi-agent control problems require agents to achieve individual goals while contributing to collective success. This multi-goal multi-agent setting poses difficulties for recent algorithms, which primarily target settings with a single global reward, due to two new challenges: efficient exploration for learning both individual goal attainment and cooperation for others' success, and credit-assignment for interactions between actions and goals of different agents. To address both challenges, we restructure the problem into a novel two-stage curriculum, in which single-agent goal attainment is learned prior to learning multi-agent cooperation, and we derive a new multi-goal multi-agent policy gradient with a credit function for localized credit assignment. We use a function augmentation scheme to bridge value and policy functions across the curriculum. The complete architecture, called CM3, learns significantly faster than direct adaptations of existing algorithms on three challenging multi-goal multi-agent problems: cooperative navigation in difficult formations, negotiating multi-vehicle lane changes in the SUMO traffic simulator, and strategic cooperation in a Checkers environment.", "pdf": "/pdf/87850f6ffe6f3c3098ce5843d5d5e9199fffa32a.pdf", "paperhash": "yang|cm3_cooperative_multigoal_multistage_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nYang2020CM3:,\ntitle={CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning},\nauthor={Jiachen Yang and Alireza Nakhaei and David Isele and Kikuo Fujimura and Hongyuan Zha},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lEX04tPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a62922022995dadb5791f84e334704eed8022a86.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "D7LHRKEVk3", "original": null, "number": 1, "cdate": 1576798712817, "ddate": null, "tcdate": 1576798712817, "tmdate": 1576800923626, "tddate": null, "forum": "S1lEX04tPr", "replyto": "S1lEX04tPr", "invitation": "ICLR.cc/2020/Conference/Paper1035/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper was generally well received by reviewers and was rated as a weak accept by all.\nThe AC recommends acceptance.\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning", "authors": ["Jiachen Yang", "Alireza Nakhaei", "David Isele", "Kikuo Fujimura", "Hongyuan Zha"], "authorids": ["yjiachen@gmail.com", "anakhaei@honda-ri.com", "disele@honda-ri.com", "kfujimura@honda-ri.com", "zha@cc.gatech.edu"], "keywords": ["multi-agent reinforcement learning"], "TL;DR": "A modular method for fully cooperative multi-goal multi-agent reinforcement learning, based on curriculum learning for efficient exploration and credit assignment for action-goal interactions.", "abstract": "A variety of cooperative multi-agent control problems require agents to achieve individual goals while contributing to collective success. This multi-goal multi-agent setting poses difficulties for recent algorithms, which primarily target settings with a single global reward, due to two new challenges: efficient exploration for learning both individual goal attainment and cooperation for others' success, and credit-assignment for interactions between actions and goals of different agents. To address both challenges, we restructure the problem into a novel two-stage curriculum, in which single-agent goal attainment is learned prior to learning multi-agent cooperation, and we derive a new multi-goal multi-agent policy gradient with a credit function for localized credit assignment. We use a function augmentation scheme to bridge value and policy functions across the curriculum. The complete architecture, called CM3, learns significantly faster than direct adaptations of existing algorithms on three challenging multi-goal multi-agent problems: cooperative navigation in difficult formations, negotiating multi-vehicle lane changes in the SUMO traffic simulator, and strategic cooperation in a Checkers environment.", "pdf": "/pdf/87850f6ffe6f3c3098ce5843d5d5e9199fffa32a.pdf", "paperhash": "yang|cm3_cooperative_multigoal_multistage_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nYang2020CM3:,\ntitle={CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning},\nauthor={Jiachen Yang and Alireza Nakhaei and David Isele and Kikuo Fujimura and Hongyuan Zha},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lEX04tPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a62922022995dadb5791f84e334704eed8022a86.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "S1lEX04tPr", "replyto": "S1lEX04tPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795720571, "tmdate": 1576800271424, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1035/-/Decision"}}}, {"id": "BklZ_qinsB", "original": null, "number": 4, "cdate": 1573857896754, "ddate": null, "tcdate": 1573857896754, "tmdate": 1573857896754, "tddate": null, "forum": "S1lEX04tPr", "replyto": "Byx5AlJdjr", "invitation": "ICLR.cc/2020/Conference/Paper1035/-/Official_Comment", "content": {"title": "Response", "comment": "Thanks for the responses. I've read through the other reviews and comments posted for this paper and still keep my original score of a weak accept."}, "signatures": ["ICLR.cc/2020/Conference/Paper1035/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1035/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning", "authors": ["Jiachen Yang", "Alireza Nakhaei", "David Isele", "Kikuo Fujimura", "Hongyuan Zha"], "authorids": ["yjiachen@gmail.com", "anakhaei@honda-ri.com", "disele@honda-ri.com", "kfujimura@honda-ri.com", "zha@cc.gatech.edu"], "keywords": ["multi-agent reinforcement learning"], "TL;DR": "A modular method for fully cooperative multi-goal multi-agent reinforcement learning, based on curriculum learning for efficient exploration and credit assignment for action-goal interactions.", "abstract": "A variety of cooperative multi-agent control problems require agents to achieve individual goals while contributing to collective success. This multi-goal multi-agent setting poses difficulties for recent algorithms, which primarily target settings with a single global reward, due to two new challenges: efficient exploration for learning both individual goal attainment and cooperation for others' success, and credit-assignment for interactions between actions and goals of different agents. To address both challenges, we restructure the problem into a novel two-stage curriculum, in which single-agent goal attainment is learned prior to learning multi-agent cooperation, and we derive a new multi-goal multi-agent policy gradient with a credit function for localized credit assignment. We use a function augmentation scheme to bridge value and policy functions across the curriculum. The complete architecture, called CM3, learns significantly faster than direct adaptations of existing algorithms on three challenging multi-goal multi-agent problems: cooperative navigation in difficult formations, negotiating multi-vehicle lane changes in the SUMO traffic simulator, and strategic cooperation in a Checkers environment.", "pdf": "/pdf/87850f6ffe6f3c3098ce5843d5d5e9199fffa32a.pdf", "paperhash": "yang|cm3_cooperative_multigoal_multistage_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nYang2020CM3:,\ntitle={CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning},\nauthor={Jiachen Yang and Alireza Nakhaei and David Isele and Kikuo Fujimura and Hongyuan Zha},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lEX04tPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a62922022995dadb5791f84e334704eed8022a86.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1lEX04tPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1035/Authors", "ICLR.cc/2020/Conference/Paper1035/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1035/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1035/Reviewers", "ICLR.cc/2020/Conference/Paper1035/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1035/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1035/Authors|ICLR.cc/2020/Conference/Paper1035/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162268, "tmdate": 1576860549491, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1035/Authors", "ICLR.cc/2020/Conference/Paper1035/Reviewers", "ICLR.cc/2020/Conference/Paper1035/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1035/-/Official_Comment"}}}, {"id": "Bkl7fr1OjB", "original": null, "number": 3, "cdate": 1573545226852, "ddate": null, "tcdate": 1573545226852, "tmdate": 1573545226852, "tddate": null, "forum": "S1lEX04tPr", "replyto": "r1e0fqAaKr", "invitation": "ICLR.cc/2020/Conference/Paper1035/-/Official_Comment", "content": {"title": "We appreciate Reviewer 2's thorough reading, constructive review, and reference to complementary work", "comment": "We thank Reviewer 2 for the positive feedback on the motivation, analysis and experimental results of the paper, and for raising pertinent and constructive questions.\n\nRegarding applicability of multi-goal MARL:\nThere are two cases to consider. 1) The problem cannot be formulated as cooperative multi-goal MARL, because there is a single team goal that cannot be decomposed into individual goals in a meaningful way. A predator-prey game may fall under this case (depending on specifics), if all predators share the same goal of capturing one prey and receive a team reward for doing so. While we acknowledge there are interesting MARL settings outside of multi-goal MARL (e.g., pure communication games, as we mentioned in the text), we also provided many important real-world problems that are naturally modeled as multi-goal MARL, e.g., autonomous driving, traffic light control, warehouse commissioning, and multi-player games among others.\n2) The multi-goal MARL formulation applies, but appears to pose difficulties for CM3\u2019s curriculum approach. In fact, the Checkers game we used for experiments falls under this case: as we described in Section 5, it is impossible for Agent A to get the maximum possible rewards if Agent B is not available to help it clear the squares with penalties for Agent A, and vice versa for Agent B. However, Figure 5(e,j) shows that the CM3 curriculum still helped, because the single agent learned useful behavior to attain non-zero rewards in the induced MDP. This suggests that as long as a single agent can receive some rewards in the induced MDP, CM3\u2019s curriculum can be applied.\nTraining only Stage 2 is equivalent to the ablation that we call 'Direct' in Figure 5. Antipodal, Cross, and SUMO show that Stage 1 is important for CM3, while results on Merge and Checkers show that CM3 can be competitive with baselines in some cases even with Stage 1 removed.\n\nRegarding convergence or reaching all goals in Stage 1:\nReviewer 2 is correct in stating that the actor-critic in Stage 1 is trained on all goals, up to random sampling. When N=1, equation (5) reduces to a single-agent actor critic, but with the key difference that the assigned goal g is randomly sampled from the set G in each episode. As stated in Section 4.5, the sampling of goal(s) is done each episode, for both the single-agent Stage 1 and the multi-agent Stage 2.\n\nRegarding initialization for the Stage 2 networks:\nTo clarify, W^1 refers to all the weights that are trained in Stage 1, so they are carried over to Stage 2 (i.e., restored from a model checkpoint), and they are not re-initialized at the start of Stage 2. We initialized the connection weights W^{1:2} with a truncated Normal(0, 0.01), matching the intuition that they should have small initial impact. Furthermore, the overall network outputs are still taken from the original outputs of \\pi^1, and only a single layer h^1_{i*} is directly affected by W^{1:2}, which mitigates the impact of initialization of Stage 2.\n\nRegarding performance versus baselines:\nThe fact that CM3 and a baseline saturate at the same maximum performance indicates there is room to experiment on more complex tasks. We appreciate that Reviewer 2 noticed the best baseline is different across tasks, while CM3 is robust on all three tasks. This motivates future investigation into whether and why different environments pose qualitatively different challenges for different algorithms. The ablations we conducted on CM3 is a start to understand which techniques are needed in which tasks. On the other hand, training speed is also a key metric in MARL, and here we do see statistically significant speedup of CM3 over all baselines except versus QMIX on SUMO.\n\nWe agree that Carion et al. 2019 is a relevant work that is complementary to ours, as they focus on optimizing high-level agent-task assignment given fixed low-level policies, while we focus on curriculum and credit assignment for low-level policies given that each agent is assigned a goal. We have updated the paper to reflect this.\n\nAgain, we thank Reviewer 2 for constructive feedback."}, "signatures": ["ICLR.cc/2020/Conference/Paper1035/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1035/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning", "authors": ["Jiachen Yang", "Alireza Nakhaei", "David Isele", "Kikuo Fujimura", "Hongyuan Zha"], "authorids": ["yjiachen@gmail.com", "anakhaei@honda-ri.com", "disele@honda-ri.com", "kfujimura@honda-ri.com", "zha@cc.gatech.edu"], "keywords": ["multi-agent reinforcement learning"], "TL;DR": "A modular method for fully cooperative multi-goal multi-agent reinforcement learning, based on curriculum learning for efficient exploration and credit assignment for action-goal interactions.", "abstract": "A variety of cooperative multi-agent control problems require agents to achieve individual goals while contributing to collective success. This multi-goal multi-agent setting poses difficulties for recent algorithms, which primarily target settings with a single global reward, due to two new challenges: efficient exploration for learning both individual goal attainment and cooperation for others' success, and credit-assignment for interactions between actions and goals of different agents. To address both challenges, we restructure the problem into a novel two-stage curriculum, in which single-agent goal attainment is learned prior to learning multi-agent cooperation, and we derive a new multi-goal multi-agent policy gradient with a credit function for localized credit assignment. We use a function augmentation scheme to bridge value and policy functions across the curriculum. The complete architecture, called CM3, learns significantly faster than direct adaptations of existing algorithms on three challenging multi-goal multi-agent problems: cooperative navigation in difficult formations, negotiating multi-vehicle lane changes in the SUMO traffic simulator, and strategic cooperation in a Checkers environment.", "pdf": "/pdf/87850f6ffe6f3c3098ce5843d5d5e9199fffa32a.pdf", "paperhash": "yang|cm3_cooperative_multigoal_multistage_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nYang2020CM3:,\ntitle={CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning},\nauthor={Jiachen Yang and Alireza Nakhaei and David Isele and Kikuo Fujimura and Hongyuan Zha},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lEX04tPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a62922022995dadb5791f84e334704eed8022a86.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1lEX04tPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1035/Authors", "ICLR.cc/2020/Conference/Paper1035/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1035/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1035/Reviewers", "ICLR.cc/2020/Conference/Paper1035/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1035/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1035/Authors|ICLR.cc/2020/Conference/Paper1035/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162268, "tmdate": 1576860549491, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1035/Authors", "ICLR.cc/2020/Conference/Paper1035/Reviewers", "ICLR.cc/2020/Conference/Paper1035/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1035/-/Official_Comment"}}}, {"id": "B1gWFfyOsH", "original": null, "number": 2, "cdate": 1573544569389, "ddate": null, "tcdate": 1573544569389, "tmdate": 1573544569389, "tddate": null, "forum": "S1lEX04tPr", "replyto": "Hyg_RfRAtr", "invitation": "ICLR.cc/2020/Conference/Paper1035/-/Official_Comment", "content": {"title": "We appreciate the constructive feedback and have updated the paper to discuss the relevant references.", "comment": "We thank Reviewer 3 for an accurate understanding of the work and for constructive feedback.\n\nWe placed the detailed definition of state, observation, actions, goals, reward and induced MDP for the 3 multi-goal Markov games in Appendix G because we want to give a full written description for reproducibility to accompany our code to be released. We also give a full written description of all neural networks for all methods in Appendix H. Combined, they take up 4-5 pages. We will continue to make an effort to move some of these details into the main text.\n\nRegarding the number of trainable parameters, we state in Algorithm Implementations in Section 5 that precise architecture details are in Appendix H. Except for QMIX\u2019s hypernetwork, all other neural networks have up to two hidden layers and one output layer, with convnets for processing the image part of observation and state in SUMO and Checkers. Regarding hyperparameters, CM3 has the same core set of hyperparameters as the baselines (see Appendix I). Analogous to QMIX\u2019 use of a hypernetwork, the function augmentation in CM3 is an additional degree of freedom but we did not face the need to tune the choice of layer i* in our experiments.\n\nRegarding IAC outperforming COMA and QMIX in cooperative navigation: \nThe updated version includes additional explanation. As Reviewer 3 point out, Singh et al. 2018 find that individual rewards help to resolve credit assignment. In Foerster et al. 2018, IAC learned from the global team reward. In our multi-goal setting, each IAC agent truly is independent and learns its own goal using its individual reward. Hence our experimental finding is consistent with Singh et al. 2018. We also note that the comparison of COMA to IAC in Foerster 2018 was conducted on the task of SC2 micromanagement, which may exhibit different challenges from the three tasks evaluated in our work. Nonetheless, our experiments in SUMO and Checkers confirm that IAC underperforms all other methods when the problem requires more sustained cooperative behavior.\n\nWe agree that Sukhbaatar et al. 2016 and Singh et al. 2018 are relevant literature that provide empirical support for the benefits of curriculum learning and individual rewards in MARL. We include this in Section 2 and 6 in the latest version. The main difference is that we explicitly train Stage 1 for all goals, and we evaluate on tasks where cooperation require physical and strategic movement without access to communication.\n\nWe work in the framework of centralized training with decentralized execution. We have added clarification in Section 4.4 that each individual agent does not use the private observation of any other agent as input to its policy. This is how we ensure decentralized execution. Instead, the centralized part of CM3 is the use of the credit function and the joint action-value function, both of which use the global state as input.\n\nAppendix E is a supplementary test of generalization in SUMO, not our primary metric of task performance and learning speed that we show in Figure 5. Appendix F describes the wall-clock time, which heavily depends on computational resources, and which is not a standard metric in the MARL literature (Hernandez-Leal et al. 2018). Hence we report the full training curve to show sample efficiency. We will improve the readability of Figure 5.\n\nThe equation in \u201cMulti-agent credit assignment\u201d in Section 3 is Equation (4) in Foerster et al. 2018, which does not appear to have a 1/n factor. Figure 6 is supplementary material to show convergence within the allotted sample count for Stage 1. We only need to state the numerical sample count in Section 6 to compare sample efficiency, so we leave the full figure in the Appendix.\n\nWe again express appreciation for Reviewer 3\u2019s constructive feedback."}, "signatures": ["ICLR.cc/2020/Conference/Paper1035/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1035/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning", "authors": ["Jiachen Yang", "Alireza Nakhaei", "David Isele", "Kikuo Fujimura", "Hongyuan Zha"], "authorids": ["yjiachen@gmail.com", "anakhaei@honda-ri.com", "disele@honda-ri.com", "kfujimura@honda-ri.com", "zha@cc.gatech.edu"], "keywords": ["multi-agent reinforcement learning"], "TL;DR": "A modular method for fully cooperative multi-goal multi-agent reinforcement learning, based on curriculum learning for efficient exploration and credit assignment for action-goal interactions.", "abstract": "A variety of cooperative multi-agent control problems require agents to achieve individual goals while contributing to collective success. This multi-goal multi-agent setting poses difficulties for recent algorithms, which primarily target settings with a single global reward, due to two new challenges: efficient exploration for learning both individual goal attainment and cooperation for others' success, and credit-assignment for interactions between actions and goals of different agents. To address both challenges, we restructure the problem into a novel two-stage curriculum, in which single-agent goal attainment is learned prior to learning multi-agent cooperation, and we derive a new multi-goal multi-agent policy gradient with a credit function for localized credit assignment. We use a function augmentation scheme to bridge value and policy functions across the curriculum. The complete architecture, called CM3, learns significantly faster than direct adaptations of existing algorithms on three challenging multi-goal multi-agent problems: cooperative navigation in difficult formations, negotiating multi-vehicle lane changes in the SUMO traffic simulator, and strategic cooperation in a Checkers environment.", "pdf": "/pdf/87850f6ffe6f3c3098ce5843d5d5e9199fffa32a.pdf", "paperhash": "yang|cm3_cooperative_multigoal_multistage_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nYang2020CM3:,\ntitle={CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning},\nauthor={Jiachen Yang and Alireza Nakhaei and David Isele and Kikuo Fujimura and Hongyuan Zha},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lEX04tPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a62922022995dadb5791f84e334704eed8022a86.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1lEX04tPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1035/Authors", "ICLR.cc/2020/Conference/Paper1035/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1035/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1035/Reviewers", "ICLR.cc/2020/Conference/Paper1035/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1035/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1035/Authors|ICLR.cc/2020/Conference/Paper1035/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162268, "tmdate": 1576860549491, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1035/Authors", "ICLR.cc/2020/Conference/Paper1035/Reviewers", "ICLR.cc/2020/Conference/Paper1035/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1035/-/Official_Comment"}}}, {"id": "Byx5AlJdjr", "original": null, "number": 1, "cdate": 1573544146278, "ddate": null, "tcdate": 1573544146278, "tmdate": 1573544146278, "tddate": null, "forum": "S1lEX04tPr", "replyto": "HJlEGDCkqS", "invitation": "ICLR.cc/2020/Conference/Paper1035/-/Official_Comment", "content": {"title": "We thank Reviewer 1 for the encouraging assessment.", "comment": "We thank Reviewer 1 for assessing our work, and we agree that investigating how best to introduce additional structure in MARL is a fruitful research direction, analogous to the case for single-agent RL. In addition to environment complexity, we believe that algorithms for MARL should be evaluated on diverse environment as well. As our experiments show, certain existing methods perform well in certain environments but underperform in others. Diversity of environments is needed to understand the relative strengths and weaknesses of each algorithm. The additional benefit of using a modular algorithm design, such as in CM3, is that ablation studies can give experimental guidance for the kinds of heuristics or structure that are useful for different kinds of problems and deserve more theoretical investigation."}, "signatures": ["ICLR.cc/2020/Conference/Paper1035/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1035/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning", "authors": ["Jiachen Yang", "Alireza Nakhaei", "David Isele", "Kikuo Fujimura", "Hongyuan Zha"], "authorids": ["yjiachen@gmail.com", "anakhaei@honda-ri.com", "disele@honda-ri.com", "kfujimura@honda-ri.com", "zha@cc.gatech.edu"], "keywords": ["multi-agent reinforcement learning"], "TL;DR": "A modular method for fully cooperative multi-goal multi-agent reinforcement learning, based on curriculum learning for efficient exploration and credit assignment for action-goal interactions.", "abstract": "A variety of cooperative multi-agent control problems require agents to achieve individual goals while contributing to collective success. This multi-goal multi-agent setting poses difficulties for recent algorithms, which primarily target settings with a single global reward, due to two new challenges: efficient exploration for learning both individual goal attainment and cooperation for others' success, and credit-assignment for interactions between actions and goals of different agents. To address both challenges, we restructure the problem into a novel two-stage curriculum, in which single-agent goal attainment is learned prior to learning multi-agent cooperation, and we derive a new multi-goal multi-agent policy gradient with a credit function for localized credit assignment. We use a function augmentation scheme to bridge value and policy functions across the curriculum. The complete architecture, called CM3, learns significantly faster than direct adaptations of existing algorithms on three challenging multi-goal multi-agent problems: cooperative navigation in difficult formations, negotiating multi-vehicle lane changes in the SUMO traffic simulator, and strategic cooperation in a Checkers environment.", "pdf": "/pdf/87850f6ffe6f3c3098ce5843d5d5e9199fffa32a.pdf", "paperhash": "yang|cm3_cooperative_multigoal_multistage_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nYang2020CM3:,\ntitle={CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning},\nauthor={Jiachen Yang and Alireza Nakhaei and David Isele and Kikuo Fujimura and Hongyuan Zha},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lEX04tPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a62922022995dadb5791f84e334704eed8022a86.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1lEX04tPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1035/Authors", "ICLR.cc/2020/Conference/Paper1035/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1035/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1035/Reviewers", "ICLR.cc/2020/Conference/Paper1035/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1035/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1035/Authors|ICLR.cc/2020/Conference/Paper1035/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162268, "tmdate": 1576860549491, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1035/Authors", "ICLR.cc/2020/Conference/Paper1035/Reviewers", "ICLR.cc/2020/Conference/Paper1035/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1035/-/Official_Comment"}}}, {"id": "r1e0fqAaKr", "original": null, "number": 1, "cdate": 1571838485698, "ddate": null, "tcdate": 1571838485698, "tmdate": 1572972520771, "tddate": null, "forum": "S1lEX04tPr", "replyto": "S1lEX04tPr", "invitation": "ICLR.cc/2020/Conference/Paper1035/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Contribution:\n\nThis paper derives a loss for cooperative MARL for the case where agents have individual goals assigned to them.\nThe assumption of individual goals allows for a better credit assignment, as well as effective pre-training of one agent to solve its own goal in isolation. Based on this observation, the paper subsequently proposes a curriculum learning scheme to fully take advantage of this property.\nExperiments in varied domains are shown.\n\nReview:\n\nThe paper is well written and easy to follow, and makes a good case by having a thorough experimental analysis, as well as a theoretical analysis of the credit function.\n\nThe applicability of the method seems rather high, even though there exists multiple MARL environments where the multi-goal assumption will be broken. In a predator-prey domain, for example, the predators can't learn anything because their tasks is not solvable with only one agent. In that context, it seems that stage 1 would be useless, but would stage 2 still work, and if yes how would that compare to other baseline methods?\n\nSome details of the multi-stage training are a bit unclear to me. Section 4.5 states \"we train an actor \\pi^1 and critic Q^1 to convergence [...]\". However, I don't fully grasp how this agent will be able to learn to solve all the goals? It seems to me that with N=1, equation (5) reduces to normal actor critic with one agent and one goal, but I'd expect that the policy must be trained on all the goals, as this is hinted at in section 5 (\" in Checkers, we alternate between training one agent as A and B\"). Could you clarify that part?\n\nAbout the function augmentation, could you clarify how the new network \\pi^2 is initialized? It seems that the initial values of W^1 and W^{1:2} in particular are quite important, because if the resulting policy is too far off from the initial policy \\pi^1, then the benefit of the pre-training could be lost on the way. Did you find that any special care like initializing W1 = I and W^{1:2} = 0 is required here?\n\n\nOne minor complaint is that the proposed method never seems to achieve statistically better performance than the best baseline on any of the tasks (for cooperative navigation it is tied with IAC and for SUMO and Checkers it is tied with QMIX). But since the best baseline is different across tasks, it suggests that the proposed method is more versatile.\n\n\nA potentially relevant missed reference: [1].\n\n\n[1] A Structured Prediction Approach for Generalization in Cooperative Multi-Agent Reinforcement Learning, Carion et al, https://arxiv.org/abs/1910.08809\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1035/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1035/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning", "authors": ["Jiachen Yang", "Alireza Nakhaei", "David Isele", "Kikuo Fujimura", "Hongyuan Zha"], "authorids": ["yjiachen@gmail.com", "anakhaei@honda-ri.com", "disele@honda-ri.com", "kfujimura@honda-ri.com", "zha@cc.gatech.edu"], "keywords": ["multi-agent reinforcement learning"], "TL;DR": "A modular method for fully cooperative multi-goal multi-agent reinforcement learning, based on curriculum learning for efficient exploration and credit assignment for action-goal interactions.", "abstract": "A variety of cooperative multi-agent control problems require agents to achieve individual goals while contributing to collective success. This multi-goal multi-agent setting poses difficulties for recent algorithms, which primarily target settings with a single global reward, due to two new challenges: efficient exploration for learning both individual goal attainment and cooperation for others' success, and credit-assignment for interactions between actions and goals of different agents. To address both challenges, we restructure the problem into a novel two-stage curriculum, in which single-agent goal attainment is learned prior to learning multi-agent cooperation, and we derive a new multi-goal multi-agent policy gradient with a credit function for localized credit assignment. We use a function augmentation scheme to bridge value and policy functions across the curriculum. The complete architecture, called CM3, learns significantly faster than direct adaptations of existing algorithms on three challenging multi-goal multi-agent problems: cooperative navigation in difficult formations, negotiating multi-vehicle lane changes in the SUMO traffic simulator, and strategic cooperation in a Checkers environment.", "pdf": "/pdf/87850f6ffe6f3c3098ce5843d5d5e9199fffa32a.pdf", "paperhash": "yang|cm3_cooperative_multigoal_multistage_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nYang2020CM3:,\ntitle={CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning},\nauthor={Jiachen Yang and Alireza Nakhaei and David Isele and Kikuo Fujimura and Hongyuan Zha},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lEX04tPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a62922022995dadb5791f84e334704eed8022a86.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1lEX04tPr", "replyto": "S1lEX04tPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1035/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1035/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575385894068, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1035/Reviewers"], "noninvitees": [], "tcdate": 1570237743350, "tmdate": 1575385894081, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1035/-/Official_Review"}}}, {"id": "Hyg_RfRAtr", "original": null, "number": 2, "cdate": 1571902160018, "ddate": null, "tcdate": 1571902160018, "tmdate": 1572972520720, "tddate": null, "forum": "S1lEX04tPr", "replyto": "S1lEX04tPr", "invitation": "ICLR.cc/2020/Conference/Paper1035/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents a new method CM3 for multi-goal multi-agent settings where agent must care about the rewards of others as well as theirs. The method restructures the problem into two-stage curriculum where individual agents solve the problem in the individual setting of the environment in the first stage and then more agents are introduced in the multi-agent setting in second stage. The method also uses function augmentation to only learn parameters which are necessary for single agent in first stage and then more parameters are introduced in the second stage. The results are shown on three environments which show that CM3 outperforms the baselines.\n\nThe paper is well-written, motivated and clear to read. A lot of important stuff has been pushed into appendix and experiments/setup require more details but overall I believe paper has significant contributions and results. Therefore, I assign a rating of weak accept which I am happy to raise if clarity of paper can be improved.\n\nThe paper doesn\u2019t comment on parameter counts of CM3 compared with baselines which is also an important factor in choosing one method over the other. I would like to see more quantitative analysis as presented in IAC to understand what is happening behind the scenes. It is very surprising to see that IAC is outperforming COMA in most of the tasks which is opposite of what COMA paper suggested.\n\nOn the note of curriculum [1] and [2] uses curriculum in traffic junction settings to improve overall performance as agent increases. This can be compared to moving from stage 1 to stage 2 but instead we move from less number of agents to more. [2] also suggests that using individualized rewards help in better credit assignment. There is also an assumption in the setup that private observations from all other agents are readily available. It would be interesting to see what would happen if agents have to communicate their private state. So, the paper is also missing discussion on communication protocols (discrete, continuous, through critic) [1][2]. \n\nIt is hard to directly compare through the charts. So, tables in Appendix E and F should be moved to the main text. The experiment section needs to be extended and the main model section needs to be decreased in the content amount.\n\nIt seems like you missed 1/n in advantage function\u2019s equation in Section 3, Multi-agent credit assignment section. Figure 6 has been mentioned in 6 second paragraph but doesn\u2019t appear until the last page.\n\n[1] Sukhbaatar, Sainbayar, and Rob Fergus. \"Learning multiagent communication with backpropagation.\" In Advances in Neural Information Processing Systems, pp. 2244-2252. 2016.\n[2] Singh, Amanpreet, Tushar Jain, and Sainbayar Sukhbaatar. \"Learning when to communicate at scale in multiagent cooperative and competitive tasks.\" arXiv preprint arXiv:1812.09755 (2018)."}, "signatures": ["ICLR.cc/2020/Conference/Paper1035/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1035/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning", "authors": ["Jiachen Yang", "Alireza Nakhaei", "David Isele", "Kikuo Fujimura", "Hongyuan Zha"], "authorids": ["yjiachen@gmail.com", "anakhaei@honda-ri.com", "disele@honda-ri.com", "kfujimura@honda-ri.com", "zha@cc.gatech.edu"], "keywords": ["multi-agent reinforcement learning"], "TL;DR": "A modular method for fully cooperative multi-goal multi-agent reinforcement learning, based on curriculum learning for efficient exploration and credit assignment for action-goal interactions.", "abstract": "A variety of cooperative multi-agent control problems require agents to achieve individual goals while contributing to collective success. This multi-goal multi-agent setting poses difficulties for recent algorithms, which primarily target settings with a single global reward, due to two new challenges: efficient exploration for learning both individual goal attainment and cooperation for others' success, and credit-assignment for interactions between actions and goals of different agents. To address both challenges, we restructure the problem into a novel two-stage curriculum, in which single-agent goal attainment is learned prior to learning multi-agent cooperation, and we derive a new multi-goal multi-agent policy gradient with a credit function for localized credit assignment. We use a function augmentation scheme to bridge value and policy functions across the curriculum. The complete architecture, called CM3, learns significantly faster than direct adaptations of existing algorithms on three challenging multi-goal multi-agent problems: cooperative navigation in difficult formations, negotiating multi-vehicle lane changes in the SUMO traffic simulator, and strategic cooperation in a Checkers environment.", "pdf": "/pdf/87850f6ffe6f3c3098ce5843d5d5e9199fffa32a.pdf", "paperhash": "yang|cm3_cooperative_multigoal_multistage_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nYang2020CM3:,\ntitle={CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning},\nauthor={Jiachen Yang and Alireza Nakhaei and David Isele and Kikuo Fujimura and Hongyuan Zha},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lEX04tPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a62922022995dadb5791f84e334704eed8022a86.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1lEX04tPr", "replyto": "S1lEX04tPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1035/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1035/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575385894068, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1035/Reviewers"], "noninvitees": [], "tcdate": 1570237743350, "tmdate": 1575385894081, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1035/-/Official_Review"}}}, {"id": "HJlEGDCkqS", "original": null, "number": 3, "cdate": 1571968780267, "ddate": null, "tcdate": 1571968780267, "tmdate": 1572972520678, "tddate": null, "forum": "S1lEX04tPr", "replyto": "S1lEX04tPr", "invitation": "ICLR.cc/2020/Conference/Paper1035/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper presents a method for adding credit assignment\nto multi-agent RL and proposes a way of adding a curriculum\nto the training process.\nThe best heuristics and structures to incorporate in the\nmodeling, learning, and exploration parts of multi-agent\nRL are still largely unknown and this paper explores some\nreasonable new ones.\nIn most of the tasks this is evaluated on in Figure 5\nthis approach adds a slight improvement to the SOTA\nand I think an exciting direction of future work is\nto continue pushing on multi-agent RL in even more\ncomplex envirnoments where the SOTA break down in\neven worse ways.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1035/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1035/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning", "authors": ["Jiachen Yang", "Alireza Nakhaei", "David Isele", "Kikuo Fujimura", "Hongyuan Zha"], "authorids": ["yjiachen@gmail.com", "anakhaei@honda-ri.com", "disele@honda-ri.com", "kfujimura@honda-ri.com", "zha@cc.gatech.edu"], "keywords": ["multi-agent reinforcement learning"], "TL;DR": "A modular method for fully cooperative multi-goal multi-agent reinforcement learning, based on curriculum learning for efficient exploration and credit assignment for action-goal interactions.", "abstract": "A variety of cooperative multi-agent control problems require agents to achieve individual goals while contributing to collective success. This multi-goal multi-agent setting poses difficulties for recent algorithms, which primarily target settings with a single global reward, due to two new challenges: efficient exploration for learning both individual goal attainment and cooperation for others' success, and credit-assignment for interactions between actions and goals of different agents. To address both challenges, we restructure the problem into a novel two-stage curriculum, in which single-agent goal attainment is learned prior to learning multi-agent cooperation, and we derive a new multi-goal multi-agent policy gradient with a credit function for localized credit assignment. We use a function augmentation scheme to bridge value and policy functions across the curriculum. The complete architecture, called CM3, learns significantly faster than direct adaptations of existing algorithms on three challenging multi-goal multi-agent problems: cooperative navigation in difficult formations, negotiating multi-vehicle lane changes in the SUMO traffic simulator, and strategic cooperation in a Checkers environment.", "pdf": "/pdf/87850f6ffe6f3c3098ce5843d5d5e9199fffa32a.pdf", "paperhash": "yang|cm3_cooperative_multigoal_multistage_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nYang2020CM3:,\ntitle={CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning},\nauthor={Jiachen Yang and Alireza Nakhaei and David Isele and Kikuo Fujimura and Hongyuan Zha},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lEX04tPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a62922022995dadb5791f84e334704eed8022a86.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1lEX04tPr", "replyto": "S1lEX04tPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1035/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1035/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575385894068, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1035/Reviewers"], "noninvitees": [], "tcdate": 1570237743350, "tmdate": 1575385894081, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1035/-/Official_Review"}}}], "count": 9}