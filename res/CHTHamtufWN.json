{"notes": [{"id": "CHTHamtufWN", "original": "f6cCd5U0Hg-", "number": 3136, "cdate": 1601308347852, "ddate": null, "tcdate": 1601308347852, "tmdate": 1614985755067, "tddate": null, "forum": "CHTHamtufWN", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Continual Invariant Risk Minimization", "authorids": ["~Francesco_Alesiani1", "~Shujian_Yu1", "~Mathias_Niepert1"], "authors": ["Francesco Alesiani", "Shujian Yu", "Mathias Niepert"], "keywords": ["Supervised Learning", "Causal Learning", "Invariant Risk Minimization", "Continual Learning"], "abstract": "Empirical risk minimization can lead to poor generalization behaviour on unseen environments if the learned model does not capture invariant feature represen- tations. Invariant risk minimization (IRM) is a recent proposal for discovering environment-invariant representations. It was introduced by Arjovsky et al. (2019) and extended by Ahuja et al. (2020). The assumption of IRM is that all environ- ments are available to the learning system at the same time. With this work, we generalize the concept of IRM to scenarios where environments are observed se- quentially. We show that existing approaches, including those designed for contin- ual learning, fail to identify the invariant features and models across sequentially presented environments. We extend IRM under a variational Bayesian and bilevel framework, creating a general approach to continual invariant risk minimization. We also describe a strategy to solve the optimization problems using a variant of the alternating direction method of multiplier (ADMM). We show empirically us- ing multiple datasets and with multiple sequential environments that the proposed methods outperforms or is competitive with prior approaches.", "one-sentence_summary": "We study the extension of Invariant Risk Minimization in sequential environments", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alesiani|continual_invariant_risk_minimization", "pdf": "/pdf/df1dd4b3bb815a98877cfa1f602aa565675dd445.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U3gQFheCfN", "_bibtex": "@misc{\nalesiani2021continual,\ntitle={Continual Invariant Risk Minimization},\nauthor={Francesco Alesiani and Shujian Yu and Mathias Niepert},\nyear={2021},\nurl={https://openreview.net/forum?id=CHTHamtufWN}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 18, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "0Mhcm6GTUWM", "original": null, "number": 1, "cdate": 1610040379675, "ddate": null, "tcdate": 1610040379675, "tmdate": 1610473972503, "tddate": null, "forum": "CHTHamtufWN", "replyto": "CHTHamtufWN", "invitation": "ICLR.cc/2021/Conference/Paper3136/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The authors address the problem of learning environment-invariant representations in the case where environments are observed sequentially.\nThis is done by using a variational Bayesian and bilevel framework.\n\nThe paper is borderline, with two reviewers (R2 and R3) favoring slightly acceptance and two reviewrs (R4 and R1) favoring rejection.\n\nR4 points out that the current experiments do not do a good job of reflecting a continual learning setup and that simple modifications on existing IRM based methods could outperform the method proposed by the authors. The authors are encouraged to take into account the reviewer's\nsuggestions to improve the paper.\n\nR1 argued initially that the proposed solution is not learning at all since it has errors very close to random guessing. While the authors have improved their method in the revision, the results are still close to random guessing, which questions the practical usefulness of the proposed approach. Also, in the revision, the authors managed to obtain better results when their method is combined with Environment Inference for Invariant Learning (EIIL), but these results are secondary and not the main part of the paper.\n\nThe authors should improve the work taking into account the reviewrs' comments."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continual Invariant Risk Minimization", "authorids": ["~Francesco_Alesiani1", "~Shujian_Yu1", "~Mathias_Niepert1"], "authors": ["Francesco Alesiani", "Shujian Yu", "Mathias Niepert"], "keywords": ["Supervised Learning", "Causal Learning", "Invariant Risk Minimization", "Continual Learning"], "abstract": "Empirical risk minimization can lead to poor generalization behaviour on unseen environments if the learned model does not capture invariant feature represen- tations. Invariant risk minimization (IRM) is a recent proposal for discovering environment-invariant representations. It was introduced by Arjovsky et al. (2019) and extended by Ahuja et al. (2020). The assumption of IRM is that all environ- ments are available to the learning system at the same time. With this work, we generalize the concept of IRM to scenarios where environments are observed se- quentially. We show that existing approaches, including those designed for contin- ual learning, fail to identify the invariant features and models across sequentially presented environments. We extend IRM under a variational Bayesian and bilevel framework, creating a general approach to continual invariant risk minimization. We also describe a strategy to solve the optimization problems using a variant of the alternating direction method of multiplier (ADMM). We show empirically us- ing multiple datasets and with multiple sequential environments that the proposed methods outperforms or is competitive with prior approaches.", "one-sentence_summary": "We study the extension of Invariant Risk Minimization in sequential environments", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alesiani|continual_invariant_risk_minimization", "pdf": "/pdf/df1dd4b3bb815a98877cfa1f602aa565675dd445.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U3gQFheCfN", "_bibtex": "@misc{\nalesiani2021continual,\ntitle={Continual Invariant Risk Minimization},\nauthor={Francesco Alesiani and Shujian Yu and Mathias Niepert},\nyear={2021},\nurl={https://openreview.net/forum?id=CHTHamtufWN}\n}"}, "tags": [], "invitation": {"reply": {"forum": "CHTHamtufWN", "replyto": "CHTHamtufWN", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040379659, "tmdate": 1610473972486, "id": "ICLR.cc/2021/Conference/Paper3136/-/Decision"}}}, {"id": "r5tbGSrYuSw", "original": null, "number": 1, "cdate": 1603772046750, "ddate": null, "tcdate": 1603772046750, "tmdate": 1606605696638, "tddate": null, "forum": "CHTHamtufWN", "replyto": "CHTHamtufWN", "invitation": "ICLR.cc/2021/Conference/Paper3136/-/Official_Review", "content": {"title": "The experimental results do not pass basic sanity checks", "review": "# Post-discussion update \n\nThe authors have significantly updated the paper during the discussion period. I have seen the changes, but they unfortunately do not substantiate the claim that the proposed methods are learning anything meaningful. \n\nIn the new results in table 1, the train accuracy now is better than random, but the test accuracy on the unseen environments is worse than a model that makes uniformly random predictions. This implies that the proposed method is not learning to ignore the spurious features at all. It might seem from table 1 that C-VIRMv1 is performing well --- it has 46% accuracy on the test environment after all. However, C-VIRMv1 also has the worse performance on the train set (50% accuracy). In-fact, the test or train performance alone does not mean anything in this benchmark. The goal of the benchmark is to do well on the train set while also generalizing to an unseen environment. Doing well on test set by doing poorly on the training set is not progress. \n\nThe new results in table 3 are equally troubling. The authors claim that the results in table 3 show that C-VIRM with EIIL is performing better than IRM with EIIL, but the data does not support the claim. Both IRM and C-VIRM are performing similarly; all the results are with-in error margins of each others, and the table can not be used to make any claims. \n\nI would encourage the authors to do a more systematic study of the proposed method. Investigate if the proposed methods are learning to ignore the spurious correlation at all (Table 1 suggests they are not) and only make claims that are supported by the data. In its current form, I cannot recommend this paper for acceptance. \n\n# Initial review\n\nI'm going to keep the initial review short to point out a fundamental issue with the paper. Until this issue is addressed, I see little point in discussing the paper at length. Hopefully the authors can address the issue in the discussion period and I will update my review to include other aspects of the paper. \n## Issue No 1 ##\n The proposed solution is arguably not learning at all. Both MNIST and FashionMNIST, as used in this paper and earlier IRM papers, are binary classification benchmarks (Five classes have label 0 and remaining five have label 1). A completely randomly initialized neural network gets ~50% accuracy on a binary classification task. This is not just speculation, I have run experiments using a randomly initialized network on these benchmarks to confirm that they do indeed get ~50% +- 2% accuracy on both train and test set. Coincidently, both C-BVIRM and C-VIRMG also get ~50% accuracy on the train/test set. This would imply that the model learned by C-BVIRM and C-VIRMG is indistinguishable from a model that does no learning. Here are some other baselines that will do as well as C-BVIRM: \n\n1. Set learning rate of ERM to be infinitesimally small so that no learning happens in the given number of steps (set learning rate to $e^{-100}$ for instance). \n\n2. Make completely random changes in the weight instead of updating the weights using gradients. \n\n3. Make no changes to the network. \n\nClearly 1, 2 and 3 are bad learning algorithms (the first and third are not learning at all whereas the second is making arbitrary updates), yet all three of them would perform as well as the two algorithms proposed in the paper. \n\nDoes this mean the two methods proposed in the paper are bad? Not necessarily. The BIRM formulation of the problem seems technically correct and the authors are tackling and important and interesting problem. However, the poor performance of C-BVIRM does imply that the authors need to look at their experiment results in more detail and do some sanity checks. \n\nA sanity check for checking if C-BVIRM is learning anything at all is to test it on a benchmark for which both train and test environments have a constant $p_c$ (say 0.1). A good learner should get 90% accuracy on both train and test environment in such a benchmark (Both IRM and ERM would get 90% accuracy). My guess is that C-BVIRM and C-VIRMG would get 50% accuracy on train/test when trained using the same parameters as used to run the experiments in the paper.  \n\n## Issue No 2 ##\nPlease see the private comment for the second issue. \n\nUntil issue 1 is addressed, discussing the technical details of the paper wouldn't make a difference and I can not recommend an acceptance. ", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3136/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3136/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continual Invariant Risk Minimization", "authorids": ["~Francesco_Alesiani1", "~Shujian_Yu1", "~Mathias_Niepert1"], "authors": ["Francesco Alesiani", "Shujian Yu", "Mathias Niepert"], "keywords": ["Supervised Learning", "Causal Learning", "Invariant Risk Minimization", "Continual Learning"], "abstract": "Empirical risk minimization can lead to poor generalization behaviour on unseen environments if the learned model does not capture invariant feature represen- tations. Invariant risk minimization (IRM) is a recent proposal for discovering environment-invariant representations. It was introduced by Arjovsky et al. (2019) and extended by Ahuja et al. (2020). The assumption of IRM is that all environ- ments are available to the learning system at the same time. With this work, we generalize the concept of IRM to scenarios where environments are observed se- quentially. We show that existing approaches, including those designed for contin- ual learning, fail to identify the invariant features and models across sequentially presented environments. We extend IRM under a variational Bayesian and bilevel framework, creating a general approach to continual invariant risk minimization. We also describe a strategy to solve the optimization problems using a variant of the alternating direction method of multiplier (ADMM). We show empirically us- ing multiple datasets and with multiple sequential environments that the proposed methods outperforms or is competitive with prior approaches.", "one-sentence_summary": "We study the extension of Invariant Risk Minimization in sequential environments", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alesiani|continual_invariant_risk_minimization", "pdf": "/pdf/df1dd4b3bb815a98877cfa1f602aa565675dd445.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U3gQFheCfN", "_bibtex": "@misc{\nalesiani2021continual,\ntitle={Continual Invariant Risk Minimization},\nauthor={Francesco Alesiani and Shujian Yu and Mathias Niepert},\nyear={2021},\nurl={https://openreview.net/forum?id=CHTHamtufWN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "CHTHamtufWN", "replyto": "CHTHamtufWN", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3136/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538081448, "tmdate": 1606915766736, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3136/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3136/-/Official_Review"}}}, {"id": "kqaX38Tdp39", "original": null, "number": 17, "cdate": 1606305658760, "ddate": null, "tcdate": 1606305658760, "tmdate": 1606305658760, "tddate": null, "forum": "CHTHamtufWN", "replyto": "Nq7iopGhwdZ", "invitation": "ICLR.cc/2021/Conference/Paper3136/-/Official_Comment", "content": {"title": "Thank you for your insightful comments", "comment": "Thank you again for your insightful comments,\n\nIndeed, from Table 4, the performance of IRMv1 is similar to C-VIRMv1. However, we also observed a constant performance gain of C-VIRMv1 + EIIL over IRMv1 + EIIL. In our Table 2, we also compared with benchmark continual learning methods, including EWC, GEM, MER and VCL. Results suggest that our methods always have better test accuracy, especially for C-VIRMv1. \n\nIn our paper, we highlighted a limitation of current methods, and also based on your intuition (thank you for that) we delineated few directions to address this problem: 1) modelling the invariant models as distributions and discover invariant distributions in sequential environments by sequential information projection and 2) infer invariant models from single dataset by dataset partition (as EIIL provides). Indeed the second approach still has the problem of caring over the learned invariant model into future datasets. Further, as highlighted in the experiments on sample size (Table 4) in the new experimental section, when the number of samples is too small, even environment inference does not help in discovering invariant models.   \n\nRegarding your point (g), we thank you again for the insightful suggestions, which we already partially incorporated in the current work, and we will consider them for future extensions. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3136/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3136/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continual Invariant Risk Minimization", "authorids": ["~Francesco_Alesiani1", "~Shujian_Yu1", "~Mathias_Niepert1"], "authors": ["Francesco Alesiani", "Shujian Yu", "Mathias Niepert"], "keywords": ["Supervised Learning", "Causal Learning", "Invariant Risk Minimization", "Continual Learning"], "abstract": "Empirical risk minimization can lead to poor generalization behaviour on unseen environments if the learned model does not capture invariant feature represen- tations. Invariant risk minimization (IRM) is a recent proposal for discovering environment-invariant representations. It was introduced by Arjovsky et al. (2019) and extended by Ahuja et al. (2020). The assumption of IRM is that all environ- ments are available to the learning system at the same time. With this work, we generalize the concept of IRM to scenarios where environments are observed se- quentially. We show that existing approaches, including those designed for contin- ual learning, fail to identify the invariant features and models across sequentially presented environments. We extend IRM under a variational Bayesian and bilevel framework, creating a general approach to continual invariant risk minimization. We also describe a strategy to solve the optimization problems using a variant of the alternating direction method of multiplier (ADMM). We show empirically us- ing multiple datasets and with multiple sequential environments that the proposed methods outperforms or is competitive with prior approaches.", "one-sentence_summary": "We study the extension of Invariant Risk Minimization in sequential environments", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alesiani|continual_invariant_risk_minimization", "pdf": "/pdf/df1dd4b3bb815a98877cfa1f602aa565675dd445.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U3gQFheCfN", "_bibtex": "@misc{\nalesiani2021continual,\ntitle={Continual Invariant Risk Minimization},\nauthor={Francesco Alesiani and Shujian Yu and Mathias Niepert},\nyear={2021},\nurl={https://openreview.net/forum?id=CHTHamtufWN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "CHTHamtufWN", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3136/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3136/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3136/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3136/Authors|ICLR.cc/2021/Conference/Paper3136/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3136/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923840692, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3136/-/Official_Comment"}}}, {"id": "Nq7iopGhwdZ", "original": null, "number": 16, "cdate": 1606286504241, "ddate": null, "tcdate": 1606286504241, "tmdate": 1606286504241, "tddate": null, "forum": "CHTHamtufWN", "replyto": "Q3q4GlAJKQ-", "invitation": "ICLR.cc/2021/Conference/Paper3136/-/Official_Comment", "content": {"title": "Response", "comment": "Thank you for running the comparison. In all the comparisons that you do when you present data from one environment only to the original IRMv1, then the only fair comparison is to use EIIL as well on top. Now after seeing your experiments it seems from Table 4 that IRMv1 with EIIL and C-VIRMv1 with EIIL are similar in performance. This was my original belief that it is important to show gains w.r.t simple baseline such as EIIL + IRMv1. \nIn light of the new comparisons, my suggestion to the authors would be:\n1.If the method can be improved further so that the gains can be more pronounced (currently everything is in the same ball park).\n2.  I feel this paper will have a good potential if more convincing experiments that reflect a continual learning setup are done. I believe in the current experiments IRMv1 with EIIL is sufficient. However, I believe the method proposed by the authors can have benefits provided right type of experiments are done (See point g) in my original response). "}, "signatures": ["ICLR.cc/2021/Conference/Paper3136/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3136/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continual Invariant Risk Minimization", "authorids": ["~Francesco_Alesiani1", "~Shujian_Yu1", "~Mathias_Niepert1"], "authors": ["Francesco Alesiani", "Shujian Yu", "Mathias Niepert"], "keywords": ["Supervised Learning", "Causal Learning", "Invariant Risk Minimization", "Continual Learning"], "abstract": "Empirical risk minimization can lead to poor generalization behaviour on unseen environments if the learned model does not capture invariant feature represen- tations. Invariant risk minimization (IRM) is a recent proposal for discovering environment-invariant representations. It was introduced by Arjovsky et al. (2019) and extended by Ahuja et al. (2020). The assumption of IRM is that all environ- ments are available to the learning system at the same time. With this work, we generalize the concept of IRM to scenarios where environments are observed se- quentially. We show that existing approaches, including those designed for contin- ual learning, fail to identify the invariant features and models across sequentially presented environments. We extend IRM under a variational Bayesian and bilevel framework, creating a general approach to continual invariant risk minimization. We also describe a strategy to solve the optimization problems using a variant of the alternating direction method of multiplier (ADMM). We show empirically us- ing multiple datasets and with multiple sequential environments that the proposed methods outperforms or is competitive with prior approaches.", "one-sentence_summary": "We study the extension of Invariant Risk Minimization in sequential environments", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alesiani|continual_invariant_risk_minimization", "pdf": "/pdf/df1dd4b3bb815a98877cfa1f602aa565675dd445.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U3gQFheCfN", "_bibtex": "@misc{\nalesiani2021continual,\ntitle={Continual Invariant Risk Minimization},\nauthor={Francesco Alesiani and Shujian Yu and Mathias Niepert},\nyear={2021},\nurl={https://openreview.net/forum?id=CHTHamtufWN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "CHTHamtufWN", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3136/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3136/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3136/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3136/Authors|ICLR.cc/2021/Conference/Paper3136/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3136/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923840692, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3136/-/Official_Comment"}}}, {"id": "WB5nCBAlvS7", "original": null, "number": 15, "cdate": 1606273631614, "ddate": null, "tcdate": 1606273631614, "tmdate": 1606273631614, "tddate": null, "forum": "CHTHamtufWN", "replyto": "CHTHamtufWN", "invitation": "ICLR.cc/2021/Conference/Paper3136/-/Official_Comment", "content": {"title": "Summary of the changes", "comment": "Dear Reviewers,\n\nWe have incorporated your valuable feedback and provided a new paper revision. We summarize here the main changes:\n\n* Updated and corrected definition 2 (section 3.3) and added the connection of Definition 1 and Definition 2 to the original definition of IRM in the Supplementary material (section A.5), supported by associated Lemmas (Lemma 6 and Lemma 7) [per request of R3]\n* Clarified the connection of the information projection and updated the description in Section 3.5. In the supplementary material (Section A.6) we provided more insight in the sequential information projection (Section 6.1 and Lemma 8) and its link to discover invariant distributions and IRM (Section A6.2). [per request of R4]\n* In section A.7 we extend out-of-distribution generalization results of Arjovsky et al. (2019) (theorem 9) to the BIRM and BVIRM problems. [per request of R3]\n* We added the new experimental section 5.3 where we evaluated the Environment Inference for continual learning invariant models, i.e. in presence of a single environment and explored the dependency on the sample size. In the supplementary material we also extended the experiment using the synthetic dataset proposed in Arjovsky et al. (2019) and compared with the method EIIL of  (Creager et al., 2020), using the experimental setup of this last work. [per request of R4]\n* We improved the presentation of the ADMM algorithm of section 3.4 and 3.4.1. [per request of R3]\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3136/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3136/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continual Invariant Risk Minimization", "authorids": ["~Francesco_Alesiani1", "~Shujian_Yu1", "~Mathias_Niepert1"], "authors": ["Francesco Alesiani", "Shujian Yu", "Mathias Niepert"], "keywords": ["Supervised Learning", "Causal Learning", "Invariant Risk Minimization", "Continual Learning"], "abstract": "Empirical risk minimization can lead to poor generalization behaviour on unseen environments if the learned model does not capture invariant feature represen- tations. Invariant risk minimization (IRM) is a recent proposal for discovering environment-invariant representations. It was introduced by Arjovsky et al. (2019) and extended by Ahuja et al. (2020). The assumption of IRM is that all environ- ments are available to the learning system at the same time. With this work, we generalize the concept of IRM to scenarios where environments are observed se- quentially. We show that existing approaches, including those designed for contin- ual learning, fail to identify the invariant features and models across sequentially presented environments. We extend IRM under a variational Bayesian and bilevel framework, creating a general approach to continual invariant risk minimization. We also describe a strategy to solve the optimization problems using a variant of the alternating direction method of multiplier (ADMM). We show empirically us- ing multiple datasets and with multiple sequential environments that the proposed methods outperforms or is competitive with prior approaches.", "one-sentence_summary": "We study the extension of Invariant Risk Minimization in sequential environments", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alesiani|continual_invariant_risk_minimization", "pdf": "/pdf/df1dd4b3bb815a98877cfa1f602aa565675dd445.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U3gQFheCfN", "_bibtex": "@misc{\nalesiani2021continual,\ntitle={Continual Invariant Risk Minimization},\nauthor={Francesco Alesiani and Shujian Yu and Mathias Niepert},\nyear={2021},\nurl={https://openreview.net/forum?id=CHTHamtufWN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "CHTHamtufWN", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3136/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3136/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3136/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3136/Authors|ICLR.cc/2021/Conference/Paper3136/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3136/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923840692, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3136/-/Official_Comment"}}}, {"id": "q2FLHNN4IMn", "original": null, "number": 14, "cdate": 1606273471663, "ddate": null, "tcdate": 1606273471663, "tmdate": 1606273471663, "tddate": null, "forum": "CHTHamtufWN", "replyto": "Ef8gE5-xPiW", "invitation": "ICLR.cc/2021/Conference/Paper3136/-/Official_Comment", "content": {"title": "Thank you for your review - Reply to your comment on scalability ", "comment": "Thank you for your comment, we address here your comment on the scalability.\n\n[Q] An evaluation of the method for larger architectures (e.g. a convolutional network) would make the approach more convincing. At the moment, I get the impression that there are issues with the scalability of the approach to larger data sets and models.\n\n[R] While Variational methods introduce additional cost of constant factor in term of computation cost and parameters space (indeed we are computing a distribution rather then a deterministic network), the bayesian methods are generally applicable to CNN [2] (actually the number of parameters in CNN is typically smaller than in FCN) and their computational complexity does not prevent the use in larger architectures. We have not provided experiments in this direction though. From a practical point of view it is also possible to use a pre-trained (pre-) feature extraction network, followed by the proposed approach. Pre-train network can also be used directly to initialize Variational networks and then followed with the normal training.\n\n[2] Zhao, C., Ni, B., Zhang, J., Zhao, Q., Zhang, W. and Tian, Q., 2019. Variational convolutional neural network pruning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 2780-2789).\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3136/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3136/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continual Invariant Risk Minimization", "authorids": ["~Francesco_Alesiani1", "~Shujian_Yu1", "~Mathias_Niepert1"], "authors": ["Francesco Alesiani", "Shujian Yu", "Mathias Niepert"], "keywords": ["Supervised Learning", "Causal Learning", "Invariant Risk Minimization", "Continual Learning"], "abstract": "Empirical risk minimization can lead to poor generalization behaviour on unseen environments if the learned model does not capture invariant feature represen- tations. Invariant risk minimization (IRM) is a recent proposal for discovering environment-invariant representations. It was introduced by Arjovsky et al. (2019) and extended by Ahuja et al. (2020). The assumption of IRM is that all environ- ments are available to the learning system at the same time. With this work, we generalize the concept of IRM to scenarios where environments are observed se- quentially. We show that existing approaches, including those designed for contin- ual learning, fail to identify the invariant features and models across sequentially presented environments. We extend IRM under a variational Bayesian and bilevel framework, creating a general approach to continual invariant risk minimization. We also describe a strategy to solve the optimization problems using a variant of the alternating direction method of multiplier (ADMM). We show empirically us- ing multiple datasets and with multiple sequential environments that the proposed methods outperforms or is competitive with prior approaches.", "one-sentence_summary": "We study the extension of Invariant Risk Minimization in sequential environments", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alesiani|continual_invariant_risk_minimization", "pdf": "/pdf/df1dd4b3bb815a98877cfa1f602aa565675dd445.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U3gQFheCfN", "_bibtex": "@misc{\nalesiani2021continual,\ntitle={Continual Invariant Risk Minimization},\nauthor={Francesco Alesiani and Shujian Yu and Mathias Niepert},\nyear={2021},\nurl={https://openreview.net/forum?id=CHTHamtufWN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "CHTHamtufWN", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3136/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3136/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3136/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3136/Authors|ICLR.cc/2021/Conference/Paper3136/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3136/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923840692, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3136/-/Official_Comment"}}}, {"id": "XsYDPQ7Mxm", "original": null, "number": 13, "cdate": 1606273352469, "ddate": null, "tcdate": 1606273352469, "tmdate": 1606273352469, "tddate": null, "forum": "CHTHamtufWN", "replyto": "EfNvGph3hyX", "invitation": "ICLR.cc/2021/Conference/Paper3136/-/Official_Comment", "content": {"title": "Thank you for your review - Reply to your Concern 1 on generalization", "comment": "Thanks for the valuable suggestions. \n\nWe added the generalization analysis in supplementary material A.7, which suggests that the Theorem 9 in Arjovsky et al. (2019) also holds for BIRM and BVIRM. "}, "signatures": ["ICLR.cc/2021/Conference/Paper3136/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3136/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continual Invariant Risk Minimization", "authorids": ["~Francesco_Alesiani1", "~Shujian_Yu1", "~Mathias_Niepert1"], "authors": ["Francesco Alesiani", "Shujian Yu", "Mathias Niepert"], "keywords": ["Supervised Learning", "Causal Learning", "Invariant Risk Minimization", "Continual Learning"], "abstract": "Empirical risk minimization can lead to poor generalization behaviour on unseen environments if the learned model does not capture invariant feature represen- tations. Invariant risk minimization (IRM) is a recent proposal for discovering environment-invariant representations. It was introduced by Arjovsky et al. (2019) and extended by Ahuja et al. (2020). The assumption of IRM is that all environ- ments are available to the learning system at the same time. With this work, we generalize the concept of IRM to scenarios where environments are observed se- quentially. We show that existing approaches, including those designed for contin- ual learning, fail to identify the invariant features and models across sequentially presented environments. We extend IRM under a variational Bayesian and bilevel framework, creating a general approach to continual invariant risk minimization. We also describe a strategy to solve the optimization problems using a variant of the alternating direction method of multiplier (ADMM). We show empirically us- ing multiple datasets and with multiple sequential environments that the proposed methods outperforms or is competitive with prior approaches.", "one-sentence_summary": "We study the extension of Invariant Risk Minimization in sequential environments", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alesiani|continual_invariant_risk_minimization", "pdf": "/pdf/df1dd4b3bb815a98877cfa1f602aa565675dd445.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U3gQFheCfN", "_bibtex": "@misc{\nalesiani2021continual,\ntitle={Continual Invariant Risk Minimization},\nauthor={Francesco Alesiani and Shujian Yu and Mathias Niepert},\nyear={2021},\nurl={https://openreview.net/forum?id=CHTHamtufWN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "CHTHamtufWN", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3136/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3136/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3136/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3136/Authors|ICLR.cc/2021/Conference/Paper3136/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3136/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923840692, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3136/-/Official_Comment"}}}, {"id": "Q3q4GlAJKQ-", "original": null, "number": 12, "cdate": 1606273214736, "ddate": null, "tcdate": 1606273214736, "tmdate": 1606273214736, "tddate": null, "forum": "CHTHamtufWN", "replyto": "RbuyYKZAzq", "invitation": "ICLR.cc/2021/Conference/Paper3136/-/Official_Comment", "content": {"title": "Thank you for your review - Comparison with Creager et al", "comment": "Thanks for your suggestion on EIIL [Creager et al. (2020)].  \n\nWe evaluate and explore EIIL from two aspects.\n\nFirst, we begin with the same synthetic regression dataset in the original IRM (Arjovsky et al., 2019). In case of only one environment data, the following table (see also Section A.13.1 and Table 5 in the revised submission) suggests that our BIRM (i.e., Eq. (8) optimized with ADMM) performs similarly to IRM + EIIL.  \n\n|        Method                   |     Causal MSE                 |            Noncausal MSE |\n| -------|:--------------:|:-------------:|\n| ERM                |        0.827 (0.016)               |                  0.824 (0.015) |\n| ICP                   |       1.000 (0)                      |                 0.756 (0.423) |\n| IRM                  |       0.280 (0.006)                |                  0.290 (0.009) |\n| BIRM                |       0.183 (0.005)               |                   0.184 (0.002) |\n| EIIL (IRM)        |       0.180 (0.026)                |                  0.188 (0.033) |\n\nSecond, motivated by the experimental setup in (Creager et al., 2020), we generalize our continual invariant learning models into an environment-agnostic setting by integrating ELLI into our proposed C-VIRMv1. We do notice an obvious performance gain of our C-VIRMv1 and IRMv1 when they are combined with EIIL. The new results in Tables 3 and 4 (Section 5.3) of the revised submission further strengthened the argument that \"inferring environments directly from observational data has the potential to improve (continual) invariant learning relative to using the hand-crafted environments\". Moreover, it seems C-VIRMv1+EIIL always outperforms IRMv1+EIIL.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3136/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3136/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continual Invariant Risk Minimization", "authorids": ["~Francesco_Alesiani1", "~Shujian_Yu1", "~Mathias_Niepert1"], "authors": ["Francesco Alesiani", "Shujian Yu", "Mathias Niepert"], "keywords": ["Supervised Learning", "Causal Learning", "Invariant Risk Minimization", "Continual Learning"], "abstract": "Empirical risk minimization can lead to poor generalization behaviour on unseen environments if the learned model does not capture invariant feature represen- tations. Invariant risk minimization (IRM) is a recent proposal for discovering environment-invariant representations. It was introduced by Arjovsky et al. (2019) and extended by Ahuja et al. (2020). The assumption of IRM is that all environ- ments are available to the learning system at the same time. With this work, we generalize the concept of IRM to scenarios where environments are observed se- quentially. We show that existing approaches, including those designed for contin- ual learning, fail to identify the invariant features and models across sequentially presented environments. We extend IRM under a variational Bayesian and bilevel framework, creating a general approach to continual invariant risk minimization. We also describe a strategy to solve the optimization problems using a variant of the alternating direction method of multiplier (ADMM). We show empirically us- ing multiple datasets and with multiple sequential environments that the proposed methods outperforms or is competitive with prior approaches.", "one-sentence_summary": "We study the extension of Invariant Risk Minimization in sequential environments", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alesiani|continual_invariant_risk_minimization", "pdf": "/pdf/df1dd4b3bb815a98877cfa1f602aa565675dd445.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U3gQFheCfN", "_bibtex": "@misc{\nalesiani2021continual,\ntitle={Continual Invariant Risk Minimization},\nauthor={Francesco Alesiani and Shujian Yu and Mathias Niepert},\nyear={2021},\nurl={https://openreview.net/forum?id=CHTHamtufWN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "CHTHamtufWN", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3136/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3136/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3136/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3136/Authors|ICLR.cc/2021/Conference/Paper3136/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3136/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923840692, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3136/-/Official_Comment"}}}, {"id": "WykHjqi_9D", "original": null, "number": 11, "cdate": 1606272994452, "ddate": null, "tcdate": 1606272994452, "tmdate": 1606272994452, "tddate": null, "forum": "CHTHamtufWN", "replyto": "HkkguB4sWuO", "invitation": "ICLR.cc/2021/Conference/Paper3136/-/Official_Comment", "content": {"title": "Thank you for your feedback", "comment": "Thanks for your question. Your comment has really helped us to improve the experimental setup. \n\nIn order to strengthen our accuracy, we made the following changes:\n1. We selected models and hyper-parameters much more carefully. Specifically, we removed the drop-out, originally proposed in (Ahuja et al., 2020), while training BVIRM methods. This was the result of selecting the hyper-parameters based on the performance on the test environment.  For the new simulations, we selected hyper-parameters that show better convergence in the training set. \n2.  We replaced \"ELU\" activation function with \"ReLU\". This was observed beneficial in the new experiments, when comparing with (Creager et al., 2020). Interestingly there was not issues for computing the second derivative, required in the IRM constraint. \n3. We cleaned the code and identified small differences in the training loop of ADMM (which may influence the performance of our C-BVIRM) and fixed that. We plan to make code publicly available upon acceptance.\n\nWe clarified experimental setup in the revised submission. We also updated quantitative results in Table 1 and Table 2, which suggest that our method offers a more reasonable performance gain over competitors. As can be seen, the accuracy values are different from 50%. \n\nFurther, based on suggestions from Reviewer 4, we noticed that, the performance of our method can be improved when it is combined with Environment Inference for Invariant Learning (EIIL) (Creager et al., 2020), a recently proposed strategy of automatic environment partition. These results are shown in Tables 3 and 4 in the revised submission.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3136/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3136/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continual Invariant Risk Minimization", "authorids": ["~Francesco_Alesiani1", "~Shujian_Yu1", "~Mathias_Niepert1"], "authors": ["Francesco Alesiani", "Shujian Yu", "Mathias Niepert"], "keywords": ["Supervised Learning", "Causal Learning", "Invariant Risk Minimization", "Continual Learning"], "abstract": "Empirical risk minimization can lead to poor generalization behaviour on unseen environments if the learned model does not capture invariant feature represen- tations. Invariant risk minimization (IRM) is a recent proposal for discovering environment-invariant representations. It was introduced by Arjovsky et al. (2019) and extended by Ahuja et al. (2020). The assumption of IRM is that all environ- ments are available to the learning system at the same time. With this work, we generalize the concept of IRM to scenarios where environments are observed se- quentially. We show that existing approaches, including those designed for contin- ual learning, fail to identify the invariant features and models across sequentially presented environments. We extend IRM under a variational Bayesian and bilevel framework, creating a general approach to continual invariant risk minimization. We also describe a strategy to solve the optimization problems using a variant of the alternating direction method of multiplier (ADMM). We show empirically us- ing multiple datasets and with multiple sequential environments that the proposed methods outperforms or is competitive with prior approaches.", "one-sentence_summary": "We study the extension of Invariant Risk Minimization in sequential environments", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alesiani|continual_invariant_risk_minimization", "pdf": "/pdf/df1dd4b3bb815a98877cfa1f602aa565675dd445.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U3gQFheCfN", "_bibtex": "@misc{\nalesiani2021continual,\ntitle={Continual Invariant Risk Minimization},\nauthor={Francesco Alesiani and Shujian Yu and Mathias Niepert},\nyear={2021},\nurl={https://openreview.net/forum?id=CHTHamtufWN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "CHTHamtufWN", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3136/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3136/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3136/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3136/Authors|ICLR.cc/2021/Conference/Paper3136/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3136/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923840692, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3136/-/Official_Comment"}}}, {"id": "RbuyYKZAzq", "original": null, "number": 10, "cdate": 1606194627026, "ddate": null, "tcdate": 1606194627026, "tmdate": 1606194627026, "tddate": null, "forum": "CHTHamtufWN", "replyto": "LbI_9J3oxOI", "invitation": "ICLR.cc/2021/Conference/Paper3136/-/Official_Comment", "content": {"title": "Response", "comment": "Thank you for your efforts to revise and provide responses. In [Re] you say IRM cannot work with one environment data. I have pointed in f), the reference to address exactly this issue. So currently a fair comparison with most important approach IRM is not done.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3136/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3136/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continual Invariant Risk Minimization", "authorids": ["~Francesco_Alesiani1", "~Shujian_Yu1", "~Mathias_Niepert1"], "authors": ["Francesco Alesiani", "Shujian Yu", "Mathias Niepert"], "keywords": ["Supervised Learning", "Causal Learning", "Invariant Risk Minimization", "Continual Learning"], "abstract": "Empirical risk minimization can lead to poor generalization behaviour on unseen environments if the learned model does not capture invariant feature represen- tations. Invariant risk minimization (IRM) is a recent proposal for discovering environment-invariant representations. It was introduced by Arjovsky et al. (2019) and extended by Ahuja et al. (2020). The assumption of IRM is that all environ- ments are available to the learning system at the same time. With this work, we generalize the concept of IRM to scenarios where environments are observed se- quentially. We show that existing approaches, including those designed for contin- ual learning, fail to identify the invariant features and models across sequentially presented environments. We extend IRM under a variational Bayesian and bilevel framework, creating a general approach to continual invariant risk minimization. We also describe a strategy to solve the optimization problems using a variant of the alternating direction method of multiplier (ADMM). We show empirically us- ing multiple datasets and with multiple sequential environments that the proposed methods outperforms or is competitive with prior approaches.", "one-sentence_summary": "We study the extension of Invariant Risk Minimization in sequential environments", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alesiani|continual_invariant_risk_minimization", "pdf": "/pdf/df1dd4b3bb815a98877cfa1f602aa565675dd445.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U3gQFheCfN", "_bibtex": "@misc{\nalesiani2021continual,\ntitle={Continual Invariant Risk Minimization},\nauthor={Francesco Alesiani and Shujian Yu and Mathias Niepert},\nyear={2021},\nurl={https://openreview.net/forum?id=CHTHamtufWN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "CHTHamtufWN", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3136/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3136/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3136/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3136/Authors|ICLR.cc/2021/Conference/Paper3136/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3136/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923840692, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3136/-/Official_Comment"}}}, {"id": "HkkguB4sWuO", "original": null, "number": 9, "cdate": 1606170357351, "ddate": null, "tcdate": 1606170357351, "tmdate": 1606170357351, "tddate": null, "forum": "CHTHamtufWN", "replyto": "ZeHHueYDMK", "invitation": "ICLR.cc/2021/Conference/Paper3136/-/Official_Comment", "content": {"title": "Thanks for running the sanity check", "comment": "Thanks for running the sanity check. While the results are not 50%, they are closer to 50% than 90% indicating that C-BVIRM, C-VIRMG, and C-VIRMv1 are indeed heavily regularizing learning. For instance, for pc=0.1 on train and test environment, ERM would learn to achieve ~ 90% accuracy on train and test environments very quickly. \n\nI'm still not sure why 50% accuracy on a binary classification task is meaningful at all as a randomly initialized network would achieve similar results. Can the authors provide any justification? \n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3136/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3136/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continual Invariant Risk Minimization", "authorids": ["~Francesco_Alesiani1", "~Shujian_Yu1", "~Mathias_Niepert1"], "authors": ["Francesco Alesiani", "Shujian Yu", "Mathias Niepert"], "keywords": ["Supervised Learning", "Causal Learning", "Invariant Risk Minimization", "Continual Learning"], "abstract": "Empirical risk minimization can lead to poor generalization behaviour on unseen environments if the learned model does not capture invariant feature represen- tations. Invariant risk minimization (IRM) is a recent proposal for discovering environment-invariant representations. It was introduced by Arjovsky et al. (2019) and extended by Ahuja et al. (2020). The assumption of IRM is that all environ- ments are available to the learning system at the same time. With this work, we generalize the concept of IRM to scenarios where environments are observed se- quentially. We show that existing approaches, including those designed for contin- ual learning, fail to identify the invariant features and models across sequentially presented environments. We extend IRM under a variational Bayesian and bilevel framework, creating a general approach to continual invariant risk minimization. We also describe a strategy to solve the optimization problems using a variant of the alternating direction method of multiplier (ADMM). We show empirically us- ing multiple datasets and with multiple sequential environments that the proposed methods outperforms or is competitive with prior approaches.", "one-sentence_summary": "We study the extension of Invariant Risk Minimization in sequential environments", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alesiani|continual_invariant_risk_minimization", "pdf": "/pdf/df1dd4b3bb815a98877cfa1f602aa565675dd445.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U3gQFheCfN", "_bibtex": "@misc{\nalesiani2021continual,\ntitle={Continual Invariant Risk Minimization},\nauthor={Francesco Alesiani and Shujian Yu and Mathias Niepert},\nyear={2021},\nurl={https://openreview.net/forum?id=CHTHamtufWN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "CHTHamtufWN", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3136/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3136/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3136/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3136/Authors|ICLR.cc/2021/Conference/Paper3136/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3136/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923840692, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3136/-/Official_Comment"}}}, {"id": "LbI_9J3oxOI", "original": null, "number": 7, "cdate": 1606088379698, "ddate": null, "tcdate": 1606088379698, "tmdate": 1606088379698, "tddate": null, "forum": "CHTHamtufWN", "replyto": "OhARvWKdaw", "invitation": "ICLR.cc/2021/Conference/Paper3136/-/Official_Comment", "content": {"title": "Thank you for your review - Reply to your Questions a, b, c and d", "comment": "We appreciate the comprehensive evaluation and valuable questions from reviewers. Please let us first address your questions a, b,c, d and e. \n\n[Qa] Information Projection (Theorem 3) In the work, the authors show in Theorem 3 as to how when the model is trained sequentially the support of the distribution shrinks. [...]\n\n[Ra] Thank you for your comment. Our description was not complete. Theorem 3 states a general property, while the sequential projection was not completely clarified and we also did not give a formal connection with the variational model. \nNow, we updated Figure 1, where the missing set has created inconsistency. We additionally provide in supplementary material A.6 how information projection is related to IRM objective, which suggests that it is the combination of IRM and KL divergence that ensures the shrink of the support. Lemma 8 provides the property of sequential information projection to formalize what the reviewer commented on.\n\n[Qb] Why penalize the IRM constraints as well? [...]\n\n[Rb] We added in the supplementary material Lemma 7 that addresses this point. The intuition is that the second equation is associated with the optimality condition on all environments on the classifier (predictor) parameter $w$ that for the variational case is in the distribution $q_w$. The loss function is the lower bound (ELBO) that contains both the error term and the divergence with respect to the prior distribution, which for $w$ is $p_w$, while the parameters of the function $\\phi$ are not used. In other words, the intuition of having a KL term in Eq.10b is that the predictor parameters\u2019 distribution  ($q_w$) shall be optimal in all environments, where the loss function includes the KL term. We also notice that a term in the Eq.10a was missing. Thank you for the question. \n\n[Qc] About the ADMM approach: I am happy to see the authors tried the ADMM approach. However, one thing that is unclear is it does not necessarily serve the purpose of solving the problem authors want. From the experiments it seems a continual extension of IRMv1 itself suffices. Is there any other reason why ADMM approach was used. Also, if ADMM approach was proposed as another way to solve standard IRM in the offline setting, then some experiments explaining any advantage would be useful. At least some offline comparisons would make a better case for using ADMM.\n\n[Rc] Thank you for the observation. ADMM provides a framework for solving BIRM and BVIRM. The advantage of ADMM is for solving in parallel multiple environments for the off-line training. More specifically Eq.11b requires synchronization among parallel environments, but Eq.11a,Eq.11c and Eq.11d can be computed in parallel. This is also now described in section 3.4.\nIn the online setting, the performances of the continual version of IRMv1 are indeed very close. However, our ADMM still provides an effective alternative and enriches our understanding of the optimization. In this sense, we feel that our ADMM also contributes a lot to the IRM and its extensions.\n\n[Qd] Why not empirically compare with Javed et al.? [...]\n\n[Rd] We did not compare with Javed since the setup is different. We would have needed to modify our approach to have a fair comparison. Further we impose a budget in the training, while the Javed approach is based on reinforcement learning and long training, that would be difficult to compare with our experimental set up. \n\n[Qe] Comparisons with offline IRM based methods (IRMv1 Arjovsky et al., IRMG Ahuja et al.) do not seem to have been carried out in a fair manner. [...]\n\n[Re] Thank you for the your question. We did not stop the training of IRMG and IRMv1. We trained all methods with the same set up. The offline IRMv1 and IRMG do not consider sequential setting, so we expected to overfit in the training. The IRMv1 and IRMG are presented with sequential environments and continuously training as new data is presented. Since they see only one environment at time it would not be possible to identify invariant models. We show this effect experimentally, but it is not a criticism of these methods. Rather, it is simply that the sequential environment breaks the assumption upon which these methods have been built. \nWe added in supplementary material the description on how IRMv1 and IRMG are trained. We do not report computational times, since these may vary with implementation, even if in general Variational methods require more computation because we learn a distribution of networks (versus a single network) and we use Monte Carlo sampling for training.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3136/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3136/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continual Invariant Risk Minimization", "authorids": ["~Francesco_Alesiani1", "~Shujian_Yu1", "~Mathias_Niepert1"], "authors": ["Francesco Alesiani", "Shujian Yu", "Mathias Niepert"], "keywords": ["Supervised Learning", "Causal Learning", "Invariant Risk Minimization", "Continual Learning"], "abstract": "Empirical risk minimization can lead to poor generalization behaviour on unseen environments if the learned model does not capture invariant feature represen- tations. Invariant risk minimization (IRM) is a recent proposal for discovering environment-invariant representations. It was introduced by Arjovsky et al. (2019) and extended by Ahuja et al. (2020). The assumption of IRM is that all environ- ments are available to the learning system at the same time. With this work, we generalize the concept of IRM to scenarios where environments are observed se- quentially. We show that existing approaches, including those designed for contin- ual learning, fail to identify the invariant features and models across sequentially presented environments. We extend IRM under a variational Bayesian and bilevel framework, creating a general approach to continual invariant risk minimization. We also describe a strategy to solve the optimization problems using a variant of the alternating direction method of multiplier (ADMM). We show empirically us- ing multiple datasets and with multiple sequential environments that the proposed methods outperforms or is competitive with prior approaches.", "one-sentence_summary": "We study the extension of Invariant Risk Minimization in sequential environments", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alesiani|continual_invariant_risk_minimization", "pdf": "/pdf/df1dd4b3bb815a98877cfa1f602aa565675dd445.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U3gQFheCfN", "_bibtex": "@misc{\nalesiani2021continual,\ntitle={Continual Invariant Risk Minimization},\nauthor={Francesco Alesiani and Shujian Yu and Mathias Niepert},\nyear={2021},\nurl={https://openreview.net/forum?id=CHTHamtufWN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "CHTHamtufWN", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3136/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3136/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3136/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3136/Authors|ICLR.cc/2021/Conference/Paper3136/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3136/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923840692, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3136/-/Official_Comment"}}}, {"id": "SxP-kymqRVQ", "original": null, "number": 6, "cdate": 1606088038926, "ddate": null, "tcdate": 1606088038926, "tmdate": 1606088038926, "tddate": null, "forum": "CHTHamtufWN", "replyto": "EfNvGph3hyX", "invitation": "ICLR.cc/2021/Conference/Paper3136/-/Official_Comment", "content": {"title": "Thank you for your review - Reply to your Concerns 2, 3 and 4", "comment": "We appreciate the positive comments and valuable questions from reviewers. Please let us firstly address your points 2, 3 and 4.\n\n[Q2] I do not think the contents in Definition 1&2 are just definitions. Instead, they should be treated as propositions, or lemma or so on, because they should be supported by proofs which seem not to be provided yet.\n\n[R2] Additional proofs are provided as suggested. In the supplementary material, we added Lemma  6 (Equivalence  of  Definition  1) to highlight the connection of Definition1 and the original IRM definition. For definition 2, this is the use of Eq.7 in Eq.8. We extend the sentence (blue color) after the Definition 2, to justify the definition. We also added Lemma 7(Definition 2) in the supplementary material to reinforce this link. Additionally, we realize that we missed one term in Equation (10a). \nWe hope that this additional content addresses your concern. \n\n[Q3]The section of Continual IRM by Approximate Bayesian Inference is not friendly with readers unfamiliar with ADMM, which makes that part hard to go through.\n\n[R3] Yes, there are various details to present and we found it hard to fit the space, we improved the presentation of Equations in section 3.4 and section 3.4.1. We also added a footnote to describe the part of the ADMM in the algorithm 1 and 2. \nIf the reviewer feels that we need to further improve the presentation, we could add in the supplementary material a further explanation. \n\n[Q4]No section index\n[R4] We added the numbers to the sections\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3136/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3136/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continual Invariant Risk Minimization", "authorids": ["~Francesco_Alesiani1", "~Shujian_Yu1", "~Mathias_Niepert1"], "authors": ["Francesco Alesiani", "Shujian Yu", "Mathias Niepert"], "keywords": ["Supervised Learning", "Causal Learning", "Invariant Risk Minimization", "Continual Learning"], "abstract": "Empirical risk minimization can lead to poor generalization behaviour on unseen environments if the learned model does not capture invariant feature represen- tations. Invariant risk minimization (IRM) is a recent proposal for discovering environment-invariant representations. It was introduced by Arjovsky et al. (2019) and extended by Ahuja et al. (2020). The assumption of IRM is that all environ- ments are available to the learning system at the same time. With this work, we generalize the concept of IRM to scenarios where environments are observed se- quentially. We show that existing approaches, including those designed for contin- ual learning, fail to identify the invariant features and models across sequentially presented environments. We extend IRM under a variational Bayesian and bilevel framework, creating a general approach to continual invariant risk minimization. We also describe a strategy to solve the optimization problems using a variant of the alternating direction method of multiplier (ADMM). We show empirically us- ing multiple datasets and with multiple sequential environments that the proposed methods outperforms or is competitive with prior approaches.", "one-sentence_summary": "We study the extension of Invariant Risk Minimization in sequential environments", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alesiani|continual_invariant_risk_minimization", "pdf": "/pdf/df1dd4b3bb815a98877cfa1f602aa565675dd445.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U3gQFheCfN", "_bibtex": "@misc{\nalesiani2021continual,\ntitle={Continual Invariant Risk Minimization},\nauthor={Francesco Alesiani and Shujian Yu and Mathias Niepert},\nyear={2021},\nurl={https://openreview.net/forum?id=CHTHamtufWN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "CHTHamtufWN", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3136/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3136/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3136/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3136/Authors|ICLR.cc/2021/Conference/Paper3136/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3136/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923840692, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3136/-/Official_Comment"}}}, {"id": "i0udmEtSTB", "original": null, "number": 5, "cdate": 1606087299172, "ddate": null, "tcdate": 1606087299172, "tmdate": 1606087299172, "tddate": null, "forum": "CHTHamtufWN", "replyto": "Ef8gE5-xPiW", "invitation": "ICLR.cc/2021/Conference/Paper3136/-/Official_Comment", "content": {"title": "Thank you for your review - Reply to your second point", "comment": "We appreciate the positive comments and valuable questions from reviewers. Please let us first clarify your second concern. \n\n[Q2] The ADMM formulation seems rather ad-hoc, and since the loss is not convex, it is unclear whether the scheme is numerically stable / convergent. The statement about convergence rates in the strongly convex / convex setting are a bit puzzling, as they do not apply to the problem at hand. What is the main advantage of the ADMM approach over, say, solving the bilevel problem using implicit differentiation?\n\n[R2] Thank you for your comment. Yes, we acknowledge that the use of ADMM is not unique, but it allows us to solve the problem at hand, even if we can not have a general guarantee of its convergence. \nOne of the Alternating Direction Method of Multipliers (ADMM) advantages is the ability to scale computationally, as for example in distributed computation. The use of direct bilevel optimization is also a possible direction [1] that we have not explored, which would not be able to scale if computation of environments is done simultaneously. The convergence guarantee is only applicable to local minima, there are no general global convergence results for the full loss function that is not convex, which also applies to SGD based methods. Both IRMv1 and ADMM are implicitly based on the relaxation of the constraint, but the advantage of ADMM is the guarantee of convergence, while in IRMv1 requires heuristics to force the second term to be zero, typically by hand increasing the value of $\\lambda$ in Eq.(4) to an arbitrary large value. Finally ADMM naturally solves the BIRM problem and can be extended to the variational case and thus also solving BVIRM. \nWe modified the submission in section 3.4 to account with the our comments. \n\n[1] Franceschi, Luca, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimilano Pontil. \"Bilevel programming for hyperparameter optimization and meta-learning.\" ICML (2018).\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3136/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3136/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continual Invariant Risk Minimization", "authorids": ["~Francesco_Alesiani1", "~Shujian_Yu1", "~Mathias_Niepert1"], "authors": ["Francesco Alesiani", "Shujian Yu", "Mathias Niepert"], "keywords": ["Supervised Learning", "Causal Learning", "Invariant Risk Minimization", "Continual Learning"], "abstract": "Empirical risk minimization can lead to poor generalization behaviour on unseen environments if the learned model does not capture invariant feature represen- tations. Invariant risk minimization (IRM) is a recent proposal for discovering environment-invariant representations. It was introduced by Arjovsky et al. (2019) and extended by Ahuja et al. (2020). The assumption of IRM is that all environ- ments are available to the learning system at the same time. With this work, we generalize the concept of IRM to scenarios where environments are observed se- quentially. We show that existing approaches, including those designed for contin- ual learning, fail to identify the invariant features and models across sequentially presented environments. We extend IRM under a variational Bayesian and bilevel framework, creating a general approach to continual invariant risk minimization. We also describe a strategy to solve the optimization problems using a variant of the alternating direction method of multiplier (ADMM). We show empirically us- ing multiple datasets and with multiple sequential environments that the proposed methods outperforms or is competitive with prior approaches.", "one-sentence_summary": "We study the extension of Invariant Risk Minimization in sequential environments", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alesiani|continual_invariant_risk_minimization", "pdf": "/pdf/df1dd4b3bb815a98877cfa1f602aa565675dd445.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U3gQFheCfN", "_bibtex": "@misc{\nalesiani2021continual,\ntitle={Continual Invariant Risk Minimization},\nauthor={Francesco Alesiani and Shujian Yu and Mathias Niepert},\nyear={2021},\nurl={https://openreview.net/forum?id=CHTHamtufWN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "CHTHamtufWN", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3136/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3136/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3136/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3136/Authors|ICLR.cc/2021/Conference/Paper3136/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3136/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923840692, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3136/-/Official_Comment"}}}, {"id": "ZeHHueYDMK", "original": null, "number": 4, "cdate": 1605727044695, "ddate": null, "tcdate": 1605727044695, "tmdate": 1605729077781, "tddate": null, "forum": "CHTHamtufWN", "replyto": "r5tbGSrYuSw", "invitation": "ICLR.cc/2021/Conference/Paper3136/-/Official_Comment", "content": {"title": "Thank you for your review - Sanity check", "comment": "Dear Reviewer,\n\nThank you for your helpful feedback. We executed the sanity check that you suggest, where test and training environments\u2019 image colours are drawn with probability pc=0.1. We considered n=2 environments for training and one for testing, where samples are MNIST images with colour correction b01. The rest of the experiment is described in the paper. As the following table (Table A) shows (averaged over 5 seeds), the learned models do not behave randomly.\n\n\nTable A\n\n| Method     | Train Accuracy   |   Test Accuracy |\n| -------------     |:-------------:    | :-------------:   |\n| C-BVIRM          | 64.6% (4.3%)      | 61.1% (4.9%)      |\n| C-VIRMG          | 69.7% (4.0%)      | 66.0% (2.3%)      |\n| C-VIRMv1         | 59.6% (3.4%)      | 59.2% (3.2%)     |\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3136/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3136/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continual Invariant Risk Minimization", "authorids": ["~Francesco_Alesiani1", "~Shujian_Yu1", "~Mathias_Niepert1"], "authors": ["Francesco Alesiani", "Shujian Yu", "Mathias Niepert"], "keywords": ["Supervised Learning", "Causal Learning", "Invariant Risk Minimization", "Continual Learning"], "abstract": "Empirical risk minimization can lead to poor generalization behaviour on unseen environments if the learned model does not capture invariant feature represen- tations. Invariant risk minimization (IRM) is a recent proposal for discovering environment-invariant representations. It was introduced by Arjovsky et al. (2019) and extended by Ahuja et al. (2020). The assumption of IRM is that all environ- ments are available to the learning system at the same time. With this work, we generalize the concept of IRM to scenarios where environments are observed se- quentially. We show that existing approaches, including those designed for contin- ual learning, fail to identify the invariant features and models across sequentially presented environments. We extend IRM under a variational Bayesian and bilevel framework, creating a general approach to continual invariant risk minimization. We also describe a strategy to solve the optimization problems using a variant of the alternating direction method of multiplier (ADMM). We show empirically us- ing multiple datasets and with multiple sequential environments that the proposed methods outperforms or is competitive with prior approaches.", "one-sentence_summary": "We study the extension of Invariant Risk Minimization in sequential environments", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alesiani|continual_invariant_risk_minimization", "pdf": "/pdf/df1dd4b3bb815a98877cfa1f602aa565675dd445.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U3gQFheCfN", "_bibtex": "@misc{\nalesiani2021continual,\ntitle={Continual Invariant Risk Minimization},\nauthor={Francesco Alesiani and Shujian Yu and Mathias Niepert},\nyear={2021},\nurl={https://openreview.net/forum?id=CHTHamtufWN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "CHTHamtufWN", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3136/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3136/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3136/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3136/Authors|ICLR.cc/2021/Conference/Paper3136/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3136/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923840692, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3136/-/Official_Comment"}}}, {"id": "OhARvWKdaw", "original": null, "number": 2, "cdate": 1603835926026, "ddate": null, "tcdate": 1603835926026, "tmdate": 1605024059737, "tddate": null, "forum": "CHTHamtufWN", "replyto": "CHTHamtufWN", "invitation": "ICLR.cc/2021/Conference/Paper3136/-/Official_Review", "content": {"title": "Paper proposes an extension of IRM for continual learning. The problem is interesting, the method proposed is natural and makes sense. Theoretical justifications are incomplete and a bit misleading. Experiments are below par and need a lot of work.", "review": "Summary:\nIn this work, the authors consider the problem of continual learning with distribution shifts.  The work extends the recent work invariant risk minimization (IRM) from Arjovsky et al. to a continual learning setup. IRM was designed as an offline learning framework. In this work, the authors consider the setting where the different domains arrive sequentially. The authors propose a Bayesian extension of the IRM framework that allows sequential updates as the environments arrive. They provide a justification for how the KL divergence term helps in shrinking the support continually to arrive at an approximately invariant support. Several experiments were carried out on colored MNIST and its variants to show how the proposed scheme is better than existing continual learning methods and existing IRM frameworks. \n\nPros: \nI like the problem the authors consider. It is indeed important to understand how to extend IRM type frameworks in settings such as continual learning. I also liked the ADMM based approach taken by the authors as this approach gives another way of learning IRM based predictors in addition to existing approaches. \n\nCons:\n\nThere are several problems with the paper. I discuss these problems below in different subsections. If these major issues are addressed in the rebuttal, I would be open to changing my score. \n\na) Information Projection (Theorem 3)\nIn the work, the authors show in Theorem 3 as to how when the model is trained sequentially the support of the distribution shrinks. \n\nLet us consider optimization in 2 time steps\n\nt=1, q_{t} = min_{q_{t}\\in P} KL (q_t||q_{t-1}) --> support of q_1 = P \\cap support of q_0 \n\nt=2, q_{t} = min_{q_{t}\\in P} KL (q_t||q_{t-1}) --> support of q_2 = P \\cap support of q_1 = P \\cap P \\cap support of q_0 = P \\cap support of q_0 \n\nIt is not clear from just the KL term why support would strictly shrink. The reason why the authors approach works is because the first term based on the IRM loss allows to update and shrink the support and the KL divergence term ensures that there is no need to expand the existing support. Define the first term in the loss associated with IRMv1 as Q_{IRMV1}(q_t)\n\nt=1, q_{t} = min_{q_{t}\\in P} Q_{IRMv1}(q_t) + KL (q_t||q_{t-1}) --> support of q_1 \\subset P \\cap support of q_0 \n\nt=2, q_{t} = min_{q_{t}\\in P} Q_{IRMv1}(q_t)  + KL (q_t||q_{t-1}) --> support of q_2 \\subset P \\cap support of q_1 \n\nIt is the combination of the IRM term with KL term that ensures shrinkage and KL term on its own only ensures support does not expand. A proof or an illustration of what I state would have been nice as the current intuition from the authors is incomplete and to some extent a bit misleading.\n\nb) Why penalize the IRM constraints as well?  \nThe authors arrived at a variational formulation in 9a and 9b with new objective functions defined in 10 a and 10b. It is unclear why it was chosen to impose KL penalty in the constraint (10b) as well. If not a theoretical justification, an experiment in the supplement illustrating why these choices were made would be nice. At least some good intuition needs to be provided. \n\nc) About the ADMM approach: \nI am happy to see the authors tried the ADMM approach. However, one thing that is unclear is it does not necessarily serve the purpose of solving the problem authors want. From the experiments it seems a continual extension of IRMv1 itself suffices. Is there any other reason why ADMM approach was used. Also, if ADMM approach was proposed as another way to solve standard                                                                                                               IRM in the offline setting, then some experiments explaining any advantage would be useful. At least some offline comparisons would make a better case for using ADMM. \n\nd) Why not empirically compare with Javed et al.?\n\nAs the authors correctly identify there is a recent work from Javed et al., which solves the same problem but makes more assumptions about the data.  I understand that the work uses one hot encoding of color etc. Despite that you can allow your model to also have access to the same data format and then see if your model has any more advantage to offer over and above what was done in Javed et al.\n\ne) Comparisons with offline IRM based methods (IRMv1 Arjovsky et al., IRMG Ahuja et al.) do not seem to have been carried out in a fair manner.\nThe authors have not provided a very clear description of how IRM based methods were trained. It seems to me that the authors are running offline IRM based methods in the following manner.  They provide the data from the first environment and then keep the offline IRM based models fixed and not update them when the data from the next environment arrives. If they indeed update the offline IRM based methods, then there is no reason why the methods perform so poorly.  What was the reason for not updating these models? The reason I am suspecting is that everytime retraining the entire model on the entire data is computationally expensive. If that is the concern, then authors should have shown a comparison of run-times. The authors should have run experiments allowing all methods the same run-times and then shown that IRM based methods are still not able to perform better. \nFor instance if the time to train continual IRM based model proposed by authors for the two environments is a total of 20 seconds. \nSuppose the total time to train a standard IRMv1 is say a total of 15 seconds for the first environment. There is still a five second window for the IRMv1 model to be updated.  If run-time comparisons are non-trivial, then at least there should be a budget on the total number of gradient computations that is fixed across methods. \n\n\nf) Compare with Creager et al. \n\nThe reason IRM based offline methods did not perform well is because they were given data from one environment only.  Recently, in Creager et al., it was shown how one can learn to split the data to create new environments and then train an IRM on those environments.  The authors should compare with the Creager et al. as follows. When the data from first environment comes in you can use the approach introduced in the paper http://www.gatsby.ucl.ac.uk/~balaji/udl2020/accepted-papers/UDL2020-paper-045.pdf to learn environments (code available at https://github.com/ecreager/eiil). Therefore, by doing this you can split the data in the first environment into smaller environments and use IRM. This would lead to a lot of performance improvement for the offline IRM based methods.\n\n\ng) Current experiments do not reflect the true potential of the method proposed by the authors: \n\nThe current experiments do not do a good job of reflecting a continual learning setup. Simple modifications on existing IRM based methods can outperform the method proposed by the authors. However, I believe the method proposed by the authors is nice and much more experimentation is needed. I make some suggestions on how to improve the experiments. \n\nIt seems that in the current experiments after 2 environments there is no more gain from using any method, in fact more environments seem to hurt. I would encourage the authors to do an experiment where adding more environments actually removes spurious correlations more and more and helps.  To this end, there are three suggestions I would like to make:\n\ni) First let us understand is why is two environment sufficing in colored MNIST dataset.  The reason is that the dimension of the spurious factor, i.e., color is small (only two colors). You can increase the number of colors, then more environments will be needed to decorrelate. \n\nii) If  adding colors and doing comparisons using raw images is hard, then you can try using the one hot encoded colored MNIST from Javed et al. and add more colors to it. \n\niii) Another suggestion would be to try the regression experiments from Arjovsky. In Arjovsky, the authors use two environments only. However, the two environments differ a lot 0.2 and 2.0 variance. If you introduce a sequence of environments from 0.2 to 2.0, say 0.2, 0.4, 0.6, ...2.0., then it is likely that more environments will have some benefit.  \n\nQuality: The solution proposed seems promising. There are problems with theoretical justification. Experiments need a lot of work as I stated above.  \n\nSignificance: The problem considered in the paper is interesting. The experiments carried out do not do a justice to the problem being considered. \n\nClarity: The writing of the paper can be improved. The experiments need to be described much more clearly and use the space in the supplement to give all the details. Also, the steps of ADMM shoould have been better explained.  \n\n\n\n\nReferences\n\nJaved et al. \"Learning causal models online\"\n\nCreager et al. \"Environment Inference for Invariant Learning\"\n\n\n \n\n\n\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3136/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3136/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continual Invariant Risk Minimization", "authorids": ["~Francesco_Alesiani1", "~Shujian_Yu1", "~Mathias_Niepert1"], "authors": ["Francesco Alesiani", "Shujian Yu", "Mathias Niepert"], "keywords": ["Supervised Learning", "Causal Learning", "Invariant Risk Minimization", "Continual Learning"], "abstract": "Empirical risk minimization can lead to poor generalization behaviour on unseen environments if the learned model does not capture invariant feature represen- tations. Invariant risk minimization (IRM) is a recent proposal for discovering environment-invariant representations. It was introduced by Arjovsky et al. (2019) and extended by Ahuja et al. (2020). The assumption of IRM is that all environ- ments are available to the learning system at the same time. With this work, we generalize the concept of IRM to scenarios where environments are observed se- quentially. We show that existing approaches, including those designed for contin- ual learning, fail to identify the invariant features and models across sequentially presented environments. We extend IRM under a variational Bayesian and bilevel framework, creating a general approach to continual invariant risk minimization. We also describe a strategy to solve the optimization problems using a variant of the alternating direction method of multiplier (ADMM). We show empirically us- ing multiple datasets and with multiple sequential environments that the proposed methods outperforms or is competitive with prior approaches.", "one-sentence_summary": "We study the extension of Invariant Risk Minimization in sequential environments", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alesiani|continual_invariant_risk_minimization", "pdf": "/pdf/df1dd4b3bb815a98877cfa1f602aa565675dd445.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U3gQFheCfN", "_bibtex": "@misc{\nalesiani2021continual,\ntitle={Continual Invariant Risk Minimization},\nauthor={Francesco Alesiani and Shujian Yu and Mathias Niepert},\nyear={2021},\nurl={https://openreview.net/forum?id=CHTHamtufWN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "CHTHamtufWN", "replyto": "CHTHamtufWN", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3136/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538081448, "tmdate": 1606915766736, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3136/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3136/-/Official_Review"}}}, {"id": "EfNvGph3hyX", "original": null, "number": 3, "cdate": 1603978808131, "ddate": null, "tcdate": 1603978808131, "tmdate": 1605024059669, "tddate": null, "forum": "CHTHamtufWN", "replyto": "CHTHamtufWN", "invitation": "ICLR.cc/2021/Conference/Paper3136/-/Official_Review", "content": {"title": "An interesting extension of IRM to the continual learning setting", "review": "This paper extends the idea of invariant risk minimization (IRM) initially introduced by Arjovsky et al. (2019) to the setting of continual learning in which environments are observed sequentially rather than concurrently. This extension is implemented under a variational Bayesian and bilevel framework and the optimization is solved using a variant of the alternating direction method of multiplier (ADMM). The authors demonstrate the superiority of the proposed methods on variants of Colored MNIST. \n\nPros:\n+ The proposed method is a natural and practical extension of IRM and IRMG, which I believe is more applicable to many real-world scenarios. \n+ The resulting bilevel problem can be efficiently addressed using the alternating direction method of multipliers.\n\nConcerns:\n- My main concern is about the generalization theory in this continual setting. In both IRM and IRMG, the authors provide the conditions under which the OOD generalization can be guaranteed, as stated in Assumption 8 and Theorem 9 of Arjovsky et al. (2019) and in Assumption 2 and Theorem 2 of Ahuja et al. (2020). I did not see any similar theoretical guarantees in this paper. Without them, it is hard to judge whether or not  the proposed method really generalizes out-of-distribution in a sequential manner. \n- I do not think the contents in Definition 1&2 are just definitions. Instead, they should be treated as propositions, or lemma or so on, because they should be supported by proofs which seem not to be provided yet. \n- The section of Continual IRM by Approximate Bayesian Inference is not friendly with readers unfamiliar with ADMM, which makes that part hard to go through. \n- No section index\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3136/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3136/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continual Invariant Risk Minimization", "authorids": ["~Francesco_Alesiani1", "~Shujian_Yu1", "~Mathias_Niepert1"], "authors": ["Francesco Alesiani", "Shujian Yu", "Mathias Niepert"], "keywords": ["Supervised Learning", "Causal Learning", "Invariant Risk Minimization", "Continual Learning"], "abstract": "Empirical risk minimization can lead to poor generalization behaviour on unseen environments if the learned model does not capture invariant feature represen- tations. Invariant risk minimization (IRM) is a recent proposal for discovering environment-invariant representations. It was introduced by Arjovsky et al. (2019) and extended by Ahuja et al. (2020). The assumption of IRM is that all environ- ments are available to the learning system at the same time. With this work, we generalize the concept of IRM to scenarios where environments are observed se- quentially. We show that existing approaches, including those designed for contin- ual learning, fail to identify the invariant features and models across sequentially presented environments. We extend IRM under a variational Bayesian and bilevel framework, creating a general approach to continual invariant risk minimization. We also describe a strategy to solve the optimization problems using a variant of the alternating direction method of multiplier (ADMM). We show empirically us- ing multiple datasets and with multiple sequential environments that the proposed methods outperforms or is competitive with prior approaches.", "one-sentence_summary": "We study the extension of Invariant Risk Minimization in sequential environments", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alesiani|continual_invariant_risk_minimization", "pdf": "/pdf/df1dd4b3bb815a98877cfa1f602aa565675dd445.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U3gQFheCfN", "_bibtex": "@misc{\nalesiani2021continual,\ntitle={Continual Invariant Risk Minimization},\nauthor={Francesco Alesiani and Shujian Yu and Mathias Niepert},\nyear={2021},\nurl={https://openreview.net/forum?id=CHTHamtufWN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "CHTHamtufWN", "replyto": "CHTHamtufWN", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3136/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538081448, "tmdate": 1606915766736, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3136/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3136/-/Official_Review"}}}, {"id": "Ef8gE5-xPiW", "original": null, "number": 4, "cdate": 1603980217328, "ddate": null, "tcdate": 1603980217328, "tmdate": 1605024059605, "tddate": null, "forum": "CHTHamtufWN", "replyto": "CHTHamtufWN", "invitation": "ICLR.cc/2021/Conference/Paper3136/-/Official_Review", "content": {"title": "Interesting paper, needs more polishing", "review": "## Summary\nThe paper proposes a generalization of the invariant risk minimization objective to the continual learning setting, where environments are observed sequentially. An ADMM strategy for the solution of the resulting bilevel problem is proposed. In extensive experiments on smaller MNIST-like datasets, the method is shown to perform favorable to recent approaches for continual learning.\n\n## Explanation of Rating\nThe main strength of the paper is that it attempts to tackle an important and open problem using a reasonably principled approach. The work is well-written, and the methods performs well in practice and is evaluated against a large set of competitors. I lean towards accepting this submission. The weaknesses of the paper are the scalability of the approach (see comment #1) and the lack of theoretical guarantees for the ad hoc ADMM scheme (see comment #2). \n\n## Detailed Comments \n1. An evaluation of the method for larger architectures (e.g. a convolutional network) would make the approach more convincing. At the moment, I get the impression that there are issues with the scalability of the approach to larger data sets and models. \n2. The ADMM formulation seems rather ad-hoc, and since the loss is not convex, it is unclear whether the scheme is numerically stable / convergent. The statement about convergence rates in the strongly convex / convex setting are a bit puzzling, as they do not apply to the problem at hand. What is the main advantage of the ADMM approach over, say, solving the bilevel problem using implicit differentiation? \n\n\n\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper3136/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3136/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continual Invariant Risk Minimization", "authorids": ["~Francesco_Alesiani1", "~Shujian_Yu1", "~Mathias_Niepert1"], "authors": ["Francesco Alesiani", "Shujian Yu", "Mathias Niepert"], "keywords": ["Supervised Learning", "Causal Learning", "Invariant Risk Minimization", "Continual Learning"], "abstract": "Empirical risk minimization can lead to poor generalization behaviour on unseen environments if the learned model does not capture invariant feature represen- tations. Invariant risk minimization (IRM) is a recent proposal for discovering environment-invariant representations. It was introduced by Arjovsky et al. (2019) and extended by Ahuja et al. (2020). The assumption of IRM is that all environ- ments are available to the learning system at the same time. With this work, we generalize the concept of IRM to scenarios where environments are observed se- quentially. We show that existing approaches, including those designed for contin- ual learning, fail to identify the invariant features and models across sequentially presented environments. We extend IRM under a variational Bayesian and bilevel framework, creating a general approach to continual invariant risk minimization. We also describe a strategy to solve the optimization problems using a variant of the alternating direction method of multiplier (ADMM). We show empirically us- ing multiple datasets and with multiple sequential environments that the proposed methods outperforms or is competitive with prior approaches.", "one-sentence_summary": "We study the extension of Invariant Risk Minimization in sequential environments", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alesiani|continual_invariant_risk_minimization", "pdf": "/pdf/df1dd4b3bb815a98877cfa1f602aa565675dd445.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U3gQFheCfN", "_bibtex": "@misc{\nalesiani2021continual,\ntitle={Continual Invariant Risk Minimization},\nauthor={Francesco Alesiani and Shujian Yu and Mathias Niepert},\nyear={2021},\nurl={https://openreview.net/forum?id=CHTHamtufWN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "CHTHamtufWN", "replyto": "CHTHamtufWN", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3136/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538081448, "tmdate": 1606915766736, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3136/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3136/-/Official_Review"}}}], "count": 19}