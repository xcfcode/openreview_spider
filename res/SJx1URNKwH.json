{"notes": [{"id": "SJx1URNKwH", "original": "ryx5i68dPS", "number": 1124, "cdate": 1569439303056, "ddate": null, "tcdate": 1569439303056, "tmdate": 1583912054726, "tddate": null, "forum": "SJx1URNKwH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "MetaPix: Few-Shot Video Retargeting", "authors": ["Jessica Lee", "Deva Ramanan", "Rohit Girdhar"], "authorids": ["jl5@cs.cmu.edu", "deva@cs.cmu.edu", "rgirdhar@cs.cmu.edu"], "keywords": ["Meta-learning", "Few-shot Learning", "Generative Adversarial Networks", "Video Retargeting"], "TL;DR": "Video retargeting typically requires large amount of target data to be effective, which may not always be available; we propose a metalearning approach that improves over popular baselines while producing temporally coherent frames.", "abstract": "We address the task of unsupervised retargeting of human actions from one video to another. We consider the challenging setting where only a few frames of the target is available. The core of our approach is a conditional generative model that can transcode input skeletal poses (automatically extracted with an off-the-shelf pose estimator) to output target frames. However, it is challenging to build a universal transcoder because humans can appear wildly different due to clothing and background scene geometry. Instead, we learn to adapt \u2013 or personalize \u2013 a universal generator to the particular human and background in the target. To do so, we make use of meta-learning to discover effective strategies for on-the-fly personalization. One significant benefit of meta-learning is that the personalized transcoder naturally enforces temporal coherence across its generated frames; all frames contain consistent clothing and background geometry of the target. We experiment on in-the-wild internet videos and images and show our approach improves over widely-used baselines for the task.\n", "pdf": "/pdf/a63a19c5457caf7a6647d054cd03a139124fb7b4.pdf", "paperhash": "lee|metapix_fewshot_video_retargeting", "_bibtex": "@inproceedings{\nLee2020MetaPix:,\ntitle={MetaPix: Few-Shot Video Retargeting},\nauthor={Jessica Lee and Deva Ramanan and Rohit Girdhar},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJx1URNKwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/860134d07533593abcbb74b5e4d3b3389484db1e.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "aPvShV3HR", "original": null, "number": 1, "cdate": 1576798715123, "ddate": null, "tcdate": 1576798715123, "tmdate": 1576800921414, "tddate": null, "forum": "SJx1URNKwH", "replyto": "SJx1URNKwH", "invitation": "ICLR.cc/2020/Conference/Paper1124/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "Three reviewers have assessed this paper and they have scored it 6/6/6 after rebuttal. Nonetheless, the reviewers have raised a number of criticisms and the authors are encouraged to resolve them for the camera-ready submission.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MetaPix: Few-Shot Video Retargeting", "authors": ["Jessica Lee", "Deva Ramanan", "Rohit Girdhar"], "authorids": ["jl5@cs.cmu.edu", "deva@cs.cmu.edu", "rgirdhar@cs.cmu.edu"], "keywords": ["Meta-learning", "Few-shot Learning", "Generative Adversarial Networks", "Video Retargeting"], "TL;DR": "Video retargeting typically requires large amount of target data to be effective, which may not always be available; we propose a metalearning approach that improves over popular baselines while producing temporally coherent frames.", "abstract": "We address the task of unsupervised retargeting of human actions from one video to another. We consider the challenging setting where only a few frames of the target is available. The core of our approach is a conditional generative model that can transcode input skeletal poses (automatically extracted with an off-the-shelf pose estimator) to output target frames. However, it is challenging to build a universal transcoder because humans can appear wildly different due to clothing and background scene geometry. Instead, we learn to adapt \u2013 or personalize \u2013 a universal generator to the particular human and background in the target. To do so, we make use of meta-learning to discover effective strategies for on-the-fly personalization. One significant benefit of meta-learning is that the personalized transcoder naturally enforces temporal coherence across its generated frames; all frames contain consistent clothing and background geometry of the target. We experiment on in-the-wild internet videos and images and show our approach improves over widely-used baselines for the task.\n", "pdf": "/pdf/a63a19c5457caf7a6647d054cd03a139124fb7b4.pdf", "paperhash": "lee|metapix_fewshot_video_retargeting", "_bibtex": "@inproceedings{\nLee2020MetaPix:,\ntitle={MetaPix: Few-Shot Video Retargeting},\nauthor={Jessica Lee and Deva Ramanan and Rohit Girdhar},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJx1URNKwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/860134d07533593abcbb74b5e4d3b3389484db1e.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SJx1URNKwH", "replyto": "SJx1URNKwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795720632, "tmdate": 1576800271501, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1124/-/Decision"}}}, {"id": "HyePk2nK5r", "original": null, "number": 3, "cdate": 1572617183130, "ddate": null, "tcdate": 1572617183130, "tmdate": 1574427573080, "tddate": null, "forum": "SJx1URNKwH", "replyto": "SJx1URNKwH", "invitation": "ICLR.cc/2020/Conference/Paper1124/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #4", "review": "This paper proposes a novel and interesting task that learn to retarget human actions with few-shot samples. The overall pipeline is built by applying  meta-learning strategy on pre-trained retargeting module. It follows a conditional generator and discriminator structure that leverages few-shot frames to retarget the action of source video. The approach is technically sound.\n\nThe evaluations are compared with two baseline methods,  Pix2PixHD and Posewarp. Their evaluations are satisfactory and convincing, the results demonstrate some improvements over baseline models regarding both the selected metrics and the visualizing performance.\n\nThough the proposed problem is novel and somewhat interesting, there are also several weaknesses of this work:\n- The novelty of methodology is somewhat limited. It is more about merging several state-of-the-art modules in different tasks to tackle the few-shot retargeting problem. Though efforts may be needed to make the pipeline work, the overall contribution is not significant. \n- The improvement obtained with proposed MetaPix module is not significant in few-shot setting according to Table 1. Additionally, could the authors provide some visualizing results for different K number, which would be interesting for analysis.\n\n\n\n\n\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper1124/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1124/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MetaPix: Few-Shot Video Retargeting", "authors": ["Jessica Lee", "Deva Ramanan", "Rohit Girdhar"], "authorids": ["jl5@cs.cmu.edu", "deva@cs.cmu.edu", "rgirdhar@cs.cmu.edu"], "keywords": ["Meta-learning", "Few-shot Learning", "Generative Adversarial Networks", "Video Retargeting"], "TL;DR": "Video retargeting typically requires large amount of target data to be effective, which may not always be available; we propose a metalearning approach that improves over popular baselines while producing temporally coherent frames.", "abstract": "We address the task of unsupervised retargeting of human actions from one video to another. We consider the challenging setting where only a few frames of the target is available. The core of our approach is a conditional generative model that can transcode input skeletal poses (automatically extracted with an off-the-shelf pose estimator) to output target frames. However, it is challenging to build a universal transcoder because humans can appear wildly different due to clothing and background scene geometry. Instead, we learn to adapt \u2013 or personalize \u2013 a universal generator to the particular human and background in the target. To do so, we make use of meta-learning to discover effective strategies for on-the-fly personalization. One significant benefit of meta-learning is that the personalized transcoder naturally enforces temporal coherence across its generated frames; all frames contain consistent clothing and background geometry of the target. We experiment on in-the-wild internet videos and images and show our approach improves over widely-used baselines for the task.\n", "pdf": "/pdf/a63a19c5457caf7a6647d054cd03a139124fb7b4.pdf", "paperhash": "lee|metapix_fewshot_video_retargeting", "_bibtex": "@inproceedings{\nLee2020MetaPix:,\ntitle={MetaPix: Few-Shot Video Retargeting},\nauthor={Jessica Lee and Deva Ramanan and Rohit Girdhar},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJx1URNKwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/860134d07533593abcbb74b5e4d3b3389484db1e.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJx1URNKwH", "replyto": "SJx1URNKwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1124/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1124/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575737321531, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1124/Reviewers"], "noninvitees": [], "tcdate": 1570237742019, "tmdate": 1575737321544, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1124/-/Official_Review"}}}, {"id": "Hkloac0Djr", "original": null, "number": 4, "cdate": 1573542595087, "ddate": null, "tcdate": 1573542595087, "tmdate": 1573543022192, "tddate": null, "forum": "SJx1URNKwH", "replyto": "BkeVb25Otr", "invitation": "ICLR.cc/2020/Conference/Paper1124/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "We thank the reviewer for their feedback. We are glad that they find our problem setting and motivations in the paper interesting. With regards to our contribution, we think that our analysis on the application is insightful with regards to gaining an empirical understanding of Reptile. Leveraging the fact that we are learning an initialization of a generative model, we can \u201cprobe\u201d the knowledge it has learned by generating images from its initialized state without any finetuning. Figure 9 shows meta-training learns to factor appearance from pose, ensuring that different pose inputs will generate consistent backgrounds and clothing cues. Then, we investigate the dynamics of specialization given K frames in Figure 7, which shows how the learned factored representation translates to more temporally coherent outputs. We refer the reviewer to our response to Reviewer 4 above where we further detail our contributions and link the video versions of Figure 7 and 9."}, "signatures": ["ICLR.cc/2020/Conference/Paper1124/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1124/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MetaPix: Few-Shot Video Retargeting", "authors": ["Jessica Lee", "Deva Ramanan", "Rohit Girdhar"], "authorids": ["jl5@cs.cmu.edu", "deva@cs.cmu.edu", "rgirdhar@cs.cmu.edu"], "keywords": ["Meta-learning", "Few-shot Learning", "Generative Adversarial Networks", "Video Retargeting"], "TL;DR": "Video retargeting typically requires large amount of target data to be effective, which may not always be available; we propose a metalearning approach that improves over popular baselines while producing temporally coherent frames.", "abstract": "We address the task of unsupervised retargeting of human actions from one video to another. We consider the challenging setting where only a few frames of the target is available. The core of our approach is a conditional generative model that can transcode input skeletal poses (automatically extracted with an off-the-shelf pose estimator) to output target frames. However, it is challenging to build a universal transcoder because humans can appear wildly different due to clothing and background scene geometry. Instead, we learn to adapt \u2013 or personalize \u2013 a universal generator to the particular human and background in the target. To do so, we make use of meta-learning to discover effective strategies for on-the-fly personalization. One significant benefit of meta-learning is that the personalized transcoder naturally enforces temporal coherence across its generated frames; all frames contain consistent clothing and background geometry of the target. We experiment on in-the-wild internet videos and images and show our approach improves over widely-used baselines for the task.\n", "pdf": "/pdf/a63a19c5457caf7a6647d054cd03a139124fb7b4.pdf", "paperhash": "lee|metapix_fewshot_video_retargeting", "_bibtex": "@inproceedings{\nLee2020MetaPix:,\ntitle={MetaPix: Few-Shot Video Retargeting},\nauthor={Jessica Lee and Deva Ramanan and Rohit Girdhar},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJx1URNKwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/860134d07533593abcbb74b5e4d3b3389484db1e.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJx1URNKwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1124/Authors", "ICLR.cc/2020/Conference/Paper1124/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1124/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1124/Reviewers", "ICLR.cc/2020/Conference/Paper1124/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1124/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1124/Authors|ICLR.cc/2020/Conference/Paper1124/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160873, "tmdate": 1576860559538, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1124/Authors", "ICLR.cc/2020/Conference/Paper1124/Reviewers", "ICLR.cc/2020/Conference/Paper1124/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1124/-/Official_Comment"}}}, {"id": "r1lyB9CPor", "original": null, "number": 3, "cdate": 1573542455053, "ddate": null, "tcdate": 1573542455053, "tmdate": 1573543008131, "tddate": null, "forum": "SJx1URNKwH", "replyto": "ByezyYV9Fr", "invitation": "ICLR.cc/2020/Conference/Paper1124/-/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "We thank the reviewer for their feedback. With regards to the originality of our method, we believe that our analysis on Reptile is insightful and novel, especially the visualization of the initialization learned. Leveraging the fact that we are meta-optimizing a generative model, we are able to \u201cprobe\u201d its knowledge simply by generating images from its initialized state without any fine-tuning. Figure 9 shows that meta-learning learns to factor appearance and pose, ensuring that different pose inputs will generate consistent backgrounds and clothing cues. Additionally, Figure 7 shows the dynamics of specialization, and shows meta-training leads to temporally coherent outputs. We also refer the reviewer to our response to Reviewer 4 above where we further detail our contributions and link the video versions of Figure 7 and 9."}, "signatures": ["ICLR.cc/2020/Conference/Paper1124/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1124/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MetaPix: Few-Shot Video Retargeting", "authors": ["Jessica Lee", "Deva Ramanan", "Rohit Girdhar"], "authorids": ["jl5@cs.cmu.edu", "deva@cs.cmu.edu", "rgirdhar@cs.cmu.edu"], "keywords": ["Meta-learning", "Few-shot Learning", "Generative Adversarial Networks", "Video Retargeting"], "TL;DR": "Video retargeting typically requires large amount of target data to be effective, which may not always be available; we propose a metalearning approach that improves over popular baselines while producing temporally coherent frames.", "abstract": "We address the task of unsupervised retargeting of human actions from one video to another. We consider the challenging setting where only a few frames of the target is available. The core of our approach is a conditional generative model that can transcode input skeletal poses (automatically extracted with an off-the-shelf pose estimator) to output target frames. However, it is challenging to build a universal transcoder because humans can appear wildly different due to clothing and background scene geometry. Instead, we learn to adapt \u2013 or personalize \u2013 a universal generator to the particular human and background in the target. To do so, we make use of meta-learning to discover effective strategies for on-the-fly personalization. One significant benefit of meta-learning is that the personalized transcoder naturally enforces temporal coherence across its generated frames; all frames contain consistent clothing and background geometry of the target. We experiment on in-the-wild internet videos and images and show our approach improves over widely-used baselines for the task.\n", "pdf": "/pdf/a63a19c5457caf7a6647d054cd03a139124fb7b4.pdf", "paperhash": "lee|metapix_fewshot_video_retargeting", "_bibtex": "@inproceedings{\nLee2020MetaPix:,\ntitle={MetaPix: Few-Shot Video Retargeting},\nauthor={Jessica Lee and Deva Ramanan and Rohit Girdhar},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJx1URNKwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/860134d07533593abcbb74b5e4d3b3389484db1e.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJx1URNKwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1124/Authors", "ICLR.cc/2020/Conference/Paper1124/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1124/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1124/Reviewers", "ICLR.cc/2020/Conference/Paper1124/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1124/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1124/Authors|ICLR.cc/2020/Conference/Paper1124/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160873, "tmdate": 1576860559538, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1124/Authors", "ICLR.cc/2020/Conference/Paper1124/Reviewers", "ICLR.cc/2020/Conference/Paper1124/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1124/-/Official_Comment"}}}, {"id": "HkedzcCPjS", "original": null, "number": 2, "cdate": 1573542415760, "ddate": null, "tcdate": 1573542415760, "tmdate": 1573542950686, "tddate": null, "forum": "SJx1URNKwH", "replyto": "HyePk2nK5r", "invitation": "ICLR.cc/2020/Conference/Paper1124/-/Official_Comment", "content": {"title": "Response to Reviewer #4", "comment": "We thank the reviewer for their feedback. We are glad they find our problem setup novel and interesting, and experiments satisfactory and convincing. We address their concerns regarding novelty and significance of the approach here. \n\n- Novelty: While we agree that some of the tools we use (GANs, Reptile etc) have previously been studied, we believe the specific combination used for the novel problem setup we introduce is non-trivially different from any previous work. As Reviewer 1 also observed, we both empirically validate that existing meta-learning methods can work in this harder task setting, as well as investigate the qualities of the meta-learned initialization. \n\nSpecifically, our paper provides more insight into what knowledge meta-learning can potentially capture. We are able to do so because we meta-learn a generative model, allowing us to \u201cprobe\u201d its knowledge simply by generating images from its initialized state without any fine-tuning (akin to a zero-shot model). Figure 9, located in the Appendix, shows that meta-learning learns to factor appearance and pose, ensuring that different pose inputs will generate consistent backgrounds and clothing cues. On the other hand, simple pre-training memorizes specific combinations of pose and appearance present in the training set (first column in Figure 9). Specifically, our results illustrate that meta-learning discovers that factored representations of appearance and pose are easier to personalize for a target appearance. Moreover, we use the same image-generation technique to visualize the dynamics of what is being learned during iterations of fine-tuning. Our visualizations demonstrate that meta-learning for longer T ensures that fine-tuning is more stable, resulting in more temporally coherent outputs (Figure 7) with consistent backgrounds and clothing appearance regardless of the pose. Such temporal consistency is crucial for accurate video retargeting. Additionally, Figure 7 also compares the initial parameters learned by metalearning as opposed to pre-training, and we observe the metalearned features are more \u201cgeneric\u201d (eg, grey backgrounds), making them amenable to specialization. In summary, we argue that meta-learning of generative models opens up novel avenues for visualizations and (meta) model interpretation. We have also included video versions of Figure 7 (https://youtu.be/bxJJXCK4IoQ ) and Figure 9 (https://youtu.be/zFoT8VcbwsU ) to showcase the temporal stability of the metalearned initialization.\n\nFinally, our work also standardizes the problem of few-shot video generation with an in-the-wild realistic benchmark and strong baselines (code for all of which will be released). We believe this is a timely and interesting problem (as all reviewers agree), and our work would be critical in spurring further research in this area.\n\n- Significance: In the Pix2PixHD case, we see around a 4% improvement in SSIM, and 18% decrease in SSIM. Looking at the Figure 4 in the paper, we observe that the results are much closer to convergence, and that the background color is much closer to the ground truth than the initial feedback, meaning that we are closer to optimal solution beforehand. Therefore, our method both quantitatively and qualitatively improves upon the baselines.  We have additional video version of Figure 4 available to further display the qualitative improvement (https://youtu.be/NlUmsd9aU-4 ). \n\n- Additional visualizations: We have updated the Appendix of the paper to include some figures of meta-learned models in the setting K=3, K=5, K=10, and K = 20 for a fixed number of steps T=20 (Figure 8). We observe that the model converges in fewer epochs given more data, which is to be expected, since Reptile works best with a fewer samples in comparison to a pretrained initialization. Note that meta-learned initialization generates a qualitatively closer output to the ground truth, including the color of the background and the clothing detail on the person, faster than in the normal setting."}, "signatures": ["ICLR.cc/2020/Conference/Paper1124/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1124/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MetaPix: Few-Shot Video Retargeting", "authors": ["Jessica Lee", "Deva Ramanan", "Rohit Girdhar"], "authorids": ["jl5@cs.cmu.edu", "deva@cs.cmu.edu", "rgirdhar@cs.cmu.edu"], "keywords": ["Meta-learning", "Few-shot Learning", "Generative Adversarial Networks", "Video Retargeting"], "TL;DR": "Video retargeting typically requires large amount of target data to be effective, which may not always be available; we propose a metalearning approach that improves over popular baselines while producing temporally coherent frames.", "abstract": "We address the task of unsupervised retargeting of human actions from one video to another. We consider the challenging setting where only a few frames of the target is available. The core of our approach is a conditional generative model that can transcode input skeletal poses (automatically extracted with an off-the-shelf pose estimator) to output target frames. However, it is challenging to build a universal transcoder because humans can appear wildly different due to clothing and background scene geometry. Instead, we learn to adapt \u2013 or personalize \u2013 a universal generator to the particular human and background in the target. To do so, we make use of meta-learning to discover effective strategies for on-the-fly personalization. One significant benefit of meta-learning is that the personalized transcoder naturally enforces temporal coherence across its generated frames; all frames contain consistent clothing and background geometry of the target. We experiment on in-the-wild internet videos and images and show our approach improves over widely-used baselines for the task.\n", "pdf": "/pdf/a63a19c5457caf7a6647d054cd03a139124fb7b4.pdf", "paperhash": "lee|metapix_fewshot_video_retargeting", "_bibtex": "@inproceedings{\nLee2020MetaPix:,\ntitle={MetaPix: Few-Shot Video Retargeting},\nauthor={Jessica Lee and Deva Ramanan and Rohit Girdhar},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJx1URNKwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/860134d07533593abcbb74b5e4d3b3389484db1e.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJx1URNKwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1124/Authors", "ICLR.cc/2020/Conference/Paper1124/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1124/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1124/Reviewers", "ICLR.cc/2020/Conference/Paper1124/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1124/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1124/Authors|ICLR.cc/2020/Conference/Paper1124/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160873, "tmdate": 1576860559538, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1124/Authors", "ICLR.cc/2020/Conference/Paper1124/Reviewers", "ICLR.cc/2020/Conference/Paper1124/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1124/-/Official_Comment"}}}, {"id": "BkeVb25Otr", "original": null, "number": 1, "cdate": 1571494907801, "ddate": null, "tcdate": 1571494907801, "tmdate": 1572972509704, "tddate": null, "forum": "SJx1URNKwH", "replyto": "SJx1URNKwH", "invitation": "ICLR.cc/2020/Conference/Paper1124/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "In this paper, authors propose to address few shot video retargeting, where one should adapt a generic generative model of human actions to a specific person given a few samples of their appearance. \n\nOverall, the paper is written with a good structure. I do like the problem setting and motivations in this paper. However, the solution is not quite novel for me. Both base model (Pix2PixHD) and few-shot adaptation (Reptile) come from the previous works. Their combination is somewhat incremental.  "}, "signatures": ["ICLR.cc/2020/Conference/Paper1124/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1124/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MetaPix: Few-Shot Video Retargeting", "authors": ["Jessica Lee", "Deva Ramanan", "Rohit Girdhar"], "authorids": ["jl5@cs.cmu.edu", "deva@cs.cmu.edu", "rgirdhar@cs.cmu.edu"], "keywords": ["Meta-learning", "Few-shot Learning", "Generative Adversarial Networks", "Video Retargeting"], "TL;DR": "Video retargeting typically requires large amount of target data to be effective, which may not always be available; we propose a metalearning approach that improves over popular baselines while producing temporally coherent frames.", "abstract": "We address the task of unsupervised retargeting of human actions from one video to another. We consider the challenging setting where only a few frames of the target is available. The core of our approach is a conditional generative model that can transcode input skeletal poses (automatically extracted with an off-the-shelf pose estimator) to output target frames. However, it is challenging to build a universal transcoder because humans can appear wildly different due to clothing and background scene geometry. Instead, we learn to adapt \u2013 or personalize \u2013 a universal generator to the particular human and background in the target. To do so, we make use of meta-learning to discover effective strategies for on-the-fly personalization. One significant benefit of meta-learning is that the personalized transcoder naturally enforces temporal coherence across its generated frames; all frames contain consistent clothing and background geometry of the target. We experiment on in-the-wild internet videos and images and show our approach improves over widely-used baselines for the task.\n", "pdf": "/pdf/a63a19c5457caf7a6647d054cd03a139124fb7b4.pdf", "paperhash": "lee|metapix_fewshot_video_retargeting", "_bibtex": "@inproceedings{\nLee2020MetaPix:,\ntitle={MetaPix: Few-Shot Video Retargeting},\nauthor={Jessica Lee and Deva Ramanan and Rohit Girdhar},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJx1URNKwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/860134d07533593abcbb74b5e4d3b3389484db1e.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJx1URNKwH", "replyto": "SJx1URNKwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1124/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1124/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575737321531, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1124/Reviewers"], "noninvitees": [], "tcdate": 1570237742019, "tmdate": 1575737321544, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1124/-/Official_Review"}}}, {"id": "ByezyYV9Fr", "original": null, "number": 2, "cdate": 1571600602016, "ddate": null, "tcdate": 1571600602016, "tmdate": 1572972509668, "tddate": null, "forum": "SJx1URNKwH", "replyto": "SJx1URNKwH", "invitation": "ICLR.cc/2020/Conference/Paper1124/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This submission proposes an application of meta-learning to video frame generation modeling conditioned on human pose information, in order to allow the model to adapt to the context of each video. This context is provided in the form of a support set of K pairs of pose/frame images for the video. Reptile is used as the meta-learning method, and applied to two recently proposed video-frame generative networks (Pix2PixHD and Posewarp). In both cases, results show that Reptile is able to produce better adaptive models, i.e. models that when fine-tuned on the support set produce better image frames.\n\nThough the originality of the work is somewhat weak (it's a relatively straightforward application of Reptile to Pix2PixHD and Posewarp), the problem setting is novel and I find the demonstration that Reptile works well in this setting interesting and valuable. The paper is also clearly written and easy to follow. For these reasons, I'm personally leaning towards recommending to accept this submission."}, "signatures": ["ICLR.cc/2020/Conference/Paper1124/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1124/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MetaPix: Few-Shot Video Retargeting", "authors": ["Jessica Lee", "Deva Ramanan", "Rohit Girdhar"], "authorids": ["jl5@cs.cmu.edu", "deva@cs.cmu.edu", "rgirdhar@cs.cmu.edu"], "keywords": ["Meta-learning", "Few-shot Learning", "Generative Adversarial Networks", "Video Retargeting"], "TL;DR": "Video retargeting typically requires large amount of target data to be effective, which may not always be available; we propose a metalearning approach that improves over popular baselines while producing temporally coherent frames.", "abstract": "We address the task of unsupervised retargeting of human actions from one video to another. We consider the challenging setting where only a few frames of the target is available. The core of our approach is a conditional generative model that can transcode input skeletal poses (automatically extracted with an off-the-shelf pose estimator) to output target frames. However, it is challenging to build a universal transcoder because humans can appear wildly different due to clothing and background scene geometry. Instead, we learn to adapt \u2013 or personalize \u2013 a universal generator to the particular human and background in the target. To do so, we make use of meta-learning to discover effective strategies for on-the-fly personalization. One significant benefit of meta-learning is that the personalized transcoder naturally enforces temporal coherence across its generated frames; all frames contain consistent clothing and background geometry of the target. We experiment on in-the-wild internet videos and images and show our approach improves over widely-used baselines for the task.\n", "pdf": "/pdf/a63a19c5457caf7a6647d054cd03a139124fb7b4.pdf", "paperhash": "lee|metapix_fewshot_video_retargeting", "_bibtex": "@inproceedings{\nLee2020MetaPix:,\ntitle={MetaPix: Few-Shot Video Retargeting},\nauthor={Jessica Lee and Deva Ramanan and Rohit Girdhar},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJx1URNKwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/860134d07533593abcbb74b5e4d3b3389484db1e.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJx1URNKwH", "replyto": "SJx1URNKwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1124/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1124/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575737321531, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1124/Reviewers"], "noninvitees": [], "tcdate": 1570237742019, "tmdate": 1575737321544, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1124/-/Official_Review"}}}], "count": 8}