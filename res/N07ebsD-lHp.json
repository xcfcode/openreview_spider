{"notes": [{"id": "N07ebsD-lHp", "original": "qrMaXji95U", "number": 1250, "cdate": 1601308139967, "ddate": null, "tcdate": 1601308139967, "tmdate": 1614985765967, "tddate": null, "forum": "N07ebsD-lHp", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Defending against black-box adversarial attacks with gradient-free trained sign activation neural networks", "authorids": ["yx277@njit.edu", "mx42@njit.edu", "zy328@njit.edu", "~Usman_Roshan1"], "authors": ["Yunzhe Xue", "Meiyan Xie", "Zhibo Yang", "Usman Roshan"], "keywords": ["sign activation neural network", "gradient-free training", "stochastic coordinate descent", "black box adversarial attack", "hopskipjump", "transferability", "image distortion"], "abstract": "While machine learning models today can achieve high accuracies on classification tasks, they can be deceived by minor imperceptible distortions to the data. These are known as adversarial attacks and can be lethal in the black-box setting which does not require knowledge of the target model type or its parameters. Binary neural networks that have sign activation and are trained with gradient descent have been shown to be harder to attack than conventional sigmoid activation networks but their improvements are marginal. We instead train sign activation networks with a novel gradient-free stochastic coordinate descent algorithm and propose an ensemble of such networks as a defense model. We evaluate the robustness of our model (a hard problem in itself) on image, text, and medical ECG data and find it to be more robust than ensembles of binary, full precision, and convolutional neural networks, and than random forests while attaining comparable clean test accuracy. In order to explain our model's robustness we show that an adversary targeting a single network in our ensemble fails to attack (and thus non-transferable to) other networks in the ensemble. Thus a datapoint requires a large distortion to fool the majority of networks in our ensemble and is likely to be detected in advance. This property of non-transferability arises naturally from the non-convexity of sign activation networks and randomization in our gradient-free training algorithm without any adversarial defense effort.", "one-sentence_summary": "We show that an ensemble of our gradient free trained sign activation networks is much more adversarially robust than ensembles of binary, full precision, convolutional neural networks, and than random forest on image, text, and medical ECG data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xue|defending_against_blackbox_adversarial_attacks_with_gradientfree_trained_sign_activation_neural_networks", "supplementary_material": "/attachment/2c05ec7265095bc3c9da11d5dd9803409e9ce6df.zip", "pdf": "/pdf/8d722bb6e4eff15530a0fc8d413ea198ad088b54.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=2qogKBs4vU", "_bibtex": "@misc{\nxue2021defending,\ntitle={Defending against black-box adversarial attacks with gradient-free trained sign activation neural networks},\nauthor={Yunzhe Xue and Meiyan Xie and Zhibo Yang and Usman Roshan},\nyear={2021},\nurl={https://openreview.net/forum?id=N07ebsD-lHp}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "tr80T0nmxx4", "original": null, "number": 1, "cdate": 1610040367381, "ddate": null, "tcdate": 1610040367381, "tmdate": 1610473958296, "tddate": null, "forum": "N07ebsD-lHp", "replyto": "N07ebsD-lHp", "invitation": "ICLR.cc/2021/Conference/Paper1250/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper proposes an algorithm to defend against black-box attacks. All the reviewers think the current experiments are not convincing enough, and the method seems to have some issues (e.g., not scalable). "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defending against black-box adversarial attacks with gradient-free trained sign activation neural networks", "authorids": ["yx277@njit.edu", "mx42@njit.edu", "zy328@njit.edu", "~Usman_Roshan1"], "authors": ["Yunzhe Xue", "Meiyan Xie", "Zhibo Yang", "Usman Roshan"], "keywords": ["sign activation neural network", "gradient-free training", "stochastic coordinate descent", "black box adversarial attack", "hopskipjump", "transferability", "image distortion"], "abstract": "While machine learning models today can achieve high accuracies on classification tasks, they can be deceived by minor imperceptible distortions to the data. These are known as adversarial attacks and can be lethal in the black-box setting which does not require knowledge of the target model type or its parameters. Binary neural networks that have sign activation and are trained with gradient descent have been shown to be harder to attack than conventional sigmoid activation networks but their improvements are marginal. We instead train sign activation networks with a novel gradient-free stochastic coordinate descent algorithm and propose an ensemble of such networks as a defense model. We evaluate the robustness of our model (a hard problem in itself) on image, text, and medical ECG data and find it to be more robust than ensembles of binary, full precision, and convolutional neural networks, and than random forests while attaining comparable clean test accuracy. In order to explain our model's robustness we show that an adversary targeting a single network in our ensemble fails to attack (and thus non-transferable to) other networks in the ensemble. Thus a datapoint requires a large distortion to fool the majority of networks in our ensemble and is likely to be detected in advance. This property of non-transferability arises naturally from the non-convexity of sign activation networks and randomization in our gradient-free training algorithm without any adversarial defense effort.", "one-sentence_summary": "We show that an ensemble of our gradient free trained sign activation networks is much more adversarially robust than ensembles of binary, full precision, convolutional neural networks, and than random forest on image, text, and medical ECG data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xue|defending_against_blackbox_adversarial_attacks_with_gradientfree_trained_sign_activation_neural_networks", "supplementary_material": "/attachment/2c05ec7265095bc3c9da11d5dd9803409e9ce6df.zip", "pdf": "/pdf/8d722bb6e4eff15530a0fc8d413ea198ad088b54.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=2qogKBs4vU", "_bibtex": "@misc{\nxue2021defending,\ntitle={Defending against black-box adversarial attacks with gradient-free trained sign activation neural networks},\nauthor={Yunzhe Xue and Meiyan Xie and Zhibo Yang and Usman Roshan},\nyear={2021},\nurl={https://openreview.net/forum?id=N07ebsD-lHp}\n}"}, "tags": [], "invitation": {"reply": {"forum": "N07ebsD-lHp", "replyto": "N07ebsD-lHp", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040367365, "tmdate": 1610473958278, "id": "ICLR.cc/2021/Conference/Paper1250/-/Decision"}}}, {"id": "KxwP4xZcVQj", "original": null, "number": 1, "cdate": 1603512756168, "ddate": null, "tcdate": 1603512756168, "tmdate": 1606313378132, "tddate": null, "forum": "N07ebsD-lHp", "replyto": "N07ebsD-lHp", "invitation": "ICLR.cc/2021/Conference/Paper1250/-/Official_Review", "content": {"title": "Lots of work is needed to improve the paper", "review": "\n\nThis paper proposes training an ensemble of binary neural networks with sign activations using gradient-free stochastic coordinate descent algorithm. \nThe nature of the training method and binary networks leads to robust models with non-transferable attacks.\n\n----\n*Pros*:\nI appreciated \n- the non-transferability experiment which demonstrated that the trained ensemble is diverse and sort-of \"orthogonal\".\n- the use of minimum distortion as a measure of the model's robustness.\n\n*Cons*:\n- It looks like the method is not scalable. Experiments were carried out on a single-hidden-layer network of 20 nodes (would really like to see experiments on bigger models)\n- Only one black-box attack is used to demonstrate the effectiveness of the method and that too under one perturbation threat (l2-norm). There are multiple black-box attacks that exploit different aspects of model vulnerabilities. It cant be claimed that the method is robust based on a single-attack single-threat.\n- The scale of experiments (10 images) does not provide significant evidence that this method is effective and the authors acknowledge that in Section 3.6\n\n\nRemarks/Comments:\n- Page 2, Line2: \"by with gradient\" -> \"with gradient\".\n- Sec 3.2: \"divide by 9900\" -> \"divide by 99\".\n- Table 1: What does \"single run\" mean in the caption? Is it a single forward-pass?\n- Fig 2: please make it more legible with markers (colors are not enough).\n\n\nBased on the above, I suggest that the authors redesign their experiment to support the paper's proposition in terms of the method scalability and robustness against more attacks & perturbation settings. Perhaps making the experiments more scalable by reducing the compute budget of HopSkipJump (1000 random init x 10 times x 100 iterations)\n\n**Post-Rebuttal**: No change in my score, please read my comments in the thread below.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1250/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1250/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defending against black-box adversarial attacks with gradient-free trained sign activation neural networks", "authorids": ["yx277@njit.edu", "mx42@njit.edu", "zy328@njit.edu", "~Usman_Roshan1"], "authors": ["Yunzhe Xue", "Meiyan Xie", "Zhibo Yang", "Usman Roshan"], "keywords": ["sign activation neural network", "gradient-free training", "stochastic coordinate descent", "black box adversarial attack", "hopskipjump", "transferability", "image distortion"], "abstract": "While machine learning models today can achieve high accuracies on classification tasks, they can be deceived by minor imperceptible distortions to the data. These are known as adversarial attacks and can be lethal in the black-box setting which does not require knowledge of the target model type or its parameters. Binary neural networks that have sign activation and are trained with gradient descent have been shown to be harder to attack than conventional sigmoid activation networks but their improvements are marginal. We instead train sign activation networks with a novel gradient-free stochastic coordinate descent algorithm and propose an ensemble of such networks as a defense model. We evaluate the robustness of our model (a hard problem in itself) on image, text, and medical ECG data and find it to be more robust than ensembles of binary, full precision, and convolutional neural networks, and than random forests while attaining comparable clean test accuracy. In order to explain our model's robustness we show that an adversary targeting a single network in our ensemble fails to attack (and thus non-transferable to) other networks in the ensemble. Thus a datapoint requires a large distortion to fool the majority of networks in our ensemble and is likely to be detected in advance. This property of non-transferability arises naturally from the non-convexity of sign activation networks and randomization in our gradient-free training algorithm without any adversarial defense effort.", "one-sentence_summary": "We show that an ensemble of our gradient free trained sign activation networks is much more adversarially robust than ensembles of binary, full precision, convolutional neural networks, and than random forest on image, text, and medical ECG data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xue|defending_against_blackbox_adversarial_attacks_with_gradientfree_trained_sign_activation_neural_networks", "supplementary_material": "/attachment/2c05ec7265095bc3c9da11d5dd9803409e9ce6df.zip", "pdf": "/pdf/8d722bb6e4eff15530a0fc8d413ea198ad088b54.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=2qogKBs4vU", "_bibtex": "@misc{\nxue2021defending,\ntitle={Defending against black-box adversarial attacks with gradient-free trained sign activation neural networks},\nauthor={Yunzhe Xue and Meiyan Xie and Zhibo Yang and Usman Roshan},\nyear={2021},\nurl={https://openreview.net/forum?id=N07ebsD-lHp}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "N07ebsD-lHp", "replyto": "N07ebsD-lHp", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1250/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538123001, "tmdate": 1606915763307, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1250/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1250/-/Official_Review"}}}, {"id": "Wbdab7hGo_A", "original": null, "number": 8, "cdate": 1606291670611, "ddate": null, "tcdate": 1606291670611, "tmdate": 1606291670611, "tddate": null, "forum": "N07ebsD-lHp", "replyto": "N07ebsD-lHp", "invitation": "ICLR.cc/2021/Conference/Paper1250/-/Official_Comment", "content": {"title": "Uploaded new rebuttal version", "comment": "We have incorporated all reviewer feedback (to which we are thankful) into our revised version. We have also included (single image) distortions on CelebA, GTSRB, and ImageNet, and single image CIFAR10 distortions of our model with different number of hidden nodes."}, "signatures": ["ICLR.cc/2021/Conference/Paper1250/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1250/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defending against black-box adversarial attacks with gradient-free trained sign activation neural networks", "authorids": ["yx277@njit.edu", "mx42@njit.edu", "zy328@njit.edu", "~Usman_Roshan1"], "authors": ["Yunzhe Xue", "Meiyan Xie", "Zhibo Yang", "Usman Roshan"], "keywords": ["sign activation neural network", "gradient-free training", "stochastic coordinate descent", "black box adversarial attack", "hopskipjump", "transferability", "image distortion"], "abstract": "While machine learning models today can achieve high accuracies on classification tasks, they can be deceived by minor imperceptible distortions to the data. These are known as adversarial attacks and can be lethal in the black-box setting which does not require knowledge of the target model type or its parameters. Binary neural networks that have sign activation and are trained with gradient descent have been shown to be harder to attack than conventional sigmoid activation networks but their improvements are marginal. We instead train sign activation networks with a novel gradient-free stochastic coordinate descent algorithm and propose an ensemble of such networks as a defense model. We evaluate the robustness of our model (a hard problem in itself) on image, text, and medical ECG data and find it to be more robust than ensembles of binary, full precision, and convolutional neural networks, and than random forests while attaining comparable clean test accuracy. In order to explain our model's robustness we show that an adversary targeting a single network in our ensemble fails to attack (and thus non-transferable to) other networks in the ensemble. Thus a datapoint requires a large distortion to fool the majority of networks in our ensemble and is likely to be detected in advance. This property of non-transferability arises naturally from the non-convexity of sign activation networks and randomization in our gradient-free training algorithm without any adversarial defense effort.", "one-sentence_summary": "We show that an ensemble of our gradient free trained sign activation networks is much more adversarially robust than ensembles of binary, full precision, convolutional neural networks, and than random forest on image, text, and medical ECG data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xue|defending_against_blackbox_adversarial_attacks_with_gradientfree_trained_sign_activation_neural_networks", "supplementary_material": "/attachment/2c05ec7265095bc3c9da11d5dd9803409e9ce6df.zip", "pdf": "/pdf/8d722bb6e4eff15530a0fc8d413ea198ad088b54.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=2qogKBs4vU", "_bibtex": "@misc{\nxue2021defending,\ntitle={Defending against black-box adversarial attacks with gradient-free trained sign activation neural networks},\nauthor={Yunzhe Xue and Meiyan Xie and Zhibo Yang and Usman Roshan},\nyear={2021},\nurl={https://openreview.net/forum?id=N07ebsD-lHp}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "N07ebsD-lHp", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1250/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1250/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1250/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1250/Authors|ICLR.cc/2021/Conference/Paper1250/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1250/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861871, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1250/-/Official_Comment"}}}, {"id": "LMNS-GiE1pi", "original": null, "number": 7, "cdate": 1606280222748, "ddate": null, "tcdate": 1606280222748, "tmdate": 1606288268984, "tddate": null, "forum": "N07ebsD-lHp", "replyto": "KxwP4xZcVQj", "invitation": "ICLR.cc/2021/Conference/Paper1250/-/Official_Comment", "content": {"title": "Added more results", "comment": "Thank you for your feedback. We have added distortion estimates by two other boundary based black box attack methods and also added distortions on more CIFAR10 datapoints. Our conclusions still hold after these additions.\n\nWe can train models with more hidden nodes and even have a model with a single convolutional layer. Due to rebuttal time constraints we are unable to complete distortion results on those models. To give you a sense of our model's scalability we provide HopSkipJump distortion estimates (min of 10 runs maxiters=100) for different hidden nodes in our single hidden layer model (100 vote ensemble):\n\nHidden-nodes:  4,      16,       20,      32,       64\n\nDistortion:         2.22,  1.98,   2.21,   2.47,    3.45\n\nWe expect our model distortion to increase with more hidden nodes and with a convolutional layer.\n\nResponse to your comments:\n\n1. Fixed\n2. Divide by 9900 is actually correct. We have a total of 100 models m0 through m99 in each ensemble. Consider the adversary x'0 that attacks model m0. The maximum number of models that the adversary x'0 succeeds in attacking by transferability (besides its target m0) is 99. Now consider adversary x'1 that attacks m1. The maximum number of models it can succeed in transferring to is 99. We sum this for each of the adversaries x'0 through x'99 and get the max value of 9900. Thus a small probability means that adversaries transfer to a small fraction of the 9900 max value.\n3. It means the total training runtime of one instance of our model. We then consider an ensemble of 100 models which means training 100 models separately. Thus the total runtime is the single run times 100 but this can be parallelized.\n4. We use different line types for the models and also ordered the (non-GN) methods from top to bottom according to their adversarial accuracy in descending rank.\n\nWe have added distortions on 100 more CIFAR10 test points by running HopSkipJump with 10 maximum iterations and fixed initial images (10 runs per image since even with a fixed initial image the distortions can vary)."}, "signatures": ["ICLR.cc/2021/Conference/Paper1250/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1250/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defending against black-box adversarial attacks with gradient-free trained sign activation neural networks", "authorids": ["yx277@njit.edu", "mx42@njit.edu", "zy328@njit.edu", "~Usman_Roshan1"], "authors": ["Yunzhe Xue", "Meiyan Xie", "Zhibo Yang", "Usman Roshan"], "keywords": ["sign activation neural network", "gradient-free training", "stochastic coordinate descent", "black box adversarial attack", "hopskipjump", "transferability", "image distortion"], "abstract": "While machine learning models today can achieve high accuracies on classification tasks, they can be deceived by minor imperceptible distortions to the data. These are known as adversarial attacks and can be lethal in the black-box setting which does not require knowledge of the target model type or its parameters. Binary neural networks that have sign activation and are trained with gradient descent have been shown to be harder to attack than conventional sigmoid activation networks but their improvements are marginal. We instead train sign activation networks with a novel gradient-free stochastic coordinate descent algorithm and propose an ensemble of such networks as a defense model. We evaluate the robustness of our model (a hard problem in itself) on image, text, and medical ECG data and find it to be more robust than ensembles of binary, full precision, and convolutional neural networks, and than random forests while attaining comparable clean test accuracy. In order to explain our model's robustness we show that an adversary targeting a single network in our ensemble fails to attack (and thus non-transferable to) other networks in the ensemble. Thus a datapoint requires a large distortion to fool the majority of networks in our ensemble and is likely to be detected in advance. This property of non-transferability arises naturally from the non-convexity of sign activation networks and randomization in our gradient-free training algorithm without any adversarial defense effort.", "one-sentence_summary": "We show that an ensemble of our gradient free trained sign activation networks is much more adversarially robust than ensembles of binary, full precision, convolutional neural networks, and than random forest on image, text, and medical ECG data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xue|defending_against_blackbox_adversarial_attacks_with_gradientfree_trained_sign_activation_neural_networks", "supplementary_material": "/attachment/2c05ec7265095bc3c9da11d5dd9803409e9ce6df.zip", "pdf": "/pdf/8d722bb6e4eff15530a0fc8d413ea198ad088b54.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=2qogKBs4vU", "_bibtex": "@misc{\nxue2021defending,\ntitle={Defending against black-box adversarial attacks with gradient-free trained sign activation neural networks},\nauthor={Yunzhe Xue and Meiyan Xie and Zhibo Yang and Usman Roshan},\nyear={2021},\nurl={https://openreview.net/forum?id=N07ebsD-lHp}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "N07ebsD-lHp", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1250/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1250/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1250/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1250/Authors|ICLR.cc/2021/Conference/Paper1250/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1250/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861871, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1250/-/Official_Comment"}}}, {"id": "iM7spXh6r_Z", "original": null, "number": 6, "cdate": 1606279854466, "ddate": null, "tcdate": 1606279854466, "tmdate": 1606286457042, "tddate": null, "forum": "N07ebsD-lHp", "replyto": "361i90E3fF", "invitation": "ICLR.cc/2021/Conference/Paper1250/-/Official_Comment", "content": {"title": "Added more data to our paper", "comment": "Thank you for your feedback. We have tried to make our case more convincing by adding more data. To address your other comments:\n\n1. The runtime of single runs are similar with low standard deviation, thus we report just one run\n2. Important values are in bold now\n3. We now specify in Table 4 caption that we use 8 votes for the text models.\n4. We have fixed the legend typo in Figure 2 and ordered the legend method according to their adversarial accuracy (methods with high accuracy are shown above the ones with lower accuracy). We have also shown the non-GN methods with different line types for better legibility.\n5. We have fixed the citations.\n\nOur running times are correct. The SCDCEBNN model performs 10000 epochs whereas the SCD01 and SCDCE has 1000 epochs and thus are much fasters. The most correct variant MLP is fastest because it uses gradient descent which is much faster than our coordinate descent.\n\nWe have added a better description of our training algorithm. \n\nThe black box attack is performed on the ensemble. Our point is that the networks in our ensemble are non-transferable and this leads to some type of orthogonality that forces an adversary to undergo many changes before it can fool the entire ensemble. In comparison ensembles of other models don't have the same level of non-transferability (orthogonality) as we show in our paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper1250/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1250/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defending against black-box adversarial attacks with gradient-free trained sign activation neural networks", "authorids": ["yx277@njit.edu", "mx42@njit.edu", "zy328@njit.edu", "~Usman_Roshan1"], "authors": ["Yunzhe Xue", "Meiyan Xie", "Zhibo Yang", "Usman Roshan"], "keywords": ["sign activation neural network", "gradient-free training", "stochastic coordinate descent", "black box adversarial attack", "hopskipjump", "transferability", "image distortion"], "abstract": "While machine learning models today can achieve high accuracies on classification tasks, they can be deceived by minor imperceptible distortions to the data. These are known as adversarial attacks and can be lethal in the black-box setting which does not require knowledge of the target model type or its parameters. Binary neural networks that have sign activation and are trained with gradient descent have been shown to be harder to attack than conventional sigmoid activation networks but their improvements are marginal. We instead train sign activation networks with a novel gradient-free stochastic coordinate descent algorithm and propose an ensemble of such networks as a defense model. We evaluate the robustness of our model (a hard problem in itself) on image, text, and medical ECG data and find it to be more robust than ensembles of binary, full precision, and convolutional neural networks, and than random forests while attaining comparable clean test accuracy. In order to explain our model's robustness we show that an adversary targeting a single network in our ensemble fails to attack (and thus non-transferable to) other networks in the ensemble. Thus a datapoint requires a large distortion to fool the majority of networks in our ensemble and is likely to be detected in advance. This property of non-transferability arises naturally from the non-convexity of sign activation networks and randomization in our gradient-free training algorithm without any adversarial defense effort.", "one-sentence_summary": "We show that an ensemble of our gradient free trained sign activation networks is much more adversarially robust than ensembles of binary, full precision, convolutional neural networks, and than random forest on image, text, and medical ECG data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xue|defending_against_blackbox_adversarial_attacks_with_gradientfree_trained_sign_activation_neural_networks", "supplementary_material": "/attachment/2c05ec7265095bc3c9da11d5dd9803409e9ce6df.zip", "pdf": "/pdf/8d722bb6e4eff15530a0fc8d413ea198ad088b54.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=2qogKBs4vU", "_bibtex": "@misc{\nxue2021defending,\ntitle={Defending against black-box adversarial attacks with gradient-free trained sign activation neural networks},\nauthor={Yunzhe Xue and Meiyan Xie and Zhibo Yang and Usman Roshan},\nyear={2021},\nurl={https://openreview.net/forum?id=N07ebsD-lHp}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "N07ebsD-lHp", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1250/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1250/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1250/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1250/Authors|ICLR.cc/2021/Conference/Paper1250/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1250/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861871, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1250/-/Official_Comment"}}}, {"id": "MwSdjXdVZQ", "original": null, "number": 5, "cdate": 1606279656020, "ddate": null, "tcdate": 1606279656020, "tmdate": 1606286367403, "tddate": null, "forum": "N07ebsD-lHp", "replyto": "TCDCUkt2ufH", "invitation": "ICLR.cc/2021/Conference/Paper1250/-/Official_Comment", "content": {"title": "Added more data to make our case convincing", "comment": "Thank you for your feedback. We agree that evaluating robustness of a model is a hard task. To make our case convincing we show that HopSkipJump distortions are tighter than Boundary Attack and RayS, both of which also estimate the adversarial distortion. We also show HopSkipJump distortions on more datapoints and find our model to still have the highest distortion. \n\nWe have fixed the citations, removed distortions on individual datapoints, and added a clearer description of our training algorithm. We have a also bounded our tables and made the legend of Figure 2 clearer.\n\nTo show distortions on adversarially trained models we would have to figure out how to adversarially train our new models. We would then have to evaluate the distortion of the adversarially trained models, both of which are impossible within the rebuttal time limitation. In the original HopSkipJump paper though adversarial training does not seem to improve the model's distortion shown on MNIST only."}, "signatures": ["ICLR.cc/2021/Conference/Paper1250/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1250/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defending against black-box adversarial attacks with gradient-free trained sign activation neural networks", "authorids": ["yx277@njit.edu", "mx42@njit.edu", "zy328@njit.edu", "~Usman_Roshan1"], "authors": ["Yunzhe Xue", "Meiyan Xie", "Zhibo Yang", "Usman Roshan"], "keywords": ["sign activation neural network", "gradient-free training", "stochastic coordinate descent", "black box adversarial attack", "hopskipjump", "transferability", "image distortion"], "abstract": "While machine learning models today can achieve high accuracies on classification tasks, they can be deceived by minor imperceptible distortions to the data. These are known as adversarial attacks and can be lethal in the black-box setting which does not require knowledge of the target model type or its parameters. Binary neural networks that have sign activation and are trained with gradient descent have been shown to be harder to attack than conventional sigmoid activation networks but their improvements are marginal. We instead train sign activation networks with a novel gradient-free stochastic coordinate descent algorithm and propose an ensemble of such networks as a defense model. We evaluate the robustness of our model (a hard problem in itself) on image, text, and medical ECG data and find it to be more robust than ensembles of binary, full precision, and convolutional neural networks, and than random forests while attaining comparable clean test accuracy. In order to explain our model's robustness we show that an adversary targeting a single network in our ensemble fails to attack (and thus non-transferable to) other networks in the ensemble. Thus a datapoint requires a large distortion to fool the majority of networks in our ensemble and is likely to be detected in advance. This property of non-transferability arises naturally from the non-convexity of sign activation networks and randomization in our gradient-free training algorithm without any adversarial defense effort.", "one-sentence_summary": "We show that an ensemble of our gradient free trained sign activation networks is much more adversarially robust than ensembles of binary, full precision, convolutional neural networks, and than random forest on image, text, and medical ECG data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xue|defending_against_blackbox_adversarial_attacks_with_gradientfree_trained_sign_activation_neural_networks", "supplementary_material": "/attachment/2c05ec7265095bc3c9da11d5dd9803409e9ce6df.zip", "pdf": "/pdf/8d722bb6e4eff15530a0fc8d413ea198ad088b54.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=2qogKBs4vU", "_bibtex": "@misc{\nxue2021defending,\ntitle={Defending against black-box adversarial attacks with gradient-free trained sign activation neural networks},\nauthor={Yunzhe Xue and Meiyan Xie and Zhibo Yang and Usman Roshan},\nyear={2021},\nurl={https://openreview.net/forum?id=N07ebsD-lHp}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "N07ebsD-lHp", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1250/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1250/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1250/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1250/Authors|ICLR.cc/2021/Conference/Paper1250/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1250/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861871, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1250/-/Official_Comment"}}}, {"id": "361i90E3fF", "original": null, "number": 2, "cdate": 1603825914390, "ddate": null, "tcdate": 1603825914390, "tmdate": 1605024491081, "tddate": null, "forum": "N07ebsD-lHp", "replyto": "N07ebsD-lHp", "invitation": "ICLR.cc/2021/Conference/Paper1250/-/Official_Review", "content": {"title": "an experimental paper that could be improved", "review": "The paper proposes an architecture (ensemble of networks) aiming at being robust against black-box attacks, based on the idea that crafting an adversarial example able to fool enough individual networks such that the majority vote changes is a more difficult task.  The paper presents ways of training such ensembles and provides several sets of experiments showing the advantage of the approach. It also contains an observation on \"non-transferability\", counting how many co-networks are fooled when only one is targetted by the blackbox attack. It turns out that this amount is lower for the proposed scheme. \n\nThe algorithms are postponed to supplementary material and the paper itself mainly report the experimental part. It concerns : \n-> training time\n-> minimum adversarial distorsion (l2 and l_inf)\n-> transferabiliy\n-> different tasks (images, text and ecg)\n\nAll tested algorithms are ensembles and dataset are subparts of actual datasets.\n\nComments: \n* Table 1 : training time of a single run : should not it be avaraged on several runs somehow?  \n* Table 2, 3, 5 : I would find it more readable if some remarkable values were bold for instance\n* Table 4 : since all models don't have the same number of weak learners, I suggest that the presentation of the table recalls it, it * has a great impact on values here\n* (a verb is missing on top of page 6. )\nFigure 2 : hard to read. Top lines appears to be  some kind of reference to be compared to and the bottom lines the proposed methods. I suggest that one color is assigned to each algorithm and different line style for with or without GN. Also I think that there's a typo in the legend (SCDCE-BNN-> SCDCE-GN?) \n* biblio : I did not check each paper on arxiv but I'm quite sure that a significative part of arvix references have some published references.  Citing preprints is ok for recent work only. The reference by Alex Krizhesky seems to lack information too. \n\nOverall, I'm not really convinced by the paper in its present form, although I recognize that the results show some interesting properties for robustness.  Experiment on running time seems a little misleading, and shows that the most efficient variant is also significantly slower. I'm actually not against slower methods, but when I read this part I have a feeling of \"is it right?\" \nI also think that there would be enough space to provide more information on algorithms in the main paper. \nThe transferability idea seems to be an interesting point, but it has to be further developped. How to apply this observation to the actual blackbox attack, which, as far as I understand, does not attack weak learners one by one?", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1250/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1250/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defending against black-box adversarial attacks with gradient-free trained sign activation neural networks", "authorids": ["yx277@njit.edu", "mx42@njit.edu", "zy328@njit.edu", "~Usman_Roshan1"], "authors": ["Yunzhe Xue", "Meiyan Xie", "Zhibo Yang", "Usman Roshan"], "keywords": ["sign activation neural network", "gradient-free training", "stochastic coordinate descent", "black box adversarial attack", "hopskipjump", "transferability", "image distortion"], "abstract": "While machine learning models today can achieve high accuracies on classification tasks, they can be deceived by minor imperceptible distortions to the data. These are known as adversarial attacks and can be lethal in the black-box setting which does not require knowledge of the target model type or its parameters. Binary neural networks that have sign activation and are trained with gradient descent have been shown to be harder to attack than conventional sigmoid activation networks but their improvements are marginal. We instead train sign activation networks with a novel gradient-free stochastic coordinate descent algorithm and propose an ensemble of such networks as a defense model. We evaluate the robustness of our model (a hard problem in itself) on image, text, and medical ECG data and find it to be more robust than ensembles of binary, full precision, and convolutional neural networks, and than random forests while attaining comparable clean test accuracy. In order to explain our model's robustness we show that an adversary targeting a single network in our ensemble fails to attack (and thus non-transferable to) other networks in the ensemble. Thus a datapoint requires a large distortion to fool the majority of networks in our ensemble and is likely to be detected in advance. This property of non-transferability arises naturally from the non-convexity of sign activation networks and randomization in our gradient-free training algorithm without any adversarial defense effort.", "one-sentence_summary": "We show that an ensemble of our gradient free trained sign activation networks is much more adversarially robust than ensembles of binary, full precision, convolutional neural networks, and than random forest on image, text, and medical ECG data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xue|defending_against_blackbox_adversarial_attacks_with_gradientfree_trained_sign_activation_neural_networks", "supplementary_material": "/attachment/2c05ec7265095bc3c9da11d5dd9803409e9ce6df.zip", "pdf": "/pdf/8d722bb6e4eff15530a0fc8d413ea198ad088b54.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=2qogKBs4vU", "_bibtex": "@misc{\nxue2021defending,\ntitle={Defending against black-box adversarial attacks with gradient-free trained sign activation neural networks},\nauthor={Yunzhe Xue and Meiyan Xie and Zhibo Yang and Usman Roshan},\nyear={2021},\nurl={https://openreview.net/forum?id=N07ebsD-lHp}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "N07ebsD-lHp", "replyto": "N07ebsD-lHp", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1250/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538123001, "tmdate": 1606915763307, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1250/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1250/-/Official_Review"}}}, {"id": "TCDCUkt2ufH", "original": null, "number": 3, "cdate": 1603897867927, "ddate": null, "tcdate": 1603897867927, "tmdate": 1605024491020, "tddate": null, "forum": "N07ebsD-lHp", "replyto": "N07ebsD-lHp", "invitation": "ICLR.cc/2021/Conference/Paper1250/-/Official_Review", "content": {"title": "Unconvincing evaluation", "review": "This paper presents a new method to defend black-box attack based on an ensemble of sign activation neural networks. The authors demonstrate their method has much higher minimum distortion using HopSkipJump attack.\n\nHowever, I have many concerns regarding this paper:\n\n-The paper organization is very weird and confusing. The author did not put their main algorithm into the main paper. Instead, they put many unimportant results (e.g. Table 1) into the main paper. If there is not enough space the author should use simpler sentences to describe their algorithm and put some results into supp. Also, the figure and table style (i.e., unbounded table, screenshot figures) makes me feel this is an undergrad project report instead of an ICLR submission.\n\n-Besides transfer attack, the authors only evaluate the black-block robustness using HopSkipJump attack. Their claim is \"Compared to other boundary attack methods it is known to give the best estimate of a datapoint\u2019s minimum adversarial distortion.\" \nIs there any paper support this claim? I don't believe one attack method is universally better than other method among all datasets.\nI think the authors should evaluate a set of attack methods instead of only one method otherwise the results are not convincing to me.\n\n-What is the purpose of providing detailed results of 10 random images in Table 2, 3? Those results not only occupied a lot of space but also did not provide any useful insight. An average number of the entire dataset is enough.\n\n-Citing issue: when referencing a paper the author should use the published version not the arxiv version if the cited paper is published.\nE.g., Angus Galloway, Graham W Taylor, and Medhat Moussa. Attacking binarized neural networks. arXiv preprint arXiv:1711.00449, 2017.\nshould be\nGalloway, Angus, Graham W. Taylor, and Medhat Moussa. Attacking Binarized Neural Networks. International Conference on Learning Representations. 2018.\n\n\n-The baseline comparison are all undefended networks. The author should compare to some other blackbox defense methods. ", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1250/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1250/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defending against black-box adversarial attacks with gradient-free trained sign activation neural networks", "authorids": ["yx277@njit.edu", "mx42@njit.edu", "zy328@njit.edu", "~Usman_Roshan1"], "authors": ["Yunzhe Xue", "Meiyan Xie", "Zhibo Yang", "Usman Roshan"], "keywords": ["sign activation neural network", "gradient-free training", "stochastic coordinate descent", "black box adversarial attack", "hopskipjump", "transferability", "image distortion"], "abstract": "While machine learning models today can achieve high accuracies on classification tasks, they can be deceived by minor imperceptible distortions to the data. These are known as adversarial attacks and can be lethal in the black-box setting which does not require knowledge of the target model type or its parameters. Binary neural networks that have sign activation and are trained with gradient descent have been shown to be harder to attack than conventional sigmoid activation networks but their improvements are marginal. We instead train sign activation networks with a novel gradient-free stochastic coordinate descent algorithm and propose an ensemble of such networks as a defense model. We evaluate the robustness of our model (a hard problem in itself) on image, text, and medical ECG data and find it to be more robust than ensembles of binary, full precision, and convolutional neural networks, and than random forests while attaining comparable clean test accuracy. In order to explain our model's robustness we show that an adversary targeting a single network in our ensemble fails to attack (and thus non-transferable to) other networks in the ensemble. Thus a datapoint requires a large distortion to fool the majority of networks in our ensemble and is likely to be detected in advance. This property of non-transferability arises naturally from the non-convexity of sign activation networks and randomization in our gradient-free training algorithm without any adversarial defense effort.", "one-sentence_summary": "We show that an ensemble of our gradient free trained sign activation networks is much more adversarially robust than ensembles of binary, full precision, convolutional neural networks, and than random forest on image, text, and medical ECG data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xue|defending_against_blackbox_adversarial_attacks_with_gradientfree_trained_sign_activation_neural_networks", "supplementary_material": "/attachment/2c05ec7265095bc3c9da11d5dd9803409e9ce6df.zip", "pdf": "/pdf/8d722bb6e4eff15530a0fc8d413ea198ad088b54.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=2qogKBs4vU", "_bibtex": "@misc{\nxue2021defending,\ntitle={Defending against black-box adversarial attacks with gradient-free trained sign activation neural networks},\nauthor={Yunzhe Xue and Meiyan Xie and Zhibo Yang and Usman Roshan},\nyear={2021},\nurl={https://openreview.net/forum?id=N07ebsD-lHp}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "N07ebsD-lHp", "replyto": "N07ebsD-lHp", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1250/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538123001, "tmdate": 1606915763307, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1250/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1250/-/Official_Review"}}}], "count": 9}