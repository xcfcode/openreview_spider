{"notes": [{"id": "HJezF3VYPB", "original": "HJl23-RcIS", "number": 70, "cdate": 1569438842046, "ddate": null, "tcdate": 1569438842046, "tmdate": 1583912034858, "tddate": null, "forum": "HJezF3VYPB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Federated Adversarial Domain Adaptation", "authors": ["Xingchao Peng", "Zijun Huang", "Yizhe Zhu", "Kate Saenko"], "authorids": ["xpeng@bu.edu", "zijun.huang@columbia.edu", "yizhe.zhu@rutgers.edu", "saenko@bu.edu"], "keywords": ["Federated Learning", "Domain Adaptation", "Transfer Learning", "Feature Disentanglement"], "TL;DR": "we present a principled approach to the problem of federated domain adaptation, which aims to align the representations learned among the different nodes with the data distribution of the target node.", "abstract": "Federated learning improves data privacy and efficiency in machine learning performed over networks of distributed devices, such as mobile phones, IoT and wearable devices, etc. Yet models trained with federated learning can still fail to generalize to new devices due to the problem of domain shift. Domain shift occurs when the labeled data collected by source nodes statistically differs from the target node's unlabeled data. In this work, we present a principled approach to the problem of federated domain adaptation, which aims to align the representations learned among the different nodes with the data distribution of the target node. Our approach extends adversarial adaptation techniques to the constraints of the federated setting. In addition, we devise a dynamic attention mechanism and leverage feature disentanglement to enhance knowledge transfer. Empirically, we perform extensive experiments on several image and text classification tasks and show promising results under unsupervised federated domain adaptation setting.", "pdf": "/pdf/0a10e9439edcc36581111096b67db724743508d3.pdf", "code": "https://drive.google.com/file/d/1OekTpqB6qLfjlE2XUjQPm3F110KDMFc0/view?usp=sharing", "paperhash": "peng|federated_adversarial_domain_adaptation", "_bibtex": "@inproceedings{\nPeng2020Federated,\ntitle={Federated Adversarial Domain Adaptation},\nauthor={Xingchao Peng and Zijun Huang and Yizhe Zhu and Kate Saenko},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJezF3VYPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/32e298a2dabba1d70bd3fe0b5e2fd117f1d667a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "_VPelBLUb", "original": null, "number": 1, "cdate": 1576798686599, "ddate": null, "tcdate": 1576798686599, "tmdate": 1576800948429, "tddate": null, "forum": "HJezF3VYPB", "replyto": "HJezF3VYPB", "invitation": "ICLR.cc/2020/Conference/Paper70/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper studies an interesting new problem, federated domain adaptation, and proposes an approach based on dynamic attention, federated adversarial alignment, and representation disentanglement.\n\nReviewers generally agree that the paper contributes a novel approach to an interesting problem with theoretical guarantees and empirical justification. While many professional concerns were raised by the reviewers, the authors managed to perform an effective rebuttal with a major revision, which addressed the concerns convincingly. AC believes that the updated version is acceptable.\n\nHence I recommend acceptance.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Federated Adversarial Domain Adaptation", "authors": ["Xingchao Peng", "Zijun Huang", "Yizhe Zhu", "Kate Saenko"], "authorids": ["xpeng@bu.edu", "zijun.huang@columbia.edu", "yizhe.zhu@rutgers.edu", "saenko@bu.edu"], "keywords": ["Federated Learning", "Domain Adaptation", "Transfer Learning", "Feature Disentanglement"], "TL;DR": "we present a principled approach to the problem of federated domain adaptation, which aims to align the representations learned among the different nodes with the data distribution of the target node.", "abstract": "Federated learning improves data privacy and efficiency in machine learning performed over networks of distributed devices, such as mobile phones, IoT and wearable devices, etc. Yet models trained with federated learning can still fail to generalize to new devices due to the problem of domain shift. Domain shift occurs when the labeled data collected by source nodes statistically differs from the target node's unlabeled data. In this work, we present a principled approach to the problem of federated domain adaptation, which aims to align the representations learned among the different nodes with the data distribution of the target node. Our approach extends adversarial adaptation techniques to the constraints of the federated setting. In addition, we devise a dynamic attention mechanism and leverage feature disentanglement to enhance knowledge transfer. Empirically, we perform extensive experiments on several image and text classification tasks and show promising results under unsupervised federated domain adaptation setting.", "pdf": "/pdf/0a10e9439edcc36581111096b67db724743508d3.pdf", "code": "https://drive.google.com/file/d/1OekTpqB6qLfjlE2XUjQPm3F110KDMFc0/view?usp=sharing", "paperhash": "peng|federated_adversarial_domain_adaptation", "_bibtex": "@inproceedings{\nPeng2020Federated,\ntitle={Federated Adversarial Domain Adaptation},\nauthor={Xingchao Peng and Zijun Huang and Yizhe Zhu and Kate Saenko},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJezF3VYPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/32e298a2dabba1d70bd3fe0b5e2fd117f1d667a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJezF3VYPB", "replyto": "HJezF3VYPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795716688, "tmdate": 1576800266885, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper70/-/Decision"}}}, {"id": "BJe8hCFWqB", "original": null, "number": 3, "cdate": 1572081325731, "ddate": null, "tcdate": 1572081325731, "tmdate": 1576224069963, "tddate": null, "forum": "HJezF3VYPB", "replyto": "HJezF3VYPB", "invitation": "ICLR.cc/2020/Conference/Paper70/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "This paper introduces an unsupervised federated domain adaptation (UFDA) problem and proposes a new model called Federated Adversarial Domain Adaptation (FADA) to transfer the knowledge learned from distributed source domains to an unlabeled target domain. This paper uses a dynamic attention mechanism by leveraging the gap statistics to transfer distributed source knowledge. This paper also proposes a method to disentangle the domain-invariant features from domain-specific features, using adversarial training. Moreover, a theoretical generalization bound for UFDA is derived. An extensive empirical evaluation is performed on UFDA vision and linguistic benchmarks.\n\nThis paper should be rejected because the total pipeline seems ad-hoc except for optimizing the weight of the source domain in the attention mechanism. Although the derivation of generalization bound for FDA in Sec.3 is excellent, it only demonstrates the importance of the weight $\\alpha$. This result is trivial if we assume to have the same source domain as the target and utterly unrelated source domain to the target domain. It seems that proving why minimizing the gap statistics contributes to FADA is more essential in the dynamic attention mechanism. Because representation disentanglement has no relation with the derived theory, it would be better to clarify whether this method is original or not.\n\nIn the UFDA setting, the reviewer has doubts about whether it is realistic that the source node has a rich labeled data assuming our smartphones. Also, the assumption that the system cannot access the source data but must access all source feature seems a significant limitation in terms of privacy issues and communication cost between the target node and the source nodes.\n\nIt is unclear what is the final target classifier. If the target can access the teaching signal (e.g., labels or tags) in the source domains, it would be better to mention whether this situation violates the assumption the authors raised or not.\n\nMinor comments\n1) What is T(p, q, \\theta) in the section of Representation Disentanglement?\n\n2) What is C_s in eq.6? C_{s_i}?\n\n3) In Fig.3, it is not proper to discuss the size of intra-class variance by just looking at the figures because the t-SNE is a non-linear mapping. It is better to show quantitative scores, such as the value of the Fisher criterion.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper70/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper70/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Federated Adversarial Domain Adaptation", "authors": ["Xingchao Peng", "Zijun Huang", "Yizhe Zhu", "Kate Saenko"], "authorids": ["xpeng@bu.edu", "zijun.huang@columbia.edu", "yizhe.zhu@rutgers.edu", "saenko@bu.edu"], "keywords": ["Federated Learning", "Domain Adaptation", "Transfer Learning", "Feature Disentanglement"], "TL;DR": "we present a principled approach to the problem of federated domain adaptation, which aims to align the representations learned among the different nodes with the data distribution of the target node.", "abstract": "Federated learning improves data privacy and efficiency in machine learning performed over networks of distributed devices, such as mobile phones, IoT and wearable devices, etc. Yet models trained with federated learning can still fail to generalize to new devices due to the problem of domain shift. Domain shift occurs when the labeled data collected by source nodes statistically differs from the target node's unlabeled data. In this work, we present a principled approach to the problem of federated domain adaptation, which aims to align the representations learned among the different nodes with the data distribution of the target node. Our approach extends adversarial adaptation techniques to the constraints of the federated setting. In addition, we devise a dynamic attention mechanism and leverage feature disentanglement to enhance knowledge transfer. Empirically, we perform extensive experiments on several image and text classification tasks and show promising results under unsupervised federated domain adaptation setting.", "pdf": "/pdf/0a10e9439edcc36581111096b67db724743508d3.pdf", "code": "https://drive.google.com/file/d/1OekTpqB6qLfjlE2XUjQPm3F110KDMFc0/view?usp=sharing", "paperhash": "peng|federated_adversarial_domain_adaptation", "_bibtex": "@inproceedings{\nPeng2020Federated,\ntitle={Federated Adversarial Domain Adaptation},\nauthor={Xingchao Peng and Zijun Huang and Yizhe Zhu and Kate Saenko},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJezF3VYPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/32e298a2dabba1d70bd3fe0b5e2fd117f1d667a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJezF3VYPB", "replyto": "HJezF3VYPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper70/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper70/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576237330075, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper70/Reviewers"], "noninvitees": [], "tcdate": 1570237757517, "tmdate": 1576237330086, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper70/-/Official_Review"}}}, {"id": "BylrOmZjsH", "original": null, "number": 10, "cdate": 1573749613285, "ddate": null, "tcdate": 1573749613285, "tmdate": 1573749613285, "tddate": null, "forum": "HJezF3VYPB", "replyto": "BJe8hCFWqB", "invitation": "ICLR.cc/2020/Conference/Paper70/-/Official_Comment", "content": {"title": "[Rebuttal 1/2] Misunderstandings clarified; Why minimizing gap statistics help FADA proved; ", "comment": "[Rebuttal 1/2]\n\nWe appreciate the reviewer for the constructive feedback on our paper!\n\n1. The total pipeline is ad-hoc\n\n-Our pipeline is not ad-hoc designed as we are aiming to generally solve the issues caused by the domain shift in the federated learning system. We want to mention that the data heterogeneity between different nodes is one of the main limitations of the federated artificial intelligence system (Yang et al [1]). Our approach extends adversarial adaptation techniques to the constraints of the federated setting and provides a generalized, applicable and robust solution to tackle the intrinsic domain shift existing in the federated learning system. \n\n-The techniques proposed in our paper are also heuristic for conventional domain adaptation without the federated setting.  \ni). Since multiple nodes exist in the federated learning (FL) system, how to efficiently leverage the useful information distributed in multiple nodes becomes a critical research topic. We propose a dynamic attention mechanism to evaluate the importance of each source domain to the target domain. This idea can be generalized to traditional multi-source domain adaptation [2].\nii). The idea of learning domain-invariant knowledge by feature disentanglement can be generalized to conventional domain adaptation.   The disentangled representation learning in our framework posits that the FL system will benefit from separating out the underlying structure of the deep features [3]. \n\n2. Although the derivation of generalization bound for FDA in Sec.3 is excellent, it only demonstrates the importance of the weight. \n\n-We would like to claim that the error bound in Theorem (2) of Sec.3 demonstrates the importance of the weight $\\alpha$, as well as the discrepancy ${d}_{\\mathcal{H}\\Delta\\mathcal{H}}({\\mathcal{D}}_{S},{\\mathcal{D}}_T)$. \n\nInspired by this, we develop the following mechanism towards tackling UFDA task: i) since the weight $\\alpha$ is important for the upper bound, we propose a dynamic attention model to learn the weight $\\alpha$ based on the gap statistics [4]; ii) as the discrepancy  ${d}_{\\mathcal{H}\\Delta\\mathcal{H}}({\\mathcal{D}}_{S},{\\mathcal{D}}_T)$ is a critical factor for the error bound, we utilize federated adversarial alignment to minimize the discrepancy between the source and target domains.          \n\n -The error bound implies that the generalization error can be bounded by the optimal source error and the domain discrepancy.\n\n3. Proving why minimizing the gap statistics of K-Means contributes to FADA is more essential in the dynamic attention mechanism.\n\n-We prove that minimizing the gap statistics contributes to FADA by showing that the optimal $\\alpha$ can be derived by the gap statistics loss between two consecutive iterations. \ni) A smaller gap statistics value indicates the feature distribution has smaller intra-class variance. \nii) A better classifier can be trained if the intra-class variance is smaller.\niii) When the gradients from specific nodes are beneficial to decreasing the gap statistics, we increase the $\\alpha$ corresponding to these nodes, otherwise, we decrease the $\\alpha$.\niv) Once the optimal $\\alpha$ is derived by minimizing the gap statistics, we have can train a better classifier on the target domain.\n\n4. Representation disentanglement has no relation with the derived theory, it would be better to clarify whether this method is original or not. \n\n-To be short, we are the first the incorporate the representation disentanglement to FL system. The schema of disentangling the domain-invariant features proposed in our paper is novel and unique. \n\n-Generally, the discrepancy ${d}_{\\mathcal{H}\\Delta\\mathcal{H}}({\\mathcal{D}}_{S},{\\mathcal{D}}_T)$ would be smaller when defined on the disentangled feature space. Thus, it's not accurate to claim \"Representation disentanglement has no relation with the derived theory\". Deep neural networks are known to extract features in which multiple hidden factors are highly entangled [5], this is especially true in the unsupervised federated domain adaptation scenario. Representation disentanglement has been proposed to explore the real-world explanatory factors. In our case, we leverage representation disentanglement to distill the domain-invariant knowledge and dispel the domain-specific information. "}, "signatures": ["ICLR.cc/2020/Conference/Paper70/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper70/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Federated Adversarial Domain Adaptation", "authors": ["Xingchao Peng", "Zijun Huang", "Yizhe Zhu", "Kate Saenko"], "authorids": ["xpeng@bu.edu", "zijun.huang@columbia.edu", "yizhe.zhu@rutgers.edu", "saenko@bu.edu"], "keywords": ["Federated Learning", "Domain Adaptation", "Transfer Learning", "Feature Disentanglement"], "TL;DR": "we present a principled approach to the problem of federated domain adaptation, which aims to align the representations learned among the different nodes with the data distribution of the target node.", "abstract": "Federated learning improves data privacy and efficiency in machine learning performed over networks of distributed devices, such as mobile phones, IoT and wearable devices, etc. Yet models trained with federated learning can still fail to generalize to new devices due to the problem of domain shift. Domain shift occurs when the labeled data collected by source nodes statistically differs from the target node's unlabeled data. In this work, we present a principled approach to the problem of federated domain adaptation, which aims to align the representations learned among the different nodes with the data distribution of the target node. Our approach extends adversarial adaptation techniques to the constraints of the federated setting. In addition, we devise a dynamic attention mechanism and leverage feature disentanglement to enhance knowledge transfer. Empirically, we perform extensive experiments on several image and text classification tasks and show promising results under unsupervised federated domain adaptation setting.", "pdf": "/pdf/0a10e9439edcc36581111096b67db724743508d3.pdf", "code": "https://drive.google.com/file/d/1OekTpqB6qLfjlE2XUjQPm3F110KDMFc0/view?usp=sharing", "paperhash": "peng|federated_adversarial_domain_adaptation", "_bibtex": "@inproceedings{\nPeng2020Federated,\ntitle={Federated Adversarial Domain Adaptation},\nauthor={Xingchao Peng and Zijun Huang and Yizhe Zhu and Kate Saenko},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJezF3VYPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/32e298a2dabba1d70bd3fe0b5e2fd117f1d667a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJezF3VYPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper70/Authors", "ICLR.cc/2020/Conference/Paper70/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper70/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper70/Reviewers", "ICLR.cc/2020/Conference/Paper70/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper70/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper70/Authors|ICLR.cc/2020/Conference/Paper70/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176831, "tmdate": 1576860532538, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper70/Authors", "ICLR.cc/2020/Conference/Paper70/Reviewers", "ICLR.cc/2020/Conference/Paper70/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper70/-/Official_Comment"}}}, {"id": "r1e5G7-isB", "original": null, "number": 9, "cdate": 1573749522495, "ddate": null, "tcdate": 1573749522495, "tmdate": 1573749522495, "tddate": null, "forum": "HJezF3VYPB", "replyto": "BJe8hCFWqB", "invitation": "ICLR.cc/2020/Conference/Paper70/-/Official_Comment", "content": {"title": "[Rebuttal 2/2] Misunderstanding Clarified; Minor Comments Addressed", "comment": "[Rebuttal 2/2]\n\n5. Whether it is realistic that the source node has a rich labeled data assuming the smartphone.\n\n-We are not assuming that our framework works narrowly on the smartphone network. In fact, our model works on data generated by networks of mobile, IoT devices, or other networks in which data privacy is important and domain shift is significant. \n  \n-Federated learning aims to accumulate the gradient from different nodes to train a large model. In our case, we are aiming to train a good target model based on the gradient from multiple source domains. When the source node has no rich labeled data, our model can accumulate the gradient from each source domain and train a good model. \n\n6. The assumption that the system must access all source features seems a significant limitation in terms of privacy issue and communication cost.\n\n-As shown in Figure 1(a), the model and the data are locally stored. The global discriminator only gets access to the output vectors. Since the model of each domain is not shared, it is infeasible to reconstruct the data for a specific domain using the output vectors of that domain. Even though the feature space is aligned, it is still impractical to recover the original data without the parameters of the feature extractor.\n\n-In terms of the communication cost, the source domains need to send a small-size feature vector for each data (4kb for 1024-dimensional  float feature vectors). In addition, the communication cost for transmitting the feature vector is smaller than that for transmitting the gradient of neural networks. Transmitting the gradient of neural networks is a basic setting for federated learning [6].\n\n7. It is unclear what is the final target classifier.\n\n-The target domain is updated with the gradients from classifiers trained on the source domain since there is no label information in the target domain. (We have updated the Algorithm 1 and adversarial feature alignment section to make this clear. Refer to the red text in the revision paper)\n\n8. If the target can access the teaching signal (e.g. labels or tags) in the source domain, it would be better to mention whether this situation violates the assumption the authors raised or not.\n\n-The target cannot get access to labels or tags from the source domain. The target domain can get access to the gradients from the source domain without violating the federated learning assumption. \n\nMinor Comments\n\n1. What is $T(p,q,\\theta)$?\n\n$T(p,q,\\theta)$ is the neural network parameteralized by $\\theta$ to estimate the mutual information between $\\mathcal{P}$ and $\\mathcal{Q}$, we refer the reviewer to \"mutual information nueral network\" [7] for more details.\n\n2. What is $C_s$ in Eq.6? \n\nThe $C_s$ is the class identifier, which helps to extract the domain-specific features. We have clarified and revised the notations in our revision paper. We also updated Figure 1 and Algorithm 1 to better illustrate our approach.\n\n\n[1] Federated Machine Learning: Concept and Applications. Qiang Yang et al. ACM Trans. Intell. Syst. Technol. 2019\n[2] Moment Matching for Multi-Source Domain Adaptation. Peng et al. ICCV 2019. \n[3] Towards a Definition of Disentangled Representations. Higgins, et al. DeepMind 2018.\n[4] Estimating the number of clusters in a dataset via the gap statistic. Journal of the Royal Statistical Society, 2001.\n[5] Representation learning: A review and new perspectives. Bengio Yoshua, PAMI 2013\n[6] Federated Learning: Strategies for Improving Communication Efficiency. Jakub Kone\u010dn\u00fd, NIPS Workshop 2016.\n[7 ] MINE: Mutual Information Neural Estimation. Mohamed Ishmael Belghazi, ICML 2018.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper70/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper70/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Federated Adversarial Domain Adaptation", "authors": ["Xingchao Peng", "Zijun Huang", "Yizhe Zhu", "Kate Saenko"], "authorids": ["xpeng@bu.edu", "zijun.huang@columbia.edu", "yizhe.zhu@rutgers.edu", "saenko@bu.edu"], "keywords": ["Federated Learning", "Domain Adaptation", "Transfer Learning", "Feature Disentanglement"], "TL;DR": "we present a principled approach to the problem of federated domain adaptation, which aims to align the representations learned among the different nodes with the data distribution of the target node.", "abstract": "Federated learning improves data privacy and efficiency in machine learning performed over networks of distributed devices, such as mobile phones, IoT and wearable devices, etc. Yet models trained with federated learning can still fail to generalize to new devices due to the problem of domain shift. Domain shift occurs when the labeled data collected by source nodes statistically differs from the target node's unlabeled data. In this work, we present a principled approach to the problem of federated domain adaptation, which aims to align the representations learned among the different nodes with the data distribution of the target node. Our approach extends adversarial adaptation techniques to the constraints of the federated setting. In addition, we devise a dynamic attention mechanism and leverage feature disentanglement to enhance knowledge transfer. Empirically, we perform extensive experiments on several image and text classification tasks and show promising results under unsupervised federated domain adaptation setting.", "pdf": "/pdf/0a10e9439edcc36581111096b67db724743508d3.pdf", "code": "https://drive.google.com/file/d/1OekTpqB6qLfjlE2XUjQPm3F110KDMFc0/view?usp=sharing", "paperhash": "peng|federated_adversarial_domain_adaptation", "_bibtex": "@inproceedings{\nPeng2020Federated,\ntitle={Federated Adversarial Domain Adaptation},\nauthor={Xingchao Peng and Zijun Huang and Yizhe Zhu and Kate Saenko},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJezF3VYPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/32e298a2dabba1d70bd3fe0b5e2fd117f1d667a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJezF3VYPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper70/Authors", "ICLR.cc/2020/Conference/Paper70/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper70/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper70/Reviewers", "ICLR.cc/2020/Conference/Paper70/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper70/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper70/Authors|ICLR.cc/2020/Conference/Paper70/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176831, "tmdate": 1576860532538, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper70/Authors", "ICLR.cc/2020/Conference/Paper70/Reviewers", "ICLR.cc/2020/Conference/Paper70/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper70/-/Official_Comment"}}}, {"id": "Bygb4cmcjS", "original": null, "number": 6, "cdate": 1573693993158, "ddate": null, "tcdate": 1573693993158, "tmdate": 1573694471129, "tddate": null, "forum": "HJezF3VYPB", "replyto": "r1xvdmZJ5B", "invitation": "ICLR.cc/2020/Conference/Paper70/-/Official_Comment", "content": {"title": "[rebuttal 2/2]  Typos addresses, Details of Algorithm 1 addressed, Implementations of our model given, Balance of l2 loss and mutual information given", "comment": "[Rebuttal Part 2/2]\n\n\nThe ${\\mathcal{L}_{ent}}$ is minimized over the parameters of the feature generator $G_i$, the disentangler $D_i$, i.e. $\\Theta^{G_i}$, $\\Theta^{D_i}$.\nFeature disentanglement facilitates the knowledge transfer by extracting  $f_{di}$ and dispelling $f_{ds}$\n\n\n3. \"Optimize following objective\" should be \"minimizing following objective\".\n\nWe thank the reviewer for pointing this out. We have revised it in the revision paper.\n\n4. The representation disentanglement process is intricate and only vaguely addressed. How to fit the neural net and use (8)? Where is l2 reconstruction loss balanced with the mutual information?\n\n-We have revised the  representation disentanglement section (refer to the red text in our paper)\n\n-To fit our model to the neural network, one should implement each module in our paper and connect them following Figure 1(b). We have released the hyper-parameters of each module in the supplemental materials. To better illustrate this, we have revised all the equations in our paper with the detailed module names. (Also, we have released our code in the submission). \n\n-To use Equation (8), one should implement the $T(p,q, \\Theta )$ as a neural network. We adopt the implementation of the paper \"Mutual Information Neural Estimator\" in our code. \n\n-The balance of the l2 reconstruction and mutual information can be achieved by adjusting the hyper-parameters of the l2 loss and mutual information loss. We revised the paper to better illustrate this. \n\n5.  Comments: symmetric difference space is incorrectly called $\\mathcal{H}\\Delta\\mathcal{H}$.\n\nWe have revised this typo in the revision paper. Thanks again for pointing this out!\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper70/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper70/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Federated Adversarial Domain Adaptation", "authors": ["Xingchao Peng", "Zijun Huang", "Yizhe Zhu", "Kate Saenko"], "authorids": ["xpeng@bu.edu", "zijun.huang@columbia.edu", "yizhe.zhu@rutgers.edu", "saenko@bu.edu"], "keywords": ["Federated Learning", "Domain Adaptation", "Transfer Learning", "Feature Disentanglement"], "TL;DR": "we present a principled approach to the problem of federated domain adaptation, which aims to align the representations learned among the different nodes with the data distribution of the target node.", "abstract": "Federated learning improves data privacy and efficiency in machine learning performed over networks of distributed devices, such as mobile phones, IoT and wearable devices, etc. Yet models trained with federated learning can still fail to generalize to new devices due to the problem of domain shift. Domain shift occurs when the labeled data collected by source nodes statistically differs from the target node's unlabeled data. In this work, we present a principled approach to the problem of federated domain adaptation, which aims to align the representations learned among the different nodes with the data distribution of the target node. Our approach extends adversarial adaptation techniques to the constraints of the federated setting. In addition, we devise a dynamic attention mechanism and leverage feature disentanglement to enhance knowledge transfer. Empirically, we perform extensive experiments on several image and text classification tasks and show promising results under unsupervised federated domain adaptation setting.", "pdf": "/pdf/0a10e9439edcc36581111096b67db724743508d3.pdf", "code": "https://drive.google.com/file/d/1OekTpqB6qLfjlE2XUjQPm3F110KDMFc0/view?usp=sharing", "paperhash": "peng|federated_adversarial_domain_adaptation", "_bibtex": "@inproceedings{\nPeng2020Federated,\ntitle={Federated Adversarial Domain Adaptation},\nauthor={Xingchao Peng and Zijun Huang and Yizhe Zhu and Kate Saenko},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJezF3VYPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/32e298a2dabba1d70bd3fe0b5e2fd117f1d667a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJezF3VYPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper70/Authors", "ICLR.cc/2020/Conference/Paper70/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper70/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper70/Reviewers", "ICLR.cc/2020/Conference/Paper70/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper70/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper70/Authors|ICLR.cc/2020/Conference/Paper70/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176831, "tmdate": 1576860532538, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper70/Authors", "ICLR.cc/2020/Conference/Paper70/Reviewers", "ICLR.cc/2020/Conference/Paper70/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper70/-/Official_Comment"}}}, {"id": "H1x1c9m9sH", "original": null, "number": 7, "cdate": 1573694087378, "ddate": null, "tcdate": 1573694087378, "tmdate": 1573694123298, "tddate": null, "forum": "HJezF3VYPB", "replyto": "r1xvdmZJ5B", "invitation": "ICLR.cc/2020/Conference/Paper70/-/Official_Comment", "content": {"title": "[Rebuttal 1/2]  Algorithm 1 Clarified, Paper Revised, Notations Clarified", "comment": "[Rebuttal 1/2]\n\nWe thank the reviewer for the positive feedback and constructive suggestions on our paper. We have revised our paper accordingly  (refer to the red text in our revised paper).\n\n1. In the dynamic attention mechanism, how to choose the number of clusters in computing the gap statistics and what is the impact?\n\n-Since we study the close-set domain adaptation, the number of categories in the target domain and the source domain is the same. So the number of clusters is set to be the same with the number of categories in the source domain. This is the optimal and most intuitive way to set the cluster numbers, though it has some limitations in the real application since we cannot assume that the target data contains at least one example for each category. \n\n-Empirically, we study the effect of the number of clusters on \"Digit-Five\" dataset (Table 1 in the paper). \n--------------------------------------------------------------------------\n#cluster            |     7     |    8    |      9    |     10     |     11    |      12     |     13     |\nresult (acc.)      |  72.4  | 73.1  | 73.5    |   73.6    |    73.6  |     72.8   |   72.9    |\n--------------------------------------------------------------------------\n\n2. The notation in the federated adversarial alignment section is unclear: what exactly are the model coefficients $\\Theta$ are being updated?\n\n-We admit that the notations in the Federated Adversarial Alignment and Representation Disentanglement part is unclear. We have revised the paper as follows to make it more clear (refer to the revision paper):\nWe have revised the paper as follows to make it more clear (refer to the revision paper):  \ni) we standardized the notation occurred in the paper and made a table to explain each notation in the supplementary material.\nii) we update Figure1(b) and Algorithm 1 to make the notations consistent and clear. \niii) we edit Equation 4~7 to clarify which parameters will the loss optimizes on.\niv) we have added high-level intuition to our paper to help readers better understand the method section.\n\n-In summary, we clarify the notation in the Federated Adversarial Alignment and Representation Disentanglement as follows:\n\n$DI$        |   domain identifier    | align the domain pair ($G_i$,$G_t$)\n$D$          |   disentangler             | disentangle the feature to domain-invariant feature $f_{di}$ and domain-specific feature $f_{ds}$.\n$C$          |   classifier                    | predict the labels for input features\n$CI$         |   class identify           | first trained with task loss, then train with the adversarial loss to extract domain-specific features. \n$G_i$      |feature generator       | extract features for $\\mathcal{D}_{S_i}$\n$G_t$      | feature generator      | extract features for ${\\mathcal{D}}_t$ \n$M$          | MI estimator               | estimate the mutual information between $f_{di}$ and $f_{ds}$.\n\n-Equation 4 has been revised to:\n$$\\underset{\\Theta^{DI_i}}{\\mathcal{L}_{{adv}_{DI_i}}}(\\mathbf{X}^{S_i}, \\mathbf{X}^T, G_i, G_t)=- \\mathbb{E}_{\\mathbf{x}^{s_i} \\sim \\mathbf{X}^{s_i}}\\left[\\log DI_i(G_i(\\mathbf{x}^{s_i}))]-\\mathbb{E}_{\\mathbf{x}^t \\sim \\mathbf{X}^t} [\\log(1 - DI_i(G_t(\\mathbf{x}^{t})))\\right]$$\n\nThe $\\mathcal{L}_{{adv}_{DI}}$ will update the parameter of $DI$, i.e. $\\Theta^{DI}$.\n\n-Equation 5 has been revised to:\n$$\\underset{\\Theta^{G_i},\\Theta^{G_t}}{\\mathcal{L}_{{adv}_G}} (\\mathbf{X}^{S_i}, \\mathbf{X}^T, DI_i) = -\n    \\mathbb{E}_{\\mathbf{x}^{s_i} \\sim \\mathbf{X}^{s_i}}[\\log DI_i(G_{i}(\\mathbf{x}^{s_i}))]-\n    \\mathbb{E}_{\\mathbf{x}^t \\sim \\mathbf{X}^t}[\\log DI_i(G_t(\\mathbf{x}^t))]$$\n\nThe $\\mathcal{L}_{{adv}_G}$ will update the parameter of $G_i$ and $G_t$, i.e. $\\Theta^{G_i}$ and $\\Theta^{G_t}$.\n\nThese two objectives are to train the domain classifier $DI$  and ($G_i$, $G_t$) in an adversarial manner: we first train $DI$ to identify which domain are the features come from, then we train the generator ($G_i$, $G_t$) to confuse the $DI$. The aim is to align the distributions of the features generated by ($G_i$, $G_t$).\n\n-Equation 6 has been revised to:\n$$\\underset{\\Theta^{G_i},\\Theta^{D_i},\\Theta^{C_i},\\Theta^{CI_i}}{\\mathcal{L}_{cross-entropy}} = -\\mathbb{E}_{(\\mathbf{x}^{s_i},\\mathbf{y}^{s_i})\\sim\\widehat{\\mathcal{D}}_{s_i}} \\sum_{k=1}^{K}{1} [k=\\mathbf{y}^{s_i}]log(C_i(f_{di}))\n    -\\mathbb{E}_{(\\mathbf{x}^{s_i},\\mathbf{y}^{s_i})\\sim\\widehat{\\mathcal{D}}_{s_i}} \\sum_{k=1}^{K}{1} [k=\\mathbf{y}^{s_i}]log(CI_i(f_{ds}))$$\n\nThe $\\mathcal{L}_{cross-entropy}$ is minimized over the parameters of the feature generator $G_i$, the disentangler $D_i$, the classifier $C$, and the class identifier $CI$, i.e. $\\Theta^{G_i}$, $\\Theta^{D_i}$, $\\Theta^{C}$, $\\Theta^{CI}$.  \n\n-Equation 7 has been revised to:\n$$\\underset{\\Theta^{D_i},\\Theta^{G_i}}{\\mathcal{L}_{ent}} = - \\frac{1}{N_{s_i}} \\sum_{j=1}^{N_{s_i}} \\log CI_i(f^j_{ds}) = - \\frac{1}{N_{s_i}} \\sum_{j=1}^{N_{s_i}} \\log CI_i(D_i(G_i(\\mathbf{x}^{s_i})))$$"}, "signatures": ["ICLR.cc/2020/Conference/Paper70/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper70/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Federated Adversarial Domain Adaptation", "authors": ["Xingchao Peng", "Zijun Huang", "Yizhe Zhu", "Kate Saenko"], "authorids": ["xpeng@bu.edu", "zijun.huang@columbia.edu", "yizhe.zhu@rutgers.edu", "saenko@bu.edu"], "keywords": ["Federated Learning", "Domain Adaptation", "Transfer Learning", "Feature Disentanglement"], "TL;DR": "we present a principled approach to the problem of federated domain adaptation, which aims to align the representations learned among the different nodes with the data distribution of the target node.", "abstract": "Federated learning improves data privacy and efficiency in machine learning performed over networks of distributed devices, such as mobile phones, IoT and wearable devices, etc. Yet models trained with federated learning can still fail to generalize to new devices due to the problem of domain shift. Domain shift occurs when the labeled data collected by source nodes statistically differs from the target node's unlabeled data. In this work, we present a principled approach to the problem of federated domain adaptation, which aims to align the representations learned among the different nodes with the data distribution of the target node. Our approach extends adversarial adaptation techniques to the constraints of the federated setting. In addition, we devise a dynamic attention mechanism and leverage feature disentanglement to enhance knowledge transfer. Empirically, we perform extensive experiments on several image and text classification tasks and show promising results under unsupervised federated domain adaptation setting.", "pdf": "/pdf/0a10e9439edcc36581111096b67db724743508d3.pdf", "code": "https://drive.google.com/file/d/1OekTpqB6qLfjlE2XUjQPm3F110KDMFc0/view?usp=sharing", "paperhash": "peng|federated_adversarial_domain_adaptation", "_bibtex": "@inproceedings{\nPeng2020Federated,\ntitle={Federated Adversarial Domain Adaptation},\nauthor={Xingchao Peng and Zijun Huang and Yizhe Zhu and Kate Saenko},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJezF3VYPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/32e298a2dabba1d70bd3fe0b5e2fd117f1d667a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJezF3VYPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper70/Authors", "ICLR.cc/2020/Conference/Paper70/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper70/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper70/Reviewers", "ICLR.cc/2020/Conference/Paper70/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper70/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper70/Authors|ICLR.cc/2020/Conference/Paper70/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176831, "tmdate": 1576860532538, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper70/Authors", "ICLR.cc/2020/Conference/Paper70/Reviewers", "ICLR.cc/2020/Conference/Paper70/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper70/-/Official_Comment"}}}, {"id": "S1g6PH4Yir", "original": null, "number": 3, "cdate": 1573631332881, "ddate": null, "tcdate": 1573631332881, "tmdate": 1573691187257, "tddate": null, "forum": "HJezF3VYPB", "replyto": "SJgnuHj6KB", "invitation": "ICLR.cc/2020/Conference/Paper70/-/Official_Comment", "content": {"title": "Algorithm in Sec 4 clarified, complexity analysis given, typos and unresolved reference issues fixed", "comment": "We thank the reviewer for the overall positive feedback on our paper. We have updated the details of the algorithm in section 4. \n\n1. The proposed algorithm is not described very clearly in Section 4. \n\nThank you for pointing this out.  We admit that the notations in the Federated Adversarial Alignment and Representation Disentanglement part are unclear, which has also been mentioned by Reviewer #1. We want to mention the following response is the same with the response to Reviewer #1, 2nd concerns.\n\nWe have revised the paper as follows to make it more clear (refer to the revision paper):  \ni) we standardized the notation occurred in the paper and made a table to explain each notation in the supplementary material (Table 6).\nii) we update Figure1(b) and Algorithm 1 to make the notations consistent and clear. \niii) we edit Equation 4~7 to clarify which parameters will the loss optimizes on.\niv) we have added high-level intuition to our paper to help readers better understand the method section.\n\n-In summary, we clarify the notation in the Federated Adversarial Alignment and Representation Disentanglement as follows:\n\n$DI$        |   domain identifier    | align the domain pair ($G_i$,$G_t$)\n$D$          |   disentangler             | disentangle the feature to domain-invariant feature $f_{di}$ and domain-specific feature $f_{ds}$.\n$C$          |   classifier                    | predict the labels for input features\n$CI$         |   class identify           | first trained with task loss, then train with the adversarial loss to extract domain-specific features. \n$G_i$      |feature generator       | extract features for $\\mathcal{D}_{S_i}$\n$G_t$      | feature generator      | extract features for ${\\mathcal{D}}_t$ \n$M$          | MI estimator               | estimate the mutual information between $f_{di}$ and $f_{ds}$.\n\n-Equation 4 has been revised to:\n$$\\underset{\\Theta^{DI_i}}{\\mathcal{L}_{{adv}_{DI_i}}}(\\mathbf{X}^{S_i}, \\mathbf{X}^T, G_i, G_t)=- \\mathbb{E}_{\\mathbf{x}^{s_i} \\sim \\mathbf{X}^{s_i}}\\left[\\log DI_i(G_i(\\mathbf{x}^{s_i}))]-\\mathbb{E}_{\\mathbf{x}^t \\sim \\mathbf{X}^t} [\\log(1 - DI_i(G_t(\\mathbf{x}^{t})))\\right]$$\n\nThe $\\mathcal{L}_{{adv}_{DI}}$ will update the parameter of $DI$, i.e. $\\Theta^{DI}$.\n\n-Equation 5 has been revised to:\n$$\\underset{\\Theta^{G_i},\\Theta^{G_t}}{\\mathcal{L}_{{adv}_G}} (\\mathbf{X}^{S_i}, \\mathbf{X}^T, DI_i) = -\n    \\mathbb{E}_{\\mathbf{x}^{s_i} \\sim \\mathbf{X}^{s_i}}[\\log DI_i(G_{i}(\\mathbf{x}^{s_i}))]-\n    \\mathbb{E}_{\\mathbf{x}^t \\sim \\mathbf{X}^t}[\\log DI_i(G_t(\\mathbf{x}^t))]$$\n\nThe $\\mathcal{L}_{{adv}_G}$ will update the parameter of $G_i$ and $G_t$, i.e. $\\Theta^{G_i}$ and $\\Theta^{G_t}$.\n\nThese two objectives are to train the domain classifier $DI$  and ($G_i$, $G_t$) in an adversarial manner: we first train $DI$ to identify which domain are the features come from, then we train the generator ($G_i$, $G_t$) to confuse the $DI$. The aim is to align the distributions of the features generated by ($G_i$, $G_t$).\n\n-Equation 6 has been revised to:\n$$\\underset{\\Theta^{G_i},\\Theta^{D_i},\\Theta^{C_i},\\Theta^{CI_i}}{\\mathcal{L}_{cross-entropy}} = -\\mathbb{E}_{(\\mathbf{x}^{s_i},\\mathbf{y}^{s_i})\\sim\\widehat{\\mathcal{D}}_{s_i}} \\sum_{k=1}^{K}1 [k=\\mathbf{y}^{s_i}]log(C_i(f_{di}))\n    -\\mathbb{E}_{(\\mathbf{x}^{s_i},\\mathbf{y}^{s_i})\\sim\\widehat{\\mathcal{D}}_{s_i}} \\sum_{k=1}^{K}1 [k=\\mathbf{y}^{s_i}]log(CI_i(f_{ds}))$$\n\nThe $\\mathcal{L}_{cross-entropy}$ is minimized over the parameters of the feature generator $G_i$, the disentangler $D_i$, the classifier $C$, and the class identifier $CI$, i.e. $\\Theta^{G_i}$, $\\Theta^{D_i}$, $\\Theta^{C}$, $\\Theta^{CI}$.  \n\n-Equation 7 has been revised to:\n$$\\underset{\\Theta^{D_i},\\Theta^{G_i}}{\\mathcal{L}_{ent}} = - \\frac{1}{N_{s_i}} \\sum_{j=1}^{N_{s_i}} \\log CI_i(f^j_{ds}) = - \\frac{1}{N_{s_i}} \\sum_{j=1}^{N_{s_i}} \\log CI_i(D_i(G_i(\\mathbf{x}^{s_i})))$$\n\nThe ${\\mathcal{L}_{ent}}$ is minimized over the parameters of the feature generator $G_i$, the disentangler $D_i$, i.e. $\\Theta^{G_i}$, $\\Theta^{D_i}$.\nFeature disentanglement facilitates the knowledge transfer by extracting  $f_{di}$ and dispelling $f_{ds}$\n\n2. Complexity analysis of the proposed algorithm.\n\n-From Algorithm 1, we observe that the four processes, i.e. feature extraction, domain alignment, domain disentangle, and mutual information minimization are linear with N, where N is the number of source domains. Thus, the time complexity of operations on deep neural networks is O(N).\n\n-The most time-consuming part of the dynamic attention model is the KMeans algorithm. The average complexity is given by $O(knT)$, where $k$ is the number of clusters, $n$ is the number of samples from the target domain, and $T$ is the number of iteration for the KMeans algorithm. Take digit experiment as an example, k=10, n=128, T=1000, it takes 0.21 second.\n\n3. Typos and unresolved reference issues\n\nWe have revised the typos unresolved reference in the updated pdf submission."}, "signatures": ["ICLR.cc/2020/Conference/Paper70/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper70/Authors", "ICLR.cc/2020/Conference/Paper70/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper70/Reviewers", "ICLR.cc/2020/Conference/Paper70/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper70/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Federated Adversarial Domain Adaptation", "authors": ["Xingchao Peng", "Zijun Huang", "Yizhe Zhu", "Kate Saenko"], "authorids": ["xpeng@bu.edu", "zijun.huang@columbia.edu", "yizhe.zhu@rutgers.edu", "saenko@bu.edu"], "keywords": ["Federated Learning", "Domain Adaptation", "Transfer Learning", "Feature Disentanglement"], "TL;DR": "we present a principled approach to the problem of federated domain adaptation, which aims to align the representations learned among the different nodes with the data distribution of the target node.", "abstract": "Federated learning improves data privacy and efficiency in machine learning performed over networks of distributed devices, such as mobile phones, IoT and wearable devices, etc. Yet models trained with federated learning can still fail to generalize to new devices due to the problem of domain shift. Domain shift occurs when the labeled data collected by source nodes statistically differs from the target node's unlabeled data. In this work, we present a principled approach to the problem of federated domain adaptation, which aims to align the representations learned among the different nodes with the data distribution of the target node. Our approach extends adversarial adaptation techniques to the constraints of the federated setting. In addition, we devise a dynamic attention mechanism and leverage feature disentanglement to enhance knowledge transfer. Empirically, we perform extensive experiments on several image and text classification tasks and show promising results under unsupervised federated domain adaptation setting.", "pdf": "/pdf/0a10e9439edcc36581111096b67db724743508d3.pdf", "code": "https://drive.google.com/file/d/1OekTpqB6qLfjlE2XUjQPm3F110KDMFc0/view?usp=sharing", "paperhash": "peng|federated_adversarial_domain_adaptation", "_bibtex": "@inproceedings{\nPeng2020Federated,\ntitle={Federated Adversarial Domain Adaptation},\nauthor={Xingchao Peng and Zijun Huang and Yizhe Zhu and Kate Saenko},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJezF3VYPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/32e298a2dabba1d70bd3fe0b5e2fd117f1d667a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJezF3VYPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper70/Authors", "ICLR.cc/2020/Conference/Paper70/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper70/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper70/Reviewers", "ICLR.cc/2020/Conference/Paper70/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper70/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper70/Authors|ICLR.cc/2020/Conference/Paper70/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176831, "tmdate": 1576860532538, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper70/Authors", "ICLR.cc/2020/Conference/Paper70/Reviewers", "ICLR.cc/2020/Conference/Paper70/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper70/-/Official_Comment"}}}, {"id": "SJgnuHj6KB", "original": null, "number": 1, "cdate": 1571825011768, "ddate": null, "tcdate": 1571825011768, "tmdate": 1572972642388, "tddate": null, "forum": "HJezF3VYPB", "replyto": "HJezF3VYPB", "invitation": "ICLR.cc/2020/Conference/Paper70/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposed and studied the unsupervised federated domain adaption problem, which aims to transfer knowledge from source nodes to a new node with different data distribution. To address the problem, a federated adversarial domain adaption (FADA) algorithm is introduced in the paper. The key idea of the algorithm is to update the target model by aggregating the gradients from source nodes, and also leverage adversarial adaption techniques to reduce the discrepancy between source features and target features. Overall, the problem studied in the paper is interesting, theoretical analysis on the error bound is provided in the paper, and the effectiveness of the proposed method has been validated in various datasets. Although the technical contributions of the paper are solid, I still have several concerns about it.\n1. The proposed algorithm is not described very clearly in section 4. According to the paper, DI is used to identify the domain from the output of Gi and Gt and align the features from those domains, then how is it related to the disentanglement in Eq 6. Also in Eq 6, symbol C_s was not introduced in the previous context, which makes it confusing to understand this objective.\n\n2. It would be better if the author(s) can provide some complexity analysis of the proposed algorithm.\n\n3. The paper still contains some typos and unresolved reference issues. "}, "signatures": ["ICLR.cc/2020/Conference/Paper70/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper70/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Federated Adversarial Domain Adaptation", "authors": ["Xingchao Peng", "Zijun Huang", "Yizhe Zhu", "Kate Saenko"], "authorids": ["xpeng@bu.edu", "zijun.huang@columbia.edu", "yizhe.zhu@rutgers.edu", "saenko@bu.edu"], "keywords": ["Federated Learning", "Domain Adaptation", "Transfer Learning", "Feature Disentanglement"], "TL;DR": "we present a principled approach to the problem of federated domain adaptation, which aims to align the representations learned among the different nodes with the data distribution of the target node.", "abstract": "Federated learning improves data privacy and efficiency in machine learning performed over networks of distributed devices, such as mobile phones, IoT and wearable devices, etc. Yet models trained with federated learning can still fail to generalize to new devices due to the problem of domain shift. Domain shift occurs when the labeled data collected by source nodes statistically differs from the target node's unlabeled data. In this work, we present a principled approach to the problem of federated domain adaptation, which aims to align the representations learned among the different nodes with the data distribution of the target node. Our approach extends adversarial adaptation techniques to the constraints of the federated setting. In addition, we devise a dynamic attention mechanism and leverage feature disentanglement to enhance knowledge transfer. Empirically, we perform extensive experiments on several image and text classification tasks and show promising results under unsupervised federated domain adaptation setting.", "pdf": "/pdf/0a10e9439edcc36581111096b67db724743508d3.pdf", "code": "https://drive.google.com/file/d/1OekTpqB6qLfjlE2XUjQPm3F110KDMFc0/view?usp=sharing", "paperhash": "peng|federated_adversarial_domain_adaptation", "_bibtex": "@inproceedings{\nPeng2020Federated,\ntitle={Federated Adversarial Domain Adaptation},\nauthor={Xingchao Peng and Zijun Huang and Yizhe Zhu and Kate Saenko},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJezF3VYPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/32e298a2dabba1d70bd3fe0b5e2fd117f1d667a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJezF3VYPB", "replyto": "HJezF3VYPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper70/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper70/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576237330075, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper70/Reviewers"], "noninvitees": [], "tcdate": 1570237757517, "tmdate": 1576237330086, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper70/-/Official_Review"}}}, {"id": "r1xvdmZJ5B", "original": null, "number": 2, "cdate": 1571914606680, "ddate": null, "tcdate": 1571914606680, "tmdate": 1572972642344, "tddate": null, "forum": "HJezF3VYPB", "replyto": "HJezF3VYPB", "invitation": "ICLR.cc/2020/Conference/Paper70/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors present a novel algorithm for dealing with domain adaptation in the setting of federated learning (classification, specifically). That is, they tackle the issue of learning a model on a new domain when access to the data points used in training the source models is not possible due to privacy constraints. The approach uses the gradients of the source models, reweighed to account for the differing shifts between the different sources and the target domain, to fit the model on the target domain.\n\nThe authors motivate their approach by providing a novel bound on the generalization error of transfer learning when the hypothesis function used on the target domain is a convex combination of hypotheses fitted on multiple source domains. This bound shows that the weighted sum of divergences in the symmetric difference hypothesis space controls the generalization error, so the authors aim at deriving feature representations and using aggregation weights that ensure this weighted sum is small. \n\nThe authors use a novel dynamic attention model to get the aggregation weights: they cluster the features in the target domain, and measure how much the intra-cluster variation decreases when information from a given source domain is incorporated. The aggregation weights for the model updates on the target domain are then weighed using a softmax transform of these contribution weights. \n \nThe motivation up through and including section 3 is clear, the theoretical results are presented clearly, but the model details in section 4 are unclear:\n-  In the dynamic attention mechanism, how does one a priori choose the number of clusters in computing the gap statistics, and what is the impact? \n- the notation in the federated adversarial alignment section is unclear: what *exactly* are the model coefficients Theta that are being updated?\n- the statement \"optimize following objective\" is made several times. this is ambiguous, and should be corrected to \"miminize\" following objective.\n- the representation disentanglement process is intricate, and only vaguely addressed. how does one fit the neural net and use (8)? where is the l2 reconstruction loss balanced with the mutual information? the vagueness of this section means Algorithm 1 is not well-specified.\n\nThe experiments are reasonable, and compare to baseline domain adaptation methods.\n\nThe problem considered is of interest, and the approach is novel and interesting. However, the algorithm is not described in sufficient detail. After reading the paper, and spending considerable time rereading section 4, I still do not understand how Algorithm 1 is implemented in practice. For that reason I lean towards reject. I will update my score if the authors clarify the details of Algorithm 1. \n\nComments:\n- the symmetric difference hypothesis space is incorrectly called the HdeltaH divergence in section 3\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper70/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper70/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Federated Adversarial Domain Adaptation", "authors": ["Xingchao Peng", "Zijun Huang", "Yizhe Zhu", "Kate Saenko"], "authorids": ["xpeng@bu.edu", "zijun.huang@columbia.edu", "yizhe.zhu@rutgers.edu", "saenko@bu.edu"], "keywords": ["Federated Learning", "Domain Adaptation", "Transfer Learning", "Feature Disentanglement"], "TL;DR": "we present a principled approach to the problem of federated domain adaptation, which aims to align the representations learned among the different nodes with the data distribution of the target node.", "abstract": "Federated learning improves data privacy and efficiency in machine learning performed over networks of distributed devices, such as mobile phones, IoT and wearable devices, etc. Yet models trained with federated learning can still fail to generalize to new devices due to the problem of domain shift. Domain shift occurs when the labeled data collected by source nodes statistically differs from the target node's unlabeled data. In this work, we present a principled approach to the problem of federated domain adaptation, which aims to align the representations learned among the different nodes with the data distribution of the target node. Our approach extends adversarial adaptation techniques to the constraints of the federated setting. In addition, we devise a dynamic attention mechanism and leverage feature disentanglement to enhance knowledge transfer. Empirically, we perform extensive experiments on several image and text classification tasks and show promising results under unsupervised federated domain adaptation setting.", "pdf": "/pdf/0a10e9439edcc36581111096b67db724743508d3.pdf", "code": "https://drive.google.com/file/d/1OekTpqB6qLfjlE2XUjQPm3F110KDMFc0/view?usp=sharing", "paperhash": "peng|federated_adversarial_domain_adaptation", "_bibtex": "@inproceedings{\nPeng2020Federated,\ntitle={Federated Adversarial Domain Adaptation},\nauthor={Xingchao Peng and Zijun Huang and Yizhe Zhu and Kate Saenko},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJezF3VYPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/32e298a2dabba1d70bd3fe0b5e2fd117f1d667a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJezF3VYPB", "replyto": "HJezF3VYPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper70/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper70/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576237330075, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper70/Reviewers"], "noninvitees": [], "tcdate": 1570237757517, "tmdate": 1576237330086, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper70/-/Official_Review"}}}, {"id": "SkgFvPsUtr", "original": null, "number": 2, "cdate": 1571366753301, "ddate": null, "tcdate": 1571366753301, "tmdate": 1571366753301, "tddate": null, "forum": "HJezF3VYPB", "replyto": "rkefDBWBtH", "invitation": "ICLR.cc/2020/Conference/Paper70/-/Official_Comment", "content": {"comment": "Thank you for your reply. \n\nThe feature disentanglement enables G_i to extract the domain-invariant features and dispelling the domain-specific features. The ultimate goal of employing feature disentanglement is to render a better G_i, and essentially, better G_t, since the gradient of G_t is the aggregation of gradient generated by each G_i. Note the task classifier is trained only on the domain-invariant features.   \n\nThe effect of feature disentanglement is not achieved by DI. DI is utilized to classify the features, i.e. to determine whether a feature vector is sampled from the source distribution or target distribution. By confusing the DI, we are aiming to align the source features with the target features. \n\nThanks again for your reply!", "title": "The benefit of feature disentanglement is not achieved by DI"}, "signatures": ["ICLR.cc/2020/Conference/Paper70/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper70/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Federated Adversarial Domain Adaptation", "authors": ["Xingchao Peng", "Zijun Huang", "Yizhe Zhu", "Kate Saenko"], "authorids": ["xpeng@bu.edu", "zijun.huang@columbia.edu", "yizhe.zhu@rutgers.edu", "saenko@bu.edu"], "keywords": ["Federated Learning", "Domain Adaptation", "Transfer Learning", "Feature Disentanglement"], "TL;DR": "we present a principled approach to the problem of federated domain adaptation, which aims to align the representations learned among the different nodes with the data distribution of the target node.", "abstract": "Federated learning improves data privacy and efficiency in machine learning performed over networks of distributed devices, such as mobile phones, IoT and wearable devices, etc. Yet models trained with federated learning can still fail to generalize to new devices due to the problem of domain shift. Domain shift occurs when the labeled data collected by source nodes statistically differs from the target node's unlabeled data. In this work, we present a principled approach to the problem of federated domain adaptation, which aims to align the representations learned among the different nodes with the data distribution of the target node. Our approach extends adversarial adaptation techniques to the constraints of the federated setting. In addition, we devise a dynamic attention mechanism and leverage feature disentanglement to enhance knowledge transfer. Empirically, we perform extensive experiments on several image and text classification tasks and show promising results under unsupervised federated domain adaptation setting.", "pdf": "/pdf/0a10e9439edcc36581111096b67db724743508d3.pdf", "code": "https://drive.google.com/file/d/1OekTpqB6qLfjlE2XUjQPm3F110KDMFc0/view?usp=sharing", "paperhash": "peng|federated_adversarial_domain_adaptation", "_bibtex": "@inproceedings{\nPeng2020Federated,\ntitle={Federated Adversarial Domain Adaptation},\nauthor={Xingchao Peng and Zijun Huang and Yizhe Zhu and Kate Saenko},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJezF3VYPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/32e298a2dabba1d70bd3fe0b5e2fd117f1d667a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJezF3VYPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper70/Authors", "ICLR.cc/2020/Conference/Paper70/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper70/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper70/Reviewers", "ICLR.cc/2020/Conference/Paper70/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper70/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper70/Authors|ICLR.cc/2020/Conference/Paper70/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176831, "tmdate": 1576860532538, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper70/Authors", "ICLR.cc/2020/Conference/Paper70/Reviewers", "ICLR.cc/2020/Conference/Paper70/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper70/-/Official_Comment"}}}, {"id": "rkefDBWBtH", "original": null, "number": 2, "cdate": 1571259738209, "ddate": null, "tcdate": 1571259738209, "tmdate": 1571259738209, "tddate": null, "forum": "HJezF3VYPB", "replyto": "B1eQ0FpmYB", "invitation": "ICLR.cc/2020/Conference/Paper70/-/Public_Comment", "content": {"comment": "Thanks for your reply and I am still confused as the following.\n\nG_i includes both the domain-invariant and domain specific information. To enable the domain adaptation, the goal is to make DI cannot tell G_i and G_t. \nIf G_i includes both information, how could the Feature disentanglement facilitate the knowledge transfer. I am confused about it. In the experiment, I have seen that the experimental part proves that using feature disentanglement  can achieve the best performance. \n\nI just don't understand why since it lacks good theoretical explanation about it. The thing makes me confused is that even though the features from G_i are disentangled, but you still use both features and how could the DI tell if the feature are disentangled or not?\n\nThanks again for your reply.", "title": "Thanks for your reply"}, "signatures": ["~Stone_Jamess1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Stone_Jamess1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Federated Adversarial Domain Adaptation", "authors": ["Xingchao Peng", "Zijun Huang", "Yizhe Zhu", "Kate Saenko"], "authorids": ["xpeng@bu.edu", "zijun.huang@columbia.edu", "yizhe.zhu@rutgers.edu", "saenko@bu.edu"], "keywords": ["Federated Learning", "Domain Adaptation", "Transfer Learning", "Feature Disentanglement"], "TL;DR": "we present a principled approach to the problem of federated domain adaptation, which aims to align the representations learned among the different nodes with the data distribution of the target node.", "abstract": "Federated learning improves data privacy and efficiency in machine learning performed over networks of distributed devices, such as mobile phones, IoT and wearable devices, etc. Yet models trained with federated learning can still fail to generalize to new devices due to the problem of domain shift. Domain shift occurs when the labeled data collected by source nodes statistically differs from the target node's unlabeled data. In this work, we present a principled approach to the problem of federated domain adaptation, which aims to align the representations learned among the different nodes with the data distribution of the target node. Our approach extends adversarial adaptation techniques to the constraints of the federated setting. In addition, we devise a dynamic attention mechanism and leverage feature disentanglement to enhance knowledge transfer. Empirically, we perform extensive experiments on several image and text classification tasks and show promising results under unsupervised federated domain adaptation setting.", "pdf": "/pdf/0a10e9439edcc36581111096b67db724743508d3.pdf", "code": "https://drive.google.com/file/d/1OekTpqB6qLfjlE2XUjQPm3F110KDMFc0/view?usp=sharing", "paperhash": "peng|federated_adversarial_domain_adaptation", "_bibtex": "@inproceedings{\nPeng2020Federated,\ntitle={Federated Adversarial Domain Adaptation},\nauthor={Xingchao Peng and Zijun Huang and Yizhe Zhu and Kate Saenko},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJezF3VYPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/32e298a2dabba1d70bd3fe0b5e2fd117f1d667a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJezF3VYPB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504214377, "tmdate": 1576860566184, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper70/Authors", "ICLR.cc/2020/Conference/Paper70/Reviewers", "ICLR.cc/2020/Conference/Paper70/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper70/-/Public_Comment"}}}, {"id": "B1eQ0FpmYB", "original": null, "number": 1, "cdate": 1571178954864, "ddate": null, "tcdate": 1571178954864, "tmdate": 1571178954864, "tddate": null, "forum": "HJezF3VYPB", "replyto": "B1x3jxPfYH", "invitation": "ICLR.cc/2020/Conference/Paper70/-/Official_Comment", "content": {"comment": "Thank you for your comment!\n\nIn this paper, we propose a new scheme called Federated Adversarial Domain Adaptation (FADA) to address the domain shift in the distributed learning system, aiming to tackle the domain shift in the federated learning system by adversarial training. Federated learning improves data privacy and efficiency in machine learning performed over networks of distributed devices. However, existing federated work mainly focuses on data communication, data encryption, and training strategy for federated systems. To the best of our knowledge,  we are the first to propose a solid deep learning framework to tackle the domain shift between different nodes on the federated system. \n\nTo answer your question:\n1).  In Figure 1, the above CI is used to perform the recognition task (i.e. in image recognition task, it predicts the correct labels for the images) and it's trained with task loss ONLY! The below CI is utilized to enhance the disentanglement and it will be trained with cross-entropy loss and adversarial loss (in this paper we use entropy loss) iteratively.\n\n2).  The output for G_i and G_t are the features for i-th source domain and target domain, respectively. \nIn Equation 4, G_t and G_t extract features for the input data x^{s_i} and x^t.\n\n3)  By disentangling the features, we are aiming to separate the deep features to domain-invariant features and domain-specific features. For example, if we forward an image (foreground: car || background: buildings, pedestrians || label: car) to a deep model, the network will encode the information from the foreground car and background buildings and pedestrians simultaneously. However, in the transfer learning task (car images in daylight -> car images at night ), only the information from the car is beneficial for the transfer task. \n\nWe define the information which benefits the transfer task as domain-invariant features and the information which hampers or does not contribute to the transfer task as domain-specific features. \n\nFeature disentanglement can facilitate the knowledge transfer as it distills the key information for the transfer task. \n\n", "title": "Feature disentanglement facilitates the knowledge transfer by distilling out the key information"}, "signatures": ["ICLR.cc/2020/Conference/Paper70/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper70/Authors", "ICLR.cc/2020/Conference/Paper70/Reviewers", "ICLR.cc/2020/Conference/Paper70/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper70/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Federated Adversarial Domain Adaptation", "authors": ["Xingchao Peng", "Zijun Huang", "Yizhe Zhu", "Kate Saenko"], "authorids": ["xpeng@bu.edu", "zijun.huang@columbia.edu", "yizhe.zhu@rutgers.edu", "saenko@bu.edu"], "keywords": ["Federated Learning", "Domain Adaptation", "Transfer Learning", "Feature Disentanglement"], "TL;DR": "we present a principled approach to the problem of federated domain adaptation, which aims to align the representations learned among the different nodes with the data distribution of the target node.", "abstract": "Federated learning improves data privacy and efficiency in machine learning performed over networks of distributed devices, such as mobile phones, IoT and wearable devices, etc. Yet models trained with federated learning can still fail to generalize to new devices due to the problem of domain shift. Domain shift occurs when the labeled data collected by source nodes statistically differs from the target node's unlabeled data. In this work, we present a principled approach to the problem of federated domain adaptation, which aims to align the representations learned among the different nodes with the data distribution of the target node. Our approach extends adversarial adaptation techniques to the constraints of the federated setting. In addition, we devise a dynamic attention mechanism and leverage feature disentanglement to enhance knowledge transfer. Empirically, we perform extensive experiments on several image and text classification tasks and show promising results under unsupervised federated domain adaptation setting.", "pdf": "/pdf/0a10e9439edcc36581111096b67db724743508d3.pdf", "code": "https://drive.google.com/file/d/1OekTpqB6qLfjlE2XUjQPm3F110KDMFc0/view?usp=sharing", "paperhash": "peng|federated_adversarial_domain_adaptation", "_bibtex": "@inproceedings{\nPeng2020Federated,\ntitle={Federated Adversarial Domain Adaptation},\nauthor={Xingchao Peng and Zijun Huang and Yizhe Zhu and Kate Saenko},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJezF3VYPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/32e298a2dabba1d70bd3fe0b5e2fd117f1d667a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJezF3VYPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper70/Authors", "ICLR.cc/2020/Conference/Paper70/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper70/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper70/Reviewers", "ICLR.cc/2020/Conference/Paper70/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper70/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper70/Authors|ICLR.cc/2020/Conference/Paper70/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176831, "tmdate": 1576860532538, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper70/Authors", "ICLR.cc/2020/Conference/Paper70/Reviewers", "ICLR.cc/2020/Conference/Paper70/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper70/-/Official_Comment"}}}, {"id": "B1x3jxPfYH", "original": null, "number": 1, "cdate": 1571086500253, "ddate": null, "tcdate": 1571086500253, "tmdate": 1571086500253, "tddate": null, "forum": "HJezF3VYPB", "replyto": "HJezF3VYPB", "invitation": "ICLR.cc/2020/Conference/Paper70/-/Public_Comment", "content": {"comment": "I have read this paper and know the authors would like to apply domain adaptation in a federated manner.\nThe idea is quite straightforward that multiple users will adapt their knowledge to one target domain.\n\nI have the following questions.\n1.  I am confused by the right part of Figure 1.  There are two CI in the figure, and what is the difference?\n2. What is the output from G_i and G_t. In Equation (6), It says domain specific information is fed to DI but n Eq.(4), it says it is a local feature extractor. I am confused by the descriptions.\n3 What is the benefit of using feature disentanglement, and how could feature disentanglement be used to improve the performance. Can you explain why you use feature disentanglement.\n\n\nThanks very much for your help.", "title": "Can you explain why use the feature disentanglement"}, "signatures": ["~Stone_Jamess1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Stone_Jamess1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Federated Adversarial Domain Adaptation", "authors": ["Xingchao Peng", "Zijun Huang", "Yizhe Zhu", "Kate Saenko"], "authorids": ["xpeng@bu.edu", "zijun.huang@columbia.edu", "yizhe.zhu@rutgers.edu", "saenko@bu.edu"], "keywords": ["Federated Learning", "Domain Adaptation", "Transfer Learning", "Feature Disentanglement"], "TL;DR": "we present a principled approach to the problem of federated domain adaptation, which aims to align the representations learned among the different nodes with the data distribution of the target node.", "abstract": "Federated learning improves data privacy and efficiency in machine learning performed over networks of distributed devices, such as mobile phones, IoT and wearable devices, etc. Yet models trained with federated learning can still fail to generalize to new devices due to the problem of domain shift. Domain shift occurs when the labeled data collected by source nodes statistically differs from the target node's unlabeled data. In this work, we present a principled approach to the problem of federated domain adaptation, which aims to align the representations learned among the different nodes with the data distribution of the target node. Our approach extends adversarial adaptation techniques to the constraints of the federated setting. In addition, we devise a dynamic attention mechanism and leverage feature disentanglement to enhance knowledge transfer. Empirically, we perform extensive experiments on several image and text classification tasks and show promising results under unsupervised federated domain adaptation setting.", "pdf": "/pdf/0a10e9439edcc36581111096b67db724743508d3.pdf", "code": "https://drive.google.com/file/d/1OekTpqB6qLfjlE2XUjQPm3F110KDMFc0/view?usp=sharing", "paperhash": "peng|federated_adversarial_domain_adaptation", "_bibtex": "@inproceedings{\nPeng2020Federated,\ntitle={Federated Adversarial Domain Adaptation},\nauthor={Xingchao Peng and Zijun Huang and Yizhe Zhu and Kate Saenko},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJezF3VYPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/32e298a2dabba1d70bd3fe0b5e2fd117f1d667a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJezF3VYPB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504214377, "tmdate": 1576860566184, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper70/Authors", "ICLR.cc/2020/Conference/Paper70/Reviewers", "ICLR.cc/2020/Conference/Paper70/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper70/-/Public_Comment"}}}], "count": 14}