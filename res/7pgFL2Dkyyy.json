{"notes": [{"id": "7pgFL2Dkyyy", "original": "1sPKjtfKYs", "number": 2020, "cdate": 1601308222586, "ddate": null, "tcdate": 1601308222586, "tmdate": 1615985585795, "tddate": null, "forum": "7pgFL2Dkyyy", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Class Normalization for (Continual)? Generalized Zero-Shot Learning", "authorids": ["~Ivan_Skorokhodov1", "~Mohamed_Elhoseiny1"], "authors": ["Ivan Skorokhodov", "Mohamed Elhoseiny"], "keywords": ["zero-shot learning", "normalization", "continual learning", "initialization"], "abstract": "Normalization techniques have proved to be a crucial ingredient of successful training in a traditional supervised learning regime. However, in the zero-shot learning (ZSL) world, these ideas have received only marginal attention. This work studies normalization in ZSL scenario from both theoretical and practical perspectives. First, we give a theoretical explanation to two popular tricks used in zero-shot learning: normalize+scale and attributes normalization and show that they help training by preserving variance during a forward pass. Next, we demonstrate that they are insufficient to normalize a deep ZSL model and propose Class Normalization (CN): a normalization scheme, which alleviates this issue both provably and in practice. Third, we show that ZSL models typically have more irregular loss surface compared to traditional classifiers and that the proposed method partially remedies this problem. Then, we test our approach on 4 standard ZSL datasets and outperform sophisticated modern SotA with a simple MLP optimized without any bells and whistles and having ~50 times faster training speed. Finally, we generalize ZSL to a broader problem \u2014 continual ZSL, and introduce some principled metrics and rigorous baselines for this new setup. The source code is available at https://github.com/universome/class-norm.", "one-sentence_summary": "We develop theoretical understanding of signal normalization inside zero-shot learning models, propose a novel normalization scheme and use it to achieve SotA ZSL performance with a simple MLP", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "skorokhodov|class_normalization_for_continual_generalized_zeroshot_learning", "pdf": "/pdf/08846e7d24687d94ea8f98091ccf05e3f001d22a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nskorokhodov2021class,\ntitle={Class Normalization for (Continual)? Generalized Zero-Shot Learning},\nauthor={Ivan Skorokhodov and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7pgFL2Dkyyy}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 26, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "LjjeRZ9ZcGt", "original": null, "number": 1, "cdate": 1610040374050, "ddate": null, "tcdate": 1610040374050, "tmdate": 1610473965967, "tddate": null, "forum": "7pgFL2Dkyyy", "replyto": "7pgFL2Dkyyy", "invitation": "ICLR.cc/2021/Conference/Paper2020/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "This paper got mixed reviews. One for reject and three for acceptance. The reviewers and authors have extensive discussion. Authors also provided additional experiments for further clarifying some questions from the reviewers. The paper has some clarify issue in the theoretical justification part as pointed out by AR1. Authors should extensively improve this part or revise the statement. However, the method proposed in this paper is simple and the results are indeed good. This paper is valuable and should be shared within the community to advance research on ZSL. Therefore, AC recommends acceptance. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Class Normalization for (Continual)? Generalized Zero-Shot Learning", "authorids": ["~Ivan_Skorokhodov1", "~Mohamed_Elhoseiny1"], "authors": ["Ivan Skorokhodov", "Mohamed Elhoseiny"], "keywords": ["zero-shot learning", "normalization", "continual learning", "initialization"], "abstract": "Normalization techniques have proved to be a crucial ingredient of successful training in a traditional supervised learning regime. However, in the zero-shot learning (ZSL) world, these ideas have received only marginal attention. This work studies normalization in ZSL scenario from both theoretical and practical perspectives. First, we give a theoretical explanation to two popular tricks used in zero-shot learning: normalize+scale and attributes normalization and show that they help training by preserving variance during a forward pass. Next, we demonstrate that they are insufficient to normalize a deep ZSL model and propose Class Normalization (CN): a normalization scheme, which alleviates this issue both provably and in practice. Third, we show that ZSL models typically have more irregular loss surface compared to traditional classifiers and that the proposed method partially remedies this problem. Then, we test our approach on 4 standard ZSL datasets and outperform sophisticated modern SotA with a simple MLP optimized without any bells and whistles and having ~50 times faster training speed. Finally, we generalize ZSL to a broader problem \u2014 continual ZSL, and introduce some principled metrics and rigorous baselines for this new setup. The source code is available at https://github.com/universome/class-norm.", "one-sentence_summary": "We develop theoretical understanding of signal normalization inside zero-shot learning models, propose a novel normalization scheme and use it to achieve SotA ZSL performance with a simple MLP", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "skorokhodov|class_normalization_for_continual_generalized_zeroshot_learning", "pdf": "/pdf/08846e7d24687d94ea8f98091ccf05e3f001d22a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nskorokhodov2021class,\ntitle={Class Normalization for (Continual)? Generalized Zero-Shot Learning},\nauthor={Ivan Skorokhodov and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7pgFL2Dkyyy}\n}"}, "tags": [], "invitation": {"reply": {"forum": "7pgFL2Dkyyy", "replyto": "7pgFL2Dkyyy", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040374036, "tmdate": 1610473965951, "id": "ICLR.cc/2021/Conference/Paper2020/-/Decision"}}}, {"id": "ZZQfNsXYK2u", "original": null, "number": 22, "cdate": 1606280174909, "ddate": null, "tcdate": 1606280174909, "tmdate": 1606284269551, "tddate": null, "forum": "7pgFL2Dkyyy", "replyto": "81DHmgR514A", "invitation": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment", "content": {"title": "Additional response for Reviewer 1's concerns on F1, F2 and H1, H2, H4, H6 including updates (Sec 3.5 and results)", "comment": "> Another major criticism is also with respect to the clarity [...] For example, see F1, F2, H2.\n\nWe provided additional details about F1, F2 and other Reviewer 1\u2019s concerns in our [previous update](https://openreview.net/forum?id=7pgFL2Dkyyy&noteId=1UGLzHUd3vl) and rewritten this section and elaborated on H in the current one. If Reviewer 1 believes that there are other places to improve the clarity in, we would appreciate the pointers.\n\n> F1: How do the authors define \u201cirregular loss surface\u201d?\n\nWe borrow the definition of the loss surface smoothness from [4] and define it via Lipshitzness (see the newly added Section 3.5).\n\n> F2: [Our statement claiming that CN smoothes the surface] This is not very precise and seems unsupported. Please make it clear how. If this is a hypothesis, please make it clear. \n\nWe greatly elaborated on this point in Section 3.5 and Appendix F. To show that CN smoothes the loss surface, we first notice that the class standardization (eq 6) is equivalent to batch normalization with 1) scaling/bias removed and 2) applied across class-dimension on top of attribute representations. This makes it straightforward to apply Theorem 4.4 from [4] (see Appendix F). Hence, this claim holds rigorously, in contrast to our other claim about more irregular loss surface of ZSL embedders which we state only as a hypothesis (but also validate empirically). See Section 3.5 for the details.\n\n> Note that my concerns in H also weaken (ii) (see also the authors' responses to H1, H2, H4, H6).\n\nWe respectfully disagree and we believe they do not. We elaborate here on our answers on H1, H2, H4 and H6 and provide an additional context since we believe that our previous response on them produced a misunderstanding. As we understood it, Reviewer 1 is concerned about our claim that zero-mean and unit-variance assumptions do not apply to attribute vectors in ZSL, which motivated us to relax these assumptions for Statement 2. Reviewer 1\u2019s point is that one can enforce these assumptions by manually standardizing attributes to zero-mean and unit variance.\n\n> H1: Would this statement still be true after we transform a_c with an MLP?\n\nThe MLP outputs would have zero-mean and some variance  (maybe unit \u2014 depending on the initialization), but here we are delegating the job of proper initialization with respect to this MLP component instead of with respect to the attributes. And this defines our class normalization purpose which is about preserving the variance when a deep MLP is employed to transform the attributes.\n\n> H2: Why is [standardization] not \u201ca sensible thing to do\u201d if we just want zero mean and unit variance?\n\nManually standardizing the attributes would simplify the variance formula (6) but at the expense of deviating from the common practice [1,2] as we elaborated above. This would slightly change this formula (6) and now instead of multiplication by $E[\\|a\\|_2^2]$, we will have the multiplication by $d_a$ (attributes dimensionality) \u2014 we show this in Appendix H. What is important here is that we still cannot employ default Kaiming/Xavier scalings since they do not account for this additional $d_a$ factor. \n\nAdditional Experiments. To demonstrate this, we ran additional experiments and added the results that describe this case in Table 10: if one standardizes attributes to zero-mean and unit-variance instead of the normalization and employs the default (Kaiming) init, then this worsens the scores. But if we adjust for this transformation with the appropriate $1/d_a$ factor (equation 90 in Appendix H), then the performance restores (and even outperforms somewhere). The factor $1/d_a$ is derived as a variance-preserving factor (eq. 90), and since it resulted in a big influence on the scores (~20% improvement \u2014 see Table 10), it suggests that our hypothesis that improving the variance improves the performance is trustworthy. \n\n> H4: What if these are things like word embeddings?\n\nAs we said in our previous response, we believe that word embeddings should follow the normal distribution but this does not invalidate the analysis: if they follow it, then we need to apply initialization formula (90), if they don\u2019t \u2014 formula (6) which covers a more general case (it is derived with the relaxed assumptions). \n\n> H6: Histograms in Fig. 13 look quite normal.\n\nWe respectfully disagree and they are not normal objectively speaking,  as we said in our previous response. To verify that, we used [D\u2019Agostino test](https://en.wikipedia.org/wiki/D%27Agostino%27s_K-squared_test) which confirmed our point with a very high confidence (p-value < 0.005 for SUN/CUB/aPY and < 0.05 for AwA). This shows that with high confidence, zero-mean and unit-variance assumptions do not hold for SUN/CUB/aPY/AwA attributes\n\n- [1] https://arxiv.org/abs/1603.00550\n- [2] https://arxiv.org/abs/1804.09458\n- [3] https://arxiv.org/abs/1909.05995\n- [4] https://arxiv.org/abs/1805.11604\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2020/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Class Normalization for (Continual)? Generalized Zero-Shot Learning", "authorids": ["~Ivan_Skorokhodov1", "~Mohamed_Elhoseiny1"], "authors": ["Ivan Skorokhodov", "Mohamed Elhoseiny"], "keywords": ["zero-shot learning", "normalization", "continual learning", "initialization"], "abstract": "Normalization techniques have proved to be a crucial ingredient of successful training in a traditional supervised learning regime. However, in the zero-shot learning (ZSL) world, these ideas have received only marginal attention. This work studies normalization in ZSL scenario from both theoretical and practical perspectives. First, we give a theoretical explanation to two popular tricks used in zero-shot learning: normalize+scale and attributes normalization and show that they help training by preserving variance during a forward pass. Next, we demonstrate that they are insufficient to normalize a deep ZSL model and propose Class Normalization (CN): a normalization scheme, which alleviates this issue both provably and in practice. Third, we show that ZSL models typically have more irregular loss surface compared to traditional classifiers and that the proposed method partially remedies this problem. Then, we test our approach on 4 standard ZSL datasets and outperform sophisticated modern SotA with a simple MLP optimized without any bells and whistles and having ~50 times faster training speed. Finally, we generalize ZSL to a broader problem \u2014 continual ZSL, and introduce some principled metrics and rigorous baselines for this new setup. The source code is available at https://github.com/universome/class-norm.", "one-sentence_summary": "We develop theoretical understanding of signal normalization inside zero-shot learning models, propose a novel normalization scheme and use it to achieve SotA ZSL performance with a simple MLP", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "skorokhodov|class_normalization_for_continual_generalized_zeroshot_learning", "pdf": "/pdf/08846e7d24687d94ea8f98091ccf05e3f001d22a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nskorokhodov2021class,\ntitle={Class Normalization for (Continual)? Generalized Zero-Shot Learning},\nauthor={Ivan Skorokhodov and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7pgFL2Dkyyy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "7pgFL2Dkyyy", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2020/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2020/Authors|ICLR.cc/2021/Conference/Paper2020/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853146, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment"}}}, {"id": "wez74wda7Lr", "original": null, "number": 23, "cdate": 1606280337357, "ddate": null, "tcdate": 1606280337357, "tmdate": 1606281891418, "tddate": null, "forum": "7pgFL2Dkyyy", "replyto": "81DHmgR514A", "invitation": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment", "content": {"title": "Elaborating on some misunderstandings", "comment": "We thank Reviewer1 for the response and we hope our detailed responses and additional experiments are showing that we are doing what is in our hands to both clarify a major misunderstanding and also to apply actionable feedback based on the right ground. If that\u2019s alright, we hope to clarify to R1 what made us deeply concerned about his/her judgment. We of course aim to have the best version of our paper but without having our key message or contributions distorted or have much deviation from it. The reason why we think we may not on the right ground is for example the quote below that mentions a claim, we think we did not make. \n\nExample claims R1 attributed to our paper that we believe we did not make. \n> The authors again claim that the main bottleneck in improving zero-shot learning is \u201cvariance control\u201d (the end of Sect. 3.3). \n\nIn our paper, we employed a hypothesis that \u201cimproving\u201d the variance improves ZSL performance. This is not a new observation looking at the wider circle of supervised learning. In the context of ZSL, it is less studied and understood and that motivated our work. Following this hypothesis, we derived CN to improve the variance which in turn improved the performance. This suggests the hypothesis to be trustworthy and this is why we allowed ourselves to say that  \u201c[AN/NS/CN] help training by preserving variance during a forward pass\u201d.\n\n> In order to judge the significance of the contribution \u201c[...]\", I think one could use (i) [\u2026] (ii) [...] (iii) [...]\n\nFirst, we believe that \u201c(i) difficulty to prove\u201d should not influence a statement's significance. Second, we believe that apart from (ii, iii), there is a more important point to value a statement\u2019s significance:\n\n- (iv) how much insight it provides to build stronger algorithms\n\nThis motivated us to build a model that is \n1) very simple, and \n2) beats the modern SotA in terms of both scores and training speed.\n\nThis is why we believe that (iv) very well applies to our work.\n\n> [...] Figures 1, 4, 5, 6, 7 and the tables shows (ii) but provides only weak empirical evidence to support (iii) (authors disagree).\n\nWe are glad that Reviewer 1 confirmed that (ii) is shown in the paper. As to (iii), we respectfully disagree with their judgement, and our reasoning is as follows. \n- `X leads to Y (from theory)`: Our theoretical statements demonstrate how AN/NS (i.e. `X`) influence the variance (i.e. `Y`). We validate them in practice in order to confirm that the assumptions are reasonable \u2014 see Figures 1,5,6,7,8.\n- `X leads to Z (from extensive experiments and analysis)`: Experiments done by us and [1,2] demonstrate that AN/NS leads to better performance (see below). To confirm this even better, we ran additional experiments for 1 and 2-layer MLP and CVC-ZSL [3] (see below).\n\nNow, the above two points implies correlation between `Y` and `Z`: when we enable AN/NS then both the variance and the performance improves, when we disable them \u2014 both worsens. Hence, the correlation.\n\nShowing `Y leads to Z` *directly* is out of reach from both theoretical and empirical perspectives:\n- Theoretical difficulties are described in one of our [previous responses](https://openreview.net/forum?id=7pgFL2Dkyyy&noteId=8MNECymkktX).\n- Practical difficulties arise because there is no clear way of how to introduce a change in the variance without changing the architecture/parametrization/optimization process. Even the simple tricks we consider in the work influence not only the variance, but also other things as well (for example, NS makes logit scores to be computed based only on vectors directions, AN reduces the gradient scale of the first layer, CN influences the loss landscape, etc.). To the best of our knowledge, there is no existing way to isolate the variance influence.\n\n> Further, the tricks have been shown to work only on one zero-shot learning MLP baseline in this paper (the authors believe this is enough)\n\n[Existing Literature] Their importance for existing methods can be found in the related papers which we discuss in Section 2, paragraph 2. For example, [1] improved on AwA/CUB by 10/20% using AN; [2] improved their model by 10-15% using NS. Also, [3] motivated the use of NS by ``variance reduction\u2019\u2019 (see Section 3.1 of [3]), but didn\u2019t provide any further exploration on that (we developed this point to a much more rigorous basis).  \n\n[More Evidence & Ablations] To incorporate more evidence for the importance of AN and NS, we additionally ablated them for Linear/2-layer MLPs and CVC-ZSL [3] with/without AN/NS for SUN/CUB/AwA1/AwA2 datasets and reported the results in Appendix D.6 that demonstrate the improvement by ~5-50%. Also, in Table 4 there is an ablation over values of $\\gamma$ for NS for Linear and 2,3 layered MLPs.\n\nPlease, also see our additional response on F1, F2 and H1, H2, H4, H6 below.\n\n[1] https://arxiv.org/abs/1603.00550\n[2] https://arxiv.org/abs/1804.09458\n[3] https://arxiv.org/abs/1909.05995\n[4] https://arxiv.org/abs/1805.11604"}, "signatures": ["ICLR.cc/2021/Conference/Paper2020/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Class Normalization for (Continual)? Generalized Zero-Shot Learning", "authorids": ["~Ivan_Skorokhodov1", "~Mohamed_Elhoseiny1"], "authors": ["Ivan Skorokhodov", "Mohamed Elhoseiny"], "keywords": ["zero-shot learning", "normalization", "continual learning", "initialization"], "abstract": "Normalization techniques have proved to be a crucial ingredient of successful training in a traditional supervised learning regime. However, in the zero-shot learning (ZSL) world, these ideas have received only marginal attention. This work studies normalization in ZSL scenario from both theoretical and practical perspectives. First, we give a theoretical explanation to two popular tricks used in zero-shot learning: normalize+scale and attributes normalization and show that they help training by preserving variance during a forward pass. Next, we demonstrate that they are insufficient to normalize a deep ZSL model and propose Class Normalization (CN): a normalization scheme, which alleviates this issue both provably and in practice. Third, we show that ZSL models typically have more irregular loss surface compared to traditional classifiers and that the proposed method partially remedies this problem. Then, we test our approach on 4 standard ZSL datasets and outperform sophisticated modern SotA with a simple MLP optimized without any bells and whistles and having ~50 times faster training speed. Finally, we generalize ZSL to a broader problem \u2014 continual ZSL, and introduce some principled metrics and rigorous baselines for this new setup. The source code is available at https://github.com/universome/class-norm.", "one-sentence_summary": "We develop theoretical understanding of signal normalization inside zero-shot learning models, propose a novel normalization scheme and use it to achieve SotA ZSL performance with a simple MLP", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "skorokhodov|class_normalization_for_continual_generalized_zeroshot_learning", "pdf": "/pdf/08846e7d24687d94ea8f98091ccf05e3f001d22a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nskorokhodov2021class,\ntitle={Class Normalization for (Continual)? Generalized Zero-Shot Learning},\nauthor={Ivan Skorokhodov and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7pgFL2Dkyyy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "7pgFL2Dkyyy", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2020/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2020/Authors|ICLR.cc/2021/Conference/Paper2020/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853146, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment"}}}, {"id": "lR4SisEFn2A", "original": null, "number": 13, "cdate": 1605894586833, "ddate": null, "tcdate": 1605894586833, "tmdate": 1606281346140, "tddate": null, "forum": "7pgFL2Dkyyy", "replyto": "7pgFL2Dkyyy", "invitation": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment", "content": {"title": "CN paper update", "comment": "We thank the reviewers for their valuable feedback. We are encouraged that they found our method to be simple and showing strong empirical results (R1), the paper to be well organized and easy to follow (R3), empirical evidence to be convincing (R4), theoretical analysis to be interesting and sensible (R3), our proposed metrics for CZSL to be more generic and realistic (R3). We have replied in detail to address the reviewer\u2019s concerns individually and incorporated the feedback.\n\n\nKey changes we made to the paper:\n- Applied our model's idea to RelationNet and CVC-ZSL methods and observed the improvement in both cases by +2.0 and +1.8 on average in terms of GZSL-H for them, respectively. We report these results in Appendix D.5 of the paper.\n- Enriched the exposition on attributes normalization, normalize+scale, and embedding-based ZSL models in the Related Work section (first two paragraphs).\n- Improved our paper's clarity for the smoothness and normalization exposition and added additional details for them (sections 3.3-3.4 and Appendix F).\n- After submission, we found an error in our evaluation procedure for AwA1/AwA2 datasets that was leading to higher scores. We still hold the SotA result for AwA2 with GZSL-H of 67.6. For AwA1, our position descended to third place (after EPGN and CVC-ZSL) with GZSL-H of 67.8. We updated Table 1, 2 from the main body and Table 5 from Appendix D accordingly.\n- Discussion of the seen class logits scaling trick in Section 5 and Appendix D.4\n- Added [1, 2] to the comparison in Table 2\n- Added CZSL baselines descriptions\n- Added discussion of Figure 12 and Figure 13 in Appendix H\n- Launched additional experiments to demonstrate the importance of AN/NS on other models: 1/2-layer MLP and CVC-ZSL (see Table 8 in Appendix D.6)\n- Added exposition with the experimental validation of what happens when we manually enforce zero-mean/unit-variance attributes assumptions instead of using the formula derived with their relaxation in Appendix H\n- Rewrote exposition on loss surface smoothness and moved it into Section 3.5\n\n**Code**\n\nReviewer 2 asked for the source code for AwA1/AwA2 datasets and we prepared an anonymized [Colab Notebook](https://gist.github.com/iclr2021-classnorm/c29c8a1d4da78eb75a4cae24348b061d) for them. We would like to draw your attention to how simple our method is: it takes just ~150 lines of code including all the data downloading/preprocessing/training/evaluation/comments. And it takes just ~30 seconds to run! \n\nReviewer 3 and Reviewer 4 also questioned how our method would perform on aPY and thus we launched the same [Colab Notebook](https://gist.github.com/iclr2021-classnorm/cebb4dd7ca9ccc9a91d18b84aee056f7) (without changing a single hyperparameter) on this dataset. We obtained GZSL-H of 38.9 which puts our method in third place after DVBE (GZSL-H = 41.8) and CVC-ZSL (GZSL-H = 39.0), performing is very similar to CVC-ZSL. It took just 8 seconds to obtain that high score! We believe that after hyperparameter tuning the result could be improved.\n\n- [1] Learning to Compare: Relation Network for Few-Shot Learning, CVPR=18\n- [2] Meta-Learning for Generalized Zero-Shot Learning\n\nWe demonstrated in our paper a rigorously validated exploration of how our proposed CN can significantly improve simple ZSL  models to be on par and sometimes surpassing the state-of-the-art. In Continual / Multitask learning setting, we showed that CN can significantly improve the performance on sequential, A-GEM, MAS, and multi-task methods (Table 3).\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2020/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Class Normalization for (Continual)? Generalized Zero-Shot Learning", "authorids": ["~Ivan_Skorokhodov1", "~Mohamed_Elhoseiny1"], "authors": ["Ivan Skorokhodov", "Mohamed Elhoseiny"], "keywords": ["zero-shot learning", "normalization", "continual learning", "initialization"], "abstract": "Normalization techniques have proved to be a crucial ingredient of successful training in a traditional supervised learning regime. However, in the zero-shot learning (ZSL) world, these ideas have received only marginal attention. This work studies normalization in ZSL scenario from both theoretical and practical perspectives. First, we give a theoretical explanation to two popular tricks used in zero-shot learning: normalize+scale and attributes normalization and show that they help training by preserving variance during a forward pass. Next, we demonstrate that they are insufficient to normalize a deep ZSL model and propose Class Normalization (CN): a normalization scheme, which alleviates this issue both provably and in practice. Third, we show that ZSL models typically have more irregular loss surface compared to traditional classifiers and that the proposed method partially remedies this problem. Then, we test our approach on 4 standard ZSL datasets and outperform sophisticated modern SotA with a simple MLP optimized without any bells and whistles and having ~50 times faster training speed. Finally, we generalize ZSL to a broader problem \u2014 continual ZSL, and introduce some principled metrics and rigorous baselines for this new setup. The source code is available at https://github.com/universome/class-norm.", "one-sentence_summary": "We develop theoretical understanding of signal normalization inside zero-shot learning models, propose a novel normalization scheme and use it to achieve SotA ZSL performance with a simple MLP", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "skorokhodov|class_normalization_for_continual_generalized_zeroshot_learning", "pdf": "/pdf/08846e7d24687d94ea8f98091ccf05e3f001d22a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nskorokhodov2021class,\ntitle={Class Normalization for (Continual)? Generalized Zero-Shot Learning},\nauthor={Ivan Skorokhodov and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7pgFL2Dkyyy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "7pgFL2Dkyyy", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2020/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2020/Authors|ICLR.cc/2021/Conference/Paper2020/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853146, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment"}}}, {"id": "1UGLzHUd3vl", "original": null, "number": 19, "cdate": 1606083280738, "ddate": null, "tcdate": 1606083280738, "tmdate": 1606281277727, "tddate": null, "forum": "7pgFL2Dkyyy", "replyto": "XsYE7DtikRt", "invitation": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment", "content": {"title": "Additional paper update based on Reviewer 1 feedback", "comment": "We want one more time to thank Reviewer 1 for their diligent feedback, and we did what was within our hands to improve the paper based upon it. Among other things, this includes the following improvements:\n\n- We additionally applied our model's idea to RelationNet and CVC-ZSL methods and observed the improvement in both cases by +2.0 and +1.8 on average in terms of GZSL-H for them, respectively. We report these results in Appendix D.5 of the paper.\n- We enriched the exposition on attributes normalization, normalize+scale, and embedding-based ZSL models in the Related Work section (first two paragraphs).\n- We improved our paper's clarity for the smoothness and normalization exposition and added additional details for them (sections 3.3-3.4 and Appendix F).\n- We added the discussion of figures 12-13 in Appendix H\n- We provided additional discussion on how realistic the assumptions are in Appendix A\n- Launched additional experiments to demonstrate the importance of AN/NS for other models: 1/2-layer MLP and CVC-ZSL (see Table 8 in Appendix D.6)\n- Added exposition with the experimental validation of what happens when we manually enforce zero-mean/unit-variance attributes assumptions instead of using the formula derived with their relaxation in Appendix H\n- Rewrote the exposition on the loss surface smoothness and moved it into a separation Section 3.5\n\nWe respectfully disagree with Reviewer 1 judgment of our theoretical contributions as unsound/unrigorous and find this feedback not being specific enough to give us a chance to act upon. Our provided empirical evidence is in line with Xavier init [1], Kaiming init [2], Hyper init [3] works, which we consider to be sound, consistent, and rigorous.\n\nFinally, we believe that Reviewer 1 rating didn't give our work credit for the practical contributions in terms of the novel, simple, and highly efficient ZSL algorithm and the novel exposition of CZSL with 5 new evaluation criteria and strong practical results.\n\n- [1] http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\n- [2] https://arxiv.org/abs/1502.01852\n- [3] https://openreview.net/forum?id=H1lma24tPB\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2020/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Class Normalization for (Continual)? Generalized Zero-Shot Learning", "authorids": ["~Ivan_Skorokhodov1", "~Mohamed_Elhoseiny1"], "authors": ["Ivan Skorokhodov", "Mohamed Elhoseiny"], "keywords": ["zero-shot learning", "normalization", "continual learning", "initialization"], "abstract": "Normalization techniques have proved to be a crucial ingredient of successful training in a traditional supervised learning regime. However, in the zero-shot learning (ZSL) world, these ideas have received only marginal attention. This work studies normalization in ZSL scenario from both theoretical and practical perspectives. First, we give a theoretical explanation to two popular tricks used in zero-shot learning: normalize+scale and attributes normalization and show that they help training by preserving variance during a forward pass. Next, we demonstrate that they are insufficient to normalize a deep ZSL model and propose Class Normalization (CN): a normalization scheme, which alleviates this issue both provably and in practice. Third, we show that ZSL models typically have more irregular loss surface compared to traditional classifiers and that the proposed method partially remedies this problem. Then, we test our approach on 4 standard ZSL datasets and outperform sophisticated modern SotA with a simple MLP optimized without any bells and whistles and having ~50 times faster training speed. Finally, we generalize ZSL to a broader problem \u2014 continual ZSL, and introduce some principled metrics and rigorous baselines for this new setup. The source code is available at https://github.com/universome/class-norm.", "one-sentence_summary": "We develop theoretical understanding of signal normalization inside zero-shot learning models, propose a novel normalization scheme and use it to achieve SotA ZSL performance with a simple MLP", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "skorokhodov|class_normalization_for_continual_generalized_zeroshot_learning", "pdf": "/pdf/08846e7d24687d94ea8f98091ccf05e3f001d22a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nskorokhodov2021class,\ntitle={Class Normalization for (Continual)? Generalized Zero-Shot Learning},\nauthor={Ivan Skorokhodov and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7pgFL2Dkyyy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "7pgFL2Dkyyy", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2020/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2020/Authors|ICLR.cc/2021/Conference/Paper2020/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853146, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment"}}}, {"id": "8fg1EZEhokW", "original": null, "number": 5, "cdate": 1605835907474, "ddate": null, "tcdate": 1605835907474, "tmdate": 1606254499918, "tddate": null, "forum": "7pgFL2Dkyyy", "replyto": "XsYE7DtikRt", "invitation": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment", "content": {"title": "Part 4: Statement 2-3", "comment": "\n> Why wouldn\u2019t the following statement in Sect. 3.3 invalidate Statement 1? \u201cThis may create an impression that it does not matter how we initialize the weights \u2014 normalization would undo any fluctuations. However it is not true, because it is still important how the signal flows, i.e. for an unnormalized and unscaled logit value\u201d\n\nStatement 1 operates on top of logits, while Statement 2 operates on top of pre-logits. Statement 1 makes a claim about the forward pass and has nothing to do with the backward pass, while Statement 2 shows that incorporating attributes normalization is equivalent to employing Xavier fan-out init, which adjusts the variance of the backward pass.\n\n> It is unclear (at least not from the beginning) why understanding attribute normalization has to do with initialization of the weights.\n\nDividing the attributes by some large value (in this case, by their norm) is equivalent to dividing the subsequent dense layer weight matrix by this large value. Hence this is equivalent to reparametrizing the weight matrix with the different scale. Thank you for pointing this out, we will clarify that moment in the updated version of the paper.\n\n> Similar to my comments to Statement 1, why should we believe that the explanation in Sect. 3.3 and Sect. 3.4 is the reason for zero-shot learning effectiveness? \n\nWe provide a large empirical investigation (Figures 1, 4, 5, 6, 7) on how the variance behaves for different setups and how they perform in practice, thus obtaining the evidence on the correlation.\n\n> The authors again claim that the main bottleneck in improving zero-shot learning is \u201cvariance control\u201d (the end of Sect. 3.3).\n\nThough we didn\u2019t claim such a thing (we claimed that Xavier/Kaiming init would use invalid assumptions for the attributes thus producing an improper scaling for weight initialization which in turn would translate into the bad variance control), our empirical results demonstrate that after adjusting it, you obtain strong state-of-the-art performance.\n\n> I also have a hard time understanding some statements in Appendix H, which is needed to motivate the following statement in Sect. 3.3: \u201cAnd these assumptions are safe to assume only for z but not for a_c, because they do not hold for the standard datasets (see Appendix H).\u201d H1: Would this statement still be true after we transform a_c with an MLP? H2: Why is it not \u201ca sensible thing to do\u201d if we just want zero mean and unit variance? H3: Why is \u201csuch an approach far from being scalable\u201d? H4: What if these are things like word embeddings? H5: Fig. 12 and Fig. 13 are not explained. H6: Histograms in Fig. 13 look quite normal.\n\n- H1: Assumptions would (approximately) start working after we transform a_c with an MLP, but if we\u2019ll apply the MLP without using eq 9 + eq 10 this will lead to a \u201cbad\u201d variance (as we demonstrate on figures 1, 4, 5, 6, 7).\n- H2: It just works worse in practice. Typically, things work the best when your data follows N(0,1). Attributes do not follow normal distribution originally and thus cannot be converted to N(0,1) just by standardization.\n- H3: Because there is no magic formula of transforming a non-normal distribution to a normal one. Especially multi-modals. You should either solve a tedious optimization task or do this manually. This is not scalable when you are solving task by task.\n- H4: It depends on the exact situation, but we believe that the coordinates in word embeddings in general follow the normal distribution. \n- H5: We missed to add references from text to the figures (thanks for pointing this out, we\u2019ll fix this), but they have explanatory captions which should not confuse a reader too much.\n- H6: In our view, histograms in Fig. 13 look is not normal for the following reasons: 1) the long tail on the right showing that the distribution is obviously skewed. (2) To remove any bias from deciding what looks normal and what is not, we incorporated D\u2019Agostino and Pearson\u2019s normality tests resulting in Figure 11, which shows that attributes are distributed very far from normal.\n\n> How useful is Statement 2? Why is the connection with Xavier initialization important?\n\nStatement 2 shows that when you use attribute normalization in a linear embedding-based setting, then your init is equivalent to Xavier fan-out init. It is just a coincidence that it turned out to be equivalent to Xavier fan-out one. Xavier fan-out init is a decent, but not the best choice for the model initialization. A more popular way toinitialize linear layers is via Kaiming fan-in init with the uniform distribution \u2014 a [default init for pytorch nn.Linear](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/linear.py#L87).\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2020/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Class Normalization for (Continual)? Generalized Zero-Shot Learning", "authorids": ["~Ivan_Skorokhodov1", "~Mohamed_Elhoseiny1"], "authors": ["Ivan Skorokhodov", "Mohamed Elhoseiny"], "keywords": ["zero-shot learning", "normalization", "continual learning", "initialization"], "abstract": "Normalization techniques have proved to be a crucial ingredient of successful training in a traditional supervised learning regime. However, in the zero-shot learning (ZSL) world, these ideas have received only marginal attention. This work studies normalization in ZSL scenario from both theoretical and practical perspectives. First, we give a theoretical explanation to two popular tricks used in zero-shot learning: normalize+scale and attributes normalization and show that they help training by preserving variance during a forward pass. Next, we demonstrate that they are insufficient to normalize a deep ZSL model and propose Class Normalization (CN): a normalization scheme, which alleviates this issue both provably and in practice. Third, we show that ZSL models typically have more irregular loss surface compared to traditional classifiers and that the proposed method partially remedies this problem. Then, we test our approach on 4 standard ZSL datasets and outperform sophisticated modern SotA with a simple MLP optimized without any bells and whistles and having ~50 times faster training speed. Finally, we generalize ZSL to a broader problem \u2014 continual ZSL, and introduce some principled metrics and rigorous baselines for this new setup. The source code is available at https://github.com/universome/class-norm.", "one-sentence_summary": "We develop theoretical understanding of signal normalization inside zero-shot learning models, propose a novel normalization scheme and use it to achieve SotA ZSL performance with a simple MLP", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "skorokhodov|class_normalization_for_continual_generalized_zeroshot_learning", "pdf": "/pdf/08846e7d24687d94ea8f98091ccf05e3f001d22a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nskorokhodov2021class,\ntitle={Class Normalization for (Continual)? Generalized Zero-Shot Learning},\nauthor={Ivan Skorokhodov and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7pgFL2Dkyyy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "7pgFL2Dkyyy", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2020/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2020/Authors|ICLR.cc/2021/Conference/Paper2020/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853146, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment"}}}, {"id": "81DHmgR514A", "original": null, "number": 21, "cdate": 1606108910567, "ddate": null, "tcdate": 1606108910567, "tmdate": 1606108910567, "tddate": null, "forum": "7pgFL2Dkyyy", "replyto": "XsYE7DtikRt", "invitation": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment", "content": {"title": "Thank you for your feedback", "comment": "I'd like to thank the authors for trying to address my concerns in detail. I'd like to first respond to your major point, which I suspect we don't agree, but I'd be happy to take in further feedback from the authors and other reviewers.\n\nIn order to judge the significance of the contribution \"showing that Statement X has Property Y\", I think one could use (i) the difficulty in showing that Statement X has Property Y in theory, (ii) how much does \"Statement X has Property Y\" hold in practice (see property Y in experiments with real datasets, reasonable assumptions?), and (iii) Property Y helps downstream Z. My understanding is that this paper doesn't claim (i) in contrast to, say, theory papers that involve a lot of technical and mental gymnastics.\n\nNow, (iii) Y and Z (zero-shot accuracy) are weakly correlated in this paper. IMO, \"extensive evaluation\" in Figures 1, 4, 5, 6, 7 and the tables shows (ii) but provides only weak empirical evidence to support (iii) (the authors disagree). Further, the tricks have been shown to work only on one zero-shot learning MLP baseline in this paper (the authors believe this is enough). How should we deduct the generalizability of this correlation? If this bar is unreasonable, I'd be willing to increase my score.\n\nAnother major criticism is also with respect to the clarity (which must reach a level to support soundness/rigorousness), and I understand that this may not be related to proving (iii). I tried to ask questions and gave a lot of examples that I feel are enough to illustrate this point. For example, see F1, F2, H2. Note that my concerns in H also weaken (ii) (see also the authors' responses to H1, H2, H4, H6)."}, "signatures": ["ICLR.cc/2021/Conference/Paper2020/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Class Normalization for (Continual)? Generalized Zero-Shot Learning", "authorids": ["~Ivan_Skorokhodov1", "~Mohamed_Elhoseiny1"], "authors": ["Ivan Skorokhodov", "Mohamed Elhoseiny"], "keywords": ["zero-shot learning", "normalization", "continual learning", "initialization"], "abstract": "Normalization techniques have proved to be a crucial ingredient of successful training in a traditional supervised learning regime. However, in the zero-shot learning (ZSL) world, these ideas have received only marginal attention. This work studies normalization in ZSL scenario from both theoretical and practical perspectives. First, we give a theoretical explanation to two popular tricks used in zero-shot learning: normalize+scale and attributes normalization and show that they help training by preserving variance during a forward pass. Next, we demonstrate that they are insufficient to normalize a deep ZSL model and propose Class Normalization (CN): a normalization scheme, which alleviates this issue both provably and in practice. Third, we show that ZSL models typically have more irregular loss surface compared to traditional classifiers and that the proposed method partially remedies this problem. Then, we test our approach on 4 standard ZSL datasets and outperform sophisticated modern SotA with a simple MLP optimized without any bells and whistles and having ~50 times faster training speed. Finally, we generalize ZSL to a broader problem \u2014 continual ZSL, and introduce some principled metrics and rigorous baselines for this new setup. The source code is available at https://github.com/universome/class-norm.", "one-sentence_summary": "We develop theoretical understanding of signal normalization inside zero-shot learning models, propose a novel normalization scheme and use it to achieve SotA ZSL performance with a simple MLP", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "skorokhodov|class_normalization_for_continual_generalized_zeroshot_learning", "pdf": "/pdf/08846e7d24687d94ea8f98091ccf05e3f001d22a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nskorokhodov2021class,\ntitle={Class Normalization for (Continual)? Generalized Zero-Shot Learning},\nauthor={Ivan Skorokhodov and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7pgFL2Dkyyy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "7pgFL2Dkyyy", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2020/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2020/Authors|ICLR.cc/2021/Conference/Paper2020/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853146, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment"}}}, {"id": "2nQu-y8neW-", "original": null, "number": 20, "cdate": 1606086469717, "ddate": null, "tcdate": 1606086469717, "tmdate": 1606086469717, "tddate": null, "forum": "7pgFL2Dkyyy", "replyto": "UqTxwzFrKG", "invitation": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment", "content": {"title": "Response to authors update", "comment": "Thanks for the authors' response. The results for aPY look also convincing and it is an interesting angle to connect the normalization to the attribute magnitude for the performance improvement. I opt to keep my score and recommend acceptance."}, "signatures": ["ICLR.cc/2021/Conference/Paper2020/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Class Normalization for (Continual)? Generalized Zero-Shot Learning", "authorids": ["~Ivan_Skorokhodov1", "~Mohamed_Elhoseiny1"], "authors": ["Ivan Skorokhodov", "Mohamed Elhoseiny"], "keywords": ["zero-shot learning", "normalization", "continual learning", "initialization"], "abstract": "Normalization techniques have proved to be a crucial ingredient of successful training in a traditional supervised learning regime. However, in the zero-shot learning (ZSL) world, these ideas have received only marginal attention. This work studies normalization in ZSL scenario from both theoretical and practical perspectives. First, we give a theoretical explanation to two popular tricks used in zero-shot learning: normalize+scale and attributes normalization and show that they help training by preserving variance during a forward pass. Next, we demonstrate that they are insufficient to normalize a deep ZSL model and propose Class Normalization (CN): a normalization scheme, which alleviates this issue both provably and in practice. Third, we show that ZSL models typically have more irregular loss surface compared to traditional classifiers and that the proposed method partially remedies this problem. Then, we test our approach on 4 standard ZSL datasets and outperform sophisticated modern SotA with a simple MLP optimized without any bells and whistles and having ~50 times faster training speed. Finally, we generalize ZSL to a broader problem \u2014 continual ZSL, and introduce some principled metrics and rigorous baselines for this new setup. The source code is available at https://github.com/universome/class-norm.", "one-sentence_summary": "We develop theoretical understanding of signal normalization inside zero-shot learning models, propose a novel normalization scheme and use it to achieve SotA ZSL performance with a simple MLP", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "skorokhodov|class_normalization_for_continual_generalized_zeroshot_learning", "pdf": "/pdf/08846e7d24687d94ea8f98091ccf05e3f001d22a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nskorokhodov2021class,\ntitle={Class Normalization for (Continual)? Generalized Zero-Shot Learning},\nauthor={Ivan Skorokhodov and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7pgFL2Dkyyy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "7pgFL2Dkyyy", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2020/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2020/Authors|ICLR.cc/2021/Conference/Paper2020/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853146, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment"}}}, {"id": "8MNECymkktX", "original": null, "number": 2, "cdate": 1605835771594, "ddate": null, "tcdate": 1605835771594, "tmdate": 1606084164821, "tddate": null, "forum": "7pgFL2Dkyyy", "replyto": "XsYE7DtikRt", "invitation": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment", "content": {"title": "Part 1: possible misunderstandings on (Empirical Results & Theoretical Analysis)", "comment": " \nWe appreciate Reviewer1\u2019s detailed and valuable feedback.  R1's review does not give credit to several efforts and key contributions in the paper, based on which we think there could be a misunderstanding. We hope it is alright that we highlight two key points that may help clarify our key message in the paper. We also acted with what is in our hands to improve the paper from an R1 perspective that we appreciated. \n\n### Misunderstandings:\n\n**1) Our method effectiveness and Experimental Validation.**\nApart from the theoretical analysis that we think is correct  and shows desirable learning characteristics of our study on normalization in ZSL, we have very solid empirical results and we believe that you didn\u2019t give us enough credit for them:\n\n- Strong empirical results for ZSL on four datasets: our method is very simple and effective  (which we believe to be an important result), trains very fast ( about 1 minute), and beats modern SotA which usually follows a very complicated design and takes >1 hour to train.\n- We proposed a more rigorous formulation of CZSL as a direct generalization of ZSL + 5 novel metrics for it. We tested several continual learning benchmarks and showed that our method improved several baselines by a lot (~40% on average). We showed the value of CN on five CL methods (A-GEM, EWC, A-GEM, Sequential, and Multitask) on CUB and SUN datasets. \n\n**2)  Theoretical Analysis.**\n We believe that the judgment used to evaluate our theoretical claims seems less related to our focus. We understood it as \u201c statement X shows that property Y holds, but this does not show that it improves ZSL performance. Thus, statement X is not sound/not rigorous.\u201d While it is true that we do not prove the increased performance for ZSL directly, we think this does not make our theoretical analysis not sound as we detail below:\n\n- For each statement, we provided very solid empirical evidence and showed that it correlates with the good ZSL performance in practice. For example, our proposed model has more steady and closer-to-unit variance (shown by statement 3 and figures 1,5,6,7,8) while other methods do not \u2014 and it also outperforms them in terms of performance. Or it has a smoother loss landscape, shown from the exposition in Appendix F.\n\n- The current state of deep learning theory is at such a level that proving that some property would directly increase the performance is almost always out of reach unless you have a very specific simplified model (like infinite-width networks) trained for a very well-studied task like binary classification where you have a lot of existing theoretical tricks to employ. Let's consider for example the task we are solving: investigating the analysis of variance inside a network. One of the most famous early works on it is Xavier init [1]. In their paper, authors derive a proper initialization scale for neural network weights that would preserve the variance during a forward or backward pass \u2014 an analysis which is similar in spirit to what we did for ZSL models. Then they provide strong empirical evidence of why it is a good thing (which we also do!) by showing that the theoretical claims hold in practice and correlates with the improved performance. But they do not have any proof that the variance preservation directly improves a model\u2019s performance \u2014 would you discard their contributions as well? And we still do not have a rigorous theoretical connection between this \u201cvariance preservation\u201d and final model performance. Why? Because it is too hard. There is a series of works on dynamical isometry [2,3,4] that took the authors years to develop, and we still do not have an exact connection between the signal propagation and the model scores! In their works, they show the \u201ctrainability\u201d of a model depending on the initialization but this \u201ctrainability\u201d is not directly connected to the performance, but rather as a property of the Jacobian matrix \u2014 which, we emphasize it again \u2014 is not directly connected to the score.\n\nWith all this being said, we disagree with how you evaluate theoretical analysis. And we believe that it is a decent way to develop the understanding of deep learning the way it is done in the paper: first, formulate and prove a statement that some property holds; and second, demonstrate rigorous evidence that supports both the statement and how it correlates with the performance.\n\n- [1] http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\n- [2] https://arxiv.org/abs/1711.04735\n- [3] https://arxiv.org/abs/1806.05393\n- [4] https://arxiv.org/abs/1901.08987\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2020/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Class Normalization for (Continual)? Generalized Zero-Shot Learning", "authorids": ["~Ivan_Skorokhodov1", "~Mohamed_Elhoseiny1"], "authors": ["Ivan Skorokhodov", "Mohamed Elhoseiny"], "keywords": ["zero-shot learning", "normalization", "continual learning", "initialization"], "abstract": "Normalization techniques have proved to be a crucial ingredient of successful training in a traditional supervised learning regime. However, in the zero-shot learning (ZSL) world, these ideas have received only marginal attention. This work studies normalization in ZSL scenario from both theoretical and practical perspectives. First, we give a theoretical explanation to two popular tricks used in zero-shot learning: normalize+scale and attributes normalization and show that they help training by preserving variance during a forward pass. Next, we demonstrate that they are insufficient to normalize a deep ZSL model and propose Class Normalization (CN): a normalization scheme, which alleviates this issue both provably and in practice. Third, we show that ZSL models typically have more irregular loss surface compared to traditional classifiers and that the proposed method partially remedies this problem. Then, we test our approach on 4 standard ZSL datasets and outperform sophisticated modern SotA with a simple MLP optimized without any bells and whistles and having ~50 times faster training speed. Finally, we generalize ZSL to a broader problem \u2014 continual ZSL, and introduce some principled metrics and rigorous baselines for this new setup. The source code is available at https://github.com/universome/class-norm.", "one-sentence_summary": "We develop theoretical understanding of signal normalization inside zero-shot learning models, propose a novel normalization scheme and use it to achieve SotA ZSL performance with a simple MLP", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "skorokhodov|class_normalization_for_continual_generalized_zeroshot_learning", "pdf": "/pdf/08846e7d24687d94ea8f98091ccf05e3f001d22a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nskorokhodov2021class,\ntitle={Class Normalization for (Continual)? Generalized Zero-Shot Learning},\nauthor={Ivan Skorokhodov and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7pgFL2Dkyyy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "7pgFL2Dkyyy", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2020/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2020/Authors|ICLR.cc/2021/Conference/Paper2020/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853146, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment"}}}, {"id": "vJTnScX3v74", "original": null, "number": 4, "cdate": 1605835867865, "ddate": null, "tcdate": 1605835867865, "tmdate": 1606079754590, "tddate": null, "forum": "7pgFL2Dkyyy", "replyto": "XsYE7DtikRt", "invitation": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment", "content": {"title": "Part 3:  Statement 1", "comment": ">I feel that we need further justifications on the correlation between Statement 1 (variance of y^_c, \u201cbetter stability\u201d and \u201cthe training would not stale\u201d) and the zero-shot learning accuracy for this to be the \u201cwhy normalization + scaling works.\u201d My understanding is that the Appendix simply validates that Eq. (4) seems to hold in practice.\n\nStatement 1 derives the formula for the logits variance when the NS trick is applied and we show in Figures 1,4,5,6, and 7 that the NS trick indeed improves it in practice when the proper $\\gamma$ is used. Tables 2 and 4 show the performance for model +NS and models for different \\gamma values and demonstrate that using the proper value of \\gamma favourable influences performance. We believe that this is enough correlation to state that the proper variance correlates with the good performance.\n\n> Moreover, is the usual search region [5,10] actually effective? Do we have stronger supporting empirical evidence than the three groups of practitioners (Li et al 2019, Zhang et al. 2019, Guo et al. 2020), who may have influenced each other, used it? \n\nWe provided several works that select their final values from this region: [Li et al. 2019] \u2014 uses \\gamma=10, [Guo et al. 2020] \u2014 uses \\gamma=8, [Zhang et al. 2019] \u2014 uses \\gamma=3 but their embedding size is much smaller (since we normalize by smaller norm, we need smaller scaling afterwards), [1] \u2014 uses \\gamma=8, [2] \u2014 uses $\\gamma=10$, [3] set it to 10 and optimized during training. They indeed may have influenced each other, but tracking down what exact hyperparameter search regions they used is out of reach and we can report only their final values.\n\n- [1] https://arxiv.org/abs/1801.07698\n- [2] https://papers.nips.cc/paper/2018/hash/1587965fb4d4b5afe8428a4a024feb0d-Abstract.html\n- [3] Dynamic few-shot visual learning without forgetting, CVPR 2018\n\n> Finally, can the authors comment on the validity of multiple assumptions in Appendix A? To which degrees does each of them hold in practice?\n\nOur assumptions 1,3,4,5 are typical for such kind of analysis and Xavier init [1], Kaiming init [2], Hyper init [3] all use them as well. Assumption 2 is a technical assumption that we just need to apply the CLT. Assumption 6, as noted in the paper, holds only for large-dimensional inputs, but this is exactly our case. Figure 2 in Appendix A demonstrates that these assumptions lead to a pretty good approximation. We will include the discussion in the paper.\n- [1] http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\n- [2] https://arxiv.org/abs/1502.01852\n- [3] https://openreview.net/forum?id=H1lma24tPB\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2020/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Class Normalization for (Continual)? Generalized Zero-Shot Learning", "authorids": ["~Ivan_Skorokhodov1", "~Mohamed_Elhoseiny1"], "authors": ["Ivan Skorokhodov", "Mohamed Elhoseiny"], "keywords": ["zero-shot learning", "normalization", "continual learning", "initialization"], "abstract": "Normalization techniques have proved to be a crucial ingredient of successful training in a traditional supervised learning regime. However, in the zero-shot learning (ZSL) world, these ideas have received only marginal attention. This work studies normalization in ZSL scenario from both theoretical and practical perspectives. First, we give a theoretical explanation to two popular tricks used in zero-shot learning: normalize+scale and attributes normalization and show that they help training by preserving variance during a forward pass. Next, we demonstrate that they are insufficient to normalize a deep ZSL model and propose Class Normalization (CN): a normalization scheme, which alleviates this issue both provably and in practice. Third, we show that ZSL models typically have more irregular loss surface compared to traditional classifiers and that the proposed method partially remedies this problem. Then, we test our approach on 4 standard ZSL datasets and outperform sophisticated modern SotA with a simple MLP optimized without any bells and whistles and having ~50 times faster training speed. Finally, we generalize ZSL to a broader problem \u2014 continual ZSL, and introduce some principled metrics and rigorous baselines for this new setup. The source code is available at https://github.com/universome/class-norm.", "one-sentence_summary": "We develop theoretical understanding of signal normalization inside zero-shot learning models, propose a novel normalization scheme and use it to achieve SotA ZSL performance with a simple MLP", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "skorokhodov|class_normalization_for_continual_generalized_zeroshot_learning", "pdf": "/pdf/08846e7d24687d94ea8f98091ccf05e3f001d22a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nskorokhodov2021class,\ntitle={Class Normalization for (Continual)? Generalized Zero-Shot Learning},\nauthor={Ivan Skorokhodov and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7pgFL2Dkyyy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "7pgFL2Dkyyy", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2020/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2020/Authors|ICLR.cc/2021/Conference/Paper2020/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853146, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment"}}}, {"id": "1KuxdNB3qXt", "original": null, "number": 18, "cdate": 1605995933845, "ddate": null, "tcdate": 1605995933845, "tmdate": 1605995933845, "tddate": null, "forum": "7pgFL2Dkyyy", "replyto": "lU_sY-8sGbH", "invitation": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment", "content": {"title": "Response to Reviewer 2 update", "comment": "Thanks, we are really grateful for your appreciation of our efforts. \n\n> In the code, the author calibrates the seen class logits by a value 0.95 (hyperparameter) and it is not mentioned in the paper. Please mention this hyperparameter in the paper and explain how you obtained it. Also, include a small ablation over this value.\n\nThat technique is not original and was considered for example in [1,2]. Our difference to [1,2] is that we are multiplying by some value $s$ instead of adding some value $\\tau$ to reweigh the logits since we think it to be more intuitive this way and the value of the weight now does not depend on the logits magnitudes. We find the optimal value of $s$ using cross-validation together with all the other hyperparameters.\n\nWe added the discussion of this trick in Section 5 and the corresponding curves of how it influences GZSL-U/GZSL-S/GZSL-H scores in Appendix D.4 on Figure 4.\n\n> It will be interesting to see how the same normalization can be applied to the generative model and how much improvement we can obtain.\n\nWe agree with this and we also think it is an exciting future direction. A key challenge here is that attribute embeddings are getting concatenated to a representation instead of being multiplied which should affect how the normalization occurs. Using other types of fusion like multiplicative interactions [3] should make the setup more similar to ours. Another challenge is that the optimization objective is very different (e.g., noise and adversarial losses), which may hide some subtleties that are important to consider while designing the normalization procedure.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2020/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Class Normalization for (Continual)? Generalized Zero-Shot Learning", "authorids": ["~Ivan_Skorokhodov1", "~Mohamed_Elhoseiny1"], "authors": ["Ivan Skorokhodov", "Mohamed Elhoseiny"], "keywords": ["zero-shot learning", "normalization", "continual learning", "initialization"], "abstract": "Normalization techniques have proved to be a crucial ingredient of successful training in a traditional supervised learning regime. However, in the zero-shot learning (ZSL) world, these ideas have received only marginal attention. This work studies normalization in ZSL scenario from both theoretical and practical perspectives. First, we give a theoretical explanation to two popular tricks used in zero-shot learning: normalize+scale and attributes normalization and show that they help training by preserving variance during a forward pass. Next, we demonstrate that they are insufficient to normalize a deep ZSL model and propose Class Normalization (CN): a normalization scheme, which alleviates this issue both provably and in practice. Third, we show that ZSL models typically have more irregular loss surface compared to traditional classifiers and that the proposed method partially remedies this problem. Then, we test our approach on 4 standard ZSL datasets and outperform sophisticated modern SotA with a simple MLP optimized without any bells and whistles and having ~50 times faster training speed. Finally, we generalize ZSL to a broader problem \u2014 continual ZSL, and introduce some principled metrics and rigorous baselines for this new setup. The source code is available at https://github.com/universome/class-norm.", "one-sentence_summary": "We develop theoretical understanding of signal normalization inside zero-shot learning models, propose a novel normalization scheme and use it to achieve SotA ZSL performance with a simple MLP", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "skorokhodov|class_normalization_for_continual_generalized_zeroshot_learning", "pdf": "/pdf/08846e7d24687d94ea8f98091ccf05e3f001d22a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nskorokhodov2021class,\ntitle={Class Normalization for (Continual)? Generalized Zero-Shot Learning},\nauthor={Ivan Skorokhodov and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7pgFL2Dkyyy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "7pgFL2Dkyyy", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2020/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2020/Authors|ICLR.cc/2021/Conference/Paper2020/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853146, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment"}}}, {"id": "prTYPzai0EO", "original": null, "number": 1, "cdate": 1603653024032, "ddate": null, "tcdate": 1603653024032, "tmdate": 1605993498260, "tddate": null, "forum": "7pgFL2Dkyyy", "replyto": "7pgFL2Dkyyy", "invitation": "ICLR.cc/2021/Conference/Paper2020/-/Official_Review", "content": {"title": "CLASS NORMALIZATION FOR ZERO-SHOT LEARNING", "review": "Summary: This paper presents a theoretical justification for normalization in model training on how it affects model performance and training time. It proposes two normalization tricks: normalize + scale trick and attributes normalization trick and apply in the zero-shot image classification task. This paper also shows that two normalization tricks are not enough to variance control in a deep architecture. To address this problem, a new initialization scheme is introduced. Apart from theoretical analysis and a new initialization scheme for normalization, it extends the zero-short learning approach in a continual learning framework. This new framework is called continual zero-shot learning (CZSL) and provides corresponding evaluation metrics. The experiments for CZSL are performed in two datasets, CUB and SUN.  This paper experimentally shows the effectiveness of the initialization, normalization, and scaling trick.\n\nStrong Points:  1-  Paper is well organized and easy to follow.\n\n2-  This paper took an interesting problem and developed a compelling investigation for how normalization affects performance. The theoretical justification for the normalization tricks sounds interesting and makes sense. \n\n3-  It introduced two new techniques for normalization and shows by the theoretical justification that only normalization techniques are not sufficient for proper model training. For good model training apart from normalization tricks, it introduced an initialization scheme.\n\n4- Using normalization tricks and a new initialization scheme reduces a significant model training time compared to previous approaches. It presents training speed results for several baseline approaches.\n \n5- Innovative attempts in introducing a new ZSL problem, and several evaluation metrics are proposed for continual ZSL.\n\n\nWeaknesses:  1- This paper presents a robust analysis of normalization, initialization, and scaling trick for ZSL. It also extends ZSL in continual learning.  I appreciate the author's effort for this solid analysis.  I expect a  new proposed model from authors to make the paper more strong. \n2-  Missing comparison:  I recommend including paper [a] in the comparison table for CZSL.  Approach [a] is the first proposed baseline for continual zero-shot learning. Therefore it must be included in the comparison table. \n3- Some recent state-of-the approaches are missing in the comparison table for ZSL. Please compare it with [b],[c] models.\n\n4- Why have aPY dataset is not included in the experiments? Does this model not perform well in aPY dataset?\n5- Is it possible for this normalization and scaling tricks for other applications such as object detection, action recognition, and image retrieval? \n6-  I wonder by the training time you reported. I understand you have used quite a small neural network. Still, to have a clear view or fair comparison, you should have compared the timings with other initialization and default normalization and scaling tricks as well.\n\n[a]- Lifelong Zero-Shot Learning, by Kun Wei et al. IJCAI 2020.\n[b]- Episode-Based Prototype Generating Network for Zero-Shot Learning,  by  Yu et al. CVPR2020.\n[c]- Meta-Learning for Generalized Zero-Shot Learning, by Verma et al. AAAI 2020.\n\nRating Reason: This paper has included a well-detailed analysis and mathematical formulations for normalization, initialization about model training. But it does not propose a novel model, which limits the novelty of the model.  CZSL formulation is also already explored in [a].", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2020/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Class Normalization for (Continual)? Generalized Zero-Shot Learning", "authorids": ["~Ivan_Skorokhodov1", "~Mohamed_Elhoseiny1"], "authors": ["Ivan Skorokhodov", "Mohamed Elhoseiny"], "keywords": ["zero-shot learning", "normalization", "continual learning", "initialization"], "abstract": "Normalization techniques have proved to be a crucial ingredient of successful training in a traditional supervised learning regime. However, in the zero-shot learning (ZSL) world, these ideas have received only marginal attention. This work studies normalization in ZSL scenario from both theoretical and practical perspectives. First, we give a theoretical explanation to two popular tricks used in zero-shot learning: normalize+scale and attributes normalization and show that they help training by preserving variance during a forward pass. Next, we demonstrate that they are insufficient to normalize a deep ZSL model and propose Class Normalization (CN): a normalization scheme, which alleviates this issue both provably and in practice. Third, we show that ZSL models typically have more irregular loss surface compared to traditional classifiers and that the proposed method partially remedies this problem. Then, we test our approach on 4 standard ZSL datasets and outperform sophisticated modern SotA with a simple MLP optimized without any bells and whistles and having ~50 times faster training speed. Finally, we generalize ZSL to a broader problem \u2014 continual ZSL, and introduce some principled metrics and rigorous baselines for this new setup. The source code is available at https://github.com/universome/class-norm.", "one-sentence_summary": "We develop theoretical understanding of signal normalization inside zero-shot learning models, propose a novel normalization scheme and use it to achieve SotA ZSL performance with a simple MLP", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "skorokhodov|class_normalization_for_continual_generalized_zeroshot_learning", "pdf": "/pdf/08846e7d24687d94ea8f98091ccf05e3f001d22a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nskorokhodov2021class,\ntitle={Class Normalization for (Continual)? Generalized Zero-Shot Learning},\nauthor={Ivan Skorokhodov and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7pgFL2Dkyyy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "7pgFL2Dkyyy", "replyto": "7pgFL2Dkyyy", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2020/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538105739, "tmdate": 1606915765215, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2020/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2020/-/Official_Review"}}}, {"id": "-souNEXprt3", "original": null, "number": 17, "cdate": 1605993269257, "ddate": null, "tcdate": 1605993269257, "tmdate": 1605993269257, "tddate": null, "forum": "7pgFL2Dkyyy", "replyto": "BGIVxjg06-k", "invitation": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment", "content": {"title": "Convince with authors response", "comment": "I appreciate to authors for their pointwise reply to each issue raised by reviewers. I have gone through other reviews and the author's response to them. I found an interesting thing that the authors did not try to escape from any reviewer's questions.\n\nThe key advantage of this work is that it is the non-generative approach, which beats several generative approaches. More importantly, it is significantly efficient and fast compared to current state-of-the-art methods for zero-shot learning. I think it is because of the proposed normalization trick, and now it will be useful in zero-shot learning. As Reviewer R2 has suspected in the code and now verified it, he is convinced with the model performance and fastness. The authors have included the missing experiments and comparisons suggested by other reviewers and me in the revised version.\n\nSuggestion: Since this paper is mainly on zero-shot learning, I want to see a more realistic setting for continuous zero-shot learning. If each task consists of seen and unseen classes, it converges to an actual zero-shot setting with the same seen and unseen classes as the standard ZSL setup during the training of the last task.  Then compare this after last task training with ZSL approaches to see the degradation of model performance in task-wise sequential training. If possible author can include it in the ablation analysis.\n\nOverall: I found this approach is very interesting, simple with good performance concerning accuracy and execution time.  I hope the proposed normalization trick will help researchers to present more robust ZSL approaches. I tend towards acceptance from the borderline."}, "signatures": ["ICLR.cc/2021/Conference/Paper2020/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Class Normalization for (Continual)? Generalized Zero-Shot Learning", "authorids": ["~Ivan_Skorokhodov1", "~Mohamed_Elhoseiny1"], "authors": ["Ivan Skorokhodov", "Mohamed Elhoseiny"], "keywords": ["zero-shot learning", "normalization", "continual learning", "initialization"], "abstract": "Normalization techniques have proved to be a crucial ingredient of successful training in a traditional supervised learning regime. However, in the zero-shot learning (ZSL) world, these ideas have received only marginal attention. This work studies normalization in ZSL scenario from both theoretical and practical perspectives. First, we give a theoretical explanation to two popular tricks used in zero-shot learning: normalize+scale and attributes normalization and show that they help training by preserving variance during a forward pass. Next, we demonstrate that they are insufficient to normalize a deep ZSL model and propose Class Normalization (CN): a normalization scheme, which alleviates this issue both provably and in practice. Third, we show that ZSL models typically have more irregular loss surface compared to traditional classifiers and that the proposed method partially remedies this problem. Then, we test our approach on 4 standard ZSL datasets and outperform sophisticated modern SotA with a simple MLP optimized without any bells and whistles and having ~50 times faster training speed. Finally, we generalize ZSL to a broader problem \u2014 continual ZSL, and introduce some principled metrics and rigorous baselines for this new setup. The source code is available at https://github.com/universome/class-norm.", "one-sentence_summary": "We develop theoretical understanding of signal normalization inside zero-shot learning models, propose a novel normalization scheme and use it to achieve SotA ZSL performance with a simple MLP", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "skorokhodov|class_normalization_for_continual_generalized_zeroshot_learning", "pdf": "/pdf/08846e7d24687d94ea8f98091ccf05e3f001d22a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nskorokhodov2021class,\ntitle={Class Normalization for (Continual)? Generalized Zero-Shot Learning},\nauthor={Ivan Skorokhodov and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7pgFL2Dkyyy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "7pgFL2Dkyyy", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2020/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2020/Authors|ICLR.cc/2021/Conference/Paper2020/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853146, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment"}}}, {"id": "C1syW17xZ10", "original": null, "number": 3, "cdate": 1605835817285, "ddate": null, "tcdate": 1605835817285, "tmdate": 1605990206017, "tddate": null, "forum": "7pgFL2Dkyyy", "replyto": "XsYE7DtikRt", "invitation": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment", "content": {"title": "Part 2: responses to minor concerns W1-W4", "comment": "### Detailed response\n> W1: The organization of the paper is such that the reader has to refer to the appendix a lot.\n\nWe think this may relate to the misunderstanding we clarified in part 1 and hence we think it is reasonable that we structured the paper in the current form. Based on part1 clarification, we include informal versions of the statements in the main paper and the appendix contains only the fine details for interested readers (assumptions, rigorous formulations, and the proofs). The purpose of the informal statements is to provide the key observations that motivate our techniques. Providing informal statements in the main body and putting the details in the appendix is common (examples [1,2,3,4]). Our main goal is to show the impact of normalization and to understand its effectiveness and we hope that our paper may encourage future work to develop more understanding on top of that.\n- [1] (Jaehoon Lee, etl, NeurIPS, 2019). https://arxiv.org/abs/1902.06720\n- [2] (Qianxiao Li, Shuji Hao etal, 2018 ),  https://arxiv.org/abs/1803.01299\n- [3](Uri Shaham, et,al, 2015)  https://arxiv.org/abs/1509.07385\n- [4] (William H. Guss, et,al,2018 ), https://arxiv.org/abs/1802.04443\n\n>  W1: My biggest concern on clarity is on the \u201ctheoretical\u201d results which are not rigorous and at times unsupported. \n\nCould you please elaborate on what you mean by \u201cnot rigorous and at times unsupported\u201d? If you found a mistake in any of the proofs, we would appreciate it if you\u2019ll point it out. Could you please be specific which theoretical result is \u201cunsupported\u201d and where?\n\n> W1:  Further, some statements/claims are not precise or clear enough for me to be convinced that the method is well-motivated and is doing what it is claimed to be doing.\n\nWe provided extensive empirical validation of our claims and presented it on Figures 1 in the main body of the paper and in appendices A, B, E and F. They clearly demonstrate that those benchmarks which tend to have more steady close-to-unit variance for logits/pre-logits activations \u2014 also tend to perform better in practice also (see Table 2 and Table 4). This gives us the evidence to state that \u201cgood\u201d variance is indeed a good thing. Our class normalization scheme is directly motivated by the theoretical statements: Statement 2 shows the effect of attributes normalization on a linear embedder. And if you now replace a linear embedder with the non-linear one, then to show that the normalization is now lost, you just need to derive the variance formula using the proof Statement 2 but replacing letter \u201ca\u201d with the letter \u201ch\u201d everywhere as we state in Section 3 and Appendix B.\n> If the main claim is really about the effectiveness of the two tricks and the proposed class normalization, then the experiments should go beyond one zero-shot learning starting point --- 3-layer MLP (Table 2)\n\nA 3-layer MLP is a good \u201cworking horse\u201d since it is much simpler than any other ZSL method and it illustrates our point the best. If we would build upon an existing method, that would only obscure things. And since we achieve state-of-the-art performance with a 3-layer MLP \u2014 this only strengthens our point. We believe that our experiments in full demonstrate the effectiveness of the two tricks and the proposed class normalization in zero-shot learning scenario.\n\n> W2\n\n please see in part 1\n\n> W3: Also mention applicability of these tricks\n\nWe thank R1 and we appreciate raising this reasonable concern. Our method is applied in those models which use structured embeddings as an additional input source: zero-shot learning, metric learning, image retrieval, etc. We are adding the discussion to the updated version of the paper and leave the experiments in other areas for future exploration.\n\n>W4:  [Related work] As I mentioned in W3, it is unclear which methods are linear/deep, and which methods have already benefited from existing/proposed tricks.\n\nLinear models were \u201cpopular\u201d in the early years of ZSL [1, 2, 3] and recent methods employ deep embedding-based ZSL models ([4, 5, 6, 7]). Normalizing attributes became the norm after [8] and normalize+scale tricks found its application in ZSL in [7,4] and their continuations.\n- [1] http://proceedings.mlr.press/v37/romera-paredes15.pdf\n- [2] https://proceedings.neurips.cc/paper/2007/file/ed265bc903a5a097f61d3ec064d96d2e-Paper.pdf\n- [3] https://arxiv.org/abs/1409.8403\n- [4] https://arxiv.org/abs/1909.05995\n- [5] https://arxiv.org/abs/1711.06025\n- [6] https://arxiv.org/abs/1611.05088\n- [7] https://papers.nips.cc/paper/2018/hash/1587965fb4d4b5afe8428a4a024feb0d-Abstract.html\n- [8] https://arxiv.org/abs/1703.04394"}, "signatures": ["ICLR.cc/2021/Conference/Paper2020/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Class Normalization for (Continual)? Generalized Zero-Shot Learning", "authorids": ["~Ivan_Skorokhodov1", "~Mohamed_Elhoseiny1"], "authors": ["Ivan Skorokhodov", "Mohamed Elhoseiny"], "keywords": ["zero-shot learning", "normalization", "continual learning", "initialization"], "abstract": "Normalization techniques have proved to be a crucial ingredient of successful training in a traditional supervised learning regime. However, in the zero-shot learning (ZSL) world, these ideas have received only marginal attention. This work studies normalization in ZSL scenario from both theoretical and practical perspectives. First, we give a theoretical explanation to two popular tricks used in zero-shot learning: normalize+scale and attributes normalization and show that they help training by preserving variance during a forward pass. Next, we demonstrate that they are insufficient to normalize a deep ZSL model and propose Class Normalization (CN): a normalization scheme, which alleviates this issue both provably and in practice. Third, we show that ZSL models typically have more irregular loss surface compared to traditional classifiers and that the proposed method partially remedies this problem. Then, we test our approach on 4 standard ZSL datasets and outperform sophisticated modern SotA with a simple MLP optimized without any bells and whistles and having ~50 times faster training speed. Finally, we generalize ZSL to a broader problem \u2014 continual ZSL, and introduce some principled metrics and rigorous baselines for this new setup. The source code is available at https://github.com/universome/class-norm.", "one-sentence_summary": "We develop theoretical understanding of signal normalization inside zero-shot learning models, propose a novel normalization scheme and use it to achieve SotA ZSL performance with a simple MLP", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "skorokhodov|class_normalization_for_continual_generalized_zeroshot_learning", "pdf": "/pdf/08846e7d24687d94ea8f98091ccf05e3f001d22a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nskorokhodov2021class,\ntitle={Class Normalization for (Continual)? Generalized Zero-Shot Learning},\nauthor={Ivan Skorokhodov and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7pgFL2Dkyyy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "7pgFL2Dkyyy", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2020/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2020/Authors|ICLR.cc/2021/Conference/Paper2020/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853146, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment"}}}, {"id": "AaUX3W04a54", "original": null, "number": 12, "cdate": 1605889840914, "ddate": null, "tcdate": 1605889840914, "tmdate": 1605960700986, "tddate": null, "forum": "7pgFL2Dkyyy", "replyto": "prTYPzai0EO", "invitation": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment", "content": {"title": "Authors' response to Reviewer 3 [Part 1]", "comment": "We thank Reviewer3 for the valuable input and for appreciating our contribution. We address below the raised concerns.\n\n> This paper presents a robust analysis of normalization, initialization, and scaling trick for ZSL. It also extends ZSL in continual learning. I appreciate the author's effort for this solid analysis. I expect a new proposed model from authors to make the paper more strong. \n\nWe believe that our paper includes multiple novel contributions. (1) we performed a novel theoretical analysis of normalization in ZSL models and validated it by extensive empirical evidence. (2) we proposed a novel class normalization technique that improves the scores considerably; (3) we also show that Class Normalization can improve performance on a continual version of zero-shot learning, CZSL, that we propose and contrasted it to related settings in the literature. \n\n> 2- Missing comparison: I recommend including paper [a] in the comparison table for CZSL. Approach [a] is the first proposed baseline for continual zero-shot learning. Therefore it must be included in the comparison \n\n1. \u201cDo we claim to be the first to study ZSL in continual setting?\u201d No, but our CZSL setting is different as we explain. In our original submission, we cited and discussed [a] as Wei et al. (2020b)  but we think the first experiments on ZSL for continual learning was even earlier in (Arslan Chaudhry, Marc'aurelio Ranzato, et al.  ICLR,  2019). We developed CZSL as a generalized version of  (Arslan Chaudhry, Marc'aurelio Ranzato, et al.  ICLR,  2019) as we argued in the last paragraph in the Related work. \n\n2. Difference between our new CZSL setting and [a]: CZSL is different from [a] and we think our CZSL benchmarks are both more challenging and more related to our goal. It is a development of the scenario proposed in Chaudhry et al. (2019), but authors there focused on ZSL performance only a single task ahead, while in our case we consider the performance on all seen (previous tasks) and all unseen data (future tasks). This definition also contrasts our work to the very recent work by Wei et al. (2020b), where a sequence of seen class splits of existing ZSL benchmarks is trained and the zero-shot performance is reported for every task individually at test time. In contrast, for our setup, the label space is not restricted and covers the spectrum of all previous tasks (seen tasks so far), and future tasks (unseen tasks so far). Due to this difference, we need to introduce a set of new metrics and benchmarks to measure this continual generalized ZSL skill over time.\u201d\n\n3. \u201dincluding paper [a] (i.e., Wei et al. (2020b)) in the comparison table\u201d:  We tried to find the code/benchmarks of  Wei et al. (2020b) but were not able to find it. We think the current setup is more related to our work. We study the increasing ability of the model to recognize future tasks in a way that is distinguishable from seen classes till task $t$. We also think that the benchmarks we developed fulfills that purpose of showing that Class Norm is helpful for continual ZSL setting. \n\n> Please compare it with [b],[c] models.\n\nThank you for the pointers! We already compare to EPGN [b] (See Table 2) and we will include the comparison to [c] for the updated version of the paper.\n\n> Why have aPY dataset not included in the experiments?\n\nThe reason is that the very recent works tend not to benchmark on aPY. Examples from Table includes  LsrGAN (2020). DVBE (2020), EPGN (2020), TF-VAEGAN (2020), F-VAEGAN-D2 (2019). There is only a single method (DVBE) which computes the scores on aPY.  After taking a deeper look, we think reasons include that small number of classes, and the very sparse attribute representation across classes questioning if it needs to be improved and enriched. . \n\naPY Experiment. We launched our method on aPY without any hyperparameters tuning whatsoever and obtained the score of 38.9. We believe that after tuning the hyperparameters, the scores could be improved. But even at the current state, this puts us on the third place after DVBE which has a score of 41.8 just right after CVC-ZSL with a score of 39.0 (very similar to our score). We prepared a [Google Colab notebook for aPY](https://gist.github.com/iclr2021-classnorm/cebb4dd7ca9ccc9a91d18b84aee056f7) that you can use to reproduce the results. We emphasize how simple our method is: all the code including data downloading/preprocessing/training/etc takes just ~150 lines of code. And for aPY, the training took just 8 seconds!"}, "signatures": ["ICLR.cc/2021/Conference/Paper2020/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Class Normalization for (Continual)? Generalized Zero-Shot Learning", "authorids": ["~Ivan_Skorokhodov1", "~Mohamed_Elhoseiny1"], "authors": ["Ivan Skorokhodov", "Mohamed Elhoseiny"], "keywords": ["zero-shot learning", "normalization", "continual learning", "initialization"], "abstract": "Normalization techniques have proved to be a crucial ingredient of successful training in a traditional supervised learning regime. However, in the zero-shot learning (ZSL) world, these ideas have received only marginal attention. This work studies normalization in ZSL scenario from both theoretical and practical perspectives. First, we give a theoretical explanation to two popular tricks used in zero-shot learning: normalize+scale and attributes normalization and show that they help training by preserving variance during a forward pass. Next, we demonstrate that they are insufficient to normalize a deep ZSL model and propose Class Normalization (CN): a normalization scheme, which alleviates this issue both provably and in practice. Third, we show that ZSL models typically have more irregular loss surface compared to traditional classifiers and that the proposed method partially remedies this problem. Then, we test our approach on 4 standard ZSL datasets and outperform sophisticated modern SotA with a simple MLP optimized without any bells and whistles and having ~50 times faster training speed. Finally, we generalize ZSL to a broader problem \u2014 continual ZSL, and introduce some principled metrics and rigorous baselines for this new setup. The source code is available at https://github.com/universome/class-norm.", "one-sentence_summary": "We develop theoretical understanding of signal normalization inside zero-shot learning models, propose a novel normalization scheme and use it to achieve SotA ZSL performance with a simple MLP", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "skorokhodov|class_normalization_for_continual_generalized_zeroshot_learning", "pdf": "/pdf/08846e7d24687d94ea8f98091ccf05e3f001d22a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nskorokhodov2021class,\ntitle={Class Normalization for (Continual)? Generalized Zero-Shot Learning},\nauthor={Ivan Skorokhodov and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7pgFL2Dkyyy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "7pgFL2Dkyyy", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2020/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2020/Authors|ICLR.cc/2021/Conference/Paper2020/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853146, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment"}}}, {"id": "lU_sY-8sGbH", "original": null, "number": 16, "cdate": 1605959745691, "ddate": null, "tcdate": 1605959745691, "tmdate": 1605959745691, "tddate": null, "forum": "7pgFL2Dkyyy", "replyto": "jugSdCGjJRu", "invitation": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment", "content": {"title": "Appreciate the response", "comment": "Thanks to the authors for the detailed response. I highly appreciate the response and updated paper.\n\nThe author answers all my query and provided the suggested comparison results. I have checked the provided code and able to reproduce the result.\n\nIn the code, the author calibrates the seen class logits by a value 0.95 (hyperparameter) and it is not mentioned in the paper (logits[:, seen_mask] *= 0.95 # Trading a bit of gzsl-s for a bit of gzsl-u). Please mention this hyperparameter in the paper and explain how you obtained it. Also, include a small ablation over this value i.e. using the value [1.0,0.95,0.9,0.8,0.7,0.6,0.5] how results change. It will help to understand the sensitivity of this calibration parameter.\n\nThe proposed model is simple and shows a significant improvement compared to recent baseline. Also, the proposed model is extremely fast to execute. The proposed metric for the continual ZSL is more realistic and the same normalization model helps to improve the continual learning baseline. I believe the proposed approach has good potential to improve the ZSL and continual ZSL framework, also in future, it will be interesting to see how the same normalization can be applied to the generative model and how much improvement we can obtain.\n\nThanks"}, "signatures": ["ICLR.cc/2021/Conference/Paper2020/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Class Normalization for (Continual)? Generalized Zero-Shot Learning", "authorids": ["~Ivan_Skorokhodov1", "~Mohamed_Elhoseiny1"], "authors": ["Ivan Skorokhodov", "Mohamed Elhoseiny"], "keywords": ["zero-shot learning", "normalization", "continual learning", "initialization"], "abstract": "Normalization techniques have proved to be a crucial ingredient of successful training in a traditional supervised learning regime. However, in the zero-shot learning (ZSL) world, these ideas have received only marginal attention. This work studies normalization in ZSL scenario from both theoretical and practical perspectives. First, we give a theoretical explanation to two popular tricks used in zero-shot learning: normalize+scale and attributes normalization and show that they help training by preserving variance during a forward pass. Next, we demonstrate that they are insufficient to normalize a deep ZSL model and propose Class Normalization (CN): a normalization scheme, which alleviates this issue both provably and in practice. Third, we show that ZSL models typically have more irregular loss surface compared to traditional classifiers and that the proposed method partially remedies this problem. Then, we test our approach on 4 standard ZSL datasets and outperform sophisticated modern SotA with a simple MLP optimized without any bells and whistles and having ~50 times faster training speed. Finally, we generalize ZSL to a broader problem \u2014 continual ZSL, and introduce some principled metrics and rigorous baselines for this new setup. The source code is available at https://github.com/universome/class-norm.", "one-sentence_summary": "We develop theoretical understanding of signal normalization inside zero-shot learning models, propose a novel normalization scheme and use it to achieve SotA ZSL performance with a simple MLP", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "skorokhodov|class_normalization_for_continual_generalized_zeroshot_learning", "pdf": "/pdf/08846e7d24687d94ea8f98091ccf05e3f001d22a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nskorokhodov2021class,\ntitle={Class Normalization for (Continual)? Generalized Zero-Shot Learning},\nauthor={Ivan Skorokhodov and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7pgFL2Dkyyy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "7pgFL2Dkyyy", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2020/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2020/Authors|ICLR.cc/2021/Conference/Paper2020/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853146, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment"}}}, {"id": "gcIGABdSmhV", "original": null, "number": 3, "cdate": 1603863486920, "ddate": null, "tcdate": 1603863486920, "tmdate": 1605957613792, "tddate": null, "forum": "7pgFL2Dkyyy", "replyto": "7pgFL2Dkyyy", "invitation": "ICLR.cc/2021/Conference/Paper2020/-/Official_Review", "content": {"title": "Simple and effective approach", "review": "The paper shows that normalization is critical for zero-shot learning (ZSL). In the ZSL randomization is coming from the two sources, attribute and feature.  Normalization of the two source helps to reduce the variance. The paper uses an embedding based model where normalize visual feature are projected to the attribute space and in the attribute space, cosine similarity is measured to predict the class label. Paper also extend ZSL framework to the continual learning ZSL (CZSL) setup where whole data are not present at a time; instead, data comes in the form of a task, sequentially. The author shows that normalization helps to improve the CZSL result.\n\nPositive:\n1: The normalization is important in the NN/CNN model, in the ZSL, there are two sources of information, and proper normalization is important on both source for the better result. The paper gives a theoretical justification of why normalization is important and how we can do the same for the performance gain.\n\n2: The proposed normalization shows the significant performance gain on the standard dataset for the GZSL setup (provided evaluation and code is correct). \n\n3: Recently generative model shows the SOTA result for the GZSL setting since they can synthesize the unseen class samples and easily can handle the data biasness. It is nice to see that the non-generative model shows a significant improvement using the simple method, and it is much faster to train. \n\nComment:\n1: The main concern is the result, I am unable to understand from where the exact gain is coming. Many previous works use Normalize+scale or normalization in the supervised learning or meta-learning scenario; it helps the generalization ability and smooth training and resulting in a performance gain. The performance gain using eq:[9] and [10] is expected, but [9]+[10] (CN) shows the much better result, I am unable to understand why this happens? I request the author; please explain the same.\n\n2: The proposed approach is an embedding based model, it does not generate the samples from the unseen classes, then how model overcome the data biasness towards the seen class? Generally, it observes that seen-class shows the high accuracy and unseen-class show the low accuracy; hence H-mean is very poor. The generative model can handle this scenario since they can generate data from the unseen class. The normalization technique does not help to overcome the data biasness towards the seen-class then how model handle the data biases?\n\n3: I appreciate the theoretical justification and identifying the problem in the deep model and providing the solution for that (section 3.4)\n\n4: The comparison with the few recent meta-learning based approach [a] [b] [c] for the ZSL are missing, can you show the result compared with these approaches? Also, I request the author please provide the result if the same normalization is applied with the approach [c] (it is also embedding based model in the meta-learning framework). \n\n[a] Episode-based Prototype Generating Network for Zero-Shot Learning, CVPR-20\n[b] Meta-Learning for Generalized Zero-Shot Learning\n[c] Learning to Compare: Relation Network for Few-Shot Learning, CVPR=18\n\nOverall I like the idea and contribution, but I suspect the provided result for the GZSL result in table-2, I request the author please provide the code for the AWA2 and AWA1 dataset. I will further increase the score on the successful verification of the result.\n\n4: If you don't use cosine similarity, then what is the dependency relation between the variance and weight W. I mean in the statement-1 if we don't use normalization then how variance depends on weight W? Also Var[\\hat(y)_c] is independent of W, but learning is not independent of the initialization of W, in this case, variance does not matter, proper learning and generalization are more important.\n\n5: I agree with the author that the provided evaluation metric for the CZSL is more generic and realistic. Here the model and CL procedure is not clear. What is Multi-task? What is sequential? How you ensure to overcome the catastrophic forgetting over the previous task while training the current task, The proper description is not provided, I request the author please provide the same.\n\n6: In continual learning scenario with the increase of task, the model performance is degraded. It is shown in the figure-9, and 10 (supplementary) with the increase of the task model's performance is increasing, it means that model does not forget anything and also you have backward transfer how this is possible? Maybe I misunderstood something please explain.", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2020/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Class Normalization for (Continual)? Generalized Zero-Shot Learning", "authorids": ["~Ivan_Skorokhodov1", "~Mohamed_Elhoseiny1"], "authors": ["Ivan Skorokhodov", "Mohamed Elhoseiny"], "keywords": ["zero-shot learning", "normalization", "continual learning", "initialization"], "abstract": "Normalization techniques have proved to be a crucial ingredient of successful training in a traditional supervised learning regime. However, in the zero-shot learning (ZSL) world, these ideas have received only marginal attention. This work studies normalization in ZSL scenario from both theoretical and practical perspectives. First, we give a theoretical explanation to two popular tricks used in zero-shot learning: normalize+scale and attributes normalization and show that they help training by preserving variance during a forward pass. Next, we demonstrate that they are insufficient to normalize a deep ZSL model and propose Class Normalization (CN): a normalization scheme, which alleviates this issue both provably and in practice. Third, we show that ZSL models typically have more irregular loss surface compared to traditional classifiers and that the proposed method partially remedies this problem. Then, we test our approach on 4 standard ZSL datasets and outperform sophisticated modern SotA with a simple MLP optimized without any bells and whistles and having ~50 times faster training speed. Finally, we generalize ZSL to a broader problem \u2014 continual ZSL, and introduce some principled metrics and rigorous baselines for this new setup. The source code is available at https://github.com/universome/class-norm.", "one-sentence_summary": "We develop theoretical understanding of signal normalization inside zero-shot learning models, propose a novel normalization scheme and use it to achieve SotA ZSL performance with a simple MLP", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "skorokhodov|class_normalization_for_continual_generalized_zeroshot_learning", "pdf": "/pdf/08846e7d24687d94ea8f98091ccf05e3f001d22a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nskorokhodov2021class,\ntitle={Class Normalization for (Continual)? Generalized Zero-Shot Learning},\nauthor={Ivan Skorokhodov and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7pgFL2Dkyyy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "7pgFL2Dkyyy", "replyto": "7pgFL2Dkyyy", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2020/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538105739, "tmdate": 1606915765215, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2020/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2020/-/Official_Review"}}}, {"id": "UqTxwzFrKG", "original": null, "number": 14, "cdate": 1605902346619, "ddate": null, "tcdate": 1605902346619, "tmdate": 1605903663034, "tddate": null, "forum": "7pgFL2Dkyyy", "replyto": "rJAddEroK-W", "invitation": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment", "content": {"title": "Authors' response to Reviewer 4", "comment": "We thank Reviewer4 for the valuable input and for appreciating our contribution. We address below the raised concerns.\n\n> The paper didn't evaluate on another widely used benchmark dataset aPY, can author explain the reason?\n\nThe reason is that the very recent works tend not to benchmark on aPY. Examples from Table includes  LsrGAN (2020). DVBE (2020), EPGN (2020), TF-VAEGAN (2020), F-VAEGAN-D2 (2019). There is only a single method (DVBE) which computes the scores on aPY.  After taking a deeper look, we think reasons \tinclude that small number of classes, and the very sparse attribute representation across classes questioning if it needs to be improved and enriched. . \n\naPY Experiment. We launched our method on aPY without any hyperparameters tuning whatsoever and obtained the score of 38.9. We believe that after tuning the hyperparameters, the scores could be improved. But even at the current state, this puts us on the third place after DVBE which has a score of 41.8 just right after CVC-ZSL with a score of 39.0 (very similar to our score). We prepared a [Google Colab notebook for aPY](https://gist.github.com/iclr2021-classnorm/cebb4dd7ca9ccc9a91d18b84aee056f7) that you can use to reproduce the results. We emphasize how simple our method is: all the code including data downloading/preprocessing/training/etc takes just ~150 lines of code. And for aPY, the training took just 8 seconds!\n\n> On CUB dataset, the proposed method has a considerably large margin to the state-of-the-art methods, in contrast to other datasets. Is there any explanation on why it is the case? Have you tried to explain this failure especially from the perspective of the proposed normalization trick?\n\nWe hypothesize that our method improves the scores by inherently allowing a model to better capture a signal from those attribute dimensions that are usually suppressed due to their low magnitude and by reducing the excessive influence of attributes with too high magnitudes. And for CUB, attributes are really well-balanced in terms of magnitudes compared to other datasets. We attach [histograms of averaged attributes magnitudes values](https://www.dropbox.com/s/xxo2mhdhl76q7rh/attributes-avg-magnitudes-hists.pdf?dl=0) for SUN, CUB, AwA and aPY. As you can see from these histograms, the attributes magnitude distribution for CUB has a much smaller tail, i.e. it is less \u201cskewed\u201d to the right. This makes the signal from large-magnitude attributes not suppress small-magnitude attributes as much as for other datasets. We achieve the best performance on AwA1/AwA2 and aPY datasets where the distribution is the most long-tailed (as we said, for aPY we achieved a GZSL-H score of 38.9 with the very first try without any bells and whistles).\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2020/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Class Normalization for (Continual)? Generalized Zero-Shot Learning", "authorids": ["~Ivan_Skorokhodov1", "~Mohamed_Elhoseiny1"], "authors": ["Ivan Skorokhodov", "Mohamed Elhoseiny"], "keywords": ["zero-shot learning", "normalization", "continual learning", "initialization"], "abstract": "Normalization techniques have proved to be a crucial ingredient of successful training in a traditional supervised learning regime. However, in the zero-shot learning (ZSL) world, these ideas have received only marginal attention. This work studies normalization in ZSL scenario from both theoretical and practical perspectives. First, we give a theoretical explanation to two popular tricks used in zero-shot learning: normalize+scale and attributes normalization and show that they help training by preserving variance during a forward pass. Next, we demonstrate that they are insufficient to normalize a deep ZSL model and propose Class Normalization (CN): a normalization scheme, which alleviates this issue both provably and in practice. Third, we show that ZSL models typically have more irregular loss surface compared to traditional classifiers and that the proposed method partially remedies this problem. Then, we test our approach on 4 standard ZSL datasets and outperform sophisticated modern SotA with a simple MLP optimized without any bells and whistles and having ~50 times faster training speed. Finally, we generalize ZSL to a broader problem \u2014 continual ZSL, and introduce some principled metrics and rigorous baselines for this new setup. The source code is available at https://github.com/universome/class-norm.", "one-sentence_summary": "We develop theoretical understanding of signal normalization inside zero-shot learning models, propose a novel normalization scheme and use it to achieve SotA ZSL performance with a simple MLP", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "skorokhodov|class_normalization_for_continual_generalized_zeroshot_learning", "pdf": "/pdf/08846e7d24687d94ea8f98091ccf05e3f001d22a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nskorokhodov2021class,\ntitle={Class Normalization for (Continual)? Generalized Zero-Shot Learning},\nauthor={Ivan Skorokhodov and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7pgFL2Dkyyy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "7pgFL2Dkyyy", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2020/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2020/Authors|ICLR.cc/2021/Conference/Paper2020/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853146, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment"}}}, {"id": "uzWjcIHfTVz", "original": null, "number": 9, "cdate": 1605848101799, "ddate": null, "tcdate": 1605848101799, "tmdate": 1605894841526, "tddate": null, "forum": "7pgFL2Dkyyy", "replyto": "gcIGABdSmhV", "invitation": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment", "content": {"title": "Part 3: Collab/Code on AWA1 and AWA2 ", "comment": "> Overall I like the idea and contribution, but I suspect the provided result for the GZSL result in table-2, I request the author please provide the code for the AWA2 and AWA1 dataset. I will further increase the score on the successful verification of the result.\n\nWe prepared a minimal example of our approach as an [anonymized Google Colab Notebook](https://gist.github.com/iclr2021-classnorm/c29c8a1d4da78eb75a4cae24348b061d) that you can use to reproduce our results on AwA1/AwA2. It runs our method from the very scratch: from downloading the official data from the GBU website, preprocessing it, training and evaluating the model. We will release the full codebase of our project upon acceptance. We also revised the hyperparameters so they work better for the baselines. This decreases the gap between them to a value you observe for SUN/CUB datasets in Table 2. \n\nThe implementation takes less than 150 lines of code including the code for data downloading, preprocessing, training, evaluation, and occasional comments. And the training time for it takes ~30 seconds! We hope that the simplicity of our approach is appreciated by the ML community. \n\nAlso, after submission, we have found a mistake in our AWA1&2 ZSL evaluation procedure that made the model being evaluated in a class-balanced setting. However, AwA1 and AwA2 datasets are class-imbalanced (SUN and CUB are perfectly class-balanced and hence the results for them do not change). After fixing this error, our scores decreased noticeably but not dramatically. We still obtain SotA performance on AwA2 of GZSL-H = 67.6 with the closest runner up being DVBE with 67.0. Our results on AwA1 is 67.8, which is inferior to EPGN and CVC-ZSL having 71.2 and 69.1 respectively. Of course, we updated our results in the paper as well.\n\nAlso, for your convenience, we launched the model for each setup of interest and saved the results as github gists (it just should be easier to browse things that way). They all differ only in the very first cell where we select the dataset and boolean flags for using/not using equation 9 or equation 10. Every other line of their code is the same:\n- [AWA1 (GZSL-H: 67.83)](https://gist.github.com/iclr2021-classnorm/c29c8a1d4da78eb75a4cae24348b061d)\n- [AWA1 without eq 9 (GZSL-H: 63.97)](https://gist.github.com/iclr2021-classnorm/22f8a40cd475898a7c25578a61c41810)\n- [AWA1 without eq 10 (GZSL-H: 65.94)](https://gist.github.com/iclr2021-classnorm/6a0abab160569573aebc54bc88c9dfef)\n- [AWA1 without eq 9 and eq 10 (GZSL-H: 62.77)](https://gist.github.com/iclr2021-classnorm/4f3c2d1a58d3e4321e545756908751a4)\n\n- [AWA2 (GZSL-H: 67.60)](https://gist.github.com/iclr2021-classnorm/dd0e89521be029dac63d1d8b0a2d9401)\n- [AWA2 without eq 9 (GZSL-H: 60.46)](https://gist.github.com/iclr2021-classnorm/d7e1723fbe6b2ed17c081b172e9c41b4)\n- [AWA2 without eq 10 (GZSL-H: 67.09)](https://gist.github.com/iclr2021-classnorm/5312dd0ccb84ae27e353213544720ad7)\n- [AWA2 without eq 9 and eq 10 (GZSL-H: 62.06)](https://gist.github.com/iclr2021-classnorm/c0a2d29c1f6b6eaed33392b930befe85)\n\n In our case, the hardware was a Tesla P100 GPU (from `nvidia-smi`) and an Intel(R) Xeon(R) CPU @ 2.20GHz (from `cat /proc/cpuinfo`) which we recommend to reproduce our results. It may differ slightly if it was run on different hardware since that floating-point operation (summation/multiplication) may depend on each hardware. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2020/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Class Normalization for (Continual)? Generalized Zero-Shot Learning", "authorids": ["~Ivan_Skorokhodov1", "~Mohamed_Elhoseiny1"], "authors": ["Ivan Skorokhodov", "Mohamed Elhoseiny"], "keywords": ["zero-shot learning", "normalization", "continual learning", "initialization"], "abstract": "Normalization techniques have proved to be a crucial ingredient of successful training in a traditional supervised learning regime. However, in the zero-shot learning (ZSL) world, these ideas have received only marginal attention. This work studies normalization in ZSL scenario from both theoretical and practical perspectives. First, we give a theoretical explanation to two popular tricks used in zero-shot learning: normalize+scale and attributes normalization and show that they help training by preserving variance during a forward pass. Next, we demonstrate that they are insufficient to normalize a deep ZSL model and propose Class Normalization (CN): a normalization scheme, which alleviates this issue both provably and in practice. Third, we show that ZSL models typically have more irregular loss surface compared to traditional classifiers and that the proposed method partially remedies this problem. Then, we test our approach on 4 standard ZSL datasets and outperform sophisticated modern SotA with a simple MLP optimized without any bells and whistles and having ~50 times faster training speed. Finally, we generalize ZSL to a broader problem \u2014 continual ZSL, and introduce some principled metrics and rigorous baselines for this new setup. The source code is available at https://github.com/universome/class-norm.", "one-sentence_summary": "We develop theoretical understanding of signal normalization inside zero-shot learning models, propose a novel normalization scheme and use it to achieve SotA ZSL performance with a simple MLP", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "skorokhodov|class_normalization_for_continual_generalized_zeroshot_learning", "pdf": "/pdf/08846e7d24687d94ea8f98091ccf05e3f001d22a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nskorokhodov2021class,\ntitle={Class Normalization for (Continual)? Generalized Zero-Shot Learning},\nauthor={Ivan Skorokhodov and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7pgFL2Dkyyy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "7pgFL2Dkyyy", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2020/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2020/Authors|ICLR.cc/2021/Conference/Paper2020/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853146, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment"}}}, {"id": "jugSdCGjJRu", "original": null, "number": 7, "cdate": 1605847410787, "ddate": null, "tcdate": 1605847410787, "tmdate": 1605891260543, "tddate": null, "forum": "7pgFL2Dkyyy", "replyto": "gcIGABdSmhV", "invitation": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment", "content": {"title": "Part 1: CN, Equation 9 and 10 and Adding [a,b,c] to the paper ", "comment": "We thank Reviewer2 for the valuable feedback. We here address the comments and will incorporate all the feedback.  \n\n\n> The main concern is the result, I am unable to understand from where the exact gain is coming. Many previous works use Normalize+scale or normalization in the supervised learning or meta-learning scenario; it helps the generalization ability and smooth training and resulting in a performance gain. The performance gain using eq:[9] and [10] is expected, but [9]+[10] (CN) shows the much better result, I am unable to understand why this happens? I request the author; please explain the same.\n\nAs you correctly noted, incorporating CN in its \u201cfull\u201d form (i.e. using both eq 9 and eq 10) gives the best performance due to the better signal normalization and improved smoothness. The theoretical problem of using only eq 9 or eq 10 is that there is just no guarantee that this will lead to anything good. And from the practical perspective, we think that it should be beneficial to use eq 9 because standardization procedure smoothes the loss surface (see Appendix F and [1]). For example, Table 2 shows that just using eq 9 gives close performance as using eq 9 + eq 10. Eq 10 only does not perform well because without eq 9 it may make your signal vanish since what it does under the hood is just reducing the scale for the output matrix to make things align with the theory (which in turn leads to the better performance). \n- [1] https://arxiv.org/abs/1805.11604\n\n> The proposed approach is an embedding based model, it does not generate the samples from the unseen classes, then how does the model overcome the data biasness towards the seen class?\n\nThanks for asking this interesting question. The \u201ctrick\u201d is the following: previously (~before [1]), people trained a model which projects data onto the labels, i.e. a mapping X -> A from data space X to attribute space A. And this results in the very exact behaviour you foresee: the model becomes biased towards the seen representations which result in very low performance on the unseen. In our case, we follow the idea of [1, 2] and learn the mapping A->X. We describe the setting in Section 3 and depict the model on Figure 3. Table 2 of [2] shows how much difference this makes compared to using X->A projection. Since the model does not a direct access to seen data anymore, and its access to attributes happens only at the final discrimination phase, this avoids it getting biased towards the seen. Thank you for bringing this up, we will include this exposition in the updated version of the paper.\n- [1]Learning a Deep Embedding Model for Zero-Shot Learning Li Zhang, Tao Xiang, Shaogang Gong,  https://arxiv.org/abs/1611.05088\n- [2] Rethinking Zero-Shot Learning: A Conditional Visual Classification Perspective, https://arxiv.org/abs/1909.05995\n\n> The comparison with the few recent meta-learning based approach [a] [b] [c] for the ZSL are missing, can you show the result compared with these approaches?\n\nWe already compare to [a] (see Table 2) and we will shortly include the missing comparisons in the nearest update.\n\nWe follow up on the remaining concerns. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2020/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Class Normalization for (Continual)? Generalized Zero-Shot Learning", "authorids": ["~Ivan_Skorokhodov1", "~Mohamed_Elhoseiny1"], "authors": ["Ivan Skorokhodov", "Mohamed Elhoseiny"], "keywords": ["zero-shot learning", "normalization", "continual learning", "initialization"], "abstract": "Normalization techniques have proved to be a crucial ingredient of successful training in a traditional supervised learning regime. However, in the zero-shot learning (ZSL) world, these ideas have received only marginal attention. This work studies normalization in ZSL scenario from both theoretical and practical perspectives. First, we give a theoretical explanation to two popular tricks used in zero-shot learning: normalize+scale and attributes normalization and show that they help training by preserving variance during a forward pass. Next, we demonstrate that they are insufficient to normalize a deep ZSL model and propose Class Normalization (CN): a normalization scheme, which alleviates this issue both provably and in practice. Third, we show that ZSL models typically have more irregular loss surface compared to traditional classifiers and that the proposed method partially remedies this problem. Then, we test our approach on 4 standard ZSL datasets and outperform sophisticated modern SotA with a simple MLP optimized without any bells and whistles and having ~50 times faster training speed. Finally, we generalize ZSL to a broader problem \u2014 continual ZSL, and introduce some principled metrics and rigorous baselines for this new setup. The source code is available at https://github.com/universome/class-norm.", "one-sentence_summary": "We develop theoretical understanding of signal normalization inside zero-shot learning models, propose a novel normalization scheme and use it to achieve SotA ZSL performance with a simple MLP", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "skorokhodov|class_normalization_for_continual_generalized_zeroshot_learning", "pdf": "/pdf/08846e7d24687d94ea8f98091ccf05e3f001d22a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nskorokhodov2021class,\ntitle={Class Normalization for (Continual)? Generalized Zero-Shot Learning},\nauthor={Ivan Skorokhodov and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7pgFL2Dkyyy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "7pgFL2Dkyyy", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2020/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2020/Authors|ICLR.cc/2021/Conference/Paper2020/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853146, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment"}}}, {"id": "m0A0af2psAk", "original": null, "number": 6, "cdate": 1605835927706, "ddate": null, "tcdate": 1605835927706, "tmdate": 1605890402683, "tddate": null, "forum": "7pgFL2Dkyyy", "replyto": "XsYE7DtikRt", "invitation": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment", "content": {"title": "Part 5: Statement 3 and other minor comments ", "comment": "> Why is \u201cpreserving the variance between z and y~\u201d in Statement 3 important for zero-shot learning?\n\nPreserving the variance was shown to be effective in classical supervised learning, demonstrated by Xavier/Kaiming inits. It make the signal propagation independent of the width (at least asymptotically) and making us to more likely expect good gradients in return. This motivates us to study the variance effect in ZSL task to see if it provides performance gains in this case. The precise, direct way of how the variance changes the final model performance is unknown even for the supervised learning regime.\n\n> Abstract: Are the authors the one to \u201cgeneralize ZSL to a broader problem\u201d? Please tone down the claim if not.\n\nOur CZSL is formulated in such a way that ZSL is a specific case of it when there are only 2 tasks, where the first task has the seen classes and the second task has the unseen classes. Thus CZSL is a generalization of ZSL.\n\n> After Eq. (2): Why does attribute normalization look \u201cinconsiderable\u201d (possibly this is not the right word?) or why is it \u201csurprising\u201d that this is preferred in practice? Don\u2019t most zero-shot learning methods use this (see for example Table 4 in [A])?\n\nA common practice is to normalize your data to zero-mean and unit-variance and here we normalize attributes to a unit norm instead which diverges with that common practice. Thus we found this to be surprising.\n\n> Suggestions for references for attribute normalization. This can be improved; I can trace this back to much earlier work such as [A] and [B] (though I think this fact is stated more explicitly in [A]).\n\nThanks, we will incorporate this in the revised version of the paper.\n\n> Under Table 1 \u201cThese two tricks work well and normalize the variance to a unit value when the underlying ZSL model is linear (see Figure 1), but they fail when we use a multi-layer architecture.\u201d: Could the authors provide a reference to evidence to support this? I think it is also important to provide a clear statement of what separates a \u201clinear\u201d or \u201cmulti-layer\u201d model.\n\nFigure 1 (two left plots) and Figures 4,5,6,7 illustrate the variances for different setups. In particular, you may see that while the variance for a linear model +NS+AN is close to unit, the variance for the non-linear model +NS+AN noticeably reduces below 1. Adding CN improves the situation. The difference between a linear and a multi-layer model is in the amount of layers being used. For a linear model we use just a single layer, for a multi-layer model we use 2 additional layers.\n\n> The first paragraph of Sect. 3: Could you provide references for motivations for different activation functions? Further, It is unclear that all of them perform normalization.\n\nCould you please elaborate on your request? It is true that not all activation functions perform normalization, but some of them do, like SELU [1]. However, it is not clear how it is related to the first paragraph of Section 3. Motivations for different activation functions are found in papers that introduce them. \n- [1] https://arxiv.org/abs/1706.02515\n\n> The second paragraph of Sect. 3: What exactly limits \u201cthe tools\u201d for zero-shot learning vs. supervised learning? Further, it would also be nice to separate traditional supervised learning where classes are balanced and imbalanced; see, e.g., [C].\n\nFirst, the task is different and you have to consider two different sources of inputs. This introduces some \u201cfusion\u201d step between the data and the attributes which complicates things. Second, attributes follow a different distribution compared to what one would assume for \u201ctraditional\u201d data.\n\n> What is the closest existing zero-shot model to the one the authors describe in Sect. 3.1? Why is the described model considered/selected? \n\nThe closest model is an embedding-based model from [https://arxiv.org/abs/1909.05995], but we do not use their sophisticated episode-based training scheme. We build upon it since it is much simpler (when you remove the episode-based training scheme) and provides decent performance (see Table 1 and Table 2 of our paper)."}, "signatures": ["ICLR.cc/2021/Conference/Paper2020/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Class Normalization for (Continual)? Generalized Zero-Shot Learning", "authorids": ["~Ivan_Skorokhodov1", "~Mohamed_Elhoseiny1"], "authors": ["Ivan Skorokhodov", "Mohamed Elhoseiny"], "keywords": ["zero-shot learning", "normalization", "continual learning", "initialization"], "abstract": "Normalization techniques have proved to be a crucial ingredient of successful training in a traditional supervised learning regime. However, in the zero-shot learning (ZSL) world, these ideas have received only marginal attention. This work studies normalization in ZSL scenario from both theoretical and practical perspectives. First, we give a theoretical explanation to two popular tricks used in zero-shot learning: normalize+scale and attributes normalization and show that they help training by preserving variance during a forward pass. Next, we demonstrate that they are insufficient to normalize a deep ZSL model and propose Class Normalization (CN): a normalization scheme, which alleviates this issue both provably and in practice. Third, we show that ZSL models typically have more irregular loss surface compared to traditional classifiers and that the proposed method partially remedies this problem. Then, we test our approach on 4 standard ZSL datasets and outperform sophisticated modern SotA with a simple MLP optimized without any bells and whistles and having ~50 times faster training speed. Finally, we generalize ZSL to a broader problem \u2014 continual ZSL, and introduce some principled metrics and rigorous baselines for this new setup. The source code is available at https://github.com/universome/class-norm.", "one-sentence_summary": "We develop theoretical understanding of signal normalization inside zero-shot learning models, propose a novel normalization scheme and use it to achieve SotA ZSL performance with a simple MLP", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "skorokhodov|class_normalization_for_continual_generalized_zeroshot_learning", "pdf": "/pdf/08846e7d24687d94ea8f98091ccf05e3f001d22a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nskorokhodov2021class,\ntitle={Class Normalization for (Continual)? Generalized Zero-Shot Learning},\nauthor={Ivan Skorokhodov and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7pgFL2Dkyyy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "7pgFL2Dkyyy", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2020/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2020/Authors|ICLR.cc/2021/Conference/Paper2020/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853146, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment"}}}, {"id": "BGIVxjg06-k", "original": null, "number": 11, "cdate": 1605889546501, "ddate": null, "tcdate": 1605889546501, "tmdate": 1605889546501, "tddate": null, "forum": "7pgFL2Dkyyy", "replyto": "prTYPzai0EO", "invitation": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment", "content": {"title": "Authors' response to Reviewer 3 [Part 2]", "comment": "> Is it possible for this normalization and scaling tricks for other applications such as object detection, action recognition, and image retrieval? \n\nWe think class normalization can be applied to other domains with multiple modalities and where $y$ is a continuous variable as in zero-shot learning represented by class attributes or image-sentence retrieval. We chose to focus in this paper on the zero-shot learning and continual ZSL tasks, due to both their importance and clarity of evaluation. \n\n> You should have compared the timings with other initialization and default normalization and scaling tricks as well.\n\nOther initialization and normalization techniques would have precisely the same timings, but very low accuracy (see table 5). Our proposed model works that well because it follows a rigorously derived normalization scheme which adds negligible computation overhead. While modern ZSL methods also attain good performance \u2014 they achieve at much higher computational cost by using some sophisticated training, like GAN/VAE training with intricate loss terms or episode-based/meta-learning training schemes. This contrast is at the heart of our work: you do not need anything sophisticated to obtain SotA results, all you need is a good signal normalization.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2020/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Class Normalization for (Continual)? Generalized Zero-Shot Learning", "authorids": ["~Ivan_Skorokhodov1", "~Mohamed_Elhoseiny1"], "authors": ["Ivan Skorokhodov", "Mohamed Elhoseiny"], "keywords": ["zero-shot learning", "normalization", "continual learning", "initialization"], "abstract": "Normalization techniques have proved to be a crucial ingredient of successful training in a traditional supervised learning regime. However, in the zero-shot learning (ZSL) world, these ideas have received only marginal attention. This work studies normalization in ZSL scenario from both theoretical and practical perspectives. First, we give a theoretical explanation to two popular tricks used in zero-shot learning: normalize+scale and attributes normalization and show that they help training by preserving variance during a forward pass. Next, we demonstrate that they are insufficient to normalize a deep ZSL model and propose Class Normalization (CN): a normalization scheme, which alleviates this issue both provably and in practice. Third, we show that ZSL models typically have more irregular loss surface compared to traditional classifiers and that the proposed method partially remedies this problem. Then, we test our approach on 4 standard ZSL datasets and outperform sophisticated modern SotA with a simple MLP optimized without any bells and whistles and having ~50 times faster training speed. Finally, we generalize ZSL to a broader problem \u2014 continual ZSL, and introduce some principled metrics and rigorous baselines for this new setup. The source code is available at https://github.com/universome/class-norm.", "one-sentence_summary": "We develop theoretical understanding of signal normalization inside zero-shot learning models, propose a novel normalization scheme and use it to achieve SotA ZSL performance with a simple MLP", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "skorokhodov|class_normalization_for_continual_generalized_zeroshot_learning", "pdf": "/pdf/08846e7d24687d94ea8f98091ccf05e3f001d22a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nskorokhodov2021class,\ntitle={Class Normalization for (Continual)? Generalized Zero-Shot Learning},\nauthor={Ivan Skorokhodov and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7pgFL2Dkyyy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "7pgFL2Dkyyy", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2020/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2020/Authors|ICLR.cc/2021/Conference/Paper2020/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853146, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment"}}}, {"id": "y_Ofd3n25PD", "original": null, "number": 10, "cdate": 1605848242237, "ddate": null, "tcdate": 1605848242237, "tmdate": 1605856682631, "tddate": null, "forum": "7pgFL2Dkyyy", "replyto": "gcIGABdSmhV", "invitation": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment", "content": {"title": "Part4: response to other questions. ", "comment": "> 4: If you don't use cosine similarity, then what is the dependency relation between the variance and weight W. I mean in the statement-1 if we don't use normalization then how variance depends on weight W?\n\nIf you do not use the normalize+scale, then the dependence would be the following (the derivation is similar to the derivation for other statements):\n$$\ny_c = z^\\top Wh_c \\Longrightarrow Var[y_c] = d_z^2 \\cdot Var[z_i] \\cdot Var[W_{ij}] \\cdot E[\\|\\| h_c \\|\\|_2^2]\n$$\nSo, it would depend linearly on the variance of $W$, multiplied by a very large constant $d_z^2$ (for our datasets of consideration, $d_z = 2048$).\n\n> 5: I agree with the author that the provided evaluation metric for the CZSL is more generic and realistic. Here the model and CL procedure is not clear. What is Multi-task? What is sequential? \n\nMulti-task model is a model that has access to all the previous data. In this way, it is an upper bound on the performance in CL. Sequential is a model that does not use any continual learning technique for its training and is used as a lower bound on the performance in CL. Thank you for the notice, we will add the description in the paper.\n\n> How you ensure to overcome the catastrophic forgetting over the previous task while training the current task, The proper description is not provided, I request the author please provide the same.\n\nCZSL provides you two ways to fight the catastrophic forgetting:\n1. By incorporating CL techniques like EWC/MAS/A-GEM/etc that directly address the forgetting.\n2. By incorporating ZSL techniques like our class normalization. And we believe that it is a much more interesting way. Because when improving your ZSL performance your model can in theory have \u201cnegative\u201d forgetting (backward transfer), i.e. it can improve on a task after it learned it long time ago. It does so by learning more general attributes representations.\n\n> 6: In continual learning scenario with the increase of task, the model performance is degraded. It is shown in the figure-9, and 10 (supplementary) with the increase of the task model's performance is increasing, it means that model does not forget anything and also you have backward transfer how this is possible? Maybe I misunderstood something please explain.\n\nThe main reason why our performance improves over time is because we consider the generalized setting, at each timestep we evaluate the model on all the tasks, including all the future ones. Imagine that we have 20 tasks to learn progressively on. After each task the model evaluated on all the 20 tasks. This means that at timestep 1 model\u2019s objective is not easier that at timestep 20: it still had to solve all 20 tasks. This is a distinguishing feature of the generalized setting. In traditional continual learning, a model\u2019s objective becomes harder and harder over time: after the first task, it should be able to solve well only the first task; after task 20 \u2014 it should be able to solve all 20 tasks. Thus in CL its performance decreases.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2020/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Class Normalization for (Continual)? Generalized Zero-Shot Learning", "authorids": ["~Ivan_Skorokhodov1", "~Mohamed_Elhoseiny1"], "authors": ["Ivan Skorokhodov", "Mohamed Elhoseiny"], "keywords": ["zero-shot learning", "normalization", "continual learning", "initialization"], "abstract": "Normalization techniques have proved to be a crucial ingredient of successful training in a traditional supervised learning regime. However, in the zero-shot learning (ZSL) world, these ideas have received only marginal attention. This work studies normalization in ZSL scenario from both theoretical and practical perspectives. First, we give a theoretical explanation to two popular tricks used in zero-shot learning: normalize+scale and attributes normalization and show that they help training by preserving variance during a forward pass. Next, we demonstrate that they are insufficient to normalize a deep ZSL model and propose Class Normalization (CN): a normalization scheme, which alleviates this issue both provably and in practice. Third, we show that ZSL models typically have more irregular loss surface compared to traditional classifiers and that the proposed method partially remedies this problem. Then, we test our approach on 4 standard ZSL datasets and outperform sophisticated modern SotA with a simple MLP optimized without any bells and whistles and having ~50 times faster training speed. Finally, we generalize ZSL to a broader problem \u2014 continual ZSL, and introduce some principled metrics and rigorous baselines for this new setup. The source code is available at https://github.com/universome/class-norm.", "one-sentence_summary": "We develop theoretical understanding of signal normalization inside zero-shot learning models, propose a novel normalization scheme and use it to achieve SotA ZSL performance with a simple MLP", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "skorokhodov|class_normalization_for_continual_generalized_zeroshot_learning", "pdf": "/pdf/08846e7d24687d94ea8f98091ccf05e3f001d22a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nskorokhodov2021class,\ntitle={Class Normalization for (Continual)? Generalized Zero-Shot Learning},\nauthor={Ivan Skorokhodov and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7pgFL2Dkyyy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "7pgFL2Dkyyy", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2020/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2020/Authors|ICLR.cc/2021/Conference/Paper2020/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853146, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment"}}}, {"id": "NC0QwNxWMB9", "original": null, "number": 8, "cdate": 1605847509384, "ddate": null, "tcdate": 1605847509384, "tmdate": 1605856120956, "tddate": null, "forum": "7pgFL2Dkyyy", "replyto": "gcIGABdSmhV", "invitation": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment", "content": {"title": "Part2: Experiment: Adding Class Norm to Learning to Compare, CVPR18", "comment": "> 4) Also, I request the author please provide the result if the same normalization is applied with the approach [c] (it is also embedding based models in the meta-learning framework).\n\nWe did what you proposed and obtained the following results. First, we launched the [official source code as a baseline](https://github.com/lzrobots/LearningToCompare_ZSL) with the official hyperparameter just to see how it performs without our class normalization:\n\nAwA1:\n- ZSL-U: 0.6872959679507896\n- GZSL-U: 0.28563595890010673\n- GZSL-S: 0.8783777127463169 \n- GZSL-H: 0.4310881673784809\n\nAwA2:\n- ZSL-U: 0.6517477335764157\n- GZSL-U:  0.10234362179045627\n- GZSL-S: 0.8809926564737267\n- GZSL-H: 0.1833838153382825\n\nCUB:\n- ZSL-U: 0.553778366712299\n- GZSL-U: 0.38336992978680834\n- GZSL-S: 0.6238278850778848\n- GZSL-H: 0.47489549514856744\n\nSo, for some reason, the official baseline with the official hyperparameters diverged for us for AwA2 (the official reported result is 45.3) and we are not yet sure why. This much lower than expected performance is also observed in some of the [github issues](https://github.com/lzrobots/LearningToCompare_ZSL/issues/7).\nThen, we incorporated our class normalization to AttributeNetwork (which is a 2-layer MLP that embeds the attributes) and observed the following improvement in terms of scores:\n\nAwA1 + CN:\n- ZSL-U: 0.7104774542522156\n- GZSL-U: 0.29828608876422863\n- GZSL-S: 0.883896683187865\n- GZSL-H: 0.44604623033783647\n\nAwA2  + CN:\n- ZSL-U: 0.6519904295711082\n- GZSL-U: 0.12715849639164672\n- GZSL-S: 0.8884011086738732\n- GZSL-H: 0.2224738924395352\n\nCUB + CN:\n- ZSL-U: 0.5628699969293023\n- GZSL-U: 0.4006298170771589\n- GZSL-S: 0.6281890183721925\n- GZSL-H: 0.48924308702107\n\nWe believe that the performance may be improved by selecting the hyperparameters that are different from the official ones but we have not done this exploration. We will incorporate these results into the paper after we\u2019ll figure out why the official code diverges for AwA2 for both their provided setup and our class normalization."}, "signatures": ["ICLR.cc/2021/Conference/Paper2020/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Class Normalization for (Continual)? Generalized Zero-Shot Learning", "authorids": ["~Ivan_Skorokhodov1", "~Mohamed_Elhoseiny1"], "authors": ["Ivan Skorokhodov", "Mohamed Elhoseiny"], "keywords": ["zero-shot learning", "normalization", "continual learning", "initialization"], "abstract": "Normalization techniques have proved to be a crucial ingredient of successful training in a traditional supervised learning regime. However, in the zero-shot learning (ZSL) world, these ideas have received only marginal attention. This work studies normalization in ZSL scenario from both theoretical and practical perspectives. First, we give a theoretical explanation to two popular tricks used in zero-shot learning: normalize+scale and attributes normalization and show that they help training by preserving variance during a forward pass. Next, we demonstrate that they are insufficient to normalize a deep ZSL model and propose Class Normalization (CN): a normalization scheme, which alleviates this issue both provably and in practice. Third, we show that ZSL models typically have more irregular loss surface compared to traditional classifiers and that the proposed method partially remedies this problem. Then, we test our approach on 4 standard ZSL datasets and outperform sophisticated modern SotA with a simple MLP optimized without any bells and whistles and having ~50 times faster training speed. Finally, we generalize ZSL to a broader problem \u2014 continual ZSL, and introduce some principled metrics and rigorous baselines for this new setup. The source code is available at https://github.com/universome/class-norm.", "one-sentence_summary": "We develop theoretical understanding of signal normalization inside zero-shot learning models, propose a novel normalization scheme and use it to achieve SotA ZSL performance with a simple MLP", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "skorokhodov|class_normalization_for_continual_generalized_zeroshot_learning", "pdf": "/pdf/08846e7d24687d94ea8f98091ccf05e3f001d22a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nskorokhodov2021class,\ntitle={Class Normalization for (Continual)? Generalized Zero-Shot Learning},\nauthor={Ivan Skorokhodov and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7pgFL2Dkyyy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "7pgFL2Dkyyy", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2020/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2020/Authors|ICLR.cc/2021/Conference/Paper2020/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853146, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2020/-/Official_Comment"}}}, {"id": "rJAddEroK-W", "original": null, "number": 2, "cdate": 1603846592702, "ddate": null, "tcdate": 1603846592702, "tmdate": 1605024306185, "tddate": null, "forum": "7pgFL2Dkyyy", "replyto": "7pgFL2Dkyyy", "invitation": "ICLR.cc/2021/Conference/Paper2020/-/Official_Review", "content": {"title": "An excellent paper investigating the normalization effect in the zero-shot learning context", "review": "================\nSummary: \n\nThis paper provides a thorough analysis in the perspective of data variances on the widely used normalization tricks in the zero-shot learning research: normalize+scale and attribute normalization. It also demonstrates these tricks are not enough w.r.t. normalizing the variance in a non-linear model and propose a normalization trick to alleviate the issue. Both theoretical and empirical analysis are provided and results look convincing. Finally the authors propose a continual zero-shot learning problem scheme and illustrate some pioneering experimental results.\n\n================\nReason for my score:\n\nThere is rare work on the normalization trick in the context of zero-shot learning, although techniques like attribute normalization are widely used in practice. This paper investigates the normalization effect extensively for zero-shot learning, and provides many insightful thoughts for utilizing these tricks. The authors also evaluate the proposed class normalization with a simple implementation on benchmark datasets and show convincing results. Such work makes good contributions to the related community and hence I give my score.\n\n================\nPros:\n\n1. The paper provides both theoretical and empirical analysis on the effect of commonly used normalization tricks for zero-shot learning, in the perspective of data variances.\n2. The paper proposes a class normalization trick to alleviate the variance inflation/diminish in the non-linear model, and demonstrates its effectiveness on benchmark datasets.\n3. The empirical analysis in the paper are extensive and convincing.\n4. The paper also proposes a new framework of continual zero-shot learning.\n\n================\nCons:\n\n1. The paper didn't evaluate on another widely used benchmark dataset aPY, can author explain the reason?\n2. On CUB dataset, the proposed method has a considerably large margin to the state-of-the-art methods, in contrast to other datasets. Is there any explanation on why it is the case? Have you tried to explain this failure especially from the perspective of the proposed normalization trick?", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2020/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Class Normalization for (Continual)? Generalized Zero-Shot Learning", "authorids": ["~Ivan_Skorokhodov1", "~Mohamed_Elhoseiny1"], "authors": ["Ivan Skorokhodov", "Mohamed Elhoseiny"], "keywords": ["zero-shot learning", "normalization", "continual learning", "initialization"], "abstract": "Normalization techniques have proved to be a crucial ingredient of successful training in a traditional supervised learning regime. However, in the zero-shot learning (ZSL) world, these ideas have received only marginal attention. This work studies normalization in ZSL scenario from both theoretical and practical perspectives. First, we give a theoretical explanation to two popular tricks used in zero-shot learning: normalize+scale and attributes normalization and show that they help training by preserving variance during a forward pass. Next, we demonstrate that they are insufficient to normalize a deep ZSL model and propose Class Normalization (CN): a normalization scheme, which alleviates this issue both provably and in practice. Third, we show that ZSL models typically have more irregular loss surface compared to traditional classifiers and that the proposed method partially remedies this problem. Then, we test our approach on 4 standard ZSL datasets and outperform sophisticated modern SotA with a simple MLP optimized without any bells and whistles and having ~50 times faster training speed. Finally, we generalize ZSL to a broader problem \u2014 continual ZSL, and introduce some principled metrics and rigorous baselines for this new setup. The source code is available at https://github.com/universome/class-norm.", "one-sentence_summary": "We develop theoretical understanding of signal normalization inside zero-shot learning models, propose a novel normalization scheme and use it to achieve SotA ZSL performance with a simple MLP", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "skorokhodov|class_normalization_for_continual_generalized_zeroshot_learning", "pdf": "/pdf/08846e7d24687d94ea8f98091ccf05e3f001d22a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nskorokhodov2021class,\ntitle={Class Normalization for (Continual)? Generalized Zero-Shot Learning},\nauthor={Ivan Skorokhodov and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7pgFL2Dkyyy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "7pgFL2Dkyyy", "replyto": "7pgFL2Dkyyy", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2020/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538105739, "tmdate": 1606915765215, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2020/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2020/-/Official_Review"}}}, {"id": "XsYE7DtikRt", "original": null, "number": 4, "cdate": 1603962629304, "ddate": null, "tcdate": 1603962629304, "tmdate": 1605024306042, "tddate": null, "forum": "7pgFL2Dkyyy", "replyto": "7pgFL2Dkyyy", "invitation": "ICLR.cc/2021/Conference/Paper2020/-/Official_Review", "content": {"title": "Several major weaknesses", "review": "# Summary\n\nThis paper claims to have 3 main contributions. \n\nC1: Understanding/Theory. It explains why the two tricks work in zero-shot learning (ZSL): (i) normalization + scaling in the compatibility function of the class features and the attributes, and (ii) attribute unit normalization.\n\nC2: Method. It proposes a \u201cclass normalization\u201d scheme (Eq. 9 and 10) and Fig. 3. \nC2.1 From a \u201ctheoretical explanation\u201d of C1 (ii), this fixes (ii) in a \u201cdeep\u201d ZSL model.\nC2.2 It improves \u201csmoothness\u201d of a \u201cirregular\u201d loss landscape in ZSL.\n\nC3: Experiments. It demonstrates strong accuracy and training speed of the proposed approach in standard generalized ZSL. It also considers continual ZSL (Sect. 4), in which the proposed method is evaluated via mean accuracy (over timesteps) accuracy metrics and a forgetting metric.\n\n###\n\n# Strengths\n\nS1. Simple method. This is a simple feature-attribute scoring function via scaled cosine similarity (with normalization).\n\nS2. Strong empirical results (on both accuracy and training speed). See Table 2.\n\n# Weaknesses\n\nW1. Clarity\n\nThe organization of the paper is such that the reader has to refer to the appendix a lot. My biggest concern on clarity is on the \u201ctheoretical\u201d results which are not rigorous and at times unsupported. Further, some statements/claims are not precise or clear enough for me to be convinced that the method is well-motivated and is doing what it is claimed to be doing. \n\nW2. Soundness\n\nI have a lot of concerns and questions here as I read through Sect. 3. At a high-level, I don\u2019t see a clear connection between \u201cimproved variance control of prediction y^ or the smoothness of loss landscape\u201d and \u201czero-shot learning effectiveness.\u201d Details below. This is in part due to poor clarity.\n\nW3. Experiments\n\nIMO, if the main claim is really about the effectiveness of the two tricks and the proposed class normalization, then the experiments should go beyond one zero-shot learning starting point --- 3-layer MLP (Table 2). \n\n- If baseline methods already adopt some of these tricks, it should be made clear and see if removing these tricks lead to inferior performance.\n- If baseline methods do not adopt some of these tricks, these tricks, especially class normalization, could be applied to show improved performance. If it is difficult to apply these tricks, further explanation should be given (generally, also mention applicability of these tricks.) \n\nThis is done to some degree in the continual setting.\n\nW4. Related work\n\nAs I mentioned in W3, it is unclear which methods are linear/deep, and which methods have already benefited from existing/proposed tricks.\n\n###\n\n# Detailed comments (mainly to clarify my points about weaknesses)\n\n\n## Statement 1\n\nThe main claim for this part is that this statement provides \u201ca theoretical understanding of the trick\u201d and \u201callows to speed up the search [of the optimal value fo \\gamma].\u201d\n\nHowever, I feel that we need further justifications on the correlation between Statement 1 (variance of y^_c, \u201cbetter stability\u201d and \u201cthe training would not stale\u201d) and the zero-shot learning accuracy for this to be the \u201cwhy normalization + scaling works.\u201d My understanding is that the Appendix simply validates that Eq. (4) seems to hold in practice.\n\nMoreover, is the usual search region [5,10] actually effective? Do we have stronger supporting empirical evidence than the three groups of practitioners (Li et al 2019, Zhang et al. 2019, Guo et al. 2020), who may have influenced each other, used it? \n\nFinally, can the authors comment on the validity of multiple assumptions in Appendix A? To which degrees does each of them hold in practice?\n\n\n## Statement 2 and 3\n\nWhy wouldn\u2019t the following statement in Sect. 3.3 invalidate Statement 1? \n\u201cThis may create an impression that it does not matter how we initialize the weights \u2014 normalization would undo any fluctuations. However it is not true, because it is still important how the signal flows, i.e. for an unnormalized and unscaled logit value\u201d\n\nIt is unclear (at least not from the beginning) why understanding attribute normalization has to do with initialization of the weights.\n\nSimilar to my comments to Statement 1, why should we believe that the explanation in Sect. 3.3 and Sect. 3.4 is the reason for zero-shot learning effectiveness? In particular, the authors again claim that the main bottleneck in improving zero-shot learning is \u201cvariance control\u201d (the end of Sect. 3.3).\n\nI also have a hard time understanding some statements in Appendix H, which is needed to motivate the following statement in Sect. 3.3: \u201cAnd these assumptions are safe to assume only for z but not for a_c, because they do not hold for the standard datasets (see Appendix H).\u201d\nH1: Would this statement still be true after we transform a_c with an MLP?\nH2: Why is it not \u201ca sensible thing to do\u201d if we just want zero mean and unit variance? \nH3: Why is \u201csuch an approach far from being scalable\u201d? \nH4: What if these are things like word embeddings?\nH5: Fig. 12 and Fig. 13 are not explained.\nH6: Histograms in Fig. 13 look quite normal.\n\nHow useful is Statement 2? Why is the connection with Xavier initialization important?\n\nWhy is \u201cpreserving the variance between z and y~\u201d in Statement 3  important for zero-shot learning?\n\n\n## Improved smoothness\n\nThe claim \u201cimproved smoothness\u201d at the end of Sect. 3 and Appendix F is really hard to understand.\nF1: How do the authors define \u201cirregular loss surface\u201d?\nF2: \u201cSanturkar et al. (2018) showed that batch-wise standardization procedure decreases the Lipschitz constant of a model, which suggests that our class-wise standardization will provide the same impact.\u201d This is not very precise and seems unsupported. Please make it clear how. If this is a hypothesis, please make it clear.\n\nSimilarly to my comments to Statement 1-3, how is improved smoothness related to zero-shot learning effectiveness?  \n\n\n## Other more minor comments\n1. Abstract: Are the authors the one to \u201cgeneralize ZSL to a broader problem\u201d? Please tone down the claim if not.\n2. After Eq. (2): Why does attribute normalization look \u201cinconsiderable\u201d (possibly this is not the right word?) or why is it \u201csurprising\u201d that this is preferred in practice? Don\u2019t most zero-shot learning methods use this (see for example Table 4 in [A])?\n3. Suggestions for references for attribute normalization. This can be improved; I can trace this back to much earlier work such as [A] and [B] (though I think this fact is stated more explicitly in [A]).\n4. Under Table 1 \u201cThese two tricks work well and normalize the variance to a unit value when the underlying ZSL model is linear (see Figure 1), but they fail when we use a multi-layer architecture.\u201d: Could the authors provide a reference to evidence to support this? I think it is also important to provide a clear statement of what separates a \u201clinear\u201d or \u201cmulti-layer\u201d model.\n5. The first paragraph of Sect. 3: Could you provide references for motivations for different activation functions? Further, It is unclear that all of them perform normalization.\n6. The second paragraph of Sect. 3: What exactly limits \u201cthe tools\u201d for zero-shot learning vs. supervised learning? Further, it would also be nice to separate traditional supervised learning where classes are balanced and imbalanced; see, e.g., [C].\n7. What is the closest existing zero-shot model to the one the authors describe in Sect. 3.1? Why is the described model considered/selected? \n\n[A] Synthesized Classifiers for Zero-Shot Learning\n\n[B] Zero-Shot Learning by Convex Combination of Semantic Embeddings\n\n[C] Class-Balanced Loss Based on Effective Number of Samples\n\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2020/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2020/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Class Normalization for (Continual)? Generalized Zero-Shot Learning", "authorids": ["~Ivan_Skorokhodov1", "~Mohamed_Elhoseiny1"], "authors": ["Ivan Skorokhodov", "Mohamed Elhoseiny"], "keywords": ["zero-shot learning", "normalization", "continual learning", "initialization"], "abstract": "Normalization techniques have proved to be a crucial ingredient of successful training in a traditional supervised learning regime. However, in the zero-shot learning (ZSL) world, these ideas have received only marginal attention. This work studies normalization in ZSL scenario from both theoretical and practical perspectives. First, we give a theoretical explanation to two popular tricks used in zero-shot learning: normalize+scale and attributes normalization and show that they help training by preserving variance during a forward pass. Next, we demonstrate that they are insufficient to normalize a deep ZSL model and propose Class Normalization (CN): a normalization scheme, which alleviates this issue both provably and in practice. Third, we show that ZSL models typically have more irregular loss surface compared to traditional classifiers and that the proposed method partially remedies this problem. Then, we test our approach on 4 standard ZSL datasets and outperform sophisticated modern SotA with a simple MLP optimized without any bells and whistles and having ~50 times faster training speed. Finally, we generalize ZSL to a broader problem \u2014 continual ZSL, and introduce some principled metrics and rigorous baselines for this new setup. The source code is available at https://github.com/universome/class-norm.", "one-sentence_summary": "We develop theoretical understanding of signal normalization inside zero-shot learning models, propose a novel normalization scheme and use it to achieve SotA ZSL performance with a simple MLP", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "skorokhodov|class_normalization_for_continual_generalized_zeroshot_learning", "pdf": "/pdf/08846e7d24687d94ea8f98091ccf05e3f001d22a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nskorokhodov2021class,\ntitle={Class Normalization for (Continual)? Generalized Zero-Shot Learning},\nauthor={Ivan Skorokhodov and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7pgFL2Dkyyy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "7pgFL2Dkyyy", "replyto": "7pgFL2Dkyyy", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2020/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538105739, "tmdate": 1606915765215, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2020/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2020/-/Official_Review"}}}], "count": 27}