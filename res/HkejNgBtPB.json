{"notes": [{"id": "HkejNgBtPB", "original": "BylMPwxFPr", "number": 2261, "cdate": 1569439795206, "ddate": null, "tcdate": 1569439795206, "tmdate": 1583912050242, "tddate": null, "forum": "HkejNgBtPB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["rye18@fudan.edu.cn", "shiwenxian@bytedance.com", "zhouhao.nlp@bytedance.com", "zywei@fudan.edu.cn", "lileilab@bytedance.com"], "title": "Variational Template Machine for Data-to-Text Generation", "authors": ["Rong Ye", "Wenxian Shi", "Hao Zhou", "Zhongyu Wei", "Lei Li"], "pdf": "/pdf/99875ef53a914c603c8893720071afa15d3d96bb.pdf", "abstract": "How to generate descriptions from structured data organized in tables? Existing approaches using neural encoder-decoder models often suffer from lacking diversity. We claim that an open set of templates is crucial for enriching the phrase constructions and realizing varied generations.Learning such templates is prohibitive since it often requires a large paired <table,description>, which is seldom available. This paper explores the problem of automatically learning reusable \"templates\" from paired and non-paired data. We propose the variational template machine (VTM), a novel method to generate text descriptions from data tables. Our contributions include:  a) we carefully devise a specific model architecture and losses to explicitly disentangle text template and semantic content information, in the latent spaces, and b) we utilize both small parallel data and large raw text without aligned tables to enrich the template learning. Experiments on datasets from a variety of different domains show that VTM is able to generate more diversely while keeping a good fluency and quality. ", "keywords": [], "paperhash": "ye|variational_template_machine_for_datatotext_generation", "_bibtex": "@inproceedings{\nYe2020Variational,\ntitle={Variational Template Machine for Data-to-Text Generation},\nauthor={Rong Ye and Wenxian Shi and Hao Zhou and Zhongyu Wei and Lei Li},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkejNgBtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f0191c4a9580dd585bc08944b225990f3db21886.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "4RIFJ4R3C7", "original": null, "number": 1, "cdate": 1576798744641, "ddate": null, "tcdate": 1576798744641, "tmdate": 1576800891525, "tddate": null, "forum": "HkejNgBtPB", "replyto": "HkejNgBtPB", "invitation": "ICLR.cc/2020/Conference/Paper2261/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "The paper addresses the problem of generating descriptions from structured data. In particular a Variational Template Machine  which explicitly disentangles templates from semantic content. They empirically demonstrate that their model performs better than existing methods on different methods. \n\nThis paper has received a strong acceptance from two reviewers. In particular, the reviewers have appreciated the novelty and empirical evaluation of the proposed approach. R3 has raised quite a few concerns but I feel they were adequately addressed by the reviewers. Hence, I recommend that the paper be accepted. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rye18@fudan.edu.cn", "shiwenxian@bytedance.com", "zhouhao.nlp@bytedance.com", "zywei@fudan.edu.cn", "lileilab@bytedance.com"], "title": "Variational Template Machine for Data-to-Text Generation", "authors": ["Rong Ye", "Wenxian Shi", "Hao Zhou", "Zhongyu Wei", "Lei Li"], "pdf": "/pdf/99875ef53a914c603c8893720071afa15d3d96bb.pdf", "abstract": "How to generate descriptions from structured data organized in tables? Existing approaches using neural encoder-decoder models often suffer from lacking diversity. We claim that an open set of templates is crucial for enriching the phrase constructions and realizing varied generations.Learning such templates is prohibitive since it often requires a large paired <table,description>, which is seldom available. This paper explores the problem of automatically learning reusable \"templates\" from paired and non-paired data. We propose the variational template machine (VTM), a novel method to generate text descriptions from data tables. Our contributions include:  a) we carefully devise a specific model architecture and losses to explicitly disentangle text template and semantic content information, in the latent spaces, and b) we utilize both small parallel data and large raw text without aligned tables to enrich the template learning. Experiments on datasets from a variety of different domains show that VTM is able to generate more diversely while keeping a good fluency and quality. ", "keywords": [], "paperhash": "ye|variational_template_machine_for_datatotext_generation", "_bibtex": "@inproceedings{\nYe2020Variational,\ntitle={Variational Template Machine for Data-to-Text Generation},\nauthor={Rong Ye and Wenxian Shi and Hao Zhou and Zhongyu Wei and Lei Li},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkejNgBtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f0191c4a9580dd585bc08944b225990f3db21886.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HkejNgBtPB", "replyto": "HkejNgBtPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795708043, "tmdate": 1576800256364, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2261/-/Decision"}}}, {"id": "rkxYRkH3sH", "original": null, "number": 4, "cdate": 1573830609289, "ddate": null, "tcdate": 1573830609289, "tmdate": 1573830785968, "tddate": null, "forum": "HkejNgBtPB", "replyto": "B1x-h8r9dH", "invitation": "ICLR.cc/2020/Conference/Paper2261/-/Official_Comment", "content": {"title": "Response to Review #1", "comment": "Thanks very much for your valuable comments.\n\nQ: It would be good if the authors could provide an analysis of the computational costs of their methods, as well as of the considered competitors. \n\nA: We compare the training and testing time cost on the WIKI dataset, and with raw data added, VTM spends more time on training but the same time on generation as Table2seq. Here is the detailed time spent ( train and test on single Tesla V100 GPU), for test computational cost, we record how much time to generate 72k sentences.\n\n                |          Table2seq              |       VTM-noraw               |              VTM\n--------------------------------------------------------------------------------------------------------------------\nTrain        |  \uff5e30 mins (6 epochs)  | \uff5e30 mins (6 epochs)  |  \uff5e160 mins (15 epochs)\n--------------------------------------------------------------------------------------------------------------------\nTest         |          ~80min                  |         ~80min                     |           ~80min \n\nVTM gives the same speed for generating sentences, but it takes more time for training, which are cost to learn the large-scaled unlabeled data, and is acceptable.\n\nAdditionally, we've added some new experiments with more sophisticated setups. In the experiments (result see Section 4.3, Figure 3), we control the same decoding strategy under the same temperature, and plot their BLEU and Self-BLEU scores in Figure 3 to analyze the quality-diversity trade-off. Experimental results show that compared to Table2seq, VTM always gives better self-BLEU when they have the same BLEU, and gives better BLEU under the same Self-BLEU. This shows that VTM outperforms Table2text consistently. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2261/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2261/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rye18@fudan.edu.cn", "shiwenxian@bytedance.com", "zhouhao.nlp@bytedance.com", "zywei@fudan.edu.cn", "lileilab@bytedance.com"], "title": "Variational Template Machine for Data-to-Text Generation", "authors": ["Rong Ye", "Wenxian Shi", "Hao Zhou", "Zhongyu Wei", "Lei Li"], "pdf": "/pdf/99875ef53a914c603c8893720071afa15d3d96bb.pdf", "abstract": "How to generate descriptions from structured data organized in tables? Existing approaches using neural encoder-decoder models often suffer from lacking diversity. We claim that an open set of templates is crucial for enriching the phrase constructions and realizing varied generations.Learning such templates is prohibitive since it often requires a large paired <table,description>, which is seldom available. This paper explores the problem of automatically learning reusable \"templates\" from paired and non-paired data. We propose the variational template machine (VTM), a novel method to generate text descriptions from data tables. Our contributions include:  a) we carefully devise a specific model architecture and losses to explicitly disentangle text template and semantic content information, in the latent spaces, and b) we utilize both small parallel data and large raw text without aligned tables to enrich the template learning. Experiments on datasets from a variety of different domains show that VTM is able to generate more diversely while keeping a good fluency and quality. ", "keywords": [], "paperhash": "ye|variational_template_machine_for_datatotext_generation", "_bibtex": "@inproceedings{\nYe2020Variational,\ntitle={Variational Template Machine for Data-to-Text Generation},\nauthor={Rong Ye and Wenxian Shi and Hao Zhou and Zhongyu Wei and Lei Li},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkejNgBtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f0191c4a9580dd585bc08944b225990f3db21886.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkejNgBtPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2261/Authors", "ICLR.cc/2020/Conference/Paper2261/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2261/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2261/Reviewers", "ICLR.cc/2020/Conference/Paper2261/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2261/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2261/Authors|ICLR.cc/2020/Conference/Paper2261/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143979, "tmdate": 1576860528881, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2261/Authors", "ICLR.cc/2020/Conference/Paper2261/Reviewers", "ICLR.cc/2020/Conference/Paper2261/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2261/-/Official_Comment"}}}, {"id": "HJepPJHniB", "original": null, "number": 3, "cdate": 1573830500859, "ddate": null, "tcdate": 1573830500859, "tmdate": 1573830752088, "tddate": null, "forum": "HkejNgBtPB", "replyto": "HJlIc6pdYH", "invitation": "ICLR.cc/2020/Conference/Paper2261/-/Official_Comment", "content": {"title": "Response to Review #3", "comment": "Thanks a lot for your insightful comments. In the following parts, we will response to your questions one by one.\n\nQ: Quality-diversity trade-off.\nA: \nQuality and diversity is a trade-off in the text generation. As we can find in Table 3 and 6, beam search (or greedy) always receives a higher quality and low diversity whereas forward sampling method can diversify the generation but with a relatively lower quality. Decoding strategy largely interferes our judgment. A fairer comparison is to keep the same decoding strategy then comparing the quality and diversity.  \n\nTherefore, we add extensive experiments in Section 4.3 (results refer Figure 4), by applying same decoding method -- sampling under the same softmax temperature, we plot their BLEU scores and Self-BLEU scores in Figure 3, which shows that compared to Table2seq, VTM always gives better self-BLEU when they have the same BLEU, and gives better BLEU under the same Self-BLEU. This shows that VTM outperforms Table2text consistently.\n\n========\n\nQ: Did the authors try other sampling methods for Table2Seq? (e.g., top-K or nucleus sampling)\nA: \nThanks very much for your kind notes, We have updated the paper, adding extensive experiments on other sampling methods and human evaluation. Results show that our method consistently outperforms the baseline model (Table2Seq).\n\nFirstly, we sample from the softmax with different temperatures (from 0.1 to 1.0) and plot the BLEU and self-BLEU trade-off curve in Figure 3. The trade-off curves show that compared to Table2seq, VTM always give better self-BLEU when they have the same BLEU, and gives better BLEU under the same Self-BLEU. This shows that VTM outperforms Table2text consistently.\n\nSecondly, human evaluation also shows that VTM can generate sentences with better accuracy and coherence. Besides, comparing to the model without raw data (VTM-no raw), there is a significantly large improvement in diversity. \nAlthough Table2seq with forward sampling has the highest diversity rating, its quality much worse than VTM. High quality is meaningful when the output quality is good enough.\n\n========\nQ: A way to incorporate this unlabeled data to Table2Seq is by first pretraining the LSTM generator on it before training it on pairwise data (or in parallel). How would this baseline model perform in comparison to VTM? \nA: \nYes, pretraining the decoder with large-scaled unlabeled data can be another alternative for including the effectiveness of the large scaled unlabeled data. However, we did a quick run and experimental results in Table 3 and Table 6 show that directly applying decoder pretraining does not get performance gain (even worse than the baseline), which might be caused by the gap between pretraining the generator as a language model and the data-to-text task.\nHere we list the BLEU and self-BLEU scores: \n\nDataset |           Model             |      BLEU    |   Self-BLEU\n-----------------------------------------------------------------------------\nWIKI      |  Table2seq-beam    |     26.74     |     92.00\n               | Table2seq-pretrain|     25.43     |     99.88   \n               |            VTM               |     25.22     |     74.86\n-----------------------------------------------------------------------------\nSPNLG   | Table2seq-beam      |     40.61    |     97.14   \n               | Table2seq-pretrain  |     40.56    |    100.00  \n               |            VTM                 |      40.04   |     88.77\n\n========\n\nQ: In the conclusion section, what does this VAE model refer to?\nA: Sorry for misleading, VAE model refers to the VTM without using raw data (i.e. VTM-noraw). We've fixed it in the updated version.\n\n=======\n\nQ: I am not convinced that the proposed method is a significant development based on the results presented in the paper. \nA: \nAs introduced in the quality-diversity trade-off, VTM tends to generate more diverse outputs with the same quality (diversity is important for text generation). \nOur proposed VTM can make full use of the raw data to learn an informative template space, and largely enrich the template of generated sentences, thus boost the diversity. VTM can be also a new approach to include unlabeled data for text generation in the VAE framework.\nTo our best knowledge, there is no related work using the similar idea in the data-to-text generation.\n\n=======\n\nQ: There are also many grammatical errors in the paper (e.g., ... only enable to sample in the latent space ..., and many others), so I think the writing of the paper can be improved.\nA: Thanks, we will proof-read carefully and fix typos in the next version.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2261/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2261/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rye18@fudan.edu.cn", "shiwenxian@bytedance.com", "zhouhao.nlp@bytedance.com", "zywei@fudan.edu.cn", "lileilab@bytedance.com"], "title": "Variational Template Machine for Data-to-Text Generation", "authors": ["Rong Ye", "Wenxian Shi", "Hao Zhou", "Zhongyu Wei", "Lei Li"], "pdf": "/pdf/99875ef53a914c603c8893720071afa15d3d96bb.pdf", "abstract": "How to generate descriptions from structured data organized in tables? Existing approaches using neural encoder-decoder models often suffer from lacking diversity. We claim that an open set of templates is crucial for enriching the phrase constructions and realizing varied generations.Learning such templates is prohibitive since it often requires a large paired <table,description>, which is seldom available. This paper explores the problem of automatically learning reusable \"templates\" from paired and non-paired data. We propose the variational template machine (VTM), a novel method to generate text descriptions from data tables. Our contributions include:  a) we carefully devise a specific model architecture and losses to explicitly disentangle text template and semantic content information, in the latent spaces, and b) we utilize both small parallel data and large raw text without aligned tables to enrich the template learning. Experiments on datasets from a variety of different domains show that VTM is able to generate more diversely while keeping a good fluency and quality. ", "keywords": [], "paperhash": "ye|variational_template_machine_for_datatotext_generation", "_bibtex": "@inproceedings{\nYe2020Variational,\ntitle={Variational Template Machine for Data-to-Text Generation},\nauthor={Rong Ye and Wenxian Shi and Hao Zhou and Zhongyu Wei and Lei Li},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkejNgBtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f0191c4a9580dd585bc08944b225990f3db21886.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkejNgBtPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2261/Authors", "ICLR.cc/2020/Conference/Paper2261/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2261/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2261/Reviewers", "ICLR.cc/2020/Conference/Paper2261/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2261/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2261/Authors|ICLR.cc/2020/Conference/Paper2261/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143979, "tmdate": 1576860528881, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2261/Authors", "ICLR.cc/2020/Conference/Paper2261/Reviewers", "ICLR.cc/2020/Conference/Paper2261/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2261/-/Official_Comment"}}}, {"id": "HJg5xRE3jS", "original": null, "number": 2, "cdate": 1573830130252, "ddate": null, "tcdate": 1573830130252, "tmdate": 1573830659079, "tddate": null, "forum": "HkejNgBtPB", "replyto": "SkeetYT-5H", "invitation": "ICLR.cc/2020/Conference/Paper2261/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "Thanks very much for your valuable comments. \n\nQ: How does the method generalize to other languages? How does it scale with (the lack of) resources?\nA: Our method could be easily generalized to other languages because no language-specific processing or resources are used. Additionally, our proposed VTM may well fit languages with fewer resources, in which case the VTM model with massive raw data (usually cheap to obtain) may significantly boost the finally performances when labeled data are hard to get.\n\nAdditionally, we've added some new experiments with more sophisticated setups. In these experiments (result see Section 4.3, Figure 3), we control the decoding strategy with the same temperature, and plot their BLEU scores and Self-BLEU scores in Figure 3 to analyze the quality-diversity trade-off. Experimental results show that compared to Table2seq, VTM always gives better self-BLEU when they have the same BLEU, and gives better BLEU under the same Self-BLEU. This shows that VTM outperforms Table2text consistently. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2261/Authors"], "readers": ["ICLR.cc/2020/Conference/Paper2261/Authors", "ICLR.cc/2020/Conference/Paper2261/Reviewers", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2261/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rye18@fudan.edu.cn", "shiwenxian@bytedance.com", "zhouhao.nlp@bytedance.com", "zywei@fudan.edu.cn", "lileilab@bytedance.com"], "title": "Variational Template Machine for Data-to-Text Generation", "authors": ["Rong Ye", "Wenxian Shi", "Hao Zhou", "Zhongyu Wei", "Lei Li"], "pdf": "/pdf/99875ef53a914c603c8893720071afa15d3d96bb.pdf", "abstract": "How to generate descriptions from structured data organized in tables? Existing approaches using neural encoder-decoder models often suffer from lacking diversity. We claim that an open set of templates is crucial for enriching the phrase constructions and realizing varied generations.Learning such templates is prohibitive since it often requires a large paired <table,description>, which is seldom available. This paper explores the problem of automatically learning reusable \"templates\" from paired and non-paired data. We propose the variational template machine (VTM), a novel method to generate text descriptions from data tables. Our contributions include:  a) we carefully devise a specific model architecture and losses to explicitly disentangle text template and semantic content information, in the latent spaces, and b) we utilize both small parallel data and large raw text without aligned tables to enrich the template learning. Experiments on datasets from a variety of different domains show that VTM is able to generate more diversely while keeping a good fluency and quality. ", "keywords": [], "paperhash": "ye|variational_template_machine_for_datatotext_generation", "_bibtex": "@inproceedings{\nYe2020Variational,\ntitle={Variational Template Machine for Data-to-Text Generation},\nauthor={Rong Ye and Wenxian Shi and Hao Zhou and Zhongyu Wei and Lei Li},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkejNgBtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f0191c4a9580dd585bc08944b225990f3db21886.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkejNgBtPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2261/Authors", "ICLR.cc/2020/Conference/Paper2261/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2261/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2261/Reviewers", "ICLR.cc/2020/Conference/Paper2261/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2261/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2261/Authors|ICLR.cc/2020/Conference/Paper2261/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143979, "tmdate": 1576860528881, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2261/Authors", "ICLR.cc/2020/Conference/Paper2261/Reviewers", "ICLR.cc/2020/Conference/Paper2261/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2261/-/Official_Comment"}}}, {"id": "ryxcKa4hsr", "original": null, "number": 1, "cdate": 1573830018113, "ddate": null, "tcdate": 1573830018113, "tmdate": 1573830018113, "tddate": null, "forum": "HkejNgBtPB", "replyto": "HkejNgBtPB", "invitation": "ICLR.cc/2020/Conference/Paper2261/-/Official_Comment", "content": {"title": "We update a new version!", "comment": "Hi, all. Thanks for reviewing my paper. We've uploaded a new version of our draft, adding more experiments:\n- Experiments on the computational cost of the models. (Table 4, Page 7)\n- Experiments on quality-diversity trade-off. (Figure 3, Page 6)\n- Human evaluation on generation accuracy, coherence and diversity. (Table 7, Page 9)\n Please take a look\uff01"}, "signatures": ["ICLR.cc/2020/Conference/Paper2261/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2261/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rye18@fudan.edu.cn", "shiwenxian@bytedance.com", "zhouhao.nlp@bytedance.com", "zywei@fudan.edu.cn", "lileilab@bytedance.com"], "title": "Variational Template Machine for Data-to-Text Generation", "authors": ["Rong Ye", "Wenxian Shi", "Hao Zhou", "Zhongyu Wei", "Lei Li"], "pdf": "/pdf/99875ef53a914c603c8893720071afa15d3d96bb.pdf", "abstract": "How to generate descriptions from structured data organized in tables? Existing approaches using neural encoder-decoder models often suffer from lacking diversity. We claim that an open set of templates is crucial for enriching the phrase constructions and realizing varied generations.Learning such templates is prohibitive since it often requires a large paired <table,description>, which is seldom available. This paper explores the problem of automatically learning reusable \"templates\" from paired and non-paired data. We propose the variational template machine (VTM), a novel method to generate text descriptions from data tables. Our contributions include:  a) we carefully devise a specific model architecture and losses to explicitly disentangle text template and semantic content information, in the latent spaces, and b) we utilize both small parallel data and large raw text without aligned tables to enrich the template learning. Experiments on datasets from a variety of different domains show that VTM is able to generate more diversely while keeping a good fluency and quality. ", "keywords": [], "paperhash": "ye|variational_template_machine_for_datatotext_generation", "_bibtex": "@inproceedings{\nYe2020Variational,\ntitle={Variational Template Machine for Data-to-Text Generation},\nauthor={Rong Ye and Wenxian Shi and Hao Zhou and Zhongyu Wei and Lei Li},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkejNgBtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f0191c4a9580dd585bc08944b225990f3db21886.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkejNgBtPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2261/Authors", "ICLR.cc/2020/Conference/Paper2261/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2261/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2261/Reviewers", "ICLR.cc/2020/Conference/Paper2261/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2261/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2261/Authors|ICLR.cc/2020/Conference/Paper2261/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143979, "tmdate": 1576860528881, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2261/Authors", "ICLR.cc/2020/Conference/Paper2261/Reviewers", "ICLR.cc/2020/Conference/Paper2261/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2261/-/Official_Comment"}}}, {"id": "B1x-h8r9dH", "original": null, "number": 1, "cdate": 1570555560941, "ddate": null, "tcdate": 1570555560941, "tmdate": 1572972362071, "tddate": null, "forum": "HkejNgBtPB", "replyto": "HkejNgBtPB", "invitation": "ICLR.cc/2020/Conference/Paper2261/-/Official_Review", "content": {"rating": "8: Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper is interesting and proposes a novel approach for addressing a currently not largely considered problem.\nThe proposed model is sound and appropriate, as it relies on state-of-the-art methodological arguments. \nThe derivations are correct; this concerns both the model definition and the algorithmic derivations of model training and inference.\nThe experimental evaluation is adequate: it compares to many popular approaches and on several datasets; the outcomes are convincing.\nIt would be good if the authors could provide an analysis of the computational costs of their methods, as well as of the considered competitors. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2261/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2261/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rye18@fudan.edu.cn", "shiwenxian@bytedance.com", "zhouhao.nlp@bytedance.com", "zywei@fudan.edu.cn", "lileilab@bytedance.com"], "title": "Variational Template Machine for Data-to-Text Generation", "authors": ["Rong Ye", "Wenxian Shi", "Hao Zhou", "Zhongyu Wei", "Lei Li"], "pdf": "/pdf/99875ef53a914c603c8893720071afa15d3d96bb.pdf", "abstract": "How to generate descriptions from structured data organized in tables? Existing approaches using neural encoder-decoder models often suffer from lacking diversity. We claim that an open set of templates is crucial for enriching the phrase constructions and realizing varied generations.Learning such templates is prohibitive since it often requires a large paired <table,description>, which is seldom available. This paper explores the problem of automatically learning reusable \"templates\" from paired and non-paired data. We propose the variational template machine (VTM), a novel method to generate text descriptions from data tables. Our contributions include:  a) we carefully devise a specific model architecture and losses to explicitly disentangle text template and semantic content information, in the latent spaces, and b) we utilize both small parallel data and large raw text without aligned tables to enrich the template learning. Experiments on datasets from a variety of different domains show that VTM is able to generate more diversely while keeping a good fluency and quality. ", "keywords": [], "paperhash": "ye|variational_template_machine_for_datatotext_generation", "_bibtex": "@inproceedings{\nYe2020Variational,\ntitle={Variational Template Machine for Data-to-Text Generation},\nauthor={Rong Ye and Wenxian Shi and Hao Zhou and Zhongyu Wei and Lei Li},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkejNgBtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f0191c4a9580dd585bc08944b225990f3db21886.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkejNgBtPB", "replyto": "HkejNgBtPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2261/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2261/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576078760704, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2261/Reviewers"], "noninvitees": [], "tcdate": 1570237725373, "tmdate": 1576078760718, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2261/-/Official_Review"}}}, {"id": "HJlIc6pdYH", "original": null, "number": 2, "cdate": 1571507597812, "ddate": null, "tcdate": 1571507597812, "tmdate": 1572972362034, "tddate": null, "forum": "HkejNgBtPB", "replyto": "HkejNgBtPB", "invitation": "ICLR.cc/2020/Conference/Paper2261/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes Variational Template Machine (VTM), a generative model to generate textual descriptions from structured data (i.e., tables). VTM is derived from the variational autoencoder, where the input is a row entry from a table and the output is the text associated with this entry. The authors introduce two latent variables to model contents and templates. The content variable is conditioned on the table entry, and generates the textual output together with the template variable. The model is trained on both paired table-to-text examples as well as unpaired (text only) examples. Experiments on the Wiki and SpNLG datasets show that models generate diverse sentences, and the overall performance in terms of BLEU is only slightly below the best baseline Table2Seq model that does not generate diverse sentences. The results also show that additional losses for preserving contents and templates introduced by the authors play an important role in the overall model performance.\u00a0\n\nI have several questions regarding the experiments:\n- For the Table2Seq baseline, how was the beam size chosen? Did it have any effect on the performance of the baseline model?\n- Did the authors try other sampling methods for Table2Seq? (e.g., top-K or nucleus sampling)\n- VTM is only able to achieve comparable performance to Table2Seq in terms of BLEU after including the unlabeled corpus, especially on the Wiki dataset. A way to incorporate this unlabeled data to Table2Seq is by first pretraining\u00a0the LSTM generator on it before training it on pairwise data (or in parallel). How would this baseline model perform in comparison to VTM?\n- In the conclusion section, the authors mentioned that VTM outperforms VAE both in terms of diversity and generation quality. What does this VAE model refer to? The experiments show that VTM is comparable to Table2Seq in terms of quality and is better in terms of diversity.\u00a0\n\nGenerating text from structured data is an interesting research area. However, I am not convinced that the proposed method is a significant development based on the results presented in the paper. There are also many grammatical errors in the paper (e.g., ... only enable to sample in the latent space ..., and many others), so I think the writing of the paper can be improved."}, "signatures": ["ICLR.cc/2020/Conference/Paper2261/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2261/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rye18@fudan.edu.cn", "shiwenxian@bytedance.com", "zhouhao.nlp@bytedance.com", "zywei@fudan.edu.cn", "lileilab@bytedance.com"], "title": "Variational Template Machine for Data-to-Text Generation", "authors": ["Rong Ye", "Wenxian Shi", "Hao Zhou", "Zhongyu Wei", "Lei Li"], "pdf": "/pdf/99875ef53a914c603c8893720071afa15d3d96bb.pdf", "abstract": "How to generate descriptions from structured data organized in tables? Existing approaches using neural encoder-decoder models often suffer from lacking diversity. We claim that an open set of templates is crucial for enriching the phrase constructions and realizing varied generations.Learning such templates is prohibitive since it often requires a large paired <table,description>, which is seldom available. This paper explores the problem of automatically learning reusable \"templates\" from paired and non-paired data. We propose the variational template machine (VTM), a novel method to generate text descriptions from data tables. Our contributions include:  a) we carefully devise a specific model architecture and losses to explicitly disentangle text template and semantic content information, in the latent spaces, and b) we utilize both small parallel data and large raw text without aligned tables to enrich the template learning. Experiments on datasets from a variety of different domains show that VTM is able to generate more diversely while keeping a good fluency and quality. ", "keywords": [], "paperhash": "ye|variational_template_machine_for_datatotext_generation", "_bibtex": "@inproceedings{\nYe2020Variational,\ntitle={Variational Template Machine for Data-to-Text Generation},\nauthor={Rong Ye and Wenxian Shi and Hao Zhou and Zhongyu Wei and Lei Li},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkejNgBtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f0191c4a9580dd585bc08944b225990f3db21886.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkejNgBtPB", "replyto": "HkejNgBtPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2261/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2261/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576078760704, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2261/Reviewers"], "noninvitees": [], "tcdate": 1570237725373, "tmdate": 1576078760718, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2261/-/Official_Review"}}}, {"id": "SkeetYT-5H", "original": null, "number": 3, "cdate": 1572096376113, "ddate": null, "tcdate": 1572096376113, "tmdate": 1572972361981, "tddate": null, "forum": "HkejNgBtPB", "replyto": "HkejNgBtPB", "invitation": "ICLR.cc/2020/Conference/Paper2261/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper proposes an approach to generate textual descriptions from structured data organized in tables, by using a \"variational template machine\" (VTM), which is essentially a generative model to separately represent template and content as disentangled latent variables to control the generation.\n\nThe contribution is well-written and well-motivated, the model exposition is clear, and the results are convincing. The experiment setup, depth, and breadth are particularly convincing. I see no reason to not accept this paper.\n\nRemarks:\n- It should be clearly stated which languages feature in the paper. From what I gather, it's only English. How does the method generalize to other languages? How does it scale with (the lack of) resources?"}, "signatures": ["ICLR.cc/2020/Conference/Paper2261/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2261/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rye18@fudan.edu.cn", "shiwenxian@bytedance.com", "zhouhao.nlp@bytedance.com", "zywei@fudan.edu.cn", "lileilab@bytedance.com"], "title": "Variational Template Machine for Data-to-Text Generation", "authors": ["Rong Ye", "Wenxian Shi", "Hao Zhou", "Zhongyu Wei", "Lei Li"], "pdf": "/pdf/99875ef53a914c603c8893720071afa15d3d96bb.pdf", "abstract": "How to generate descriptions from structured data organized in tables? Existing approaches using neural encoder-decoder models often suffer from lacking diversity. We claim that an open set of templates is crucial for enriching the phrase constructions and realizing varied generations.Learning such templates is prohibitive since it often requires a large paired <table,description>, which is seldom available. This paper explores the problem of automatically learning reusable \"templates\" from paired and non-paired data. We propose the variational template machine (VTM), a novel method to generate text descriptions from data tables. Our contributions include:  a) we carefully devise a specific model architecture and losses to explicitly disentangle text template and semantic content information, in the latent spaces, and b) we utilize both small parallel data and large raw text without aligned tables to enrich the template learning. Experiments on datasets from a variety of different domains show that VTM is able to generate more diversely while keeping a good fluency and quality. ", "keywords": [], "paperhash": "ye|variational_template_machine_for_datatotext_generation", "_bibtex": "@inproceedings{\nYe2020Variational,\ntitle={Variational Template Machine for Data-to-Text Generation},\nauthor={Rong Ye and Wenxian Shi and Hao Zhou and Zhongyu Wei and Lei Li},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkejNgBtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f0191c4a9580dd585bc08944b225990f3db21886.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkejNgBtPB", "replyto": "HkejNgBtPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2261/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2261/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576078760704, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2261/Reviewers"], "noninvitees": [], "tcdate": 1570237725373, "tmdate": 1576078760718, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2261/-/Official_Review"}}}], "count": 9}