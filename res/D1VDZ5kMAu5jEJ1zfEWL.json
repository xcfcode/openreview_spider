{"notes": [{"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1459314878261, "tcdate": 1459314878261, "id": "p8W8xLgRzFnQVOGWfpxP", "invitation": "ICLR.cc/2016/workshop/-/paper/80/comment", "forum": "D1VDZ5kMAu5jEJ1zfEWL", "replyto": "L7VjR3lN0IRNGwArs4mM", "signatures": ["~Jianmin_Chen1"], "readers": ["everyone"], "writers": ["~Jianmin_Chen1"], "content": {"title": "Thanks a lot for your review.", "comment": "-The algorithm setting (gradient step policy, etc) could certainly be adapted to a specific implementation (synchronous or asynchronous). It seems that the same parameter adaptation method is used for both the synchronous and asynchronous methods. Does it provide a fair comparison? These parameters might influence the performance as much as the 2 different implementations do. How robust is the comparison wrt this aspect? E.g. could the performance order be reversed with another version of SGD?\n\nAns: All the hyper-params are actually tuned for asynchronous setting, the synchronous runs just used those hyper-params or at most slightly changed the learning rate. We have not done serious tuning for the synchronous runs yet.\n\n-There is no indication on the distribution of work to the backup workers.  How do you distribute data to these specific workers and how different is it from the other workers? \n\nAns: The distribution of work to the backup is the same as other workers. In fact, any worker can become the backup if it gets slower or preempted. The data used by the backup computation will be simply dropped, which should be fine if we have a lot of epochs or data is well randomized. This turned out to be not an issue according in our experiments. \n\n-There is no discussion on mixed synchronous - asynchronous implementations. This could generalize the idea of backup workers. Could this be an interesting option and why?\n\nAns: Actually we also tried clustering/grouping (sync intra group and out of sync inter group) but the result is not as good so we didn\u2019t add it due to space limit. But apparently people are interested in this so we will briefly mention this in the final paper. Thanks.\n\n-The passage on the overlap of the gradient computation for the different layers by the end of section is not clear for me and could be made more precise.\n\nAns: Applying the gradients of upper layers can be overlapped by the gradient computation of bottom layers. You can think of it as a streaming/pipelining so the real overhead is only waiting to collect/apply gradients on the bottom layers. We will also try to make it more clear in the paper. Thanks for pointing it out."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Revisiting Distributed Synchronous SGD", "abstract": "The recent success of deep learning approaches for domains like speech recognition (Hinton et al., 2012) and computer vision (Ioffe & Szegedy, 2015) stems from many algorithmic improvements but also from the fact that the size of available training data has grown significantly over the years, together with the computing power, in terms of both CPUs and GPUs. While a single GPU often provides algorithmic simplicity and speed up to a given scale of data and model, there exist an operating point where a distributed implementation of training algorithms for deep architectures becomes necessary. \n\nPrevious works have been focusing on asynchronous SGD training, which works well up to a few dozens of workers for some models. In this work, we show that synchronous SGD training, with the help of backup workers, can not only achieve better accuracy, but also reach convergence faster with respect to wall time, i.e. use more workers more efficiently.", "pdf": "/pdf/D1VDZ5kMAu5jEJ1zfEWL.pdf", "paperhash": "chen|revisiting_distributed_synchronous_sgd", "conflicts": ["google.com"], "authors": ["Jianmin Chen", "Rajat Monga", "Samy Bengio", "Rafal Jozefowicz"], "authorids": ["jmchen@google.com", "rajatmonga@google.com", "bengio@google.com", "rafalj@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455778296383, "ddate": null, "super": null, "final": null, "tcdate": 1455778296383, "id": "ICLR.cc/2016/workshop/-/paper/80/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "D1VDZ5kMAu5jEJ1zfEWL", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/80/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1459314660345, "tcdate": 1459314660345, "id": "1WkWKBDlvFMnPB1oinmB", "invitation": "ICLR.cc/2016/workshop/-/paper/80/comment", "forum": "D1VDZ5kMAu5jEJ1zfEWL", "replyto": "3QxzNJ96BSp7y9wltPQy", "signatures": ["~Jianmin_Chen1"], "readers": ["everyone"], "writers": ["~Jianmin_Chen1"], "content": {"title": "Thanks a lot for your review.", "comment": "- I'm wondering if the authors have not used the gradient clipping in the RNN experiments as well. If the provided results are with clipping, I'm also wondering how the method would perform without the clipping in RNN.\n\nAns: we didn't use clipping in LM character predictions (either sync or async) and there are no really long term dependencies in the data so the clipping shouldn't be critical. In some experiments char LSTM was running for only 12 steps and the blowups with async continued\n\n- In the DRAW experiments, the authors only mention the convergence speed, but not provide the accuracies.\n\nAns: the numbers at convergence for sync and async were within 0.1% from each other (82.82 vs 82.88). Aside from noise, we didn't anneal learning rate to 0 at the end of the training and for sync we were using much higher learning rates (it typically helps to finetune the model at the end with smaller LRs).\n\n- The Async-SGD seems to be a simple one in the class of Async-SGD methods. It would be interesting to see comparisons to more advanced Async-SGD methods as well.\n\nAns: We are not sure which \u201csimple\u201d you are referring to here. The rmsprop with momentum seems to be one of the more complex SGD algorithms. In terms of different synchronization methods, actually we also tried clustering/grouping (sync intra group and out of sync inter group) but the result is not as good so we didn\u2019t add it due to space limit. But apparently people are interested in this so we will briefly mention this in the final paper. Thanks.\n\n- It would have been interesting to see how the Sync-SGD is affected by increasing the response time of the slowest worker.\n\nAns: Increasing time of slowest worker is a less severe case for a \u201cdead\u201d worker, the backup workers can handle this well. Even if you have too many dead workers, our system can still work with some worker doing computation for multiple batches, which will be slower but expected if the resource is limited."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Revisiting Distributed Synchronous SGD", "abstract": "The recent success of deep learning approaches for domains like speech recognition (Hinton et al., 2012) and computer vision (Ioffe & Szegedy, 2015) stems from many algorithmic improvements but also from the fact that the size of available training data has grown significantly over the years, together with the computing power, in terms of both CPUs and GPUs. While a single GPU often provides algorithmic simplicity and speed up to a given scale of data and model, there exist an operating point where a distributed implementation of training algorithms for deep architectures becomes necessary. \n\nPrevious works have been focusing on asynchronous SGD training, which works well up to a few dozens of workers for some models. In this work, we show that synchronous SGD training, with the help of backup workers, can not only achieve better accuracy, but also reach convergence faster with respect to wall time, i.e. use more workers more efficiently.", "pdf": "/pdf/D1VDZ5kMAu5jEJ1zfEWL.pdf", "paperhash": "chen|revisiting_distributed_synchronous_sgd", "conflicts": ["google.com"], "authors": ["Jianmin Chen", "Rajat Monga", "Samy Bengio", "Rafal Jozefowicz"], "authorids": ["jmchen@google.com", "rajatmonga@google.com", "bengio@google.com", "rafalj@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455778296383, "ddate": null, "super": null, "final": null, "tcdate": 1455778296383, "id": "ICLR.cc/2016/workshop/-/paper/80/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "D1VDZ5kMAu5jEJ1zfEWL", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/80/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1459314228523, "tcdate": 1459314228523, "id": "vl6lE85MwH7OYLG5in8D", "invitation": "ICLR.cc/2016/workshop/-/paper/80/comment", "forum": "D1VDZ5kMAu5jEJ1zfEWL", "replyto": "wVqPGwjQ9IG0qV7mtLxk", "signatures": ["~Jianmin_Chen1"], "readers": ["everyone"], "writers": ["~Jianmin_Chen1"], "content": {"title": "Thanks a lot for your review.", "comment": "We think the most important information about the hardware system is the K40 GPUs we use which is described in the paper. Other than that, it is basically the standard intel servers with standard networking. One thing to note (also added to the paper) is that the resources were shared with other jobs so the worker could also be slowed/preempted by other jobs besides broken hardware. And our system handles that well."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Revisiting Distributed Synchronous SGD", "abstract": "The recent success of deep learning approaches for domains like speech recognition (Hinton et al., 2012) and computer vision (Ioffe & Szegedy, 2015) stems from many algorithmic improvements but also from the fact that the size of available training data has grown significantly over the years, together with the computing power, in terms of both CPUs and GPUs. While a single GPU often provides algorithmic simplicity and speed up to a given scale of data and model, there exist an operating point where a distributed implementation of training algorithms for deep architectures becomes necessary. \n\nPrevious works have been focusing on asynchronous SGD training, which works well up to a few dozens of workers for some models. In this work, we show that synchronous SGD training, with the help of backup workers, can not only achieve better accuracy, but also reach convergence faster with respect to wall time, i.e. use more workers more efficiently.", "pdf": "/pdf/D1VDZ5kMAu5jEJ1zfEWL.pdf", "paperhash": "chen|revisiting_distributed_synchronous_sgd", "conflicts": ["google.com"], "authors": ["Jianmin Chen", "Rajat Monga", "Samy Bengio", "Rafal Jozefowicz"], "authorids": ["jmchen@google.com", "rajatmonga@google.com", "bengio@google.com", "rafalj@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455778296383, "ddate": null, "super": null, "final": null, "tcdate": 1455778296383, "id": "ICLR.cc/2016/workshop/-/paper/80/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "D1VDZ5kMAu5jEJ1zfEWL", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/80/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458161232574, "tcdate": 1458161232574, "id": "wVqPGwjQ9IG0qV7mtLxk", "invitation": "ICLR.cc/2016/workshop/-/paper/80/review/12", "forum": "D1VDZ5kMAu5jEJ1zfEWL", "replyto": "D1VDZ5kMAu5jEJ1zfEWL", "signatures": ["ICLR.cc/2016/workshop/paper/80/reviewer/12"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/80/reviewer/12"], "content": {"title": "", "rating": "7: Good paper, accept", "review": "The paper discusses a very important problem which deserves more attention from the academic community. Large-scale deep learning is important for industrial applications. The paper proposes a slight modification to classical sync-SGD where instead of waiting on all threads, the central controller only waits for a majority fraction of it, and proceeds as if it has received information from all threads. This is motivated from the observation that in multi-worker systems, typically, there are only a few outlying slow workers which negatively impact the overall system's speed (Pareto principle). Ignoring these workers should thus give a huge speedup.\n\nAs with any systems machine learning paper, details about the actual system used would have been very useful, i.e. more information about the hardware used.\n\nThe results in the paper are impressive, which makes this paper well worth talking about, and thus deserves to be in the workshop proceedings.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Revisiting Distributed Synchronous SGD", "abstract": "The recent success of deep learning approaches for domains like speech recognition (Hinton et al., 2012) and computer vision (Ioffe & Szegedy, 2015) stems from many algorithmic improvements but also from the fact that the size of available training data has grown significantly over the years, together with the computing power, in terms of both CPUs and GPUs. While a single GPU often provides algorithmic simplicity and speed up to a given scale of data and model, there exist an operating point where a distributed implementation of training algorithms for deep architectures becomes necessary. \n\nPrevious works have been focusing on asynchronous SGD training, which works well up to a few dozens of workers for some models. In this work, we show that synchronous SGD training, with the help of backup workers, can not only achieve better accuracy, but also reach convergence faster with respect to wall time, i.e. use more workers more efficiently.", "pdf": "/pdf/D1VDZ5kMAu5jEJ1zfEWL.pdf", "paperhash": "chen|revisiting_distributed_synchronous_sgd", "conflicts": ["google.com"], "authors": ["Jianmin Chen", "Rajat Monga", "Samy Bengio", "Rafal Jozefowicz"], "authorids": ["jmchen@google.com", "rajatmonga@google.com", "bengio@google.com", "rafalj@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456579944451, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456579944451, "id": "ICLR.cc/2016/workshop/-/paper/80/review/12", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "D1VDZ5kMAu5jEJ1zfEWL", "replyto": "D1VDZ5kMAu5jEJ1zfEWL", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/80/reviewer/12", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457647072204, "tcdate": 1457647072204, "id": "3QxzNJ96BSp7y9wltPQy", "invitation": "ICLR.cc/2016/workshop/-/paper/80/review/10", "forum": "D1VDZ5kMAu5jEJ1zfEWL", "replyto": "D1VDZ5kMAu5jEJ1zfEWL", "signatures": ["ICLR.cc/2016/workshop/paper/80/reviewer/10"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/80/reviewer/10"], "content": {"title": "Revisited the potential of the synchronous SGD method for distributed training of large deep learning models by performing experiments on a few of models. The results will be useful to the community. But the novelty and originality is rather limited. It seems okay as a workshop paper.", "rating": "6: Marginally above acceptance threshold", "review": "- Assuming that all nodes run at about the same speeds, using the Sync-SGD instead of the Async-SGD seems a reasonable thing that one could try. And it is interesting to see actually that Sync-SGD performs better than Async-SGD.\n\n- I'm wondering if the authors have not used the gradient clipping in the RNN experiments as well. If the provided results are with clipping, I'm also wondering how the method would perform without the clipping in RNN.\n\n- In the DRAW experiments, the authors only mention the convergence speed, but not provide the accuracies.\n\n- The Async-SGD seems to be a simple one in the class of Async-SGD methods. It would be interesting to see comparisons to more advanced Async-SGD methods as well.\n\n- It would have been interesting to see how the Sync-SGD is affected by increasing the response time of the slowest worker.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Revisiting Distributed Synchronous SGD", "abstract": "The recent success of deep learning approaches for domains like speech recognition (Hinton et al., 2012) and computer vision (Ioffe & Szegedy, 2015) stems from many algorithmic improvements but also from the fact that the size of available training data has grown significantly over the years, together with the computing power, in terms of both CPUs and GPUs. While a single GPU often provides algorithmic simplicity and speed up to a given scale of data and model, there exist an operating point where a distributed implementation of training algorithms for deep architectures becomes necessary. \n\nPrevious works have been focusing on asynchronous SGD training, which works well up to a few dozens of workers for some models. In this work, we show that synchronous SGD training, with the help of backup workers, can not only achieve better accuracy, but also reach convergence faster with respect to wall time, i.e. use more workers more efficiently.", "pdf": "/pdf/D1VDZ5kMAu5jEJ1zfEWL.pdf", "paperhash": "chen|revisiting_distributed_synchronous_sgd", "conflicts": ["google.com"], "authors": ["Jianmin Chen", "Rajat Monga", "Samy Bengio", "Rafal Jozefowicz"], "authorids": ["jmchen@google.com", "rajatmonga@google.com", "bengio@google.com", "rafalj@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456579945084, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456579945084, "id": "ICLR.cc/2016/workshop/-/paper/80/review/10", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "D1VDZ5kMAu5jEJ1zfEWL", "replyto": "D1VDZ5kMAu5jEJ1zfEWL", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/80/reviewer/10", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457611317770, "tcdate": 1457611317770, "id": "L7VjR3lN0IRNGwArs4mM", "invitation": "ICLR.cc/2016/workshop/-/paper/80/review/11", "forum": "D1VDZ5kMAu5jEJ1zfEWL", "replyto": "D1VDZ5kMAu5jEJ1zfEWL", "signatures": ["~gallinari_patrick1"], "readers": ["everyone"], "writers": ["~gallinari_patrick1"], "content": {"title": "    aa", "rating": "9: Top 15% of accepted papers, strong accept", "review": "The paper introduces a synchronous parallel stochastic gradient descent algorithm and compares its performance on different tasks with a reference asynchronous SGD. The behavior of the two algorithms are compared for different configurations (number of parallel machines - tasks).\n\nThe paper addresses an important problem: defining efficient distributed algorithms for large scale deep architectures.  Its contribution is to provide experimental results on different problem types and these results will certainly be interesting for a large community. The comparison of synchronous \u2013 asynchronous optimization methods is also a topic of interest by itself and this paper contributes in this direction.\n\nIt is not clear in the paper if the parameter servers in the comparison have the same configuration.\nThe algorithm setting (gradient step policy, etc) could certainly be adapted to a specific implementation (synchronous or asynchronous). It seems that the same parameter adaptation method is used for both the synchronous and asynchronous methods. Does it provide a fair comparison? These parameters might influence the performance as much as the 2 different implementations do. How robust is the comparison wrt this aspect? E.g. could the performance order be reversed with another version of SGD?\n\nThere is no indication on the distribution of work to the backup workers.  How do you distribute data to these specific workers and how different is it from the other workers? \n\nThere is no discussion on mixed synchronous - asynchronous implementations. This could generalize the idea of backup workers. Could this be an interesting option and why?\n\nThe passage on the overlap of the gradient computation for the different layers by the end of section is not clear for me and could be made more precise.\n\n\nPro: comparison of two different options (synchronous vs asynchronous) for parallelizing SGD. Experiments on different configurations.\n\nCons: not clear (for me) if the SGD algorithm parameters setting itself is not as important as the option choice (sync. vs async) for the performance.\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Revisiting Distributed Synchronous SGD", "abstract": "The recent success of deep learning approaches for domains like speech recognition (Hinton et al., 2012) and computer vision (Ioffe & Szegedy, 2015) stems from many algorithmic improvements but also from the fact that the size of available training data has grown significantly over the years, together with the computing power, in terms of both CPUs and GPUs. While a single GPU often provides algorithmic simplicity and speed up to a given scale of data and model, there exist an operating point where a distributed implementation of training algorithms for deep architectures becomes necessary. \n\nPrevious works have been focusing on asynchronous SGD training, which works well up to a few dozens of workers for some models. In this work, we show that synchronous SGD training, with the help of backup workers, can not only achieve better accuracy, but also reach convergence faster with respect to wall time, i.e. use more workers more efficiently.", "pdf": "/pdf/D1VDZ5kMAu5jEJ1zfEWL.pdf", "paperhash": "chen|revisiting_distributed_synchronous_sgd", "conflicts": ["google.com"], "authors": ["Jianmin Chen", "Rajat Monga", "Samy Bengio", "Rafal Jozefowicz"], "authorids": ["jmchen@google.com", "rajatmonga@google.com", "bengio@google.com", "rafalj@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456579944696, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456579944696, "id": "ICLR.cc/2016/workshop/-/paper/80/review/11", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "D1VDZ5kMAu5jEJ1zfEWL", "replyto": "D1VDZ5kMAu5jEJ1zfEWL", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/80/reviewer/11", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1455778293662, "tcdate": 1455778293662, "id": "D1VDZ5kMAu5jEJ1zfEWL", "invitation": "ICLR.cc/2016/workshop/-/submission", "forum": "D1VDZ5kMAu5jEJ1zfEWL", "signatures": ["~Jianmin_Chen1"], "readers": ["everyone"], "writers": ["~Jianmin_Chen1"], "content": {"CMT_id": "", "title": "Revisiting Distributed Synchronous SGD", "abstract": "The recent success of deep learning approaches for domains like speech recognition (Hinton et al., 2012) and computer vision (Ioffe & Szegedy, 2015) stems from many algorithmic improvements but also from the fact that the size of available training data has grown significantly over the years, together with the computing power, in terms of both CPUs and GPUs. While a single GPU often provides algorithmic simplicity and speed up to a given scale of data and model, there exist an operating point where a distributed implementation of training algorithms for deep architectures becomes necessary. \n\nPrevious works have been focusing on asynchronous SGD training, which works well up to a few dozens of workers for some models. In this work, we show that synchronous SGD training, with the help of backup workers, can not only achieve better accuracy, but also reach convergence faster with respect to wall time, i.e. use more workers more efficiently.", "pdf": "/pdf/D1VDZ5kMAu5jEJ1zfEWL.pdf", "paperhash": "chen|revisiting_distributed_synchronous_sgd", "conflicts": ["google.com"], "authors": ["Jianmin Chen", "Rajat Monga", "Samy Bengio", "Rafal Jozefowicz"], "authorids": ["jmchen@google.com", "rajatmonga@google.com", "bengio@google.com", "rafalj@google.com"]}, "nonreaders": [], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1454464564200, "ddate": null, "super": null, "final": null, "duedate": 1455833700000, "tcdate": 1454464564200, "id": "ICLR.cc/2016/workshop/-/submission", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"order": 4, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv.", "value-regex": "upload|http://arxiv.org/pdf/.+"}, "title": {"order": 3, "description": "Title of paper.", "value-regex": ".{0,500}"}, "abstract": {"order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"order": 1, "description": "Comma separated list of author names, as they appear in the paper.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "author_emails": {"order": 2, "description": "Comma separated list of author email addresses, in the same order as above.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "conflicts": {"order": 100, "description": "Semi-colon separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.).", "value-regex": "^([a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))+(;[a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))*$"}, "CMT_id": {"order": 5, "value-regex": ".*", "description": "If the paper is a resubmission from the ICLR 2016 Conference Track, enter its CMT ID; otherwise, leave blank."}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "expdate": 1463609700000}}}], "count": 7}