{"notes": [{"tddate": null, "ddate": null, "original": null, "tmdate": 1525914431258, "tcdate": 1510752390645, "number": 1, "cdate": 1510752390645, "id": "Bk15lpF1G", "invitation": "ICLR.cc/2018/Conference/-/Paper204/Official_Review", "forum": "S1uxsye0Z", "replyto": "S1uxsye0Z", "signatures": ["ICLR.cc/2018/Conference/Paper204/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "mathematical analysis seems not sound", "rating": "6: Marginally above acceptance threshold", "review": "This paper studies the adjustment of dropout rates which is a useful tool to prevent the overfitting of deep neural networks. The authors derive a generalization error bound in terms of dropout rates. Based on this, the authors propose a regularization framework to adaptively select dropout rates. Experimental results are also given to verify the theory.\n\nMajor comments:\n(1) The Empirical Rademacher complexity is not defined. For completeness, it would be better to define it at least in the appendix.\n(2) I can not follow the inequality (5). Especially, according to the main text, f^L is a vector-valued function . Therefore, it is not clear to me the meaning of \\sum\\sigma_if^L(x_i,w) in (5).\n(3) I can also not see clearly the third equality in (9). Note that f^l is a vector-valued function. It is not clear to me how it is related to a summation over j there.\n(4) There is a linear dependency on the number of classes in Theorem 3.1. Is it possible to further improve this dependency?\n\nMinor comments:\n(1) Section 4: 1e-3,1e-4,1e-5 is not consistent with 1e^{-3}, 1e^{-4},1e^{-5}\n(2) Abstract: there should be a space before \"Experiments\".\n(3) It would be better to give more details (e.g., page, section) in citing a book in the proof of Theorem 3.1\n\nSummary:\nThe mathematical analysis in the present version is not rigorous. The authors should improve the mathematical analysis.\n\n----------------------------\nAfter Rebuttal:\nThank you for revising the paper. I think there are still some possible problems. \nLet us consider eq (12) in the appendix on the contraction property of Rademacher complexity (RC).\n(1) Since you consider a variant of RC with absolute value inside the supermum, to my best knowledge, the contraction property (12) should involve an additional factor of 2, see, e.g., Theorem 12 of \"Rademacher and Gaussian Complexities: Risk Bounds and Structural Results\" by Bartlett and Mendelson. Since you need to apply this contraction property L times, there should be a factor of 2^L in the error bound. This make the bound not appealing for neural networks with a moderate L.\n(2) Second, the function g involves an expectation w.r.t. r before the activation function. I am not sure whether this existence of expectation w.r.t. r would make the contraction property applicable in this case.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Adaptive Dropout with Rademacher Complexity Regularization", "abstract": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art deep learning algorithms impose dropout strategy to prevent feature co-adaptation. However, choosing the dropout rates remains an art of heuristics or relies on empirical grid-search over some hyperparameter space. In this work, we show the network Rademacher complexity is bounded by a function related to the dropout rate vectors and the weight coefficient matrices. Subsequently, we impose this bound as a regularizer and provide a theoretical justified way to trade-off between model complexity and representation power. Therefore, the dropout rates and the empirical loss are unified into the same objective function, which is then optimized using the block coordinate descent algorithm. We discover that the adaptively adjusted dropout rates converge to some interesting distributions that reveal meaningful patterns.Experiments on the task of image and document classification also show our method achieves better performance compared to the state-of the-art dropout algorithms.", "pdf": "/pdf/a4b4dcae4dcddfaf2bee5828f6d8452898c52821.pdf", "TL;DR": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.", "paperhash": "zhai|adaptive_dropout_with_rademacher_complexity_regularization", "_bibtex": "@inproceedings{\nzhai2018adaptive,\ntitle={Adaptive Dropout with Rademacher Complexity Regularization},\nauthor={Ke Zhai and Huan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=S1uxsye0Z},\n}", "keywords": ["model complexity", "regularization", "deep learning", "model generalization", "adaptive dropout"], "authors": ["Ke Zhai", "Huan Wang"], "authorids": ["zhaikedavy@gmail.com", "joyousprince@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642408510, "id": "ICLR.cc/2018/Conference/-/Paper204/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper204/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper204/AnonReviewer3", "ICLR.cc/2018/Conference/Paper204/AnonReviewer1", "ICLR.cc/2018/Conference/Paper204/AnonReviewer2"], "reply": {"forum": "S1uxsye0Z", "replyto": "S1uxsye0Z", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper204/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642408510}}}, {"tddate": null, "ddate": null, "tmdate": 1519339021168, "tcdate": 1509059311749, "number": 204, "cdate": 1518730185517, "id": "S1uxsye0Z", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "S1uxsye0Z", "original": "B1wgiJgAb", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Adaptive Dropout with Rademacher Complexity Regularization", "abstract": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art deep learning algorithms impose dropout strategy to prevent feature co-adaptation. However, choosing the dropout rates remains an art of heuristics or relies on empirical grid-search over some hyperparameter space. In this work, we show the network Rademacher complexity is bounded by a function related to the dropout rate vectors and the weight coefficient matrices. Subsequently, we impose this bound as a regularizer and provide a theoretical justified way to trade-off between model complexity and representation power. Therefore, the dropout rates and the empirical loss are unified into the same objective function, which is then optimized using the block coordinate descent algorithm. We discover that the adaptively adjusted dropout rates converge to some interesting distributions that reveal meaningful patterns.Experiments on the task of image and document classification also show our method achieves better performance compared to the state-of the-art dropout algorithms.", "pdf": "/pdf/a4b4dcae4dcddfaf2bee5828f6d8452898c52821.pdf", "TL;DR": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.", "paperhash": "zhai|adaptive_dropout_with_rademacher_complexity_regularization", "_bibtex": "@inproceedings{\nzhai2018adaptive,\ntitle={Adaptive Dropout with Rademacher Complexity Regularization},\nauthor={Ke Zhai and Huan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=S1uxsye0Z},\n}", "keywords": ["model complexity", "regularization", "deep learning", "model generalization", "adaptive dropout"], "authors": ["Ke Zhai", "Huan Wang"], "authorids": ["zhaikedavy@gmail.com", "joyousprince@gmail.com"]}, "nonreaders": [], "details": {"replyCount": 22, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "tmdate": 1517951572798, "tcdate": 1517001806398, "number": 17, "cdate": 1517001806398, "id": "HyUUnfKHz", "invitation": "ICLR.cc/2018/Conference/-/Paper204/Official_Comment", "forum": "S1uxsye0Z", "replyto": "BkYWorDrf", "signatures": ["ICLR.cc/2018/Conference/Paper204/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper204/Authors"], "content": {"title": "Thanks for the comments and suggestions.", "comment": "We added David McAllister\u2019s paper in our latest revision. \n\n1. The PAC-Bayes theorem holds for all \u201cpriors\u201d. The Rademacher bound holds for all \"hypothesis\" in the class. We do not assume there is any probability measure over the hypothesis class. But we agree adding priors often gives us convenience in the proof.\n\n2. Thanks for the great suggestion. Extension of David\u2019s bound to different dropout rates is definitely worth trying.\n\n############################################################################\n\nThe issue pointed out by reviewer 3 is valid. Though we fixed the issue, it leads to a slight change of the bound when q > 1. As a result, some of the experiments with the setting q > 1 need to be modified and rerun. \n\nAt this point we are considering withdraw and resubmit. \n\nThanks very much for your time and effort. \n\n############################################################################\nWe updated the experiments and now they are consistent with the updated theorem. Thanks for your patience and understanding. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Dropout with Rademacher Complexity Regularization", "abstract": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art deep learning algorithms impose dropout strategy to prevent feature co-adaptation. However, choosing the dropout rates remains an art of heuristics or relies on empirical grid-search over some hyperparameter space. In this work, we show the network Rademacher complexity is bounded by a function related to the dropout rate vectors and the weight coefficient matrices. Subsequently, we impose this bound as a regularizer and provide a theoretical justified way to trade-off between model complexity and representation power. Therefore, the dropout rates and the empirical loss are unified into the same objective function, which is then optimized using the block coordinate descent algorithm. We discover that the adaptively adjusted dropout rates converge to some interesting distributions that reveal meaningful patterns.Experiments on the task of image and document classification also show our method achieves better performance compared to the state-of the-art dropout algorithms.", "pdf": "/pdf/a4b4dcae4dcddfaf2bee5828f6d8452898c52821.pdf", "TL;DR": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.", "paperhash": "zhai|adaptive_dropout_with_rademacher_complexity_regularization", "_bibtex": "@inproceedings{\nzhai2018adaptive,\ntitle={Adaptive Dropout with Rademacher Complexity Regularization},\nauthor={Ke Zhai and Huan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=S1uxsye0Z},\n}", "keywords": ["model complexity", "regularization", "deep learning", "model generalization", "adaptive dropout"], "authors": ["Ke Zhai", "Huan Wang"], "authorids": ["zhaikedavy@gmail.com", "joyousprince@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825737612, "id": "ICLR.cc/2018/Conference/-/Paper204/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "S1uxsye0Z", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper204/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper204/Authors|ICLR.cc/2018/Conference/Paper204/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper204/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper204/Authors|ICLR.cc/2018/Conference/Paper204/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper204/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper204/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper204/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper204/Reviewers", "ICLR.cc/2018/Conference/Paper204/Authors", "ICLR.cc/2018/Conference/Paper204/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825737612}}}, {"tddate": null, "ddate": null, "tmdate": 1517879144757, "tcdate": 1517879144757, "number": 19, "cdate": 1517879144757, "id": "rJb_1Y88z", "invitation": "ICLR.cc/2018/Conference/-/Paper204/Official_Comment", "forum": "S1uxsye0Z", "replyto": "By4ny9FHG", "signatures": ["ICLR.cc/2018/Conference/Paper204/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper204/Authors"], "content": {"title": "we updated the experiments", "comment": "Your comments and suggestions are always welcome. We just updated the experiments with \\|\\theta\\|_1^{1/q} so that they are consistent with the theorem. Please feel free to add more comments if you have further concerns. Thank you."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Dropout with Rademacher Complexity Regularization", "abstract": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art deep learning algorithms impose dropout strategy to prevent feature co-adaptation. However, choosing the dropout rates remains an art of heuristics or relies on empirical grid-search over some hyperparameter space. In this work, we show the network Rademacher complexity is bounded by a function related to the dropout rate vectors and the weight coefficient matrices. Subsequently, we impose this bound as a regularizer and provide a theoretical justified way to trade-off between model complexity and representation power. Therefore, the dropout rates and the empirical loss are unified into the same objective function, which is then optimized using the block coordinate descent algorithm. We discover that the adaptively adjusted dropout rates converge to some interesting distributions that reveal meaningful patterns.Experiments on the task of image and document classification also show our method achieves better performance compared to the state-of the-art dropout algorithms.", "pdf": "/pdf/a4b4dcae4dcddfaf2bee5828f6d8452898c52821.pdf", "TL;DR": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.", "paperhash": "zhai|adaptive_dropout_with_rademacher_complexity_regularization", "_bibtex": "@inproceedings{\nzhai2018adaptive,\ntitle={Adaptive Dropout with Rademacher Complexity Regularization},\nauthor={Ke Zhai and Huan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=S1uxsye0Z},\n}", "keywords": ["model complexity", "regularization", "deep learning", "model generalization", "adaptive dropout"], "authors": ["Ke Zhai", "Huan Wang"], "authorids": ["zhaikedavy@gmail.com", "joyousprince@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825737612, "id": "ICLR.cc/2018/Conference/-/Paper204/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "S1uxsye0Z", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper204/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper204/Authors|ICLR.cc/2018/Conference/Paper204/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper204/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper204/Authors|ICLR.cc/2018/Conference/Paper204/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper204/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper204/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper204/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper204/Reviewers", "ICLR.cc/2018/Conference/Paper204/Authors", "ICLR.cc/2018/Conference/Paper204/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825737612}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260095530, "tcdate": 1517249448077, "number": 216, "cdate": 1517249448057, "id": "rJxnmyTHM", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "S1uxsye0Z", "replyto": "S1uxsye0Z", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"title": "ICLR 2018 Conference Acceptance Decision", "comment": "The reviewers agreed that the work addresses an important problem. There was disagreement as to the correctness of the arguments in the paper: one of these reviewers was eventually convinced. The other pointed out another two issue in their final post, but it seems that 1. the first is easily adopted and does not affect the correctness of the experiments and 2. the second was fixed in the second revision. Ideally these would be rechecked by the third reviewer, but ultimately the correctness of the work is the authors' responsibility.\n\nSome related work (by McAllister) was pointed out late in the process. I encourage the authors to take this related work seriously in any revisions. It deserves more than two sentences.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Dropout with Rademacher Complexity Regularization", "abstract": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art deep learning algorithms impose dropout strategy to prevent feature co-adaptation. However, choosing the dropout rates remains an art of heuristics or relies on empirical grid-search over some hyperparameter space. In this work, we show the network Rademacher complexity is bounded by a function related to the dropout rate vectors and the weight coefficient matrices. Subsequently, we impose this bound as a regularizer and provide a theoretical justified way to trade-off between model complexity and representation power. Therefore, the dropout rates and the empirical loss are unified into the same objective function, which is then optimized using the block coordinate descent algorithm. We discover that the adaptively adjusted dropout rates converge to some interesting distributions that reveal meaningful patterns.Experiments on the task of image and document classification also show our method achieves better performance compared to the state-of the-art dropout algorithms.", "pdf": "/pdf/a4b4dcae4dcddfaf2bee5828f6d8452898c52821.pdf", "TL;DR": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.", "paperhash": "zhai|adaptive_dropout_with_rademacher_complexity_regularization", "_bibtex": "@inproceedings{\nzhai2018adaptive,\ntitle={Adaptive Dropout with Rademacher Complexity Regularization},\nauthor={Ke Zhai and Huan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=S1uxsye0Z},\n}", "keywords": ["model complexity", "regularization", "deep learning", "model generalization", "adaptive dropout"], "authors": ["Ke Zhai", "Huan Wang"], "authorids": ["zhaikedavy@gmail.com", "joyousprince@gmail.com"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "tmdate": 1517031339842, "tcdate": 1517031339842, "number": 18, "cdate": 1517031339842, "id": "By4ny9FHG", "invitation": "ICLR.cc/2018/Conference/-/Paper204/Official_Comment", "forum": "S1uxsye0Z", "replyto": "r1cQ9zYSf", "signatures": ["ICLR.cc/2018/Conference/Paper204/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper204/AnonReviewer3"], "content": {"title": "thanks for the update of the draft", "comment": "Thanks for the revision addressing my concerns on the deduction. Also, feel regretful that the change of arguments leads to a different bound."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Dropout with Rademacher Complexity Regularization", "abstract": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art deep learning algorithms impose dropout strategy to prevent feature co-adaptation. However, choosing the dropout rates remains an art of heuristics or relies on empirical grid-search over some hyperparameter space. In this work, we show the network Rademacher complexity is bounded by a function related to the dropout rate vectors and the weight coefficient matrices. Subsequently, we impose this bound as a regularizer and provide a theoretical justified way to trade-off between model complexity and representation power. Therefore, the dropout rates and the empirical loss are unified into the same objective function, which is then optimized using the block coordinate descent algorithm. We discover that the adaptively adjusted dropout rates converge to some interesting distributions that reveal meaningful patterns.Experiments on the task of image and document classification also show our method achieves better performance compared to the state-of the-art dropout algorithms.", "pdf": "/pdf/a4b4dcae4dcddfaf2bee5828f6d8452898c52821.pdf", "TL;DR": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.", "paperhash": "zhai|adaptive_dropout_with_rademacher_complexity_regularization", "_bibtex": "@inproceedings{\nzhai2018adaptive,\ntitle={Adaptive Dropout with Rademacher Complexity Regularization},\nauthor={Ke Zhai and Huan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=S1uxsye0Z},\n}", "keywords": ["model complexity", "regularization", "deep learning", "model generalization", "adaptive dropout"], "authors": ["Ke Zhai", "Huan Wang"], "authorids": ["zhaikedavy@gmail.com", "joyousprince@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825737612, "id": "ICLR.cc/2018/Conference/-/Paper204/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "S1uxsye0Z", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper204/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper204/Authors|ICLR.cc/2018/Conference/Paper204/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper204/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper204/Authors|ICLR.cc/2018/Conference/Paper204/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper204/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper204/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper204/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper204/Reviewers", "ICLR.cc/2018/Conference/Paper204/Authors", "ICLR.cc/2018/Conference/Paper204/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825737612}}}, {"tddate": null, "ddate": null, "tmdate": 1517001249904, "tcdate": 1517001249904, "number": 16, "cdate": 1517001249904, "id": "r1cQ9zYSf", "invitation": "ICLR.cc/2018/Conference/-/Paper204/Official_Comment", "forum": "S1uxsye0Z", "replyto": "rkZCWLOHf", "signatures": ["ICLR.cc/2018/Conference/Paper204/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper204/Authors"], "content": {"title": "Thanks for your insightful comments.", "comment": "You are right. After the update, the upper bound blows a little bit from \\|\\theta\\|_q to \\|\\theta\\|_1^{1/q}. The claim of the theorem now only holds when q = 1.  When q>1, we need to adjust the bound. We updated our draft to make sure the theory part is sound.\n\nSince we also have some experiments on q = 2 and q=\\infty, this suggests part of our experiments needs to be modified and rerun. At this point we are considering withdraw and resubmit later."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Dropout with Rademacher Complexity Regularization", "abstract": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art deep learning algorithms impose dropout strategy to prevent feature co-adaptation. However, choosing the dropout rates remains an art of heuristics or relies on empirical grid-search over some hyperparameter space. In this work, we show the network Rademacher complexity is bounded by a function related to the dropout rate vectors and the weight coefficient matrices. Subsequently, we impose this bound as a regularizer and provide a theoretical justified way to trade-off between model complexity and representation power. Therefore, the dropout rates and the empirical loss are unified into the same objective function, which is then optimized using the block coordinate descent algorithm. We discover that the adaptively adjusted dropout rates converge to some interesting distributions that reveal meaningful patterns.Experiments on the task of image and document classification also show our method achieves better performance compared to the state-of the-art dropout algorithms.", "pdf": "/pdf/a4b4dcae4dcddfaf2bee5828f6d8452898c52821.pdf", "TL;DR": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.", "paperhash": "zhai|adaptive_dropout_with_rademacher_complexity_regularization", "_bibtex": "@inproceedings{\nzhai2018adaptive,\ntitle={Adaptive Dropout with Rademacher Complexity Regularization},\nauthor={Ke Zhai and Huan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=S1uxsye0Z},\n}", "keywords": ["model complexity", "regularization", "deep learning", "model generalization", "adaptive dropout"], "authors": ["Ke Zhai", "Huan Wang"], "authorids": ["zhaikedavy@gmail.com", "joyousprince@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825737612, "id": "ICLR.cc/2018/Conference/-/Paper204/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "S1uxsye0Z", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper204/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper204/Authors|ICLR.cc/2018/Conference/Paper204/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper204/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper204/Authors|ICLR.cc/2018/Conference/Paper204/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper204/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper204/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper204/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper204/Reviewers", "ICLR.cc/2018/Conference/Paper204/Authors", "ICLR.cc/2018/Conference/Paper204/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825737612}}}, {"tddate": null, "ddate": null, "tmdate": 1516949961475, "tcdate": 1516949961475, "number": 15, "cdate": 1516949961475, "id": "rkZCWLOHf", "invitation": "ICLR.cc/2018/Conference/-/Paper204/Official_Comment", "forum": "S1uxsye0Z", "replyto": "HJkIwe_Sz", "signatures": ["ICLR.cc/2018/Conference/Paper204/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper204/AnonReviewer3"], "content": {"title": "thanks for the update of the draft", "comment": "Thank you for considering the comments in the revision. Here are some further comments:\n\nin eq (12), the last two equations are the same and one can be removed.\nin eq (14), it seems that \\|\\theta\\|_q should be \\|\\theta\\|_1^{1/q}? Please check the deductions:\nE\\|r\\|_q = E [ [\\sum_i|r_i|^q]^{1/q} ] = E [ [\\sum_i r_i ]^{1/q} ] \\leq [\\sum_i E r_i]^{1/q} = \\|\\theta\\|_1^{1/q}"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Dropout with Rademacher Complexity Regularization", "abstract": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art deep learning algorithms impose dropout strategy to prevent feature co-adaptation. However, choosing the dropout rates remains an art of heuristics or relies on empirical grid-search over some hyperparameter space. In this work, we show the network Rademacher complexity is bounded by a function related to the dropout rate vectors and the weight coefficient matrices. Subsequently, we impose this bound as a regularizer and provide a theoretical justified way to trade-off between model complexity and representation power. Therefore, the dropout rates and the empirical loss are unified into the same objective function, which is then optimized using the block coordinate descent algorithm. We discover that the adaptively adjusted dropout rates converge to some interesting distributions that reveal meaningful patterns.Experiments on the task of image and document classification also show our method achieves better performance compared to the state-of the-art dropout algorithms.", "pdf": "/pdf/a4b4dcae4dcddfaf2bee5828f6d8452898c52821.pdf", "TL;DR": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.", "paperhash": "zhai|adaptive_dropout_with_rademacher_complexity_regularization", "_bibtex": "@inproceedings{\nzhai2018adaptive,\ntitle={Adaptive Dropout with Rademacher Complexity Regularization},\nauthor={Ke Zhai and Huan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=S1uxsye0Z},\n}", "keywords": ["model complexity", "regularization", "deep learning", "model generalization", "adaptive dropout"], "authors": ["Ke Zhai", "Huan Wang"], "authorids": ["zhaikedavy@gmail.com", "joyousprince@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825737612, "id": "ICLR.cc/2018/Conference/-/Paper204/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "S1uxsye0Z", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper204/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper204/Authors|ICLR.cc/2018/Conference/Paper204/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper204/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper204/Authors|ICLR.cc/2018/Conference/Paper204/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper204/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper204/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper204/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper204/Reviewers", "ICLR.cc/2018/Conference/Paper204/Authors", "ICLR.cc/2018/Conference/Paper204/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825737612}}}, {"tddate": null, "ddate": null, "tmdate": 1516926957384, "tcdate": 1516926791296, "number": 14, "cdate": 1516926791296, "id": "HJkIwe_Sz", "invitation": "ICLR.cc/2018/Conference/-/Paper204/Official_Comment", "forum": "S1uxsye0Z", "replyto": "rJlVd7PrM", "signatures": ["ICLR.cc/2018/Conference/Paper204/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper204/Authors"], "content": {"title": "The issues pointed out by reviewer 3 are valid. We fixed the issues in our updated draft.", "comment": "Thanks very much for pointing out the issues. We are sorry in the last rebuttal we missed your point. Both issues you suggested are valid.\n\nFor your first concern:\nYes there is an absolute value operator inside, so this requires a factor of 2. Suppose we have L layers, there should be a factor of 2^L as you correctly pointed out. \nFortunately, in our empirical evaluation L is fixed, so 2^L is a constant. In this way it won\u2019t affect the experimental observation.\n\nFor your second concern:\nYou are right, the original procedure has issues due to the existence of E inside \\sup. We fixed the issue in our latest revision. The final claim in the theorem stays the same (except there is an extra 2^L term as you suggested earlier). Please check our updated proof.\n\nWe sincerely thank reviewer 3 for the great contribution and efforts. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Dropout with Rademacher Complexity Regularization", "abstract": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art deep learning algorithms impose dropout strategy to prevent feature co-adaptation. However, choosing the dropout rates remains an art of heuristics or relies on empirical grid-search over some hyperparameter space. In this work, we show the network Rademacher complexity is bounded by a function related to the dropout rate vectors and the weight coefficient matrices. Subsequently, we impose this bound as a regularizer and provide a theoretical justified way to trade-off between model complexity and representation power. Therefore, the dropout rates and the empirical loss are unified into the same objective function, which is then optimized using the block coordinate descent algorithm. We discover that the adaptively adjusted dropout rates converge to some interesting distributions that reveal meaningful patterns.Experiments on the task of image and document classification also show our method achieves better performance compared to the state-of the-art dropout algorithms.", "pdf": "/pdf/a4b4dcae4dcddfaf2bee5828f6d8452898c52821.pdf", "TL;DR": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.", "paperhash": "zhai|adaptive_dropout_with_rademacher_complexity_regularization", "_bibtex": "@inproceedings{\nzhai2018adaptive,\ntitle={Adaptive Dropout with Rademacher Complexity Regularization},\nauthor={Ke Zhai and Huan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=S1uxsye0Z},\n}", "keywords": ["model complexity", "regularization", "deep learning", "model generalization", "adaptive dropout"], "authors": ["Ke Zhai", "Huan Wang"], "authorids": ["zhaikedavy@gmail.com", "joyousprince@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825737612, "id": "ICLR.cc/2018/Conference/-/Paper204/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "S1uxsye0Z", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper204/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper204/Authors|ICLR.cc/2018/Conference/Paper204/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper204/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper204/Authors|ICLR.cc/2018/Conference/Paper204/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper204/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper204/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper204/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper204/Reviewers", "ICLR.cc/2018/Conference/Paper204/Authors", "ICLR.cc/2018/Conference/Paper204/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825737612}}}, {"tddate": null, "ddate": null, "tmdate": 1516882689428, "tcdate": 1516882689428, "number": 12, "cdate": 1516882689428, "id": "BkYWorDrf", "invitation": "ICLR.cc/2018/Conference/-/Paper204/Official_Comment", "forum": "S1uxsye0Z", "replyto": "r1wH68UHz", "signatures": ["ICLR.cc/2018/Conference/Paper204/Area_Chair"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper204/Area_Chair"], "content": {"title": "further thoughts", "comment": "Re: 1.   The PAC-Bayes theorems hold for all \"priors\".  Each \"prior\" gives you a different bound. So it is not right to say that David \"assumes\" a prior. There is no assumption. In the application of PAC-Bayes bounds, priors are often chosen for convenience to yield tractable KL divergence terms. This is the case in David's bounds.\n\nRe: 2.  I suspect the extension of David's bound to different dropout rates is straightforward."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Dropout with Rademacher Complexity Regularization", "abstract": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art deep learning algorithms impose dropout strategy to prevent feature co-adaptation. However, choosing the dropout rates remains an art of heuristics or relies on empirical grid-search over some hyperparameter space. In this work, we show the network Rademacher complexity is bounded by a function related to the dropout rate vectors and the weight coefficient matrices. Subsequently, we impose this bound as a regularizer and provide a theoretical justified way to trade-off between model complexity and representation power. Therefore, the dropout rates and the empirical loss are unified into the same objective function, which is then optimized using the block coordinate descent algorithm. We discover that the adaptively adjusted dropout rates converge to some interesting distributions that reveal meaningful patterns.Experiments on the task of image and document classification also show our method achieves better performance compared to the state-of the-art dropout algorithms.", "pdf": "/pdf/a4b4dcae4dcddfaf2bee5828f6d8452898c52821.pdf", "TL;DR": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.", "paperhash": "zhai|adaptive_dropout_with_rademacher_complexity_regularization", "_bibtex": "@inproceedings{\nzhai2018adaptive,\ntitle={Adaptive Dropout with Rademacher Complexity Regularization},\nauthor={Ke Zhai and Huan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=S1uxsye0Z},\n}", "keywords": ["model complexity", "regularization", "deep learning", "model generalization", "adaptive dropout"], "authors": ["Ke Zhai", "Huan Wang"], "authorids": ["zhaikedavy@gmail.com", "joyousprince@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825737612, "id": "ICLR.cc/2018/Conference/-/Paper204/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "S1uxsye0Z", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper204/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper204/Authors|ICLR.cc/2018/Conference/Paper204/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper204/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper204/Authors|ICLR.cc/2018/Conference/Paper204/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper204/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper204/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper204/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper204/Reviewers", "ICLR.cc/2018/Conference/Paper204/Authors", "ICLR.cc/2018/Conference/Paper204/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825737612}}}, {"tddate": null, "ddate": null, "tmdate": 1516873767842, "tcdate": 1516873767842, "number": 11, "cdate": 1516873767842, "id": "rJlVd7PrM", "invitation": "ICLR.cc/2018/Conference/-/Paper204/Official_Comment", "forum": "S1uxsye0Z", "replyto": "rJYg3KLEf", "signatures": ["ICLR.cc/2018/Conference/Paper204/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper204/AnonReviewer3"], "content": {"title": "response to the authors", "comment": "Thanks for the response. In eq (12), the left-hand side is E \\sup |E  \\phi(f)|, while the right-hand side is E \\sup |f|.\nHere E denotes the expectation. Firstly, there is an absolute value operator here which requires a factor of 2 in the application of contraction property. Secondly, an expectation is inside the absolute value operator, so, as far as i can see, the standard contraction property can not be applied in this way . "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Dropout with Rademacher Complexity Regularization", "abstract": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art deep learning algorithms impose dropout strategy to prevent feature co-adaptation. However, choosing the dropout rates remains an art of heuristics or relies on empirical grid-search over some hyperparameter space. In this work, we show the network Rademacher complexity is bounded by a function related to the dropout rate vectors and the weight coefficient matrices. Subsequently, we impose this bound as a regularizer and provide a theoretical justified way to trade-off between model complexity and representation power. Therefore, the dropout rates and the empirical loss are unified into the same objective function, which is then optimized using the block coordinate descent algorithm. We discover that the adaptively adjusted dropout rates converge to some interesting distributions that reveal meaningful patterns.Experiments on the task of image and document classification also show our method achieves better performance compared to the state-of the-art dropout algorithms.", "pdf": "/pdf/a4b4dcae4dcddfaf2bee5828f6d8452898c52821.pdf", "TL;DR": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.", "paperhash": "zhai|adaptive_dropout_with_rademacher_complexity_regularization", "_bibtex": "@inproceedings{\nzhai2018adaptive,\ntitle={Adaptive Dropout with Rademacher Complexity Regularization},\nauthor={Ke Zhai and Huan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=S1uxsye0Z},\n}", "keywords": ["model complexity", "regularization", "deep learning", "model generalization", "adaptive dropout"], "authors": ["Ke Zhai", "Huan Wang"], "authorids": ["zhaikedavy@gmail.com", "joyousprince@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825737612, "id": "ICLR.cc/2018/Conference/-/Paper204/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "S1uxsye0Z", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper204/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper204/Authors|ICLR.cc/2018/Conference/Paper204/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper204/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper204/Authors|ICLR.cc/2018/Conference/Paper204/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper204/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper204/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper204/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper204/Reviewers", "ICLR.cc/2018/Conference/Paper204/Authors", "ICLR.cc/2018/Conference/Paper204/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825737612}}}, {"tddate": null, "ddate": null, "tmdate": 1516837107683, "tcdate": 1516837107683, "number": 10, "cdate": 1516837107683, "id": "rJ3xKc8BM", "invitation": "ICLR.cc/2018/Conference/-/Paper204/Official_Comment", "forum": "S1uxsye0Z", "replyto": "BJefd2BBf", "signatures": ["ICLR.cc/2018/Conference/Paper204/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper204/AnonReviewer2"], "content": {"title": "correcting misunderstanding", "comment": "(it looks like my previous reply did not appear, so trying again)\nThanks for correcting my mistakes on both the L1 norm and the data dependence. It seems plausible in that adding Rad is justifiable. I think 6.3 is important in the chain of reasoning. Right now, it is suggestive but does not sufficiently justify the approach. It would be valuable to verify that Rad give a reasonable dependence both in terms of n and d as a regularizer. Secondly, it still suggests that bounds of Rad seems to be common regularizers, rather than that Rad itself is good.\n\nHowever, given that it is intuitive to optimize a generalization bound, and my previous misunderstanding, I changed my rating to weak accept."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Dropout with Rademacher Complexity Regularization", "abstract": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art deep learning algorithms impose dropout strategy to prevent feature co-adaptation. However, choosing the dropout rates remains an art of heuristics or relies on empirical grid-search over some hyperparameter space. In this work, we show the network Rademacher complexity is bounded by a function related to the dropout rate vectors and the weight coefficient matrices. Subsequently, we impose this bound as a regularizer and provide a theoretical justified way to trade-off between model complexity and representation power. Therefore, the dropout rates and the empirical loss are unified into the same objective function, which is then optimized using the block coordinate descent algorithm. We discover that the adaptively adjusted dropout rates converge to some interesting distributions that reveal meaningful patterns.Experiments on the task of image and document classification also show our method achieves better performance compared to the state-of the-art dropout algorithms.", "pdf": "/pdf/a4b4dcae4dcddfaf2bee5828f6d8452898c52821.pdf", "TL;DR": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.", "paperhash": "zhai|adaptive_dropout_with_rademacher_complexity_regularization", "_bibtex": "@inproceedings{\nzhai2018adaptive,\ntitle={Adaptive Dropout with Rademacher Complexity Regularization},\nauthor={Ke Zhai and Huan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=S1uxsye0Z},\n}", "keywords": ["model complexity", "regularization", "deep learning", "model generalization", "adaptive dropout"], "authors": ["Ke Zhai", "Huan Wang"], "authorids": ["zhaikedavy@gmail.com", "joyousprince@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825737612, "id": "ICLR.cc/2018/Conference/-/Paper204/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "S1uxsye0Z", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper204/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper204/Authors|ICLR.cc/2018/Conference/Paper204/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper204/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper204/Authors|ICLR.cc/2018/Conference/Paper204/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper204/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper204/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper204/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper204/Reviewers", "ICLR.cc/2018/Conference/Paper204/Authors", "ICLR.cc/2018/Conference/Paper204/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825737612}}}, {"tddate": null, "ddate": null, "tmdate": 1516821822599, "tcdate": 1516821822599, "number": 9, "cdate": 1516821822599, "id": "r1wH68UHz", "invitation": "ICLR.cc/2018/Conference/-/Paper204/Official_Comment", "forum": "S1uxsye0Z", "replyto": "BksvXe8rG", "signatures": ["ICLR.cc/2018/Conference/Paper204/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper204/Authors"], "content": {"title": "we will include a reference to David's work on the PAC-Bayes bound", "comment": "Thanks so much for pointing out this fantastic work. Much appreciated. We apologize that we were not aware of the work by David McAllister before. We believe the work is definitely related to ours. We will add a reference in our next revision.\n\nThe work by David provides a bound on the expected loss from a nice but different point of view.  Some differences from our work:\n1. The PAC-Bayesian bound assumes a distribution over the hypothesis class. The Rademacher bound we proved does not make that assumption. \n2. The bound David proved assumes one universal dropout rate so \\alpha in their paper is a scalar. To tune the retain rate for each individual neuron, the retain probabilities we used in our bound are vectors. That is, we assume different neurons may have different dropout rates.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Dropout with Rademacher Complexity Regularization", "abstract": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art deep learning algorithms impose dropout strategy to prevent feature co-adaptation. However, choosing the dropout rates remains an art of heuristics or relies on empirical grid-search over some hyperparameter space. In this work, we show the network Rademacher complexity is bounded by a function related to the dropout rate vectors and the weight coefficient matrices. Subsequently, we impose this bound as a regularizer and provide a theoretical justified way to trade-off between model complexity and representation power. Therefore, the dropout rates and the empirical loss are unified into the same objective function, which is then optimized using the block coordinate descent algorithm. We discover that the adaptively adjusted dropout rates converge to some interesting distributions that reveal meaningful patterns.Experiments on the task of image and document classification also show our method achieves better performance compared to the state-of the-art dropout algorithms.", "pdf": "/pdf/a4b4dcae4dcddfaf2bee5828f6d8452898c52821.pdf", "TL;DR": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.", "paperhash": "zhai|adaptive_dropout_with_rademacher_complexity_regularization", "_bibtex": "@inproceedings{\nzhai2018adaptive,\ntitle={Adaptive Dropout with Rademacher Complexity Regularization},\nauthor={Ke Zhai and Huan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=S1uxsye0Z},\n}", "keywords": ["model complexity", "regularization", "deep learning", "model generalization", "adaptive dropout"], "authors": ["Ke Zhai", "Huan Wang"], "authorids": ["zhaikedavy@gmail.com", "joyousprince@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825737612, "id": "ICLR.cc/2018/Conference/-/Paper204/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "S1uxsye0Z", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper204/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper204/Authors|ICLR.cc/2018/Conference/Paper204/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper204/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper204/Authors|ICLR.cc/2018/Conference/Paper204/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper204/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper204/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper204/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper204/Reviewers", "ICLR.cc/2018/Conference/Paper204/Authors", "ICLR.cc/2018/Conference/Paper204/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825737612}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1516804705445, "tcdate": 1511901864415, "number": 3, "cdate": 1511901864415, "id": "Syx39Bsgf", "invitation": "ICLR.cc/2018/Conference/-/Paper204/Official_Review", "forum": "S1uxsye0Z", "replyto": "S1uxsye0Z", "signatures": ["ICLR.cc/2018/Conference/Paper204/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "does not actual say why using Rademacher as a regularizer is theoretically justified, and why the loose bound is reasonable", "rating": "6: Marginally above acceptance threshold", "review": "==Main comments\n\nThe authors connect dropout parameters to a bound of the Rademacher complexity (Rad) of the network. While it is great to see deep learning techniques inspired by learning theory, I think the paper makes too many leaps and the Rad story is ultimately unconvincing.  Perhaps it is better to start with the resulting regularizer, and the interesting direct optimization of dropout parameters. In its current form, the following leaps problematic and were not addressed in the paper:\n\n1) Why is is adding Rad as a regularizer reasonable? Rad is usually hard to compute, and most useful for bounding the generalization error. It would be interesting if it also turns out to be a good regularizer, but the authors do not say why nor cite anything. Like the VC dimension, Rad itself depends on the model class, and cannot be directly optimized. Even if you can somehow optimize over the model class, these quantities give very loose bounds, and do not equal to generalization error. For example, I feel even just adding the actual generalization error bound is more natural. Would it make sense to just add Rad to the objective in this way for a linear model?\n\n2) Why is it reasonable to go from a regularizer based on RC to a loose bound of Rad? The actual resulting regularizer turns out to be a weight penalty but this seems to be a rather loose bound that might not have too much to do with Rad anymore. There should be some analysis on how loose this bound is, and if this looseness matter at all. \n\nThe empirical results themselves seem reasonable, but the results are not actually better than simpler methods in the corresponding tasks, the interpretation is less confident. Afterall, it seems that the proposed method had several parameters that were turned, where the analogous parameters are not present in the competing methods. And the per unit dropout rates are themselves additional parameters, but are they actually good use of parameters?\n\n==Minor comments\n\nThe optimization is perhaps also not quite right, since this requires taking the gradient of the dropout parameter in the original objective. While the authors point out that one can use the mean, but that is more problematic for the gradient than for normal forward predictions. The gradient used for regular learning is not based on the mean prediction, but rather the samples.\n\ntiny columns surrounding figures are ugly and hard to read\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Adaptive Dropout with Rademacher Complexity Regularization", "abstract": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art deep learning algorithms impose dropout strategy to prevent feature co-adaptation. However, choosing the dropout rates remains an art of heuristics or relies on empirical grid-search over some hyperparameter space. In this work, we show the network Rademacher complexity is bounded by a function related to the dropout rate vectors and the weight coefficient matrices. Subsequently, we impose this bound as a regularizer and provide a theoretical justified way to trade-off between model complexity and representation power. Therefore, the dropout rates and the empirical loss are unified into the same objective function, which is then optimized using the block coordinate descent algorithm. We discover that the adaptively adjusted dropout rates converge to some interesting distributions that reveal meaningful patterns.Experiments on the task of image and document classification also show our method achieves better performance compared to the state-of the-art dropout algorithms.", "pdf": "/pdf/a4b4dcae4dcddfaf2bee5828f6d8452898c52821.pdf", "TL;DR": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.", "paperhash": "zhai|adaptive_dropout_with_rademacher_complexity_regularization", "_bibtex": "@inproceedings{\nzhai2018adaptive,\ntitle={Adaptive Dropout with Rademacher Complexity Regularization},\nauthor={Ke Zhai and Huan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=S1uxsye0Z},\n}", "keywords": ["model complexity", "regularization", "deep learning", "model generalization", "adaptive dropout"], "authors": ["Ke Zhai", "Huan Wang"], "authorids": ["zhaikedavy@gmail.com", "joyousprince@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642408510, "id": "ICLR.cc/2018/Conference/-/Paper204/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper204/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper204/AnonReviewer3", "ICLR.cc/2018/Conference/Paper204/AnonReviewer1", "ICLR.cc/2018/Conference/Paper204/AnonReviewer2"], "reply": {"forum": "S1uxsye0Z", "replyto": "S1uxsye0Z", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper204/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642408510}}}, {"tddate": null, "ddate": null, "tmdate": 1516794723157, "tcdate": 1516794723157, "number": 8, "cdate": 1516794723157, "id": "BksvXe8rG", "invitation": "ICLR.cc/2018/Conference/-/Paper204/Official_Comment", "forum": "S1uxsye0Z", "replyto": "S1uxsye0Z", "signatures": ["ICLR.cc/2018/Conference/Paper204/Area_Chair"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper204/Area_Chair"], "content": {"title": "PAC-Bayes prior work", "comment": "Are the authors aware of the work by David McAllister using PAC-Bayes bounds to analyze dropout?  Last I saw, it was not mentioned in the paper.  IT seems like important related work.  Could the authors, very quickly (!), comment as to the relationship and explain what, if any, changes they would make to address this gap in related work?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Dropout with Rademacher Complexity Regularization", "abstract": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art deep learning algorithms impose dropout strategy to prevent feature co-adaptation. However, choosing the dropout rates remains an art of heuristics or relies on empirical grid-search over some hyperparameter space. In this work, we show the network Rademacher complexity is bounded by a function related to the dropout rate vectors and the weight coefficient matrices. Subsequently, we impose this bound as a regularizer and provide a theoretical justified way to trade-off between model complexity and representation power. Therefore, the dropout rates and the empirical loss are unified into the same objective function, which is then optimized using the block coordinate descent algorithm. We discover that the adaptively adjusted dropout rates converge to some interesting distributions that reveal meaningful patterns.Experiments on the task of image and document classification also show our method achieves better performance compared to the state-of the-art dropout algorithms.", "pdf": "/pdf/a4b4dcae4dcddfaf2bee5828f6d8452898c52821.pdf", "TL;DR": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.", "paperhash": "zhai|adaptive_dropout_with_rademacher_complexity_regularization", "_bibtex": "@inproceedings{\nzhai2018adaptive,\ntitle={Adaptive Dropout with Rademacher Complexity Regularization},\nauthor={Ke Zhai and Huan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=S1uxsye0Z},\n}", "keywords": ["model complexity", "regularization", "deep learning", "model generalization", "adaptive dropout"], "authors": ["Ke Zhai", "Huan Wang"], "authorids": ["zhaikedavy@gmail.com", "joyousprince@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825737612, "id": "ICLR.cc/2018/Conference/-/Paper204/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "S1uxsye0Z", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper204/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper204/Authors|ICLR.cc/2018/Conference/Paper204/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper204/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper204/Authors|ICLR.cc/2018/Conference/Paper204/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper204/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper204/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper204/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper204/Reviewers", "ICLR.cc/2018/Conference/Paper204/Authors", "ICLR.cc/2018/Conference/Paper204/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825737612}}}, {"tddate": null, "ddate": null, "tmdate": 1516779528392, "tcdate": 1516779528392, "number": 7, "cdate": 1516779528392, "id": "BJefd2BBf", "invitation": "ICLR.cc/2018/Conference/-/Paper204/Official_Comment", "forum": "S1uxsye0Z", "replyto": "HJHht9rHM", "signatures": ["ICLR.cc/2018/Conference/Paper204/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper204/Authors"], "content": {"title": "more explanations on the concerns by Reviewer #2", "comment": "Thanks very much for the comments. We would like to clarify some misunderstandings:\n\nQ: for L1, Rad is the infinity norm, which is not the one you wanted.\nA: section 6.3, for L1, Rad contains the 1 norm (B_1). The L_infty norm is on the samples, just like the max norm shown in our bound. Note here B1 instead of L_infty is the bound on the model parameters.\n\nQ: Rad only depends on the hypothesis class and not on how much data your have and properties of the data\nA: This is wrong. Not only does Rademacher complexity depend on the hypothesis class, but it also depends on the sample distribution because it is taking the expectation of the empirical Rademacher complexity over all samples of size n.\nOn the other hand, note what we proved is the upper bound for the EMPIRICAL Rademacher complexity rather than the Rademacher complexity itself. That is why it has the dependency on the sample size. \nBy measure concentration the EMPIRICAL Rademacher complexity is used to bound the Rademacher complexity.\n\nQ: the hypothesis is now: bounds of Rad makes good regularizers, instead of Rademacher makes good regularizers.\nA:  As stated in section 6.3, the regularizer used in ridge regression as well as Lasso can be interpreted as terms related to the upper bound of the empirical Rademacher complexity. Rademacher itself may make a good regularizer, but it is simply too complex to optimize and evaluate. That\u2019s why we used the upper bound instead. \nSimilar ways of approximation have also been used in the numeric optimization community, where if the objective is too hard to optimize, one may choose to optimize its convex envelop instead. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Dropout with Rademacher Complexity Regularization", "abstract": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art deep learning algorithms impose dropout strategy to prevent feature co-adaptation. However, choosing the dropout rates remains an art of heuristics or relies on empirical grid-search over some hyperparameter space. In this work, we show the network Rademacher complexity is bounded by a function related to the dropout rate vectors and the weight coefficient matrices. Subsequently, we impose this bound as a regularizer and provide a theoretical justified way to trade-off between model complexity and representation power. Therefore, the dropout rates and the empirical loss are unified into the same objective function, which is then optimized using the block coordinate descent algorithm. We discover that the adaptively adjusted dropout rates converge to some interesting distributions that reveal meaningful patterns.Experiments on the task of image and document classification also show our method achieves better performance compared to the state-of the-art dropout algorithms.", "pdf": "/pdf/a4b4dcae4dcddfaf2bee5828f6d8452898c52821.pdf", "TL;DR": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.", "paperhash": "zhai|adaptive_dropout_with_rademacher_complexity_regularization", "_bibtex": "@inproceedings{\nzhai2018adaptive,\ntitle={Adaptive Dropout with Rademacher Complexity Regularization},\nauthor={Ke Zhai and Huan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=S1uxsye0Z},\n}", "keywords": ["model complexity", "regularization", "deep learning", "model generalization", "adaptive dropout"], "authors": ["Ke Zhai", "Huan Wang"], "authorids": ["zhaikedavy@gmail.com", "joyousprince@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825737612, "id": "ICLR.cc/2018/Conference/-/Paper204/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "S1uxsye0Z", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper204/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper204/Authors|ICLR.cc/2018/Conference/Paper204/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper204/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper204/Authors|ICLR.cc/2018/Conference/Paper204/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper204/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper204/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper204/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper204/Reviewers", "ICLR.cc/2018/Conference/Paper204/Authors", "ICLR.cc/2018/Conference/Paper204/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825737612}}}, {"tddate": null, "ddate": null, "tmdate": 1516771757308, "tcdate": 1516771757308, "number": 6, "cdate": 1516771757308, "id": "HJHht9rHM", "invitation": "ICLR.cc/2018/Conference/-/Paper204/Official_Comment", "forum": "S1uxsye0Z", "replyto": "S18-Tp-zz", "signatures": ["ICLR.cc/2018/Conference/Paper204/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper204/AnonReviewer2"], "content": {"title": "explain/motivate convincingly why adding Rademacher complexity itself is reasonable", "comment": "Not motivating why adding Rademacher itself is in my opinion the biggest weakness in the paper. \nWhile section 6.3 is a good step (though I think it belong to the main paper given the story), it still seems inadequate. For one, the hypothesis is now: bounds of Rad makes good regularizers, instead of Rademacher makes good regularizers. Secondly, for L1, Rad is the infinity norm, which is not the one you wanted.\nLastly, Rad only depends on the hypothesis class and not on how much data your have and properties of the data, which are clearly important for picking regularizers in practice (or their strength through cross validation), which suggests it might not be justifiable.\n\nI still think the paper is borderline and weak reject unless this issue can be addressed convincingly."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Dropout with Rademacher Complexity Regularization", "abstract": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art deep learning algorithms impose dropout strategy to prevent feature co-adaptation. However, choosing the dropout rates remains an art of heuristics or relies on empirical grid-search over some hyperparameter space. In this work, we show the network Rademacher complexity is bounded by a function related to the dropout rate vectors and the weight coefficient matrices. Subsequently, we impose this bound as a regularizer and provide a theoretical justified way to trade-off between model complexity and representation power. Therefore, the dropout rates and the empirical loss are unified into the same objective function, which is then optimized using the block coordinate descent algorithm. We discover that the adaptively adjusted dropout rates converge to some interesting distributions that reveal meaningful patterns.Experiments on the task of image and document classification also show our method achieves better performance compared to the state-of the-art dropout algorithms.", "pdf": "/pdf/a4b4dcae4dcddfaf2bee5828f6d8452898c52821.pdf", "TL;DR": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.", "paperhash": "zhai|adaptive_dropout_with_rademacher_complexity_regularization", "_bibtex": "@inproceedings{\nzhai2018adaptive,\ntitle={Adaptive Dropout with Rademacher Complexity Regularization},\nauthor={Ke Zhai and Huan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=S1uxsye0Z},\n}", "keywords": ["model complexity", "regularization", "deep learning", "model generalization", "adaptive dropout"], "authors": ["Ke Zhai", "Huan Wang"], "authorids": ["zhaikedavy@gmail.com", "joyousprince@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825737612, "id": "ICLR.cc/2018/Conference/-/Paper204/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "S1uxsye0Z", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper204/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper204/Authors|ICLR.cc/2018/Conference/Paper204/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper204/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper204/Authors|ICLR.cc/2018/Conference/Paper204/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper204/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper204/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper204/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper204/Reviewers", "ICLR.cc/2018/Conference/Paper204/Authors", "ICLR.cc/2018/Conference/Paper204/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825737612}}}, {"tddate": null, "ddate": null, "tmdate": 1515785200985, "tcdate": 1515785200985, "number": 5, "cdate": 1515785200985, "id": "rJYg3KLEf", "invitation": "ICLR.cc/2018/Conference/-/Paper204/Official_Comment", "forum": "S1uxsye0Z", "replyto": "Bk15lpF1G", "signatures": ["ICLR.cc/2018/Conference/Paper204/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper204/Authors"], "content": {"title": "About the questions on the rebuttal raised by Reviewer #3:", "comment": "Thanks very much for your careful examination. We do appreciate it.\n(1) If you look at the final Rademacher complexity bound we are proving, it has no absolute value inside the supremum. The contraction lemma is applied to the Rademacher complexity without absolute value. That is why equation (7) comes after the contraction. We understand this is confusing. We will make it clear in the next version.\n(2) As you mentioned, if we take expectation with respect to r, then f^L is not a function of r any more. Actually in our definition, the final prediction function f^L is a deterministic function (since we take the expectation w.r.t. r)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Dropout with Rademacher Complexity Regularization", "abstract": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art deep learning algorithms impose dropout strategy to prevent feature co-adaptation. However, choosing the dropout rates remains an art of heuristics or relies on empirical grid-search over some hyperparameter space. In this work, we show the network Rademacher complexity is bounded by a function related to the dropout rate vectors and the weight coefficient matrices. Subsequently, we impose this bound as a regularizer and provide a theoretical justified way to trade-off between model complexity and representation power. Therefore, the dropout rates and the empirical loss are unified into the same objective function, which is then optimized using the block coordinate descent algorithm. We discover that the adaptively adjusted dropout rates converge to some interesting distributions that reveal meaningful patterns.Experiments on the task of image and document classification also show our method achieves better performance compared to the state-of the-art dropout algorithms.", "pdf": "/pdf/a4b4dcae4dcddfaf2bee5828f6d8452898c52821.pdf", "TL;DR": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.", "paperhash": "zhai|adaptive_dropout_with_rademacher_complexity_regularization", "_bibtex": "@inproceedings{\nzhai2018adaptive,\ntitle={Adaptive Dropout with Rademacher Complexity Regularization},\nauthor={Ke Zhai and Huan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=S1uxsye0Z},\n}", "keywords": ["model complexity", "regularization", "deep learning", "model generalization", "adaptive dropout"], "authors": ["Ke Zhai", "Huan Wang"], "authorids": ["zhaikedavy@gmail.com", "joyousprince@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825737612, "id": "ICLR.cc/2018/Conference/-/Paper204/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "S1uxsye0Z", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper204/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper204/Authors|ICLR.cc/2018/Conference/Paper204/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper204/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper204/Authors|ICLR.cc/2018/Conference/Paper204/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper204/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper204/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper204/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper204/Reviewers", "ICLR.cc/2018/Conference/Paper204/Authors", "ICLR.cc/2018/Conference/Paper204/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825737612}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642408576, "tcdate": 1511805775704, "number": 2, "cdate": 1511805775704, "id": "HkOUXRFlz", "invitation": "ICLR.cc/2018/Conference/-/Paper204/Official_Review", "forum": "S1uxsye0Z", "replyto": "S1uxsye0Z", "signatures": ["ICLR.cc/2018/Conference/Paper204/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "This is an important piece of work that relates complexity of networks' learnability to dropout rates in backpropagation. This paper answers some critical questions about dropout learning.", "rating": "7: Good paper, accept", "review": "An important contribution. The paper is well written. Some questions that needs to be better answered are listed here.\n1. The theorem is difficult to decipher. Some remarks needs to be included explaining the terms on the right and what they mean with respect to learnability or complexity. \n2. How does the regularization term in eq (2) relate to the existing (currently used) norm based regularizers in deep network learning? It may be straight forward but some small simulation/plots explaining this is important. \n3. Apart from the accuracy results, the change in computational time for working with eq (2), rather than using existing state-of-the-art deep network optimization needs to be reported? How does this change vary with respect to dataset and network size (beyond the description of scaled regularization in section 4)?\n4. Confidence intervals needs to be computed for the retain-rates (reported as a function of epoch). This is critical both to evaluate the stability of regularizers as well as whether the bound from theorem is strong. \n5. Did the evaluations show some patterns on the retain rates across different layers? It seems from Figure 3,4 that retain rates in lower layers are more closer to 1 and they decrease to 0.5 as depth increases. Is this a general pattern? \n6. It has been long known that dropout relates to non-negative weighted averaging of partially learned neural networks and dropout rate of 0.5 provides best dymanics. The evaluations say that clearly 0.5 for all units/layers us not correct. What does this mean in terms of network architecture? Is it that some layers are easy to average (nothing is learned there, so dropped networks have small variance), while some other layers are sensitive? \n7. What are some simple guidelines for choosing the values of p and q? Again it appears p=q=2 is the best, but need confidence intervals here to say anything substantial. ", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Dropout with Rademacher Complexity Regularization", "abstract": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art deep learning algorithms impose dropout strategy to prevent feature co-adaptation. However, choosing the dropout rates remains an art of heuristics or relies on empirical grid-search over some hyperparameter space. In this work, we show the network Rademacher complexity is bounded by a function related to the dropout rate vectors and the weight coefficient matrices. Subsequently, we impose this bound as a regularizer and provide a theoretical justified way to trade-off between model complexity and representation power. Therefore, the dropout rates and the empirical loss are unified into the same objective function, which is then optimized using the block coordinate descent algorithm. We discover that the adaptively adjusted dropout rates converge to some interesting distributions that reveal meaningful patterns.Experiments on the task of image and document classification also show our method achieves better performance compared to the state-of the-art dropout algorithms.", "pdf": "/pdf/a4b4dcae4dcddfaf2bee5828f6d8452898c52821.pdf", "TL;DR": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.", "paperhash": "zhai|adaptive_dropout_with_rademacher_complexity_regularization", "_bibtex": "@inproceedings{\nzhai2018adaptive,\ntitle={Adaptive Dropout with Rademacher Complexity Regularization},\nauthor={Ke Zhai and Huan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=S1uxsye0Z},\n}", "keywords": ["model complexity", "regularization", "deep learning", "model generalization", "adaptive dropout"], "authors": ["Ke Zhai", "Huan Wang"], "authorids": ["zhaikedavy@gmail.com", "joyousprince@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642408510, "id": "ICLR.cc/2018/Conference/-/Paper204/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper204/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper204/AnonReviewer3", "ICLR.cc/2018/Conference/Paper204/AnonReviewer1", "ICLR.cc/2018/Conference/Paper204/AnonReviewer2"], "reply": {"forum": "S1uxsye0Z", "replyto": "S1uxsye0Z", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper204/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642408510}}}, {"tddate": null, "ddate": null, "tmdate": 1515113734077, "tcdate": 1515113734077, "number": 4, "cdate": 1515113734077, "id": "S1CZ6BnQM", "invitation": "ICLR.cc/2018/Conference/-/Paper204/Official_Comment", "forum": "S1uxsye0Z", "replyto": "S1uxsye0Z", "signatures": ["ICLR.cc/2018/Conference/Paper204/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper204/Authors"], "content": {"title": "We have posted a revision of the draft", "comment": "Update list:\n\n1. added a subsection 6.5 to the appendix to empirically demonstrate the relations between the stochastic objective and the deterministic approximation. (minor comments from Reviewer#2)\n2. added a subsection 6.6 to the appendix to empirically show the stability of the dropout rate convergence. (suggestion 4 from Reviewer #1)\n3. added one paragraph of descriptions of the terms used in our bound following the theorem 3.1 as suggested by Reviewer #1 (Q1). \n4. added two cases when our bounds are tight. (the last paragraph in section 3.1) This responds to the second concern raised by Review #2.\n5. added the definition of the empirical Rademacher complexity to section 3.1 as suggested by Reviwer #3.\n6. added a paragraph about the different notations used for vectors and scalars in subsection 6.1. (second paragraph of the proof) This is to respond the questions 2 and 3 raised by Reviewer #3.\n7. fixed all the typos pointed out by Reviewer #3. (comment 1 and 2 from Reviewer #3)\n8. added references of pages and chapters suggested by Reviewer #3.\n9. fixed the tiny column issues mentioned by Review #2 \n10. for the first concern raised by Review #2, we suggest reading section 6.3 in our appendix.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Dropout with Rademacher Complexity Regularization", "abstract": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art deep learning algorithms impose dropout strategy to prevent feature co-adaptation. However, choosing the dropout rates remains an art of heuristics or relies on empirical grid-search over some hyperparameter space. In this work, we show the network Rademacher complexity is bounded by a function related to the dropout rate vectors and the weight coefficient matrices. Subsequently, we impose this bound as a regularizer and provide a theoretical justified way to trade-off between model complexity and representation power. Therefore, the dropout rates and the empirical loss are unified into the same objective function, which is then optimized using the block coordinate descent algorithm. We discover that the adaptively adjusted dropout rates converge to some interesting distributions that reveal meaningful patterns.Experiments on the task of image and document classification also show our method achieves better performance compared to the state-of the-art dropout algorithms.", "pdf": "/pdf/a4b4dcae4dcddfaf2bee5828f6d8452898c52821.pdf", "TL;DR": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.", "paperhash": "zhai|adaptive_dropout_with_rademacher_complexity_regularization", "_bibtex": "@inproceedings{\nzhai2018adaptive,\ntitle={Adaptive Dropout with Rademacher Complexity Regularization},\nauthor={Ke Zhai and Huan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=S1uxsye0Z},\n}", "keywords": ["model complexity", "regularization", "deep learning", "model generalization", "adaptive dropout"], "authors": ["Ke Zhai", "Huan Wang"], "authorids": ["zhaikedavy@gmail.com", "joyousprince@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825737612, "id": "ICLR.cc/2018/Conference/-/Paper204/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "S1uxsye0Z", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper204/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper204/Authors|ICLR.cc/2018/Conference/Paper204/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper204/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper204/Authors|ICLR.cc/2018/Conference/Paper204/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper204/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper204/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper204/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper204/Reviewers", "ICLR.cc/2018/Conference/Paper204/Authors", "ICLR.cc/2018/Conference/Paper204/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825737612}}}, {"tddate": null, "ddate": null, "tmdate": 1513377943276, "tcdate": 1513377431289, "number": 2, "cdate": 1513377431289, "id": "ry1sRp-ff", "invitation": "ICLR.cc/2018/Conference/-/Paper204/Official_Comment", "forum": "S1uxsye0Z", "replyto": "HkOUXRFlz", "signatures": ["ICLR.cc/2018/Conference/Paper204/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper204/Authors"], "content": {"title": "some thoughts about the questions raised by reviewer #1", "comment": "Thanks very much for your encouraging comments and helpful suggestions. \n\n1. The upper bound suggests that layers affect the complexity in a multiplicative way. An extreme case as we described in the last paragraph of section 3.2 is, if the dropout retain rates for one layer are all zeros, then the empirical Rademacher complexity for the whole network is zero since the network is doing random guess for predictions. In this case the bound is tight. We will put more descriptions about the terms in our bound.\n\n2. This is an interesting suggestion. Norm based regularizers currently used are imposed on the weights of each layer without considering the retain rates and the regularization is done on each layer independently. We suggest organizing them in a systematic way. \n\n3. In terms of running time, the proposed framework takes one additional backpropagation compared to its standard deep network counterpart. In practice, we find the running time per epoch after introducing the regularizer is approximately 1.6 to 1.9 times that of the current standard deep network. \n\n4: Thanks for the great suggestions.  There are some potential issues with drawing the confidence intervals for the retain rate of a particular neuron. For example, permuting the neurons does not change the network structure but it may lead to some identifiability issues. Instead to demo the stability of the algorithm we may add a plot showing the histograms of the theta with different initializations.  \n\n5. This is an excellent question! In fact, we are conducting additional evaluations to verify this pattern. We had some preliminary empirical observations that, as the layer goes higher, fewer neurons get high retain rates . This is somewhat consistent with the fact that people tend to set the number of neurons smaller for higher layers. We still need more experiments to tell if this is a general pattern.\n\n6. This is another great question. It is also related to an on-going follow-up work we are currently investigating as stated in the conclusion and future work section of our paper. If we use the setting of p=\\infty and q=1, the L1 norm regularizer may produce sparse retain rates. Subsequently, we could prune the corresponding neuron. Therefore we could use the algorithm as a way to determine the number of neurons used in hidden layers, i.e., we can use the regularizer to tune the network architecture. Similarly, if we use p=1 and q=\\infty, then we can expect sparse coefficients on W due to the property of the L1 norm, in this way the regularizer can also be used to prune the internal neural connections. \n\n7. Currently we do not have any theory for choosing p and q. As we stated above, one way is to choose p and q based on the sparsity desire. If we would like to impose sparsity on the number of neurons to fire,we may set q=1 to promote sparse retain rates. On the other hand, if we would like to impose sparsity on the number of internal connections, i.e., have a sparse coefficient matrix W, we may set p=1 instead."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Dropout with Rademacher Complexity Regularization", "abstract": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art deep learning algorithms impose dropout strategy to prevent feature co-adaptation. However, choosing the dropout rates remains an art of heuristics or relies on empirical grid-search over some hyperparameter space. In this work, we show the network Rademacher complexity is bounded by a function related to the dropout rate vectors and the weight coefficient matrices. Subsequently, we impose this bound as a regularizer and provide a theoretical justified way to trade-off between model complexity and representation power. Therefore, the dropout rates and the empirical loss are unified into the same objective function, which is then optimized using the block coordinate descent algorithm. We discover that the adaptively adjusted dropout rates converge to some interesting distributions that reveal meaningful patterns.Experiments on the task of image and document classification also show our method achieves better performance compared to the state-of the-art dropout algorithms.", "pdf": "/pdf/a4b4dcae4dcddfaf2bee5828f6d8452898c52821.pdf", "TL;DR": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.", "paperhash": "zhai|adaptive_dropout_with_rademacher_complexity_regularization", "_bibtex": "@inproceedings{\nzhai2018adaptive,\ntitle={Adaptive Dropout with Rademacher Complexity Regularization},\nauthor={Ke Zhai and Huan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=S1uxsye0Z},\n}", "keywords": ["model complexity", "regularization", "deep learning", "model generalization", "adaptive dropout"], "authors": ["Ke Zhai", "Huan Wang"], "authorids": ["zhaikedavy@gmail.com", "joyousprince@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825737612, "id": "ICLR.cc/2018/Conference/-/Paper204/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "S1uxsye0Z", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper204/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper204/Authors|ICLR.cc/2018/Conference/Paper204/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper204/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper204/Authors|ICLR.cc/2018/Conference/Paper204/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper204/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper204/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper204/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper204/Reviewers", "ICLR.cc/2018/Conference/Paper204/Authors", "ICLR.cc/2018/Conference/Paper204/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825737612}}}, {"tddate": null, "ddate": null, "tmdate": 1513377798818, "tcdate": 1513377798818, "number": 3, "cdate": 1513377798818, "id": "SJkzxRWzz", "invitation": "ICLR.cc/2018/Conference/-/Paper204/Official_Comment", "forum": "S1uxsye0Z", "replyto": "Bk15lpF1G", "signatures": ["ICLR.cc/2018/Conference/Paper204/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper204/Authors"], "content": {"title": "some explanations on the questions raised by reviewer #3", "comment": "Thanks very much for your review and comments. \n\nAbout your major comments \n\n(1):\nThanks for your suggestion, we will include the definition of empirical Rademacher complexity in our revision.\n\n(2) and (3):\nAs we stated in the first paragraph of our proof 6.1, we treat the functions fed into the neurons of the l-th layer as one class of functions. Therefore, f^L(x;W) is a vector as you correctly pointed out, but f^L(x;w) is a scalar. So each dimension of f^L(x;W) is viewed as one instance coming from the same function class f^L(x;w). Similar ways of proof have been adopted in Wan et al. (2013). We are sorry about the confusion. We will add more descriptions about it to make that clear in our revision. \n\n(4)\nIt is a good question. The dependency on the number of classes comes from the contraction lemma. However, what we proved is only a weak bound on the Rademacher complexity. We are still working on further tightening the bound. For now, we are not sure if we can reduce the dependency on the number of classes to sub-linear. We hope this work will also open additional research directions and future extensions to the community. You are always welcome to add to it.\n\nAbout your minor comments:\n\n(1) (2) Thanks for the careful examination. We will fix the typos in the next version.\n\n(3) Thanks for the comments. \nContraction lemma (Shalev-Shwartz & Ben-David, 2014) is a variant of the Lemma 26.9 located on page 381, Chapter 26.\nLemma 26.11 in Shalev-Shwartz & Ben-David (2014) is located on page 383, Chapter 26.2. \nWe will add the chapters and pages to the proof. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Dropout with Rademacher Complexity Regularization", "abstract": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art deep learning algorithms impose dropout strategy to prevent feature co-adaptation. However, choosing the dropout rates remains an art of heuristics or relies on empirical grid-search over some hyperparameter space. In this work, we show the network Rademacher complexity is bounded by a function related to the dropout rate vectors and the weight coefficient matrices. Subsequently, we impose this bound as a regularizer and provide a theoretical justified way to trade-off between model complexity and representation power. Therefore, the dropout rates and the empirical loss are unified into the same objective function, which is then optimized using the block coordinate descent algorithm. We discover that the adaptively adjusted dropout rates converge to some interesting distributions that reveal meaningful patterns.Experiments on the task of image and document classification also show our method achieves better performance compared to the state-of the-art dropout algorithms.", "pdf": "/pdf/a4b4dcae4dcddfaf2bee5828f6d8452898c52821.pdf", "TL;DR": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.", "paperhash": "zhai|adaptive_dropout_with_rademacher_complexity_regularization", "_bibtex": "@inproceedings{\nzhai2018adaptive,\ntitle={Adaptive Dropout with Rademacher Complexity Regularization},\nauthor={Ke Zhai and Huan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=S1uxsye0Z},\n}", "keywords": ["model complexity", "regularization", "deep learning", "model generalization", "adaptive dropout"], "authors": ["Ke Zhai", "Huan Wang"], "authorids": ["zhaikedavy@gmail.com", "joyousprince@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825737612, "id": "ICLR.cc/2018/Conference/-/Paper204/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "S1uxsye0Z", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper204/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper204/Authors|ICLR.cc/2018/Conference/Paper204/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper204/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper204/Authors|ICLR.cc/2018/Conference/Paper204/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper204/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper204/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper204/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper204/Reviewers", "ICLR.cc/2018/Conference/Paper204/Authors", "ICLR.cc/2018/Conference/Paper204/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825737612}}}, {"tddate": null, "ddate": null, "tmdate": 1513377022230, "tcdate": 1513377022230, "number": 1, "cdate": 1513377022230, "id": "S18-Tp-zz", "invitation": "ICLR.cc/2018/Conference/-/Paper204/Official_Comment", "forum": "S1uxsye0Z", "replyto": "Syx39Bsgf", "signatures": ["ICLR.cc/2018/Conference/Paper204/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper204/Authors"], "content": {"title": "some comments about the questions raised by reviewer#2", "comment": "Thanks very much for your valuable comments and helpful suggestions. \n\nQ: Why adding Rad as a regularizer reasonable? Why is it reasonable to go from a regularizer based on RC to a loose bound of Rad?\nA: These are great questions. We agree we do not have a rigorous way to prove adding an approximate upper bound to the objective can lead to any theoretical guarantee as you correctly pointed out.  The theorem of the upper bound in the paper is rigorous but why adding the upper bound to the objective can help is heuristic and empirical.\n\nOn the other hand, adding an approximate term that is related to the upper bound of the Rademacher complexity is not something new. For example, the squared L2 norm regularizer used in the ridge regression, though there are explanations such as Bayesian priors, can be interpreted as a term related to the upper bound of the Rademacher complexity of linear classes . People are already using it. Similarly, the L1 regularizer used in LASSO can also be interpreted as a term related to the Rademacher complexity bound. We have put a section in the Appendix (Section 6.3) to somewhat justify it in a heuristic way. \n\nQ: The actual resulting regularizer turns out to be\u2026 rather loose bound\u2026\nA: We agree that the bound proved in the paper could be a bit loose. Still in some extreme cases it is tight. For example, as we indicated in the paragraph before Section 3.3, if the retain rates in one layer are all zeros, the model always makes random guess for prediction. In this case the empirical Rademacher complexity is zero and our bound is tight. In general, even if the bound is loose, it still gives some justification on the norms used in today\u2019s neural network regularizations. Additionally, it leads to a systematic way of weighting the norms as well as the retain rates.\n\nMinor comments:\nQ: While the authors point out that one can use the mean, but that is more problematic for the gradient than for normal forward predictions. After all, the gradient used for regular learning is not based on the mean prediction, but rather the samples.\nA: This is an excellent question. As we stated in Section 3.3, \u201cthis is an approximation to the true f^L(x;W, \u03b8)\u201d. Using the mean is purely an approximation used for the sake of optimization efficiency. By design we should use the samples. However empirically we found that optimizing based on the mean (instead of the actual sampling) still leads to a decrease of the objective. We will add additional figures to better illustrate the point in our next revision.\n\nQ: tiny columns surrounding figures are ugly and hard to read\nA: Thanks for the suggestion. We will fix it in our revision.\n\nQ: dropout rate is perhaps more common than retain rate\nA: We use the retain rate instead just to make the upper bound look less messy. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Dropout with Rademacher Complexity Regularization", "abstract": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art deep learning algorithms impose dropout strategy to prevent feature co-adaptation. However, choosing the dropout rates remains an art of heuristics or relies on empirical grid-search over some hyperparameter space. In this work, we show the network Rademacher complexity is bounded by a function related to the dropout rate vectors and the weight coefficient matrices. Subsequently, we impose this bound as a regularizer and provide a theoretical justified way to trade-off between model complexity and representation power. Therefore, the dropout rates and the empirical loss are unified into the same objective function, which is then optimized using the block coordinate descent algorithm. We discover that the adaptively adjusted dropout rates converge to some interesting distributions that reveal meaningful patterns.Experiments on the task of image and document classification also show our method achieves better performance compared to the state-of the-art dropout algorithms.", "pdf": "/pdf/a4b4dcae4dcddfaf2bee5828f6d8452898c52821.pdf", "TL;DR": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.", "paperhash": "zhai|adaptive_dropout_with_rademacher_complexity_regularization", "_bibtex": "@inproceedings{\nzhai2018adaptive,\ntitle={Adaptive Dropout with Rademacher Complexity Regularization},\nauthor={Ke Zhai and Huan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=S1uxsye0Z},\n}", "keywords": ["model complexity", "regularization", "deep learning", "model generalization", "adaptive dropout"], "authors": ["Ke Zhai", "Huan Wang"], "authorids": ["zhaikedavy@gmail.com", "joyousprince@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825737612, "id": "ICLR.cc/2018/Conference/-/Paper204/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "S1uxsye0Z", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper204/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper204/Authors|ICLR.cc/2018/Conference/Paper204/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper204/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper204/Authors|ICLR.cc/2018/Conference/Paper204/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper204/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper204/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper204/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper204/Reviewers", "ICLR.cc/2018/Conference/Paper204/Authors", "ICLR.cc/2018/Conference/Paper204/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825737612}}}], "count": 23}