{"notes": [{"id": "Skep6TVYDB", "original": "r1eB67bdvr", "number": 836, "cdate": 1569439173186, "ddate": null, "tcdate": 1569439173186, "tmdate": 1583912037237, "tddate": null, "forum": "Skep6TVYDB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Gradientless Descent: High-Dimensional Zeroth-Order Optimization", "authors": ["Daniel Golovin", "John Karro", "Greg Kochanski", "Chansoo Lee", "Xingyou Song", "Qiuyi Zhang"], "authorids": ["dgg@google.com", "karro@google.com", "gpk@google.com", "chansoo@google.com", "xingyousong@google.com", "qiuyiz@google.com"], "keywords": ["Zeroth Order Optimization"], "TL;DR": "Gradientless Descent is a provably efficient gradient-free algorithm that is monotone-invariant and fast for high-dimensional zero-th order optimization.", "abstract": "Zeroth-order optimization is the process of minimizing an objective $f(x)$, given oracle access to evaluations at adaptively chosen inputs $x$. In this paper, we present two simple yet powerful GradientLess Descent (GLD) algorithms that do not rely on an underlying gradient estimate and are numerically stable. We analyze our algorithm from a novel geometric perspective and we show that for {\\it any monotone transform} of a smooth and strongly convex objective with latent dimension $k \\ge n$, we present a novel analysis that shows convergence within an $\\epsilon$-ball of the optimum in $O(kQ\\log(n)\\log(R/\\epsilon))$ evaluations, where the input dimension is $n$, $R$ is the diameter of the input space and $Q$ is the condition number. Our rates are the first of its kind to be both 1) poly-logarithmically dependent on dimensionality and 2) invariant under monotone transformations. We further leverage our geometric perspective to show that our analysis is optimal. Both monotone invariance and its ability to utilize a low latent dimensionality are key to the empirical success of our algorithms, as demonstrated on synthetic and MuJoCo benchmarks.\n", "pdf": "/pdf/e6a8deb0be680bca761c33514d9ae4a5b7e42868.pdf", "paperhash": "golovin|gradientless_descent_highdimensional_zerothorder_optimization", "_bibtex": "@inproceedings{\nGolovin2020Gradientless,\ntitle={Gradientless Descent: High-Dimensional Zeroth-Order Optimization},\nauthor={Daniel Golovin and John Karro and Greg Kochanski and Chansoo Lee and Xingyou Song and Qiuyi Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Skep6TVYDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/92231498c15cc2fa7c0b6ee565d5b9561d8b5b83.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "9imcaj1Tnn", "original": null, "number": 1, "cdate": 1576798707421, "ddate": null, "tcdate": 1576798707421, "tmdate": 1576800928922, "tddate": null, "forum": "Skep6TVYDB", "replyto": "Skep6TVYDB", "invitation": "ICLR.cc/2020/Conference/Paper836/-/Decision", "content": {"decision": "Accept (Spotlight)", "comment": "The paper considers an interesting algorithm on zeorth-order optimization and contains strong theory. All the reviewers agree to accept.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradientless Descent: High-Dimensional Zeroth-Order Optimization", "authors": ["Daniel Golovin", "John Karro", "Greg Kochanski", "Chansoo Lee", "Xingyou Song", "Qiuyi Zhang"], "authorids": ["dgg@google.com", "karro@google.com", "gpk@google.com", "chansoo@google.com", "xingyousong@google.com", "qiuyiz@google.com"], "keywords": ["Zeroth Order Optimization"], "TL;DR": "Gradientless Descent is a provably efficient gradient-free algorithm that is monotone-invariant and fast for high-dimensional zero-th order optimization.", "abstract": "Zeroth-order optimization is the process of minimizing an objective $f(x)$, given oracle access to evaluations at adaptively chosen inputs $x$. In this paper, we present two simple yet powerful GradientLess Descent (GLD) algorithms that do not rely on an underlying gradient estimate and are numerically stable. We analyze our algorithm from a novel geometric perspective and we show that for {\\it any monotone transform} of a smooth and strongly convex objective with latent dimension $k \\ge n$, we present a novel analysis that shows convergence within an $\\epsilon$-ball of the optimum in $O(kQ\\log(n)\\log(R/\\epsilon))$ evaluations, where the input dimension is $n$, $R$ is the diameter of the input space and $Q$ is the condition number. Our rates are the first of its kind to be both 1) poly-logarithmically dependent on dimensionality and 2) invariant under monotone transformations. We further leverage our geometric perspective to show that our analysis is optimal. Both monotone invariance and its ability to utilize a low latent dimensionality are key to the empirical success of our algorithms, as demonstrated on synthetic and MuJoCo benchmarks.\n", "pdf": "/pdf/e6a8deb0be680bca761c33514d9ae4a5b7e42868.pdf", "paperhash": "golovin|gradientless_descent_highdimensional_zerothorder_optimization", "_bibtex": "@inproceedings{\nGolovin2020Gradientless,\ntitle={Gradientless Descent: High-Dimensional Zeroth-Order Optimization},\nauthor={Daniel Golovin and John Karro and Greg Kochanski and Chansoo Lee and Xingyou Song and Qiuyi Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Skep6TVYDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/92231498c15cc2fa7c0b6ee565d5b9561d8b5b83.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Skep6TVYDB", "replyto": "Skep6TVYDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795717330, "tmdate": 1576800267611, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper836/-/Decision"}}}, {"id": "B1geUE_Hcr", "original": null, "number": 3, "cdate": 1572336712101, "ddate": null, "tcdate": 1572336712101, "tmdate": 1574421718483, "tddate": null, "forum": "Skep6TVYDB", "replyto": "Skep6TVYDB", "invitation": "ICLR.cc/2020/Conference/Paper836/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "\nUpdate after rebuttal: I found the rebuttal convincing and I liked the fact that concerns regarding empirical justification were addressed. Consequently, I increase my score from \"Weak Reject\" to \"Weak Accept\".\n--------------------------\nThis paper focuses on derivative-free, or zero-th order, optimization. That is the setting where a function may be continuous and/or smooth and/or (quasi)-convex, however, we do not have access to the gradients. As such, it is not possible to apply standard gradient descent methods. The paper proposes an algorithm for \"gradientless\" descend. The basic intuition is that by careful randomly sampling it is quite probable that a lower objective value will be attained. Doing so recursively can then lead to the optimum with high probability. Clearly, in such a setting it is quite important to clarify what is \"careful random sampling\". To this end the paper casts this as sampling from a Gaussian ball of specific radius, chosen such that the samples are with high probality below the current level set (the hyperplance of equivalent solutions f(x) as our current solution f(x_t)). The paper derives and proves various theorems on how to select the optimal radius and how to perform the sampling. Specifically, the case of strongly convex and smooth functions is analyzed, however, the paper also shows how this generalizes to functions after a monotone transformation (thus leading to quasi-convex functions) and with extra error perturbations. The proposed algorithm is compared on a synthetic experiment, and a selection of MuJoCo benchmarks.\n\nStrengths:\n+ The derivations and the theorems are non-trivial. There is some serious analysis regarding the selection of radius of Gaussian balls. I would like to congratulate the authors for this. I particularly like the extension to having a perturbing function h(x), leading to a more realistic setup.\n+ I also particularly like that the algorithm is able to recover subspaces automatically. This definitely makes the algorithm much more practical and more efficient.\n+ The writing and the presentation are rather clear and well taken care of. Although there are several theorems, it was not too hard to follow the flow of the paper. The algorithm boxes are also concise and clear, helping with understanding the final result.\n\nWeaknesses:\n+ Although the contributions of the work are mostly on the theoretical side, I have a hard time grasping how useful is the algorithm in practice. For one, there is the assumption of strongly convex and smooth function. Granted, there is the relaxed case of having the perturbing function h(x), howevrer, in that case it seems that the algorithm becomes a slower by an order of 60 \u03b4 k Q_g(A). How fast or slow is this in practice? Even with applying the monotone function, the algorithm becomes more practical by being applicable to quasi-convex setups. However, how realistic is that a function will in practice be strictly monotone? While most of this may be hard to be theoretically proven, they can be experimentally tested.\n\n+ Also, given that the paper is interested in black box functions, we cannot have much information regarding the function. So, what happens when the function is not strongly convex or not always smooth? Furthermore, how realistic is to know the condition number, that is the maximum derivative (or an upper bound of it) since we do not have access to the gradients in the first place?\n\n+ I would say that the paper could benefit from a more extensive experimental section. Currently only a single synthetic function is analyzed, also under a single monotone exponential transformation. From a more practical point of view, MuJoCo environments are also examined. However, there exist no comparisons with other methods in the literature, including ARS. Another relevant algorithm to compare with would be the stochastic tree points (Bergou et al., 2019), if not experimentally at least theoretically. In the end, it quite unclear whether the algorithm work well in practice. Some experiments that could shed light would relate to how sensitive the algorithm is to the convexity/smoothness assumptions, how sensitive the algorithm is to the perturbing function h(x), how sensitive is the algorithm to the present of a lower-dimensional subspace that needs to be discovered. And for the MuJoCo experiments, the algorithm can compare at least with ARS.\n\n+ In the experiments it seems the paper is particularly good in high dimensions. Can this be more precisely connected to the derived theory in the discussion of the experiments? Does this relate to the better subspaces k that are discovered automatically by the algorithm?\n\n+ It is unclear how many evaluations are needed per step, that is what is the K value in the algorithm box? Also, there are at least two balls to sample from, so twice as many evaluations, correct?\n\n+ It is unclear why Bayesian Optimization is not considered for at least comparing experimentally. Currently, the paper discards them on the grounds that they do not provide strong theoretically guarantees. However, it would be interesting to examine at least in practice how good/bad are these algorithms in comparison to the proposed one. Two recent bayesian optimization papers that can be considered for continuous and discrete inputs are\n\n[1] BOCK: Bayesian Optimization with Cylindrical Kernels, C. Oh, E. Gavves, M. Welling, ICML 2018\n[2] BOCS: Bayesian Optimization of Combinatorial Structures, R. Baptista, M. Poloczek, ICML 2018\n\n+ The paper does not have a conclusion. That shows great sloppiness. Also, i/n the abstract, do you mean to say k>=n or k<=n?\n\nTo conclude, I recommend weak rejection only because I am not completely convinced by the experiments and do not know if the proposed algorithm is competitive against reasonable baselines and in more complex setups. I am more than happy to upgrade my score if experiments become more clear.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper836/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper836/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradientless Descent: High-Dimensional Zeroth-Order Optimization", "authors": ["Daniel Golovin", "John Karro", "Greg Kochanski", "Chansoo Lee", "Xingyou Song", "Qiuyi Zhang"], "authorids": ["dgg@google.com", "karro@google.com", "gpk@google.com", "chansoo@google.com", "xingyousong@google.com", "qiuyiz@google.com"], "keywords": ["Zeroth Order Optimization"], "TL;DR": "Gradientless Descent is a provably efficient gradient-free algorithm that is monotone-invariant and fast for high-dimensional zero-th order optimization.", "abstract": "Zeroth-order optimization is the process of minimizing an objective $f(x)$, given oracle access to evaluations at adaptively chosen inputs $x$. In this paper, we present two simple yet powerful GradientLess Descent (GLD) algorithms that do not rely on an underlying gradient estimate and are numerically stable. We analyze our algorithm from a novel geometric perspective and we show that for {\\it any monotone transform} of a smooth and strongly convex objective with latent dimension $k \\ge n$, we present a novel analysis that shows convergence within an $\\epsilon$-ball of the optimum in $O(kQ\\log(n)\\log(R/\\epsilon))$ evaluations, where the input dimension is $n$, $R$ is the diameter of the input space and $Q$ is the condition number. Our rates are the first of its kind to be both 1) poly-logarithmically dependent on dimensionality and 2) invariant under monotone transformations. We further leverage our geometric perspective to show that our analysis is optimal. Both monotone invariance and its ability to utilize a low latent dimensionality are key to the empirical success of our algorithms, as demonstrated on synthetic and MuJoCo benchmarks.\n", "pdf": "/pdf/e6a8deb0be680bca761c33514d9ae4a5b7e42868.pdf", "paperhash": "golovin|gradientless_descent_highdimensional_zerothorder_optimization", "_bibtex": "@inproceedings{\nGolovin2020Gradientless,\ntitle={Gradientless Descent: High-Dimensional Zeroth-Order Optimization},\nauthor={Daniel Golovin and John Karro and Greg Kochanski and Chansoo Lee and Xingyou Song and Qiuyi Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Skep6TVYDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/92231498c15cc2fa7c0b6ee565d5b9561d8b5b83.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Skep6TVYDB", "replyto": "Skep6TVYDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper836/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper836/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575017662361, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper836/Reviewers"], "noninvitees": [], "tcdate": 1570237746291, "tmdate": 1575017662375, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper836/-/Official_Review"}}}, {"id": "B1xE7meqsH", "original": null, "number": 2, "cdate": 1573679900509, "ddate": null, "tcdate": 1573679900509, "tmdate": 1573682940830, "tddate": null, "forum": "Skep6TVYDB", "replyto": "B1geUE_Hcr", "invitation": "ICLR.cc/2020/Conference/Paper836/-/Official_Comment", "content": {"title": "Author Response to Official Blind Review #1", "comment": "We thank Reviewer 1 for the compliments and suggestions. \n\n*** In response to the main concern, please see our meta-response, where we have added extra experimentation in Appendix C to compare GLD against Accelerated Random Search on numerous modified Blackbox functions. For Mujoco, we have also added comparisons against the reward threshold in the ARS (not to be confused with Accelerated Random Search) paper for RL [Mania18] to show that our algorithm remains competitive in finding the optimal reward. If after looking at the experiments and our other responses you believe that we have addressed your concerns well and our paper is interesting enough to be accepted at ICLR, we hope that you would kindly increase your rating.\n\nWe address other concerns raised below.\n\n>> Fast or slow in practice?: \nWe believe that our theory guides practice and (as commonplace in optimization literature) algorithms that provide guarantees only in the convex setting are often useful even when applied in a non-convex setting.  Also, please see our meta-response.\n\n>>Relaxed case? \nWe would like to clarify that the perturbation function h(x) is not a noise term but any small adversarial addition. We would like to note that this does not slow down the convergence of the algorithm until the error becomes smaller than \u03b4 k Q_g(A). Furthermore, we demonstrate in our benchmarks that our algorithms do well in high dimensions (often much better than others), even for functions that are not explicitly low rank. This shows that our algorithm is practically quite effective in this regime. We believe that this is explained via our convergence bounds when the function is dominant only in a few directions.\n\n>> Strict monotoncity?:\nWe would like to note that our algorithm does not assume monotone functions, rather monotone transformations--this follows from the fact that we depend only on the ranks of function values. In practice, monotone transformations (such as applying a log transformation) are often applied prior to optimization. \n\n>> \u201cSo, what happens when the function is not strongly convex or not always smooth? Furthermore, how realistic is to know the condition number, that is the maximum derivative (or an upper bound of it) since we do not have access to the gradients in the first place?\u201d\nThis is a valid point: condition numbers are not known in practice and convexity is mostly non-existent. Our GLD-Search algorithm copes with this by applying an extensive binary search procedure. However, as experimentally verified on MuJoCo and newly added BBOB benchmarks, we see that performance does not deteriorate significantly due to lack of knowledge of condition numbers or lack of convexity. More is mentioned in our general response.\n\n>> \u201cIn the experiments it seems the paper is particularly good in high dimensions. Can this be more precisely connected to the derived theory in the discussion of the experiments? Does this relate to the better subspaces k that are discovered automatically by the algorithm?\u201d \nIn the Mujoco benchmarks, we would like to note that we explicitly tested affine invariance by using a lift of our input space from 17 variables to 200 variables. We discuss our results in the experiments section that GLD is not sensitive to a massive increase in dimensionality. This does indeed relate to the subspaces of lower dimension (k = 17 in this case) that are automatically discovered by the algorithm.\n\n>> \u201cIt is unclear how many evaluations are needed per step, that is what is the K value in the algorithm box? Also, there are at least two balls to sample from, so twice as many evaluations, correct?\u201d\nWe would like to clarify that the K value is the number of evaluations per iteration and is set at the beginning of both GLD algorithms. This is required since ball sampling is sensitive to the radius and so K radii is attempted, via binary search. There is only one ball that is sampled at a specific time in the algorithm. While the proofs use a two-ball intersection analysis, please note that this is purely for analysis. Our search over ball radii will increase the number of evaluations, but this is accounted for in the theorem and proofs. \n\n>> Bayesian Optimization Comparisons\nPlease see our meta response.\n\n>> Conclusion and \u201cAlso, i/n the abstract, do you mean to say k>=n or k<=n?\u201d\nWe thank the reviewer for the suggestion and for catching the typo. We have changed the abstract and have added a conclusion. The conclusion now briefly summarizes our contributions and provides possibilities for new research directions.\n\n########################## References ##########################\n[Mania18]: Simple random search provides a competitive approach to reinforcement learning, NeurIPS 2018.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper836/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper836/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradientless Descent: High-Dimensional Zeroth-Order Optimization", "authors": ["Daniel Golovin", "John Karro", "Greg Kochanski", "Chansoo Lee", "Xingyou Song", "Qiuyi Zhang"], "authorids": ["dgg@google.com", "karro@google.com", "gpk@google.com", "chansoo@google.com", "xingyousong@google.com", "qiuyiz@google.com"], "keywords": ["Zeroth Order Optimization"], "TL;DR": "Gradientless Descent is a provably efficient gradient-free algorithm that is monotone-invariant and fast for high-dimensional zero-th order optimization.", "abstract": "Zeroth-order optimization is the process of minimizing an objective $f(x)$, given oracle access to evaluations at adaptively chosen inputs $x$. In this paper, we present two simple yet powerful GradientLess Descent (GLD) algorithms that do not rely on an underlying gradient estimate and are numerically stable. We analyze our algorithm from a novel geometric perspective and we show that for {\\it any monotone transform} of a smooth and strongly convex objective with latent dimension $k \\ge n$, we present a novel analysis that shows convergence within an $\\epsilon$-ball of the optimum in $O(kQ\\log(n)\\log(R/\\epsilon))$ evaluations, where the input dimension is $n$, $R$ is the diameter of the input space and $Q$ is the condition number. Our rates are the first of its kind to be both 1) poly-logarithmically dependent on dimensionality and 2) invariant under monotone transformations. We further leverage our geometric perspective to show that our analysis is optimal. Both monotone invariance and its ability to utilize a low latent dimensionality are key to the empirical success of our algorithms, as demonstrated on synthetic and MuJoCo benchmarks.\n", "pdf": "/pdf/e6a8deb0be680bca761c33514d9ae4a5b7e42868.pdf", "paperhash": "golovin|gradientless_descent_highdimensional_zerothorder_optimization", "_bibtex": "@inproceedings{\nGolovin2020Gradientless,\ntitle={Gradientless Descent: High-Dimensional Zeroth-Order Optimization},\nauthor={Daniel Golovin and John Karro and Greg Kochanski and Chansoo Lee and Xingyou Song and Qiuyi Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Skep6TVYDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/92231498c15cc2fa7c0b6ee565d5b9561d8b5b83.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Skep6TVYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper836/Authors", "ICLR.cc/2020/Conference/Paper836/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper836/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper836/Reviewers", "ICLR.cc/2020/Conference/Paper836/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper836/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper836/Authors|ICLR.cc/2020/Conference/Paper836/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504165469, "tmdate": 1576860532869, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper836/Authors", "ICLR.cc/2020/Conference/Paper836/Reviewers", "ICLR.cc/2020/Conference/Paper836/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper836/-/Official_Comment"}}}, {"id": "H1lvhGxcsr", "original": null, "number": 1, "cdate": 1573679790704, "ddate": null, "tcdate": 1573679790704, "tmdate": 1573682286035, "tddate": null, "forum": "Skep6TVYDB", "replyto": "Skep6TVYDB", "invitation": "ICLR.cc/2020/Conference/Paper836/-/Official_Comment", "content": {"title": "Meta-Response", "comment": "We thank the reviewers for their time and extensive feedback, which is helpful for improving the paper. We have provided a revised version of the paper with more extensive experimentation and clarifications.\n\nBelow, we provide answers to common questions:\n\n-----------------1. Empirical evaluation - Please see Section C.1 of the Appendix. In line with suggestions and concerns, we have **added numerous experimentation with BBOB (BlackBox Optimization Benchmarking) functions** using GLD and Accelerated Random Search.\n\nThis further demonstrates the utility of Gradientless Descent on functions that are not strongly smooth or convex and/or have multiple local optima.  GLD also seems to perform better in higher dimensions, implying that the algorithm is not sensitive to the lack of explicit low-dimensional structure. This result is not coincidental. GLD is invariant to the scaling of function values (invariance to monotonic transformations) and does a binary search over the step sizes. Accelerated Random Search is more reliant on the function\u2019s smoothness, because it performs an adaptive gradient from estimated gradients.\n\nWe would like to first note that algorithms such as Adam [Reddi18] and Adagrad [Duchi10] only have guarantees in convex settings, but they are very successfully applied to non-convex settings. In non-convex settings, our theory guarantees the convergence to the \u201cbasin of attraction\u201d within a locally strongly convex and smooth structure (and more generally to monotone transformations of such structures). In our original submission, we showed empirically that our algorithm is successfully applied to MuJoCo benchmarks, where the objective function is not strongly convex and smooth.\n\n\n-----------------2. Bayesian Optimization (BO) Comparisons - We agree that Bayesian optimization is a good candidate, often used to solve these problems. In light of these concerns, we have added some comparisons in our introduction. However, they are not within the context of our paper and our comparisons for multiple reasons: \n\n**** While there are known regret bounds for Bayesian optimization on non-convex problems, they are considerably narrower than bounds available under the assumption of convex objectives [Freitas12].\n\n**** BO is a complex system of algorithms, which consists of (a) a model-building algorithm (e.g. Gaussian Process), which often includes hyperparameter optimization and regularization, (b) an acquisition function and an optimization strategy (which can be quite complicated [Wilson18]) for the acquisition function, and sometimes (c) pre-processing for the support points. Given all this complexity, there is room for both misconfiguration of a BO system, and over-adaption of a BO system to a given data set, and a proper comparison is outside the time and space of our paper. We further note that because GLD is also practially used as a component of (b) in a large-scale industrial zero-order optimization system [Golovin17], our theoretical analysis also benefits understanding of BO.\n\n**** Bayesian optimization is notoriously slow for the inversion of the kernel matrix for the evaluation of the acquisition function [Rasmussen06], as shown in Figure 6 of [Golovin17].  The cost of generating a new step in a Bayesian Optimization system can be large, sometimes even dominating the cost of evaluating the objective function.  Consequently, while BO is valuable for expensive functions; there is an important niche where one needs to rapidly optimize functions that aren't expensive to compute.\n\n\n\n########################## References ##########################\n[Rasmussen06]: Gaussian Processes for Machine Learning, TextBook, 2006.\n[Freitas12]: Exponential Regret Bounds for Gaussian Process Bandits with\nDeterministic Observations, ICML 2012.\n[Duchi11]: Adaptive Subgradient Methods for Online Learning and Stochastic Optimization, COLT 2010.\n[Reddi18]: On the Convergence of Adam and Beyond, ICLR 2018.\n[Wilson18]: Maximizing acquisition functions for Bayesian optimization, NeurIPS 2018.\n[Golovin17]: Google Vizier: A Service for Black-Box Optimization, KDD 2017.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper836/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper836/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradientless Descent: High-Dimensional Zeroth-Order Optimization", "authors": ["Daniel Golovin", "John Karro", "Greg Kochanski", "Chansoo Lee", "Xingyou Song", "Qiuyi Zhang"], "authorids": ["dgg@google.com", "karro@google.com", "gpk@google.com", "chansoo@google.com", "xingyousong@google.com", "qiuyiz@google.com"], "keywords": ["Zeroth Order Optimization"], "TL;DR": "Gradientless Descent is a provably efficient gradient-free algorithm that is monotone-invariant and fast for high-dimensional zero-th order optimization.", "abstract": "Zeroth-order optimization is the process of minimizing an objective $f(x)$, given oracle access to evaluations at adaptively chosen inputs $x$. In this paper, we present two simple yet powerful GradientLess Descent (GLD) algorithms that do not rely on an underlying gradient estimate and are numerically stable. We analyze our algorithm from a novel geometric perspective and we show that for {\\it any monotone transform} of a smooth and strongly convex objective with latent dimension $k \\ge n$, we present a novel analysis that shows convergence within an $\\epsilon$-ball of the optimum in $O(kQ\\log(n)\\log(R/\\epsilon))$ evaluations, where the input dimension is $n$, $R$ is the diameter of the input space and $Q$ is the condition number. Our rates are the first of its kind to be both 1) poly-logarithmically dependent on dimensionality and 2) invariant under monotone transformations. We further leverage our geometric perspective to show that our analysis is optimal. Both monotone invariance and its ability to utilize a low latent dimensionality are key to the empirical success of our algorithms, as demonstrated on synthetic and MuJoCo benchmarks.\n", "pdf": "/pdf/e6a8deb0be680bca761c33514d9ae4a5b7e42868.pdf", "paperhash": "golovin|gradientless_descent_highdimensional_zerothorder_optimization", "_bibtex": "@inproceedings{\nGolovin2020Gradientless,\ntitle={Gradientless Descent: High-Dimensional Zeroth-Order Optimization},\nauthor={Daniel Golovin and John Karro and Greg Kochanski and Chansoo Lee and Xingyou Song and Qiuyi Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Skep6TVYDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/92231498c15cc2fa7c0b6ee565d5b9561d8b5b83.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Skep6TVYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper836/Authors", "ICLR.cc/2020/Conference/Paper836/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper836/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper836/Reviewers", "ICLR.cc/2020/Conference/Paper836/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper836/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper836/Authors|ICLR.cc/2020/Conference/Paper836/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504165469, "tmdate": 1576860532869, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper836/Authors", "ICLR.cc/2020/Conference/Paper836/Reviewers", "ICLR.cc/2020/Conference/Paper836/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper836/-/Official_Comment"}}}, {"id": "rkett7l5oB", "original": null, "number": 4, "cdate": 1573680000756, "ddate": null, "tcdate": 1573680000756, "tmdate": 1573680000756, "tddate": null, "forum": "Skep6TVYDB", "replyto": "H1ePXMhaYS", "invitation": "ICLR.cc/2020/Conference/Paper836/-/Official_Comment", "content": {"title": "Author Response to Official Blind Review #3", "comment": "Thank you for your review and encouraging comments!\n\n>> Empirical Evaluation\nPlease see our meta-response.\n\n>> 1) Comparison to Bayesian Optimization\nPlease see our meta-response.\n\n>> 2) \u201cThm7: r = 2^k1 C_1 and r = 2^-k2 C_2 are the only two possible radii? Is the statement valid for any choice in the range?\u201d\nWe would like to clarify that our theorem proves the existence of two radii that can be found via binary search that satisfy our descent criterion. The reviewer is correct that the statement is valid for any choice in the range. We would like to note that we only have to show the existence of two radii for our algorithms to provably succeed.\n\n>> 3) \u201cThm13: Unlike the statements in Sect.3.2 and 3.3, here the result is reported in terms of x_T (instead of f(x_T)). This is perfectly fine, but the guarantee you obtain is not an epsilon accuracy, but Q^{3/2}epsilon. If we want to obtain an epsilon accuracy, how much is the number of evaluation going to change? It seems like it would just make an additional Q appear in the log, but I would like the authors to confirm.\u201d\nWe would like to clarify that our theorem will sometimes return an accuracy that is dependent on Q in that case where an upper bound on Q is unknown. The reviewer is correct that the iteration complexity will simply make an addition Q appear in the logarithm. \n\n>> 4) \u201cThm13: \"High-probability\": could you make this more explicit? Can I make the probability arbitrarily close to 1? How would it appear in the number of iterations? As just a log(1/delta) term?\u201d\nWe would like to clarify that high probability means that the failure probability is 1/poly(n) and can be made arbitrarily close to 1 (as long as the failure probability is not exponentially small). If a dependence on failure probability delta were to be made explicit, the reviewer is correct that it will be an additional log(1/delta) (and remove a log(n) from the iteration bound). \n\n>> 5) \u201cThm14 is reported for \u2018suitable parameters\u2019. Although this choice of parameter actually appears in Alg.2, it would be more complete to report it in the statement as well.\u201d\nWe would like to clarify that we try to only include parameters that appear in the final iteration complexity bound, so as to not overwhelm the reader. The reviewer is correct in stating that all details are mentioned in the final algorithm.\n\n>> 6) \u201cFig1 bottom line, first two charts display a weird behavior for GLD-Fast, where the error seems to plateau and spot decreasing. Can you explain why this is happening? Is it due to wrong parameters \\hat alpha and \\hat beta?\u201d\nWe would like to clarify that GLD-Fast can often overshoot when it is extremely close to the optima due to the restrictions on the radii sizes and the binary search procedure can try a lot of large radii sizes, leading to overshooting. We would like to note that GLD-Fast only plateaus when the optimality gap is exceedingly small.\n\n>> Minor Comments.\nWe thank the reviewer for the minor comments and we have edited the paper to get rid of typos and clarify things. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper836/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper836/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradientless Descent: High-Dimensional Zeroth-Order Optimization", "authors": ["Daniel Golovin", "John Karro", "Greg Kochanski", "Chansoo Lee", "Xingyou Song", "Qiuyi Zhang"], "authorids": ["dgg@google.com", "karro@google.com", "gpk@google.com", "chansoo@google.com", "xingyousong@google.com", "qiuyiz@google.com"], "keywords": ["Zeroth Order Optimization"], "TL;DR": "Gradientless Descent is a provably efficient gradient-free algorithm that is monotone-invariant and fast for high-dimensional zero-th order optimization.", "abstract": "Zeroth-order optimization is the process of minimizing an objective $f(x)$, given oracle access to evaluations at adaptively chosen inputs $x$. In this paper, we present two simple yet powerful GradientLess Descent (GLD) algorithms that do not rely on an underlying gradient estimate and are numerically stable. We analyze our algorithm from a novel geometric perspective and we show that for {\\it any monotone transform} of a smooth and strongly convex objective with latent dimension $k \\ge n$, we present a novel analysis that shows convergence within an $\\epsilon$-ball of the optimum in $O(kQ\\log(n)\\log(R/\\epsilon))$ evaluations, where the input dimension is $n$, $R$ is the diameter of the input space and $Q$ is the condition number. Our rates are the first of its kind to be both 1) poly-logarithmically dependent on dimensionality and 2) invariant under monotone transformations. We further leverage our geometric perspective to show that our analysis is optimal. Both monotone invariance and its ability to utilize a low latent dimensionality are key to the empirical success of our algorithms, as demonstrated on synthetic and MuJoCo benchmarks.\n", "pdf": "/pdf/e6a8deb0be680bca761c33514d9ae4a5b7e42868.pdf", "paperhash": "golovin|gradientless_descent_highdimensional_zerothorder_optimization", "_bibtex": "@inproceedings{\nGolovin2020Gradientless,\ntitle={Gradientless Descent: High-Dimensional Zeroth-Order Optimization},\nauthor={Daniel Golovin and John Karro and Greg Kochanski and Chansoo Lee and Xingyou Song and Qiuyi Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Skep6TVYDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/92231498c15cc2fa7c0b6ee565d5b9561d8b5b83.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Skep6TVYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper836/Authors", "ICLR.cc/2020/Conference/Paper836/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper836/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper836/Reviewers", "ICLR.cc/2020/Conference/Paper836/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper836/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper836/Authors|ICLR.cc/2020/Conference/Paper836/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504165469, "tmdate": 1576860532869, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper836/Authors", "ICLR.cc/2020/Conference/Paper836/Reviewers", "ICLR.cc/2020/Conference/Paper836/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper836/-/Official_Comment"}}}, {"id": "rkeEw7x5sB", "original": null, "number": 3, "cdate": 1573679963565, "ddate": null, "tcdate": 1573679963565, "tmdate": 1573679963565, "tddate": null, "forum": "Skep6TVYDB", "replyto": "r1gG4uu3FB", "invitation": "ICLR.cc/2020/Conference/Paper836/-/Official_Comment", "content": {"title": "Author Response to Official Blind Review #2", "comment": "Thank you for your positive review.\n\n>> New Noise Model\nWe thank the reviewer for the proposal of a new noise model. We would like to note the noise model proposed can be reduced to our current noise model. Note that using h_2(x) = g(P_Ax + h(x)) - g(P_Ax) reduces to our case with h_2(x) as our perturbation (i.e., h(x)). Furthermore, since we are working with the case when h(x) is small and g is smooth, we also know that h_2(x), by Taylor expansion, is approximately g\u2019(P_Ax) h(x), which can be bounded. Therefore, any theoretical bounds we have can be translated to the proposed noise model.\n\n>> \u201cThe experiments on Mujoco do not satisfy the assumption previous. Is there any real-world application which matches the theoretical analysis?\u201d\nPlease see our meta-response.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper836/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper836/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradientless Descent: High-Dimensional Zeroth-Order Optimization", "authors": ["Daniel Golovin", "John Karro", "Greg Kochanski", "Chansoo Lee", "Xingyou Song", "Qiuyi Zhang"], "authorids": ["dgg@google.com", "karro@google.com", "gpk@google.com", "chansoo@google.com", "xingyousong@google.com", "qiuyiz@google.com"], "keywords": ["Zeroth Order Optimization"], "TL;DR": "Gradientless Descent is a provably efficient gradient-free algorithm that is monotone-invariant and fast for high-dimensional zero-th order optimization.", "abstract": "Zeroth-order optimization is the process of minimizing an objective $f(x)$, given oracle access to evaluations at adaptively chosen inputs $x$. In this paper, we present two simple yet powerful GradientLess Descent (GLD) algorithms that do not rely on an underlying gradient estimate and are numerically stable. We analyze our algorithm from a novel geometric perspective and we show that for {\\it any monotone transform} of a smooth and strongly convex objective with latent dimension $k \\ge n$, we present a novel analysis that shows convergence within an $\\epsilon$-ball of the optimum in $O(kQ\\log(n)\\log(R/\\epsilon))$ evaluations, where the input dimension is $n$, $R$ is the diameter of the input space and $Q$ is the condition number. Our rates are the first of its kind to be both 1) poly-logarithmically dependent on dimensionality and 2) invariant under monotone transformations. We further leverage our geometric perspective to show that our analysis is optimal. Both monotone invariance and its ability to utilize a low latent dimensionality are key to the empirical success of our algorithms, as demonstrated on synthetic and MuJoCo benchmarks.\n", "pdf": "/pdf/e6a8deb0be680bca761c33514d9ae4a5b7e42868.pdf", "paperhash": "golovin|gradientless_descent_highdimensional_zerothorder_optimization", "_bibtex": "@inproceedings{\nGolovin2020Gradientless,\ntitle={Gradientless Descent: High-Dimensional Zeroth-Order Optimization},\nauthor={Daniel Golovin and John Karro and Greg Kochanski and Chansoo Lee and Xingyou Song and Qiuyi Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Skep6TVYDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/92231498c15cc2fa7c0b6ee565d5b9561d8b5b83.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Skep6TVYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper836/Authors", "ICLR.cc/2020/Conference/Paper836/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper836/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper836/Reviewers", "ICLR.cc/2020/Conference/Paper836/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper836/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper836/Authors|ICLR.cc/2020/Conference/Paper836/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504165469, "tmdate": 1576860532869, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper836/Authors", "ICLR.cc/2020/Conference/Paper836/Reviewers", "ICLR.cc/2020/Conference/Paper836/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper836/-/Official_Comment"}}}, {"id": "r1gG4uu3FB", "original": null, "number": 1, "cdate": 1571747882199, "ddate": null, "tcdate": 1571747882199, "tmdate": 1572972546179, "tddate": null, "forum": "Skep6TVYDB", "replyto": "Skep6TVYDB", "invitation": "ICLR.cc/2020/Conference/Paper836/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes stable GradientLess Descent (GLD) algorithms that do not rely on gradient estimate. Based on the low-rank assumption on P_A, the iteration complexity is poly-logarithmically dependent on dimensionality. The theoretical analysis of the main results is based on a geometric perspective, which is interesting. The experimental results on synthetic and MuJoCo datasets validate the effectiveness of the proposed algorithms.\n\nThe theoretical contribution of this paper is nice and valuable. My main concern is the structure f(x) = g(P_A x) + h(x) looks somewhat limited. A more natural form is moving the perturbation into g, i.e, f(x) = g(P_A x + h(x)). \n\nThe experiments on Mujoco do not satisfy the assumption previous. Is there any real-world application which matches the theoretical analysis?\n\nIn summary, I think this is a good paper and tend to accept it.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper836/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper836/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradientless Descent: High-Dimensional Zeroth-Order Optimization", "authors": ["Daniel Golovin", "John Karro", "Greg Kochanski", "Chansoo Lee", "Xingyou Song", "Qiuyi Zhang"], "authorids": ["dgg@google.com", "karro@google.com", "gpk@google.com", "chansoo@google.com", "xingyousong@google.com", "qiuyiz@google.com"], "keywords": ["Zeroth Order Optimization"], "TL;DR": "Gradientless Descent is a provably efficient gradient-free algorithm that is monotone-invariant and fast for high-dimensional zero-th order optimization.", "abstract": "Zeroth-order optimization is the process of minimizing an objective $f(x)$, given oracle access to evaluations at adaptively chosen inputs $x$. In this paper, we present two simple yet powerful GradientLess Descent (GLD) algorithms that do not rely on an underlying gradient estimate and are numerically stable. We analyze our algorithm from a novel geometric perspective and we show that for {\\it any monotone transform} of a smooth and strongly convex objective with latent dimension $k \\ge n$, we present a novel analysis that shows convergence within an $\\epsilon$-ball of the optimum in $O(kQ\\log(n)\\log(R/\\epsilon))$ evaluations, where the input dimension is $n$, $R$ is the diameter of the input space and $Q$ is the condition number. Our rates are the first of its kind to be both 1) poly-logarithmically dependent on dimensionality and 2) invariant under monotone transformations. We further leverage our geometric perspective to show that our analysis is optimal. Both monotone invariance and its ability to utilize a low latent dimensionality are key to the empirical success of our algorithms, as demonstrated on synthetic and MuJoCo benchmarks.\n", "pdf": "/pdf/e6a8deb0be680bca761c33514d9ae4a5b7e42868.pdf", "paperhash": "golovin|gradientless_descent_highdimensional_zerothorder_optimization", "_bibtex": "@inproceedings{\nGolovin2020Gradientless,\ntitle={Gradientless Descent: High-Dimensional Zeroth-Order Optimization},\nauthor={Daniel Golovin and John Karro and Greg Kochanski and Chansoo Lee and Xingyou Song and Qiuyi Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Skep6TVYDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/92231498c15cc2fa7c0b6ee565d5b9561d8b5b83.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Skep6TVYDB", "replyto": "Skep6TVYDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper836/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper836/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575017662361, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper836/Reviewers"], "noninvitees": [], "tcdate": 1570237746291, "tmdate": 1575017662375, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper836/-/Official_Review"}}}, {"id": "H1ePXMhaYS", "original": null, "number": 2, "cdate": 1571828254909, "ddate": null, "tcdate": 1571828254909, "tmdate": 1572972546144, "tddate": null, "forum": "Skep6TVYDB", "replyto": "Skep6TVYDB", "invitation": "ICLR.cc/2020/Conference/Paper836/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "** Summary\nThe paper proposes a novel zeroth-order algorithm for high-dimensional optimization. In particular, the algorithm as an instance of direct search algorithms where no attempt is made to estimate the gradient of the function during the optimization process. The authors study the optimization of monotone transformations of strongly-convex and smooth functions and they prove complexity bounds as a function of the condition number, the dimensionality and the desired accuracy. These results are also extended to the case where the function actually depends on a lower-dimensional input. Without any knowledge of the actual subspace of interest, the algorithm is able to adapt to the (lower) dimensionality of the problem. The proposed algorithms are tested on synthetic optimization problems and in a few Mujoco environments for policy optimization.\n\n** Overall evaluation\nThe paper is a solid theoretical and algorithmic contribution to the zeroth-gradient optimization literature. The positive aspects of the paper are:\n- Novel algorithm with strong theoretical guarantees improving or generalizing previous state-of-the-art methods.\n- Ability to adapt to low-dimensional problems and more in general to monotone transformations of convex functions.\n- Efficient version.\n\nNegative aspects of the paper that the authors may address are:\n- The empirical validation is rather weak at the moment. It provides some evidence of the effectiveness of the proposed method but it uses only one baseline and a very few type of optimization problems. Although in my opinion the main contribution is on the theoretical side, a more thorough empirical validation would be welcome.\n- Some theorem statements can be made clearer and some comparisons should be more explicit (see detailed comments later).\n\nDetailed comments:\n1- The authors explicitly mentioned in the introduction that they do not compare/discuss alternative approaches such as Bayesian optimization (BO). Although I agree the approaches may be different, BO is probably the most popular type of black-box optimization. Furthermore, many methods (e.g., GP-UCB https://arxiv.org/abs/0912.3995) come with strong theoretical guarantees on the regret and so optimization performance both under the Bayesian assumption (i.e., the function is generated from a prior) and the \"frequentist\" case (i.e., the function is an arbitrary element of a bounded RKHS). Furthermore, there are also adaptive BO methods that adapt to the actual dimensionality of the problem, in a similar spirit as the low-dimensional case studied in this paper. See e.g., https://arxiv.org/abs/1903.05594 and http://papers.nips.cc/paper/8115-efficient-high-dimensional-bayesian-optimization-with-additivity-and-quadrature-fourier-features. I would appreciate if the authors would at least provide a high-level discussion on similarities and differences between these type of approaches.\n2- Thm7: r = 2^k1 C_1 and r = 2^-k2 C_2 are the only two possible radii? Is the statement valid for any choice in the range?\n3- Thm13: Unlike the statements in Sect.3.2 and 3.3, here the result is reported in terms of x_T (instead of f(x_T)). This is perfectly fine, but the guarantee you obtain is not an epsilon accuracy, but Q^{3/2}epsilon. If we want to obtain an epsilon accuracy, how much is the number of evaluation going to change? It seems like it would just make an additional Q appear in the log, but I would like the authors to confirm.\n4- Thm13: \"High-probability\": could you make this more explicit? Can I make the probability arbitrarily close to 1? How would it appear in the number of iterations? As just a log(1/delta) term?\n5- Thm14 is reported for \"suitable parameters\". Although this choice of parameter actually appears in Alg.2, it would be more complete to report it in the statement as well.\n6- Fig1 bottom line, first two charts display a weird behavior for GLD-Fast, where the error seems to plateau and spot decreasing. Can you explain why this is happening? Is it due to wrong parameters \\hat alpha and \\hat beta?\n\nMinor comments:\n- In the proof of Lem.8, it would be helpful to have a graphical representation of the spheres and the hyperspherial caps.\n- In the proof of Lem.15, you mention \"strong smoothness assumption\", it should be just smoothness.\n- It would be helpful to have more intuition on the why the algorithm is able to adapt to the actual dimensionality of the problem. My understanding is that the probability to pick a point of lower value is increased and since the algorithm is testing different radii and pick the best point, it successfully adapt to this better situation."}, "signatures": ["ICLR.cc/2020/Conference/Paper836/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper836/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradientless Descent: High-Dimensional Zeroth-Order Optimization", "authors": ["Daniel Golovin", "John Karro", "Greg Kochanski", "Chansoo Lee", "Xingyou Song", "Qiuyi Zhang"], "authorids": ["dgg@google.com", "karro@google.com", "gpk@google.com", "chansoo@google.com", "xingyousong@google.com", "qiuyiz@google.com"], "keywords": ["Zeroth Order Optimization"], "TL;DR": "Gradientless Descent is a provably efficient gradient-free algorithm that is monotone-invariant and fast for high-dimensional zero-th order optimization.", "abstract": "Zeroth-order optimization is the process of minimizing an objective $f(x)$, given oracle access to evaluations at adaptively chosen inputs $x$. In this paper, we present two simple yet powerful GradientLess Descent (GLD) algorithms that do not rely on an underlying gradient estimate and are numerically stable. We analyze our algorithm from a novel geometric perspective and we show that for {\\it any monotone transform} of a smooth and strongly convex objective with latent dimension $k \\ge n$, we present a novel analysis that shows convergence within an $\\epsilon$-ball of the optimum in $O(kQ\\log(n)\\log(R/\\epsilon))$ evaluations, where the input dimension is $n$, $R$ is the diameter of the input space and $Q$ is the condition number. Our rates are the first of its kind to be both 1) poly-logarithmically dependent on dimensionality and 2) invariant under monotone transformations. We further leverage our geometric perspective to show that our analysis is optimal. Both monotone invariance and its ability to utilize a low latent dimensionality are key to the empirical success of our algorithms, as demonstrated on synthetic and MuJoCo benchmarks.\n", "pdf": "/pdf/e6a8deb0be680bca761c33514d9ae4a5b7e42868.pdf", "paperhash": "golovin|gradientless_descent_highdimensional_zerothorder_optimization", "_bibtex": "@inproceedings{\nGolovin2020Gradientless,\ntitle={Gradientless Descent: High-Dimensional Zeroth-Order Optimization},\nauthor={Daniel Golovin and John Karro and Greg Kochanski and Chansoo Lee and Xingyou Song and Qiuyi Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Skep6TVYDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/92231498c15cc2fa7c0b6ee565d5b9561d8b5b83.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Skep6TVYDB", "replyto": "Skep6TVYDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper836/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper836/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575017662361, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper836/Reviewers"], "noninvitees": [], "tcdate": 1570237746291, "tmdate": 1575017662375, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper836/-/Official_Review"}}}], "count": 9}