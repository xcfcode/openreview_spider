{"notes": [{"tddate": null, "ddate": null, "original": null, "tmdate": 1521573617255, "tcdate": 1521573617255, "number": 311, "cdate": 1521573616914, "id": "r1YeykJqG", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "HyJf8QJDz", "replyto": "HyJf8QJDz", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "This paper was invited to the workshop track based on reviews at the main conference."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms", "abstract": "The question why deep learning algorithms generalize so well has attracted increasing\nresearch interest. However, most of the well-established approaches,\nsuch as hypothesis capacity, stability or sparseness, have not provided complete\nexplanations (Zhang et al., 2016; Kawaguchi et al., 2017). In this work, we focus\non the robustness approach (Xu & Mannor, 2012), i.e., if the error of a hypothesis\nwill not change much due to perturbations of its training examples, then it\nwill also generalize well. As most deep learning algorithms are stochastic (e.g.,\nStochastic Gradient Descent, Dropout, and Bayes-by-backprop), we revisit the robustness\narguments of Xu & Mannor, and introduce a new approach \u2013 ensemble\nrobustness \u2013 that concerns the robustness of a population of hypotheses. Through\nthe lens of ensemble robustness, we reveal that a stochastic learning algorithm can\ngeneralize well as long as its sensitiveness to adversarial perturbations is bounded\nin average over training examples. Moreover, an algorithm may be sensitive to\nsome adversarial examples (Goodfellow et al., 2015) but still generalize well. To\nsupport our claims, we provide extensive simulations for different deep learning\nalgorithms and different network architectures exhibiting a strong correlation between\nensemble robustness and the ability to generalize.", "pdf": "/pdf/fe30c4710fe1a12a2c0094777825b217669d8195.pdf", "TL;DR": "Explaining the generalization of stochastic deep learning algorithms, theoretically and empirically, via ensemble robustness", "paperhash": "zahavy|ensemble_robustness_and_generalization_of_stochastic_deep_learning_algorithms", "_bibtex": "@misc{\nzahavy2018ensemble,\ntitle={Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms},\nauthor={Tom Zahavy, Bingyi Kang, Alex Sivak, Jiashi Feng, Huan Xu, Shie Mannor},\nyear={2018},\nurl={https://openreview.net/forum?id=r1YUtYx0-},\n}", "keywords": ["Robustness", "Generalization", "Deep Learning", "Adversarial Learning"], "authors": ["Tom Zahavy", "Bingyi Kang", "Alex Sivak", "Jiashi Feng", "Huan Xu", "Shie Mannor"], "authorids": ["tomzahavy@gmail.com", "bingykang@gmail.com", "silex@campus.technion.ac.il", "jshfeng@gmail.com", "huan.xu@isye.gatech.edu", "shiemannor@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "ddate": null, "tmdate": 1518730181276, "tcdate": 1518446087461, "number": 126, "cdate": 1518446087461, "id": "HyJf8QJDz", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "HyJf8QJDz", "original": "r1YUtYx0-", "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms", "abstract": "The question why deep learning algorithms generalize so well has attracted increasing\nresearch interest. However, most of the well-established approaches,\nsuch as hypothesis capacity, stability or sparseness, have not provided complete\nexplanations (Zhang et al., 2016; Kawaguchi et al., 2017). In this work, we focus\non the robustness approach (Xu & Mannor, 2012), i.e., if the error of a hypothesis\nwill not change much due to perturbations of its training examples, then it\nwill also generalize well. As most deep learning algorithms are stochastic (e.g.,\nStochastic Gradient Descent, Dropout, and Bayes-by-backprop), we revisit the robustness\narguments of Xu & Mannor, and introduce a new approach \u2013 ensemble\nrobustness \u2013 that concerns the robustness of a population of hypotheses. Through\nthe lens of ensemble robustness, we reveal that a stochastic learning algorithm can\ngeneralize well as long as its sensitiveness to adversarial perturbations is bounded\nin average over training examples. Moreover, an algorithm may be sensitive to\nsome adversarial examples (Goodfellow et al., 2015) but still generalize well. To\nsupport our claims, we provide extensive simulations for different deep learning\nalgorithms and different network architectures exhibiting a strong correlation between\nensemble robustness and the ability to generalize.", "pdf": "/pdf/fe30c4710fe1a12a2c0094777825b217669d8195.pdf", "TL;DR": "Explaining the generalization of stochastic deep learning algorithms, theoretically and empirically, via ensemble robustness", "paperhash": "zahavy|ensemble_robustness_and_generalization_of_stochastic_deep_learning_algorithms", "_bibtex": "@misc{\nzahavy2018ensemble,\ntitle={Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms},\nauthor={Tom Zahavy, Bingyi Kang, Alex Sivak, Jiashi Feng, Huan Xu, Shie Mannor},\nyear={2018},\nurl={https://openreview.net/forum?id=r1YUtYx0-},\n}", "keywords": ["Robustness", "Generalization", "Deep Learning", "Adversarial Learning"], "authors": ["Tom Zahavy", "Bingyi Kang", "Alex Sivak", "Jiashi Feng", "Huan Xu", "Shie Mannor"], "authorids": ["tomzahavy@gmail.com", "bingykang@gmail.com", "silex@campus.technion.ac.il", "jshfeng@gmail.com", "huan.xu@isye.gatech.edu", "shiemannor@gmail.com"]}, "nonreaders": [], "details": {"replyCount": 1, "writable": false, "overwriting": [], "revisions": false, "tags": [], "original": {"tddate": null, "ddate": null, "tmdate": 1518730181276, "tcdate": 1509099857249, "number": 338, "cdate": 1518730181267, "id": "r1YUtYx0-", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "r1YUtYx0-", "original": "S1OIKYeRW", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms", "abstract": "The question why deep learning algorithms generalize so well has attracted increasing\nresearch interest. However, most of the well-established approaches,\nsuch as hypothesis capacity, stability or sparseness, have not provided complete\nexplanations (Zhang et al., 2016; Kawaguchi et al., 2017). In this work, we focus\non the robustness approach (Xu & Mannor, 2012), i.e., if the error of a hypothesis\nwill not change much due to perturbations of its training examples, then it\nwill also generalize well. As most deep learning algorithms are stochastic (e.g.,\nStochastic Gradient Descent, Dropout, and Bayes-by-backprop), we revisit the robustness\narguments of Xu & Mannor, and introduce a new approach \u2013 ensemble\nrobustness \u2013 that concerns the robustness of a population of hypotheses. Through\nthe lens of ensemble robustness, we reveal that a stochastic learning algorithm can\ngeneralize well as long as its sensitiveness to adversarial perturbations is bounded\nin average over training examples. Moreover, an algorithm may be sensitive to\nsome adversarial examples (Goodfellow et al., 2015) but still generalize well. To\nsupport our claims, we provide extensive simulations for different deep learning\nalgorithms and different network architectures exhibiting a strong correlation between\nensemble robustness and the ability to generalize.", "pdf": "/pdf/fe30c4710fe1a12a2c0094777825b217669d8195.pdf", "TL;DR": "Explaining the generalization of stochastic deep learning algorithms, theoretically and empirically, via ensemble robustness", "paperhash": "zahavy|ensemble_robustness_and_generalization_of_stochastic_deep_learning_algorithms", "_bibtex": "@misc{\nzahavy2018ensemble,\ntitle={Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms},\nauthor={Tom Zahavy and Bingyi Kang and Alex Sivak and Jiashi Feng and Huan Xu and Shie Mannor},\nyear={2018},\nurl={https://openreview.net/forum?id=r1YUtYx0-},\n}", "keywords": ["Robustness", "Generalization", "Deep Learning", "Adversarial Learning"], "authors": ["Tom Zahavy", "Bingyi Kang", "Alex Sivak", "Jiashi Feng", "Huan Xu", "Shie Mannor"], "authorids": ["tomzahavy@gmail.com", "bingykang@gmail.com", "silex@campus.technion.ac.il", "jshfeng@gmail.com", "huan.xu@isye.gatech.edu", "shiemannor@gmail.com"]}, "nonreaders": []}, "originalWritable": false, "originalInvitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}, "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}, "tauthor": "ICLR.cc/2018/Workshop"}], "count": 2}