{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488575780798, "tcdate": 1478298528047, "number": 520, "id": "ry_sjFqgx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "ry_sjFqgx", "signatures": ["~Pavol_Bielik1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Program Synthesis for Character Level Language Modeling", "abstract": "We propose a statistical model applicable to character level language modeling and show that it is a good fit for both, program source code and English text. The model is parameterized by a program from a domain-specific language (DSL) that allows expressing non-trivial data dependencies. Learning is done in two phases: (i) we synthesize a program from the DSL, essentially learning a good representation for the data, and (ii) we learn parameters from the training data - the process is done via counting, as in simple language models such as n-gram.\n\nOur experiments show that the precision of our model is comparable to that of neural networks while sharing a number of advantages with n-gram models such as fast query time and the capability to quickly add and remove training data samples. Further, the model is parameterized by a program that can be manually inspected, understood and updated, addressing a major problem of neural networks.", "pdf": "/pdf/8433d48d1786dfa7e377e8d0f89dc984e7490117.pdf", "paperhash": "bielik|program_synthesis_for_character_level_language_modeling", "conflicts": ["inf.ethz.ch"], "keywords": [], "authors": ["Pavol Bielik", "Veselin Raychev", "Martin Vechev"], "authorids": ["pavol.bielik@inf.ethz.ch", "veselin.raychev@inf.ethz.ch", "martin.vechev@inf.ethz.ch"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396654301, "tcdate": 1486396654301, "number": 1, "id": "SJIgpGLdg", "invitation": "ICLR.cc/2017/conference/-/paper520/acceptance", "forum": "ry_sjFqgx", "replyto": "ry_sjFqgx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "This work was a very controversial submission, with two strong accepts and one initial strong reject. There was significant discussion about the replicability of the paper, although all reviewers seem interested in the methods. \n \n Pro:\n - There is strong originality to this work. One of the few submissions in the area not just using RNNs for LM.\n - The paper shows strong empirical results on PL language modeling, comparative results in natural language modeling, fast lookup compared to neural models, and presents a significantly different approach then the currently accepted NN methods. \n \n \n Cons:\n - The original draft has clarity issues. In particular the MCMC approach is very difficult to follow and lacks clear explanation for this community, the interpretability of the method is not demonstrated, and too much of the work is devoted to laying out the language itself. (Note The authors have gone out of their way to include an appendix which caused one reviewer to go from recommending strong rejection to weak rejection, but also requires a much larger submission size. While two reviewers do like the work, their reviews are mainly based on the empirical sucess at program language modeling.)\n \n Overall, the PCs have determined that this work deserves to appear at the conference.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Program Synthesis for Character Level Language Modeling", "abstract": "We propose a statistical model applicable to character level language modeling and show that it is a good fit for both, program source code and English text. The model is parameterized by a program from a domain-specific language (DSL) that allows expressing non-trivial data dependencies. Learning is done in two phases: (i) we synthesize a program from the DSL, essentially learning a good representation for the data, and (ii) we learn parameters from the training data - the process is done via counting, as in simple language models such as n-gram.\n\nOur experiments show that the precision of our model is comparable to that of neural networks while sharing a number of advantages with n-gram models such as fast query time and the capability to quickly add and remove training data samples. Further, the model is parameterized by a program that can be manually inspected, understood and updated, addressing a major problem of neural networks.", "pdf": "/pdf/8433d48d1786dfa7e377e8d0f89dc984e7490117.pdf", "paperhash": "bielik|program_synthesis_for_character_level_language_modeling", "conflicts": ["inf.ethz.ch"], "keywords": [], "authors": ["Pavol Bielik", "Veselin Raychev", "Martin Vechev"], "authorids": ["pavol.bielik@inf.ethz.ch", "veselin.raychev@inf.ethz.ch", "martin.vechev@inf.ethz.ch"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396654796, "id": "ICLR.cc/2017/conference/-/paper520/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "ry_sjFqgx", "replyto": "ry_sjFqgx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396654796}}}, {"tddate": null, "tmdate": 1485024575056, "tcdate": 1482019106294, "number": 3, "id": "BkcQb8X4x", "invitation": "ICLR.cc/2017/conference/-/paper520/official/review", "forum": "ry_sjFqgx", "replyto": "ry_sjFqgx", "signatures": ["ICLR.cc/2017/conference/paper520/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper520/AnonReviewer1"], "content": {"title": "", "rating": "5: Marginally below acceptance threshold", "review": "This paper proposes an approach to character language modeling (CLMs) based on developing a domain specific language to represent CLMs. The experiments show mixed performance versus neural CLM approaches to modeling linux kernel data and wikipedia text, however the proposed DSL models are slightly more compact and fast to query as compared with neural CLMs. The proposed approach is difficult to understand overall and perhaps is aimed towards the sub-community already working on this sort of approach but lacks sufficient explanation for the ICLR audience. Critically the paper glosses over the major issues of demonstrating the proposed DSL is a valid probabilistic model and how training is performed to fit the model to data (there is clearly not a gradient-based training approach used). FInally the experiments feel incomplete without showing samples drawn from the generative model or analyzing the learned model to determine what it has learned. Overall I feel this paper does not describe the approach in enough depth for readers to understand or re-implement it.\n\nAlmost all of the model section is devoted to exposition of the DSL without specifying how probabilities are computed using this model and how training is performed. How are probabilities actually encoded? The DSL description seems to have only discrete decisions rather than probabilities.\n\nTraining is perhaps covered in previous papers but there needs to be some discussion of how it works here. Section 2.5 does not do enough to explain how training works or how any measure of optimality is achieved.\n\nGiven this model is quite a different hypothesis space from neural models or n-grams, looking and samples drawn from the model seems critical. The current experiments show it can score utterances relatively well but it would be very interesting if the model can sample more structured samples than neural approaches (for example long-range syntax constraints like brackets)", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Program Synthesis for Character Level Language Modeling", "abstract": "We propose a statistical model applicable to character level language modeling and show that it is a good fit for both, program source code and English text. The model is parameterized by a program from a domain-specific language (DSL) that allows expressing non-trivial data dependencies. Learning is done in two phases: (i) we synthesize a program from the DSL, essentially learning a good representation for the data, and (ii) we learn parameters from the training data - the process is done via counting, as in simple language models such as n-gram.\n\nOur experiments show that the precision of our model is comparable to that of neural networks while sharing a number of advantages with n-gram models such as fast query time and the capability to quickly add and remove training data samples. Further, the model is parameterized by a program that can be manually inspected, understood and updated, addressing a major problem of neural networks.", "pdf": "/pdf/8433d48d1786dfa7e377e8d0f89dc984e7490117.pdf", "paperhash": "bielik|program_synthesis_for_character_level_language_modeling", "conflicts": ["inf.ethz.ch"], "keywords": [], "authors": ["Pavol Bielik", "Veselin Raychev", "Martin Vechev"], "authorids": ["pavol.bielik@inf.ethz.ch", "veselin.raychev@inf.ethz.ch", "martin.vechev@inf.ethz.ch"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512555726, "id": "ICLR.cc/2017/conference/-/paper520/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper520/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper520/AnonReviewer2", "ICLR.cc/2017/conference/paper520/AnonReviewer3", "ICLR.cc/2017/conference/paper520/AnonReviewer1"], "reply": {"forum": "ry_sjFqgx", "replyto": "ry_sjFqgx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper520/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper520/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512555726}}}, {"tddate": null, "tmdate": 1484166201616, "tcdate": 1484166201616, "number": 6, "id": "r1fH4zVUx", "invitation": "ICLR.cc/2017/conference/-/paper520/public/comment", "forum": "ry_sjFqgx", "replyto": "Sk4mNvGEe", "signatures": ["~Pavol_Bielik1"], "readers": ["everyone"], "writers": ["~Pavol_Bielik1"], "content": {"title": "Response", "comment": "Thanks for the suggestions, we added a thorough Appendix that provides all details. We also included details about the used hardware in our experimental setup in the updated version. All of our experiments were performed on a machine with Intel(R) Xeon(R) CPU E5-2690 with 14 cores. All training times are reported for parallel training on CPU. Using GPUs for training of the neural networks is likely to provide additional improvement in training time."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Program Synthesis for Character Level Language Modeling", "abstract": "We propose a statistical model applicable to character level language modeling and show that it is a good fit for both, program source code and English text. The model is parameterized by a program from a domain-specific language (DSL) that allows expressing non-trivial data dependencies. Learning is done in two phases: (i) we synthesize a program from the DSL, essentially learning a good representation for the data, and (ii) we learn parameters from the training data - the process is done via counting, as in simple language models such as n-gram.\n\nOur experiments show that the precision of our model is comparable to that of neural networks while sharing a number of advantages with n-gram models such as fast query time and the capability to quickly add and remove training data samples. Further, the model is parameterized by a program that can be manually inspected, understood and updated, addressing a major problem of neural networks.", "pdf": "/pdf/8433d48d1786dfa7e377e8d0f89dc984e7490117.pdf", "paperhash": "bielik|program_synthesis_for_character_level_language_modeling", "conflicts": ["inf.ethz.ch"], "keywords": [], "authors": ["Pavol Bielik", "Veselin Raychev", "Martin Vechev"], "authorids": ["pavol.bielik@inf.ethz.ch", "veselin.raychev@inf.ethz.ch", "martin.vechev@inf.ethz.ch"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287539414, "id": "ICLR.cc/2017/conference/-/paper520/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ry_sjFqgx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper520/reviewers", "ICLR.cc/2017/conference/paper520/areachairs"], "cdate": 1485287539414}}}, {"tddate": null, "tmdate": 1484166180644, "tcdate": 1484166180644, "number": 5, "id": "Skpm4GEIg", "invitation": "ICLR.cc/2017/conference/-/paper520/public/comment", "forum": "ry_sjFqgx", "replyto": "HkPzKlz4x", "signatures": ["~Pavol_Bielik1"], "readers": ["everyone"], "writers": ["~Pavol_Bielik1"], "content": {"title": "Response", "comment": "Given the limits on the length of the submission it was not possible to include all details. However, as suggested by all reviewers, we added an Appendix describing all relevant details of the synthesis, formal semantics and how the probability distributions are built. We also updated Table 2 in the main paper to include results for n-gram (1.94 BPC)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Program Synthesis for Character Level Language Modeling", "abstract": "We propose a statistical model applicable to character level language modeling and show that it is a good fit for both, program source code and English text. The model is parameterized by a program from a domain-specific language (DSL) that allows expressing non-trivial data dependencies. Learning is done in two phases: (i) we synthesize a program from the DSL, essentially learning a good representation for the data, and (ii) we learn parameters from the training data - the process is done via counting, as in simple language models such as n-gram.\n\nOur experiments show that the precision of our model is comparable to that of neural networks while sharing a number of advantages with n-gram models such as fast query time and the capability to quickly add and remove training data samples. Further, the model is parameterized by a program that can be manually inspected, understood and updated, addressing a major problem of neural networks.", "pdf": "/pdf/8433d48d1786dfa7e377e8d0f89dc984e7490117.pdf", "paperhash": "bielik|program_synthesis_for_character_level_language_modeling", "conflicts": ["inf.ethz.ch"], "keywords": [], "authors": ["Pavol Bielik", "Veselin Raychev", "Martin Vechev"], "authorids": ["pavol.bielik@inf.ethz.ch", "veselin.raychev@inf.ethz.ch", "martin.vechev@inf.ethz.ch"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287539414, "id": "ICLR.cc/2017/conference/-/paper520/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ry_sjFqgx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper520/reviewers", "ICLR.cc/2017/conference/paper520/areachairs"], "cdate": 1485287539414}}}, {"tddate": null, "tmdate": 1484166150389, "tcdate": 1484166150389, "number": 4, "id": "H1C-4zVIg", "invitation": "ICLR.cc/2017/conference/-/paper520/public/comment", "forum": "ry_sjFqgx", "replyto": "BkcQb8X4x", "signatures": ["~Pavol_Bielik1"], "readers": ["everyone"], "writers": ["~Pavol_Bielik1"], "content": {"title": "Reponse", "comment": "Thanks for the suggestions. In the Appendix, we clarify the training procedure as well as how the probabilistic model is built. It is true the model is based on discrete and deterministic decisions obtained by executing the learned program. This is also true for the probabilistic model which uses maximum likelihood estimation based on counting (as in n-gram models). It is an interesting future work direction to extend to model with continuous word representations as done in neural networks."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Program Synthesis for Character Level Language Modeling", "abstract": "We propose a statistical model applicable to character level language modeling and show that it is a good fit for both, program source code and English text. The model is parameterized by a program from a domain-specific language (DSL) that allows expressing non-trivial data dependencies. Learning is done in two phases: (i) we synthesize a program from the DSL, essentially learning a good representation for the data, and (ii) we learn parameters from the training data - the process is done via counting, as in simple language models such as n-gram.\n\nOur experiments show that the precision of our model is comparable to that of neural networks while sharing a number of advantages with n-gram models such as fast query time and the capability to quickly add and remove training data samples. Further, the model is parameterized by a program that can be manually inspected, understood and updated, addressing a major problem of neural networks.", "pdf": "/pdf/8433d48d1786dfa7e377e8d0f89dc984e7490117.pdf", "paperhash": "bielik|program_synthesis_for_character_level_language_modeling", "conflicts": ["inf.ethz.ch"], "keywords": [], "authors": ["Pavol Bielik", "Veselin Raychev", "Martin Vechev"], "authorids": ["pavol.bielik@inf.ethz.ch", "veselin.raychev@inf.ethz.ch", "martin.vechev@inf.ethz.ch"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287539414, "id": "ICLR.cc/2017/conference/-/paper520/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ry_sjFqgx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper520/reviewers", "ICLR.cc/2017/conference/paper520/areachairs"], "cdate": 1485287539414}}}, {"tddate": null, "tmdate": 1484166116393, "tcdate": 1484166116393, "number": 3, "id": "HJnk4G48x", "invitation": "ICLR.cc/2017/conference/-/paper520/public/comment", "forum": "ry_sjFqgx", "replyto": "S1kUe2RHx", "signatures": ["~Pavol_Bielik1"], "readers": ["everyone"], "writers": ["~Pavol_Bielik1"], "content": {"title": "Response", "comment": "To address the main questions raised by the reviewers, we extended our submission with a new 7-page Appendix that precisely describes:\n\n\u2192  how a probabilistic model is obtained.\n\u2192  how programs are synthesised from data, and\n\u2192  formal semantics of TChar programs. \n\nWe believe this makes our approach self-contained and reproducible, but we are happy to add more details, if requested."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Program Synthesis for Character Level Language Modeling", "abstract": "We propose a statistical model applicable to character level language modeling and show that it is a good fit for both, program source code and English text. The model is parameterized by a program from a domain-specific language (DSL) that allows expressing non-trivial data dependencies. Learning is done in two phases: (i) we synthesize a program from the DSL, essentially learning a good representation for the data, and (ii) we learn parameters from the training data - the process is done via counting, as in simple language models such as n-gram.\n\nOur experiments show that the precision of our model is comparable to that of neural networks while sharing a number of advantages with n-gram models such as fast query time and the capability to quickly add and remove training data samples. Further, the model is parameterized by a program that can be manually inspected, understood and updated, addressing a major problem of neural networks.", "pdf": "/pdf/8433d48d1786dfa7e377e8d0f89dc984e7490117.pdf", "paperhash": "bielik|program_synthesis_for_character_level_language_modeling", "conflicts": ["inf.ethz.ch"], "keywords": [], "authors": ["Pavol Bielik", "Veselin Raychev", "Martin Vechev"], "authorids": ["pavol.bielik@inf.ethz.ch", "veselin.raychev@inf.ethz.ch", "martin.vechev@inf.ethz.ch"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287539414, "id": "ICLR.cc/2017/conference/-/paper520/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ry_sjFqgx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper520/reviewers", "ICLR.cc/2017/conference/paper520/areachairs"], "cdate": 1485287539414}}}, {"tddate": null, "tmdate": 1483812934940, "tcdate": 1483812934940, "number": 7, "id": "S1kUe2RHx", "invitation": "ICLR.cc/2017/conference/-/paper520/official/comment", "forum": "ry_sjFqgx", "replyto": "ry_sjFqgx", "signatures": ["ICLR.cc/2017/conference/paper520/areachair1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper520/areachair1"], "content": {"title": "Authors: Please post a response", "comment": "Currently this paper has a quite controversial score 8/8/3. Generally reviewers seem quite willing to consider a paper like this for ICLR, particularly due to the experimental results, but are concerned that there is very-little-to-zero details given about the synthesis approach. This makes the details of the paper hard to check and unreplicable. \n\nIf you feel like there is something you can do to sway them on this point, now would be a great time to respond. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Program Synthesis for Character Level Language Modeling", "abstract": "We propose a statistical model applicable to character level language modeling and show that it is a good fit for both, program source code and English text. The model is parameterized by a program from a domain-specific language (DSL) that allows expressing non-trivial data dependencies. Learning is done in two phases: (i) we synthesize a program from the DSL, essentially learning a good representation for the data, and (ii) we learn parameters from the training data - the process is done via counting, as in simple language models such as n-gram.\n\nOur experiments show that the precision of our model is comparable to that of neural networks while sharing a number of advantages with n-gram models such as fast query time and the capability to quickly add and remove training data samples. Further, the model is parameterized by a program that can be manually inspected, understood and updated, addressing a major problem of neural networks.", "pdf": "/pdf/8433d48d1786dfa7e377e8d0f89dc984e7490117.pdf", "paperhash": "bielik|program_synthesis_for_character_level_language_modeling", "conflicts": ["inf.ethz.ch"], "keywords": [], "authors": ["Pavol Bielik", "Veselin Raychev", "Martin Vechev"], "authorids": ["pavol.bielik@inf.ethz.ch", "veselin.raychev@inf.ethz.ch", "martin.vechev@inf.ethz.ch"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287539166, "id": "ICLR.cc/2017/conference/-/paper520/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "ry_sjFqgx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper520/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper520/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper520/reviewers", "ICLR.cc/2017/conference/paper520/areachairs"], "cdate": 1485287539166}}}, {"tddate": null, "tmdate": 1481958427712, "tcdate": 1481958427712, "number": 2, "id": "Sk4mNvGEe", "invitation": "ICLR.cc/2017/conference/-/paper520/official/review", "forum": "ry_sjFqgx", "replyto": "ry_sjFqgx", "signatures": ["ICLR.cc/2017/conference/paper520/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper520/AnonReviewer3"], "content": {"title": "Novel approach to language modeling", "rating": "8: Top 50% of accepted papers, clear accept", "review": "The authors propose a method for language modeling by first generating a program from a DSL, then learning the count-based parameters of that program. Pros include: The proposed method is innovative and highly different from standard LSTM-based approaches of late. The model should also be much quicker to apply at query time. Strong empirical results are obtained on modeling code, though there is some gap between the synthesis method and neural methods on the Hutter task. A detailed description of the language syntax is provided.\n\nCons/suggestions:\n- The synthesis procedure using MCMC is left very vague, even though being able to make this procedure efficient is one of the key questions.\n- The work builds on work from the PL literature; surely the related work could also be expanded and this work better put in context.\n- More compact/convincing examples of human interpretability would be helpful.\n\nOther comments\n- Training time evaluation in Table 1 should give basic information such as whether training was done on GPU/CPU, CPU specs, etc.\n", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Program Synthesis for Character Level Language Modeling", "abstract": "We propose a statistical model applicable to character level language modeling and show that it is a good fit for both, program source code and English text. The model is parameterized by a program from a domain-specific language (DSL) that allows expressing non-trivial data dependencies. Learning is done in two phases: (i) we synthesize a program from the DSL, essentially learning a good representation for the data, and (ii) we learn parameters from the training data - the process is done via counting, as in simple language models such as n-gram.\n\nOur experiments show that the precision of our model is comparable to that of neural networks while sharing a number of advantages with n-gram models such as fast query time and the capability to quickly add and remove training data samples. Further, the model is parameterized by a program that can be manually inspected, understood and updated, addressing a major problem of neural networks.", "pdf": "/pdf/8433d48d1786dfa7e377e8d0f89dc984e7490117.pdf", "paperhash": "bielik|program_synthesis_for_character_level_language_modeling", "conflicts": ["inf.ethz.ch"], "keywords": [], "authors": ["Pavol Bielik", "Veselin Raychev", "Martin Vechev"], "authorids": ["pavol.bielik@inf.ethz.ch", "veselin.raychev@inf.ethz.ch", "martin.vechev@inf.ethz.ch"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512555726, "id": "ICLR.cc/2017/conference/-/paper520/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper520/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper520/AnonReviewer2", "ICLR.cc/2017/conference/paper520/AnonReviewer3", "ICLR.cc/2017/conference/paper520/AnonReviewer1"], "reply": {"forum": "ry_sjFqgx", "replyto": "ry_sjFqgx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper520/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper520/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512555726}}}, {"tddate": null, "tmdate": 1481931023115, "tcdate": 1481931023115, "number": 1, "id": "HkPzKlz4x", "invitation": "ICLR.cc/2017/conference/-/paper520/official/review", "forum": "ry_sjFqgx", "replyto": "ry_sjFqgx", "signatures": ["ICLR.cc/2017/conference/paper520/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper520/AnonReviewer2"], "content": {"title": "Novel approach, good results", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper introduces a novel method for language modeling which is suitable for both modeling programming language as well as natural language. The approach uses a program synthesis algorithm to search over program space and uses count-based estimation of the weights of the program. This is a departure from neural network-based approaches which rely on gradient descent, and thus are extremely slow to estimate. Count-based method such as regular n-gram models suffer because of their simplicity, i.e. not being able to model large context, and scaling badly as context increases. The proposed approach synthesizes programs using MCMC which learn context-sensitive probabilities using count-based estimation, and thus is both fast and able to model long-range context.\n\nExperiments on a programming language datasets, the linux kernel corpus, show that this method is vastly better than both LSTM and n-gram language models. Experiments on the Wikipedia corpus show that the method is competitive, but not better, to SOTA models. Both estimation and query time are significantly better than LSTM LMs, and competitive to n-gram LMs.\n\nIt's debatable whether this paper is suitable for ICLR, due to ICLR's focus on neural network-based approaches. However, in the interest of diversity and novelty, such \"outside\" papers should be accepted to ICLR. This paper is likely to inspire more research into fusion of program synthesis and machine learning methods, which was a popular theme at NIPS 2016.\n\n*Pros*\n1. Novel approach.\n2. Good results.\n\n*Cons*\n1. Some significant algorithmic details are not included in the paper. They should at least be included in an appendix for comprehensiveness.\n\n*Comments*\n1. Please include n-gram results in the table for Wikipedia results.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Program Synthesis for Character Level Language Modeling", "abstract": "We propose a statistical model applicable to character level language modeling and show that it is a good fit for both, program source code and English text. The model is parameterized by a program from a domain-specific language (DSL) that allows expressing non-trivial data dependencies. Learning is done in two phases: (i) we synthesize a program from the DSL, essentially learning a good representation for the data, and (ii) we learn parameters from the training data - the process is done via counting, as in simple language models such as n-gram.\n\nOur experiments show that the precision of our model is comparable to that of neural networks while sharing a number of advantages with n-gram models such as fast query time and the capability to quickly add and remove training data samples. Further, the model is parameterized by a program that can be manually inspected, understood and updated, addressing a major problem of neural networks.", "pdf": "/pdf/8433d48d1786dfa7e377e8d0f89dc984e7490117.pdf", "paperhash": "bielik|program_synthesis_for_character_level_language_modeling", "conflicts": ["inf.ethz.ch"], "keywords": [], "authors": ["Pavol Bielik", "Veselin Raychev", "Martin Vechev"], "authorids": ["pavol.bielik@inf.ethz.ch", "veselin.raychev@inf.ethz.ch", "martin.vechev@inf.ethz.ch"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512555726, "id": "ICLR.cc/2017/conference/-/paper520/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper520/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper520/AnonReviewer2", "ICLR.cc/2017/conference/paper520/AnonReviewer3", "ICLR.cc/2017/conference/paper520/AnonReviewer1"], "reply": {"forum": "ry_sjFqgx", "replyto": "ry_sjFqgx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper520/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper520/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512555726}}}, {"tddate": null, "tmdate": 1481040796381, "tcdate": 1481040796373, "number": 2, "id": "rk4oXvVQx", "invitation": "ICLR.cc/2017/conference/-/paper520/public/comment", "forum": "ry_sjFqgx", "replyto": "Hk3LqNz7l", "signatures": ["~Pavol_Bielik1"], "readers": ["everyone"], "writers": ["~Pavol_Bielik1"], "content": {"title": "Answers", "comment": "Thanks for the questions. Please see our answers provided below:\n\nQ1: Could the authors provide and explain specific examples where this method yields interpretable behavior?\n\nA: Yes, by inspecting the learned program for the Linux kernel, also provided at http://www.srl.inf.ethz.ch/charmodel.html, we can see that the program initially learns to predict indentation at the beginning of each line (programs 67, 110), new lines and spaces (program 949), static constants (program 1250), first character of a word (program 275), end of line semicolons and pointer access (program 328) and the rest. These programs are then further refined (e.g., to predict whether the return value is void or a value).\n\nFor example, in label 110, we have:\nswitch LEFT LEFT WRITE: on \"10|92\" goto 74; on \"41\" goto 85; on \"44|58|123\" goto 86; on \"47\" goto 91; on \"59\" goto 100; on \"62\" goto 101; on \"125\" goto 105; else goto 109\nThe numbers in the quotes are ASCII values. We can see that the program reads a character that is two positions left of the current position and applies different models (goto \u2026) if it is new line (10) or \u201c/\u201d (92), closing bracket \u201c)\u201d (41) , etc.\n\nQ2: Are there experimental settings that differ from the experiments in Karpathy et al. 2015 for the Linux Kernel Dataset? The BPC corresponding to the CE loss that Karpathy et al. obtain seems significantly lower than in Table 1 in this paper.\n\nA: Using the publicly available implementation of Karpathy et.al. we were able to reproduce the LSTM results from their paper. However, these numbers differ from what we report as unfortunately, upon closer inspection of their implementation, we found what we believe is a bug that affects their results. The issue is that they do not split the dataset in the standard way which would be say: first 80% for training, next 10% for validation, and last 10% for testing. Instead, they draw the validation and training data periodically throughout the whole dataset: first 8 batches for training, next for validation, next for testing, next 8 again for training, etc. This is a problem as the split is not representative of how the model is queried in practice (e.g., with previously unseen code). We fixed the implementation so it uses a standard split, retrained their LSTM model, and reported the numbers. We also could not reproduce their n-gram baseline that had very similar CE loss compared to the LSTM/GRU models (in our experiments the n-gram was worse than LSTM). We contacted the authors to determine the issue, but they did not respond (they did respond to other inquiries, but not to this one).\n\nIn terms of notation, they report entropy in nats (also seen in their code) whereas we report entropy in bits. It is easy to convert nats to bits to match our results by multiplying their numbers by ~1.44.\n\nQ3:  What smoothing method was ultimately used for the n-gram benchmarks on the Linux Kernel Dataset?\n\nA: Our evaluation results use Witten-Bell smoothing everywhere. We note that KN has similar performance, but we did not include the numbers."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Program Synthesis for Character Level Language Modeling", "abstract": "We propose a statistical model applicable to character level language modeling and show that it is a good fit for both, program source code and English text. The model is parameterized by a program from a domain-specific language (DSL) that allows expressing non-trivial data dependencies. Learning is done in two phases: (i) we synthesize a program from the DSL, essentially learning a good representation for the data, and (ii) we learn parameters from the training data - the process is done via counting, as in simple language models such as n-gram.\n\nOur experiments show that the precision of our model is comparable to that of neural networks while sharing a number of advantages with n-gram models such as fast query time and the capability to quickly add and remove training data samples. Further, the model is parameterized by a program that can be manually inspected, understood and updated, addressing a major problem of neural networks.", "pdf": "/pdf/8433d48d1786dfa7e377e8d0f89dc984e7490117.pdf", "paperhash": "bielik|program_synthesis_for_character_level_language_modeling", "conflicts": ["inf.ethz.ch"], "keywords": [], "authors": ["Pavol Bielik", "Veselin Raychev", "Martin Vechev"], "authorids": ["pavol.bielik@inf.ethz.ch", "veselin.raychev@inf.ethz.ch", "martin.vechev@inf.ethz.ch"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287539414, "id": "ICLR.cc/2017/conference/-/paper520/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ry_sjFqgx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper520/reviewers", "ICLR.cc/2017/conference/paper520/areachairs"], "cdate": 1485287539414}}}, {"tddate": null, "tmdate": 1481040617285, "tcdate": 1481040617279, "number": 1, "id": "B1-x7PVQx", "invitation": "ICLR.cc/2017/conference/-/paper520/public/comment", "forum": "ry_sjFqgx", "replyto": "ByhJuwZXe", "signatures": ["~Pavol_Bielik1"], "readers": ["everyone"], "writers": ["~Pavol_Bielik1"], "content": {"title": "Answers", "comment": "Thanks for the questions. Please see our answers provided below:\n\nQ1a: It would be helpful if the paper had an example of the program. \n\nA: We provide a visualization of the learned program for the Linux Kernel dataset at http://www.srl.inf.ethz.ch/charmodel.html (link is also provided in the paper).\n\nQ1b: If I understand things correctly, after training is done, there is one program synthesized for the whole dataset?\n\nA: Yes, this is correct.\n\nQ2a: What is the synthesis algorithm used? \n\nA: The challenging part of the synthesis algorithm is finding SwitchPrograms that contain branches. The algorithm we use is based on decision tree learning and is described in detail in the following paper: Probabilistic model for code with decision trees. ACM OOPSLA\u201916:\nhttp://www.srl.inf.ethz.ch/papers/oopsla16-dt.pdf\n\nQ2b: How you estimate the cost of a program?\n\nA: For a given dataset, the cost of a program is the average bits-per-character (log likelihood of a given prediction) required by the language model specified by the program (evaluated on the validation set). This is also the metric that is minimized during training.\n\nQ2c: Are there any tricks to make the cost estimation fast?\n\nA: The cost estimation in our model is inherently fast as it is based on counting, as in simple language models such as n-gram, and requires only several hash lookups for each prediction (in addition to executing the learned program which is fast). Therefore it is enough to use a high performance hashtable implementation. We did not use any speed-up tricks to estimate the cost faster.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Program Synthesis for Character Level Language Modeling", "abstract": "We propose a statistical model applicable to character level language modeling and show that it is a good fit for both, program source code and English text. The model is parameterized by a program from a domain-specific language (DSL) that allows expressing non-trivial data dependencies. Learning is done in two phases: (i) we synthesize a program from the DSL, essentially learning a good representation for the data, and (ii) we learn parameters from the training data - the process is done via counting, as in simple language models such as n-gram.\n\nOur experiments show that the precision of our model is comparable to that of neural networks while sharing a number of advantages with n-gram models such as fast query time and the capability to quickly add and remove training data samples. Further, the model is parameterized by a program that can be manually inspected, understood and updated, addressing a major problem of neural networks.", "pdf": "/pdf/8433d48d1786dfa7e377e8d0f89dc984e7490117.pdf", "paperhash": "bielik|program_synthesis_for_character_level_language_modeling", "conflicts": ["inf.ethz.ch"], "keywords": [], "authors": ["Pavol Bielik", "Veselin Raychev", "Martin Vechev"], "authorids": ["pavol.bielik@inf.ethz.ch", "veselin.raychev@inf.ethz.ch", "martin.vechev@inf.ethz.ch"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287539414, "id": "ICLR.cc/2017/conference/-/paper520/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ry_sjFqgx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper520/reviewers", "ICLR.cc/2017/conference/paper520/areachairs"], "cdate": 1485287539414}}}, {"tddate": null, "tmdate": 1480899155637, "tcdate": 1480899155631, "number": 2, "id": "Hk3LqNz7l", "invitation": "ICLR.cc/2017/conference/-/paper520/pre-review/question", "forum": "ry_sjFqgx", "replyto": "ry_sjFqgx", "signatures": ["ICLR.cc/2017/conference/paper520/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper520/AnonReviewer3"], "content": {"title": "Questions", "question": "1. Could the authors provide and explain specific examples where this method yields interpretable behavior?\n2. Are there experimental settings that differ from the experiments in Karpathy et al. 2015 for the Linux Kernel Dataset? The BPC corresponding to the CE loss that Karpathy et al. obtain seems significantly lower than in Table 1 in this paper.\n3. What smoothing method was ultimately used for the n-gram benchmarks on the Linux Kernel Dataset?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Program Synthesis for Character Level Language Modeling", "abstract": "We propose a statistical model applicable to character level language modeling and show that it is a good fit for both, program source code and English text. The model is parameterized by a program from a domain-specific language (DSL) that allows expressing non-trivial data dependencies. Learning is done in two phases: (i) we synthesize a program from the DSL, essentially learning a good representation for the data, and (ii) we learn parameters from the training data - the process is done via counting, as in simple language models such as n-gram.\n\nOur experiments show that the precision of our model is comparable to that of neural networks while sharing a number of advantages with n-gram models such as fast query time and the capability to quickly add and remove training data samples. Further, the model is parameterized by a program that can be manually inspected, understood and updated, addressing a major problem of neural networks.", "pdf": "/pdf/8433d48d1786dfa7e377e8d0f89dc984e7490117.pdf", "paperhash": "bielik|program_synthesis_for_character_level_language_modeling", "conflicts": ["inf.ethz.ch"], "keywords": [], "authors": ["Pavol Bielik", "Veselin Raychev", "Martin Vechev"], "authorids": ["pavol.bielik@inf.ethz.ch", "veselin.raychev@inf.ethz.ch", "martin.vechev@inf.ethz.ch"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959234719, "id": "ICLR.cc/2017/conference/-/paper520/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper520/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper520/AnonReviewer2", "ICLR.cc/2017/conference/paper520/AnonReviewer3"], "reply": {"forum": "ry_sjFqgx", "replyto": "ry_sjFqgx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper520/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper520/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959234719}}}, {"tddate": null, "tmdate": 1480845284552, "tcdate": 1480845284548, "number": 1, "id": "ByhJuwZXe", "invitation": "ICLR.cc/2017/conference/-/paper520/pre-review/question", "forum": "ry_sjFqgx", "replyto": "ry_sjFqgx", "signatures": ["ICLR.cc/2017/conference/paper520/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper520/AnonReviewer2"], "content": {"title": "Questions", "question": "1. It would be helpful if the paper had an example of the program. An example would be much more helpful than the grammar specification. If I understand things correctly, after training is done, there is one program synthesized for the whole dataset? In that case, it would be good to include the synthesized program in the paper.\n2. What is the synthesis algorithm used? I understand that it's MCMC, but it would be useful to allocate some space to the algorithm. In particular, I'm also interested in knowing how you estimate the value of a program. Are there any tricks to make the value estimation fast?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Program Synthesis for Character Level Language Modeling", "abstract": "We propose a statistical model applicable to character level language modeling and show that it is a good fit for both, program source code and English text. The model is parameterized by a program from a domain-specific language (DSL) that allows expressing non-trivial data dependencies. Learning is done in two phases: (i) we synthesize a program from the DSL, essentially learning a good representation for the data, and (ii) we learn parameters from the training data - the process is done via counting, as in simple language models such as n-gram.\n\nOur experiments show that the precision of our model is comparable to that of neural networks while sharing a number of advantages with n-gram models such as fast query time and the capability to quickly add and remove training data samples. Further, the model is parameterized by a program that can be manually inspected, understood and updated, addressing a major problem of neural networks.", "pdf": "/pdf/8433d48d1786dfa7e377e8d0f89dc984e7490117.pdf", "paperhash": "bielik|program_synthesis_for_character_level_language_modeling", "conflicts": ["inf.ethz.ch"], "keywords": [], "authors": ["Pavol Bielik", "Veselin Raychev", "Martin Vechev"], "authorids": ["pavol.bielik@inf.ethz.ch", "veselin.raychev@inf.ethz.ch", "martin.vechev@inf.ethz.ch"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959234719, "id": "ICLR.cc/2017/conference/-/paper520/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper520/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper520/AnonReviewer2", "ICLR.cc/2017/conference/paper520/AnonReviewer3"], "reply": {"forum": "ry_sjFqgx", "replyto": "ry_sjFqgx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper520/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper520/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959234719}}}], "count": 14}