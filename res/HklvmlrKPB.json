{"notes": [{"id": "HklvmlrKPB", "original": "Hye84ExFPS", "number": 2214, "cdate": 1569439775015, "ddate": null, "tcdate": 1569439775015, "tmdate": 1577168278509, "tddate": null, "forum": "HklvmlrKPB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["jmarino@caltech.edu", "lei_chen_4@sfu.ca", "jiawei_he_2@sfu.ca", "stephan.mandt@gmail.com"], "title": "Improving Sequential Latent Variable Models with Autoregressive Flows", "authors": ["Joseph Marino", "Lei Chen", "Jiawei He", "Stephan Mandt"], "pdf": "/pdf/e62dd47e2346912a2e38bbd17d28bc8740db819c.pdf", "TL;DR": "We show how autoregressive flows can be used to improve sequential latent variable models.", "abstract": "We propose an approach for sequence modeling based on autoregressive normalizing flows. Each autoregressive transform, acting across time, serves as a moving reference frame for modeling higher-level dynamics. This technique provides a simple, general-purpose method for improving sequence modeling, with connections to existing and classical techniques. We demonstrate the proposed approach both with standalone models, as well as a part of larger sequential latent variable models. Results are presented on three benchmark video datasets, where flow-based dynamics improve log-likelihood performance over baseline models.", "code": "https://anonymous.4open.science/r/f02199f7-86d2-45ee-ad23-3f13f769ee10/", "keywords": ["Autoregressive Flows", "Sequence Modeling", "Latent Variable Models", "Video Modeling", "Variational Inference"], "paperhash": "marino|improving_sequential_latent_variable_models_with_autoregressive_flows", "original_pdf": "/attachment/2b5d4c96251209d9c3a87139cf8688d3e3590d51.pdf", "_bibtex": "@misc{\nmarino2020improving,\ntitle={Improving Sequential Latent Variable Models with Autoregressive Flows},\nauthor={Joseph Marino and Lei Chen and Jiawei He and Stephan Mandt},\nyear={2020},\nurl={https://openreview.net/forum?id=HklvmlrKPB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "L6uhbfuLi", "original": null, "number": 1, "cdate": 1576798743393, "ddate": null, "tcdate": 1576798743393, "tmdate": 1576800892816, "tddate": null, "forum": "HklvmlrKPB", "replyto": "HklvmlrKPB", "invitation": "ICLR.cc/2020/Conference/Paper2214/-/Decision", "content": {"decision": "Reject", "comment": "The paper scores low on novelty. The experiments and model analysis are not very strong.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jmarino@caltech.edu", "lei_chen_4@sfu.ca", "jiawei_he_2@sfu.ca", "stephan.mandt@gmail.com"], "title": "Improving Sequential Latent Variable Models with Autoregressive Flows", "authors": ["Joseph Marino", "Lei Chen", "Jiawei He", "Stephan Mandt"], "pdf": "/pdf/e62dd47e2346912a2e38bbd17d28bc8740db819c.pdf", "TL;DR": "We show how autoregressive flows can be used to improve sequential latent variable models.", "abstract": "We propose an approach for sequence modeling based on autoregressive normalizing flows. Each autoregressive transform, acting across time, serves as a moving reference frame for modeling higher-level dynamics. This technique provides a simple, general-purpose method for improving sequence modeling, with connections to existing and classical techniques. We demonstrate the proposed approach both with standalone models, as well as a part of larger sequential latent variable models. Results are presented on three benchmark video datasets, where flow-based dynamics improve log-likelihood performance over baseline models.", "code": "https://anonymous.4open.science/r/f02199f7-86d2-45ee-ad23-3f13f769ee10/", "keywords": ["Autoregressive Flows", "Sequence Modeling", "Latent Variable Models", "Video Modeling", "Variational Inference"], "paperhash": "marino|improving_sequential_latent_variable_models_with_autoregressive_flows", "original_pdf": "/attachment/2b5d4c96251209d9c3a87139cf8688d3e3590d51.pdf", "_bibtex": "@misc{\nmarino2020improving,\ntitle={Improving Sequential Latent Variable Models with Autoregressive Flows},\nauthor={Joseph Marino and Lei Chen and Jiawei He and Stephan Mandt},\nyear={2020},\nurl={https://openreview.net/forum?id=HklvmlrKPB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HklvmlrKPB", "replyto": "HklvmlrKPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795712537, "tmdate": 1576800261939, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2214/-/Decision"}}}, {"id": "HylbHoHSFS", "original": null, "number": 2, "cdate": 1571277624732, "ddate": null, "tcdate": 1571277624732, "tmdate": 1574198281541, "tddate": null, "forum": "HklvmlrKPB", "replyto": "HklvmlrKPB", "invitation": "ICLR.cc/2020/Conference/Paper2214/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "\nSummary \nThe paper proposes to combine the video modeling approaches based on autoregressive flows (e.g. Kumar\u201919) with amortized variational inference (e.g. Denton\u201918), wherein an autoregressive latent variable model optimized with variational inference is extended with an autoregressive flow that further transforms the output of the latent variable model while allowing to compute exact conditional probability. This is motivated with a physical intuition, where a dynamics model can benefit from decorrelating the inputs, and it is demonstrated that layers of autoregressive flows can represent derivatives of the original signal. In a proof-of-concept experiment, it is shown that using a layer of autoregressive flow improves NLL of a latent variable model.\n\nDecision\nThe paper presents an interesting method and tackles an important problem. At the same time, the properties of the proposed method are not well exposed and the experimental evaluation is incomplete. Moreover, the motivation of the paper is confusingly disconnected from the proposed model. I rate this paper as borderline, but am hopeful that some of the issues will be clarified during the discussion period.\n\nPros\n- The paper is well-motivated and tackles a significant problem.\n- The proposed method is novel.\n- The paper is well-written.\n\nCons\n- The experimental evaluation is incomplete and does not expose the properties of the method fully. Comparisons to prior art are missing. (see below)\n- The motivation is disconnected from the proposed model. The introduction of the paper motivates a model that hierarchically decorrelates a sequence of frames to arrive at a fully factorized model, which is later motivated with a physical example. However, the method proposed in the paper is instead a single layer of autoregressive flow on top of a powerful latent variable model! This is expressed in the title, but only glossed over in the abstract and introduction. The writing has to be updated to coherently focus on the contribution of the paper. \n\nQuestions (ordered by decreasing importance)\n1. In table 1, quantitative results are reported for the introduced methods. It is shown that introducing autoregressive flows achieves better likelihood and better generalization. However, quantitative comparisons with published methods that were evaluated on these datasets are missing, such as Denton\u201918 and Kumar\u201919. A quick calculation shows that Kumar et al. achieves a log-likelihood of -0.43 in Table 1 when converted to this paper\u2019s metric, although it is possible my conversion is incorrect. Is the presented model competitive with previously published results? \n2. No qualitative generation results are presented. Since the model achieves a high likelihood it is likely to do well on one-frame prediction, and possibly would even work on autoregressive multi-step prediction. Is the model capable of generation of diverse and plausible video?\n3. The paper has a lengthy section 3.1 that convincingly explains that decorrelating latent variables in time is important for sequence modeling. However the proposed approach in fact produces latents that are correlated in time! Since the prior over latent variables is conditioned on past frames, the model can in fact learn a correlated representation and still achieve optimal likelihood. Moreover, the position of both the digit and the robot arm could be seen in what should be the decorrelated image in Fig 4. Is there solid quantitative (or even qualitative) evidence that the model learns a \u2018more decorrelated\u2019 representation beyond the fact that it copies the background and that the likelihood improves? The evaluation in this paper does not convince me that the model learns a temporally decorrelated representation.\n4. Were modern techniques beyond affine flows considered, such as from Kingma\u201918, Kumar\u201919? Two layers of affine flows are likely insufficient to model the complexity of these data, which makes the comparison to the purely flow-based models somewhat unfair.\n5. It is stated that the paper is \u201cthe first to demonstrate flows across time steps for video data\u201d, however, the related work by Kumar et al. proposes a somewhat similar model in which conditional flows are used to model video data. Do Kumar et al. not \u201cdemonstrate flows across time steps\u201d?\n\nMinor comments\n1. Eq (10) and (12) seem to be inconsistent. Perhaps x_t = x_t-1 + u_t-1 was meant in eq (10)?\n2. Line before eq(14): it not true that u_t-1 = x_t-1 - x_t-2. It would be true if the deterministic x_t = x_t-1 + u_t-1 model was assumed instead of the gaussian N(x_t; x_t-1 + u_t-1, Sigma). It is possible that eq(14) is still correct as the variance of Gaussians is additive.\n3. The following work uses autoregressive flows for modeling temporal dynamics and should be cited: Rhinehart\u201918,19\n\nRhinehart et al, Deep Imitative Models for Flexible Inference, Planning, and Control\nRhinehart et al, PRECOG: PREdiction Conditioned On Goals in Visual Multi-Agent Settings\n\n--------------------- Update 11.19 -----------------------\nThe newly provided experiments support some of the claims of the paper. In particular, I appreciate the plot showing that the proposed method successfully learns a more decorrelated representation over time, and the provided qualitative samples from the model. The authors also clarified my questions about motivation. At the same time, the proposed method is not shown to compare well to state-of-the-art approaches. I am leaning towards accepting the paper, but I believe the method would have a much larger impact if its properties were more fully exposed.\n\n== comparison with Denton&Fergus'18 (SVG) ==\nWhen trained with beta=1, as the authors suggest for comparison, this method is known to perform poorly. There are two possible ways of alleviating this: 1) to train with the modified objective as in the paper but evaluate the true lower bound on the likelihood, or 2) interpret the beta as the fixed variance of the decoder distribution. Given the results the authors have provided, I believe the latter option will lead to SVG outperforming the proposed approach.\n\n== Correlation plot == \nThanks for performing this experiment! While measuring correlation only captures linear dependencies, which is likely mostly the background image, this plot shows that the model indeed learns to (linearly) decorrelate the frames in the sequence. \n\n== Samples == \nThanks for providing samples from the model! While the performance on BAIR is not quite convincing, the MNIST samples look very good.\n\n= Kumar et al. comparison ==\nThe author's response convinces me that the proposed model is significantly different from Kumar et al. in scope, as Kumar et al simply use a per-frame normalizing flow encoder coupled with a sequential prior.\n\n== eqs. 10, 12 ==\nThe authors' response cleared my confusion, the equations are correct.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2214/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2214/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jmarino@caltech.edu", "lei_chen_4@sfu.ca", "jiawei_he_2@sfu.ca", "stephan.mandt@gmail.com"], "title": "Improving Sequential Latent Variable Models with Autoregressive Flows", "authors": ["Joseph Marino", "Lei Chen", "Jiawei He", "Stephan Mandt"], "pdf": "/pdf/e62dd47e2346912a2e38bbd17d28bc8740db819c.pdf", "TL;DR": "We show how autoregressive flows can be used to improve sequential latent variable models.", "abstract": "We propose an approach for sequence modeling based on autoregressive normalizing flows. Each autoregressive transform, acting across time, serves as a moving reference frame for modeling higher-level dynamics. This technique provides a simple, general-purpose method for improving sequence modeling, with connections to existing and classical techniques. We demonstrate the proposed approach both with standalone models, as well as a part of larger sequential latent variable models. Results are presented on three benchmark video datasets, where flow-based dynamics improve log-likelihood performance over baseline models.", "code": "https://anonymous.4open.science/r/f02199f7-86d2-45ee-ad23-3f13f769ee10/", "keywords": ["Autoregressive Flows", "Sequence Modeling", "Latent Variable Models", "Video Modeling", "Variational Inference"], "paperhash": "marino|improving_sequential_latent_variable_models_with_autoregressive_flows", "original_pdf": "/attachment/2b5d4c96251209d9c3a87139cf8688d3e3590d51.pdf", "_bibtex": "@misc{\nmarino2020improving,\ntitle={Improving Sequential Latent Variable Models with Autoregressive Flows},\nauthor={Joseph Marino and Lei Chen and Jiawei He and Stephan Mandt},\nyear={2020},\nurl={https://openreview.net/forum?id=HklvmlrKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HklvmlrKPB", "replyto": "HklvmlrKPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2214/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2214/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575832749418, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2214/Reviewers"], "noninvitees": [], "tcdate": 1570237726068, "tmdate": 1575832749429, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2214/-/Official_Review"}}}, {"id": "r1eoA5cssB", "original": null, "number": 4, "cdate": 1573788371086, "ddate": null, "tcdate": 1573788371086, "tmdate": 1573788371086, "tddate": null, "forum": "HklvmlrKPB", "replyto": "HyejGEhEFS", "invitation": "ICLR.cc/2020/Conference/Paper2214/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "Thank you for your comments! Here, we will attempt to address additional specific points:\n\n\u201cIs the claimed contribution new methodology for modeling sequences? In my opinion, using flows as VAE decoders, or adding latent variables to a flow model and training it variationally, are standard applications of existing techniques and I wouldn't consider them particularly novel.\u201d\n\nAs mentioned, flows and VAE models have been combined in various ways (though, in our opinion, this is still under-explored), and we do not claim to introduce this combination. While affine autoregressive flows are a popular class of flow-based models, to the best of our knowledge, the application of these flows across time steps for the purposes of simplifying video modeling is novel. Specifically, our main contribution is identifying flows as a useful technique for pre-processing sequences to simplify downstream modeling.\n\n\u201cIs the claimed contribution improved modeling performance? The main results are that (a) replacing Gaussian decoders with autoregressive flows improves performance, and (b) adding latent variables to the base distribution of an affine autoregressive flow also improves performance. Both of these results are exactly what one would expect from our experience with these methods.\u201d\n\nImproved modeling performance is one of the results of our method. It does seem that including more flexible model components should obviously improve performance. However, from the perspective from a sequential latent variable model, it is unclear where to incorporate dynamics. For example, one could include more latent variables or recurrent networks at various stages. We specifically propose using autoregressive flows as a type of \u2018pre-processing\u2019 stage, resulting in a new sequence with dynamics that are hopefully easier to model. In our experiments, we attempt to control for the complexity of each model.\n\n\u201cIs the claimed contribution useful representations?\u201d\n\nThis is not something that we investigated in this paper, although we intend to investigate this more thoroughly. \n\n\u201cEq. (8): As written, the expression makes little sense as \\sigma is a vector. I understand that there is supposed to be a sum over the elements of log\\sigma, so I'd suggest expressing that more clearly.\u201d\n\nYou are correct. This has been updated in the submission. Thank you!\n\n\u201cEq. (9): It seems to me that the last Jacobian is upside down.\u201d\n\nIndeed. Thanks again!\n\n\u201cIn the particle analogy of the motivating example of section 3.1, it would be good to say explicitly that x is the position, u is the velocity and w is the force, to make the example even more intuitive.\u201d\n\nWe have stated this in the updated submission.\n\n\u201cThe paper only considers affine autoregressive flows, but there has been a lot of recent work on non-affine autoregressive flows that are more expressive\u2026At the very least, it would be good to discuss them as more flexible alternatives.\u201d\n\nWe have included a discussion of these non-affine flows in the updated submission. We chose affine flows for their relative simplicity while still yielding reasonable performance. \n\n\u201cIn section 3.2, a third and very significant limitation of the flows discussed here is that they act elementwise on the dimensions (e.g. pixels) of y_t.\u201d\n\nWe have stated this limitation more specifically. However, for the purposes of removing correlations across time, rather than space, they are useful.\n\n\u201cIn the experimental section, it would be good to describe on a high level what the architecture of the VAE is, especially the architecture of the prior and the encoder, and the types of distributions used there (e.g. diagonal Gaussians or otherwise).\u201d\n\nWe have included a more thorough discussion of the model architectures, as well as diagrams in Appendix B.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2214/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper2214/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2214/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jmarino@caltech.edu", "lei_chen_4@sfu.ca", "jiawei_he_2@sfu.ca", "stephan.mandt@gmail.com"], "title": "Improving Sequential Latent Variable Models with Autoregressive Flows", "authors": ["Joseph Marino", "Lei Chen", "Jiawei He", "Stephan Mandt"], "pdf": "/pdf/e62dd47e2346912a2e38bbd17d28bc8740db819c.pdf", "TL;DR": "We show how autoregressive flows can be used to improve sequential latent variable models.", "abstract": "We propose an approach for sequence modeling based on autoregressive normalizing flows. Each autoregressive transform, acting across time, serves as a moving reference frame for modeling higher-level dynamics. This technique provides a simple, general-purpose method for improving sequence modeling, with connections to existing and classical techniques. We demonstrate the proposed approach both with standalone models, as well as a part of larger sequential latent variable models. Results are presented on three benchmark video datasets, where flow-based dynamics improve log-likelihood performance over baseline models.", "code": "https://anonymous.4open.science/r/f02199f7-86d2-45ee-ad23-3f13f769ee10/", "keywords": ["Autoregressive Flows", "Sequence Modeling", "Latent Variable Models", "Video Modeling", "Variational Inference"], "paperhash": "marino|improving_sequential_latent_variable_models_with_autoregressive_flows", "original_pdf": "/attachment/2b5d4c96251209d9c3a87139cf8688d3e3590d51.pdf", "_bibtex": "@misc{\nmarino2020improving,\ntitle={Improving Sequential Latent Variable Models with Autoregressive Flows},\nauthor={Joseph Marino and Lei Chen and Jiawei He and Stephan Mandt},\nyear={2020},\nurl={https://openreview.net/forum?id=HklvmlrKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HklvmlrKPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2214/Authors", "ICLR.cc/2020/Conference/Paper2214/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2214/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2214/Reviewers", "ICLR.cc/2020/Conference/Paper2214/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2214/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2214/Authors|ICLR.cc/2020/Conference/Paper2214/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144668, "tmdate": 1576860536108, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2214/Authors", "ICLR.cc/2020/Conference/Paper2214/Reviewers", "ICLR.cc/2020/Conference/Paper2214/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2214/-/Official_Comment"}}}, {"id": "rkepiqqsir", "original": null, "number": 3, "cdate": 1573788325483, "ddate": null, "tcdate": 1573788325483, "tmdate": 1573788325483, "tddate": null, "forum": "HklvmlrKPB", "replyto": "HylbHoHSFS", "invitation": "ICLR.cc/2020/Conference/Paper2214/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "Thank you for your comments! Here, we will attempt to address additional specific points:\n\n\u201cThe paper has a lengthy section 3.1 that convincingly explains that decorrelating latent variables in time is important for sequence modeling. However the proposed approach in fact produces latents that are correlated in time!\u2026 Is there solid quantitative (or even qualitative) evidence that the model learns a \u2018more decorrelated\u2019 representation\u201d\n\nIt should be noted that while these flows have the capability of removing temporal correlations, they may not be able to remove all temporal dependencies. Thus, it can still be beneficial to model these dependencies in the base distribution of the flow. The motivation is not that we want to remove all temporal dependencies, but rather that we would like to remove as much as possible to simplify modeling for the sequential latent variable model. Based on your suggestion, we have provided a quantitative confirmation that the result of the flow is less temporally correlated than the input.\n\n\u201cWere modern techniques beyond affine flows considered, such as from Kingma\u201918, Kumar\u201919? Two layers of affine flows are likely insufficient to model the complexity of these data, which makes the comparison to the purely flow-based models somewhat unfair.\u201d\n\nIt is important to note that we are applying flows across time steps, rather than within a time step. If we were to apply a method like GLOW (which is affine) in an analogous way, this would involve applying the flow on half the time steps as a function of the other half of the steps. Methods like VideoFlow apply flows within time steps. While this may further improve performance by removing spatial correlations, this is not the motivation of our work. Many flows are part of the general family of affine flows (e.g. NICE, RealNVP, IAF, MAF, GLOW), so we felt this was an important place to start in developing this technique. We included comparisons with standalone flow-based models to demonstrate that these models work well as generative models on their own. Note that a single affine autoregressive flow is exactly equivalent to an autoregressive model, which can perform quite well in practice.\n\n\u201cDo Kumar et al. not \u201cdemonstrate flows across time steps\u201d?\u201d\n\nWhile Kumar et al. do apply flows within a sequential context, we mean to distinguish between applying flows within a time step vs. across time steps, as in our work. Kumar et al. use non-flow-based models to model temporal dependencies. Other works, such as van den Oord et al. with audio data, do use flows across time steps, as we do here.\n\n\u201cEq (10) and (12) seem to be inconsistent. Perhaps x_t = x_t-1 + u_t-1 was meant in eq (10)?\u201d\n\nWe understand the point of confusion, however, Eqs. 10 and 12 are consistent. In Eq. 10, x_t = x_t-1 + u_t gives the exact value of x_t, but in Eq. 12, x_t-1 + u_t-1 gives the mean of the Gaussian distribution over x_t. This can be seen by plugging the random variable for u_t, i.e. Eq. 13, into Eq. 10.\n\n\u201cLine before eq(14): it not true that u_t-1 = x_t-1 - x_t-2. It would be true if the deterministic x_t = x_t-1 + u_t-1 model was assumed instead of the gaussian N(x_t; x_t-1 + u_t-1, Sigma). It is possible that eq(14) is still correct as the variance of Gaussians is additive.\u201d\n\nThis follows directly from the definition of u. To be clear, x and u are simply different ways of expressing the same randomness, subject to different offset values. This is because the transform between u_t and x_t is deterministic. All of the stochasticity originates from w_t.\n\n\u201cThe following work uses autoregressive flows for modeling temporal dynamics and should be cited: Rhinehart\u201918,19\u201d\n\nThank you for these references. We have included them in the updated draft."}, "signatures": ["ICLR.cc/2020/Conference/Paper2214/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper2214/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2214/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jmarino@caltech.edu", "lei_chen_4@sfu.ca", "jiawei_he_2@sfu.ca", "stephan.mandt@gmail.com"], "title": "Improving Sequential Latent Variable Models with Autoregressive Flows", "authors": ["Joseph Marino", "Lei Chen", "Jiawei He", "Stephan Mandt"], "pdf": "/pdf/e62dd47e2346912a2e38bbd17d28bc8740db819c.pdf", "TL;DR": "We show how autoregressive flows can be used to improve sequential latent variable models.", "abstract": "We propose an approach for sequence modeling based on autoregressive normalizing flows. Each autoregressive transform, acting across time, serves as a moving reference frame for modeling higher-level dynamics. This technique provides a simple, general-purpose method for improving sequence modeling, with connections to existing and classical techniques. We demonstrate the proposed approach both with standalone models, as well as a part of larger sequential latent variable models. Results are presented on three benchmark video datasets, where flow-based dynamics improve log-likelihood performance over baseline models.", "code": "https://anonymous.4open.science/r/f02199f7-86d2-45ee-ad23-3f13f769ee10/", "keywords": ["Autoregressive Flows", "Sequence Modeling", "Latent Variable Models", "Video Modeling", "Variational Inference"], "paperhash": "marino|improving_sequential_latent_variable_models_with_autoregressive_flows", "original_pdf": "/attachment/2b5d4c96251209d9c3a87139cf8688d3e3590d51.pdf", "_bibtex": "@misc{\nmarino2020improving,\ntitle={Improving Sequential Latent Variable Models with Autoregressive Flows},\nauthor={Joseph Marino and Lei Chen and Jiawei He and Stephan Mandt},\nyear={2020},\nurl={https://openreview.net/forum?id=HklvmlrKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HklvmlrKPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2214/Authors", "ICLR.cc/2020/Conference/Paper2214/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2214/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2214/Reviewers", "ICLR.cc/2020/Conference/Paper2214/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2214/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2214/Authors|ICLR.cc/2020/Conference/Paper2214/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144668, "tmdate": 1576860536108, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2214/Authors", "ICLR.cc/2020/Conference/Paper2214/Reviewers", "ICLR.cc/2020/Conference/Paper2214/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2214/-/Official_Comment"}}}, {"id": "r1eNXq5ioH", "original": null, "number": 1, "cdate": 1573788188369, "ddate": null, "tcdate": 1573788188369, "tmdate": 1573788289718, "tddate": null, "forum": "HklvmlrKPB", "replyto": "HklvmlrKPB", "invitation": "ICLR.cc/2020/Conference/Paper2214/-/Official_Comment", "content": {"title": "Response to All Reviewers", "comment": "We would like to thank the reviewers for their feedback; we found their comments insightful and constructive. We were also happy to see that the reviewers found the idea \u2018crystal clear.\u2019 The draft has been updated to reflect their comments. We will attempt to address common points in this post, with separate comments to each reviewer addressing specific points.\n\n\u2014Comparison with prior work\n\nThe motivation behind our work is to provide a general-purpose technique for improving sequence modeling. To that end, we are not proposing a specific model architecture, instead focusing on relative improvements over a representative video modeling architecture. The sequential latent variable model architecture that we used for conducting experiments is a fairly standard convolutional encoder-decoder architecture with fully-connected latent variables. This architecture resembles previous works like world models (Ha & Schmidhuber, 2018) and SVG (Denton & Fergus, 2018).\n\nHowever, the difficulty in comparing video modeling performance is that many previous works employ a variety of custom techniques. For instance, many previous works do not train or evaluate their models with proper log-likelihood (or lower bound) objectives, e.g. Ha & Schmidhuber, 2018 and Denton & Fergus, 2018 both down-weight the KL term in the objective, yielding an improper lower bound. These previous works also evaluate squared pixel error, implicitly setting the std. dev. to 1. Indeed, when SVG is trained with a variational bound, the results are comparable with our reported results, e.g. -2.86 nats/dim (SVG) vs. -2.39 nats/dim (ours) on KTH Actions (Marino et al., 2018). Other works, like Hafner et al., 2019 restrict the bit-depth of the images, yielding log-likelihood results that are not directly comparable with ours. Still other works, like SAVP (Lee et al., 2018), apply combinations of lower bound and adversarial losses. To our knowledge, the most similar recent work to evaluate log-likelihood performance is VideoFlow (Kumar et al., 2019). While their quoted performance is significantly higher than our models, their models are also substantially larger, consisting of 3 levels of latent variables, with 24 steps of flows between each level. In contrast, we use 1 or 2 levels of latent variables, we only 1 or 2 steps of flow. We have noted this in the updated draft.\n\nWe focused on log-likelihood as a metric of model performance, choosing video data for its ability to visualize aspects of autoregressive flows. Importantly, quantitative and qualitative metrics of images are not always well-aligned (Theis et al., 2015). Indeed, Kumar et al., 2019 note that there is only a weak correlation. While we agree that employing the aforementioned techniques to improve qualitative metrics is a useful direction, we felt it was more important to establish performance improvements on a clear quantitative basis. We intend to run an even more comprehensive set of experiments before the camera-ready submission deadline to investigate these possible improvements.\n\n\u2014Main contribution and related work\n\nWe have updated the background section to include the references suggested by the reviewers, as well as to clarify the main contribution of the work relative to these previous works. In our updated draft, we clarify that \u201cwe demonstrate that autoregressive flows can serve as a useful, general-purpose technique for improving sequence modeling as components of sequential latent variable models. To the best of our knowledge, our work is the first to focus on the aspect of using flows to pre-process sequential data to improve downstream dynamics modeling.\u201d While Kumar et al., 2019 apply flows to video data, they do so by applying flows separately within each time step. Their process attempts to remove spatial correlations to get to the base distribution. We, instead, apply flows across time steps, processing the current frame based on previous frames. This process attempts to remove temporal correlations.\n\n\u2014Quantitative evaluation of decorrelation\n\nWe have updated the draft with an additional quantitative analysis of the temporal correlation. Indeed, we find that temporal correlation decreases in all three cases, corroborating the qualitative results. We also present a plot in Appendix C showing this temporal correlation decreasing during training, suggesting that these flows gradually learn a simplified basis for downstream dynamics estimation. We would like to thank R2 for this useful suggestion.\n\n\u2014Generated samples\n\nWe have included sets of generated samples in Appendix C. While these samples do not remain sharp over long temporal horizons, they capture backgrounds reasonably well. As noted above, we did not employ the range of techniques used to improve sample generation in video modeling, instead focusing on quantitative metrics. We intend to run additional experiments to improve sample generation quality, such as adjusting sampling temperature (Kumar et al., 2019)."}, "signatures": ["ICLR.cc/2020/Conference/Paper2214/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper2214/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2214/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jmarino@caltech.edu", "lei_chen_4@sfu.ca", "jiawei_he_2@sfu.ca", "stephan.mandt@gmail.com"], "title": "Improving Sequential Latent Variable Models with Autoregressive Flows", "authors": ["Joseph Marino", "Lei Chen", "Jiawei He", "Stephan Mandt"], "pdf": "/pdf/e62dd47e2346912a2e38bbd17d28bc8740db819c.pdf", "TL;DR": "We show how autoregressive flows can be used to improve sequential latent variable models.", "abstract": "We propose an approach for sequence modeling based on autoregressive normalizing flows. Each autoregressive transform, acting across time, serves as a moving reference frame for modeling higher-level dynamics. This technique provides a simple, general-purpose method for improving sequence modeling, with connections to existing and classical techniques. We demonstrate the proposed approach both with standalone models, as well as a part of larger sequential latent variable models. Results are presented on three benchmark video datasets, where flow-based dynamics improve log-likelihood performance over baseline models.", "code": "https://anonymous.4open.science/r/f02199f7-86d2-45ee-ad23-3f13f769ee10/", "keywords": ["Autoregressive Flows", "Sequence Modeling", "Latent Variable Models", "Video Modeling", "Variational Inference"], "paperhash": "marino|improving_sequential_latent_variable_models_with_autoregressive_flows", "original_pdf": "/attachment/2b5d4c96251209d9c3a87139cf8688d3e3590d51.pdf", "_bibtex": "@misc{\nmarino2020improving,\ntitle={Improving Sequential Latent Variable Models with Autoregressive Flows},\nauthor={Joseph Marino and Lei Chen and Jiawei He and Stephan Mandt},\nyear={2020},\nurl={https://openreview.net/forum?id=HklvmlrKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HklvmlrKPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2214/Authors", "ICLR.cc/2020/Conference/Paper2214/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2214/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2214/Reviewers", "ICLR.cc/2020/Conference/Paper2214/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2214/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2214/Authors|ICLR.cc/2020/Conference/Paper2214/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144668, "tmdate": 1576860536108, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2214/Authors", "ICLR.cc/2020/Conference/Paper2214/Reviewers", "ICLR.cc/2020/Conference/Paper2214/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2214/-/Official_Comment"}}}, {"id": "ryeqOccosH", "original": null, "number": 2, "cdate": 1573788274029, "ddate": null, "tcdate": 1573788274029, "tmdate": 1573788274029, "tddate": null, "forum": "HklvmlrKPB", "replyto": "Skld8grnYB", "invitation": "ICLR.cc/2020/Conference/Paper2214/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "Thank you for your comments! Here, we will attempt to address additional specific points:\n\n\u201cthe paper misses broader performance comparisons against other state of the art models, in particular videoflow which is quite related to the models introduced in this paper.\u201d\n\nAs discussed in our common response, our goal was not to propose a specific video-modeling architecture, but rather to propose a technique for improving sequence modeling. VideoFlow applies flows within each time step, unlike our proposed technique, which operates across time steps. VideoFlow is also significantly larger than the models that we investigated, consisting of 3 levels of latent variables, each with 24 steps of flow, and each flow containing 5 residual blocks. In contrast, the models in our experiments consist of just one or two flows, with each component of our models parameterized with relatively simple convolutional or recurrent networks. As stated in our common response, our quantitative results are on-par with log-likelihood estimates for previous works, like SVG (Denton & Fergus, 2018).\n\n\u201cWhat would happen if we used the same trick of modeling the conditional likelihood in this way in other SOTA models?\u201d\n\nWe chose a representative sequential latent variable model for our experiments. However, we suspect this technique will apply broadly to many sequence modeling settings. Indeed, as we noted in our submission, VideoFlow models differences in variables, which we discuss as a special case of our technique. Before the camera-ready deadline, we intend to conduct additional experiments applying our technique to some of these previously proposed models.\n\n\u201cwhat are the computational requirements of the models presented in this paper?\u201d\n\nAutoregressive flows, in the sequential context, add only a constant computational cost to each time step, requiring only a single forward pass for evaluation and generation."}, "signatures": ["ICLR.cc/2020/Conference/Paper2214/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper2214/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2214/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jmarino@caltech.edu", "lei_chen_4@sfu.ca", "jiawei_he_2@sfu.ca", "stephan.mandt@gmail.com"], "title": "Improving Sequential Latent Variable Models with Autoregressive Flows", "authors": ["Joseph Marino", "Lei Chen", "Jiawei He", "Stephan Mandt"], "pdf": "/pdf/e62dd47e2346912a2e38bbd17d28bc8740db819c.pdf", "TL;DR": "We show how autoregressive flows can be used to improve sequential latent variable models.", "abstract": "We propose an approach for sequence modeling based on autoregressive normalizing flows. Each autoregressive transform, acting across time, serves as a moving reference frame for modeling higher-level dynamics. This technique provides a simple, general-purpose method for improving sequence modeling, with connections to existing and classical techniques. We demonstrate the proposed approach both with standalone models, as well as a part of larger sequential latent variable models. Results are presented on three benchmark video datasets, where flow-based dynamics improve log-likelihood performance over baseline models.", "code": "https://anonymous.4open.science/r/f02199f7-86d2-45ee-ad23-3f13f769ee10/", "keywords": ["Autoregressive Flows", "Sequence Modeling", "Latent Variable Models", "Video Modeling", "Variational Inference"], "paperhash": "marino|improving_sequential_latent_variable_models_with_autoregressive_flows", "original_pdf": "/attachment/2b5d4c96251209d9c3a87139cf8688d3e3590d51.pdf", "_bibtex": "@misc{\nmarino2020improving,\ntitle={Improving Sequential Latent Variable Models with Autoregressive Flows},\nauthor={Joseph Marino and Lei Chen and Jiawei He and Stephan Mandt},\nyear={2020},\nurl={https://openreview.net/forum?id=HklvmlrKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HklvmlrKPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2214/Authors", "ICLR.cc/2020/Conference/Paper2214/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2214/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2214/Reviewers", "ICLR.cc/2020/Conference/Paper2214/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2214/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2214/Authors|ICLR.cc/2020/Conference/Paper2214/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144668, "tmdate": 1576860536108, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2214/Authors", "ICLR.cc/2020/Conference/Paper2214/Reviewers", "ICLR.cc/2020/Conference/Paper2214/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2214/-/Official_Comment"}}}, {"id": "HyejGEhEFS", "original": null, "number": 1, "cdate": 1571238931385, "ddate": null, "tcdate": 1571238931385, "tmdate": 1572972368147, "tddate": null, "forum": "HklvmlrKPB", "replyto": "HklvmlrKPB", "invitation": "ICLR.cc/2020/Conference/Paper2214/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary:\n\nThe paper discusses ways to use autoregressive flows in sequence modelling. Two main variants are considered:\n(a) An affine autoregressive flow directly modelling the data.\n(b) An affine autoregressive flow whose base distribution is a sequential VAE; equivalently, a sequential VAE whose decoder is an affine autoregressive flow.\n\nPros:\n\nThe paper is very well written and crystal clear. I particularly appreciated the motivating example that shows how each layer of an affine autoregressive flow reduces the order of a linear dynamical system by 1, and the connections with modelling temporal changes and moving reference frames.\n\nThe methods are technically correct and well-motivated. The experiments are done well.\n\nOverall, the paper scores high on writing and technical quality.\n\nCons:\n\nIn my opinion, the paper scores low on novelty and original contribution.\n\nIn general, it's not clear to me what the claimed contribution is. More specifically:\n\nIs the claimed contribution new methodology for modelling sequences? In my opinion, using flows as VAE decoders, or adding latent variables to a flow model and training it variationally, are standard applications of existing techniques and I wouldn't consider them particularly novel.\n\nIs the claimed contribution improved modelling performance? The main results are that (a) replacing Gaussian decoders with autoregressive flows improves performance, and (b) adding latent variables to the base distribution of an affine autoregressive flow also improves performance. Both of these results are exactly what one would expect from our experience with these methods. Other than that, the paper doesn't present any results that indicate the particular models used enable us to do things we couldn't do before, or improve against the state of the art in sequence modelling.\n\nIs the claimed contribution useful representations? The motivation for using the flow in this particular way as a VAE decoder is that the flow will model low-level correlations whereas the latent variables will capture high-level dynamics. However, the experiments (e.g. the visualizations) don't support this claim, and the usefulness of the learned representations hasn't been demonstrated in an alternative way,\n\nDecision:\n\nEven though the paper is technically correct and well written, my decision is weak reject because of the lack of novelty and original contribution.\n\nSuggestions for improvement:\n\nMy main suggestion to the authors is to keep up the good work, but also reflect on what the specific contribution of the paper is, and try to make a stronger case for it. Some minor suggestions/corrections follow:\n\nEq. (8): As written, the expression makes little sense as \\sigma is a vector. I understand that there is supposed to be a sum over the elements of log\\sigma, so I'd suggest expressing that more clearly.\n\nEq. (9): It seems to me that the last Jacobian is upside down.\n\nIn general, it would be good to be more thorough on how this paper is similar to related work and how it differs. There is also this related work which may be good to discuss:\n\nLatent Normalizing Flows for Discrete Sequences, https://arxiv.org/abs/1901.10548\n\nIn the particle analogy of the motivating example of section 3.1, it would be good to say explicitly that x is the position, u is the velocity and w is the force, to make the example even more intuitive.\n\nThe paper only considers affine autoregressive flows, but there has been a lot of recent work on non-affine autoregressive flows that are more expressive, for example:\n\nNeural Autoregressive Flows, https://arxiv.org/abs/1804.00779\nSum-Of-Squares Polynomial Flow, https://arxiv.org/abs/1905.02325\nNeural Spline Flows, https://arxiv.org/abs/1906.04032\n\nSuch flows could improve the experimental results of the paper. At the very least, it would be good to discuss them as more flexible alternatives.\n\nIn section 3.2, a third and very significant limitation of the flows discussed here is that they act elementwise on the dimensions (e.g. pixels) of y_t.\n\nIn the experimental section, it would be good to describe on a high level what the architecture of the VAE is, especially the architecture of the prior and the encoder, and the types of distributions used there (e.g. diagonal Gaussians or otherwise).\n\nIt would be good to show samples from the models in the experimental results."}, "signatures": ["ICLR.cc/2020/Conference/Paper2214/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2214/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jmarino@caltech.edu", "lei_chen_4@sfu.ca", "jiawei_he_2@sfu.ca", "stephan.mandt@gmail.com"], "title": "Improving Sequential Latent Variable Models with Autoregressive Flows", "authors": ["Joseph Marino", "Lei Chen", "Jiawei He", "Stephan Mandt"], "pdf": "/pdf/e62dd47e2346912a2e38bbd17d28bc8740db819c.pdf", "TL;DR": "We show how autoregressive flows can be used to improve sequential latent variable models.", "abstract": "We propose an approach for sequence modeling based on autoregressive normalizing flows. Each autoregressive transform, acting across time, serves as a moving reference frame for modeling higher-level dynamics. This technique provides a simple, general-purpose method for improving sequence modeling, with connections to existing and classical techniques. We demonstrate the proposed approach both with standalone models, as well as a part of larger sequential latent variable models. Results are presented on three benchmark video datasets, where flow-based dynamics improve log-likelihood performance over baseline models.", "code": "https://anonymous.4open.science/r/f02199f7-86d2-45ee-ad23-3f13f769ee10/", "keywords": ["Autoregressive Flows", "Sequence Modeling", "Latent Variable Models", "Video Modeling", "Variational Inference"], "paperhash": "marino|improving_sequential_latent_variable_models_with_autoregressive_flows", "original_pdf": "/attachment/2b5d4c96251209d9c3a87139cf8688d3e3590d51.pdf", "_bibtex": "@misc{\nmarino2020improving,\ntitle={Improving Sequential Latent Variable Models with Autoregressive Flows},\nauthor={Joseph Marino and Lei Chen and Jiawei He and Stephan Mandt},\nyear={2020},\nurl={https://openreview.net/forum?id=HklvmlrKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HklvmlrKPB", "replyto": "HklvmlrKPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2214/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2214/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575832749418, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2214/Reviewers"], "noninvitees": [], "tcdate": 1570237726068, "tmdate": 1575832749429, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2214/-/Official_Review"}}}, {"id": "Skld8grnYB", "original": null, "number": 3, "cdate": 1571733583613, "ddate": null, "tcdate": 1571733583613, "tmdate": 1572972368109, "tddate": null, "forum": "HklvmlrKPB", "replyto": "HklvmlrKPB", "invitation": "ICLR.cc/2020/Conference/Paper2214/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes to model temporal sequences using autoregressive flows across time steps, that allow to model more explicitly temporal changes of the input, i.e. how the input x_t has changed w.r.t x_{<t}. As also stated by the authors, this is a generalization of other work that instead of modelling the input at each time step, models temporal differences between consecutive time steps.\nTo the best of my knowledge, this is the first work that models normalizing flows in the sequential setting in this way (to be fair however, the idea is fairly obvious).\n\nOverall I found the paper interesting, and I think it is well written, so I am leaning towards acceptance. My biggest concern in the paper is the experimental section that could be improved in several ways:\n- the paper misses broader perfoemance comparisons against other state of the art models, in particular videoflow which is quite related to the models introduced in this paper.\n- how does the model perform on longer sequences, e.g. for long term generation? I would expect that such a direct dependence of the temporal dynamics on the frames of the video may make it hard for the model to coherently predict future latent states for many time steps.\n- What would happen if we used the same trick of modelling the conditional likelihood in this way in other SOTA models?\n- what are the computational requirements of the models presented in this paper? "}, "signatures": ["ICLR.cc/2020/Conference/Paper2214/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2214/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jmarino@caltech.edu", "lei_chen_4@sfu.ca", "jiawei_he_2@sfu.ca", "stephan.mandt@gmail.com"], "title": "Improving Sequential Latent Variable Models with Autoregressive Flows", "authors": ["Joseph Marino", "Lei Chen", "Jiawei He", "Stephan Mandt"], "pdf": "/pdf/e62dd47e2346912a2e38bbd17d28bc8740db819c.pdf", "TL;DR": "We show how autoregressive flows can be used to improve sequential latent variable models.", "abstract": "We propose an approach for sequence modeling based on autoregressive normalizing flows. Each autoregressive transform, acting across time, serves as a moving reference frame for modeling higher-level dynamics. This technique provides a simple, general-purpose method for improving sequence modeling, with connections to existing and classical techniques. We demonstrate the proposed approach both with standalone models, as well as a part of larger sequential latent variable models. Results are presented on three benchmark video datasets, where flow-based dynamics improve log-likelihood performance over baseline models.", "code": "https://anonymous.4open.science/r/f02199f7-86d2-45ee-ad23-3f13f769ee10/", "keywords": ["Autoregressive Flows", "Sequence Modeling", "Latent Variable Models", "Video Modeling", "Variational Inference"], "paperhash": "marino|improving_sequential_latent_variable_models_with_autoregressive_flows", "original_pdf": "/attachment/2b5d4c96251209d9c3a87139cf8688d3e3590d51.pdf", "_bibtex": "@misc{\nmarino2020improving,\ntitle={Improving Sequential Latent Variable Models with Autoregressive Flows},\nauthor={Joseph Marino and Lei Chen and Jiawei He and Stephan Mandt},\nyear={2020},\nurl={https://openreview.net/forum?id=HklvmlrKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HklvmlrKPB", "replyto": "HklvmlrKPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2214/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2214/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575832749418, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2214/Reviewers"], "noninvitees": [], "tcdate": 1570237726068, "tmdate": 1575832749429, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2214/-/Official_Review"}}}], "count": 9}