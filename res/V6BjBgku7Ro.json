{"notes": [{"id": "V6BjBgku7Ro", "original": "t7SUdcTFjw", "number": 2661, "cdate": 1601308294776, "ddate": null, "tcdate": 1601308294776, "tmdate": 1615945095987, "tddate": null, "forum": "V6BjBgku7Ro", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Planning from Pixels using Inverse Dynamics Models", "authorids": ["~Keiran_Paster1", "~Sheila_A._McIlraith1", "~Jimmy_Ba1"], "authors": ["Keiran Paster", "Sheila A. McIlraith", "Jimmy Ba"], "keywords": ["model based reinforcement learning", "deep reinforcement learning", "multi-task learning", "deep learning", "goal-conditioned reinforcement learning"], "abstract": "Learning dynamics models in high-dimensional observation spaces can be challenging for model-based RL agents. We propose a novel way to learn models in a latent space by learning to predict sequences of future actions conditioned on task completion. These models track task-relevant environment dynamics over a distribution of tasks, while simultaneously serving as an effective heuristic for planning with sparse rewards. We evaluate our method on challenging visual goal completion tasks and show a substantial increase in performance compared to prior model-free approaches.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "paster|planning_from_pixels_using_inverse_dynamics_models", "one-sentence_summary": "GLAMOR learns a latent world model by learning to predict action sequences conditioned on task completion.", "pdf": "/pdf/e1667f4513f3892a3eac139e23ee5198363e6741.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npaster2021planning,\ntitle={Planning from Pixels using Inverse Dynamics Models},\nauthor={Keiran Paster and Sheila A. McIlraith and Jimmy Ba},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=V6BjBgku7Ro}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "dzvlCiZ90Xt", "original": null, "number": 1, "cdate": 1610040422054, "ddate": null, "tcdate": 1610040422054, "tmdate": 1610474020851, "tddate": null, "forum": "V6BjBgku7Ro", "replyto": "V6BjBgku7Ro", "invitation": "ICLR.cc/2021/Conference/Paper2661/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "While many work in the literature (PlaNet (2018), Dreamer (2020), SimPLe (2019), etc.) learn world models to perform well on a particular task at hand, the motivation behind this work is that dynamics models benefit if they are task-agnostic, hence would be able to perform a wider range of tasks, as opposed to just doing one task really well. In order to do this, they propose to learn a latent representation that models inverse dynamics of the system / environment rather than capturing information about the task-specific rewards, and incorporate a planning for solving specific tasks in which they can measure performance.\n\nTo show broad applicability of their method, the authors tested their approach on Atari and DM Control Suite (from pixels), and also simple grid worlds to illustrate the concepts, and demonstrated strong performance over SOTA model-free algorithms (even the ones that do not have open-source implementations). Reviewers and myself agree that the paper is well written, easy to follow, and the approach is well-motivated.\n\nAfter the review period, the authors have done work to improve the draft, particularly including ablation studies with and without planning, addition comparisons, and improved visualizations, after taking in the comments and feedback from the reviewers after the initial reviews, which satisfied some of the reviewers. One reviewer asked for a real robotic task, but I feel that while it will help the paper, many existing works focus purely on DM control from pixels, and this work has performed experiments on both DM Control and Atari, two reasonably different domains, and IMO makes up for the lack of real-world robotics experiment. That being said, a discussion on how the proposed method would work in a real-robotic task, as suggested by R4 would be good to have.\n\nI believe the work in its current state is ready for acceptance for ICLR 2021, and should be a fine contribution to the visual model-based RL works. I'm excited to see this work presented to the community, and I'm going to recommend acceptance (Poster)."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Planning from Pixels using Inverse Dynamics Models", "authorids": ["~Keiran_Paster1", "~Sheila_A._McIlraith1", "~Jimmy_Ba1"], "authors": ["Keiran Paster", "Sheila A. McIlraith", "Jimmy Ba"], "keywords": ["model based reinforcement learning", "deep reinforcement learning", "multi-task learning", "deep learning", "goal-conditioned reinforcement learning"], "abstract": "Learning dynamics models in high-dimensional observation spaces can be challenging for model-based RL agents. We propose a novel way to learn models in a latent space by learning to predict sequences of future actions conditioned on task completion. These models track task-relevant environment dynamics over a distribution of tasks, while simultaneously serving as an effective heuristic for planning with sparse rewards. We evaluate our method on challenging visual goal completion tasks and show a substantial increase in performance compared to prior model-free approaches.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "paster|planning_from_pixels_using_inverse_dynamics_models", "one-sentence_summary": "GLAMOR learns a latent world model by learning to predict action sequences conditioned on task completion.", "pdf": "/pdf/e1667f4513f3892a3eac139e23ee5198363e6741.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npaster2021planning,\ntitle={Planning from Pixels using Inverse Dynamics Models},\nauthor={Keiran Paster and Sheila A. McIlraith and Jimmy Ba},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=V6BjBgku7Ro}\n}"}, "tags": [], "invitation": {"reply": {"forum": "V6BjBgku7Ro", "replyto": "V6BjBgku7Ro", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040422040, "tmdate": 1610474020831, "id": "ICLR.cc/2021/Conference/Paper2661/-/Decision"}}}, {"id": "-jzVQ1YrQbl", "original": null, "number": 1, "cdate": 1603847186295, "ddate": null, "tcdate": 1603847186295, "tmdate": 1606711603350, "tddate": null, "forum": "V6BjBgku7Ro", "replyto": "V6BjBgku7Ro", "invitation": "ICLR.cc/2021/Conference/Paper2661/-/Official_Review", "content": {"title": "This paper tackles the problem of visual goal completion. The authors proposed to learn inverse dynamics (actions instead of states) as the knowledge of the world for planning. They tested their results on several benchmarking tasks and compared to recent methods proposed on goal-conditioned RL and policy learning.", "review": "The authors adopted an interesting idea for learning inverse dynamics which is different from common approaches where oftentimes a state-transition model is learned for planning. Though in their scenario this does not make any guarantees on the improvement on generalization, conditioning on the rich semantics could be contained in actions, it might be a justification for learning the inverse dynamics model for tasks that test generalization capabilities. \n\nHowever, the idea of using relabeling to facilitate the learning of goal-conditioned policies and world models is not new as discussed by the authors. This makes the learning framework somewhat less significant in novelty. More specifically, the authors put quite a lot of effort into distinguishing their method from a recent method GCSL. The differences actually lead to a question on the action prior. I'm curious if this prior distribution is different from a uniform distribution over all available paths starting from $s_1$, it is not well justified why we need to parameterize this with LSTM. \n\nThere are also several, I believe, typos that might need fixing. The first term in Equation (9), should it be $\\mathbb{E}[-\\log p_{\\theta}(a_1, \\cdots, a_k | s_1, s_k)]? In the experiment section, the model was trained with 500k agent steps while in the figure it was 5M, this leads to the problem of sample efficiency (section 4.6).\n\nOverall, I feel that though the idea of adopting inverse dynamics for learning action model is interesting, it is still somewhat incremental to prior works with a similar framework. The missing justification and details also contribute to the decision of putting a decision of borderline during pre-rebuttal period.\n\n================================================================================================================\n\nThe authors has addressed my concerns well in the discussion period, I have increased the score to 6.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2661/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2661/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Planning from Pixels using Inverse Dynamics Models", "authorids": ["~Keiran_Paster1", "~Sheila_A._McIlraith1", "~Jimmy_Ba1"], "authors": ["Keiran Paster", "Sheila A. McIlraith", "Jimmy Ba"], "keywords": ["model based reinforcement learning", "deep reinforcement learning", "multi-task learning", "deep learning", "goal-conditioned reinforcement learning"], "abstract": "Learning dynamics models in high-dimensional observation spaces can be challenging for model-based RL agents. We propose a novel way to learn models in a latent space by learning to predict sequences of future actions conditioned on task completion. These models track task-relevant environment dynamics over a distribution of tasks, while simultaneously serving as an effective heuristic for planning with sparse rewards. We evaluate our method on challenging visual goal completion tasks and show a substantial increase in performance compared to prior model-free approaches.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "paster|planning_from_pixels_using_inverse_dynamics_models", "one-sentence_summary": "GLAMOR learns a latent world model by learning to predict action sequences conditioned on task completion.", "pdf": "/pdf/e1667f4513f3892a3eac139e23ee5198363e6741.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npaster2021planning,\ntitle={Planning from Pixels using Inverse Dynamics Models},\nauthor={Keiran Paster and Sheila A. McIlraith and Jimmy Ba},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=V6BjBgku7Ro}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "V6BjBgku7Ro", "replyto": "V6BjBgku7Ro", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2661/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538091292, "tmdate": 1606915778702, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2661/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2661/-/Official_Review"}}}, {"id": "SaAICBJEUS4", "original": null, "number": 3, "cdate": 1604037499897, "ddate": null, "tcdate": 1604037499897, "tmdate": 1606541357839, "tddate": null, "forum": "V6BjBgku7Ro", "replyto": "V6BjBgku7Ro", "invitation": "ICLR.cc/2021/Conference/Paper2661/-/Official_Review", "content": {"title": "Review for Planning from Pixels using Inverse Dynamics Models", "review": "Summary: The author proposes Goal-Conditioned Latent Action Models for RL (GLAMOR) a novel approach to learn latent world models by modeling inverse dynamics. The proposed approach learns to track task-relevant dynamics for a diverse distribution of tasks and provide a strong heuristic that enables efficient planning. GLAMOR demonstrates good performance against its baselines in terms of achieving accurately goals, sample efficiency and effective planning.\n\n\nReason for the score:\nPaper address an interesting question related to Reinforcement Learning and provides strong results to back the proposed method.\n\n\nPros:\n-good flow of the paper\n\n-Correct situation of the problem with respect to current research\n\n-A good problem to address in Reinforcement Learning area\n\n-Strong experimental results\n\n\nCons:\n-Can the authors talk about how the proposed method perform in a real-robotic task?\n\n-In the equation 5, a_{<i} means?\n\n-in page 3 last para is not clear to me. Can you rewrite or provide explanation of how it is possible?\n\n-In para 3 of Section 3.4, why you sample from Boltzmann distribution and not uniform distribution?\n\n-Have you considered other exploration methods than of epsilon-greedy?\n\n\nQuestions for the rebuttal:\nPlease address and clarify the cons above \n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2661/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2661/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Planning from Pixels using Inverse Dynamics Models", "authorids": ["~Keiran_Paster1", "~Sheila_A._McIlraith1", "~Jimmy_Ba1"], "authors": ["Keiran Paster", "Sheila A. McIlraith", "Jimmy Ba"], "keywords": ["model based reinforcement learning", "deep reinforcement learning", "multi-task learning", "deep learning", "goal-conditioned reinforcement learning"], "abstract": "Learning dynamics models in high-dimensional observation spaces can be challenging for model-based RL agents. We propose a novel way to learn models in a latent space by learning to predict sequences of future actions conditioned on task completion. These models track task-relevant environment dynamics over a distribution of tasks, while simultaneously serving as an effective heuristic for planning with sparse rewards. We evaluate our method on challenging visual goal completion tasks and show a substantial increase in performance compared to prior model-free approaches.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "paster|planning_from_pixels_using_inverse_dynamics_models", "one-sentence_summary": "GLAMOR learns a latent world model by learning to predict action sequences conditioned on task completion.", "pdf": "/pdf/e1667f4513f3892a3eac139e23ee5198363e6741.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npaster2021planning,\ntitle={Planning from Pixels using Inverse Dynamics Models},\nauthor={Keiran Paster and Sheila A. McIlraith and Jimmy Ba},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=V6BjBgku7Ro}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "V6BjBgku7Ro", "replyto": "V6BjBgku7Ro", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2661/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538091292, "tmdate": 1606915778702, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2661/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2661/-/Official_Review"}}}, {"id": "hmIcdTY7-f", "original": null, "number": 9, "cdate": 1606245687061, "ddate": null, "tcdate": 1606245687061, "tmdate": 1606245687061, "tddate": null, "forum": "V6BjBgku7Ro", "replyto": "2DD3suxBNB", "invitation": "ICLR.cc/2021/Conference/Paper2661/-/Official_Comment", "content": {"title": "Response to the author", "comment": "Thank you for your clarification and adjustments made to make this paper better. The prior distribution problem makes sense and solved my concern. As for the comparison between the learned inverse dynamic model and commonly-used expected reward prediction, I agree with the authors that sometimes strong action sequences lead to faster and better performance. However, one new concern from that point in the rebuttal is, having a strong action sequence distribution from training experience is not always a good idea, especially when meeting changing environments with similar dynamics (e.g. a simple grid world with traps but in different positions). Though beyond the current paper's scope, to which problems will the proposed method give performance boosts on is also an interesting topic to discuss."}, "signatures": ["ICLR.cc/2021/Conference/Paper2661/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2661/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Planning from Pixels using Inverse Dynamics Models", "authorids": ["~Keiran_Paster1", "~Sheila_A._McIlraith1", "~Jimmy_Ba1"], "authors": ["Keiran Paster", "Sheila A. McIlraith", "Jimmy Ba"], "keywords": ["model based reinforcement learning", "deep reinforcement learning", "multi-task learning", "deep learning", "goal-conditioned reinforcement learning"], "abstract": "Learning dynamics models in high-dimensional observation spaces can be challenging for model-based RL agents. We propose a novel way to learn models in a latent space by learning to predict sequences of future actions conditioned on task completion. These models track task-relevant environment dynamics over a distribution of tasks, while simultaneously serving as an effective heuristic for planning with sparse rewards. We evaluate our method on challenging visual goal completion tasks and show a substantial increase in performance compared to prior model-free approaches.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "paster|planning_from_pixels_using_inverse_dynamics_models", "one-sentence_summary": "GLAMOR learns a latent world model by learning to predict action sequences conditioned on task completion.", "pdf": "/pdf/e1667f4513f3892a3eac139e23ee5198363e6741.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npaster2021planning,\ntitle={Planning from Pixels using Inverse Dynamics Models},\nauthor={Keiran Paster and Sheila A. McIlraith and Jimmy Ba},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=V6BjBgku7Ro}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "V6BjBgku7Ro", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2661/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2661/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2661/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2661/Authors|ICLR.cc/2021/Conference/Paper2661/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2661/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845791, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2661/-/Official_Comment"}}}, {"id": "zAYdfJHfa8n", "original": null, "number": 3, "cdate": 1605587288813, "ddate": null, "tcdate": 1605587288813, "tmdate": 1606182073852, "tddate": null, "forum": "V6BjBgku7Ro", "replyto": "V6BjBgku7Ro", "invitation": "ICLR.cc/2021/Conference/Paper2661/-/Official_Comment", "content": {"title": "General Comments to All Reviewers", "comment": "We thank all the reviewers for the valuable comments. We agree with the common sentiment that while the differences between our method and the baselines are clear in terms of performance, our experiments didn\u2019t do enough to explain exactly why this difference exists. We have uploaded a new version of our PDF that has several changes and new experiments that hope to address these concerns. These changes are:\n\n1. Added an ablation study in Appendix A.4. This section includes an experiment testing the effect of using the action prior (without the action prior, performance is worse) and without planning (without planning, performance is significantly worse). We also tested GLAMOR and GCSL trained on a dataset collected with a random policy. GLAMOR performs almost as well as the full algorithm while GCSL sees a large decrease in performance (this was also shown in their original paper). \n2. In section 4.7.3, we tested the agent\u2019s performance when we restricted the planner to only use sparse information rather than use the intermediate probabilities provided by the inverse dynamics and action prior. This resulted in much lower performance with equal levels of search compute.\n3. We added a new experiment in section 4.7.2 that shows that our GLAMOR agent can adjust its behavior to reach a goal at a specific time-step, achieving nearly the same performance as the version which can terminate an episode early. The agent\u2019s world model has knowledge about many different action sequences that lead to the goal - a benefit over the baselines. This also shows that the performance improvement is not mainly due to differences in evaluation.\n4. We moved the old section 3.6 (GCSL) to the appendix and replaced it with a new \u201cComparison to Prior Methods\u201d section. This section highlights that GLAMOR: learns a world model that is more flexible than a simple policy or Q-network; GLAMOR performs well with off-policy data; and GLAMOR uses an action prior to avoid the interference problem in GCSL.\n5. We added Appendix A.3 with hyperparameters and implementation details. We also added a figure illustrating the network architecture (Figure 1).\n6. We added a link to an anonymous website with videos of the GLAMOR agent reaching goals in all tested environments as well as videos of the termination strategy experiments.\n7. (New 11/23) We updated the styling of the plots to improve visibility."}, "signatures": ["ICLR.cc/2021/Conference/Paper2661/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2661/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Planning from Pixels using Inverse Dynamics Models", "authorids": ["~Keiran_Paster1", "~Sheila_A._McIlraith1", "~Jimmy_Ba1"], "authors": ["Keiran Paster", "Sheila A. McIlraith", "Jimmy Ba"], "keywords": ["model based reinforcement learning", "deep reinforcement learning", "multi-task learning", "deep learning", "goal-conditioned reinforcement learning"], "abstract": "Learning dynamics models in high-dimensional observation spaces can be challenging for model-based RL agents. We propose a novel way to learn models in a latent space by learning to predict sequences of future actions conditioned on task completion. These models track task-relevant environment dynamics over a distribution of tasks, while simultaneously serving as an effective heuristic for planning with sparse rewards. We evaluate our method on challenging visual goal completion tasks and show a substantial increase in performance compared to prior model-free approaches.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "paster|planning_from_pixels_using_inverse_dynamics_models", "one-sentence_summary": "GLAMOR learns a latent world model by learning to predict action sequences conditioned on task completion.", "pdf": "/pdf/e1667f4513f3892a3eac139e23ee5198363e6741.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npaster2021planning,\ntitle={Planning from Pixels using Inverse Dynamics Models},\nauthor={Keiran Paster and Sheila A. McIlraith and Jimmy Ba},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=V6BjBgku7Ro}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "V6BjBgku7Ro", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2661/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2661/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2661/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2661/Authors|ICLR.cc/2021/Conference/Paper2661/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2661/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845791, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2661/-/Official_Comment"}}}, {"id": "2DD3suxBNB", "original": null, "number": 7, "cdate": 1605588662120, "ddate": null, "tcdate": 1605588662120, "tmdate": 1605588662120, "tddate": null, "forum": "V6BjBgku7Ro", "replyto": "-jzVQ1YrQbl", "invitation": "ICLR.cc/2021/Conference/Paper2661/-/Official_Comment", "content": {"title": "Thank you for the comments", "comment": "Thank you for the valuable comments. We have uploaded a new PDF. Please see general comments to all reviewers for details about what has changed.\n\nQ: The learning framework is not novel.\n\nOur work is the first to our knowledge to use temporally-extended inverse dynamics to learn a latent world model, and the first to show how this model can be combined with an action prior to efficiently plan to control the state of an environment. Although GCSL seems similar since it also trains on its own, relabeled, trajectories, we think we may have overstated the similarities in our original submission. GCSL is fundamentally a model-free RL algorithm which learns a policy to achieve goals while GLAMOR learns a latent dynamics model that predicts entire action sequences.\n\nWe updated section (3.6) in the updated PDF to outline some advantages of our approach over the prior works. Additionally, we added some additional experiments that we think empirically show the advantage of GLAMOR over prior works.\n\nSection 4.7.2: GLAMOR\u2019s latent world model is more powerful than a policy. We ran an experiment where, at test time, the agent searches for action sequences that lands it in the goal state at exactly the last time-step in the episode rather than searching for a shortest path. Although GLAMOR is not trained on this particular task, it still achieved impressive performance because the world model knows about many different action sequences that lead the agent to the goal.\n\nSection 4.7.3: One of the main benefits of using an inverse dynamics model is that we can use the factored probabilities to guide the search for a strong action sequence. This is in contrast to how an algorithm like MuZero (Schrittwieser et al., 2019) models the expected reward at each time-step and learns a separate policy/value function to guide search. We added a new experiment to test the extent to which using an inverse dynamics model helps with planning versus directly modeling the reward the agent cares about (probability of reaching the goal) and planning using just sparse information. \n\nWe used RS in this experiment as well, sampling action sequences at random and picking the sequence with the highest score out of the sampled sequences. We found that in our environments, this agent performed significantly worse with the same level of compute, showing the value added by learning an inverse dynamics model.\n\nAppendix A.4: We added an ablation study which shows: GLAMOR clearly benefits from using the action prior; planning during training is important, but GLAMOR can get away with using relatively little (5 samples performs almost as well); and GLAMOR performs much better than GCSL on truly off-policy data collected via a random walk.\n\nQ: Is the action prior different from a uniform distribution?\n\nIndeed, if the training policy is uniform then the action prior is very simple. But when we do iterative training or we have a non-uniform training policy, the action prior is a complicated combination of the training policies averaged over the goal states in the dataset. We use an LSTM to learn this distribution. Appendix A.4.1 shows that disabling the action prior when training in Pong results in substantially worse performance compared to the full GLAMOR algorithm.\n\nQ: Equation (9) typo\n\nFixed in the new draft.\n\nQ: Table shows agents at 500k steps\n\nWe show a table of the results at 500k steps because it highlights that our method performs really well even with that low amount of training data. The training curves are for the full 5M steps of training.\n\nQ: Justification and Details:\n\nPlease see the appendix for details, including an ablation study that provides the reader with further insight into the effectiveness of the technique. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2661/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2661/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Planning from Pixels using Inverse Dynamics Models", "authorids": ["~Keiran_Paster1", "~Sheila_A._McIlraith1", "~Jimmy_Ba1"], "authors": ["Keiran Paster", "Sheila A. McIlraith", "Jimmy Ba"], "keywords": ["model based reinforcement learning", "deep reinforcement learning", "multi-task learning", "deep learning", "goal-conditioned reinforcement learning"], "abstract": "Learning dynamics models in high-dimensional observation spaces can be challenging for model-based RL agents. We propose a novel way to learn models in a latent space by learning to predict sequences of future actions conditioned on task completion. These models track task-relevant environment dynamics over a distribution of tasks, while simultaneously serving as an effective heuristic for planning with sparse rewards. We evaluate our method on challenging visual goal completion tasks and show a substantial increase in performance compared to prior model-free approaches.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "paster|planning_from_pixels_using_inverse_dynamics_models", "one-sentence_summary": "GLAMOR learns a latent world model by learning to predict action sequences conditioned on task completion.", "pdf": "/pdf/e1667f4513f3892a3eac139e23ee5198363e6741.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npaster2021planning,\ntitle={Planning from Pixels using Inverse Dynamics Models},\nauthor={Keiran Paster and Sheila A. McIlraith and Jimmy Ba},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=V6BjBgku7Ro}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "V6BjBgku7Ro", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2661/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2661/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2661/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2661/Authors|ICLR.cc/2021/Conference/Paper2661/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2661/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845791, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2661/-/Official_Comment"}}}, {"id": "HRO3lV2MyeB", "original": null, "number": 6, "cdate": 1605588360609, "ddate": null, "tcdate": 1605588360609, "tmdate": 1605588360609, "tddate": null, "forum": "V6BjBgku7Ro", "replyto": "X7sBnBgmSIa", "invitation": "ICLR.cc/2021/Conference/Paper2661/-/Official_Comment", "content": {"title": "Thank you for the comments", "comment": "Thank you for the valuable comments. We have uploaded a new PDF. Please see general comments to all reviewers for details about what has changed.\n\nQ: authors do not discuss the learning complexity of training two networks compared to one policy network as done in baselines\n\nGood question. Interestingly, the addition of an additional network has minimal impact in practice. This is because: 1) the bottleneck in the algorithm is collecting data from the environment rather than training the neural networks; and 2) we share parameters between the ResNet used to encode observations in the inverse dynamics model and the action prior, which reduces the number of additional parameters and allows us to only compute it once.\n\nQ: I believe it would help to evaluate the method better if they also show the number of trials used when comparing to baselines.\n\nWe have added an extensive ablation study in Appendix A.4. Among other experiments, we show that: the performance of our algorithm is still strong when it is trained even on data collected via a random policy; using training trials as low as 5 still yields strong, state of the art performance; and even when using just one planning sample at test time (equivalent compute to model-free), GLAMOR suffers only a minor hit in performance and still achieves more goals than the baselines.\n\nQ: Can the authors provide any real-time performance guarantees at test time?\n\nGLAMOR needs to search for a strong action sequence while the baseline methods have a policy/q-function that make inference much quicker at test time. However, we find that our planner only takes around 0.1 seconds to plan on a P100 GPU. These plans contain not only the next action, but also the entire sequence of actions that the agent currently thinks would be optimal, meaning that if real-time performance is a concern, re-planning could happen only every few steps and the agent could follow the open-loop plan in between re-planning. Another option is to directly sample actions rather than planning, as in Appendix A.4.2, which we show only slightly reduces performance.\n\nQ: The planner is used to generate a sequence of actions but an e-greedy policy is used for exploration.\n\nWe apologize for not being clear about the implementation of this. We have fixed it in the updated paper. At the beginning of each episode, a goal is sampled and the agent generates a sequence of actions. At each step during that episode, the agent takes the ith action in the plan with probability 1-eps and takes an action randomly with probability eps.\n\nQ: The paper claims that the inverse dynamics formulation learns to represent controllable aspects of the state. Has this been theoretically proven before?\n\nJust like in MuZero (Schrittwieser et al., 2019) and related works, the latent dynamics model in GLAMOR is not required to model all the information in the original observation. Instead, the latent states are only incentivized to contain information necessary to predict the actions that are necessary to take the agent to the goal state. We are not aware of any theoretical work about the relationship between inverse dynamics and controllability, but this claim is also made in ICM (Pathak et al., 2017).\n\nQ: And have any empirical studies been done to show that this actually helps model-based RL to perform better than when using forward models? It would be interesting to compare against model-based RL techniques based on forward models, if available, for this work.\n\nOne of the main benefits of using an inverse dynamics model is that we can use the factored probabilities to guide the search for a strong action sequence. This is in contrast to how an algorithm like MuZero (Schrittwieser et al., 2019) models the expected reward at each time-step and learns a separate policy/value function to guide search. We added a new experiment in our updated PDF to test the extent to which using an inverse dynamics model helps with planning versus directly modeling the reward the agent cares about (probability of reaching the goal) and planning using just sparse information. \n\nWe used RS in this experiment as well, sampling action sequences at random and picking the sequence with the highest score out of the sampled sequences. We found that in our environments, this agent performed significantly worse with the same level of compute, showing the value added by learning the inverse dynamics model.\n\nWe leave further comparisons with other MBRL techniques to future work.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2661/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2661/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Planning from Pixels using Inverse Dynamics Models", "authorids": ["~Keiran_Paster1", "~Sheila_A._McIlraith1", "~Jimmy_Ba1"], "authors": ["Keiran Paster", "Sheila A. McIlraith", "Jimmy Ba"], "keywords": ["model based reinforcement learning", "deep reinforcement learning", "multi-task learning", "deep learning", "goal-conditioned reinforcement learning"], "abstract": "Learning dynamics models in high-dimensional observation spaces can be challenging for model-based RL agents. We propose a novel way to learn models in a latent space by learning to predict sequences of future actions conditioned on task completion. These models track task-relevant environment dynamics over a distribution of tasks, while simultaneously serving as an effective heuristic for planning with sparse rewards. We evaluate our method on challenging visual goal completion tasks and show a substantial increase in performance compared to prior model-free approaches.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "paster|planning_from_pixels_using_inverse_dynamics_models", "one-sentence_summary": "GLAMOR learns a latent world model by learning to predict action sequences conditioned on task completion.", "pdf": "/pdf/e1667f4513f3892a3eac139e23ee5198363e6741.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npaster2021planning,\ntitle={Planning from Pixels using Inverse Dynamics Models},\nauthor={Keiran Paster and Sheila A. McIlraith and Jimmy Ba},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=V6BjBgku7Ro}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "V6BjBgku7Ro", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2661/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2661/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2661/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2661/Authors|ICLR.cc/2021/Conference/Paper2661/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2661/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845791, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2661/-/Official_Comment"}}}, {"id": "RaNy_jQABNj", "original": null, "number": 5, "cdate": 1605587907437, "ddate": null, "tcdate": 1605587907437, "tmdate": 1605587907437, "tddate": null, "forum": "V6BjBgku7Ro", "replyto": "SaAICBJEUS4", "invitation": "ICLR.cc/2021/Conference/Paper2661/-/Official_Comment", "content": {"title": "Thank you for the comments", "comment": "Thank you for the valuable comments. We have uploaded a new PDF. Please see general comments to all reviewers for details about what has changed.\n\nQ: Can the authors talk about how the proposed method performs in a real-robotic task?\n\nWe think that our method is a good fit for real-robot tasks, due to its especially strong performance with off-policy data (appendix A.4.1) and the fact that the goal-achievement task probably won\u2019t require expensive env.reset() operations, which are usually a pain-point in real-robot experiments. These experiments still require expensive equipment and significant time, so we leave such experimentation to future work.\n\nQ: Confused about what  a_{<i} means:\n\nWe have clarified the notation in our updated PDF. This just means a_1, \u2026, a_{i-1}.\n\nQ: On page 3 the last para is not clear to me. Can you rewrite or provide an explanation of how it is possible?\n\nWe apologize that this was not clear. We have rewritten the paragraph (now on page 4). The main point is that, with our objective factored over the individual actions, we can use this information to guide our search. It is similar to in NLP, where if we wanted to find a sequence of tokens that have a high probability under a language model without using any intermediate information, it would be quite challenging. Since we have access to the intermediate p(token_i|token_{<i}) however, the search becomes a lot easier.\n\nQ: In para 3 of Section 3.4, why do you sample from a Boltzmann distribution and not uniform distribution?\n\nThis is just one way to guide the search based on the factored probabilities. As in the example above, using a uniform distribution to search for action sequences doesn\u2019t use this information and won\u2019t result in a very good action sequence (see section 4.7.3 in the updated PDF, where we added an additional experiment trying exactly this). When we sample using the z_i, almost every action sequence sampled gets the agent to the goal state, so the search becomes easy.\n\nQ: Have you considered other exploration methods than epsilon-greedy?\n\nWe use epsilon-greedy simply to guarantee that each goal has at least some probability of being reached (needed for our derivation). As mentioned in the new section 3.6, GLAMOR\u2019s strong performance with off-policy data suggests that more sophisticated exploration schemes could probably be used. However, we found that epsilon-greedy was sufficient, so we did not experiment with different methods. We consider this direction to be orthogonal to the main contribution of this paper. Future extensions could include different ways to sample goals that lead to more efficient exploration, or ways to choose actions specifically to gain the most information about the parameters of the inverse dynamics model. This is indeed an interesting area for future work.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2661/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2661/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Planning from Pixels using Inverse Dynamics Models", "authorids": ["~Keiran_Paster1", "~Sheila_A._McIlraith1", "~Jimmy_Ba1"], "authors": ["Keiran Paster", "Sheila A. McIlraith", "Jimmy Ba"], "keywords": ["model based reinforcement learning", "deep reinforcement learning", "multi-task learning", "deep learning", "goal-conditioned reinforcement learning"], "abstract": "Learning dynamics models in high-dimensional observation spaces can be challenging for model-based RL agents. We propose a novel way to learn models in a latent space by learning to predict sequences of future actions conditioned on task completion. These models track task-relevant environment dynamics over a distribution of tasks, while simultaneously serving as an effective heuristic for planning with sparse rewards. We evaluate our method on challenging visual goal completion tasks and show a substantial increase in performance compared to prior model-free approaches.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "paster|planning_from_pixels_using_inverse_dynamics_models", "one-sentence_summary": "GLAMOR learns a latent world model by learning to predict action sequences conditioned on task completion.", "pdf": "/pdf/e1667f4513f3892a3eac139e23ee5198363e6741.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npaster2021planning,\ntitle={Planning from Pixels using Inverse Dynamics Models},\nauthor={Keiran Paster and Sheila A. McIlraith and Jimmy Ba},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=V6BjBgku7Ro}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "V6BjBgku7Ro", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2661/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2661/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2661/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2661/Authors|ICLR.cc/2021/Conference/Paper2661/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2661/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845791, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2661/-/Official_Comment"}}}, {"id": "4eQtOZYBeeV", "original": null, "number": 4, "cdate": 1605587742609, "ddate": null, "tcdate": 1605587742609, "tmdate": 1605587742609, "tddate": null, "forum": "V6BjBgku7Ro", "replyto": "YNM9ZNrfayX", "invitation": "ICLR.cc/2021/Conference/Paper2661/-/Official_Comment", "content": {"title": "Thank you for the comments", "comment": "Thank you for the valuable comments. \n\nWe agree that it was unclear exactly what was contributing to the gap in performance between GLAMOR and baselines. To answer your questions, we have uploaded a new PDF (please also see general comments to all reviewers for more details about what has changed) with many extra experiments. What follows is a summary of the new experiments and results:\n\n1. We ran an experiment in Appendix A.4.1 that shows that if we simply sample from the inverse dynamics model in GLAMOR during training (essentially the same as just sampling one action sequence in our planner), the performance is much worse than when it plans (samples many sequences and selects the one with the highest score).\n2. Also in Appendix A.4.1, we also show that without using the action prior, GLAMOR performs significantly worse. \n3. We also show that GLAMOR performs significantly better when training data produced by a random agent. While GCSL relies on iterates of \u201cgather training data from the current policy\u201d and \u201ctrain on the relabeled data\u201d, our algorithm has no such requirement. In fact, data collected from a uniformly random policy is great for GLAMOR since the action prior will be trivial.\n4. GLAMOR has the ability to predict that it is currently at the goal state and terminate the episode and evaluate early while baselines always evaluate whether the goal was achieved after a fixed number of steps. In Section 4.7.2, we evaluated the effects of early termination on the performance of the agent to see whether the performance gain could be attributed to this. We found that early termination strategy in GLAMOR is not necessary if the planner explicitly plans to reach the goal at the last time-step in the trajectory. This also illustrates a benefit of GLAMOR: it learns a world model rather than a policy, which makes it significantly more flexible.\n5. One of the main benefits of using an inverse dynamics model is that we can use the factored probabilities to guide the search for a strong action sequence. This is in contrast to how an algorithm like MuZero (Schrittwieser et al., 2019) models the expected reward at each time-step and learns a separate policy/value function to guide search. We added a new experiment in section 4.7.3 to test the extent to which using an inverse dynamics model helps with planning versus directly modeling the reward the agent cares about (probability of reaching the goal) and planning using just sparse information. \n\n  We used RS in this experiment as well, sampling action sequences at random and picking the sequence with the highest score out of the sampled sequences. We found that in our environments, this agent performed significantly worse with the same level of compute, showing the value added by learning an inverse dynamics model.\n\nWe believe these results provide better insight into where the differences in performance come from.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2661/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2661/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Planning from Pixels using Inverse Dynamics Models", "authorids": ["~Keiran_Paster1", "~Sheila_A._McIlraith1", "~Jimmy_Ba1"], "authors": ["Keiran Paster", "Sheila A. McIlraith", "Jimmy Ba"], "keywords": ["model based reinforcement learning", "deep reinforcement learning", "multi-task learning", "deep learning", "goal-conditioned reinforcement learning"], "abstract": "Learning dynamics models in high-dimensional observation spaces can be challenging for model-based RL agents. We propose a novel way to learn models in a latent space by learning to predict sequences of future actions conditioned on task completion. These models track task-relevant environment dynamics over a distribution of tasks, while simultaneously serving as an effective heuristic for planning with sparse rewards. We evaluate our method on challenging visual goal completion tasks and show a substantial increase in performance compared to prior model-free approaches.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "paster|planning_from_pixels_using_inverse_dynamics_models", "one-sentence_summary": "GLAMOR learns a latent world model by learning to predict action sequences conditioned on task completion.", "pdf": "/pdf/e1667f4513f3892a3eac139e23ee5198363e6741.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npaster2021planning,\ntitle={Planning from Pixels using Inverse Dynamics Models},\nauthor={Keiran Paster and Sheila A. McIlraith and Jimmy Ba},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=V6BjBgku7Ro}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "V6BjBgku7Ro", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2661/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2661/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2661/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2661/Authors|ICLR.cc/2021/Conference/Paper2661/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2661/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845791, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2661/-/Official_Comment"}}}, {"id": "YNM9ZNrfayX", "original": null, "number": 4, "cdate": 1604384821303, "ddate": null, "tcdate": 1604384821303, "tmdate": 1605214007265, "tddate": null, "forum": "V6BjBgku7Ro", "replyto": "V6BjBgku7Ro", "invitation": "ICLR.cc/2021/Conference/Paper2661/-/Official_Review", "content": {"title": "Interesting work. Need more justification on the main claim", "review": "The paper proposes a model-based reinforcement learning method. The method builds a partial model of the environment through learning inverse dynamics, which is the distribution of action sequences that would bring one state to another state. Through training the model with an iterative relabeling scheme, the model is able to learn to reach goals in a subset of DM Control and Atari domains. \n\nComments:\n+ This MBRL formulation is conceptually simple and does not rely on learning the full model of an environment, potentially enabling long-horizon planning while being able to high-dimensional input.\n+ The relationship and differences to the closely related work Ghosh et al., 2020 is clearly stated and elaborated.\n\n- My main concern about the paper is that although the difference between GCSL and the proposed method is exposited in theory (Proposition 3.1), there is no direct empirical experiment explaining how this difference might play out in practice. Specifically, there clearly are performance gaps between the proposed method and GCSL, but I cannot seem to grasp what EXACTLY is causing this gap. Is it ONLY because of the difference stated in proposition 3.1? If so, I'd like to see a minimum toy experiment clearly showing the qualitative differences. If there are other differences, I'd like to see them addressed in greater details. None of the reasons (i)-(iii) stated in section 3.6 are substantiated experimentally. ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2661/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2661/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Planning from Pixels using Inverse Dynamics Models", "authorids": ["~Keiran_Paster1", "~Sheila_A._McIlraith1", "~Jimmy_Ba1"], "authors": ["Keiran Paster", "Sheila A. McIlraith", "Jimmy Ba"], "keywords": ["model based reinforcement learning", "deep reinforcement learning", "multi-task learning", "deep learning", "goal-conditioned reinforcement learning"], "abstract": "Learning dynamics models in high-dimensional observation spaces can be challenging for model-based RL agents. We propose a novel way to learn models in a latent space by learning to predict sequences of future actions conditioned on task completion. These models track task-relevant environment dynamics over a distribution of tasks, while simultaneously serving as an effective heuristic for planning with sparse rewards. We evaluate our method on challenging visual goal completion tasks and show a substantial increase in performance compared to prior model-free approaches.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "paster|planning_from_pixels_using_inverse_dynamics_models", "one-sentence_summary": "GLAMOR learns a latent world model by learning to predict action sequences conditioned on task completion.", "pdf": "/pdf/e1667f4513f3892a3eac139e23ee5198363e6741.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npaster2021planning,\ntitle={Planning from Pixels using Inverse Dynamics Models},\nauthor={Keiran Paster and Sheila A. McIlraith and Jimmy Ba},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=V6BjBgku7Ro}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "V6BjBgku7Ro", "replyto": "V6BjBgku7Ro", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2661/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538091292, "tmdate": 1606915778702, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2661/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2661/-/Official_Review"}}}, {"id": "X7sBnBgmSIa", "original": null, "number": 2, "cdate": 1603904395847, "ddate": null, "tcdate": 1603904395847, "tmdate": 1605024158936, "tddate": null, "forum": "V6BjBgku7Ro", "replyto": "V6BjBgku7Ro", "invitation": "ICLR.cc/2021/Conference/Paper2661/-/Official_Review", "content": {"title": "Planning from Pixels using Inverse Dynamics Models", "review": "The paper proposes to learn task-agnostic world models in latent space, using a goal-conditioned inverse dynamics formulation and an action priors model, that learn to predict action sequences. The approach is compared against a model-free unsupervised control method and an imitation learning method on the subset of Atari games and a continuous control benchmark, and showed to achieve superior performance in most tasks based on goal completion ratio and sample efficiency. \n\nThe proposed approach learns two networks, the inverse dynamics model and the action prior model, and uses this to plan a sequence of control actions, rather than learning the policy directly as done by the baselines. Although this has shown to achieve more goals in the 2 benchmarks, the authors do not discuss the learning complexity of training two networks compared to one policy network as done in baselines. The authors conduct an ablation study on the computational complexity of planning at test-time for their approach. I believe it would help to evaluate the method better if they also show the number of trials used when comparing to baselines. \n\nCan the authors provide any real-time performance guarantees at test time?\n\nIt's not clear to me how the exploration and exploitation is traded off in the Data Collection section. The planner is used to generate a sequence of actions but an e-greedy policy is used for exploration. This is confusing to me.\n\nThe paper claims that the inverse dynamics formulation learns to represent controllabe aspects of the state. Has this been theoretically proven before? And have any empirical studies been done to show that this actually helps model-based RL to perform better than when using forward models? It would be interesting to compare against model-based RL techniques based on forward models, if avaialble, for this work.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2661/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2661/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Planning from Pixels using Inverse Dynamics Models", "authorids": ["~Keiran_Paster1", "~Sheila_A._McIlraith1", "~Jimmy_Ba1"], "authors": ["Keiran Paster", "Sheila A. McIlraith", "Jimmy Ba"], "keywords": ["model based reinforcement learning", "deep reinforcement learning", "multi-task learning", "deep learning", "goal-conditioned reinforcement learning"], "abstract": "Learning dynamics models in high-dimensional observation spaces can be challenging for model-based RL agents. We propose a novel way to learn models in a latent space by learning to predict sequences of future actions conditioned on task completion. These models track task-relevant environment dynamics over a distribution of tasks, while simultaneously serving as an effective heuristic for planning with sparse rewards. We evaluate our method on challenging visual goal completion tasks and show a substantial increase in performance compared to prior model-free approaches.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "paster|planning_from_pixels_using_inverse_dynamics_models", "one-sentence_summary": "GLAMOR learns a latent world model by learning to predict action sequences conditioned on task completion.", "pdf": "/pdf/e1667f4513f3892a3eac139e23ee5198363e6741.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npaster2021planning,\ntitle={Planning from Pixels using Inverse Dynamics Models},\nauthor={Keiran Paster and Sheila A. McIlraith and Jimmy Ba},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=V6BjBgku7Ro}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "V6BjBgku7Ro", "replyto": "V6BjBgku7Ro", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2661/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538091292, "tmdate": 1606915778702, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2661/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2661/-/Official_Review"}}}], "count": 12}