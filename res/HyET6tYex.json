{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396364353, "tcdate": 1486396364353, "number": 1, "id": "By40oMIdg", "invitation": "ICLR.cc/2017/conference/-/paper114/acceptance", "forum": "HyET6tYex", "replyto": "HyET6tYex", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "My overall conclusion is that the paper needs more work to be sufficiently convincing. I think the reviews are sufficiently careful. My recommendation is based on the fact that none of the reviewers supports acceptance."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Universality in halting time", "abstract": "The authors present empirical distributions for the halting time (measured by the number of iterations to reach a given accuracy) of optimization algorithms applied to two random systems: spin glasses and deep learning. Given an algorithm, which we take to be both the optimization routine and the form of the random landscape, the fluctuations of the halting time follow a distribution that remains unchanged even when the input is changed drastically. We observe two main classes, a Gumbel-like distribution that appears in Google searches, human decision times, QR factorization and spin glasses, and a Gaussian-like distribution that appears in conjugate gradient method, deep network with MNIST input data and deep network with random input data. This empirical evidence suggests presence of a class of distributions for which the halting time is independent of the underlying distribution under some conditions.", "pdf": "/pdf/9ba24ff08dfbca6e21b3bde7fba80f966453c21e.pdf", "TL;DR": "Normalized halting time distributions are independent of the input data distribution.", "paperhash": "sagun|universality_in_halting_time", "keywords": ["Optimization"], "conflicts": ["nyu.edu", "fb.com", "math.uci.edu"], "authors": ["Levent Sagun", "Thomas Trogdon", "Yann LeCun"], "authorids": ["leventsagun@gmail.com", "tom.trogdon@gmail.com", "yann@cs.nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396364854, "id": "ICLR.cc/2017/conference/-/paper114/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "HyET6tYex", "replyto": "HyET6tYex", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396364854}}}, {"tddate": null, "tmdate": 1485020900008, "tcdate": 1485020900008, "number": 1, "id": "B12JyQWvg", "invitation": "ICLR.cc/2017/conference/-/paper114/official/comment", "forum": "HyET6tYex", "replyto": "rJpQpu2Ig", "signatures": ["ICLR.cc/2017/conference/paper114/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper114/AnonReviewer3"], "content": {"title": "Regarding the conclusion", "comment": "The problem right now with your conclusion is not that you have not answered all questions in full detail in your paper. The problem is that you pretty much claim that you did.\n\nYour comment suggests that you are aware that the contribution is less important that what is portrayed in the conclusion. In fact you do not present any argument to show that your conclusion is accurate as it is.\n\n- If an accurate summary of your work is (as in your above comment):\nA: \"The primary goal of this work was to demonstrate the universality in the halting time can occur within machine learning and to propose potential implications.\"\n\n- Then what is written right now in your conclusion is not:\nB: \"This research attempts to exhibit cases where one can extract **answers to these questions in a robust and quantitative way**.\"\nand\nC: \"It also **validates the broad claims** made in Deift et al. (2015) that **universality is present in all or nearly all (sensible) computation**\n\nThe differences are striking:\n\"answers in a robust and quantitative way\" (in your paper) VS \"propose potential implications\" (in your above comment)\n\"universality is present  in nearly all computation\" (in your paper) VS \"universality [...] *can occur*\" (in your above comment)\n\n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Universality in halting time", "abstract": "The authors present empirical distributions for the halting time (measured by the number of iterations to reach a given accuracy) of optimization algorithms applied to two random systems: spin glasses and deep learning. Given an algorithm, which we take to be both the optimization routine and the form of the random landscape, the fluctuations of the halting time follow a distribution that remains unchanged even when the input is changed drastically. We observe two main classes, a Gumbel-like distribution that appears in Google searches, human decision times, QR factorization and spin glasses, and a Gaussian-like distribution that appears in conjugate gradient method, deep network with MNIST input data and deep network with random input data. This empirical evidence suggests presence of a class of distributions for which the halting time is independent of the underlying distribution under some conditions.", "pdf": "/pdf/9ba24ff08dfbca6e21b3bde7fba80f966453c21e.pdf", "TL;DR": "Normalized halting time distributions are independent of the input data distribution.", "paperhash": "sagun|universality_in_halting_time", "keywords": ["Optimization"], "conflicts": ["nyu.edu", "fb.com", "math.uci.edu"], "authors": ["Levent Sagun", "Thomas Trogdon", "Yann LeCun"], "authorids": ["leventsagun@gmail.com", "tom.trogdon@gmail.com", "yann@cs.nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287722401, "id": "ICLR.cc/2017/conference/-/paper114/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "HyET6tYex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper114/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper114/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper114/reviewers", "ICLR.cc/2017/conference/paper114/areachairs"], "cdate": 1485287722401}}}, {"tddate": null, "tmdate": 1484844071821, "tcdate": 1482066221717, "number": 1, "id": "BkLVK-NEx", "invitation": "ICLR.cc/2017/conference/-/paper114/official/review", "forum": "HyET6tYex", "replyto": "HyET6tYex", "signatures": ["ICLR.cc/2017/conference/paper114/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper114/AnonReviewer1"], "content": {"title": "Hard to see universality yet.", "rating": "5: Marginally below acceptance threshold", "review": "The idea of universality that is independent of input distribution and dimension, depending only on the algorithm is an appealing one. However, as an empirical study, this paper comes up somewhat short:\n\n1. Exactly one algorithm is shown for the deep learning example. It would have been more convincing to compare distributions with one or more algorithms. \n\n2. The definition (1),  and much of the work of Section 2.1 seems to have already been covered in Deift (2014), Section 1.3. In that paper, a number of different algorithms for the solution of linear systems are considered, and then the concept of universality becomes more plausible. I do not see enough of such algorithmic comparisons in this paper (same problem setup, different algorithms).\n\n3.  It seems to me that what practitioners might care about in practice are both the mean and variance in running times; these quantities are buried in (1). So I question how useful the distribution itself might be for algorithm tuning. \n\nAt the least, many more empirical comparisons should be provided to convince me that the universality holds across a broad range of algorithms.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Universality in halting time", "abstract": "The authors present empirical distributions for the halting time (measured by the number of iterations to reach a given accuracy) of optimization algorithms applied to two random systems: spin glasses and deep learning. Given an algorithm, which we take to be both the optimization routine and the form of the random landscape, the fluctuations of the halting time follow a distribution that remains unchanged even when the input is changed drastically. We observe two main classes, a Gumbel-like distribution that appears in Google searches, human decision times, QR factorization and spin glasses, and a Gaussian-like distribution that appears in conjugate gradient method, deep network with MNIST input data and deep network with random input data. This empirical evidence suggests presence of a class of distributions for which the halting time is independent of the underlying distribution under some conditions.", "pdf": "/pdf/9ba24ff08dfbca6e21b3bde7fba80f966453c21e.pdf", "TL;DR": "Normalized halting time distributions are independent of the input data distribution.", "paperhash": "sagun|universality_in_halting_time", "keywords": ["Optimization"], "conflicts": ["nyu.edu", "fb.com", "math.uci.edu"], "authors": ["Levent Sagun", "Thomas Trogdon", "Yann LeCun"], "authorids": ["leventsagun@gmail.com", "tom.trogdon@gmail.com", "yann@cs.nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483466144346, "id": "ICLR.cc/2017/conference/-/paper114/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper114/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper114/AnonReviewer1", "ICLR.cc/2017/conference/paper114/AnonReviewer3", "ICLR.cc/2017/conference/paper114/AnonReviewer4"], "reply": {"forum": "HyET6tYex", "replyto": "HyET6tYex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper114/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper114/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483466144346}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1484717853700, "tcdate": 1478233532476, "number": 114, "id": "HyET6tYex", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "HyET6tYex", "signatures": ["~Levent_Sagun1"], "readers": ["everyone"], "content": {"title": "Universality in halting time", "abstract": "The authors present empirical distributions for the halting time (measured by the number of iterations to reach a given accuracy) of optimization algorithms applied to two random systems: spin glasses and deep learning. Given an algorithm, which we take to be both the optimization routine and the form of the random landscape, the fluctuations of the halting time follow a distribution that remains unchanged even when the input is changed drastically. We observe two main classes, a Gumbel-like distribution that appears in Google searches, human decision times, QR factorization and spin glasses, and a Gaussian-like distribution that appears in conjugate gradient method, deep network with MNIST input data and deep network with random input data. This empirical evidence suggests presence of a class of distributions for which the halting time is independent of the underlying distribution under some conditions.", "pdf": "/pdf/9ba24ff08dfbca6e21b3bde7fba80f966453c21e.pdf", "TL;DR": "Normalized halting time distributions are independent of the input data distribution.", "paperhash": "sagun|universality_in_halting_time", "keywords": ["Optimization"], "conflicts": ["nyu.edu", "fb.com", "math.uci.edu"], "authors": ["Levent Sagun", "Thomas Trogdon", "Yann LeCun"], "authorids": ["leventsagun@gmail.com", "tom.trogdon@gmail.com", "yann@cs.nyu.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1484717672328, "tcdate": 1484717672328, "number": 7, "id": "rJgO0unUx", "invitation": "ICLR.cc/2017/conference/-/paper114/public/comment", "forum": "HyET6tYex", "replyto": "r1uorDYBe", "signatures": ["~Levent_Sagun1"], "readers": ["everyone"], "writers": ["~Levent_Sagun1"], "content": {"title": "Revisions and clarifications", "comment": "We agree that one would like to understand the true halting time.  But, if the halting time is universal (or nearly so), after normalizing to mean zero and variance one, it follows that the mean and the variance of the actual halting time are the relevant quantities: Higher moments are not important.\n\nWe appreciate the ICLR specific suggestions. They are certainly relevant experiments to perform. In our experience, this type of universality, being different in spirit than a lot of work in the community, can be difficult for the reader to digest. To attempt to remedy this issue, we decided to focus on stochastic gradient descent, giving plenty of space for an introduction. In the continuation of this work, we will certainly make an effort in the direction of these suggestions.\n\nRegarding the clarity (in the same order):\n- We changed this sentence to read: \"... follow a distribution that, after centering and scaling, remains unchanged even when the distribution on the landscape is changed.\" \n- We changed this sentence to read: \"... the stopping condition produces a halting time that is, essentially, the time to find the minimum.\"  This should be contrasted with the non-convex case where the true minimum is never found.\n- Section 1.2 is devoted to an example that was initially examined in Pfrang, Deift & Menon (2014).  We added a sentence, that points forward to this section, highlighting the meaning of the parameter:\n\"For example, in Section 1.2, A is the QR eigenvalue algorithm, B is the size of the matrix, epsilon is a small tolerance and E is given by a distribution on complex Hermitian (or real symmetric) matrices.\"\n- We modified this to read:  Here $x^{\\ell} \\in Z$ for $\\ell \\in  \\{1,...,S\\}$, where $Z$ is a random (ordered) sample of size $S$ from the training examples.\n- We added the following sentence near where $M$ is introduced, \"The choice of the integer $M$, which is the inner dimension of the matrices in the product $XX^*$, is critical for the existence of universality.\"\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Universality in halting time", "abstract": "The authors present empirical distributions for the halting time (measured by the number of iterations to reach a given accuracy) of optimization algorithms applied to two random systems: spin glasses and deep learning. Given an algorithm, which we take to be both the optimization routine and the form of the random landscape, the fluctuations of the halting time follow a distribution that remains unchanged even when the input is changed drastically. We observe two main classes, a Gumbel-like distribution that appears in Google searches, human decision times, QR factorization and spin glasses, and a Gaussian-like distribution that appears in conjugate gradient method, deep network with MNIST input data and deep network with random input data. This empirical evidence suggests presence of a class of distributions for which the halting time is independent of the underlying distribution under some conditions.", "pdf": "/pdf/9ba24ff08dfbca6e21b3bde7fba80f966453c21e.pdf", "TL;DR": "Normalized halting time distributions are independent of the input data distribution.", "paperhash": "sagun|universality_in_halting_time", "keywords": ["Optimization"], "conflicts": ["nyu.edu", "fb.com", "math.uci.edu"], "authors": ["Levent Sagun", "Thomas Trogdon", "Yann LeCun"], "authorids": ["leventsagun@gmail.com", "tom.trogdon@gmail.com", "yann@cs.nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287722532, "id": "ICLR.cc/2017/conference/-/paper114/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HyET6tYex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper114/reviewers", "ICLR.cc/2017/conference/paper114/areachairs"], "cdate": 1485287722532}}}, {"tddate": null, "tmdate": 1484717348790, "tcdate": 1484717348790, "number": 6, "id": "rJpQpu2Ig", "invitation": "ICLR.cc/2017/conference/-/paper114/public/comment", "forum": "HyET6tYex", "replyto": "H1Wt5Cd4x", "signatures": ["~Levent_Sagun1"], "readers": ["everyone"], "writers": ["~Levent_Sagun1"], "content": {"title": "A note of dispute and a constructive response to reviewers concerns", "comment": "We appreciate the detailed look on the paper and we strongly dispute the charge that our methods are unsound. This paper aims solely to present experimentally observed phenomena that machine learning computations exhibit. We address the specific concerns below.  \n\nRegarding the epsilon dependency:\nIn the recent paper of Deift & Trogdon (2017, arXiv:1701.01896) the authors prove universality for the time to compute the top eigenvalue of a matrix. With the chosen scaling, the gap between the top eigenvalue and the second-largest eigenvalue shrinks with N. And so, epsilon must shrink with N so that one can be sure the top eigenvalue is the one being computed. But in practice, epsilon = 10^(-14) will suffice for all matrices encountered. In short, the N dependence is needed for theoretical justification and is often an artifact of the scaling of the problem, and importantly, the dependence could be trivial.\n\nRegarding naive algorithms:\nClearly, an algorithm is naive if there is a better algorithm. But, such an over-simplified characterization may not be of any use in computation: Given an algorithm, how does one know it is naive? Is a naive algorithm still faster on some problems than a more sophisticated algorithm? Is it comparable in a statistical sense? The phenomenon that we discuss was shown by Deift et al. to persist in numerous algorithms. Our work aims to demonstrate that this phenomenon exists in machine learning and optimization (in 4 settings, with additional compelling data from Google searches). \n\nRegarding Eq 1 and figures:\nIn our computations we chose N large and epsilon small. It is true that epsilon and N should vary and we did do these computations and we have plots for them in the appendix. We will add their halting time histograms, as well.\n\nRegarding the questions raised:\nThe referee raises many questions, and points out that we have not answered questions we raise in full detail. We accept this criticism. The primary goal of this work was to demonstrate the universality in the halting time can occur within machine learning and to propose potential implications. There are many open problems related to this line of research and we do not claim that the book is closed on such matters.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Universality in halting time", "abstract": "The authors present empirical distributions for the halting time (measured by the number of iterations to reach a given accuracy) of optimization algorithms applied to two random systems: spin glasses and deep learning. Given an algorithm, which we take to be both the optimization routine and the form of the random landscape, the fluctuations of the halting time follow a distribution that remains unchanged even when the input is changed drastically. We observe two main classes, a Gumbel-like distribution that appears in Google searches, human decision times, QR factorization and spin glasses, and a Gaussian-like distribution that appears in conjugate gradient method, deep network with MNIST input data and deep network with random input data. This empirical evidence suggests presence of a class of distributions for which the halting time is independent of the underlying distribution under some conditions.", "pdf": "/pdf/9ba24ff08dfbca6e21b3bde7fba80f966453c21e.pdf", "TL;DR": "Normalized halting time distributions are independent of the input data distribution.", "paperhash": "sagun|universality_in_halting_time", "keywords": ["Optimization"], "conflicts": ["nyu.edu", "fb.com", "math.uci.edu"], "authors": ["Levent Sagun", "Thomas Trogdon", "Yann LeCun"], "authorids": ["leventsagun@gmail.com", "tom.trogdon@gmail.com", "yann@cs.nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287722532, "id": "ICLR.cc/2017/conference/-/paper114/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HyET6tYex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper114/reviewers", "ICLR.cc/2017/conference/paper114/areachairs"], "cdate": 1485287722532}}}, {"tddate": null, "tmdate": 1484716230644, "tcdate": 1484716230644, "number": 5, "id": "Hy1C_OnUe", "invitation": "ICLR.cc/2017/conference/-/paper114/public/comment", "forum": "HyET6tYex", "replyto": "BkLVK-NEx", "signatures": ["~Levent_Sagun1"], "readers": ["everyone"], "writers": ["~Levent_Sagun1"], "content": {"title": "Clarification for the reviewer comments", "comment": "In this paper we largely focus on one widely used method: (stochastic) gradient descent.  We do apply this to 4 different random landscapes.  We consider the method and the random landscape, combined, to constitute an algorithm.   This adds to the convincing empirical and rigorous evidence obtained by Deift et al. that universality is a persistent phenomenon in numerical computation. We also present the example of Google searches --- yet another piece of evidence.  For this reason we believe the current work is sufficiently comprehensive.  But we do concede that there is much more work to be done in these directions, both rigorous and experimental.\n\nWe agree that the mean and the variance are likely the most important in practice.  When considering the Central Limit Theorem, one knows that the mean and the variance are the only required pieces of information precisely because the theorem gives a universal limit.  This is our point here: To consistently argue that the mean and the variance are the most important, one wants a universal limit theorem."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Universality in halting time", "abstract": "The authors present empirical distributions for the halting time (measured by the number of iterations to reach a given accuracy) of optimization algorithms applied to two random systems: spin glasses and deep learning. Given an algorithm, which we take to be both the optimization routine and the form of the random landscape, the fluctuations of the halting time follow a distribution that remains unchanged even when the input is changed drastically. We observe two main classes, a Gumbel-like distribution that appears in Google searches, human decision times, QR factorization and spin glasses, and a Gaussian-like distribution that appears in conjugate gradient method, deep network with MNIST input data and deep network with random input data. This empirical evidence suggests presence of a class of distributions for which the halting time is independent of the underlying distribution under some conditions.", "pdf": "/pdf/9ba24ff08dfbca6e21b3bde7fba80f966453c21e.pdf", "TL;DR": "Normalized halting time distributions are independent of the input data distribution.", "paperhash": "sagun|universality_in_halting_time", "keywords": ["Optimization"], "conflicts": ["nyu.edu", "fb.com", "math.uci.edu"], "authors": ["Levent Sagun", "Thomas Trogdon", "Yann LeCun"], "authorids": ["leventsagun@gmail.com", "tom.trogdon@gmail.com", "yann@cs.nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287722532, "id": "ICLR.cc/2017/conference/-/paper114/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HyET6tYex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper114/reviewers", "ICLR.cc/2017/conference/paper114/areachairs"], "cdate": 1485287722532}}}, {"tddate": null, "tmdate": 1483466143699, "tcdate": 1483466143699, "number": 3, "id": "r1uorDYBe", "invitation": "ICLR.cc/2017/conference/-/paper114/official/review", "forum": "HyET6tYex", "replyto": "HyET6tYex", "signatures": ["ICLR.cc/2017/conference/paper114/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper114/AnonReviewer4"], "content": {"title": "interesting idea, shortcomings in evaluation and clarity", "rating": "5: Marginally below acceptance threshold", "review": "The authors explore whether the halting time distributions for various algorithms in various settings exhibit \"universality\", i.e. after rescaling to zero mean and unit variance, the distribution does not depend on stopping parameter, dimensionality and ensemble.\n\nThe idea of the described universality is very interesting. However I see several shortcomings in the paper:\n\nIn order to be of practical relevance, the actual stopping time might be more relevant than the scaled one. The discussion of exponential tailed halting time distributions is a good start, but I am not sure how often this might be actually helpful. Still, the findings in the paper might be interesting from a theoretical point of view.\n\nEspecially for ICLR, I think it would have been more interesting to look into comparisons between stochastic gradient descent, momentum, ADAM etc on different deep learning architectures. Over which of those parameters does universality hold?. How can different initializations influence the halting time distribution? I would expect a sensible initialization to cut of part of the right tail of the distribution.\n\nAdditionally, I found the paper quite hard to read. Here are some clarity issues:\n\n- abstract: \"even when the input is changed drastically\": From the abstract I'm not sure what \"input\" refers to, here\n- I. Introduction: \"where the stopping condition is, essentially, the time to find the minimum\": this doesn't seem to make sense, a condition is not a time. I guess the authors wanted to say that the stopping condition is that the minimum has been reached?\n- I.1 the notions of dimension N, epsilon and ensemble E are introduced without any clarification what they are. From the later parts of the paper I got some ideas and examples, but here it is very hard to understand what these parameters should be (just some examples would be already helpful)\n- I.3 \"We use x^\\ell for \\ell \\in Z=\\{1, \\dots, S\\} where Z is a random sample from of training samples\" This formulation doesn't make sense. Either Z is a random sample, or Z={1, ..., S}.\n- II.1 it took me a long time to find the meaning of M. As this parameter seems to be crucial for universality in this case, it would be very helpful to point out more explicitly what it refers to.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Universality in halting time", "abstract": "The authors present empirical distributions for the halting time (measured by the number of iterations to reach a given accuracy) of optimization algorithms applied to two random systems: spin glasses and deep learning. Given an algorithm, which we take to be both the optimization routine and the form of the random landscape, the fluctuations of the halting time follow a distribution that remains unchanged even when the input is changed drastically. We observe two main classes, a Gumbel-like distribution that appears in Google searches, human decision times, QR factorization and spin glasses, and a Gaussian-like distribution that appears in conjugate gradient method, deep network with MNIST input data and deep network with random input data. This empirical evidence suggests presence of a class of distributions for which the halting time is independent of the underlying distribution under some conditions.", "pdf": "/pdf/9ba24ff08dfbca6e21b3bde7fba80f966453c21e.pdf", "TL;DR": "Normalized halting time distributions are independent of the input data distribution.", "paperhash": "sagun|universality_in_halting_time", "keywords": ["Optimization"], "conflicts": ["nyu.edu", "fb.com", "math.uci.edu"], "authors": ["Levent Sagun", "Thomas Trogdon", "Yann LeCun"], "authorids": ["leventsagun@gmail.com", "tom.trogdon@gmail.com", "yann@cs.nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483466144346, "id": "ICLR.cc/2017/conference/-/paper114/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper114/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper114/AnonReviewer1", "ICLR.cc/2017/conference/paper114/AnonReviewer3", "ICLR.cc/2017/conference/paper114/AnonReviewer4"], "reply": {"forum": "HyET6tYex", "replyto": "HyET6tYex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper114/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper114/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483466144346}}}, {"tddate": null, "tmdate": 1482381945053, "tcdate": 1482381945053, "number": 2, "id": "H1Wt5Cd4x", "invitation": "ICLR.cc/2017/conference/-/paper114/official/review", "forum": "HyET6tYex", "replyto": "HyET6tYex", "signatures": ["ICLR.cc/2017/conference/paper114/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper114/AnonReviewer3"], "content": {"title": "Unusual research. Unsound methodology. conclusions are not supported by the content.", "rating": "2: Strong rejection", "review": "Summary\n\n\nFor several algorithms, previous research has shown that the halting time follows a two-parameter distribution (the so-called universal property investigated by the authors). In this work, the authors extend the investigation to new algorithms (spin-glass, gradient descent in deep learning).\n\nAn algorithm is considered to satisfy the universality property when the centered/scaled halting time fluctuations (empirical distribution of halting times) depend on the algorithm but do not depend on the target accuracy epsilon, an intrinsic measure of dimension N, the probability distribution/random ensemble. (This is clear from Eq 1 where on the left the empirical halting time distribution depends on epsilon, N, A, E and on the right, the approximation only depends on the algorithm)\n\nThe authors argue that empirically, the universal property is observed when both algorithms (spin glass and deep learning) perform well and that it is not observed when they do not perform well.\n\nA moment-based indicator is introduced to assess whether universality is observed.\n\n\nReview\n\n\nThis paper presents several problems.\n\n\n\npage 2: \u201c[\u2026] for sufficiently large N and eps = eps(N)\u201d\n\nThe dependence of epsilon on N is troubling.\n\n\n\npage 3: \u201cUniversality is a measure of stability in an algorithm [\u2026] For example [\u2026] halting time for the power method [\u2026] has infinite expectation and hence this type of universality is *not* present. One could use this to conclude that the power method is naive. Therefore the presence of universality is a desirable feature of a numerical method\u201d\n\nNo. An algorithm is naive if there are better ways to answer the problem. One could not conclude from a halting time with infinite expectation (e.g. solving a problem extremely quickly 99% of the time, and looping forever in 1% of cases) or infinite variance, that the algorithm is naive.\u2028\nMoreover the universal property is more restrictive than having a finite halting time expectation. Even if in many specific cases, having a finite halting time expectation is a desirable property, showing that the presence of universality is desirable would require a demonstration that the other more restrictive aspects are also desirable.\u2028\nAlso, the paragraph only concerns one algorithm. why would the conclusions generalise to all numerical methods ?\u2028\nEven if the universality property is arguably desirable (i.e. event if the conclusion of this paragraph is assumed correct), the paragraph does not support the given conclusion.\n\n\n\nComparing Eq 1 and figures 2,3,4,5\u2028\nFrom Eq 1, universality means that the centered/scaled halting time fluctuations (which depend on A, epsilon, N, E) can be approximated by a distribution that only depends on A (not on epsilon, N, E) but in the experiments only E varies (figures 2,3,4,5). The validity of the approximation with varying epsilon or N is never tested\n\n\n\nThe ensembles/distributions parameter E (on which halting fluctuations should not depend) and the algorithm A (on which halting fluctuations are allowed to depend) are not well defined, especially w.r.t. the common use of the words. In the optimisation setting we are told that the functional form of the landscape function is part of A (in answer to the question of a reviewer) but what is part of the functional form ? what about computations where the landscape has no known functional form (black box) ?\n\n\n\nThe conclusion claims that the paper \u201cattempts to exhibit cases\u201d where one can answer 5 questions in a robust and quantitative way.\n\nQuestion 1: \u201cWhat are the conditions on the ensembles and the model that lead to such universality ?\u201d\u2028The only quantitative way would be to use the moments based indicator however there is only one example of universality not being observed which concerns only one algorithm (conjugate gradient) and one type of failure (when M = N). This does not demonstrate robustness of the method.\n\nQuestion 2: \u201cWhat constitutes a good set of hyper parameters for a given algorithm ?\u201d\nThe proposed way to choose would be to test whether universality is observed. If it is then the hyper parameters are good, if not the hyper parameters are bad. The correspondance between bad hyper-parameters and observing no universality concerns only one algorithm and one type of failure. Other algorithms may fail in the universal regime or perform well in the non universal regime. The paper does not show how to answer this question in a robust way.\n\nQuestion 3: \"How can we go beyond inspection when tuning a system ?\u2028\"\nThe question is too vague and general and there is probably no robust and quantitative way to answer it at all.\n\nQuestion 4: \"How can we infer if an algorithm is a good match to the system at hand ?\u2028\"\nThe paper fails to demonstrate convincingly that universality is either a good or robust way to approach the very few studied algorithms. The suggested generalisation to all systems and algorithms is extremely far fetched.\n\nQuestion 5: \"What is the connection between the universal regime and the structure of the landscape ?\"\n\u2028Same as before, the question is extremely vague and cannot be answered in a robust or quantitative way at all. The fact that what corresponds to A and what corresponds to E is not clear does not help.\n\n\nIn the conclusion it is written that the paper validates the claim that universality is present in all or nearly all sensible computation. It does not. The paper does not properly test whether universality is present (only 1 parameter in 3 that should not vary is tested). The paper does not properly test whether universality is lost when the computation is no longer sensible (only one failure case tested). Finally the experiments do not apply to all or nearly all computations but only to very few  specific algorithms.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Universality in halting time", "abstract": "The authors present empirical distributions for the halting time (measured by the number of iterations to reach a given accuracy) of optimization algorithms applied to two random systems: spin glasses and deep learning. Given an algorithm, which we take to be both the optimization routine and the form of the random landscape, the fluctuations of the halting time follow a distribution that remains unchanged even when the input is changed drastically. We observe two main classes, a Gumbel-like distribution that appears in Google searches, human decision times, QR factorization and spin glasses, and a Gaussian-like distribution that appears in conjugate gradient method, deep network with MNIST input data and deep network with random input data. This empirical evidence suggests presence of a class of distributions for which the halting time is independent of the underlying distribution under some conditions.", "pdf": "/pdf/9ba24ff08dfbca6e21b3bde7fba80f966453c21e.pdf", "TL;DR": "Normalized halting time distributions are independent of the input data distribution.", "paperhash": "sagun|universality_in_halting_time", "keywords": ["Optimization"], "conflicts": ["nyu.edu", "fb.com", "math.uci.edu"], "authors": ["Levent Sagun", "Thomas Trogdon", "Yann LeCun"], "authorids": ["leventsagun@gmail.com", "tom.trogdon@gmail.com", "yann@cs.nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483466144346, "id": "ICLR.cc/2017/conference/-/paper114/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper114/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper114/AnonReviewer1", "ICLR.cc/2017/conference/paper114/AnonReviewer3", "ICLR.cc/2017/conference/paper114/AnonReviewer4"], "reply": {"forum": "HyET6tYex", "replyto": "HyET6tYex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper114/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper114/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483466144346}}}, {"tddate": null, "tmdate": 1482340495519, "tcdate": 1482340495519, "number": 4, "id": "ByPqOVuEl", "invitation": "ICLR.cc/2017/conference/-/paper114/public/comment", "forum": "HyET6tYex", "replyto": "rJca3f-4l", "signatures": ["~Levent_Sagun1"], "readers": ["everyone"], "writers": ["~Levent_Sagun1"], "content": {"title": "Response to the example", "comment": "The given example would indeed have a bimodal halting time distribution. This doesn't present any issues from our formalism as the universal distribution depends on the algorithm.  We are also careful to include the functional form of the cost function as part of the algorithm. An ingredient that is missing in the above example is the underlying source of randomness.  With appropriate randomness (in both the initial guesses and the cost function), the statement of universality would just boil down to seeing the same shape independent of the distribution of the underlying samples (be it uni-modal or multi-modal). This is indeed what we see in the examples (convex and non-convex) that we examined. \n\nWe also attempted to clarify the terminology. Universality, when literally taken, can be understood to be quite broad, and it certainly sounds too strong. However, in our work we stick to the statistical physics notion of universality, in that, the phenomenon we observe at the large scale, under certain assumptions, is independent of the structure of the microstructure. Clearly devising tailor-made examples that break the phenomenon would not be hard, for instance, breaking the classical central limit theorem is easily accomplished by using heavy-tailed distributions."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Universality in halting time", "abstract": "The authors present empirical distributions for the halting time (measured by the number of iterations to reach a given accuracy) of optimization algorithms applied to two random systems: spin glasses and deep learning. Given an algorithm, which we take to be both the optimization routine and the form of the random landscape, the fluctuations of the halting time follow a distribution that remains unchanged even when the input is changed drastically. We observe two main classes, a Gumbel-like distribution that appears in Google searches, human decision times, QR factorization and spin glasses, and a Gaussian-like distribution that appears in conjugate gradient method, deep network with MNIST input data and deep network with random input data. This empirical evidence suggests presence of a class of distributions for which the halting time is independent of the underlying distribution under some conditions.", "pdf": "/pdf/9ba24ff08dfbca6e21b3bde7fba80f966453c21e.pdf", "TL;DR": "Normalized halting time distributions are independent of the input data distribution.", "paperhash": "sagun|universality_in_halting_time", "keywords": ["Optimization"], "conflicts": ["nyu.edu", "fb.com", "math.uci.edu"], "authors": ["Levent Sagun", "Thomas Trogdon", "Yann LeCun"], "authorids": ["leventsagun@gmail.com", "tom.trogdon@gmail.com", "yann@cs.nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287722532, "id": "ICLR.cc/2017/conference/-/paper114/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HyET6tYex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper114/reviewers", "ICLR.cc/2017/conference/paper114/areachairs"], "cdate": 1485287722532}}}, {"tddate": null, "tmdate": 1481874626149, "tcdate": 1481874626149, "number": 3, "id": "rJca3f-4l", "invitation": "ICLR.cc/2017/conference/-/paper114/public/comment", "forum": "HyET6tYex", "replyto": "HyET6tYex", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "From ICLR 2016", "comment": "It is an interesting paper. However, I would like to describe a hypothesis/counter-example and I hope you will\neasily reject it. Otherwise, I will fail to see any universality in the discussed \"universality in halting time\".\n\nI would like to present a toy minimization problem where f(x) is defined in [0,1] and has two global\noptima in 0.0 and 1.0 respectively. f(x) has its maximum in 0.5 but f(x) decays somewhat differently to the \"left\"\nand to the \"right\" optima. These somewhat different decays (shapes of f(x)) impact the way some optimizer A minimizes\nf(x). My A can be e.g. a deterministic local search algorithm such that if it is initialized in the basin of attraction [0,0.5], it will stay/search there. Thus, the starting position will determine in which global optima it will end up. To observe the halting distribution, I run A starting from different starting positions generated uniformly at random in [0,1]. I expect the halting distribution to have two peaks: the first peak corresponds to a typical number of iterations required to reach some epsilon around the \"left\" optima, the second peak would correspond to the \"right\" optima. Question: Why there are two peaks and not one? Answer: Because, as mentioned before, the shapes/decays of f(x) on the left and right side are different and this affects our algorithm A, i.e., the number of iterations to reach some epsilon precision. Question: Is this a problem to have two peaks in the halting distribution not like the ones shown in figures? Answer: No, it is perfectly fine. What is not fine is that when I run the same algorithm A on another problem which is unimodal, then I will get a completely different halting distribution, e.g., like the \"one peak\" ones shown in the paper. Question: Where is the problem? Answer: The problem is that these observations contradict the discussed universality in halting time distribution which is an intrinsic thing to A and does not depend on the problem. Question: This was stated for large dimension N and not one-dimensional problem. Answer: Increase the dimensionality of the toy problem preserving the idea of the two (multiple) optima with, e.g., basins of attraction of equal size (keep in mind the  course of dimensionality). Question: There is a restriction on the class of ensembles E. Answer: If we restrict our-self to unimodal problems then I barely see \"universality\". Moreover, unimodal non-convex problems may exhibit the same properties as multi-modal ones, e.g., make a horizontal one-dimensional slice of 2-dimensional Rosenbrock function. Question: Why we see these nice-looking distribution for very different problems? Answer: because for some problems the multi-modality is negligeable and the dynamical system behind the optimizer may behave similarly for different values of N, especially for the large ones. Nevertheless, to claim it to be the \"universal law\" is misleading since it depends on the properties of the problem at hand and is not strictly intrinsic to the optimizer. Moreover, for the same (class of) problem the landscape may qualitatively change with increasing N. Rosenbrock function is a good example, it is uni-modal for 2 variables and multi-modal for multiple variables which would affect the halting time distribution. The \"threshold\" dimension value 2 can be turned to 2 billion if needed."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Universality in halting time", "abstract": "The authors present empirical distributions for the halting time (measured by the number of iterations to reach a given accuracy) of optimization algorithms applied to two random systems: spin glasses and deep learning. Given an algorithm, which we take to be both the optimization routine and the form of the random landscape, the fluctuations of the halting time follow a distribution that remains unchanged even when the input is changed drastically. We observe two main classes, a Gumbel-like distribution that appears in Google searches, human decision times, QR factorization and spin glasses, and a Gaussian-like distribution that appears in conjugate gradient method, deep network with MNIST input data and deep network with random input data. This empirical evidence suggests presence of a class of distributions for which the halting time is independent of the underlying distribution under some conditions.", "pdf": "/pdf/9ba24ff08dfbca6e21b3bde7fba80f966453c21e.pdf", "TL;DR": "Normalized halting time distributions are independent of the input data distribution.", "paperhash": "sagun|universality_in_halting_time", "keywords": ["Optimization"], "conflicts": ["nyu.edu", "fb.com", "math.uci.edu"], "authors": ["Levent Sagun", "Thomas Trogdon", "Yann LeCun"], "authorids": ["leventsagun@gmail.com", "tom.trogdon@gmail.com", "yann@cs.nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287722532, "id": "ICLR.cc/2017/conference/-/paper114/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HyET6tYex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper114/reviewers", "ICLR.cc/2017/conference/paper114/areachairs"], "cdate": 1485287722532}}}, {"tddate": null, "tmdate": 1481661352784, "tcdate": 1481661352777, "number": 2, "id": "Hy-nsRpXe", "invitation": "ICLR.cc/2017/conference/-/paper114/public/comment", "forum": "HyET6tYex", "replyto": "BJFE0_17g", "signatures": ["~Levent_Sagun1"], "readers": ["everyone"], "writers": ["~Levent_Sagun1"], "content": {"title": "Clarification on motivation", "comment": "A short answer would be to be able to estimate the time it takes for an algorithm to converge at a given accuracy: Once the shape is known, one would collect small number of samples to estimate the mean and the standard deviation of the halting time for an approximation. \n\nSuch an approximation may have both applied and theoretical consequences: On the one hand, a lot of theoretical research typically focus on the error bounds as a function of accuracy, step-size, number of steps and possibly other parameters with possible asymptotical tightness. It may well be possible that such bounds are much further away from the right tail of the distributions we find much like the comparison between exact Chebyshev bounds for the worst case and CLT approximations for typical behaviors. We believe that such an approximation would be at least as valuable as tight-asymptotic bounds. \n\nFrom an applied point of view, the typical behaviour may be an indicative of robustness for the system depending on wheter the halting time distribution falls into the desired universal class or not which may be tested with random data on a smaller model. With this in mind, the question of the hyperparameter choice, and algorithm-system match can be interpreted as the inverse questions to what we present in our work. For instance, setting a sharper stopping criteria might result in the loss of universality. It can be seen in figure 6 of the appendix, that the right side with sharper stopping criteria corresponds to a region where the training time increases by quite a lot without any meaningful returns in the performance of the system. If we are able to determine the universal regime as a function of such parameters, or if we can experimentally identify such a region, then we would be able to assign more principled targets for the given algorithm-system. \n\nThank you for bringing this question to our attention for further emphasis. We added more clarification of the points above in our conclusion section and at the beginning of our appendix inculing further references. And we would like to pursue similar explorations (possibly in collaboration with interested readers) of this inverse question as part of the implications of our current work in the near future.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Universality in halting time", "abstract": "The authors present empirical distributions for the halting time (measured by the number of iterations to reach a given accuracy) of optimization algorithms applied to two random systems: spin glasses and deep learning. Given an algorithm, which we take to be both the optimization routine and the form of the random landscape, the fluctuations of the halting time follow a distribution that remains unchanged even when the input is changed drastically. We observe two main classes, a Gumbel-like distribution that appears in Google searches, human decision times, QR factorization and spin glasses, and a Gaussian-like distribution that appears in conjugate gradient method, deep network with MNIST input data and deep network with random input data. This empirical evidence suggests presence of a class of distributions for which the halting time is independent of the underlying distribution under some conditions.", "pdf": "/pdf/9ba24ff08dfbca6e21b3bde7fba80f966453c21e.pdf", "TL;DR": "Normalized halting time distributions are independent of the input data distribution.", "paperhash": "sagun|universality_in_halting_time", "keywords": ["Optimization"], "conflicts": ["nyu.edu", "fb.com", "math.uci.edu"], "authors": ["Levent Sagun", "Thomas Trogdon", "Yann LeCun"], "authorids": ["leventsagun@gmail.com", "tom.trogdon@gmail.com", "yann@cs.nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287722532, "id": "ICLR.cc/2017/conference/-/paper114/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HyET6tYex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper114/reviewers", "ICLR.cc/2017/conference/paper114/areachairs"], "cdate": 1485287722532}}}, {"tddate": null, "tmdate": 1481658147180, "tcdate": 1481658147174, "number": 1, "id": "Skj7JATXe", "invitation": "ICLR.cc/2017/conference/-/paper114/public/comment", "forum": "HyET6tYex", "replyto": "r13W9vuml", "signatures": ["~Levent_Sagun1"], "readers": ["everyone"], "writers": ["~Levent_Sagun1"], "content": {"title": "x-axis clarification for figure 1", "comment": "It should have been the halting time fluctuations just like the x-axis for other figures in the paper. The halting times, in the case of figure 1, are obtained by normalizing the measured time it takes to get an answer for the two experiments described: Google searches and questions asked on subjects. We fixed the x-axis and will include in the updated pdf."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Universality in halting time", "abstract": "The authors present empirical distributions for the halting time (measured by the number of iterations to reach a given accuracy) of optimization algorithms applied to two random systems: spin glasses and deep learning. Given an algorithm, which we take to be both the optimization routine and the form of the random landscape, the fluctuations of the halting time follow a distribution that remains unchanged even when the input is changed drastically. We observe two main classes, a Gumbel-like distribution that appears in Google searches, human decision times, QR factorization and spin glasses, and a Gaussian-like distribution that appears in conjugate gradient method, deep network with MNIST input data and deep network with random input data. This empirical evidence suggests presence of a class of distributions for which the halting time is independent of the underlying distribution under some conditions.", "pdf": "/pdf/9ba24ff08dfbca6e21b3bde7fba80f966453c21e.pdf", "TL;DR": "Normalized halting time distributions are independent of the input data distribution.", "paperhash": "sagun|universality_in_halting_time", "keywords": ["Optimization"], "conflicts": ["nyu.edu", "fb.com", "math.uci.edu"], "authors": ["Levent Sagun", "Thomas Trogdon", "Yann LeCun"], "authorids": ["leventsagun@gmail.com", "tom.trogdon@gmail.com", "yann@cs.nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287722532, "id": "ICLR.cc/2017/conference/-/paper114/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HyET6tYex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper114/reviewers", "ICLR.cc/2017/conference/paper114/areachairs"], "cdate": 1485287722532}}}, {"tddate": null, "tmdate": 1481304580197, "tcdate": 1481304580192, "number": 2, "id": "r13W9vuml", "invitation": "ICLR.cc/2017/conference/-/paper114/pre-review/question", "forum": "HyET6tYex", "replyto": "HyET6tYex", "signatures": ["ICLR.cc/2017/conference/paper114/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper114/AnonReviewer1"], "content": {"title": "Figure 1", "question": "In Figure 1, what does the x-axis represent? The term \"search time fluctuations\" is a little confusing. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Universality in halting time", "abstract": "The authors present empirical distributions for the halting time (measured by the number of iterations to reach a given accuracy) of optimization algorithms applied to two random systems: spin glasses and deep learning. Given an algorithm, which we take to be both the optimization routine and the form of the random landscape, the fluctuations of the halting time follow a distribution that remains unchanged even when the input is changed drastically. We observe two main classes, a Gumbel-like distribution that appears in Google searches, human decision times, QR factorization and spin glasses, and a Gaussian-like distribution that appears in conjugate gradient method, deep network with MNIST input data and deep network with random input data. This empirical evidence suggests presence of a class of distributions for which the halting time is independent of the underlying distribution under some conditions.", "pdf": "/pdf/9ba24ff08dfbca6e21b3bde7fba80f966453c21e.pdf", "TL;DR": "Normalized halting time distributions are independent of the input data distribution.", "paperhash": "sagun|universality_in_halting_time", "keywords": ["Optimization"], "conflicts": ["nyu.edu", "fb.com", "math.uci.edu"], "authors": ["Levent Sagun", "Thomas Trogdon", "Yann LeCun"], "authorids": ["leventsagun@gmail.com", "tom.trogdon@gmail.com", "yann@cs.nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481304580913, "id": "ICLR.cc/2017/conference/-/paper114/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper114/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper114/AnonReviewer3", "ICLR.cc/2017/conference/paper114/AnonReviewer1"], "reply": {"forum": "HyET6tYex", "replyto": "HyET6tYex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper114/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper114/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481304580913}}}, {"tddate": null, "tmdate": 1480719921193, "tcdate": 1480719921184, "number": 1, "id": "BJFE0_17g", "invitation": "ICLR.cc/2017/conference/-/paper114/pre-review/question", "forum": "HyET6tYex", "replyto": "HyET6tYex", "signatures": ["ICLR.cc/2017/conference/paper114/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper114/AnonReviewer3"], "content": {"title": "Why", "question": "What you do in the paper is quite clear however i do not fully understand your motivations. In a spirit of fairness, I prefer to candidly ask difficult questions now that you have the time to answer them.\n\nWhy should I care about the distribution halting time and the universal property ? How does this relate (in a useful way) to deep networks, learning representations ?\n\nIn the conclusion you write:\n\n\"What constitutes a good set of hyperparameters for a given algorithm? [...] How can we infer if an algorithm is a good match to the system at hand? [NDR: two questions I relate to]\nYou then write:\n\"This research attempts to exhibit cases where one can extract answers to these questions in a robust and quantitative way\"\nHowever I do not find where this paper answers (or attempts to answer) these questions. Could you point me to the right pages/paragraphs ?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Universality in halting time", "abstract": "The authors present empirical distributions for the halting time (measured by the number of iterations to reach a given accuracy) of optimization algorithms applied to two random systems: spin glasses and deep learning. Given an algorithm, which we take to be both the optimization routine and the form of the random landscape, the fluctuations of the halting time follow a distribution that remains unchanged even when the input is changed drastically. We observe two main classes, a Gumbel-like distribution that appears in Google searches, human decision times, QR factorization and spin glasses, and a Gaussian-like distribution that appears in conjugate gradient method, deep network with MNIST input data and deep network with random input data. This empirical evidence suggests presence of a class of distributions for which the halting time is independent of the underlying distribution under some conditions.", "pdf": "/pdf/9ba24ff08dfbca6e21b3bde7fba80f966453c21e.pdf", "TL;DR": "Normalized halting time distributions are independent of the input data distribution.", "paperhash": "sagun|universality_in_halting_time", "keywords": ["Optimization"], "conflicts": ["nyu.edu", "fb.com", "math.uci.edu"], "authors": ["Levent Sagun", "Thomas Trogdon", "Yann LeCun"], "authorids": ["leventsagun@gmail.com", "tom.trogdon@gmail.com", "yann@cs.nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481304580913, "id": "ICLR.cc/2017/conference/-/paper114/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper114/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper114/AnonReviewer3", "ICLR.cc/2017/conference/paper114/AnonReviewer1"], "reply": {"forum": "HyET6tYex", "replyto": "HyET6tYex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper114/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper114/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481304580913}}}], "count": 15}