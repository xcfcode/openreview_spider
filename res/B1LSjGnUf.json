{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124476703, "tcdate": 1518246718039, "number": 54, "cdate": 1518246718039, "id": "B1LSjGnUf", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "B1LSjGnUf", "signatures": ["~Hyo-Eun_Kim1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Exploiting model capacity by constraining within-batch features to be orthogonal", "abstract": "Deep networks have been shown to greatly benefit from large model capacity when trained using various recent deep learning techniques. But at the same time, features in such large capacity networks have a potential to be redundant. In this work, we propose a new regularization method to exploit the given network capacity effectively. By minimizing the redundancy among in-layer filters and the correlation between in-batch features at the same time, we are able to achieve better performance with the same network architecture. Experiments with CIFAR-10/100 show that simultaneously constraining both the in-layer filters to be orthonormal and the in-batch features to be orthogonal is beneficial in efficiently utilizing the model capacity.", "paperhash": "kim|exploiting_model_capacity_by_constraining_withinbatch_features_to_be_orthogonal", "keywords": ["regularization"], "_bibtex": "@misc{\n  kim2018exploiting,\n  title={Exploiting model capacity by constraining within-batch features to be orthogonal},\n  author={Hyo-Eun Kim},\n  year={2018},\n  url={https://openreview.net/forum?id=B1LSjGnUf}\n}", "authorids": ["hekim@lunit.io"], "authors": ["Hyo-Eun Kim"], "TL;DR": "Reduce redundancy between features by controlling correlation of filters and their features concurrently", "pdf": "/pdf/058f277baab3b796acff1ebee79a095129988e54.pdf"}, "nonreaders": [], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "tmdate": 1522066354893, "tcdate": 1522066354893, "number": 3, "cdate": 1522066354893, "id": "SksnmPIcM", "invitation": "ICLR.cc/2018/Workshop/-/Paper54/Public_Comment", "forum": "B1LSjGnUf", "replyto": "ryxguIUqG", "signatures": ["~Hyo-Eun_Kim1"], "readers": ["everyone"], "writers": ["~Hyo-Eun_Kim1"], "content": {"title": "Answer to your comment", "comment": "Sorry for my mistake. It is possible. The purpose of the proposed method is to reduce the correlation between n_out number of n_in dimensional feature vectors. Each n_in dimensional feature vector is representative of each filter. \n\nW W^T \\in R^{n_out, n_out} regularizes the filter-representative features to have minimum correlation each other, while reducing correlation between different feature extraction paths. E.g., for the case of z = f3(f2(f1(x))), where f1,2,3 are the 1st, 2nd, 3rd layers, x is an input example, and z is the output feature maps, z_i is the i^th feature map which is representative of the i^th filter of f3. By minimizing the correlation between z_i and z_j (except for the i=j), we can reduce global correlation between different feature extraction paths (from x to f3). \n\nI think regularizing W^T W is not appropriate for this purpose, because W^T W \\in R^{n_in, n_in} will reduce the correlation between n_out dimensional feature vectors."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploiting model capacity by constraining within-batch features to be orthogonal", "abstract": "Deep networks have been shown to greatly benefit from large model capacity when trained using various recent deep learning techniques. But at the same time, features in such large capacity networks have a potential to be redundant. In this work, we propose a new regularization method to exploit the given network capacity effectively. By minimizing the redundancy among in-layer filters and the correlation between in-batch features at the same time, we are able to achieve better performance with the same network architecture. Experiments with CIFAR-10/100 show that simultaneously constraining both the in-layer filters to be orthonormal and the in-batch features to be orthogonal is beneficial in efficiently utilizing the model capacity.", "paperhash": "kim|exploiting_model_capacity_by_constraining_withinbatch_features_to_be_orthogonal", "keywords": ["regularization"], "_bibtex": "@misc{\n  kim2018exploiting,\n  title={Exploiting model capacity by constraining within-batch features to be orthogonal},\n  author={Hyo-Eun Kim},\n  year={2018},\n  url={https://openreview.net/forum?id=B1LSjGnUf}\n}", "authorids": ["hekim@lunit.io"], "authors": ["Hyo-Eun Kim"], "TL;DR": "Reduce redundancy between features by controlling correlation of filters and their features concurrently", "pdf": "/pdf/058f277baab3b796acff1ebee79a095129988e54.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518712627380, "id": "ICLR.cc/2018/Workshop/-/Paper54/Public_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper54/Reviewers"], "reply": {"replyto": null, "forum": "B1LSjGnUf", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1518712627380}}}, {"tddate": null, "ddate": null, "tmdate": 1522063335693, "tcdate": 1522063335693, "number": 2, "cdate": 1522063335693, "id": "ryxguIUqG", "invitation": "ICLR.cc/2018/Workshop/-/Paper54/Official_Comment", "forum": "B1LSjGnUf", "replyto": "SJNGty85f", "signatures": ["ICLR.cc/2018/Workshop/Paper54/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper54/AnonReviewer1"], "content": {"title": "Reply to Your Answer ", "comment": "Why is W^T W not possible? It will be of shape n_in by n_in, right? It also captures the orthogonality constraint but in your input dimension. I am curious how it differs to your regularization."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploiting model capacity by constraining within-batch features to be orthogonal", "abstract": "Deep networks have been shown to greatly benefit from large model capacity when trained using various recent deep learning techniques. But at the same time, features in such large capacity networks have a potential to be redundant. In this work, we propose a new regularization method to exploit the given network capacity effectively. By minimizing the redundancy among in-layer filters and the correlation between in-batch features at the same time, we are able to achieve better performance with the same network architecture. Experiments with CIFAR-10/100 show that simultaneously constraining both the in-layer filters to be orthonormal and the in-batch features to be orthogonal is beneficial in efficiently utilizing the model capacity.", "paperhash": "kim|exploiting_model_capacity_by_constraining_withinbatch_features_to_be_orthogonal", "keywords": ["regularization"], "_bibtex": "@misc{\n  kim2018exploiting,\n  title={Exploiting model capacity by constraining within-batch features to be orthogonal},\n  author={Hyo-Eun Kim},\n  year={2018},\n  url={https://openreview.net/forum?id=B1LSjGnUf}\n}", "authorids": ["hekim@lunit.io"], "authors": ["Hyo-Eun Kim"], "TL;DR": "Reduce redundancy between features by controlling correlation of filters and their features concurrently", "pdf": "/pdf/058f277baab3b796acff1ebee79a095129988e54.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1519222450000, "id": "ICLR.cc/2018/Workshop/-/Paper54/Official_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "B1LSjGnUf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper54/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper54/Authors|ICLR.cc/2018/Workshop/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper54/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper54/Authors|ICLR.cc/2018/Workshop/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Workshop/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Workshop/Paper54/Reviewers", "ICLR.cc/2018/Workshop/Paper54/Authors", "ICLR.cc/2018/Workshop/Program_Chairs"], "cdate": 1519222450000}}}, {"tddate": null, "ddate": null, "tmdate": 1522034956421, "tcdate": 1522034956421, "number": 2, "cdate": 1522034956421, "id": "SJNGty85f", "invitation": "ICLR.cc/2018/Workshop/-/Paper54/Public_Comment", "forum": "B1LSjGnUf", "replyto": "BkoMA0rcz", "signatures": ["~Hyo-Eun_Kim1"], "readers": ["everyone"], "writers": ["~Hyo-Eun_Kim1"], "content": {"title": "Answer to the second question", "comment": "W \\in R^{n_out, n_in}, where n_out and n_in are the number of filters (or features) and the dimension of each filter (or feature). So, WW^T \\in R^{n_out, n_out} consists of possible correlations between any two filters (or features) among all the n_out number of filters (or features). As you mentioned that W is typically not square, W^T W is not possible in general.\n\nNow, I'm doing experiments on the imagenet dataset, and the proposed method shows similar (or even better) results compared to the experiments with CIFAR datasets (in the original manuscript). I'll share the results in the near future via arXiv. Thanks!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploiting model capacity by constraining within-batch features to be orthogonal", "abstract": "Deep networks have been shown to greatly benefit from large model capacity when trained using various recent deep learning techniques. But at the same time, features in such large capacity networks have a potential to be redundant. In this work, we propose a new regularization method to exploit the given network capacity effectively. By minimizing the redundancy among in-layer filters and the correlation between in-batch features at the same time, we are able to achieve better performance with the same network architecture. Experiments with CIFAR-10/100 show that simultaneously constraining both the in-layer filters to be orthonormal and the in-batch features to be orthogonal is beneficial in efficiently utilizing the model capacity.", "paperhash": "kim|exploiting_model_capacity_by_constraining_withinbatch_features_to_be_orthogonal", "keywords": ["regularization"], "_bibtex": "@misc{\n  kim2018exploiting,\n  title={Exploiting model capacity by constraining within-batch features to be orthogonal},\n  author={Hyo-Eun Kim},\n  year={2018},\n  url={https://openreview.net/forum?id=B1LSjGnUf}\n}", "authorids": ["hekim@lunit.io"], "authors": ["Hyo-Eun Kim"], "TL;DR": "Reduce redundancy between features by controlling correlation of filters and their features concurrently", "pdf": "/pdf/058f277baab3b796acff1ebee79a095129988e54.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518712627380, "id": "ICLR.cc/2018/Workshop/-/Paper54/Public_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper54/Reviewers"], "reply": {"replyto": null, "forum": "B1LSjGnUf", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1518712627380}}}, {"tddate": null, "ddate": null, "tmdate": 1522032146854, "tcdate": 1522032146854, "number": 1, "cdate": 1522032146854, "id": "BkoMA0rcz", "invitation": "ICLR.cc/2018/Workshop/-/Paper54/Official_Comment", "forum": "B1LSjGnUf", "replyto": "ry2uQMy9M", "signatures": ["ICLR.cc/2018/Workshop/Paper54/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper54/AnonReviewer1"], "content": {"title": "Reply to Your Answer", "comment": "Sorry for my mistake on the unit norm. I am actually about to ask the following question. Since the filter W is typically not square, what is the difference between regularize | WW^T - I |^2 and | W^TW - I |^2?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploiting model capacity by constraining within-batch features to be orthogonal", "abstract": "Deep networks have been shown to greatly benefit from large model capacity when trained using various recent deep learning techniques. But at the same time, features in such large capacity networks have a potential to be redundant. In this work, we propose a new regularization method to exploit the given network capacity effectively. By minimizing the redundancy among in-layer filters and the correlation between in-batch features at the same time, we are able to achieve better performance with the same network architecture. Experiments with CIFAR-10/100 show that simultaneously constraining both the in-layer filters to be orthonormal and the in-batch features to be orthogonal is beneficial in efficiently utilizing the model capacity.", "paperhash": "kim|exploiting_model_capacity_by_constraining_withinbatch_features_to_be_orthogonal", "keywords": ["regularization"], "_bibtex": "@misc{\n  kim2018exploiting,\n  title={Exploiting model capacity by constraining within-batch features to be orthogonal},\n  author={Hyo-Eun Kim},\n  year={2018},\n  url={https://openreview.net/forum?id=B1LSjGnUf}\n}", "authorids": ["hekim@lunit.io"], "authors": ["Hyo-Eun Kim"], "TL;DR": "Reduce redundancy between features by controlling correlation of filters and their features concurrently", "pdf": "/pdf/058f277baab3b796acff1ebee79a095129988e54.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1519222450000, "id": "ICLR.cc/2018/Workshop/-/Paper54/Official_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "B1LSjGnUf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper54/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper54/Authors|ICLR.cc/2018/Workshop/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper54/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper54/Authors|ICLR.cc/2018/Workshop/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Workshop/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Workshop/Paper54/Reviewers", "ICLR.cc/2018/Workshop/Paper54/Authors", "ICLR.cc/2018/Workshop/Program_Chairs"], "cdate": 1519222450000}}}, {"tddate": null, "ddate": null, "tmdate": 1521587060225, "tcdate": 1521587060225, "number": 1, "cdate": 1521587060225, "id": "ry2uQMy9M", "invitation": "ICLR.cc/2018/Workshop/-/Paper54/Public_Comment", "forum": "B1LSjGnUf", "replyto": "SkA1SK6uf", "signatures": ["~Hyo-Eun_Kim1"], "readers": ["everyone"], "writers": ["~Hyo-Eun_Kim1"], "content": {"title": "Answer to the Reviewer1", "comment": "Eq.(1): W W^T = I ensures that the original filters in W to be \"orthonormal\" (not orthogonal). It is clear.\n\n\\hat(W) \\hat(W)^T = I (your recommendation; \\hat(W) = unit-vectorized W) ensures that the original filters in W to be \"orthogonal\", because it indirectly regularizes the original filter vectors by regularizing the unit-vectors of W to be orthonormal.\n\nThanks for your comment."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploiting model capacity by constraining within-batch features to be orthogonal", "abstract": "Deep networks have been shown to greatly benefit from large model capacity when trained using various recent deep learning techniques. But at the same time, features in such large capacity networks have a potential to be redundant. In this work, we propose a new regularization method to exploit the given network capacity effectively. By minimizing the redundancy among in-layer filters and the correlation between in-batch features at the same time, we are able to achieve better performance with the same network architecture. Experiments with CIFAR-10/100 show that simultaneously constraining both the in-layer filters to be orthonormal and the in-batch features to be orthogonal is beneficial in efficiently utilizing the model capacity.", "paperhash": "kim|exploiting_model_capacity_by_constraining_withinbatch_features_to_be_orthogonal", "keywords": ["regularization"], "_bibtex": "@misc{\n  kim2018exploiting,\n  title={Exploiting model capacity by constraining within-batch features to be orthogonal},\n  author={Hyo-Eun Kim},\n  year={2018},\n  url={https://openreview.net/forum?id=B1LSjGnUf}\n}", "authorids": ["hekim@lunit.io"], "authors": ["Hyo-Eun Kim"], "TL;DR": "Reduce redundancy between features by controlling correlation of filters and their features concurrently", "pdf": "/pdf/058f277baab3b796acff1ebee79a095129988e54.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518712627380, "id": "ICLR.cc/2018/Workshop/-/Paper54/Public_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper54/Reviewers"], "reply": {"replyto": null, "forum": "B1LSjGnUf", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1518712627380}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582912413, "tcdate": 1520436453764, "number": 1, "cdate": 1520436453764, "id": "SkA1SK6uf", "invitation": "ICLR.cc/2018/Workshop/-/Paper54/Official_Review", "forum": "B1LSjGnUf", "replyto": "B1LSjGnUf", "signatures": ["ICLR.cc/2018/Workshop/Paper54/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper54/AnonReviewer1"], "content": {"title": "Very Incremental", "rating": "5: Marginally below acceptance threshold", "review": "Authors in this paper builds on top of the previous orthogonal filter regularization and add a similar regularization on the feature maps of the CNN. Experiments on CIFAR10 and CIFAR100 show that the proposed technique can slightly improve the generalization. \n\nI have a few concerns:\n(1) Overall, the contribution of this paper is very incremental and experimental results are not significant. Given that the technical contribution is weak, I recommend you add more experiments to make the paper more convincing, e.g., adding comparison on ImageNet.\n(2) From your regularization in Eq. (1), it only ensures the filter to be orthogonal rather than orthonormal, i.e., filters are not unit vectors. The naming is problematic.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploiting model capacity by constraining within-batch features to be orthogonal", "abstract": "Deep networks have been shown to greatly benefit from large model capacity when trained using various recent deep learning techniques. But at the same time, features in such large capacity networks have a potential to be redundant. In this work, we propose a new regularization method to exploit the given network capacity effectively. By minimizing the redundancy among in-layer filters and the correlation between in-batch features at the same time, we are able to achieve better performance with the same network architecture. Experiments with CIFAR-10/100 show that simultaneously constraining both the in-layer filters to be orthonormal and the in-batch features to be orthogonal is beneficial in efficiently utilizing the model capacity.", "paperhash": "kim|exploiting_model_capacity_by_constraining_withinbatch_features_to_be_orthogonal", "keywords": ["regularization"], "_bibtex": "@misc{\n  kim2018exploiting,\n  title={Exploiting model capacity by constraining within-batch features to be orthogonal},\n  author={Hyo-Eun Kim},\n  year={2018},\n  url={https://openreview.net/forum?id=B1LSjGnUf}\n}", "authorids": ["hekim@lunit.io"], "authors": ["Hyo-Eun Kim"], "TL;DR": "Reduce redundancy between features by controlling correlation of filters and their features concurrently", "pdf": "/pdf/058f277baab3b796acff1ebee79a095129988e54.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582912224, "id": "ICLR.cc/2018/Workshop/-/Paper54/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper54/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper54/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper54/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper54/AnonReviewer2"], "reply": {"forum": "B1LSjGnUf", "replyto": "B1LSjGnUf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper54/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper54/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582912224}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582750673, "tcdate": 1520654448222, "number": 2, "cdate": 1520654448222, "id": "HkdOuAeKf", "invitation": "ICLR.cc/2018/Workshop/-/Paper54/Official_Review", "forum": "B1LSjGnUf", "replyto": "B1LSjGnUf", "signatures": ["ICLR.cc/2018/Workshop/Paper54/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper54/AnonReviewer3"], "content": {"title": "This paper is easy to follow, but the experiments lack some important things. ", "rating": "6: Marginally above acceptance threshold", "review": "This paper seeks to enforce orthonormality over model parameters of deep neural networks.  This is done by introducing two loss terms that reflecting orthonormality over parameters and features, respectively. This paper is easy to follow, but the experiments lack some important things. \n\nThe experiments lack results on large data set. \n\nHow to set lambda_1 and lambda_2?\n\nI think both lambda_1 and lambda_2 should be in [0,1]", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploiting model capacity by constraining within-batch features to be orthogonal", "abstract": "Deep networks have been shown to greatly benefit from large model capacity when trained using various recent deep learning techniques. But at the same time, features in such large capacity networks have a potential to be redundant. In this work, we propose a new regularization method to exploit the given network capacity effectively. By minimizing the redundancy among in-layer filters and the correlation between in-batch features at the same time, we are able to achieve better performance with the same network architecture. Experiments with CIFAR-10/100 show that simultaneously constraining both the in-layer filters to be orthonormal and the in-batch features to be orthogonal is beneficial in efficiently utilizing the model capacity.", "paperhash": "kim|exploiting_model_capacity_by_constraining_withinbatch_features_to_be_orthogonal", "keywords": ["regularization"], "_bibtex": "@misc{\n  kim2018exploiting,\n  title={Exploiting model capacity by constraining within-batch features to be orthogonal},\n  author={Hyo-Eun Kim},\n  year={2018},\n  url={https://openreview.net/forum?id=B1LSjGnUf}\n}", "authorids": ["hekim@lunit.io"], "authors": ["Hyo-Eun Kim"], "TL;DR": "Reduce redundancy between features by controlling correlation of filters and their features concurrently", "pdf": "/pdf/058f277baab3b796acff1ebee79a095129988e54.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582912224, "id": "ICLR.cc/2018/Workshop/-/Paper54/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper54/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper54/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper54/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper54/AnonReviewer2"], "reply": {"forum": "B1LSjGnUf", "replyto": "B1LSjGnUf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper54/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper54/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582912224}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582615335, "tcdate": 1520876238231, "number": 3, "cdate": 1520876238231, "id": "Bk8AqNEKz", "invitation": "ICLR.cc/2018/Workshop/-/Paper54/Official_Review", "forum": "B1LSjGnUf", "replyto": "B1LSjGnUf", "signatures": ["ICLR.cc/2018/Workshop/Paper54/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper54/AnonReviewer2"], "content": {"title": "Interesting but very limited work", "rating": "5: Marginally below acceptance threshold", "review": "The authors propose  (approximate) orthonormalization of the in-layer features combined with (approximate) orthogonalization of the in-batch features.\n\nThey claim (based on evidence on CIFAR-10 and CIFAR-100) that this combination is an effective regularizer.\n\nThere is a significant literature of constraining weight matrices to be orthonormal (as it is known to be a good pre-conditioner), but people mostly try hard constraints instead of soft constraints.\n\nThere are two drawbacks of the proposed method:\n- It is relatively costly to batch-orthogonalize the activations.\n- Current best results are achieved with wider architectures (Wide Residual Networks by Zagoruyko and Komodakis) and there is current theoretical evidence that over-parametrization helps optimization.  (On the Optimization of Deep Networks:\nImplicit Acceleration by Overparameterization by Arora et al.)\n\nGiven the fact that the batch-orthogonalization is quite costly, questionably efficient on wide networks, the baselines on cifar-10 and 100 are not up-to-date (Wide Residual Networks came out in 2016 and reach 3.8% error on cifar-10 and about 20% error on cifar-100), I wonder why this work was not based on that work. For this reason, I am not convinced about the scope of this work.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploiting model capacity by constraining within-batch features to be orthogonal", "abstract": "Deep networks have been shown to greatly benefit from large model capacity when trained using various recent deep learning techniques. But at the same time, features in such large capacity networks have a potential to be redundant. In this work, we propose a new regularization method to exploit the given network capacity effectively. By minimizing the redundancy among in-layer filters and the correlation between in-batch features at the same time, we are able to achieve better performance with the same network architecture. Experiments with CIFAR-10/100 show that simultaneously constraining both the in-layer filters to be orthonormal and the in-batch features to be orthogonal is beneficial in efficiently utilizing the model capacity.", "paperhash": "kim|exploiting_model_capacity_by_constraining_withinbatch_features_to_be_orthogonal", "keywords": ["regularization"], "_bibtex": "@misc{\n  kim2018exploiting,\n  title={Exploiting model capacity by constraining within-batch features to be orthogonal},\n  author={Hyo-Eun Kim},\n  year={2018},\n  url={https://openreview.net/forum?id=B1LSjGnUf}\n}", "authorids": ["hekim@lunit.io"], "authors": ["Hyo-Eun Kim"], "TL;DR": "Reduce redundancy between features by controlling correlation of filters and their features concurrently", "pdf": "/pdf/058f277baab3b796acff1ebee79a095129988e54.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582912224, "id": "ICLR.cc/2018/Workshop/-/Paper54/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper54/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper54/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper54/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper54/AnonReviewer2"], "reply": {"forum": "B1LSjGnUf", "replyto": "B1LSjGnUf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper54/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper54/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582912224}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573582210, "tcdate": 1521573582210, "number": 167, "cdate": 1521573581869, "id": "rkUACCRtf", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "B1LSjGnUf", "replyto": "B1LSjGnUf", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploiting model capacity by constraining within-batch features to be orthogonal", "abstract": "Deep networks have been shown to greatly benefit from large model capacity when trained using various recent deep learning techniques. But at the same time, features in such large capacity networks have a potential to be redundant. In this work, we propose a new regularization method to exploit the given network capacity effectively. By minimizing the redundancy among in-layer filters and the correlation between in-batch features at the same time, we are able to achieve better performance with the same network architecture. Experiments with CIFAR-10/100 show that simultaneously constraining both the in-layer filters to be orthonormal and the in-batch features to be orthogonal is beneficial in efficiently utilizing the model capacity.", "paperhash": "kim|exploiting_model_capacity_by_constraining_withinbatch_features_to_be_orthogonal", "keywords": ["regularization"], "_bibtex": "@misc{\n  kim2018exploiting,\n  title={Exploiting model capacity by constraining within-batch features to be orthogonal},\n  author={Hyo-Eun Kim},\n  year={2018},\n  url={https://openreview.net/forum?id=B1LSjGnUf}\n}", "authorids": ["hekim@lunit.io"], "authors": ["Hyo-Eun Kim"], "TL;DR": "Reduce redundancy between features by controlling correlation of filters and their features concurrently", "pdf": "/pdf/058f277baab3b796acff1ebee79a095129988e54.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 10}