{"notes": [{"id": "Byx55pVKDB", "original": "HJlYfZ0PvB", "number": 716, "cdate": 1569439121584, "ddate": null, "tcdate": 1569439121584, "tmdate": 1577168247574, "tddate": null, "forum": "Byx55pVKDB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "How the Softmax Activation Hinders the Detection of Adversarial and Out-of-Distribution Examples in Neural Networks", "authors": ["Jonathan Aigrain", "Marcin Detyniecki"], "authorids": ["jonathan.aigrain@axa.com", "marcin.detyniecki@axa.com"], "keywords": ["Adversarial examples", "out-of-distribution", "detection", "softmax", "logits"], "TL;DR": " The softmax activation hinders the detection of adversarial and out-of-distribution examples, as it masks a significant part of the relevant information present in the logits.", "abstract": "Despite having excellent performances for a wide variety of tasks, modern neural networks are unable to provide a prediction with a reliable confidence estimate which would allow to detect misclassifications. This limitation is at the heart of what is known as an adversarial example, where the network provides a wrong prediction associated with a strong confidence to a slightly modified image. Moreover, this overconfidence issue has also been observed for out-of-distribution data. We show through several experiments that the softmax activation, usually placed as the last layer of modern neural networks, is partly responsible for this behaviour. We give qualitative insights about its impact on the MNIST dataset, showing that relevant information present in the logits is lost once the softmax function is applied. The same observation is made through quantitative analysis, as we show that two out-of-distribution and adversarial example detectors obtain competitive results when using logit values as inputs, but provide considerably lower performances if they use softmax probabilities instead: from 98.0% average AUROC to 56.8% in some settings. These results provide evidence that the softmax activation hinders the detection of adversarial and out-of-distribution examples, as it masks a significant part of the relevant information present in the logits.", "pdf": "/pdf/78804458f698f966691634d4c3c3662ae27938d7.pdf", "paperhash": "aigrain|how_the_softmax_activation_hinders_the_detection_of_adversarial_and_outofdistribution_examples_in_neural_networks", "original_pdf": "/attachment/78804458f698f966691634d4c3c3662ae27938d7.pdf", "_bibtex": "@misc{\naigrain2020how,\ntitle={How the Softmax Activation Hinders the Detection of Adversarial and Out-of-Distribution Examples in Neural Networks},\nauthor={Jonathan Aigrain and Marcin Detyniecki},\nyear={2020},\nurl={https://openreview.net/forum?id=Byx55pVKDB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "Lwturqal5", "original": null, "number": 1, "cdate": 1576798704084, "ddate": null, "tcdate": 1576798704084, "tmdate": 1576800931978, "tddate": null, "forum": "Byx55pVKDB", "replyto": "Byx55pVKDB", "invitation": "ICLR.cc/2020/Conference/Paper716/-/Decision", "content": {"decision": "Reject", "comment": "\nThe paper investigates how the softmax activation hinders the detection of out-of-distribution examples.\n\nAll the reviewers felt that the paper requires more work before it can be accepted. In particular, the reviewers raised several concerns about theoretical justification, comparison to other existing methods, discussion of connection to existing methods and scalability to larger number of classes.\n\nI encourage the authors to revise the draft based on the reviewers\u2019 feedback and resubmit to a different venue.\n\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How the Softmax Activation Hinders the Detection of Adversarial and Out-of-Distribution Examples in Neural Networks", "authors": ["Jonathan Aigrain", "Marcin Detyniecki"], "authorids": ["jonathan.aigrain@axa.com", "marcin.detyniecki@axa.com"], "keywords": ["Adversarial examples", "out-of-distribution", "detection", "softmax", "logits"], "TL;DR": " The softmax activation hinders the detection of adversarial and out-of-distribution examples, as it masks a significant part of the relevant information present in the logits.", "abstract": "Despite having excellent performances for a wide variety of tasks, modern neural networks are unable to provide a prediction with a reliable confidence estimate which would allow to detect misclassifications. This limitation is at the heart of what is known as an adversarial example, where the network provides a wrong prediction associated with a strong confidence to a slightly modified image. Moreover, this overconfidence issue has also been observed for out-of-distribution data. We show through several experiments that the softmax activation, usually placed as the last layer of modern neural networks, is partly responsible for this behaviour. We give qualitative insights about its impact on the MNIST dataset, showing that relevant information present in the logits is lost once the softmax function is applied. The same observation is made through quantitative analysis, as we show that two out-of-distribution and adversarial example detectors obtain competitive results when using logit values as inputs, but provide considerably lower performances if they use softmax probabilities instead: from 98.0% average AUROC to 56.8% in some settings. These results provide evidence that the softmax activation hinders the detection of adversarial and out-of-distribution examples, as it masks a significant part of the relevant information present in the logits.", "pdf": "/pdf/78804458f698f966691634d4c3c3662ae27938d7.pdf", "paperhash": "aigrain|how_the_softmax_activation_hinders_the_detection_of_adversarial_and_outofdistribution_examples_in_neural_networks", "original_pdf": "/attachment/78804458f698f966691634d4c3c3662ae27938d7.pdf", "_bibtex": "@misc{\naigrain2020how,\ntitle={How the Softmax Activation Hinders the Detection of Adversarial and Out-of-Distribution Examples in Neural Networks},\nauthor={Jonathan Aigrain and Marcin Detyniecki},\nyear={2020},\nurl={https://openreview.net/forum?id=Byx55pVKDB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Byx55pVKDB", "replyto": "Byx55pVKDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795728740, "tmdate": 1576800281205, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper716/-/Decision"}}}, {"id": "ByegKbDOsS", "original": null, "number": 3, "cdate": 1573577079775, "ddate": null, "tcdate": 1573577079775, "tmdate": 1573577079775, "tddate": null, "forum": "Byx55pVKDB", "replyto": "Syx2J82otS", "invitation": "ICLR.cc/2020/Conference/Paper716/-/Official_Comment", "content": {"title": "Answer to Reviewer 3", "comment": "Thank you for your comment and feedback. We respectfully disagree that the experiments with the NN-based detector should be ignored in your evaluation. As the goal of this paper is not to compare differences in performances between methods, but to compare differences in performances when using logit vs softmax values, we believe that the NN-based detector represents a best-case scenario where relevant features can be computed by the NN. We consider that it is interesting to observe that, even with an advantage as unfair as being able to peak at the test distribution, the softmax NN detector obtain overall worse results than the logits KDE detector on adversarial examples. Thus we argue that these experiments should be taken into account.\n\nWe agree that it would be interesting to evaluate the KDE on datasets with more classes such as CIFAR-100. We will include this experiment in the next version of the paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper716/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper716/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How the Softmax Activation Hinders the Detection of Adversarial and Out-of-Distribution Examples in Neural Networks", "authors": ["Jonathan Aigrain", "Marcin Detyniecki"], "authorids": ["jonathan.aigrain@axa.com", "marcin.detyniecki@axa.com"], "keywords": ["Adversarial examples", "out-of-distribution", "detection", "softmax", "logits"], "TL;DR": " The softmax activation hinders the detection of adversarial and out-of-distribution examples, as it masks a significant part of the relevant information present in the logits.", "abstract": "Despite having excellent performances for a wide variety of tasks, modern neural networks are unable to provide a prediction with a reliable confidence estimate which would allow to detect misclassifications. This limitation is at the heart of what is known as an adversarial example, where the network provides a wrong prediction associated with a strong confidence to a slightly modified image. Moreover, this overconfidence issue has also been observed for out-of-distribution data. We show through several experiments that the softmax activation, usually placed as the last layer of modern neural networks, is partly responsible for this behaviour. We give qualitative insights about its impact on the MNIST dataset, showing that relevant information present in the logits is lost once the softmax function is applied. The same observation is made through quantitative analysis, as we show that two out-of-distribution and adversarial example detectors obtain competitive results when using logit values as inputs, but provide considerably lower performances if they use softmax probabilities instead: from 98.0% average AUROC to 56.8% in some settings. These results provide evidence that the softmax activation hinders the detection of adversarial and out-of-distribution examples, as it masks a significant part of the relevant information present in the logits.", "pdf": "/pdf/78804458f698f966691634d4c3c3662ae27938d7.pdf", "paperhash": "aigrain|how_the_softmax_activation_hinders_the_detection_of_adversarial_and_outofdistribution_examples_in_neural_networks", "original_pdf": "/attachment/78804458f698f966691634d4c3c3662ae27938d7.pdf", "_bibtex": "@misc{\naigrain2020how,\ntitle={How the Softmax Activation Hinders the Detection of Adversarial and Out-of-Distribution Examples in Neural Networks},\nauthor={Jonathan Aigrain and Marcin Detyniecki},\nyear={2020},\nurl={https://openreview.net/forum?id=Byx55pVKDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Byx55pVKDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper716/Authors", "ICLR.cc/2020/Conference/Paper716/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper716/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper716/Reviewers", "ICLR.cc/2020/Conference/Paper716/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper716/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper716/Authors|ICLR.cc/2020/Conference/Paper716/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167306, "tmdate": 1576860548090, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper716/Authors", "ICLR.cc/2020/Conference/Paper716/Reviewers", "ICLR.cc/2020/Conference/Paper716/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper716/-/Official_Comment"}}}, {"id": "ByelL-vOiH", "original": null, "number": 2, "cdate": 1573577031692, "ddate": null, "tcdate": 1573577031692, "tmdate": 1573577031692, "tddate": null, "forum": "Byx55pVKDB", "replyto": "HyemRBD8cB", "invitation": "ICLR.cc/2020/Conference/Paper716/-/Official_Comment", "content": {"title": "Answer to Reviewer 5", "comment": "Thank you for you comments and feedback. We agree that our work would benefit from using more datasets for evaluation and comparing with more methods. We will include these improvements in the next version of the paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper716/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper716/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How the Softmax Activation Hinders the Detection of Adversarial and Out-of-Distribution Examples in Neural Networks", "authors": ["Jonathan Aigrain", "Marcin Detyniecki"], "authorids": ["jonathan.aigrain@axa.com", "marcin.detyniecki@axa.com"], "keywords": ["Adversarial examples", "out-of-distribution", "detection", "softmax", "logits"], "TL;DR": " The softmax activation hinders the detection of adversarial and out-of-distribution examples, as it masks a significant part of the relevant information present in the logits.", "abstract": "Despite having excellent performances for a wide variety of tasks, modern neural networks are unable to provide a prediction with a reliable confidence estimate which would allow to detect misclassifications. This limitation is at the heart of what is known as an adversarial example, where the network provides a wrong prediction associated with a strong confidence to a slightly modified image. Moreover, this overconfidence issue has also been observed for out-of-distribution data. We show through several experiments that the softmax activation, usually placed as the last layer of modern neural networks, is partly responsible for this behaviour. We give qualitative insights about its impact on the MNIST dataset, showing that relevant information present in the logits is lost once the softmax function is applied. The same observation is made through quantitative analysis, as we show that two out-of-distribution and adversarial example detectors obtain competitive results when using logit values as inputs, but provide considerably lower performances if they use softmax probabilities instead: from 98.0% average AUROC to 56.8% in some settings. These results provide evidence that the softmax activation hinders the detection of adversarial and out-of-distribution examples, as it masks a significant part of the relevant information present in the logits.", "pdf": "/pdf/78804458f698f966691634d4c3c3662ae27938d7.pdf", "paperhash": "aigrain|how_the_softmax_activation_hinders_the_detection_of_adversarial_and_outofdistribution_examples_in_neural_networks", "original_pdf": "/attachment/78804458f698f966691634d4c3c3662ae27938d7.pdf", "_bibtex": "@misc{\naigrain2020how,\ntitle={How the Softmax Activation Hinders the Detection of Adversarial and Out-of-Distribution Examples in Neural Networks},\nauthor={Jonathan Aigrain and Marcin Detyniecki},\nyear={2020},\nurl={https://openreview.net/forum?id=Byx55pVKDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Byx55pVKDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper716/Authors", "ICLR.cc/2020/Conference/Paper716/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper716/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper716/Reviewers", "ICLR.cc/2020/Conference/Paper716/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper716/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper716/Authors|ICLR.cc/2020/Conference/Paper716/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167306, "tmdate": 1576860548090, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper716/Authors", "ICLR.cc/2020/Conference/Paper716/Reviewers", "ICLR.cc/2020/Conference/Paper716/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper716/-/Official_Comment"}}}, {"id": "Hyx9mbDdiB", "original": null, "number": 1, "cdate": 1573576993809, "ddate": null, "tcdate": 1573576993809, "tmdate": 1573576993809, "tddate": null, "forum": "Byx55pVKDB", "replyto": "Bylx9EfY5B", "invitation": "ICLR.cc/2020/Conference/Paper716/-/Official_Comment", "content": {"title": "Answer to Reviewer 4", "comment": "Thank you for your comments and feedback. Indeed, temperature scaling does limit the effect of using the exponential function to amplify differences in logits. However, it does not change the impact of the normalization, as the information about the logit absolute values is still lost after temperature. But we agree that this method and ODIN are interesting comparison baselines and we will include them in the next version of the paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper716/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper716/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How the Softmax Activation Hinders the Detection of Adversarial and Out-of-Distribution Examples in Neural Networks", "authors": ["Jonathan Aigrain", "Marcin Detyniecki"], "authorids": ["jonathan.aigrain@axa.com", "marcin.detyniecki@axa.com"], "keywords": ["Adversarial examples", "out-of-distribution", "detection", "softmax", "logits"], "TL;DR": " The softmax activation hinders the detection of adversarial and out-of-distribution examples, as it masks a significant part of the relevant information present in the logits.", "abstract": "Despite having excellent performances for a wide variety of tasks, modern neural networks are unable to provide a prediction with a reliable confidence estimate which would allow to detect misclassifications. This limitation is at the heart of what is known as an adversarial example, where the network provides a wrong prediction associated with a strong confidence to a slightly modified image. Moreover, this overconfidence issue has also been observed for out-of-distribution data. We show through several experiments that the softmax activation, usually placed as the last layer of modern neural networks, is partly responsible for this behaviour. We give qualitative insights about its impact on the MNIST dataset, showing that relevant information present in the logits is lost once the softmax function is applied. The same observation is made through quantitative analysis, as we show that two out-of-distribution and adversarial example detectors obtain competitive results when using logit values as inputs, but provide considerably lower performances if they use softmax probabilities instead: from 98.0% average AUROC to 56.8% in some settings. These results provide evidence that the softmax activation hinders the detection of adversarial and out-of-distribution examples, as it masks a significant part of the relevant information present in the logits.", "pdf": "/pdf/78804458f698f966691634d4c3c3662ae27938d7.pdf", "paperhash": "aigrain|how_the_softmax_activation_hinders_the_detection_of_adversarial_and_outofdistribution_examples_in_neural_networks", "original_pdf": "/attachment/78804458f698f966691634d4c3c3662ae27938d7.pdf", "_bibtex": "@misc{\naigrain2020how,\ntitle={How the Softmax Activation Hinders the Detection of Adversarial and Out-of-Distribution Examples in Neural Networks},\nauthor={Jonathan Aigrain and Marcin Detyniecki},\nyear={2020},\nurl={https://openreview.net/forum?id=Byx55pVKDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Byx55pVKDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper716/Authors", "ICLR.cc/2020/Conference/Paper716/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper716/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper716/Reviewers", "ICLR.cc/2020/Conference/Paper716/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper716/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper716/Authors|ICLR.cc/2020/Conference/Paper716/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167306, "tmdate": 1576860548090, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper716/Authors", "ICLR.cc/2020/Conference/Paper716/Reviewers", "ICLR.cc/2020/Conference/Paper716/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper716/-/Official_Comment"}}}, {"id": "Syx2J82otS", "original": null, "number": 1, "cdate": 1571698148254, "ddate": null, "tcdate": 1571698148254, "tmdate": 1572972561299, "tddate": null, "forum": "Byx55pVKDB", "replyto": "Byx55pVKDB", "invitation": "ICLR.cc/2020/Conference/Paper716/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper suggests that the logits cary more information than the maximum softmax probability for OOD detection. They suggest this with scatterplots and develop techniques to support this claim.\nThey use logits for as features for OOD detection with by using a kernel density estimator. They also use a NN trained with logits features, but this assumes we can peak at the test distribution, so I ignore this entirely in this evaluation.\nUnfortunately their KDE density estimator does not perform better than the maximum softmax probability for OOD detection on CIFAR-10 (74.3 vs 91.7).\nThey do not show results on CIFAR-100, but since the dimensionality of the logits would increase by an order of magnitude, one would expect kernel density estimation to perform much worse. The authors should include an evaluation on CIFAR-100 for completeness.\n\nSmall comments:\n\nTable 3 is a comment about featurization. Does this hold when taking the log of the softmax probabilities (not the same as logits)? If not, then this isn't much a count against the softmax per se. Even then, this is a comment on using softmax information for KDE, not using the maximum softmax probability itself for OOD detection.\n\nThe full results for Table 2 are needed. Perhaps place this in an appendix.\n\nThey repeat that the logits contain more information than the maximum softmax probability, but so does the raw input. The challenge is not introducing more noise/variance when introducing more information.\n\nI was confused at the experimental description. The information should be contained in one location. They train one of their CIFAR networks for 30 epochs, which isn't enough training time. Consequently, I suspect that those results are not worth drawing implications from since the accuracy is presumably low.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper716/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper716/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How the Softmax Activation Hinders the Detection of Adversarial and Out-of-Distribution Examples in Neural Networks", "authors": ["Jonathan Aigrain", "Marcin Detyniecki"], "authorids": ["jonathan.aigrain@axa.com", "marcin.detyniecki@axa.com"], "keywords": ["Adversarial examples", "out-of-distribution", "detection", "softmax", "logits"], "TL;DR": " The softmax activation hinders the detection of adversarial and out-of-distribution examples, as it masks a significant part of the relevant information present in the logits.", "abstract": "Despite having excellent performances for a wide variety of tasks, modern neural networks are unable to provide a prediction with a reliable confidence estimate which would allow to detect misclassifications. This limitation is at the heart of what is known as an adversarial example, where the network provides a wrong prediction associated with a strong confidence to a slightly modified image. Moreover, this overconfidence issue has also been observed for out-of-distribution data. We show through several experiments that the softmax activation, usually placed as the last layer of modern neural networks, is partly responsible for this behaviour. We give qualitative insights about its impact on the MNIST dataset, showing that relevant information present in the logits is lost once the softmax function is applied. The same observation is made through quantitative analysis, as we show that two out-of-distribution and adversarial example detectors obtain competitive results when using logit values as inputs, but provide considerably lower performances if they use softmax probabilities instead: from 98.0% average AUROC to 56.8% in some settings. These results provide evidence that the softmax activation hinders the detection of adversarial and out-of-distribution examples, as it masks a significant part of the relevant information present in the logits.", "pdf": "/pdf/78804458f698f966691634d4c3c3662ae27938d7.pdf", "paperhash": "aigrain|how_the_softmax_activation_hinders_the_detection_of_adversarial_and_outofdistribution_examples_in_neural_networks", "original_pdf": "/attachment/78804458f698f966691634d4c3c3662ae27938d7.pdf", "_bibtex": "@misc{\naigrain2020how,\ntitle={How the Softmax Activation Hinders the Detection of Adversarial and Out-of-Distribution Examples in Neural Networks},\nauthor={Jonathan Aigrain and Marcin Detyniecki},\nyear={2020},\nurl={https://openreview.net/forum?id=Byx55pVKDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Byx55pVKDB", "replyto": "Byx55pVKDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper716/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper716/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576036135205, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper716/Reviewers"], "noninvitees": [], "tcdate": 1570237748126, "tmdate": 1576036135223, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper716/-/Official_Review"}}}, {"id": "HyemRBD8cB", "original": null, "number": 2, "cdate": 1572398538954, "ddate": null, "tcdate": 1572398538954, "tmdate": 1572972561267, "tddate": null, "forum": "Byx55pVKDB", "replyto": "Byx55pVKDB", "invitation": "ICLR.cc/2020/Conference/Paper716/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #5", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary\n\nThis paper showed that out-of-distribution and adversarial samples can be detected effectively if we utilize logits (without softmax activations). Based on this observation, the authors proposed 2-logit based detectors and showed that they outperform the detectors utilizing softmax activations using MNIST and CIFAR-10 datasets.\n\nI\u2019d like to recommend \"reject\" due to the following\n\nThe main observation (removing softmax activation can be useful for detecting abnormal samples) is a bit interesting (but not surprising) but there is no theoretical analysis for this. It would be better if the authors can provide the reason why softmax activation hinders the novelty detection.\n\nThe logit-based detectors proposed in the paper are simple variants of existing methods. Because of that, it is hard to say that technical contributions are very significant.\n\nQuestions\n\nFor evaluation, could the authors compare the performance with feature-based methods like Mahalanobis [1] and LID [2]?\n\nI would be appreciated if the author can evaluate their hypothesis using various datasets like CIFAR-100, SVHN, and ImageNet.\n\n[1] Lee, K., Lee, K., Lee, H. and Shin, J., 2018. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. In Advances in Neural Information Processing Systems (pp. 7167-7177).\n\n[2] Ma, X., Li, B., Wang, Y., Erfani, S.M., Wijewickrema, S., Schoenebeck, G., Song, D., Houle, M.E. and Bailey, J., 2018. Characterizing adversarial subspaces using local intrinsic dimensionality. arXiv preprint arXiv:1801.02613."}, "signatures": ["ICLR.cc/2020/Conference/Paper716/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper716/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How the Softmax Activation Hinders the Detection of Adversarial and Out-of-Distribution Examples in Neural Networks", "authors": ["Jonathan Aigrain", "Marcin Detyniecki"], "authorids": ["jonathan.aigrain@axa.com", "marcin.detyniecki@axa.com"], "keywords": ["Adversarial examples", "out-of-distribution", "detection", "softmax", "logits"], "TL;DR": " The softmax activation hinders the detection of adversarial and out-of-distribution examples, as it masks a significant part of the relevant information present in the logits.", "abstract": "Despite having excellent performances for a wide variety of tasks, modern neural networks are unable to provide a prediction with a reliable confidence estimate which would allow to detect misclassifications. This limitation is at the heart of what is known as an adversarial example, where the network provides a wrong prediction associated with a strong confidence to a slightly modified image. Moreover, this overconfidence issue has also been observed for out-of-distribution data. We show through several experiments that the softmax activation, usually placed as the last layer of modern neural networks, is partly responsible for this behaviour. We give qualitative insights about its impact on the MNIST dataset, showing that relevant information present in the logits is lost once the softmax function is applied. The same observation is made through quantitative analysis, as we show that two out-of-distribution and adversarial example detectors obtain competitive results when using logit values as inputs, but provide considerably lower performances if they use softmax probabilities instead: from 98.0% average AUROC to 56.8% in some settings. These results provide evidence that the softmax activation hinders the detection of adversarial and out-of-distribution examples, as it masks a significant part of the relevant information present in the logits.", "pdf": "/pdf/78804458f698f966691634d4c3c3662ae27938d7.pdf", "paperhash": "aigrain|how_the_softmax_activation_hinders_the_detection_of_adversarial_and_outofdistribution_examples_in_neural_networks", "original_pdf": "/attachment/78804458f698f966691634d4c3c3662ae27938d7.pdf", "_bibtex": "@misc{\naigrain2020how,\ntitle={How the Softmax Activation Hinders the Detection of Adversarial and Out-of-Distribution Examples in Neural Networks},\nauthor={Jonathan Aigrain and Marcin Detyniecki},\nyear={2020},\nurl={https://openreview.net/forum?id=Byx55pVKDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Byx55pVKDB", "replyto": "Byx55pVKDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper716/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper716/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576036135205, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper716/Reviewers"], "noninvitees": [], "tcdate": 1570237748126, "tmdate": 1576036135223, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper716/-/Official_Review"}}}, {"id": "Bylx9EfY5B", "original": null, "number": 3, "cdate": 1572574344458, "ddate": null, "tcdate": 1572574344458, "tmdate": 1572972561224, "tddate": null, "forum": "Byx55pVKDB", "replyto": "Byx55pVKDB", "invitation": "ICLR.cc/2020/Conference/Paper716/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This simple paper shows that the normalization of softmax causes a loss of information compared to using the unnormalized logits when trying to do OOD and adversarial example detection.  The main reason for this is of course the normalization used by the softmax. The paper is mostly empirical following this specific observation, and uses a number of examples on MNIST and CIFAR to show the improvement in performance by using unnormalized logits instead of softmax.\n\nWhile interesting, it is to be noted that methods such as ODIN and temperature scaling specifically include a temperature to exactly overcome this same issue with softmax. The lack of comparison to such baselines makes this paper quite incomplete, especially as it is an empirical paper itself. "}, "signatures": ["ICLR.cc/2020/Conference/Paper716/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper716/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How the Softmax Activation Hinders the Detection of Adversarial and Out-of-Distribution Examples in Neural Networks", "authors": ["Jonathan Aigrain", "Marcin Detyniecki"], "authorids": ["jonathan.aigrain@axa.com", "marcin.detyniecki@axa.com"], "keywords": ["Adversarial examples", "out-of-distribution", "detection", "softmax", "logits"], "TL;DR": " The softmax activation hinders the detection of adversarial and out-of-distribution examples, as it masks a significant part of the relevant information present in the logits.", "abstract": "Despite having excellent performances for a wide variety of tasks, modern neural networks are unable to provide a prediction with a reliable confidence estimate which would allow to detect misclassifications. This limitation is at the heart of what is known as an adversarial example, where the network provides a wrong prediction associated with a strong confidence to a slightly modified image. Moreover, this overconfidence issue has also been observed for out-of-distribution data. We show through several experiments that the softmax activation, usually placed as the last layer of modern neural networks, is partly responsible for this behaviour. We give qualitative insights about its impact on the MNIST dataset, showing that relevant information present in the logits is lost once the softmax function is applied. The same observation is made through quantitative analysis, as we show that two out-of-distribution and adversarial example detectors obtain competitive results when using logit values as inputs, but provide considerably lower performances if they use softmax probabilities instead: from 98.0% average AUROC to 56.8% in some settings. These results provide evidence that the softmax activation hinders the detection of adversarial and out-of-distribution examples, as it masks a significant part of the relevant information present in the logits.", "pdf": "/pdf/78804458f698f966691634d4c3c3662ae27938d7.pdf", "paperhash": "aigrain|how_the_softmax_activation_hinders_the_detection_of_adversarial_and_outofdistribution_examples_in_neural_networks", "original_pdf": "/attachment/78804458f698f966691634d4c3c3662ae27938d7.pdf", "_bibtex": "@misc{\naigrain2020how,\ntitle={How the Softmax Activation Hinders the Detection of Adversarial and Out-of-Distribution Examples in Neural Networks},\nauthor={Jonathan Aigrain and Marcin Detyniecki},\nyear={2020},\nurl={https://openreview.net/forum?id=Byx55pVKDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Byx55pVKDB", "replyto": "Byx55pVKDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper716/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper716/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576036135205, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper716/Reviewers"], "noninvitees": [], "tcdate": 1570237748126, "tmdate": 1576036135223, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper716/-/Official_Review"}}}], "count": 8}