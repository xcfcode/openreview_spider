{"notes": [{"id": "r1gOe209t7", "original": "HJeOZ-CcFQ", "number": 1092, "cdate": 1538087920335, "ddate": null, "tcdate": 1538087920335, "tmdate": 1545355417404, "tddate": null, "forum": "r1gOe209t7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Reconciling Feature-Reuse and Overfitting in DenseNet with Specialized Dropout", "abstract": "Recently convolutional neural networks (CNNs) achieve great accuracy in visual recognition tasks. DenseNet becomes one of the most popular CNN models due to its effectiveness in feature-reuse. However, like other CNN models, DenseNets also face overfitting problem if not severer. Existing dropout method can be applied but not as effective due to the introduced nonlinear connections. In particular, the property of feature-reuse in DenseNet will be impeded, and the dropout effect will be weakened by the spatial correlation inside feature maps. To address these problems, we craft the design of a specialized dropout method from three aspects, dropout location, dropout granularity, and dropout probability. The insights attained here could potentially be applied as a general approach for boosting the accuracy of other CNN models with similar nonlinear connections. Experimental results show that DenseNets with our specialized dropout method yield better accuracy compared to vanilla DenseNet and state-of-the-art CNN models, and such accuracy boost increases with the model depth.", "keywords": ["Specialized dropout", "computer vision"], "authorids": ["kun@cs.ucsb.edu", "boyuan@cs.ucsb.edu", "xielingwei@stu.xmu.edu.cn", "yufeiding@cs.ucsb.edu"], "authors": ["Kun Wan", "Boyuan Feng", "Lingwei Xie", "Yufei Ding"], "TL;DR": "Realizing the drawbacks when applying original dropout on DenseNet, we craft the design of dropout method from three aspects, the idea of which could also be applied on other CNN models.", "pdf": "/pdf/2939c75d31519701b96f705bb886e29725429b45.pdf", "paperhash": "wan|reconciling_featurereuse_and_overfitting_in_densenet_with_specialized_dropout", "_bibtex": "@misc{\nwan2019reconciling,\ntitle={Reconciling Feature-Reuse and Overfitting in DenseNet with Specialized Dropout},\nauthor={Kun Wan and Boyuan Feng and Lingwei Xie and Yufei Ding},\nyear={2019},\nurl={https://openreview.net/forum?id=r1gOe209t7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "Skgz4lOj14", "original": null, "number": 1, "cdate": 1544417321643, "ddate": null, "tcdate": 1544417321643, "tmdate": 1545354497499, "tddate": null, "forum": "r1gOe209t7", "replyto": "r1gOe209t7", "invitation": "ICLR.cc/2019/Conference/-/Paper1092/Meta_Review", "content": {"metareview": "All reviewers recommend reject and there is no rebuttal. There is no basis on which to accept the paper.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "metareview"}, "signatures": ["ICLR.cc/2019/Conference/Paper1092/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1092/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reconciling Feature-Reuse and Overfitting in DenseNet with Specialized Dropout", "abstract": "Recently convolutional neural networks (CNNs) achieve great accuracy in visual recognition tasks. DenseNet becomes one of the most popular CNN models due to its effectiveness in feature-reuse. However, like other CNN models, DenseNets also face overfitting problem if not severer. Existing dropout method can be applied but not as effective due to the introduced nonlinear connections. In particular, the property of feature-reuse in DenseNet will be impeded, and the dropout effect will be weakened by the spatial correlation inside feature maps. To address these problems, we craft the design of a specialized dropout method from three aspects, dropout location, dropout granularity, and dropout probability. The insights attained here could potentially be applied as a general approach for boosting the accuracy of other CNN models with similar nonlinear connections. Experimental results show that DenseNets with our specialized dropout method yield better accuracy compared to vanilla DenseNet and state-of-the-art CNN models, and such accuracy boost increases with the model depth.", "keywords": ["Specialized dropout", "computer vision"], "authorids": ["kun@cs.ucsb.edu", "boyuan@cs.ucsb.edu", "xielingwei@stu.xmu.edu.cn", "yufeiding@cs.ucsb.edu"], "authors": ["Kun Wan", "Boyuan Feng", "Lingwei Xie", "Yufei Ding"], "TL;DR": "Realizing the drawbacks when applying original dropout on DenseNet, we craft the design of dropout method from three aspects, the idea of which could also be applied on other CNN models.", "pdf": "/pdf/2939c75d31519701b96f705bb886e29725429b45.pdf", "paperhash": "wan|reconciling_featurereuse_and_overfitting_in_densenet_with_specialized_dropout", "_bibtex": "@misc{\nwan2019reconciling,\ntitle={Reconciling Feature-Reuse and Overfitting in DenseNet with Specialized Dropout},\nauthor={Kun Wan and Boyuan Feng and Lingwei Xie and Yufei Ding},\nyear={2019},\nurl={https://openreview.net/forum?id=r1gOe209t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1092/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352969627, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1gOe209t7", "replyto": "r1gOe209t7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1092/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1092/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1092/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352969627}}}, {"id": "S1gRqRvC37", "original": null, "number": 3, "cdate": 1541467798195, "ddate": null, "tcdate": 1541467798195, "tmdate": 1541533429496, "tddate": null, "forum": "r1gOe209t7", "replyto": "r1gOe209t7", "invitation": "ICLR.cc/2019/Conference/-/Paper1092/Official_Review", "content": {"title": "This paper concerns the application of different binary dropout structures and schedules with the specific aim to regularise the DenseNet architecture.", "review": "Overall Thoughts:\n\nI think the use of regularisation to improve performance in DenseNet architectures is a topic of interest to the community. My concern with the paper in it\u2019s current form is that the different dropout structures/schedules are priors and it is not clear from the current analysis exactly what prior is being specified and how to match that to a particular dataset. Further, I believe that the current presentation of the empirical results does not support the nature of the claims being made by the authors. I would be very interested to hear the authors\u2019 comments on the following questions.\n\nSpecific Comments/Questions:\n\nSec1: Sorry if I have missed something but for the two reasons against std dropout on dense net, the reference supports the second claim but could a reference be provided to substantiate the first?\n\nSec1/2: The discussion around feature re-use needs to be clarified slightly in my opinion. Dropout can provide regularisation in a number of regimes - the term \u201cfeature reuse\u201d is a little tricky because I can see the argument from both sides - under the authors arguments, forcing different features to be used can be a source or robustness so would not the level of granularity be something to be put in as a prior and not necessarily inherently correct or incorrect?\n\nSec3: The key contribution (in my opinion) suggested by the authors is the \u201cdetailed analysis\u201d of their dropout structures. I\u2019m afraid I didn\u2019t see this in this section - there are a number of approaches that have been taken in the literature to analyse the regularisation properties of dropout - e.g. the insightful approach of Gal and Ghahramani on dropout as a Bayesian regulariser (as well as others). I was expecting to see something similar to this - could the authors comment on this? Would such an analysis be possible - it would reveal the true priors being applied by the different approaches and allow an analysis of the priors being applied by the different methods?\n\nSec3: Similarly, with the dropout probability schedules, there are practical methods for learning such probabilities during training (e.g. Concrete Dropout) - would it not be possible to learn these parameters with these approaches? Why do we need to set them according to fixed schedules? I think it would be necessary to demonstrate that a fixed schedule outperforms learned parameters.\n\nSec4: My main difficulty here is that the other key contribution of the paper are the claims constructed around empirical results. Throughout the results section, only single values are presented without attempt to measure the distributions of the results (not even error bars). Without this information it is impossible to make any statements on the significance of the results. Ideally histograms should be provided (rather than just error bars). How do we know the changes conferred are significant for the particular problems? How do we know that they are causal from the new structures and not from hyper parameters or optimisation effects?\n\nSec4: Dropout is the application of a prior - how do we know what this prior is doing and when it is sensible to apply it? How do we know the results will transfer to datasets other than CIFAR?\n\nSec4: Please could the authors provide justification to the claim that the improvements would increase with the depth of the network?\n\nRefs: Please could the authors be sure to cite the published versions of articles (not ArXiv versions) when papers have been peer reviewed - e.g. the citation for DenseNet (among others)\n\nOther Points:\n\nCould the authors use text mode for sub or superscripts in maths equations when using words as opposed to symbols?\n\nThere are a number of uses of \u201ccould\u201d when I don\u2019t think the authors mean \u201ccould\u201d - please could this be checked?\n\nTypos:\n\np4 replying -> relying, whcih -> which", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1092/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reconciling Feature-Reuse and Overfitting in DenseNet with Specialized Dropout", "abstract": "Recently convolutional neural networks (CNNs) achieve great accuracy in visual recognition tasks. DenseNet becomes one of the most popular CNN models due to its effectiveness in feature-reuse. However, like other CNN models, DenseNets also face overfitting problem if not severer. Existing dropout method can be applied but not as effective due to the introduced nonlinear connections. In particular, the property of feature-reuse in DenseNet will be impeded, and the dropout effect will be weakened by the spatial correlation inside feature maps. To address these problems, we craft the design of a specialized dropout method from three aspects, dropout location, dropout granularity, and dropout probability. The insights attained here could potentially be applied as a general approach for boosting the accuracy of other CNN models with similar nonlinear connections. Experimental results show that DenseNets with our specialized dropout method yield better accuracy compared to vanilla DenseNet and state-of-the-art CNN models, and such accuracy boost increases with the model depth.", "keywords": ["Specialized dropout", "computer vision"], "authorids": ["kun@cs.ucsb.edu", "boyuan@cs.ucsb.edu", "xielingwei@stu.xmu.edu.cn", "yufeiding@cs.ucsb.edu"], "authors": ["Kun Wan", "Boyuan Feng", "Lingwei Xie", "Yufei Ding"], "TL;DR": "Realizing the drawbacks when applying original dropout on DenseNet, we craft the design of dropout method from three aspects, the idea of which could also be applied on other CNN models.", "pdf": "/pdf/2939c75d31519701b96f705bb886e29725429b45.pdf", "paperhash": "wan|reconciling_featurereuse_and_overfitting_in_densenet_with_specialized_dropout", "_bibtex": "@misc{\nwan2019reconciling,\ntitle={Reconciling Feature-Reuse and Overfitting in DenseNet with Specialized Dropout},\nauthor={Kun Wan and Boyuan Feng and Lingwei Xie and Yufei Ding},\nyear={2019},\nurl={https://openreview.net/forum?id=r1gOe209t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1092/Official_Review", "cdate": 1542234308088, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1gOe209t7", "replyto": "r1gOe209t7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1092/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335870927, "tmdate": 1552335870927, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1092/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJxe-1m22Q", "original": null, "number": 2, "cdate": 1541316344442, "ddate": null, "tcdate": 1541316344442, "tmdate": 1541533429253, "tddate": null, "forum": "r1gOe209t7", "replyto": "r1gOe209t7", "invitation": "ICLR.cc/2019/Conference/-/Paper1092/Official_Review", "content": {"title": "interesting heuristics but no justification either theoretically or empirically", "review": "This paper proposes a special dropout procedure for densenet. The main argument is standard dropout strategy may impede the feature-reuse in Densenet, so the authors propose a pre-dropout technique, which implements the dropout before the nonlinear activation function so that it can be feeded to later layers. Also other tricks are discussed, for example, channel-wise dropout, and probability schedule that assigns different probabilities for different layers in a heuristic way. \n\nTo me this is a mediocre paper. No theoretical justification is given on why their pre-dropout structure could benefit compared to the standard dropout. Why impeding the feature-reuse in the standard dropout strategy is bad? Actually I am not quite sure if reusing the features is the true reason densenet works well in applications.\n\nHeuristic is good if enough empirical evidence is shown, but I do not think the experiment part is solid either. The authors only report results on CIFAR-10 and CIFAR-100. Those are relatively small data sets. I would expect more results on larger sets such as image net.\n\nCifar-10 is small, and most of the networks work fairly well on it. Showing a slight improvement on CIFAR-10 (less than 1 point) does not impress me at all, especially given the way more complicated way of the dropout procedure. \n\nThe result of the pre-dropout on CIFAR-100 is actually worse than the original densenet paper using standard dropout. Densenet-BC (k=24) has an error rate of 19.64, while the pre-dropout is 19.75.\n\nAlso, the result is NOT the-state-of-the-art. Wide-ResNet with standard dropout has better result on both CIFAR-10 and CIFAR-100, but the authors did not mention it.  \n", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1092/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reconciling Feature-Reuse and Overfitting in DenseNet with Specialized Dropout", "abstract": "Recently convolutional neural networks (CNNs) achieve great accuracy in visual recognition tasks. DenseNet becomes one of the most popular CNN models due to its effectiveness in feature-reuse. However, like other CNN models, DenseNets also face overfitting problem if not severer. Existing dropout method can be applied but not as effective due to the introduced nonlinear connections. In particular, the property of feature-reuse in DenseNet will be impeded, and the dropout effect will be weakened by the spatial correlation inside feature maps. To address these problems, we craft the design of a specialized dropout method from three aspects, dropout location, dropout granularity, and dropout probability. The insights attained here could potentially be applied as a general approach for boosting the accuracy of other CNN models with similar nonlinear connections. Experimental results show that DenseNets with our specialized dropout method yield better accuracy compared to vanilla DenseNet and state-of-the-art CNN models, and such accuracy boost increases with the model depth.", "keywords": ["Specialized dropout", "computer vision"], "authorids": ["kun@cs.ucsb.edu", "boyuan@cs.ucsb.edu", "xielingwei@stu.xmu.edu.cn", "yufeiding@cs.ucsb.edu"], "authors": ["Kun Wan", "Boyuan Feng", "Lingwei Xie", "Yufei Ding"], "TL;DR": "Realizing the drawbacks when applying original dropout on DenseNet, we craft the design of dropout method from three aspects, the idea of which could also be applied on other CNN models.", "pdf": "/pdf/2939c75d31519701b96f705bb886e29725429b45.pdf", "paperhash": "wan|reconciling_featurereuse_and_overfitting_in_densenet_with_specialized_dropout", "_bibtex": "@misc{\nwan2019reconciling,\ntitle={Reconciling Feature-Reuse and Overfitting in DenseNet with Specialized Dropout},\nauthor={Kun Wan and Boyuan Feng and Lingwei Xie and Yufei Ding},\nyear={2019},\nurl={https://openreview.net/forum?id=r1gOe209t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1092/Official_Review", "cdate": 1542234308088, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1gOe209t7", "replyto": "r1gOe209t7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1092/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335870927, "tmdate": 1552335870927, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1092/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HyemrA0d2X", "original": null, "number": 1, "cdate": 1541103162903, "ddate": null, "tcdate": 1541103162903, "tmdate": 1541533429053, "tddate": null, "forum": "r1gOe209t7", "replyto": "r1gOe209t7", "invitation": "ICLR.cc/2019/Conference/-/Paper1092/Official_Review", "content": {"title": "Evaluation of dropout regimes for DenseNets", "review": "The paper studies the effect of different dropout regimes (unit-wise, channel-wise and layer-wise), locations and probability affect the performance of DenseNet classification model. The experiments are performed on two datasets: CIFAR10 and CIFAR100.\n\nIn order to improve the paper, the authors could take into consideration the following points:\n\n1. The experimental validation is rather limited. Additional experiments on large scale datasets should be performed (e. g. on ImageNet).\n2. The design choices are rather arbitrary. The authors study three different probability schedules. Wouldn't it be better to learn them using recent advances in neural architecture search or in RL.\n3. \"The test error is reported after every epoch and ...\". This suggest that the authors are monitoring the test set throughout the training. Thus, the hyper parameters selected (e. g. the dropout regimes) might reflect overfitting to the test set.\n4. Table 1 misses some important results on CIFAR10 and CIFAR100, as is, the Table suggest that the method described in the paper is the best performing method on these datasets (and it is not the case). Moreover, the inclusion criteria for papers to appear in Table 1 is not clear. Could the authors correct the Table and add recent results on CIFAR10 and CIFAR100?\n5. Section 4.1: \"... a perfect size for a model of normal size to overfit.\"  This statement is not clear to me. What is a normal size model? Moreover, claiming that CIFAR10 and CIFAR100 is of perfect size to overfit seems to be a bit misleading too. Please rephrase.\n6. Section 3.3: what do the authors mean by deterministic probability model?\n7. Abstract: \"DenseNets also face overfitting problem if not severer\". I'm not aware of any evidence for this. Could the authors add citations accordingly?\n8. Some discussions on recent approaches to model regularizations and connections to proposed approach are missing. The authors might consider including the following papers: https://arxiv.org/pdf/1708.04552.pdf, https://arxiv.org/pdf/1802.02375.pdf, among others.\n\nOverall, the paper is easy to understand. However, the originality of the paper is rather limited and it is not clear what is the added value to for the community from such paper. I'd encourage the authors to include additional experiments, correct misleading statements and add a discussion of model regularization techniques in the related work section.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1092/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reconciling Feature-Reuse and Overfitting in DenseNet with Specialized Dropout", "abstract": "Recently convolutional neural networks (CNNs) achieve great accuracy in visual recognition tasks. DenseNet becomes one of the most popular CNN models due to its effectiveness in feature-reuse. However, like other CNN models, DenseNets also face overfitting problem if not severer. Existing dropout method can be applied but not as effective due to the introduced nonlinear connections. In particular, the property of feature-reuse in DenseNet will be impeded, and the dropout effect will be weakened by the spatial correlation inside feature maps. To address these problems, we craft the design of a specialized dropout method from three aspects, dropout location, dropout granularity, and dropout probability. The insights attained here could potentially be applied as a general approach for boosting the accuracy of other CNN models with similar nonlinear connections. Experimental results show that DenseNets with our specialized dropout method yield better accuracy compared to vanilla DenseNet and state-of-the-art CNN models, and such accuracy boost increases with the model depth.", "keywords": ["Specialized dropout", "computer vision"], "authorids": ["kun@cs.ucsb.edu", "boyuan@cs.ucsb.edu", "xielingwei@stu.xmu.edu.cn", "yufeiding@cs.ucsb.edu"], "authors": ["Kun Wan", "Boyuan Feng", "Lingwei Xie", "Yufei Ding"], "TL;DR": "Realizing the drawbacks when applying original dropout on DenseNet, we craft the design of dropout method from three aspects, the idea of which could also be applied on other CNN models.", "pdf": "/pdf/2939c75d31519701b96f705bb886e29725429b45.pdf", "paperhash": "wan|reconciling_featurereuse_and_overfitting_in_densenet_with_specialized_dropout", "_bibtex": "@misc{\nwan2019reconciling,\ntitle={Reconciling Feature-Reuse and Overfitting in DenseNet with Specialized Dropout},\nauthor={Kun Wan and Boyuan Feng and Lingwei Xie and Yufei Ding},\nyear={2019},\nurl={https://openreview.net/forum?id=r1gOe209t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1092/Official_Review", "cdate": 1542234308088, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1gOe209t7", "replyto": "r1gOe209t7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1092/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335870927, "tmdate": 1552335870927, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1092/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}