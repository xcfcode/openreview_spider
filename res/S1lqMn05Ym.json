{"notes": [{"id": "S1lqMn05Ym", "original": "BygZzp35YX", "number": 1293, "cdate": 1538087954394, "ddate": null, "tcdate": 1538087954394, "tmdate": 1550861216685, "tddate": null, "forum": "S1lqMn05Ym", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Information asymmetry in KL-regularized RL", "abstract": "Many real world tasks exhibit rich structure that is repeated across different parts of the state space or in time. In this work we study the possibility of leveraging such repeated structure to speed up and regularize learning. We start from the KL regularized expected reward objective which introduces an additional component, a default policy. Instead of relying on a fixed default policy, we learn it from data. But crucially, we restrict the amount of information the default policy receives, forcing it to learn reusable behaviors that help the policy learn faster. We formalize this strategy and discuss connections to information bottleneck approaches and to the variational EM algorithm. We present empirical results in both discrete and continuous action domains and demonstrate that, for certain tasks, learning a default policy alongside the policy can significantly speed up and improve learning.\nPlease watch the video demonstrating learned experts and default policies on several continuous control tasks ( https://youtu.be/U2qA3llzus8 ).", "keywords": ["Deep Reinforcement Learning", "Continuous Control", "RL as Inference"], "authorids": ["agalashov@google.com", "sidmj@google.com", "leonardh@google.com", "dhruvat@google.com", "schwarzjn@google.com", "gdesjardins@google.com", "lejlot@google.com", "ywteh@google.com", "razp@google.com", "heess@google.com"], "authors": ["Alexandre Galashov", "Siddhant M. Jayakumar", "Leonard Hasenclever", "Dhruva Tirumala", "Jonathan Schwarz", "Guillaume Desjardins", "Wojciech M. Czarnecki", "Yee Whye Teh", "Razvan Pascanu", "Nicolas Heess"], "TL;DR": "Limiting state information for the default policy can improvement performance, in a KL-regularized RL framework where both agent and default policy are optimized together", "pdf": "/pdf/b702f9a26f2cd46be2d9fed0c8a09c176930e77c.pdf", "paperhash": "galashov|information_asymmetry_in_klregularized_rl", "_bibtex": "@inproceedings{\ngalashov2018information,\ntitle={Information asymmetry in {KL}-regularized {RL}},\nauthor={Alexandre Galashov and Siddhant Jayakumar and Leonard Hasenclever and Dhruva Tirumala and Jonathan Schwarz and Guillaume Desjardins and Wojtek M. Czarnecki and Yee Whye Teh and Razvan Pascanu and Nicolas Heess},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lqMn05Ym},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HJlqtuYllE", "original": null, "number": 1, "cdate": 1544751234436, "ddate": null, "tcdate": 1544751234436, "tmdate": 1545354516618, "tddate": null, "forum": "S1lqMn05Ym", "replyto": "S1lqMn05Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper1293/Meta_Review", "content": {"metareview": "Strengths\n\nThe paper introduces a promising and novel idea, i.e., regularizing RL via an informationally asymmetric default policy \nThe paper is well written.  It has solid and extensive experimental results.\n\nWeaknesses\n\n\nThere is a lack of benefit on dense-reward problems as a limitation, which the authors further\nacknowledge as a limitation. There also some similarities to HRL approaches. \nA lack of theoretical results is also suggested. To be fair, the paper makes a number of connections\nwith various bits of theory, although it perhaps does not directly result in any new theoretical analysis.\nA concern of one reviewer is the need for extensive compute, and making comparisons to stronger (maxent) baselines.\nThe authors provide a convincing reply on these issues.\n\nPoints of Contention\n\nWhile the scores are non-uniform (7,7,5), the most critical review, R1(5), is in fact quite positive on many\naspects of the paper, i.e., \"this paper would have good impact in coming up with new \nlearning algorithms which are inspired from cognitive science literature as well as mathematically grounded.\"\nThe specific critiques of R1 were covered in detail by the authors.\n\nOverall\n\nThe paper presents a novel and fairly intuitive idea, with very solid experimental results.  \nWhile the methods has theoretical results, the results themselves are more experimental than theoretic.\nThe reviewers are largely enthused about the paper.  The AC recommends acceptance as a poster.\n", "confidence": "5: The area chair is absolutely certain", "recommendation": "Accept (Poster)", "title": "intuitive idea & theoretical connections; solid experimental results"}, "signatures": ["ICLR.cc/2019/Conference/Paper1293/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1293/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Information asymmetry in KL-regularized RL", "abstract": "Many real world tasks exhibit rich structure that is repeated across different parts of the state space or in time. In this work we study the possibility of leveraging such repeated structure to speed up and regularize learning. We start from the KL regularized expected reward objective which introduces an additional component, a default policy. Instead of relying on a fixed default policy, we learn it from data. But crucially, we restrict the amount of information the default policy receives, forcing it to learn reusable behaviors that help the policy learn faster. We formalize this strategy and discuss connections to information bottleneck approaches and to the variational EM algorithm. We present empirical results in both discrete and continuous action domains and demonstrate that, for certain tasks, learning a default policy alongside the policy can significantly speed up and improve learning.\nPlease watch the video demonstrating learned experts and default policies on several continuous control tasks ( https://youtu.be/U2qA3llzus8 ).", "keywords": ["Deep Reinforcement Learning", "Continuous Control", "RL as Inference"], "authorids": ["agalashov@google.com", "sidmj@google.com", "leonardh@google.com", "dhruvat@google.com", "schwarzjn@google.com", "gdesjardins@google.com", "lejlot@google.com", "ywteh@google.com", "razp@google.com", "heess@google.com"], "authors": ["Alexandre Galashov", "Siddhant M. Jayakumar", "Leonard Hasenclever", "Dhruva Tirumala", "Jonathan Schwarz", "Guillaume Desjardins", "Wojciech M. Czarnecki", "Yee Whye Teh", "Razvan Pascanu", "Nicolas Heess"], "TL;DR": "Limiting state information for the default policy can improvement performance, in a KL-regularized RL framework where both agent and default policy are optimized together", "pdf": "/pdf/b702f9a26f2cd46be2d9fed0c8a09c176930e77c.pdf", "paperhash": "galashov|information_asymmetry_in_klregularized_rl", "_bibtex": "@inproceedings{\ngalashov2018information,\ntitle={Information asymmetry in {KL}-regularized {RL}},\nauthor={Alexandre Galashov and Siddhant Jayakumar and Leonard Hasenclever and Dhruva Tirumala and Jonathan Schwarz and Guillaume Desjardins and Wojtek M. Czarnecki and Yee Whye Teh and Razvan Pascanu and Nicolas Heess},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lqMn05Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1293/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352891315, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lqMn05Ym", "replyto": "S1lqMn05Ym", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1293/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1293/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1293/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352891315}}}, {"id": "rJxZQQzMCX", "original": null, "number": 4, "cdate": 1542755097489, "ddate": null, "tcdate": 1542755097489, "tmdate": 1542755097489, "tddate": null, "forum": "S1lqMn05Ym", "replyto": "Bye57sxdhQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1293/Official_Comment", "content": {"title": "Thank you for the insightful comments. 1. Many modern algorithms use actor-critic architecture though we will investigate other algorithms in the future work. 2. Theory in the follow-up. 3. Improved clarity", "comment": "We appreciate the reviewers positive feedback and the insightful comments. Thank you. Below we provide replies to the three concerns raised by the reviewer.\n\nComment: My understanding is that this \"informationally asymmetric\" KL-regularization approach is a general approach and can be combined with many policy learning algorithms. It is not completely clear to me why the authors choose to combine it with an actor-critic approach (see Algorithm 1)? Why not combine it with other policy learning algorithms? Please explain\n\nAnswer: This is an important point. The core idea of the paper is indeed largely algorithm agnostic; in principle, any algorithm suitable for optimizing the KL-regularized expected reward objective could be considered. For the experiments we chose well established algorithms for learning. As it turns out, many modern algorithms use an actor-critic architecture of some kind. In the paper, we already perform experiments with three reference algorithms:  IMPALA [1], SVG(0) [2] + Retrace [3], continuous version of IMPALA (which we refer as Vtrace) [1] (for vision foraging task, details are in the appendix D).  These differ along important dimensions (e.g. learning Q-function vs. learning V-function only; score-function estimator vs. reparametrization for policy gradient step; use of replay buffer or not). Of course testing other algorithms will further reinforce the significance of our approach, and we intend to do so in future work. But this also represents a non-trivial amount of work, and might be out of scope for a single publication.\n\nComment: This paper does not have any theoretical results. [...] I recommend the authors to analyze (possibly a variant of) the proposed algorithm in a simplified setting (e.g. the network has only one layer, or even is linear) to further strengthen the results.\nAnswer: Thank you for your suggestion. In the paper we established a theoretical insight by connecting it to the information bottleneck and variational EM. One additional simple analysis we could add is similar to the one from Soft Actor-Critic [4] paper where we could show that soft policy iteration algorithm with additional default policy optimization step will converge to the optimal policy (within the restricted set of policies). We are currently thinking about more detailed theoretical analysis, and decided to leave this for the future work (putting all analysis together) since it represents a non trivial amount of effort.\n\nComment: The experiment results of this paper are interesting [...] I recommend the authors to try to provide intuitive explanation for all such interesting observations in the paper.\nAnswer: We did attempt to provide an intuitive explanation for such observations, e.g. on page 7, where we discuss \u201cdense-reward\u201d case. (There is also appendix E, with additional experimental results and explanations). But we agree with the reviewer that the clarity of these points can be improved. Please see the updated version of the paper with improved clarity of the interesting observations in the paper. For the dense reward case in particular please also see our reply to AnonReviewer2 above.\n\n\nReferences:\n\n[1] Espeholt L., Soyer. H., Munos R., Simonyan K., Mnih V., Ward T., Doron Y., Firoiu V., Harley T., Dunning I., Legg S., Kavukcuoglu K., \"IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures\", 2018, https://arxiv.org/abs/1802.01561\n\n[2] Heess N., Wayne G., Silver D., Lillicrap T., Tassa Y., Erez T., \"Learning Continuous Control Policies by Stochastic Value Gradients\", 2015, https://arxiv.org/abs/1510.09142\n\n[3] Munos R., Stepleton T., Harutyunyan A., Bellemare M.G., \"Safe and Efficient Off-Policy Reinforcement Learning\", 2016, https://arxiv.org/abs/1606.02647\n\n[4] Haarnoja T., Zhou A., Abbeel P., Levine S., \"Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\", 2018, https://arxiv.org/abs/1801.01290\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1293/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1293/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1293/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Information asymmetry in KL-regularized RL", "abstract": "Many real world tasks exhibit rich structure that is repeated across different parts of the state space or in time. In this work we study the possibility of leveraging such repeated structure to speed up and regularize learning. We start from the KL regularized expected reward objective which introduces an additional component, a default policy. Instead of relying on a fixed default policy, we learn it from data. But crucially, we restrict the amount of information the default policy receives, forcing it to learn reusable behaviors that help the policy learn faster. We formalize this strategy and discuss connections to information bottleneck approaches and to the variational EM algorithm. We present empirical results in both discrete and continuous action domains and demonstrate that, for certain tasks, learning a default policy alongside the policy can significantly speed up and improve learning.\nPlease watch the video demonstrating learned experts and default policies on several continuous control tasks ( https://youtu.be/U2qA3llzus8 ).", "keywords": ["Deep Reinforcement Learning", "Continuous Control", "RL as Inference"], "authorids": ["agalashov@google.com", "sidmj@google.com", "leonardh@google.com", "dhruvat@google.com", "schwarzjn@google.com", "gdesjardins@google.com", "lejlot@google.com", "ywteh@google.com", "razp@google.com", "heess@google.com"], "authors": ["Alexandre Galashov", "Siddhant M. Jayakumar", "Leonard Hasenclever", "Dhruva Tirumala", "Jonathan Schwarz", "Guillaume Desjardins", "Wojciech M. Czarnecki", "Yee Whye Teh", "Razvan Pascanu", "Nicolas Heess"], "TL;DR": "Limiting state information for the default policy can improvement performance, in a KL-regularized RL framework where both agent and default policy are optimized together", "pdf": "/pdf/b702f9a26f2cd46be2d9fed0c8a09c176930e77c.pdf", "paperhash": "galashov|information_asymmetry_in_klregularized_rl", "_bibtex": "@inproceedings{\ngalashov2018information,\ntitle={Information asymmetry in {KL}-regularized {RL}},\nauthor={Alexandre Galashov and Siddhant Jayakumar and Leonard Hasenclever and Dhruva Tirumala and Jonathan Schwarz and Guillaume Desjardins and Wojtek M. Czarnecki and Yee Whye Teh and Razvan Pascanu and Nicolas Heess},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lqMn05Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1293/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616844, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lqMn05Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1293/Authors", "ICLR.cc/2019/Conference/Paper1293/Reviewers", "ICLR.cc/2019/Conference/Paper1293/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1293/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1293/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1293/Authors|ICLR.cc/2019/Conference/Paper1293/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1293/Reviewers", "ICLR.cc/2019/Conference/Paper1293/Authors", "ICLR.cc/2019/Conference/Paper1293/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616844}}}, {"id": "BJxuzZMMCm", "original": null, "number": 3, "cdate": 1542754576337, "ddate": null, "tcdate": 1542754576337, "tmdate": 1542754576337, "tddate": null, "forum": "S1lqMn05Ym", "replyto": "S1gXUjKKhm", "invitation": "ICLR.cc/2019/Conference/-/Paper1293/Official_Comment", "content": {"title": "Thank you for the comment. 1. Scale of our experiments is comparable to the existing DeepRL approaches. 2. We eventually compare to similar to SAC baseline", "comment": "We appreciate the reviewer comments about the habit-based perspective on the decision making which as he points out gives an intuition for using information theoretic objective to encourage an agent to follow default behaviors on average and being penalized by deviating. We thank the reviewer for the insights and for the positive feedback. Below we address two raised concerns.\n\nComment:\nMost of the experiments in this paper use of the order of 10^9 or even 10^10 steps[...]\n\nAnswer:\nWe apologize for the confusion. In our setting, actors are collecting experience that is sent to a centralized learner that computes gradients using the stored trajectories. The learner will go over the stored trajectories multiple times. We report performance as a function of the number of non-unique environment transitions processed by the learner. To compute, for instance, the actual number of weight updates we thus need to divide the number on the x-axis of our plots  by the batch size (512) and sequence length (10). The number of updates applied to the model in our setup is thus approximately 3 orders of magnitude lower than suggested by the plots and well within the standard number of updates used in most deep learning or deep RL papers. (e.g. IMPALA [1], Soft Actor-Critic [2], etc.). We will further improve the explanation of the setup in the paper [in the originally submission located at the end of page 6 - beginning of page 7, \"Experimental setup\" paragraph, last 5 sentences] in a revised version of the paper.\nTo make this point more concrete, in Soft Actor-Critic paper [2], the authors report the number of executed environment steps, where for each step, they apply a batched (of size 256) gradient update (in our terminology it corresponds to the 256 agent steps) on the trajectories sampled from the replay buffer to modify agent parameters (see open-sourced code at https://github.com/haarnoja/sac/blob/master/sac/algos/base.py, lines 89-110; Page 5, last sentence at the right bottom: \u201c[...] In practice, we take a single environment step followed by one or several gradient steps). If we take an example of Ant-v1 task, and try to map their results onto a similar plotting schema as in our paper, we should multiple their number of steps (3e6) by their batch size (256), and we obtain 7.68 * 1e8 \u201cagent steps\u201d (non-unique transitions processed by learner). The order of magnitude is similar to some of our simpler tasks, such as the task at Figure 5. This is, of course, not an entirely direct comparison because we have a distributed actor-learner setup, but it demonstrates that the order of magnitude of learning updates in our setup is consistent with the methods reported in Deep RL literature. Nevertheless, we do understand the reproducibility point of view, and while we do not agree this paper has an issue with running experiments that are at too large scales, it is an important consideration for future works for the field in general. Finally, we provide a large number of different experiments (including ones in Appendix E) demonstrating our idea. The submission also already contains some results for a non-distributed (Appendix B, figure 7) setup.\n\nComment:\nIt becomes more important to compare to stronger baselines like maximum entropy RL (Soft Actor Critic)[...]\n\nAnswer:\nWe effectively provide this baseline already [Figure 5, left.]. We compare to two entropy regularized baselines:  SVG(0) [3] with entropy regularization, and SVG(0) with entropy bonus. The former optimizes the entropy regularized expected reward objective using a Q-function and policy updates via reparametrization in the same way as SAC. The latter also implements SVG(0) policy updates (Q function + reparametrization and back-propagation) but includes entropy only in the policy update as e.g. is general practice in many DRL papers (e.g. [4]). We optimize hyperparameters for each algorithm separately. Compared to the SAC algorithm there are some minor differences in the use of target networks, which are, however, orthogonal to the ideas of our paper (which in unreported  experiments made no qualitative difference to the results). \n\nReferences:\n\n[1] Espeholt L., Soyer. H., Munos R., Simonyan K., Mnih V., Ward T., Doron Y., Firoiu V., Harley T., Dunning I., Legg S., Kavukcuoglu K., \"IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures\", 2018, https://arxiv.org/abs/1802.01561\n[2] Haarnoja T., Zhou A., Abbeel P., Levine S., \"Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\", 2018, https://arxiv.org/abs/1801.01290\n[3] Heess N., Wayne G., Silver D., Lillicrap T., Tassa Y., Erez T., \"Learning Continuous Control Policies by Stochastic Value Gradients\", 2015, https://arxiv.org/abs/1510.09142\n[4] Asynchronous Methods for Deep Reinforcement Learning, Mnih V.,  Badia A.P., Mirza M., Graves A., Lillicrap T.P., Harley T., Silver D., Kavukcuoglu K., 2016, https://arxiv.org/pdf/1602.01783.pdf"}, "signatures": ["ICLR.cc/2019/Conference/Paper1293/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1293/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1293/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Information asymmetry in KL-regularized RL", "abstract": "Many real world tasks exhibit rich structure that is repeated across different parts of the state space or in time. In this work we study the possibility of leveraging such repeated structure to speed up and regularize learning. We start from the KL regularized expected reward objective which introduces an additional component, a default policy. Instead of relying on a fixed default policy, we learn it from data. But crucially, we restrict the amount of information the default policy receives, forcing it to learn reusable behaviors that help the policy learn faster. We formalize this strategy and discuss connections to information bottleneck approaches and to the variational EM algorithm. We present empirical results in both discrete and continuous action domains and demonstrate that, for certain tasks, learning a default policy alongside the policy can significantly speed up and improve learning.\nPlease watch the video demonstrating learned experts and default policies on several continuous control tasks ( https://youtu.be/U2qA3llzus8 ).", "keywords": ["Deep Reinforcement Learning", "Continuous Control", "RL as Inference"], "authorids": ["agalashov@google.com", "sidmj@google.com", "leonardh@google.com", "dhruvat@google.com", "schwarzjn@google.com", "gdesjardins@google.com", "lejlot@google.com", "ywteh@google.com", "razp@google.com", "heess@google.com"], "authors": ["Alexandre Galashov", "Siddhant M. Jayakumar", "Leonard Hasenclever", "Dhruva Tirumala", "Jonathan Schwarz", "Guillaume Desjardins", "Wojciech M. Czarnecki", "Yee Whye Teh", "Razvan Pascanu", "Nicolas Heess"], "TL;DR": "Limiting state information for the default policy can improvement performance, in a KL-regularized RL framework where both agent and default policy are optimized together", "pdf": "/pdf/b702f9a26f2cd46be2d9fed0c8a09c176930e77c.pdf", "paperhash": "galashov|information_asymmetry_in_klregularized_rl", "_bibtex": "@inproceedings{\ngalashov2018information,\ntitle={Information asymmetry in {KL}-regularized {RL}},\nauthor={Alexandre Galashov and Siddhant Jayakumar and Leonard Hasenclever and Dhruva Tirumala and Jonathan Schwarz and Guillaume Desjardins and Wojtek M. Czarnecki and Yee Whye Teh and Razvan Pascanu and Nicolas Heess},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lqMn05Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1293/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616844, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lqMn05Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1293/Authors", "ICLR.cc/2019/Conference/Paper1293/Reviewers", "ICLR.cc/2019/Conference/Paper1293/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1293/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1293/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1293/Authors|ICLR.cc/2019/Conference/Paper1293/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1293/Reviewers", "ICLR.cc/2019/Conference/Paper1293/Authors", "ICLR.cc/2019/Conference/Paper1293/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616844}}}, {"id": "B1gITyzGCQ", "original": null, "number": 2, "cdate": 1542754237925, "ddate": null, "tcdate": 1542754237925, "tmdate": 1542754237925, "tddate": null, "forum": "S1lqMn05Ym", "replyto": "HJxrOYAJpX", "invitation": "ICLR.cc/2019/Conference/-/Paper1293/Official_Comment", "content": {"title": "Thank you. Summary: 1. HRL differs from our setup to be directly comparable. The connections are left for future work. 2. Dense-reward setting is too simple for the regularizer to give a significant improvement", "comment": "We thank the reviewer for their time, positive feedback, and insightful comments. Below are replies to the two questions the reviewer raised:\n\nComment:\nThe idea of separating policy and default policy seems similar to having high and low level controller [...] Would have liked to see comparison between the proposed method and hierarchical control.\n\nAnswer:\nThis is indeed a very interesting question. The proposed framework currently differs from \u201cclassical\u201d HRL ideas in the following sense:\n    - The agent policy is not hierarchical (there is no notion of high-level actions).\n    - The default policy is an external component, and not a part of the agent (such as LLC in HRL). In practice you need the default policy only during training, not at test time.\nThese differences suggest that there is no simple 1-1 mapping to HRL frameworks that are focused on HL actions which makes a sensible comparison difficult. Nevertheless our method does, of course, share important intuitions with work on HRL in the sense of learning about reusable behavioral structure. In the paper, we propose a theoretical justification for the current framework from the information bottleneck perspective. In the appendix A.2, we do indeed derive the form of the objective for latent-variable policies, which can be seen as having HL (latent space) and LL (action space). This suggests that these ideas can eventually be unified, and we are currently investigating these connections in detail. These results are, however, beyond the scope of the current work.\n\nComment:\nAs mentioned, the proposed method does not offer significant speed-up in dense-reward settings. [...] it'd be nice to have experiments to show that for some environments the proposed method can out-perform baseline methods even in dense-reward settings.\n\nAnswer:\nAs we mention in the paper, in the dense-reward setup the problem of learning the policy with KL-regularization to the default one, is not simpler than regular policy learning. We explain it in the appendix E.1 that it is probably due to already strong reward signal. If everywhere in the state space we get a sufficient learning signal to learn the relevant behavior then the point of the default policy (which should help to provide a structured exploration strategy, asking the agent to act consistently to other regions it has seen and learned) is somewhat reduced. Nevertheless, in the Appendix E, we provide additional results for the dense-reward tasks and show that the current method performance doesn\u2019t become worse comparing to the baseline. Our intuition is that an example where our method would also help in the dense-reward scenario would be the one with weak shaping reward combined with complex action space (e.g. humanoid). Finding such scenarios is left for the follow-up work."}, "signatures": ["ICLR.cc/2019/Conference/Paper1293/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1293/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1293/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Information asymmetry in KL-regularized RL", "abstract": "Many real world tasks exhibit rich structure that is repeated across different parts of the state space or in time. In this work we study the possibility of leveraging such repeated structure to speed up and regularize learning. We start from the KL regularized expected reward objective which introduces an additional component, a default policy. Instead of relying on a fixed default policy, we learn it from data. But crucially, we restrict the amount of information the default policy receives, forcing it to learn reusable behaviors that help the policy learn faster. We formalize this strategy and discuss connections to information bottleneck approaches and to the variational EM algorithm. We present empirical results in both discrete and continuous action domains and demonstrate that, for certain tasks, learning a default policy alongside the policy can significantly speed up and improve learning.\nPlease watch the video demonstrating learned experts and default policies on several continuous control tasks ( https://youtu.be/U2qA3llzus8 ).", "keywords": ["Deep Reinforcement Learning", "Continuous Control", "RL as Inference"], "authorids": ["agalashov@google.com", "sidmj@google.com", "leonardh@google.com", "dhruvat@google.com", "schwarzjn@google.com", "gdesjardins@google.com", "lejlot@google.com", "ywteh@google.com", "razp@google.com", "heess@google.com"], "authors": ["Alexandre Galashov", "Siddhant M. Jayakumar", "Leonard Hasenclever", "Dhruva Tirumala", "Jonathan Schwarz", "Guillaume Desjardins", "Wojciech M. Czarnecki", "Yee Whye Teh", "Razvan Pascanu", "Nicolas Heess"], "TL;DR": "Limiting state information for the default policy can improvement performance, in a KL-regularized RL framework where both agent and default policy are optimized together", "pdf": "/pdf/b702f9a26f2cd46be2d9fed0c8a09c176930e77c.pdf", "paperhash": "galashov|information_asymmetry_in_klregularized_rl", "_bibtex": "@inproceedings{\ngalashov2018information,\ntitle={Information asymmetry in {KL}-regularized {RL}},\nauthor={Alexandre Galashov and Siddhant Jayakumar and Leonard Hasenclever and Dhruva Tirumala and Jonathan Schwarz and Guillaume Desjardins and Wojtek M. Czarnecki and Yee Whye Teh and Razvan Pascanu and Nicolas Heess},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lqMn05Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1293/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616844, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lqMn05Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1293/Authors", "ICLR.cc/2019/Conference/Paper1293/Reviewers", "ICLR.cc/2019/Conference/Paper1293/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1293/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1293/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1293/Authors|ICLR.cc/2019/Conference/Paper1293/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1293/Reviewers", "ICLR.cc/2019/Conference/Paper1293/Authors", "ICLR.cc/2019/Conference/Paper1293/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616844}}}, {"id": "rygVCXQWR7", "original": null, "number": 1, "cdate": 1542693835914, "ddate": null, "tcdate": 1542693835914, "tmdate": 1542693835914, "tddate": null, "forum": "S1lqMn05Ym", "replyto": "S1lqMn05Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper1293/Official_Comment", "content": {"title": "thanks for the reviews; authors response?", "comment": "Thanks for the detailed review comments thus far.\nDo the authors wish to add anything or respond in any way?\n-- area chair"}, "signatures": ["ICLR.cc/2019/Conference/Paper1293/Area_Chair1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1293/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1293/Area_Chair1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Information asymmetry in KL-regularized RL", "abstract": "Many real world tasks exhibit rich structure that is repeated across different parts of the state space or in time. In this work we study the possibility of leveraging such repeated structure to speed up and regularize learning. We start from the KL regularized expected reward objective which introduces an additional component, a default policy. Instead of relying on a fixed default policy, we learn it from data. But crucially, we restrict the amount of information the default policy receives, forcing it to learn reusable behaviors that help the policy learn faster. We formalize this strategy and discuss connections to information bottleneck approaches and to the variational EM algorithm. We present empirical results in both discrete and continuous action domains and demonstrate that, for certain tasks, learning a default policy alongside the policy can significantly speed up and improve learning.\nPlease watch the video demonstrating learned experts and default policies on several continuous control tasks ( https://youtu.be/U2qA3llzus8 ).", "keywords": ["Deep Reinforcement Learning", "Continuous Control", "RL as Inference"], "authorids": ["agalashov@google.com", "sidmj@google.com", "leonardh@google.com", "dhruvat@google.com", "schwarzjn@google.com", "gdesjardins@google.com", "lejlot@google.com", "ywteh@google.com", "razp@google.com", "heess@google.com"], "authors": ["Alexandre Galashov", "Siddhant M. Jayakumar", "Leonard Hasenclever", "Dhruva Tirumala", "Jonathan Schwarz", "Guillaume Desjardins", "Wojciech M. Czarnecki", "Yee Whye Teh", "Razvan Pascanu", "Nicolas Heess"], "TL;DR": "Limiting state information for the default policy can improvement performance, in a KL-regularized RL framework where both agent and default policy are optimized together", "pdf": "/pdf/b702f9a26f2cd46be2d9fed0c8a09c176930e77c.pdf", "paperhash": "galashov|information_asymmetry_in_klregularized_rl", "_bibtex": "@inproceedings{\ngalashov2018information,\ntitle={Information asymmetry in {KL}-regularized {RL}},\nauthor={Alexandre Galashov and Siddhant Jayakumar and Leonard Hasenclever and Dhruva Tirumala and Jonathan Schwarz and Guillaume Desjardins and Wojtek M. Czarnecki and Yee Whye Teh and Razvan Pascanu and Nicolas Heess},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lqMn05Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1293/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616844, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lqMn05Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1293/Authors", "ICLR.cc/2019/Conference/Paper1293/Reviewers", "ICLR.cc/2019/Conference/Paper1293/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1293/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1293/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1293/Authors|ICLR.cc/2019/Conference/Paper1293/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1293/Reviewers", "ICLR.cc/2019/Conference/Paper1293/Authors", "ICLR.cc/2019/Conference/Paper1293/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616844}}}, {"id": "HJxrOYAJpX", "original": null, "number": 3, "cdate": 1541560685241, "ddate": null, "tcdate": 1541560685241, "tmdate": 1541560685241, "tddate": null, "forum": "S1lqMn05Ym", "replyto": "S1lqMn05Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper1293/Official_Review", "content": {"title": "Novel approach", "review": "This paper shows that significant speed-up gains can be achieved by using KL-regularization with information asymmetry in sparse-reward settings.  Different from previous works, the policy and default policy are learned simultaneously.  Furthermore, it demonstrates that the default policy can be used to perform transfer learning.\n\nPros:\n\n- Overall the paper is well-written and the organization is easy to follow.  The approach is novel and most relevant works are compared and contrasted.  The intuitions provided nicely complements the concepts and experiments are thorough.\n\nCons:\n\n- The idea of separating policy and default policy seems similar to having high and low level controller (HLC and LLC) in hierarchical control -- where LLC takes proprioceptive observations as input, and HLC handles task specific goals.  In contrast, one advantage of the proposed method in this work is that the training is end-to-end.  Would have liked to see comparison between the proposed method and hierarchical control.\n\n- As mentioned, the proposed method does not offer significant speed-up in dense-reward settings.  Considering that most of the tasks experimented in the paper can leverage dense shaping to achieve speed-up over sparse rewards, it'd be nice to have experiments to show that for some environments the proposed method can out-perform baseline methods even in dense-reward settings.\n\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1293/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Information asymmetry in KL-regularized RL", "abstract": "Many real world tasks exhibit rich structure that is repeated across different parts of the state space or in time. In this work we study the possibility of leveraging such repeated structure to speed up and regularize learning. We start from the KL regularized expected reward objective which introduces an additional component, a default policy. Instead of relying on a fixed default policy, we learn it from data. But crucially, we restrict the amount of information the default policy receives, forcing it to learn reusable behaviors that help the policy learn faster. We formalize this strategy and discuss connections to information bottleneck approaches and to the variational EM algorithm. We present empirical results in both discrete and continuous action domains and demonstrate that, for certain tasks, learning a default policy alongside the policy can significantly speed up and improve learning.\nPlease watch the video demonstrating learned experts and default policies on several continuous control tasks ( https://youtu.be/U2qA3llzus8 ).", "keywords": ["Deep Reinforcement Learning", "Continuous Control", "RL as Inference"], "authorids": ["agalashov@google.com", "sidmj@google.com", "leonardh@google.com", "dhruvat@google.com", "schwarzjn@google.com", "gdesjardins@google.com", "lejlot@google.com", "ywteh@google.com", "razp@google.com", "heess@google.com"], "authors": ["Alexandre Galashov", "Siddhant M. Jayakumar", "Leonard Hasenclever", "Dhruva Tirumala", "Jonathan Schwarz", "Guillaume Desjardins", "Wojciech M. Czarnecki", "Yee Whye Teh", "Razvan Pascanu", "Nicolas Heess"], "TL;DR": "Limiting state information for the default policy can improvement performance, in a KL-regularized RL framework where both agent and default policy are optimized together", "pdf": "/pdf/b702f9a26f2cd46be2d9fed0c8a09c176930e77c.pdf", "paperhash": "galashov|information_asymmetry_in_klregularized_rl", "_bibtex": "@inproceedings{\ngalashov2018information,\ntitle={Information asymmetry in {KL}-regularized {RL}},\nauthor={Alexandre Galashov and Siddhant Jayakumar and Leonard Hasenclever and Dhruva Tirumala and Jonathan Schwarz and Guillaume Desjardins and Wojtek M. Czarnecki and Yee Whye Teh and Razvan Pascanu and Nicolas Heess},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lqMn05Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1293/Official_Review", "cdate": 1542234261846, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1lqMn05Ym", "replyto": "S1lqMn05Ym", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1293/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335915042, "tmdate": 1552335915042, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1293/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1gXUjKKhm", "original": null, "number": 2, "cdate": 1541147466988, "ddate": null, "tcdate": 1541147466988, "tmdate": 1541533261075, "tddate": null, "forum": "S1lqMn05Ym", "replyto": "S1lqMn05Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper1293/Official_Review", "content": {"title": "Review", "review": "This is a very interesting piece of work. We know from cognitive science literature, that there are 2 distinct modes of decision making - habit based and top-down control (goal directed) decision making. The paper proposes to use this intuition by using information theoretic objective such that the agent follows \"default\" policy on average and agent gets penalized for changing its \"default\" behaviour, and the idea is to minimize this cost on average across states.\n\nThe paper is very well written. I think, this paper would have good impact in coming up with new learning algorithms which are inspired from cognitive science literature as well as mathematically grounded. But I dont think, paper in its current form is suitable for publication. \n\nThere are several reasons, but most important:\n\n1) Most of the experiments in this paper use of the order of  10^9 or even 10^10 steps. Its practically not possible for anyone in academia to have such a compute. Now, that said, I do think this paper is pretty interesting. Hence, Is it possible to construct a toy problem which has similar characteristics, and then show similar results using like 10^6 or 10^7 steps ? I think it would be easy to construct a 2D POMPD maze navigation env and test similar results. This would improve the paper, as well as could provide a baseline which people in the future can compare to.\n\n2) It becomes more important to compare to stronger baselines like maximum entropy RL ( for ex. Soft Actor Critic). And spend some good of amount time getting these baselines right on these new environments. ", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1293/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Information asymmetry in KL-regularized RL", "abstract": "Many real world tasks exhibit rich structure that is repeated across different parts of the state space or in time. In this work we study the possibility of leveraging such repeated structure to speed up and regularize learning. We start from the KL regularized expected reward objective which introduces an additional component, a default policy. Instead of relying on a fixed default policy, we learn it from data. But crucially, we restrict the amount of information the default policy receives, forcing it to learn reusable behaviors that help the policy learn faster. We formalize this strategy and discuss connections to information bottleneck approaches and to the variational EM algorithm. We present empirical results in both discrete and continuous action domains and demonstrate that, for certain tasks, learning a default policy alongside the policy can significantly speed up and improve learning.\nPlease watch the video demonstrating learned experts and default policies on several continuous control tasks ( https://youtu.be/U2qA3llzus8 ).", "keywords": ["Deep Reinforcement Learning", "Continuous Control", "RL as Inference"], "authorids": ["agalashov@google.com", "sidmj@google.com", "leonardh@google.com", "dhruvat@google.com", "schwarzjn@google.com", "gdesjardins@google.com", "lejlot@google.com", "ywteh@google.com", "razp@google.com", "heess@google.com"], "authors": ["Alexandre Galashov", "Siddhant M. Jayakumar", "Leonard Hasenclever", "Dhruva Tirumala", "Jonathan Schwarz", "Guillaume Desjardins", "Wojciech M. Czarnecki", "Yee Whye Teh", "Razvan Pascanu", "Nicolas Heess"], "TL;DR": "Limiting state information for the default policy can improvement performance, in a KL-regularized RL framework where both agent and default policy are optimized together", "pdf": "/pdf/b702f9a26f2cd46be2d9fed0c8a09c176930e77c.pdf", "paperhash": "galashov|information_asymmetry_in_klregularized_rl", "_bibtex": "@inproceedings{\ngalashov2018information,\ntitle={Information asymmetry in {KL}-regularized {RL}},\nauthor={Alexandre Galashov and Siddhant Jayakumar and Leonard Hasenclever and Dhruva Tirumala and Jonathan Schwarz and Guillaume Desjardins and Wojtek M. Czarnecki and Yee Whye Teh and Razvan Pascanu and Nicolas Heess},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lqMn05Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1293/Official_Review", "cdate": 1542234261846, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1lqMn05Ym", "replyto": "S1lqMn05Ym", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1293/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335915042, "tmdate": 1552335915042, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1293/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Bye57sxdhQ", "original": null, "number": 1, "cdate": 1541045026069, "ddate": null, "tcdate": 1541045026069, "tmdate": 1541533260870, "tddate": null, "forum": "S1lqMn05Ym", "replyto": "S1lqMn05Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper1293/Official_Review", "content": {"title": "Good work in general", "review": "\n-- Originality --\n\nThis paper studies how to use KL-regularization with information asymmetry to speed up and improve reinforcement learning (RL). Compared with existing work, the major novelty in the proposed algorithm is that it uses a default policy learned from data, rather than a fixed default policy. Moreover, the proposed algorithm also limits the amount of information the default policy receives, i.e., there is an \"information asymmetry\" between the agent policy and the default policy. In many applications, the default policy is purposely chosen to be \"goal agnostic\" and hence conducts the \"transfer learning\". To the best of my knowledge, this \"informationally asymmetric\" KL-regularization approach is novel.\n\n-- Clarify --\n\nThe paper is well written in general and is easy to follow.\n\n-- Significance --\n\nI think the idea of regularizing RL via an informationally asymmetric default policy is interesting. It might be an efficient way to do transfer learning (generalization) in some RL applications. This paper has also done extensive and rigorous experiments. Some experiment results are thought-provoking.\n\n-- Pros and Cons\n\nPros:\n\n1)  The idea of regularizing RL via an informationally asymmetric default policy is interesting. To the best of my knowledge, this \"informationally asymmetric\" KL-regularization approach is novel.\n\n2) The experiment results are extensive, rigorous, and thought-provoking.\n\nCons:\n\n1) My understanding is that this \"informationally asymmetric\" KL-regularization approach is a general approach and can be combined with many policy learning algorithms. It is not completely clear to me why the authors choose to combine it with an actor-critic approach (see Algorithm 1)? Why not combine it with other policy learning algorithms? Please explain.\n\n2) This paper does not have any theoretical results. I fully understand that it is highly non-trivial or even impossible to analyze the proposed algorithm in the general case. However, I recommend the authors to analyze (possibly a variant of) the proposed algorithm in a simplified setting (e.g. the network has only one layer, or even is linear) to further strengthen the results.\n\n3) The experiment results of this paper are interesting, but I think the authors can do a better job of intuitively explaining the experiment results. For instance, the experiment results show that when the reward is \"dense shaping\", the proposed method and the baseline perform similarly. Might the authors provide an intuitive explanation for this observation? I recommend the authors to try to provide intuitive explanation for all such interesting observations in the paper. \n\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1293/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Information asymmetry in KL-regularized RL", "abstract": "Many real world tasks exhibit rich structure that is repeated across different parts of the state space or in time. In this work we study the possibility of leveraging such repeated structure to speed up and regularize learning. We start from the KL regularized expected reward objective which introduces an additional component, a default policy. Instead of relying on a fixed default policy, we learn it from data. But crucially, we restrict the amount of information the default policy receives, forcing it to learn reusable behaviors that help the policy learn faster. We formalize this strategy and discuss connections to information bottleneck approaches and to the variational EM algorithm. We present empirical results in both discrete and continuous action domains and demonstrate that, for certain tasks, learning a default policy alongside the policy can significantly speed up and improve learning.\nPlease watch the video demonstrating learned experts and default policies on several continuous control tasks ( https://youtu.be/U2qA3llzus8 ).", "keywords": ["Deep Reinforcement Learning", "Continuous Control", "RL as Inference"], "authorids": ["agalashov@google.com", "sidmj@google.com", "leonardh@google.com", "dhruvat@google.com", "schwarzjn@google.com", "gdesjardins@google.com", "lejlot@google.com", "ywteh@google.com", "razp@google.com", "heess@google.com"], "authors": ["Alexandre Galashov", "Siddhant M. Jayakumar", "Leonard Hasenclever", "Dhruva Tirumala", "Jonathan Schwarz", "Guillaume Desjardins", "Wojciech M. Czarnecki", "Yee Whye Teh", "Razvan Pascanu", "Nicolas Heess"], "TL;DR": "Limiting state information for the default policy can improvement performance, in a KL-regularized RL framework where both agent and default policy are optimized together", "pdf": "/pdf/b702f9a26f2cd46be2d9fed0c8a09c176930e77c.pdf", "paperhash": "galashov|information_asymmetry_in_klregularized_rl", "_bibtex": "@inproceedings{\ngalashov2018information,\ntitle={Information asymmetry in {KL}-regularized {RL}},\nauthor={Alexandre Galashov and Siddhant Jayakumar and Leonard Hasenclever and Dhruva Tirumala and Jonathan Schwarz and Guillaume Desjardins and Wojtek M. Czarnecki and Yee Whye Teh and Razvan Pascanu and Nicolas Heess},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lqMn05Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1293/Official_Review", "cdate": 1542234261846, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1lqMn05Ym", "replyto": "S1lqMn05Ym", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1293/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335915042, "tmdate": 1552335915042, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1293/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 9}