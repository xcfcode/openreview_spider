{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396467659, "tcdate": 1486396467659, "number": 1, "id": "BkhNhMIue", "invitation": "ICLR.cc/2017/conference/-/paper261/acceptance", "forum": "rJq_YBqxx", "replyto": "rJq_YBqxx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "This paper is concurrently one of the first successful attempts to do machine translation using a character-character MT modeling, and generally the authors liked the approach. However, the reviewers raised several issues with the novelty and experimental setup of the work. \n \n Pros:\n - The analysis of the work was strong. This illustrated the underlying property, and all reviewers praised these figures.\n \n Mixed: \n - Some found the paper clear, praising it as a \"well-written paper\", however other found that important details were lacking and the notation was improperly overloaded. As the reviewers were generally experts in the area, this should be improved\n - Reviewers were also split on results. Some found the results quite \"compelling\" and comprehensive, but others thought there should be more comparison to BPE and other morphogically based work\n - Modeling novelty was also questionable. Reviewers like the partial novelty of the character based approach, but felt like the ML contributions were too shallow for ICLR\n \n Cons:\n - Reviewers generally found the model itself to be overly complicated and the paper to focus too much on engineering. \n - There were questions about experimental setup. In particular, a call for more speed numbers and broader comparison."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Character-Level Neural Machine Translation By Learning Morphology", "abstract": "Neural machine translation aims at building a single large neural network that can be trained to maximize translation performance. The encoder-decoder architecture with an attention mechanism achieves a translation performance comparable to the existing state-of-the-art phrase-based systems. However, the use of large vocabulary becomes the bottleneck in both training and improving the performance. In this paper, we propose a novel architecture which learns morphology by using two recurrent networks and a hierarchical decoder which translates at character level. This gives rise to a deep character-level model consisting of six recurrent networks. Such a deep model has two major advantages. It avoids the large vocabulary issue radically; at the same time, it is more efficient in training than word-based models. Our model obtains a higher BLEU score than the bpe-based model after training for one epoch on En-Fr and En-Cs translation tasks. Further analyses show that our model is able to learn morphology.\n\n", "pdf": "/pdf/d4495be4c260cf351432b34cb5232967aeabe5c4.pdf", "TL;DR": "We devise a character-level neural machine translation built on six recurrent networks, and obtain a  BLEU score comparable to the state-of-the-art NMT on En-Fr and Cs-En translation tasks. ", "paperhash": "zhao|deep_characterlevel_neural_machine_translation_by_learning_morphology", "conflicts": ["sjtu.edu.cn", "pku.edu.cn", "zju.edu.cn", "ust.hk"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Shenjian Zhao", "Zhihua Zhang"], "authorids": ["sword.york@gmail.com", "zhzhang@math.pku.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396468181, "id": "ICLR.cc/2017/conference/-/paper261/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "rJq_YBqxx", "replyto": "rJq_YBqxx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396468181}}}, {"tddate": null, "tmdate": 1482372429268, "tcdate": 1481930081288, "number": 2, "id": "BJKwHefNl", "invitation": "ICLR.cc/2017/conference/-/paper261/official/review", "forum": "rJq_YBqxx", "replyto": "rJq_YBqxx", "signatures": ["ICLR.cc/2017/conference/paper261/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper261/AnonReviewer3"], "content": {"title": "Well-executed paper with good analysis but little novelty", "rating": "5: Marginally below acceptance threshold", "review": "Update after reading the authors' responses & the paper revision dated Dec 21:\nI have removed the comment \"insufficient comparison to past work\" in the title & update the score from 3 -> 5.\nThe main reason for the score is on novelty. The proposal of HGRU & the use of the R matrix are basically just to achieve the effect of \"whether to continue from character-level states or using word-level states\". It seems that these solutions are specific to symbolic frameworks like Theano (which the authors used) and TensorFlow. This, however, is not a problem for languages like Matlab (which Luong & Manning used) or Torch.\n\n-----\n\nThis is a well-written paper with good analysis in which I especially like Figure 5. However I think there is little novelty in this work. The title is about learning morphology but there is nothing specifically enforced in the model to learn morphemes or subword units. For example, maybe some constraints can be put on the weights in w_i in Figure 1 to detect morpheme boundaries or some additional objective like MDL can be used (though it's not clear how these constraints can be incorporated cleanly). \n\nMoreover, I'm very surprised that litte comparison (only a brief mention) was given to the work of (Luong & Manning, 2016) [1], which trains deep 8-layer word-character models and achieves much better results on English-Czech, e.g., 19.6 BLEU compared to 17.0 BLEU achieved in the paper. I think the HGRU thing is over-complicated in terms of presentation. If I read correctly, what HGRU does is basically either continue the character decoder or reset using word-level states at boundaries, which is what was done in [1]. Luong & Manning (2016) even make it more efficient by not having to decode all target words at the morpheme level & it would be good to know the speed of the model proposed in this ICLR submission. What end up new in this paper are perhaps different analyses on what a character-based model learns & adding an additional RNN layer in the encoder.\n\nOne minor comment: annotate h_t in Figure 1.\n\n[1] Minh-Thang Luong and Christopher D. Manning. 2016. Achieving Open Vocabulary Neural Machine Translation\nwith Hybrid Word-Character Models. ACL. https://arxiv.org/pdf/1604.00788v2.pdf", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Character-Level Neural Machine Translation By Learning Morphology", "abstract": "Neural machine translation aims at building a single large neural network that can be trained to maximize translation performance. The encoder-decoder architecture with an attention mechanism achieves a translation performance comparable to the existing state-of-the-art phrase-based systems. However, the use of large vocabulary becomes the bottleneck in both training and improving the performance. In this paper, we propose a novel architecture which learns morphology by using two recurrent networks and a hierarchical decoder which translates at character level. This gives rise to a deep character-level model consisting of six recurrent networks. Such a deep model has two major advantages. It avoids the large vocabulary issue radically; at the same time, it is more efficient in training than word-based models. Our model obtains a higher BLEU score than the bpe-based model after training for one epoch on En-Fr and En-Cs translation tasks. Further analyses show that our model is able to learn morphology.\n\n", "pdf": "/pdf/d4495be4c260cf351432b34cb5232967aeabe5c4.pdf", "TL;DR": "We devise a character-level neural machine translation built on six recurrent networks, and obtain a  BLEU score comparable to the state-of-the-art NMT on En-Fr and Cs-En translation tasks. ", "paperhash": "zhao|deep_characterlevel_neural_machine_translation_by_learning_morphology", "conflicts": ["sjtu.edu.cn", "pku.edu.cn", "zju.edu.cn", "ust.hk"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Shenjian Zhao", "Zhihua Zhang"], "authorids": ["sword.york@gmail.com", "zhzhang@math.pku.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512644406, "id": "ICLR.cc/2017/conference/-/paper261/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper261/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper261/AnonReviewer2", "ICLR.cc/2017/conference/paper261/AnonReviewer3", "ICLR.cc/2017/conference/paper261/AnonReviewer1"], "reply": {"forum": "rJq_YBqxx", "replyto": "rJq_YBqxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper261/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper261/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512644406}}}, {"tddate": null, "tmdate": 1482367027620, "tcdate": 1482365981299, "number": 11, "id": "BkSQh9u4e", "invitation": "ICLR.cc/2017/conference/-/paper261/public/comment", "forum": "rJq_YBqxx", "replyto": "rJJHggGNx", "signatures": ["~Shenjian_Zhao1"], "readers": ["everyone"], "writers": ["~Shenjian_Zhao1"], "content": {"title": "Appendix", "comment": "Dear reviewer,\n\nWe have added an appendix that described the model in detail. If it is still unclear, please let us know.\n\nThanks."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Character-Level Neural Machine Translation By Learning Morphology", "abstract": "Neural machine translation aims at building a single large neural network that can be trained to maximize translation performance. The encoder-decoder architecture with an attention mechanism achieves a translation performance comparable to the existing state-of-the-art phrase-based systems. However, the use of large vocabulary becomes the bottleneck in both training and improving the performance. In this paper, we propose a novel architecture which learns morphology by using two recurrent networks and a hierarchical decoder which translates at character level. This gives rise to a deep character-level model consisting of six recurrent networks. Such a deep model has two major advantages. It avoids the large vocabulary issue radically; at the same time, it is more efficient in training than word-based models. Our model obtains a higher BLEU score than the bpe-based model after training for one epoch on En-Fr and En-Cs translation tasks. Further analyses show that our model is able to learn morphology.\n\n", "pdf": "/pdf/d4495be4c260cf351432b34cb5232967aeabe5c4.pdf", "TL;DR": "We devise a character-level neural machine translation built on six recurrent networks, and obtain a  BLEU score comparable to the state-of-the-art NMT on En-Fr and Cs-En translation tasks. ", "paperhash": "zhao|deep_characterlevel_neural_machine_translation_by_learning_morphology", "conflicts": ["sjtu.edu.cn", "pku.edu.cn", "zju.edu.cn", "ust.hk"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Shenjian Zhao", "Zhihua Zhang"], "authorids": ["sword.york@gmail.com", "zhzhang@math.pku.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287660517, "id": "ICLR.cc/2017/conference/-/paper261/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJq_YBqxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper261/reviewers", "ICLR.cc/2017/conference/paper261/areachairs"], "cdate": 1485287660517}}}, {"tddate": null, "tmdate": 1482366821600, "tcdate": 1482365733395, "number": 9, "id": "ryp7jq_4g", "invitation": "ICLR.cc/2017/conference/-/paper261/public/comment", "forum": "rJq_YBqxx", "replyto": "H1aZfRINx", "signatures": ["~Shenjian_Zhao1"], "readers": ["everyone"], "writers": ["~Shenjian_Zhao1"], "content": {"title": "Responses", "comment": "Thank you for your insightful comments.\n \nWe have added the size of models in Table 1. We could find that our model is the smallest one to achieve the comparable performance. We also added more translation samples in the appendix. Thanks for your suggestion.\n \nBelow are the one-to-one responses to the comments.\n \nComment 1: Mainly an architecture engineering/application paper, not much novelty.\nResponse: We combine 6 RNNs to build this network. We think it is important to build from the basic components for new functionality in deep learning. The famous networks such as ResNet and GoogLeNet are also built from several existing components.\n \nComment 2: The idea of using hierarchical decoders.\nResponse:  It's true that the idea of using hierarchical decoders have been explored before and we have cited the paper [4] in the revision.  Various hierarchical decoders operate on different level. For example, the hierarchical decoder in [4] decodes from sentence level to word level. However, none of the prior hierarchical decoders are applicable to NMT because of the training inefficiency. For example, Ling et al, [2] and Luong et al, [3] have tried the hierarchical decoder, but both are not efficient. The model of Luong et al, [3] needs to take 3 months to train the purely character models (see, Table 1). To our knowledge, our method is the first to make the hierarchical decoder work on NMT.\n \nComment 3: Learning words representation.\nResponse: We would like to highlight that another major novelty of our work is learning words representation that an ordinary RNN failed to learn (as shown in Figure 3 and Figure 5). It is not only useful in NMT, but also potentially useful in other NLP areas like language models.\n \nComment 4: The model is complicated.\nResponse: We admit that it is a little complicated to implement this network using existent deep learning frameworks, thus we make our codes available online.  However, in terms of the model size, our model is the simplest and smallest (we have added the size of models in the revision, thanks for your suggestion.). More importantly, we think our system is less complicated compared to [1] which uses thousands of filters (their model is much larger than ours). It's more natural and straightforward to using RNNs in this context (we describe the detailed architecture in Appendix). Because of using RNNs, our model is less redundant. \n \nComment 5: The proposed model is potentially slower.\nResponse: Our model is slower than the word-base models. However, the training efficiency of our model is comparable to Lee et al, [1] which is also a character-based model (see, Table 1). Moreover, the character-based models avoid the large vocabulary and the large softmax function at the cost of the training speed.\n \nIf you have any further comments, please let us know. Thanks.\n \n \n[1] Lee, Jason, Kyunghyun Cho, and Thomas Hofmann. \"Fully Character-Level Neural Machine Translation without Explicit Segmentation.\" \n[2] Ling, Wang, et al. \"Character-based Neural Machine Translation.\" \n[3] Minh-Thang Luong and Christopher D. Manning. 2016. \"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models.\"\n[4] Serban IV, Sordoni A, Bengio Y, Courville A, Pineau J. Hierarchical neural network generative models for movie dialogues. arXiv preprint arXiv:1507.04808. 2015 Jul 17."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Character-Level Neural Machine Translation By Learning Morphology", "abstract": "Neural machine translation aims at building a single large neural network that can be trained to maximize translation performance. The encoder-decoder architecture with an attention mechanism achieves a translation performance comparable to the existing state-of-the-art phrase-based systems. However, the use of large vocabulary becomes the bottleneck in both training and improving the performance. In this paper, we propose a novel architecture which learns morphology by using two recurrent networks and a hierarchical decoder which translates at character level. This gives rise to a deep character-level model consisting of six recurrent networks. Such a deep model has two major advantages. It avoids the large vocabulary issue radically; at the same time, it is more efficient in training than word-based models. Our model obtains a higher BLEU score than the bpe-based model after training for one epoch on En-Fr and En-Cs translation tasks. Further analyses show that our model is able to learn morphology.\n\n", "pdf": "/pdf/d4495be4c260cf351432b34cb5232967aeabe5c4.pdf", "TL;DR": "We devise a character-level neural machine translation built on six recurrent networks, and obtain a  BLEU score comparable to the state-of-the-art NMT on En-Fr and Cs-En translation tasks. ", "paperhash": "zhao|deep_characterlevel_neural_machine_translation_by_learning_morphology", "conflicts": ["sjtu.edu.cn", "pku.edu.cn", "zju.edu.cn", "ust.hk"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Shenjian Zhao", "Zhihua Zhang"], "authorids": ["sword.york@gmail.com", "zhzhang@math.pku.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287660517, "id": "ICLR.cc/2017/conference/-/paper261/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJq_YBqxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper261/reviewers", "ICLR.cc/2017/conference/paper261/areachairs"], "cdate": 1485287660517}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1482366455535, "tcdate": 1478281586541, "number": 261, "id": "rJq_YBqxx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "rJq_YBqxx", "signatures": ["~Shenjian_Zhao1"], "readers": ["everyone"], "content": {"title": "Deep Character-Level Neural Machine Translation By Learning Morphology", "abstract": "Neural machine translation aims at building a single large neural network that can be trained to maximize translation performance. The encoder-decoder architecture with an attention mechanism achieves a translation performance comparable to the existing state-of-the-art phrase-based systems. However, the use of large vocabulary becomes the bottleneck in both training and improving the performance. In this paper, we propose a novel architecture which learns morphology by using two recurrent networks and a hierarchical decoder which translates at character level. This gives rise to a deep character-level model consisting of six recurrent networks. Such a deep model has two major advantages. It avoids the large vocabulary issue radically; at the same time, it is more efficient in training than word-based models. Our model obtains a higher BLEU score than the bpe-based model after training for one epoch on En-Fr and En-Cs translation tasks. Further analyses show that our model is able to learn morphology.\n\n", "pdf": "/pdf/d4495be4c260cf351432b34cb5232967aeabe5c4.pdf", "TL;DR": "We devise a character-level neural machine translation built on six recurrent networks, and obtain a  BLEU score comparable to the state-of-the-art NMT on En-Fr and Cs-En translation tasks. ", "paperhash": "zhao|deep_characterlevel_neural_machine_translation_by_learning_morphology", "conflicts": ["sjtu.edu.cn", "pku.edu.cn", "zju.edu.cn", "ust.hk"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Shenjian Zhao", "Zhihua Zhang"], "authorids": ["sword.york@gmail.com", "zhzhang@math.pku.edu.cn"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 19, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1482365919857, "tcdate": 1482365919857, "number": 10, "id": "BJuknqdVl", "invitation": "ICLR.cc/2017/conference/-/paper261/public/comment", "forum": "rJq_YBqxx", "replyto": "HkTDuirNe", "signatures": ["~Shenjian_Zhao1"], "readers": ["everyone"], "writers": ["~Shenjian_Zhao1"], "content": {"title": "The size of models", "comment": "Dear reviewer,\n\nWe have added the comparison of the model size to Table 1 in the revision. From Table 1, we could find that Luong's model is nearly five times as large as ours, thus it is not surprised that their model outperforms the others.\nThanks for your time and effort on our paper.\n\nThanks."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Character-Level Neural Machine Translation By Learning Morphology", "abstract": "Neural machine translation aims at building a single large neural network that can be trained to maximize translation performance. The encoder-decoder architecture with an attention mechanism achieves a translation performance comparable to the existing state-of-the-art phrase-based systems. However, the use of large vocabulary becomes the bottleneck in both training and improving the performance. In this paper, we propose a novel architecture which learns morphology by using two recurrent networks and a hierarchical decoder which translates at character level. This gives rise to a deep character-level model consisting of six recurrent networks. Such a deep model has two major advantages. It avoids the large vocabulary issue radically; at the same time, it is more efficient in training than word-based models. Our model obtains a higher BLEU score than the bpe-based model after training for one epoch on En-Fr and En-Cs translation tasks. Further analyses show that our model is able to learn morphology.\n\n", "pdf": "/pdf/d4495be4c260cf351432b34cb5232967aeabe5c4.pdf", "TL;DR": "We devise a character-level neural machine translation built on six recurrent networks, and obtain a  BLEU score comparable to the state-of-the-art NMT on En-Fr and Cs-En translation tasks. ", "paperhash": "zhao|deep_characterlevel_neural_machine_translation_by_learning_morphology", "conflicts": ["sjtu.edu.cn", "pku.edu.cn", "zju.edu.cn", "ust.hk"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Shenjian Zhao", "Zhihua Zhang"], "authorids": ["sword.york@gmail.com", "zhzhang@math.pku.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287660517, "id": "ICLR.cc/2017/conference/-/paper261/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJq_YBqxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper261/reviewers", "ICLR.cc/2017/conference/paper261/areachairs"], "cdate": 1485287660517}}}, {"tddate": null, "tmdate": 1482365499187, "tcdate": 1482365499187, "number": 8, "id": "SyXrqcONx", "invitation": "ICLR.cc/2017/conference/-/paper261/public/comment", "forum": "rJq_YBqxx", "replyto": "rJq_YBqxx", "signatures": ["~Shenjian_Zhao1"], "readers": ["everyone"], "writers": ["~Shenjian_Zhao1"], "content": {"title": "Adding an appendix and model size comparison", "comment": "Dear reviewers,\n\nWe uploaded an updated version with an appendix.  In the appendix, we describe our model in detail and add more translation samples. \nWe have added the size of models in Table 1. Table 1 becomes more comprehensive, thanks for your insightful suggestions.\n\nThanks.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Character-Level Neural Machine Translation By Learning Morphology", "abstract": "Neural machine translation aims at building a single large neural network that can be trained to maximize translation performance. The encoder-decoder architecture with an attention mechanism achieves a translation performance comparable to the existing state-of-the-art phrase-based systems. However, the use of large vocabulary becomes the bottleneck in both training and improving the performance. In this paper, we propose a novel architecture which learns morphology by using two recurrent networks and a hierarchical decoder which translates at character level. This gives rise to a deep character-level model consisting of six recurrent networks. Such a deep model has two major advantages. It avoids the large vocabulary issue radically; at the same time, it is more efficient in training than word-based models. Our model obtains a higher BLEU score than the bpe-based model after training for one epoch on En-Fr and En-Cs translation tasks. Further analyses show that our model is able to learn morphology.\n\n", "pdf": "/pdf/d4495be4c260cf351432b34cb5232967aeabe5c4.pdf", "TL;DR": "We devise a character-level neural machine translation built on six recurrent networks, and obtain a  BLEU score comparable to the state-of-the-art NMT on En-Fr and Cs-En translation tasks. ", "paperhash": "zhao|deep_characterlevel_neural_machine_translation_by_learning_morphology", "conflicts": ["sjtu.edu.cn", "pku.edu.cn", "zju.edu.cn", "ust.hk"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Shenjian Zhao", "Zhihua Zhang"], "authorids": ["sword.york@gmail.com", "zhzhang@math.pku.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287660517, "id": "ICLR.cc/2017/conference/-/paper261/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJq_YBqxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper261/reviewers", "ICLR.cc/2017/conference/paper261/areachairs"], "cdate": 1485287660517}}}, {"tddate": null, "tmdate": 1482248789924, "tcdate": 1482248709317, "number": 3, "id": "H1aZfRINx", "invitation": "ICLR.cc/2017/conference/-/paper261/official/review", "forum": "rJq_YBqxx", "replyto": "rJq_YBqxx", "signatures": ["ICLR.cc/2017/conference/paper261/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper261/AnonReviewer1"], "content": {"title": "A well written paper", "rating": "6: Marginally above acceptance threshold", "review": "\n* Summary: This paper proposes a neural machine translation model that translates the source and the target texts in an end to end manner from characters to characters. The model can learn morphology in the encoder and in the decoder the authors use a hierarchical decoder. Authors provide very compelling results on various bilingual corpora for different language pairs. The paper is well-written, the results are competitive compared to other baselines in the literature.\n\n\n* Review:\n     - I think the paper is very well written, I like the analysis presented in this paper. It is clean and precise. \n     - The idea of using hierarchical decoders have been explored before, e.g. [1]. Can you cite those papers?\n     - This paper is mainly an application paper and it is mainly the application of several existing components on the character-level NMT tasks. In this sense, it is good that authors made their codes available online. However, the contributions from the general ML point of view is still limited.\n                   \n* Some Requests:\n -Can you add the size of the models to the Table 1? \n- Can you add some of the failure cases of your model, where the model failed to translate correctly?\n\n* An Overview of the Review:\n\nPros:\n    - The paper is well written\n    - Extensive analysis of the model on various language pairs\n    - Convincing experimental results.    \n    \nCons:\n    - The model is complicated.\n    - Mainly an architecture engineering/application paper(bringing together various well-known techniques), not much novelty.\n    - The proposed model is potentially slower than the regular models since it needs to operate over the characters instead of the words and uses several RNNs.\n\n[1] Serban IV, Sordoni A, Bengio Y, Courville A, Pineau J. Hierarchical neural network generative models for movie dialogues. arXiv preprint arXiv:1507.04808. 2015 Jul 17.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Character-Level Neural Machine Translation By Learning Morphology", "abstract": "Neural machine translation aims at building a single large neural network that can be trained to maximize translation performance. The encoder-decoder architecture with an attention mechanism achieves a translation performance comparable to the existing state-of-the-art phrase-based systems. However, the use of large vocabulary becomes the bottleneck in both training and improving the performance. In this paper, we propose a novel architecture which learns morphology by using two recurrent networks and a hierarchical decoder which translates at character level. This gives rise to a deep character-level model consisting of six recurrent networks. Such a deep model has two major advantages. It avoids the large vocabulary issue radically; at the same time, it is more efficient in training than word-based models. Our model obtains a higher BLEU score than the bpe-based model after training for one epoch on En-Fr and En-Cs translation tasks. Further analyses show that our model is able to learn morphology.\n\n", "pdf": "/pdf/d4495be4c260cf351432b34cb5232967aeabe5c4.pdf", "TL;DR": "We devise a character-level neural machine translation built on six recurrent networks, and obtain a  BLEU score comparable to the state-of-the-art NMT on En-Fr and Cs-En translation tasks. ", "paperhash": "zhao|deep_characterlevel_neural_machine_translation_by_learning_morphology", "conflicts": ["sjtu.edu.cn", "pku.edu.cn", "zju.edu.cn", "ust.hk"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Shenjian Zhao", "Zhihua Zhang"], "authorids": ["sword.york@gmail.com", "zhzhang@math.pku.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512644406, "id": "ICLR.cc/2017/conference/-/paper261/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper261/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper261/AnonReviewer2", "ICLR.cc/2017/conference/paper261/AnonReviewer3", "ICLR.cc/2017/conference/paper261/AnonReviewer1"], "reply": {"forum": "rJq_YBqxx", "replyto": "rJq_YBqxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper261/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper261/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512644406}}}, {"tddate": null, "tmdate": 1482172517254, "tcdate": 1482172517254, "number": 3, "id": "HkTDuirNe", "invitation": "ICLR.cc/2017/conference/-/paper261/official/comment", "forum": "rJq_YBqxx", "replyto": "By7sjAmNl", "signatures": ["ICLR.cc/2017/conference/paper261/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper261/AnonReviewer3"], "content": {"title": "Will reevaluate", "comment": "Thanks for your comments and paper revisions. I'll take a look later and reevaluate the paper again."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Character-Level Neural Machine Translation By Learning Morphology", "abstract": "Neural machine translation aims at building a single large neural network that can be trained to maximize translation performance. The encoder-decoder architecture with an attention mechanism achieves a translation performance comparable to the existing state-of-the-art phrase-based systems. However, the use of large vocabulary becomes the bottleneck in both training and improving the performance. In this paper, we propose a novel architecture which learns morphology by using two recurrent networks and a hierarchical decoder which translates at character level. This gives rise to a deep character-level model consisting of six recurrent networks. Such a deep model has two major advantages. It avoids the large vocabulary issue radically; at the same time, it is more efficient in training than word-based models. Our model obtains a higher BLEU score than the bpe-based model after training for one epoch on En-Fr and En-Cs translation tasks. Further analyses show that our model is able to learn morphology.\n\n", "pdf": "/pdf/d4495be4c260cf351432b34cb5232967aeabe5c4.pdf", "TL;DR": "We devise a character-level neural machine translation built on six recurrent networks, and obtain a  BLEU score comparable to the state-of-the-art NMT on En-Fr and Cs-En translation tasks. ", "paperhash": "zhao|deep_characterlevel_neural_machine_translation_by_learning_morphology", "conflicts": ["sjtu.edu.cn", "pku.edu.cn", "zju.edu.cn", "ust.hk"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Shenjian Zhao", "Zhihua Zhang"], "authorids": ["sword.york@gmail.com", "zhzhang@math.pku.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287659975, "id": "ICLR.cc/2017/conference/-/paper261/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "rJq_YBqxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper261/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper261/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper261/reviewers", "ICLR.cc/2017/conference/paper261/areachairs"], "cdate": 1485287659975}}}, {"tddate": null, "tmdate": 1482066117800, "tcdate": 1482054554780, "number": 6, "id": "By7sjAmNl", "invitation": "ICLR.cc/2017/conference/-/paper261/public/comment", "forum": "rJq_YBqxx", "replyto": "Sk9ap8m4l", "signatures": ["~Shenjian_Zhao1"], "readers": ["everyone"], "writers": ["~Shenjian_Zhao1"], "content": {"title": "Add comparison with Luong & Manning, 2016 and efficiency of the hierarchical decoder.", "comment": "Thank you for your patience and insightful comments.\nBelow are the one-to-one responses to the comments.\n \nComment 1: Comparison with Luong & Manning, 2016 [1]. \nResponse: We have added the comparison with Luong & Manning, 2016 [1] in Table 1 and Section 3.2, to help readers more clearly understand the difference and efficiency of the hierarchical decoder in our model.\n \nComment 2: Implementing the behavior of HGRU.\nResponse: When training in a batch manner, different sentences have different word boundaries. For example, when the target sequence batch contains the following sentences:\n      \" Hello world ! \n        I am fine . \"\nthe number of words and boundaries are different for these two sentences. At the framework-level, it will be intractable to conditionally pick outputs from the first-level decoder when training in batch manner. As far as we know, it is impossible for Theano and other symbolic deep learning frameworks, because we need to build the symbolic expression before training. The second-level decoder doesn't know when to reset using word-level states at boundaries for each sentences during training. Thus, it is tricky. Luong & Manning, 2016 [1] uses two forward passes (one for word-level and another for character-level) in batch training, which is less efficient (we have checked their code [https://github.com/lmthang/nmt.hybrid/blob/master/code/lstmCostGrad.m] and Section 4.3 in their paper [https://arxiv.org/pdf/1604.00788v1.pdf]). However, in our model, we use a matrix R to unfold the outputs of the first-level decoder and utilize the auxiliary sequence to build the symbolic expression. It makes the batch training process more efficient (single forward pass as usual).  We think it is the main reason why our model is much faster than their purely character model. \n \nBesides, the vector used to seed second-level decoder in our model (see, Eqn. (8) in the revision) is very different from that in Luong & Manning, 2016 [1] (see, Eqn. (6) in their paper). The source word encoder in our model (see, Figure 1) is completely different from that in Luong & Manning, 2016 [1] (see, Figure 1 in their paper). \n \nWe have clarified the \"decoder\" when describing HGRU in the revision. Thanks for your suggestion.\n \nIf you have any further comments, please let us know. Thanks.\n\n[1] Minh-Thang Luong and Christopher D. Manning. 2016. Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models. ACL. https://arxiv.org/pdf/1604.00788v2.pdf"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Character-Level Neural Machine Translation By Learning Morphology", "abstract": "Neural machine translation aims at building a single large neural network that can be trained to maximize translation performance. The encoder-decoder architecture with an attention mechanism achieves a translation performance comparable to the existing state-of-the-art phrase-based systems. However, the use of large vocabulary becomes the bottleneck in both training and improving the performance. In this paper, we propose a novel architecture which learns morphology by using two recurrent networks and a hierarchical decoder which translates at character level. This gives rise to a deep character-level model consisting of six recurrent networks. Such a deep model has two major advantages. It avoids the large vocabulary issue radically; at the same time, it is more efficient in training than word-based models. Our model obtains a higher BLEU score than the bpe-based model after training for one epoch on En-Fr and En-Cs translation tasks. Further analyses show that our model is able to learn morphology.\n\n", "pdf": "/pdf/d4495be4c260cf351432b34cb5232967aeabe5c4.pdf", "TL;DR": "We devise a character-level neural machine translation built on six recurrent networks, and obtain a  BLEU score comparable to the state-of-the-art NMT on En-Fr and Cs-En translation tasks. ", "paperhash": "zhao|deep_characterlevel_neural_machine_translation_by_learning_morphology", "conflicts": ["sjtu.edu.cn", "pku.edu.cn", "zju.edu.cn", "ust.hk"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Shenjian Zhao", "Zhihua Zhang"], "authorids": ["sword.york@gmail.com", "zhzhang@math.pku.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287660517, "id": "ICLR.cc/2017/conference/-/paper261/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJq_YBqxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper261/reviewers", "ICLR.cc/2017/conference/paper261/areachairs"], "cdate": 1485287660517}}}, {"tddate": null, "tmdate": 1482055160612, "tcdate": 1482055160612, "number": 7, "id": "SkZ-CC74l", "invitation": "ICLR.cc/2017/conference/-/paper261/public/comment", "forum": "rJq_YBqxx", "replyto": "rJq_YBqxx", "signatures": ["~Shenjian_Zhao1"], "readers": ["everyone"], "writers": ["~Shenjian_Zhao1"], "content": {"title": "Adding several comparisons", "comment": "Dear reviewers,\n\nWe uploaded a slightly modified version. \nWe have further explained the novelty of the hierarchical decoder in Section 3.2.\nWe have added the comparison with Luong & Manning, 2016 [1] and the trivial baseline (CNMT) to Table 1.\nThe trivial baseline is the old version of this submission which takes the last hidden state of RNN as the representation of the source word. You could find it on arxiv (https://arxiv.org/pdf/1608.04738v2.pdf).\n\nThanks!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Character-Level Neural Machine Translation By Learning Morphology", "abstract": "Neural machine translation aims at building a single large neural network that can be trained to maximize translation performance. The encoder-decoder architecture with an attention mechanism achieves a translation performance comparable to the existing state-of-the-art phrase-based systems. However, the use of large vocabulary becomes the bottleneck in both training and improving the performance. In this paper, we propose a novel architecture which learns morphology by using two recurrent networks and a hierarchical decoder which translates at character level. This gives rise to a deep character-level model consisting of six recurrent networks. Such a deep model has two major advantages. It avoids the large vocabulary issue radically; at the same time, it is more efficient in training than word-based models. Our model obtains a higher BLEU score than the bpe-based model after training for one epoch on En-Fr and En-Cs translation tasks. Further analyses show that our model is able to learn morphology.\n\n", "pdf": "/pdf/d4495be4c260cf351432b34cb5232967aeabe5c4.pdf", "TL;DR": "We devise a character-level neural machine translation built on six recurrent networks, and obtain a  BLEU score comparable to the state-of-the-art NMT on En-Fr and Cs-En translation tasks. ", "paperhash": "zhao|deep_characterlevel_neural_machine_translation_by_learning_morphology", "conflicts": ["sjtu.edu.cn", "pku.edu.cn", "zju.edu.cn", "ust.hk"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Shenjian Zhao", "Zhihua Zhang"], "authorids": ["sword.york@gmail.com", "zhzhang@math.pku.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287660517, "id": "ICLR.cc/2017/conference/-/paper261/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJq_YBqxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper261/reviewers", "ICLR.cc/2017/conference/paper261/areachairs"], "cdate": 1485287660517}}}, {"tddate": null, "tmdate": 1482022465287, "tcdate": 1482022338274, "number": 2, "id": "Sk9ap8m4l", "invitation": "ICLR.cc/2017/conference/-/paper261/official/comment", "forum": "rJq_YBqxx", "replyto": "S1dwLRzVe", "signatures": ["ICLR.cc/2017/conference/paper261/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper261/AnonReviewer3"], "content": {"title": "Further review", "comment": "Thanks for your responses.\n\nMy point in referring to the work of (Luong & Manning, 2016) is to point out that their work shares some similarity with your work, e.g., having two levels of decoders & testing on WMT'15 En-cs. Adding comparison to their system doesn't bring down the value of your work; it helps readers understand more the difference and you can in fact highlight why your models are more efficient given that your revised version now includes training time. Btw, I checked Luong & Manning's paper, the hybrid system with 19.6 BLEU only takes 3 weeks to train though you're right that their purely character models took 3 months.\n\nI have tried to reread the description of HGRU below Eq. (7) but am having some difficulty in understanding the text. There are two decoders and it wasn't clear which one these phrases, \"outputs of decoder\", \"outputs from the decoder\", \"outputs of the decoder\"are referring to. It would be beneficial to revise that paragraph to make things easier to understand. Even so, at the high-level, I can't see why it is tricky to implement the following behavior of HGRU \"either continue the character decoder or reset using word-level states at boundaries\": once you have chosen which states (word or character) are used for different examples in a mini-batch as input, batch training with matrix-matrix multiplication should be possible?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Character-Level Neural Machine Translation By Learning Morphology", "abstract": "Neural machine translation aims at building a single large neural network that can be trained to maximize translation performance. The encoder-decoder architecture with an attention mechanism achieves a translation performance comparable to the existing state-of-the-art phrase-based systems. However, the use of large vocabulary becomes the bottleneck in both training and improving the performance. In this paper, we propose a novel architecture which learns morphology by using two recurrent networks and a hierarchical decoder which translates at character level. This gives rise to a deep character-level model consisting of six recurrent networks. Such a deep model has two major advantages. It avoids the large vocabulary issue radically; at the same time, it is more efficient in training than word-based models. Our model obtains a higher BLEU score than the bpe-based model after training for one epoch on En-Fr and En-Cs translation tasks. Further analyses show that our model is able to learn morphology.\n\n", "pdf": "/pdf/d4495be4c260cf351432b34cb5232967aeabe5c4.pdf", "TL;DR": "We devise a character-level neural machine translation built on six recurrent networks, and obtain a  BLEU score comparable to the state-of-the-art NMT on En-Fr and Cs-En translation tasks. ", "paperhash": "zhao|deep_characterlevel_neural_machine_translation_by_learning_morphology", "conflicts": ["sjtu.edu.cn", "pku.edu.cn", "zju.edu.cn", "ust.hk"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Shenjian Zhao", "Zhihua Zhang"], "authorids": ["sword.york@gmail.com", "zhzhang@math.pku.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287659975, "id": "ICLR.cc/2017/conference/-/paper261/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "rJq_YBqxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper261/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper261/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper261/reviewers", "ICLR.cc/2017/conference/paper261/areachairs"], "cdate": 1485287659975}}}, {"tddate": null, "tmdate": 1481988525035, "tcdate": 1481987679692, "number": 4, "id": "S1dwLRzVe", "invitation": "ICLR.cc/2017/conference/-/paper261/public/comment", "forum": "rJq_YBqxx", "replyto": "BJKwHefNl", "signatures": ["~Shenjian_Zhao1"], "readers": ["everyone"], "writers": ["~Shenjian_Zhao1"], "content": {"title": "Comparison with Luong & Manning, 2016 and novelty.", "comment": "Thanks for your comments!  \nBelow are the one-to-one responses to the comments.\n\nComment 1: Comparison with Luong & Manning, 2016 [1]. \nResponse: We think it is unfair to compare with such a deep model, neither Lee et al, 2016 [2] nor Chung et al 2016 [3] compared with Luong & Manning, 2016 [1]. Besides, our model is much efficient than [1], because their model is trained for 3 months (you could check the training time in their paper). However, our En-Cs model is trained for only 15 days on a single NVIDIA TITAN X GPU. We have added the detailed training time in the revision. Thanks for your suggestion!\n\nComment 2: The HGRU thing is over-complicated in terms of presentation.\nResponse: It is correct that HGRU is either continue the character decoder or reset using word-level states at boundaries. However, it is tricky when implementing. We have presented the trick (by using an auxiliary sequence and the matrix R) of our implementation in the submission. Besides, it is not only more elegant but also more efficient (trained for only 15 days) to decode all target words at the character level with HGRU. Please refer to the revision for the training time in Table 1. \n\nComment 3: Novelty and specific constraints.\nResponse: We think another major novelty of our work is the word encoder. As you can see, the attention mechanism also does not specifically enforce the model to align on one word, the alignment is learnt from data.  Similarly, the morphemes or subword units are learnt from data. Actually, it does provide the nice embedding (Figure 3) and segmentation (Figure 5).  We have tried several constraints like sparsity, but less useful. It is better to model the words as an energy model (like the attention mechanism) which is used in the submission. \n\nComment 4: Annotate h_t in Figure 1.\nResponse: The h_t is the hidden state of RNN which is omitted in Figure 1. To clarify we have added it to Figure 1 in the revision.\n\nThanks!\n\n\n[1] Minh-Thang Luong and Christopher D. Manning. 2016. Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models. ACL. https://arxiv.org/pdf/1604.00788v2.pdf\n[2] Lee, Jason, Kyunghyun Cho, and Thomas Hofmann. \"Fully Character-Level Neural Machine Translation without Explicit Segmentation.\" arXiv preprint arXiv:1610.03017 (2016).\n[3] Chung, Junyoung, Kyunghyun Cho, and Yoshua Bengio. \"A character-level decoder without explicit segmentation for neural machine translation.\" arXiv preprint arXiv:1603.06147 (2016)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Character-Level Neural Machine Translation By Learning Morphology", "abstract": "Neural machine translation aims at building a single large neural network that can be trained to maximize translation performance. The encoder-decoder architecture with an attention mechanism achieves a translation performance comparable to the existing state-of-the-art phrase-based systems. However, the use of large vocabulary becomes the bottleneck in both training and improving the performance. In this paper, we propose a novel architecture which learns morphology by using two recurrent networks and a hierarchical decoder which translates at character level. This gives rise to a deep character-level model consisting of six recurrent networks. Such a deep model has two major advantages. It avoids the large vocabulary issue radically; at the same time, it is more efficient in training than word-based models. Our model obtains a higher BLEU score than the bpe-based model after training for one epoch on En-Fr and En-Cs translation tasks. Further analyses show that our model is able to learn morphology.\n\n", "pdf": "/pdf/d4495be4c260cf351432b34cb5232967aeabe5c4.pdf", "TL;DR": "We devise a character-level neural machine translation built on six recurrent networks, and obtain a  BLEU score comparable to the state-of-the-art NMT on En-Fr and Cs-En translation tasks. ", "paperhash": "zhao|deep_characterlevel_neural_machine_translation_by_learning_morphology", "conflicts": ["sjtu.edu.cn", "pku.edu.cn", "zju.edu.cn", "ust.hk"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Shenjian Zhao", "Zhihua Zhang"], "authorids": ["sword.york@gmail.com", "zhzhang@math.pku.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287660517, "id": "ICLR.cc/2017/conference/-/paper261/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJq_YBqxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper261/reviewers", "ICLR.cc/2017/conference/paper261/areachairs"], "cdate": 1485287660517}}}, {"tddate": null, "tmdate": 1481988268051, "tcdate": 1481988268051, "number": 5, "id": "Sy4n_0MNg", "invitation": "ICLR.cc/2017/conference/-/paper261/public/comment", "forum": "rJq_YBqxx", "replyto": "rJq_YBqxx", "signatures": ["~Shenjian_Zhao1"], "readers": ["everyone"], "writers": ["~Shenjian_Zhao1"], "content": {"title": "Training time", "comment": "Dear reviewers,\n\nWe uploaded a slightly modified version. We have added the training time for each model to Table 1 and clarified some notations.\n\nThanks!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Character-Level Neural Machine Translation By Learning Morphology", "abstract": "Neural machine translation aims at building a single large neural network that can be trained to maximize translation performance. The encoder-decoder architecture with an attention mechanism achieves a translation performance comparable to the existing state-of-the-art phrase-based systems. However, the use of large vocabulary becomes the bottleneck in both training and improving the performance. In this paper, we propose a novel architecture which learns morphology by using two recurrent networks and a hierarchical decoder which translates at character level. This gives rise to a deep character-level model consisting of six recurrent networks. Such a deep model has two major advantages. It avoids the large vocabulary issue radically; at the same time, it is more efficient in training than word-based models. Our model obtains a higher BLEU score than the bpe-based model after training for one epoch on En-Fr and En-Cs translation tasks. Further analyses show that our model is able to learn morphology.\n\n", "pdf": "/pdf/d4495be4c260cf351432b34cb5232967aeabe5c4.pdf", "TL;DR": "We devise a character-level neural machine translation built on six recurrent networks, and obtain a  BLEU score comparable to the state-of-the-art NMT on En-Fr and Cs-En translation tasks. ", "paperhash": "zhao|deep_characterlevel_neural_machine_translation_by_learning_morphology", "conflicts": ["sjtu.edu.cn", "pku.edu.cn", "zju.edu.cn", "ust.hk"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Shenjian Zhao", "Zhihua Zhang"], "authorids": ["sword.york@gmail.com", "zhzhang@math.pku.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287660517, "id": "ICLR.cc/2017/conference/-/paper261/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJq_YBqxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper261/reviewers", "ICLR.cc/2017/conference/paper261/areachairs"], "cdate": 1485287660517}}}, {"tddate": null, "tmdate": 1481987331357, "tcdate": 1481987211837, "number": 3, "id": "By45ECGEl", "invitation": "ICLR.cc/2017/conference/-/paper261/public/comment", "forum": "rJq_YBqxx", "replyto": "rJJHggGNx", "signatures": ["~Shenjian_Zhao1"], "readers": ["everyone"], "writers": ["~Shenjian_Zhao1"], "content": {"title": "Trivial baseline and training time", "comment": "Thanks for your nice comments!\nWe think our system is less complicated compared to [1] which uses thousands of filters, and it's more natural to encode words with RNN.\n\nBelow are the one-to-one responses to the comments.\n\nComment 1: Presentation.\nResponse: As Figure 2 suggests, the RNN sentence encoder receives one vector per word. Notation h_t indicates the hidden state of RNN unit, thus we used it in both Subsection 3.1 and 3.2. We have clarified it in the revision and we will add an Appendix to explain unambiguously how the model works. Thanks for your suggestion.\n\nComment 2: Training time.\nResponse: En-Cs model is trained for 15 days and Cs-En model is trained for about 22 days, so the training speed is similar to Lee et al, 2016 [1]. We have added the comparison in Table 1 in the revision. As mentioned in the paper, all the models are trained on a single NVIDIA TITAN X GPU. You could test the code which released on github (https://github.com/swordyork/dcnmt). Actually, it takes less memory and much easier for back-propagation because of the hierarchical architecture.\n\nComment 3: Trivial baseline.\nResponse: As mentioned in the introduction, the encoder in Ling et al, 2015 [2] has encountered some problems; they just take the last hidden state for each RNN (C2W model) and their results are not competitive (the result in their paper and the result in Table 1). Besides, our previous version (https://arxiv.org/pdf/1608.04738.pdf and the corresponding code https://github.com/swordyork/dcnmt/tree/old-version) also could be considered as a trivial baseline which takes the last hidden state of each RNN. However, the results are not such competitive (En-Fr BLEU on newstest2014 is 31.76, and this version is 32.85.). Thus we have devised such an architecture which encodes the source word and source sentence more detailed.\n\nThanks! \n\n[1] Lee, Jason, Kyunghyun Cho, and Thomas Hofmann. \"Fully Character-Level Neural Machine Translation without Explicit Segmentation.\" \n[2] Ling, Wang, et al. \"Character-based Neural Machine Translation.\" "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Character-Level Neural Machine Translation By Learning Morphology", "abstract": "Neural machine translation aims at building a single large neural network that can be trained to maximize translation performance. The encoder-decoder architecture with an attention mechanism achieves a translation performance comparable to the existing state-of-the-art phrase-based systems. However, the use of large vocabulary becomes the bottleneck in both training and improving the performance. In this paper, we propose a novel architecture which learns morphology by using two recurrent networks and a hierarchical decoder which translates at character level. This gives rise to a deep character-level model consisting of six recurrent networks. Such a deep model has two major advantages. It avoids the large vocabulary issue radically; at the same time, it is more efficient in training than word-based models. Our model obtains a higher BLEU score than the bpe-based model after training for one epoch on En-Fr and En-Cs translation tasks. Further analyses show that our model is able to learn morphology.\n\n", "pdf": "/pdf/d4495be4c260cf351432b34cb5232967aeabe5c4.pdf", "TL;DR": "We devise a character-level neural machine translation built on six recurrent networks, and obtain a  BLEU score comparable to the state-of-the-art NMT on En-Fr and Cs-En translation tasks. ", "paperhash": "zhao|deep_characterlevel_neural_machine_translation_by_learning_morphology", "conflicts": ["sjtu.edu.cn", "pku.edu.cn", "zju.edu.cn", "ust.hk"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Shenjian Zhao", "Zhihua Zhang"], "authorids": ["sword.york@gmail.com", "zhzhang@math.pku.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287660517, "id": "ICLR.cc/2017/conference/-/paper261/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJq_YBqxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper261/reviewers", "ICLR.cc/2017/conference/paper261/areachairs"], "cdate": 1485287660517}}}, {"tddate": null, "tmdate": 1481928759088, "tcdate": 1481928759088, "number": 1, "id": "rJJHggGNx", "invitation": "ICLR.cc/2017/conference/-/paper261/official/review", "forum": "rJq_YBqxx", "replyto": "rJq_YBqxx", "signatures": ["ICLR.cc/2017/conference/paper261/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper261/AnonReviewer2"], "content": {"title": "Good paper, accept", "rating": "7: Good paper, accept", "review": "The paper presents one of the first neural translation systems that operates purely at the character-level, another one being https://arxiv.org/abs/1610.03017 , which can be considered a concurrent work. The system is rather complicated and consists of a lot of recurrent networks.  The quantitative results are quite good and the qualitative results are quite encouraging.\n\nFirst, a few words about the quality of presentation. Despite being an expert in the area, it is hard for me to be sure that I exactly understood what is being done. The Subsections 3.1 and 3.2 sketch two main features of the architecture at a rather high-level. For example, does the RNN sentence encoder receive one vector per word as input or more? Figure 2 suggests that it\u2019s just one. The notation h_t is overloaded, used in both Subsection 3.1 and 3.2 with clearly different meaning. An Appendix that explains unambiguously how the model works would be in order. Also, the approach appears to be limited by its reliance on the availability of blanks between words, a trait which not all languages possess.\n\nSecond, the results seem to be quite good. However, no significant improvement over bpe2char systems is reported. Also, I would be curious to know how long it takes to train such a model,  because from the description it seems like the model would be very slow to train (400 steps of BiNNN). On a related note, normally an ablation test is a must for such papers, to show that the architectural enhancements applied were actually necessary. I can imagine that this would take a lot of GPU time for such a complex model.\n\nOn the bright side, Figure 3 presents some really interesting properties that of the embeddings that the model learnt. Likewise interesting is Figure 5.\n\nTo conclude, I think that this an interesting application paper, but the execution quality could be improved. I am ready to increase my score if an ablation test confirms that the considered encoder is better than a trivial baseline, that e.g. takes the last hidden state for each RNN. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Character-Level Neural Machine Translation By Learning Morphology", "abstract": "Neural machine translation aims at building a single large neural network that can be trained to maximize translation performance. The encoder-decoder architecture with an attention mechanism achieves a translation performance comparable to the existing state-of-the-art phrase-based systems. However, the use of large vocabulary becomes the bottleneck in both training and improving the performance. In this paper, we propose a novel architecture which learns morphology by using two recurrent networks and a hierarchical decoder which translates at character level. This gives rise to a deep character-level model consisting of six recurrent networks. Such a deep model has two major advantages. It avoids the large vocabulary issue radically; at the same time, it is more efficient in training than word-based models. Our model obtains a higher BLEU score than the bpe-based model after training for one epoch on En-Fr and En-Cs translation tasks. Further analyses show that our model is able to learn morphology.\n\n", "pdf": "/pdf/d4495be4c260cf351432b34cb5232967aeabe5c4.pdf", "TL;DR": "We devise a character-level neural machine translation built on six recurrent networks, and obtain a  BLEU score comparable to the state-of-the-art NMT on En-Fr and Cs-En translation tasks. ", "paperhash": "zhao|deep_characterlevel_neural_machine_translation_by_learning_morphology", "conflicts": ["sjtu.edu.cn", "pku.edu.cn", "zju.edu.cn", "ust.hk"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Shenjian Zhao", "Zhihua Zhang"], "authorids": ["sword.york@gmail.com", "zhzhang@math.pku.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512644406, "id": "ICLR.cc/2017/conference/-/paper261/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper261/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper261/AnonReviewer2", "ICLR.cc/2017/conference/paper261/AnonReviewer3", "ICLR.cc/2017/conference/paper261/AnonReviewer1"], "reply": {"forum": "rJq_YBqxx", "replyto": "rJq_YBqxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper261/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper261/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512644406}}}, {"tddate": null, "tmdate": 1481898409202, "tcdate": 1481898409202, "number": 2, "id": "ryb3t_ZNg", "invitation": "ICLR.cc/2017/conference/-/paper261/public/comment", "forum": "rJq_YBqxx", "replyto": "SkRe98b4l", "signatures": ["~Shenjian_Zhao1"], "readers": ["everyone"], "writers": ["~Shenjian_Zhao1"], "content": {"title": "About the revision and the hierarchical attention", "comment": "Thank you for your insightful comments!   \n\n1. Lee et al, 2016 [1] submitted their paper to arxiv on October 10th and released the code on October 29th. We found that they trained with longer sentences and much more iterations. Thus we re-trained our model with similar settings, and encouragingly we obtained a comparable performance as shown in the revision.\n\n2. We haven't tried the hierarchical attention [2]. After having used the attention mechanism [3] in the decoder, we found it better to enhance the encoder based on the experiments (and other publications [1] also enhanced the encoder). Thus we have devised such an architecture which encodes the source word and source sentence more detailed. Besides, it is possible to apply our approach to text summarization (as mentioned in the conclusion); we could use the architecture in Figure 1 to encode the sentence and the relevant words may have high energies (weights). It is under developing and testing.  \nIt is a good idea to try the hierarchical attention [2] which may further improve the performance. Thanks for your suggestion. We will try this architecture.\n\n[1] Lee, Jason, Kyunghyun Cho, and Thomas Hofmann. \"Fully Character-Level Neural Machine Translation without Explicit Segmentation.\" \n[2] Nallapati, Ramesh, Bowen Zhou, \u00c7aglar Gul\u00e7ehre, and Bing Xiang. \"Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond.\"\n[3] Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. \"Neural machine translation by jointly learning to align and translate.\""}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Character-Level Neural Machine Translation By Learning Morphology", "abstract": "Neural machine translation aims at building a single large neural network that can be trained to maximize translation performance. The encoder-decoder architecture with an attention mechanism achieves a translation performance comparable to the existing state-of-the-art phrase-based systems. However, the use of large vocabulary becomes the bottleneck in both training and improving the performance. In this paper, we propose a novel architecture which learns morphology by using two recurrent networks and a hierarchical decoder which translates at character level. This gives rise to a deep character-level model consisting of six recurrent networks. Such a deep model has two major advantages. It avoids the large vocabulary issue radically; at the same time, it is more efficient in training than word-based models. Our model obtains a higher BLEU score than the bpe-based model after training for one epoch on En-Fr and En-Cs translation tasks. Further analyses show that our model is able to learn morphology.\n\n", "pdf": "/pdf/d4495be4c260cf351432b34cb5232967aeabe5c4.pdf", "TL;DR": "We devise a character-level neural machine translation built on six recurrent networks, and obtain a  BLEU score comparable to the state-of-the-art NMT on En-Fr and Cs-En translation tasks. ", "paperhash": "zhao|deep_characterlevel_neural_machine_translation_by_learning_morphology", "conflicts": ["sjtu.edu.cn", "pku.edu.cn", "zju.edu.cn", "ust.hk"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Shenjian Zhao", "Zhihua Zhang"], "authorids": ["sword.york@gmail.com", "zhzhang@math.pku.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287660517, "id": "ICLR.cc/2017/conference/-/paper261/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJq_YBqxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper261/reviewers", "ICLR.cc/2017/conference/paper261/areachairs"], "cdate": 1485287660517}}}, {"tddate": null, "tmdate": 1481890293553, "tcdate": 1481890293553, "number": 2, "id": "SkRe98b4l", "invitation": "ICLR.cc/2017/conference/-/paper261/pre-review/question", "forum": "rJq_YBqxx", "replyto": "rJq_YBqxx", "signatures": ["ICLR.cc/2017/conference/paper261/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper261/AnonReviewer1"], "content": {"title": "Re: Deep Character-Level Neural Machine Translation By Learning Morphology", "question": "Sorry for the delay in the pre-review questions, overall the paper is well-written. \n\n- One of the first question, popped up in my mind as the other reviewer raised was to compare against to the previously published results. However, in the revised version of the paper those results seemed to be added. That is very encouraging.\n- Have you tried using hierarchical attention on the decoder[1]? Do you think it makes sense for this task? \n\n\n[1]Nallapati, Ramesh, Bowen Zhou, \u00c7aglar Gul\u00e7ehre, and Bing Xiang. \"Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond.\"\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Character-Level Neural Machine Translation By Learning Morphology", "abstract": "Neural machine translation aims at building a single large neural network that can be trained to maximize translation performance. The encoder-decoder architecture with an attention mechanism achieves a translation performance comparable to the existing state-of-the-art phrase-based systems. However, the use of large vocabulary becomes the bottleneck in both training and improving the performance. In this paper, we propose a novel architecture which learns morphology by using two recurrent networks and a hierarchical decoder which translates at character level. This gives rise to a deep character-level model consisting of six recurrent networks. Such a deep model has two major advantages. It avoids the large vocabulary issue radically; at the same time, it is more efficient in training than word-based models. Our model obtains a higher BLEU score than the bpe-based model after training for one epoch on En-Fr and En-Cs translation tasks. Further analyses show that our model is able to learn morphology.\n\n", "pdf": "/pdf/d4495be4c260cf351432b34cb5232967aeabe5c4.pdf", "TL;DR": "We devise a character-level neural machine translation built on six recurrent networks, and obtain a  BLEU score comparable to the state-of-the-art NMT on En-Fr and Cs-En translation tasks. ", "paperhash": "zhao|deep_characterlevel_neural_machine_translation_by_learning_morphology", "conflicts": ["sjtu.edu.cn", "pku.edu.cn", "zju.edu.cn", "ust.hk"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Shenjian Zhao", "Zhihua Zhang"], "authorids": ["sword.york@gmail.com", "zhzhang@math.pku.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481890294220, "id": "ICLR.cc/2017/conference/-/paper261/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper261/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper261/AnonReviewer2", "ICLR.cc/2017/conference/paper261/AnonReviewer1"], "reply": {"forum": "rJq_YBqxx", "replyto": "rJq_YBqxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper261/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper261/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481890294220}}}, {"tddate": null, "tmdate": 1480574592277, "tcdate": 1480571496244, "number": 1, "id": "SyeO5EaGe", "invitation": "ICLR.cc/2017/conference/-/paper261/public/comment", "forum": "rJq_YBqxx", "replyto": "r1x6SRjGg", "signatures": ["~Shenjian_Zhao1"], "readers": ["everyone"], "writers": ["~Shenjian_Zhao1"], "content": {"title": "Comparison with Chung et al, 2016 and Lee et al, 2016 and additional comparison with \"Hierarchical Multiscale Recurrent Neural Networks\"", "comment": "Thanks for your comments.\nAfter having used the similar setting and training for 1,280,000 iterations on Cs-En task (Lee et al, 2016 trained for 1,500,000 iterations), we achieve a comparable performance. Please refer to the updated paper for details.\n\nIn the revision, we have changed our source word encoder to use a bidirectional RNN to compute the energy (weight) as shown in Figure 1. \n\nAdditionally, we compare our results with \"Hierarchical Multiscale Recurrent Neural Networks\" (J Chung et al, 2016) in Figure 5, which may be interesting. \n\nAs for \"learning morphology could speed up learning\", it is mainly based on the analysis in Section 5.2. This has also been shown in Table 1 (En-Fr and En-Cs task) from which we see that when we train our model just for one epoch, the obtained result even outperforms the final result with the bpe baseline. We have changed this description and Table 1 to avoid misunderstanding."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Character-Level Neural Machine Translation By Learning Morphology", "abstract": "Neural machine translation aims at building a single large neural network that can be trained to maximize translation performance. The encoder-decoder architecture with an attention mechanism achieves a translation performance comparable to the existing state-of-the-art phrase-based systems. However, the use of large vocabulary becomes the bottleneck in both training and improving the performance. In this paper, we propose a novel architecture which learns morphology by using two recurrent networks and a hierarchical decoder which translates at character level. This gives rise to a deep character-level model consisting of six recurrent networks. Such a deep model has two major advantages. It avoids the large vocabulary issue radically; at the same time, it is more efficient in training than word-based models. Our model obtains a higher BLEU score than the bpe-based model after training for one epoch on En-Fr and En-Cs translation tasks. Further analyses show that our model is able to learn morphology.\n\n", "pdf": "/pdf/d4495be4c260cf351432b34cb5232967aeabe5c4.pdf", "TL;DR": "We devise a character-level neural machine translation built on six recurrent networks, and obtain a  BLEU score comparable to the state-of-the-art NMT on En-Fr and Cs-En translation tasks. ", "paperhash": "zhao|deep_characterlevel_neural_machine_translation_by_learning_morphology", "conflicts": ["sjtu.edu.cn", "pku.edu.cn", "zju.edu.cn", "ust.hk"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Shenjian Zhao", "Zhihua Zhang"], "authorids": ["sword.york@gmail.com", "zhzhang@math.pku.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287660517, "id": "ICLR.cc/2017/conference/-/paper261/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJq_YBqxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper261/reviewers", "ICLR.cc/2017/conference/paper261/areachairs"], "cdate": 1485287660517}}}, {"tddate": null, "tmdate": 1480480183905, "tcdate": 1480480183901, "number": 1, "id": "r1x6SRjGg", "invitation": "ICLR.cc/2017/conference/-/paper261/pre-review/question", "forum": "rJq_YBqxx", "replyto": "rJq_YBqxx", "signatures": ["ICLR.cc/2017/conference/paper261/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper261/AnonReviewer2"], "content": {"title": "A few questions", "question": "Hi, interesting paper!\n\nI have two questions. First, are you still planning to compare your results to the ones of Chung et al, 2016 and Lee et al, 2016? These seem to be absolutely necessary comparisons for your paper. Second, the first paragraph of Subsection 5.3 where you say that \"learning morphology could speed up learning\" leaves me confused. You would have to compare to the performance of the bpe baseline after one epoch, and I don't think you have this number in Table 1. Can you please clarify?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Character-Level Neural Machine Translation By Learning Morphology", "abstract": "Neural machine translation aims at building a single large neural network that can be trained to maximize translation performance. The encoder-decoder architecture with an attention mechanism achieves a translation performance comparable to the existing state-of-the-art phrase-based systems. However, the use of large vocabulary becomes the bottleneck in both training and improving the performance. In this paper, we propose a novel architecture which learns morphology by using two recurrent networks and a hierarchical decoder which translates at character level. This gives rise to a deep character-level model consisting of six recurrent networks. Such a deep model has two major advantages. It avoids the large vocabulary issue radically; at the same time, it is more efficient in training than word-based models. Our model obtains a higher BLEU score than the bpe-based model after training for one epoch on En-Fr and En-Cs translation tasks. Further analyses show that our model is able to learn morphology.\n\n", "pdf": "/pdf/d4495be4c260cf351432b34cb5232967aeabe5c4.pdf", "TL;DR": "We devise a character-level neural machine translation built on six recurrent networks, and obtain a  BLEU score comparable to the state-of-the-art NMT on En-Fr and Cs-En translation tasks. ", "paperhash": "zhao|deep_characterlevel_neural_machine_translation_by_learning_morphology", "conflicts": ["sjtu.edu.cn", "pku.edu.cn", "zju.edu.cn", "ust.hk"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Shenjian Zhao", "Zhihua Zhang"], "authorids": ["sword.york@gmail.com", "zhzhang@math.pku.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481890294220, "id": "ICLR.cc/2017/conference/-/paper261/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper261/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper261/AnonReviewer2", "ICLR.cc/2017/conference/paper261/AnonReviewer1"], "reply": {"forum": "rJq_YBqxx", "replyto": "rJq_YBqxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper261/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper261/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481890294220}}}], "count": 20}