{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396681755, "tcdate": 1486396681755, "number": 1, "id": "HJzf6z8dx", "invitation": "ICLR.cc/2017/conference/-/paper567/acceptance", "forum": "ry7O1ssex", "replyto": "ry7O1ssex", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "This paper is timely since it addresses the connections between energy-based models, GANS, and the general space of generative models. The two principal concerns about the paper are: lack of clarity and coherence in the paper; inability to effectively judge the effectiveness of the method. The responses of the authors address these concern to some extent, but these concerns remain. The paper will have much higher-impact after further introspection, but at present, the paper is not yet ready for acceptance at the conference."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Generative Adversarial Networks as Variational Training of Energy Based Models", "abstract": "In this paper, we study deep generative models for effective unsupervised learning. We propose VGAN, which works by minimizing a variational lower bound of the negative log likelihood (NLL) of an energy based model (EBM), where the model density $p(\\mathbf{x})$ is approximated by a variational distribution $q(\\mathbf{x})$ that is easy to sample from. The training of VGAN takes a two step procedure: given $p(\\mathbf{x})$, $q(\\mathbf{x})$ is updated to maximize the lower bound; $p(\\mathbf{x})$ is then updated one step with samples drawn from $q(\\mathbf{x})$ to decrease the lower bound. VGAN is inspired by the generative adversarial networks (GANs), where $p(\\mathbf{x})$ corresponds to the discriminator and $q(\\mathbf{x})$ corresponds to the generator, but with several notable differences. We hence name our model variational GANs (VGANs). VGAN provides a practical solution to training deep EBMs in high dimensional space, by eliminating the need of MCMC sampling. From this view, we are also able to identify causes to the difficulty of training GANs and propose viable solutions.", "pdf": "/pdf/dd97f6c4ef644bca2ec96f6e3eafa0bd8151a839.pdf", "paperhash": "zhai|generative_adversarial_networks_as_variational_training_of_energy_based_models", "conflicts": ["binghamton.edu"], "keywords": [], "authors": ["Shuangfei Zhai", "Yu Cheng", "Rogerio Feris", "Zhongfei Zhang"], "authorids": ["szhai2@binghamton.edu", "chengyu@us.ibm.com", "rsferis@us.ibm.com", "zhongfei@cs.binghamton.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396682294, "id": "ICLR.cc/2017/conference/-/paper567/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "ry7O1ssex", "replyto": "ry7O1ssex", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396682294}}}, {"tddate": null, "tmdate": 1486083544788, "tcdate": 1486083544788, "number": 10, "id": "BkbyUIb_l", "invitation": "ICLR.cc/2017/conference/-/paper567/public/comment", "forum": "ry7O1ssex", "replyto": "ByYqfXmwx", "signatures": ["~Shuangfei_Zhai1"], "readers": ["everyone"], "writers": ["~Shuangfei_Zhai1"], "content": {"title": "Re: review", "comment": "Hi,\n\nThanks a lot for the comments.  The order of min-max of GAN is an easily overlooked problem, which we try to shed light on from the view of EBMs. This difference indeed causes VGAN and GAN to have different training dynamics in theory. In practice, we are not aware of any work that suggests increasing the number of updates for D improves the stability; on the other hand, there has been efforts in \"slowing down\" the updates of D that has been shown useful (e.g., label smoothing in Improved GAN). We have also verified in our experiments that increasing the updates of G benefits both GAN and VGAN, given an effective  entropy approximation (otherwise G will be encouraged to collapse even more). These empirical evidences lead us to the belief that in practice GAN behaves very similar to VGAN, both of which are consistent with the principle of variational training of an EBM. We believe it is thus easier to understand, diagnose and evaluate GAN from the perspective of an EBM, which may also lead to new directions in this area.\n\nThank,\nShuangfei"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Generative Adversarial Networks as Variational Training of Energy Based Models", "abstract": "In this paper, we study deep generative models for effective unsupervised learning. We propose VGAN, which works by minimizing a variational lower bound of the negative log likelihood (NLL) of an energy based model (EBM), where the model density $p(\\mathbf{x})$ is approximated by a variational distribution $q(\\mathbf{x})$ that is easy to sample from. The training of VGAN takes a two step procedure: given $p(\\mathbf{x})$, $q(\\mathbf{x})$ is updated to maximize the lower bound; $p(\\mathbf{x})$ is then updated one step with samples drawn from $q(\\mathbf{x})$ to decrease the lower bound. VGAN is inspired by the generative adversarial networks (GANs), where $p(\\mathbf{x})$ corresponds to the discriminator and $q(\\mathbf{x})$ corresponds to the generator, but with several notable differences. We hence name our model variational GANs (VGANs). VGAN provides a practical solution to training deep EBMs in high dimensional space, by eliminating the need of MCMC sampling. From this view, we are also able to identify causes to the difficulty of training GANs and propose viable solutions.", "pdf": "/pdf/dd97f6c4ef644bca2ec96f6e3eafa0bd8151a839.pdf", "paperhash": "zhai|generative_adversarial_networks_as_variational_training_of_energy_based_models", "conflicts": ["binghamton.edu"], "keywords": [], "authors": ["Shuangfei Zhai", "Yu Cheng", "Rogerio Feris", "Zhongfei Zhang"], "authorids": ["szhai2@binghamton.edu", "chengyu@us.ibm.com", "rsferis@us.ibm.com", "zhongfei@cs.binghamton.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287517819, "id": "ICLR.cc/2017/conference/-/paper567/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ry7O1ssex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper567/reviewers", "ICLR.cc/2017/conference/paper567/areachairs"], "cdate": 1485287517819}}}, {"tddate": null, "tmdate": 1485188739872, "tcdate": 1482235473948, "number": 3, "id": "S15LA5LVe", "invitation": "ICLR.cc/2017/conference/-/paper567/official/review", "forum": "ry7O1ssex", "replyto": "ry7O1ssex", "signatures": ["ICLR.cc/2017/conference/paper567/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper567/AnonReviewer2"], "content": {"title": "Final Review", "rating": "4: Ok but not good enough - rejection", "review": "The paper drives a variant of GAN's min max game optimizations from EBM's NLL minimization and propose a new generative model from this derivation.\n\nIn introduction, could you elaborate why accurate distance metric is needed for unsupervised learning.\n\nIt's a bit hard to read the paper as it jumps from points to points without clear connection and laying down the background.  The introduction, refers to many different concepts without any clear connection. And other sections as well is very incomprehensible even if one is familiar with the concepts.\n\nThere has been lots of interests in similar area recently, Authors do cite some of them in introduction and related work but the relationship and comparison are missing.\n\nIn the results, if it's comparison of the quality of samples it has to include other recent works and compare those as well. And if it's semi-supervised learning, again, the numbers should be compared to other works.\n\nIn summary, unfortunately the paper is very clear and cumbersome to read which makes it hard to judge it fairly. I strongly suggest re-write of the paper in more coherent matter to make it easier to read. And also extend the experiments with more comparisons with other works.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Generative Adversarial Networks as Variational Training of Energy Based Models", "abstract": "In this paper, we study deep generative models for effective unsupervised learning. We propose VGAN, which works by minimizing a variational lower bound of the negative log likelihood (NLL) of an energy based model (EBM), where the model density $p(\\mathbf{x})$ is approximated by a variational distribution $q(\\mathbf{x})$ that is easy to sample from. The training of VGAN takes a two step procedure: given $p(\\mathbf{x})$, $q(\\mathbf{x})$ is updated to maximize the lower bound; $p(\\mathbf{x})$ is then updated one step with samples drawn from $q(\\mathbf{x})$ to decrease the lower bound. VGAN is inspired by the generative adversarial networks (GANs), where $p(\\mathbf{x})$ corresponds to the discriminator and $q(\\mathbf{x})$ corresponds to the generator, but with several notable differences. We hence name our model variational GANs (VGANs). VGAN provides a practical solution to training deep EBMs in high dimensional space, by eliminating the need of MCMC sampling. From this view, we are also able to identify causes to the difficulty of training GANs and propose viable solutions.", "pdf": "/pdf/dd97f6c4ef644bca2ec96f6e3eafa0bd8151a839.pdf", "paperhash": "zhai|generative_adversarial_networks_as_variational_training_of_energy_based_models", "conflicts": ["binghamton.edu"], "keywords": [], "authors": ["Shuangfei Zhai", "Yu Cheng", "Rogerio Feris", "Zhongfei Zhang"], "authorids": ["szhai2@binghamton.edu", "chengyu@us.ibm.com", "rsferis@us.ibm.com", "zhongfei@cs.binghamton.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512538413, "id": "ICLR.cc/2017/conference/-/paper567/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper567/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper567/AnonReviewer1", "ICLR.cc/2017/conference/paper567/AnonReviewer3", "ICLR.cc/2017/conference/paper567/AnonReviewer2"], "reply": {"forum": "ry7O1ssex", "replyto": "ry7O1ssex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper567/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper567/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512538413}}}, {"tddate": null, "tmdate": 1485152912818, "tcdate": 1485152912818, "number": 1, "id": "ByYqfXmwx", "invitation": "ICLR.cc/2017/conference/-/paper567/public/review", "forum": "ry7O1ssex", "replyto": "ry7O1ssex", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Nice insights, clarity needs improvement", "rating": "8: Top 50% of accepted papers, clear accept", "review": "Our understanding of GAN to date is still vague. Although there have been some efforts relating GAN to energy models, I personally consider that the perspective of this paper, namelying understanding GAN (a variant of GAN, to be more precise) as variational training of an energy model is the most natural and elegant.  The derivation up to equation (5) and the reduction to (7) are very nice.  I think this is the most important contribution of the paper. \n\nThe techniques introduced in sections 5 and 6 are somewhat ad hoc, and lack clarity. Referring to the version I looked at (not sure though if it is the latest), section 6 contain some errors/typos (stuff around p_z(x|\\tilde{x}). The presentation of section 6 needs to improve in clarity.  But I think this does not shadow the main contribution of the paper, namely, that perspectives given in sections 2-4.  \n\nOverall I very much enjoy the presented insight of this paper into GAN.\n\nI do have some comment/question regarding equation (7). This equation formulates a variant of GAN, or a model resembling GAN. I am happy to see that the entropy term pops up there, which should save GAN from degenerating its generative distribution or from missing modes.  The swapping of the min-max order in this formulation however makes me wonder if this variant of GAN indeed reflects the \"principle\" of GAN, or it is in fact a different principle, which happens to gives rise to a model  that \"resembles\" GAN.  Of course, my question may be merely philosophical rather than mathematical, and I won't expect a precise anwer. Nonetheless, if the author can provide additional insignts on this, it would be appreciated.\n\n\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Generative Adversarial Networks as Variational Training of Energy Based Models", "abstract": "In this paper, we study deep generative models for effective unsupervised learning. We propose VGAN, which works by minimizing a variational lower bound of the negative log likelihood (NLL) of an energy based model (EBM), where the model density $p(\\mathbf{x})$ is approximated by a variational distribution $q(\\mathbf{x})$ that is easy to sample from. The training of VGAN takes a two step procedure: given $p(\\mathbf{x})$, $q(\\mathbf{x})$ is updated to maximize the lower bound; $p(\\mathbf{x})$ is then updated one step with samples drawn from $q(\\mathbf{x})$ to decrease the lower bound. VGAN is inspired by the generative adversarial networks (GANs), where $p(\\mathbf{x})$ corresponds to the discriminator and $q(\\mathbf{x})$ corresponds to the generator, but with several notable differences. We hence name our model variational GANs (VGANs). VGAN provides a practical solution to training deep EBMs in high dimensional space, by eliminating the need of MCMC sampling. From this view, we are also able to identify causes to the difficulty of training GANs and propose viable solutions.", "pdf": "/pdf/dd97f6c4ef644bca2ec96f6e3eafa0bd8151a839.pdf", "paperhash": "zhai|generative_adversarial_networks_as_variational_training_of_energy_based_models", "conflicts": ["binghamton.edu"], "keywords": [], "authors": ["Shuangfei Zhai", "Yu Cheng", "Rogerio Feris", "Zhongfei Zhang"], "authorids": ["szhai2@binghamton.edu", "chengyu@us.ibm.com", "rsferis@us.ibm.com", "zhongfei@cs.binghamton.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485152913571, "id": "ICLR.cc/2017/conference/-/paper567/public/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ry7O1ssex", "replyto": "ry7O1ssex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "noninvitees": ["szhai2@binghamton.edu", "chengyu@us.ibm.com", "rsferis@us.ibm.com", "zhongfei@cs.binghamton.edu", "ICLR.cc/2017/conference/paper567/reviewers", "ICLR.cc/2017/conference/paper567/areachairs", "(anonymous)"], "cdate": 1485152913571}}}, {"tddate": null, "tmdate": 1483862591350, "tcdate": 1483862591350, "number": 9, "id": "SkDBz_1Lg", "invitation": "ICLR.cc/2017/conference/-/paper567/public/comment", "forum": "ry7O1ssex", "replyto": "ry7O1ssex", "signatures": ["~Shuangfei_Zhai1"], "readers": ["everyone"], "writers": ["~Shuangfei_Zhai1"], "content": {"title": "Re: AnonReviewer2", "comment": "Thanks for the comments. \n\nAbout the distance metric: unsupervised learning does not necessarily need an accurate distance metric, it just happens that most of the commonly used methods do assume a simple distance metric (such Euclidean distance), such as K-means, Gaussian Mixture Models, PCA, Autoencoders, VAE and etc.. These models either directly use a predefined distance metric in the input space, or assume equivalent distributions (such as Gaussian distributions) in the input space, which directly limits their usefulness for high dimensional inputs where there exists no reasonable simple distance metric.\n\nWe do agree that we could do a better job at elaborating the relationship with existing works, we will provide an updated version to clarify the unclearness.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Generative Adversarial Networks as Variational Training of Energy Based Models", "abstract": "In this paper, we study deep generative models for effective unsupervised learning. We propose VGAN, which works by minimizing a variational lower bound of the negative log likelihood (NLL) of an energy based model (EBM), where the model density $p(\\mathbf{x})$ is approximated by a variational distribution $q(\\mathbf{x})$ that is easy to sample from. The training of VGAN takes a two step procedure: given $p(\\mathbf{x})$, $q(\\mathbf{x})$ is updated to maximize the lower bound; $p(\\mathbf{x})$ is then updated one step with samples drawn from $q(\\mathbf{x})$ to decrease the lower bound. VGAN is inspired by the generative adversarial networks (GANs), where $p(\\mathbf{x})$ corresponds to the discriminator and $q(\\mathbf{x})$ corresponds to the generator, but with several notable differences. We hence name our model variational GANs (VGANs). VGAN provides a practical solution to training deep EBMs in high dimensional space, by eliminating the need of MCMC sampling. From this view, we are also able to identify causes to the difficulty of training GANs and propose viable solutions.", "pdf": "/pdf/dd97f6c4ef644bca2ec96f6e3eafa0bd8151a839.pdf", "paperhash": "zhai|generative_adversarial_networks_as_variational_training_of_energy_based_models", "conflicts": ["binghamton.edu"], "keywords": [], "authors": ["Shuangfei Zhai", "Yu Cheng", "Rogerio Feris", "Zhongfei Zhang"], "authorids": ["szhai2@binghamton.edu", "chengyu@us.ibm.com", "rsferis@us.ibm.com", "zhongfei@cs.binghamton.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287517819, "id": "ICLR.cc/2017/conference/-/paper567/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ry7O1ssex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper567/reviewers", "ICLR.cc/2017/conference/paper567/areachairs"], "cdate": 1485287517819}}}, {"tddate": null, "tmdate": 1483862149904, "tcdate": 1483862149904, "number": 8, "id": "S1CFgd1Ug", "invitation": "ICLR.cc/2017/conference/-/paper567/public/comment", "forum": "ry7O1ssex", "replyto": "r1ir0mX4e", "signatures": ["~Shuangfei_Zhai1"], "readers": ["everyone"], "writers": ["~Shuangfei_Zhai1"], "content": {"title": "Re: AnonReviewer3", "comment": "Thanks for the comments. We agree that including more related results would be useful for calibrating the contribution of the paper, we will provide an updated version later. However, we do not agree with the reviewer's point about \"Although the relationship between energy-based models and GANs is abundantly clear in the literature\".  We argue that our paper provide novel views to this connection, even considering all the parallel submissions to this conference. For example, when treating GAN as minimizing a variational lower bound of an NLL, it is critical to update G k steps per D update to tighten the lower bound, which is in contradiction with GAN's proposal. More recent findings, such as [Towards Principled Methods for Training\nGenerative Adversarial Networks, Arjovsky et al. 2016], also supports our argument that training D multiple steps according to the GAN theory leads to worse results. Although we do not think that our solution is perfect (simply updating G k steps), this remains an important issue to be solved to fully understand both GAN and the variational training of EBMs. Our other contributions that are non-existing in the previous literature also include the discussion of proper choices of energy functions, which is extremely important for designing and training new families of EBMs."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Generative Adversarial Networks as Variational Training of Energy Based Models", "abstract": "In this paper, we study deep generative models for effective unsupervised learning. We propose VGAN, which works by minimizing a variational lower bound of the negative log likelihood (NLL) of an energy based model (EBM), where the model density $p(\\mathbf{x})$ is approximated by a variational distribution $q(\\mathbf{x})$ that is easy to sample from. The training of VGAN takes a two step procedure: given $p(\\mathbf{x})$, $q(\\mathbf{x})$ is updated to maximize the lower bound; $p(\\mathbf{x})$ is then updated one step with samples drawn from $q(\\mathbf{x})$ to decrease the lower bound. VGAN is inspired by the generative adversarial networks (GANs), where $p(\\mathbf{x})$ corresponds to the discriminator and $q(\\mathbf{x})$ corresponds to the generator, but with several notable differences. We hence name our model variational GANs (VGANs). VGAN provides a practical solution to training deep EBMs in high dimensional space, by eliminating the need of MCMC sampling. From this view, we are also able to identify causes to the difficulty of training GANs and propose viable solutions.", "pdf": "/pdf/dd97f6c4ef644bca2ec96f6e3eafa0bd8151a839.pdf", "paperhash": "zhai|generative_adversarial_networks_as_variational_training_of_energy_based_models", "conflicts": ["binghamton.edu"], "keywords": [], "authors": ["Shuangfei Zhai", "Yu Cheng", "Rogerio Feris", "Zhongfei Zhang"], "authorids": ["szhai2@binghamton.edu", "chengyu@us.ibm.com", "rsferis@us.ibm.com", "zhongfei@cs.binghamton.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287517819, "id": "ICLR.cc/2017/conference/-/paper567/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ry7O1ssex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper567/reviewers", "ICLR.cc/2017/conference/paper567/areachairs"], "cdate": 1485287517819}}}, {"tddate": null, "tmdate": 1482010179367, "tcdate": 1482010179367, "number": 2, "id": "r1ir0mX4e", "invitation": "ICLR.cc/2017/conference/-/paper567/official/review", "forum": "ry7O1ssex", "replyto": "ry7O1ssex", "signatures": ["ICLR.cc/2017/conference/paper567/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper567/AnonReviewer3"], "content": {"title": "Review", "rating": "4: Ok but not good enough - rejection", "review": "This paper presents an adversarial training formulation for energy models. Although the relationship between energy-based models and GANs is abundantly clear in the literature, the contributions of this paper seems to be a multimodal energy estimate, and training a transition operator instead of a sampler. The core technical contribution of the paper is not clear. The only two quantitative results in the paper demonstrate that the proposed method learns better features than a traditional GAN, and that the proposed method does a better job in SSL than the authors' chosen baseline. In either case, no comparison to existing literature is made, even though GANs have been used for SSL previously (https://arxiv.org/abs/1606.03498). The qualitative results (visual comparison of quality of samples) is inconclusive.\n\nThe lack of comparison to existing literature makes this paper a clear reject. To improve this paper, the authors need to make the core contribution of the paper much better, and situate the paper very clearly wrt existing literature. The motivation for the development of the model should be clearer. Further, experiments need to compare with existing literature.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Generative Adversarial Networks as Variational Training of Energy Based Models", "abstract": "In this paper, we study deep generative models for effective unsupervised learning. We propose VGAN, which works by minimizing a variational lower bound of the negative log likelihood (NLL) of an energy based model (EBM), where the model density $p(\\mathbf{x})$ is approximated by a variational distribution $q(\\mathbf{x})$ that is easy to sample from. The training of VGAN takes a two step procedure: given $p(\\mathbf{x})$, $q(\\mathbf{x})$ is updated to maximize the lower bound; $p(\\mathbf{x})$ is then updated one step with samples drawn from $q(\\mathbf{x})$ to decrease the lower bound. VGAN is inspired by the generative adversarial networks (GANs), where $p(\\mathbf{x})$ corresponds to the discriminator and $q(\\mathbf{x})$ corresponds to the generator, but with several notable differences. We hence name our model variational GANs (VGANs). VGAN provides a practical solution to training deep EBMs in high dimensional space, by eliminating the need of MCMC sampling. From this view, we are also able to identify causes to the difficulty of training GANs and propose viable solutions.", "pdf": "/pdf/dd97f6c4ef644bca2ec96f6e3eafa0bd8151a839.pdf", "paperhash": "zhai|generative_adversarial_networks_as_variational_training_of_energy_based_models", "conflicts": ["binghamton.edu"], "keywords": [], "authors": ["Shuangfei Zhai", "Yu Cheng", "Rogerio Feris", "Zhongfei Zhang"], "authorids": ["szhai2@binghamton.edu", "chengyu@us.ibm.com", "rsferis@us.ibm.com", "zhongfei@cs.binghamton.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512538413, "id": "ICLR.cc/2017/conference/-/paper567/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper567/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper567/AnonReviewer1", "ICLR.cc/2017/conference/paper567/AnonReviewer3", "ICLR.cc/2017/conference/paper567/AnonReviewer2"], "reply": {"forum": "ry7O1ssex", "replyto": "ry7O1ssex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper567/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper567/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512538413}}}, {"tddate": null, "tmdate": 1481996042671, "tcdate": 1481996042671, "number": 7, "id": "HJQGveX4x", "invitation": "ICLR.cc/2017/conference/-/paper567/public/comment", "forum": "ry7O1ssex", "replyto": "S1xk8af4x", "signatures": ["~Shuangfei_Zhai1"], "readers": ["everyone"], "writers": ["~Shuangfei_Zhai1"], "content": {"title": "Re: AnonReviewer1", "comment": "Thanks for the comments.\n\nThe goal of this paper is to provide a principled view to guide the training of GAN-like models, which we interpret as minimizing a variational lower bound of the negative log likelihood of an EBM. From this view, we are able to identify several important issues of training GANs and EMBs that are not revealed in the previous literature. For example, we identified that minimizing a lower bound is a major source of instability, as when the bound is not tight, there is no guarantee that the true NLL is minimized. Therefore, it is important to switch the order of the G and D updates to update G K steps to tighten the bound then update D one step, instead of the other way around as proposed in the original GAN paper (see Table 1 and Figure 2). We also identified the importance of bounded energy parameterization in light of this training strategy, which is important but often ignored in related EBM literature. \n\nWe did not include the (failed) results trained without the entropy approximation, as the importance of diversity promoting regularizations is well recognized in recent GAN related papers (e.g., batch discrimination in improved gan). Moreover, the related work EBGAN that the reviewer pointed out actually does have a similar term as well, which they call the \"repelling regularizer\" (section 2.4 of Zhao et al.). We argue that observing the diversity of the generated samples is sufficient to verify the effectiveness of the proposed approximation, although we make no claim that our proposal is by any means an accurate approximation of the true entropy.\n\nThe motivation of VCD comes exactly from the difficulty of accurate approximation of the entropy term. VCD initializes p_g(x) as a distribution close to p_data(x), which essential bypasses the need of directly approximating and maximizing its entropy. The fact that this works well is not surprising either due to its similarity to contrastive divergence and the vast success CD achieves in training EBMs. Our way of evaluating the quality of generated samples with the proposed semi-supervised learning benchmark is also novel, and an arguably more objective metric those used previously such as visual quality, parzen window-based likelihood estimation and inception scores. We argue that the improvements gained by using only generated samples as data augmentation are nontrivial, and demonstrate the usefulness of the generative model in an objective way. We will update our paper with related semi-supervised learning methods in a later version for a more complete comparison.\n\nThanks,\nShuangfei"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Generative Adversarial Networks as Variational Training of Energy Based Models", "abstract": "In this paper, we study deep generative models for effective unsupervised learning. We propose VGAN, which works by minimizing a variational lower bound of the negative log likelihood (NLL) of an energy based model (EBM), where the model density $p(\\mathbf{x})$ is approximated by a variational distribution $q(\\mathbf{x})$ that is easy to sample from. The training of VGAN takes a two step procedure: given $p(\\mathbf{x})$, $q(\\mathbf{x})$ is updated to maximize the lower bound; $p(\\mathbf{x})$ is then updated one step with samples drawn from $q(\\mathbf{x})$ to decrease the lower bound. VGAN is inspired by the generative adversarial networks (GANs), where $p(\\mathbf{x})$ corresponds to the discriminator and $q(\\mathbf{x})$ corresponds to the generator, but with several notable differences. We hence name our model variational GANs (VGANs). VGAN provides a practical solution to training deep EBMs in high dimensional space, by eliminating the need of MCMC sampling. From this view, we are also able to identify causes to the difficulty of training GANs and propose viable solutions.", "pdf": "/pdf/dd97f6c4ef644bca2ec96f6e3eafa0bd8151a839.pdf", "paperhash": "zhai|generative_adversarial_networks_as_variational_training_of_energy_based_models", "conflicts": ["binghamton.edu"], "keywords": [], "authors": ["Shuangfei Zhai", "Yu Cheng", "Rogerio Feris", "Zhongfei Zhang"], "authorids": ["szhai2@binghamton.edu", "chengyu@us.ibm.com", "rsferis@us.ibm.com", "zhongfei@cs.binghamton.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287517819, "id": "ICLR.cc/2017/conference/-/paper567/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ry7O1ssex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper567/reviewers", "ICLR.cc/2017/conference/paper567/areachairs"], "cdate": 1485287517819}}}, {"tddate": null, "tmdate": 1481991921983, "tcdate": 1481991921983, "number": 6, "id": "SJqgPJQ4g", "invitation": "ICLR.cc/2017/conference/-/paper567/public/comment", "forum": "ry7O1ssex", "replyto": "HkKS8TGVg", "signatures": ["~Shuangfei_Zhai1"], "readers": ["everyone"], "writers": ["~Shuangfei_Zhai1"], "content": {"title": "Re: Questions", "comment": "Thanks for the questions.\n\nWe believe Equation 9 is a reasonable approximation of the entropy in the sense that it encourages diversified samples from the generator. And no it is not a degenerate solution. To see this, when updating G, the objective subject to maximization (shown in algorithm 1) can be rewritten as: \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{K} KL(\\sigma_j^i \\| \\bar{\\sigma_j}), where \\sigma_j^i is the j-th sigmoid activation for the i-th generated sample, and \\bar{\\sigma_j} is the averaged activation over the generated mini-batch for the j-th sigmoid. Maximizing the this KL divergence encourages G to generate samples that cause each sigmoid unit to be either 0 or 1 with probability 0.5. As each W_j is initialized randomly, each generated sample tens to randomly fall into one of the 2^K optimal sigmoid activation patterns. Note that this step does not affect W_j, which is part of D. When updating D, the same KL divergence is maximized on a training data mini-batch (see algorithm 1), where the case when W_j are all zero yields a minimum KL divergence (least optimal solution) and will never occur with a correct optimization procedure.\n\nThe evidence that this does not happen is obvious. When all W_j is set to all zero vectors, the model crashes and no learning can proceed: the EBM simply encodes a uniform distribution over the input space. The fact that we are able to generate quality samples directly contradicts this hypothesis. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Generative Adversarial Networks as Variational Training of Energy Based Models", "abstract": "In this paper, we study deep generative models for effective unsupervised learning. We propose VGAN, which works by minimizing a variational lower bound of the negative log likelihood (NLL) of an energy based model (EBM), where the model density $p(\\mathbf{x})$ is approximated by a variational distribution $q(\\mathbf{x})$ that is easy to sample from. The training of VGAN takes a two step procedure: given $p(\\mathbf{x})$, $q(\\mathbf{x})$ is updated to maximize the lower bound; $p(\\mathbf{x})$ is then updated one step with samples drawn from $q(\\mathbf{x})$ to decrease the lower bound. VGAN is inspired by the generative adversarial networks (GANs), where $p(\\mathbf{x})$ corresponds to the discriminator and $q(\\mathbf{x})$ corresponds to the generator, but with several notable differences. We hence name our model variational GANs (VGANs). VGAN provides a practical solution to training deep EBMs in high dimensional space, by eliminating the need of MCMC sampling. From this view, we are also able to identify causes to the difficulty of training GANs and propose viable solutions.", "pdf": "/pdf/dd97f6c4ef644bca2ec96f6e3eafa0bd8151a839.pdf", "paperhash": "zhai|generative_adversarial_networks_as_variational_training_of_energy_based_models", "conflicts": ["binghamton.edu"], "keywords": [], "authors": ["Shuangfei Zhai", "Yu Cheng", "Rogerio Feris", "Zhongfei Zhang"], "authorids": ["szhai2@binghamton.edu", "chengyu@us.ibm.com", "rsferis@us.ibm.com", "zhongfei@cs.binghamton.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287517819, "id": "ICLR.cc/2017/conference/-/paper567/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ry7O1ssex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper567/reviewers", "ICLR.cc/2017/conference/paper567/areachairs"], "cdate": 1485287517819}}}, {"tddate": null, "tmdate": 1481983552751, "tcdate": 1481983552751, "number": 3, "id": "HkKS8TGVg", "invitation": "ICLR.cc/2017/conference/-/paper567/pre-review/question", "forum": "ry7O1ssex", "replyto": "ry7O1ssex", "signatures": ["ICLR.cc/2017/conference/paper567/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper567/AnonReviewer1"], "content": {"title": "Questions", "question": "Regarding the entropy approximation in equation 9, do you have any reason to think this is a reasonable approximation? You state that there is no theoretical guarantee \"recovers the true entropy\", but do you have any empirical evidence that this is a reasonable approximation in principle, or that the gradient is a reasonable approximation of the entropy gradient?\n\nYou state that maximizing the entropy approximation in equation 9 \"serves the same purpose of encouraging the generated samples to be diverse\", but it seems like a degenerate solution where all the W_j weights are zero would also maximize equation 9, but not at all encourage diversity. First, do you agree? Second, do you have evidence that this does not happen?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Generative Adversarial Networks as Variational Training of Energy Based Models", "abstract": "In this paper, we study deep generative models for effective unsupervised learning. We propose VGAN, which works by minimizing a variational lower bound of the negative log likelihood (NLL) of an energy based model (EBM), where the model density $p(\\mathbf{x})$ is approximated by a variational distribution $q(\\mathbf{x})$ that is easy to sample from. The training of VGAN takes a two step procedure: given $p(\\mathbf{x})$, $q(\\mathbf{x})$ is updated to maximize the lower bound; $p(\\mathbf{x})$ is then updated one step with samples drawn from $q(\\mathbf{x})$ to decrease the lower bound. VGAN is inspired by the generative adversarial networks (GANs), where $p(\\mathbf{x})$ corresponds to the discriminator and $q(\\mathbf{x})$ corresponds to the generator, but with several notable differences. We hence name our model variational GANs (VGANs). VGAN provides a practical solution to training deep EBMs in high dimensional space, by eliminating the need of MCMC sampling. From this view, we are also able to identify causes to the difficulty of training GANs and propose viable solutions.", "pdf": "/pdf/dd97f6c4ef644bca2ec96f6e3eafa0bd8151a839.pdf", "paperhash": "zhai|generative_adversarial_networks_as_variational_training_of_energy_based_models", "conflicts": ["binghamton.edu"], "keywords": [], "authors": ["Shuangfei Zhai", "Yu Cheng", "Rogerio Feris", "Zhongfei Zhang"], "authorids": ["szhai2@binghamton.edu", "chengyu@us.ibm.com", "rsferis@us.ibm.com", "zhongfei@cs.binghamton.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481983553310, "id": "ICLR.cc/2017/conference/-/paper567/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper567/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper567/AnonReviewer2", "ICLR.cc/2017/conference/paper567/AnonReviewer3", "ICLR.cc/2017/conference/paper567/AnonReviewer1"], "reply": {"forum": "ry7O1ssex", "replyto": "ry7O1ssex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper567/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper567/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481983553310}}}, {"tddate": null, "tmdate": 1481983448335, "tcdate": 1481983448335, "number": 1, "id": "S1xk8af4x", "invitation": "ICLR.cc/2017/conference/-/paper567/official/review", "forum": "ry7O1ssex", "replyto": "ry7O1ssex", "signatures": ["ICLR.cc/2017/conference/paper567/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper567/AnonReviewer1"], "content": {"title": "Review", "rating": "4: Ok but not good enough - rejection", "review": "This paper presents a bridging of energy-based models and GANs, where -- starting from the energy-based formalism -- they derive an additional entropy term that is not present in the original GAN formulation and is also absent in the EBGAN model of Zhao et al (2016, cited work). The relation between GANs and energy-based models was, as far as I know, first described in Kim and Bengio (2016, cited work) who also introduced the entropy regularization. It is also discussed in another ICLR submission (Dai et al. \"Calibrating Energy-based Generative Adversarial Networks\"). There are two novel contribution of this paper: (1) VGAN: the introduction of a novel entropy approximation; (2) VCD: variational contrastive divergence is introduced as a  novel learning algorithm (however, in fact, a different model). The specific motivation for this second contribution is not particularly clear. \n\nThe two contributions offered by the authors are quite interesting and are well motivated in the sense that the address the important problem of avoiding dealing with the generally intractable entropy term. However, unfortunately the authors present no results directly supporting either contribution. For example, it is not at all clear what role, if any, the entropy approximation plays in the samples generated from the VGAN model. Especially in the light of the impressive samples from the EBGAN model that has no corresponding term. The results provided to support the VCD algorithm, come in the form of a comparison of samples and reconstructions. But the samples provided here strike me as slightly less impressive compared to either the VGAN results or the SOTA in the literature - this is, of course, difficult to evaluate. \n\nThe results the authors do present include qualitative results in the form of reasonably compelling samples from the model trained on CIFAR-10, MNIST and SVHN datasets. They also present quantitative results int he form of semi-supervised learning tasks on the MNIST and SVHN. However these\nquantitative results are not particularly compelling as they show limited improvement over baselines. Also, there is no reference to the many\nexisting semi-supervised results on these datasets. \n\nSummary: The authors identify an important problem and offer two novel and intriguing solutions. Unfortunately the results to not sufficiently support either\ncontribution.\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Generative Adversarial Networks as Variational Training of Energy Based Models", "abstract": "In this paper, we study deep generative models for effective unsupervised learning. We propose VGAN, which works by minimizing a variational lower bound of the negative log likelihood (NLL) of an energy based model (EBM), where the model density $p(\\mathbf{x})$ is approximated by a variational distribution $q(\\mathbf{x})$ that is easy to sample from. The training of VGAN takes a two step procedure: given $p(\\mathbf{x})$, $q(\\mathbf{x})$ is updated to maximize the lower bound; $p(\\mathbf{x})$ is then updated one step with samples drawn from $q(\\mathbf{x})$ to decrease the lower bound. VGAN is inspired by the generative adversarial networks (GANs), where $p(\\mathbf{x})$ corresponds to the discriminator and $q(\\mathbf{x})$ corresponds to the generator, but with several notable differences. We hence name our model variational GANs (VGANs). VGAN provides a practical solution to training deep EBMs in high dimensional space, by eliminating the need of MCMC sampling. From this view, we are also able to identify causes to the difficulty of training GANs and propose viable solutions.", "pdf": "/pdf/dd97f6c4ef644bca2ec96f6e3eafa0bd8151a839.pdf", "paperhash": "zhai|generative_adversarial_networks_as_variational_training_of_energy_based_models", "conflicts": ["binghamton.edu"], "keywords": [], "authors": ["Shuangfei Zhai", "Yu Cheng", "Rogerio Feris", "Zhongfei Zhang"], "authorids": ["szhai2@binghamton.edu", "chengyu@us.ibm.com", "rsferis@us.ibm.com", "zhongfei@cs.binghamton.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512538413, "id": "ICLR.cc/2017/conference/-/paper567/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper567/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper567/AnonReviewer1", "ICLR.cc/2017/conference/paper567/AnonReviewer3", "ICLR.cc/2017/conference/paper567/AnonReviewer2"], "reply": {"forum": "ry7O1ssex", "replyto": "ry7O1ssex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper567/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper567/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512538413}}}, {"tddate": null, "tmdate": 1481811788835, "tcdate": 1481811788829, "number": 5, "id": "H1r8DQxVe", "invitation": "ICLR.cc/2017/conference/-/paper567/public/comment", "forum": "ry7O1ssex", "replyto": "ryB6CWyXg", "signatures": ["~Shuangfei_Zhai1"], "readers": ["everyone"], "writers": ["~Shuangfei_Zhai1"], "content": {"title": "Re: Learning Curve", "comment": "Hi, \n\nWe did not record multiple runs in our experiments, but we experienced very similar behaviors across different random initializations. You are also welcome to try out our code, which is available through the link given in the paper, and reproduce our experiments.\n\nThanks,\nShuangfei"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Generative Adversarial Networks as Variational Training of Energy Based Models", "abstract": "In this paper, we study deep generative models for effective unsupervised learning. We propose VGAN, which works by minimizing a variational lower bound of the negative log likelihood (NLL) of an energy based model (EBM), where the model density $p(\\mathbf{x})$ is approximated by a variational distribution $q(\\mathbf{x})$ that is easy to sample from. The training of VGAN takes a two step procedure: given $p(\\mathbf{x})$, $q(\\mathbf{x})$ is updated to maximize the lower bound; $p(\\mathbf{x})$ is then updated one step with samples drawn from $q(\\mathbf{x})$ to decrease the lower bound. VGAN is inspired by the generative adversarial networks (GANs), where $p(\\mathbf{x})$ corresponds to the discriminator and $q(\\mathbf{x})$ corresponds to the generator, but with several notable differences. We hence name our model variational GANs (VGANs). VGAN provides a practical solution to training deep EBMs in high dimensional space, by eliminating the need of MCMC sampling. From this view, we are also able to identify causes to the difficulty of training GANs and propose viable solutions.", "pdf": "/pdf/dd97f6c4ef644bca2ec96f6e3eafa0bd8151a839.pdf", "paperhash": "zhai|generative_adversarial_networks_as_variational_training_of_energy_based_models", "conflicts": ["binghamton.edu"], "keywords": [], "authors": ["Shuangfei Zhai", "Yu Cheng", "Rogerio Feris", "Zhongfei Zhang"], "authorids": ["szhai2@binghamton.edu", "chengyu@us.ibm.com", "rsferis@us.ibm.com", "zhongfei@cs.binghamton.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287517819, "id": "ICLR.cc/2017/conference/-/paper567/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ry7O1ssex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper567/reviewers", "ICLR.cc/2017/conference/paper567/areachairs"], "cdate": 1485287517819}}}, {"tddate": null, "tmdate": 1481810810456, "tcdate": 1481810810451, "number": 4, "id": "HkftXQxVx", "invitation": "ICLR.cc/2017/conference/-/paper567/public/comment", "forum": "ry7O1ssex", "replyto": "HJyIpR1Xl", "signatures": ["~Shuangfei_Zhai1"], "readers": ["everyone"], "writers": ["~Shuangfei_Zhai1"], "content": {"title": "Re: Questions", "comment": "Hi, Thanks for the comments!\n\n1. The results in Figure 2 are trained without the transition operator, which demonstrates the importance of using a bounded multi-modal energy formulation. We did not try GAN with the transition operator, mainly because we are interested in a broader family of energy based models, which is not specific to GAN. In our framework, this amounts to directly using Equation (7) as the objective, which is actually slightly different from GAN (in the second term of the objective). \n\n2. H tilde is defined in Equation (9) and the following paragraph, which is the same as used in Algorithm 1. It is a differentiable function of either the discriminator or generator parameters, thus can be optimized by gradient descent."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Generative Adversarial Networks as Variational Training of Energy Based Models", "abstract": "In this paper, we study deep generative models for effective unsupervised learning. We propose VGAN, which works by minimizing a variational lower bound of the negative log likelihood (NLL) of an energy based model (EBM), where the model density $p(\\mathbf{x})$ is approximated by a variational distribution $q(\\mathbf{x})$ that is easy to sample from. The training of VGAN takes a two step procedure: given $p(\\mathbf{x})$, $q(\\mathbf{x})$ is updated to maximize the lower bound; $p(\\mathbf{x})$ is then updated one step with samples drawn from $q(\\mathbf{x})$ to decrease the lower bound. VGAN is inspired by the generative adversarial networks (GANs), where $p(\\mathbf{x})$ corresponds to the discriminator and $q(\\mathbf{x})$ corresponds to the generator, but with several notable differences. We hence name our model variational GANs (VGANs). VGAN provides a practical solution to training deep EBMs in high dimensional space, by eliminating the need of MCMC sampling. From this view, we are also able to identify causes to the difficulty of training GANs and propose viable solutions.", "pdf": "/pdf/dd97f6c4ef644bca2ec96f6e3eafa0bd8151a839.pdf", "paperhash": "zhai|generative_adversarial_networks_as_variational_training_of_energy_based_models", "conflicts": ["binghamton.edu"], "keywords": [], "authors": ["Shuangfei Zhai", "Yu Cheng", "Rogerio Feris", "Zhongfei Zhang"], "authorids": ["szhai2@binghamton.edu", "chengyu@us.ibm.com", "rsferis@us.ibm.com", "zhongfei@cs.binghamton.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287517819, "id": "ICLR.cc/2017/conference/-/paper567/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ry7O1ssex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper567/reviewers", "ICLR.cc/2017/conference/paper567/areachairs"], "cdate": 1485287517819}}}, {"tddate": null, "tmdate": 1480744262871, "tcdate": 1480744262867, "number": 2, "id": "HJyIpR1Xl", "invitation": "ICLR.cc/2017/conference/-/paper567/pre-review/question", "forum": "ry7O1ssex", "replyto": "ry7O1ssex", "signatures": ["ICLR.cc/2017/conference/paper567/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper567/AnonReviewer3"], "content": {"title": "Questions", "question": "1. You use a transition operator to serve as a generator, and it appears that this choice contributes significantly to the results. To understand the role of the transition operator better, have you tried either training your model without transition operator, or training a regular GAN with transition operator?\n\n2. What is H tilde mentioned in algorithm 2? Is it a parametrized model? How do you optimize it?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Generative Adversarial Networks as Variational Training of Energy Based Models", "abstract": "In this paper, we study deep generative models for effective unsupervised learning. We propose VGAN, which works by minimizing a variational lower bound of the negative log likelihood (NLL) of an energy based model (EBM), where the model density $p(\\mathbf{x})$ is approximated by a variational distribution $q(\\mathbf{x})$ that is easy to sample from. The training of VGAN takes a two step procedure: given $p(\\mathbf{x})$, $q(\\mathbf{x})$ is updated to maximize the lower bound; $p(\\mathbf{x})$ is then updated one step with samples drawn from $q(\\mathbf{x})$ to decrease the lower bound. VGAN is inspired by the generative adversarial networks (GANs), where $p(\\mathbf{x})$ corresponds to the discriminator and $q(\\mathbf{x})$ corresponds to the generator, but with several notable differences. We hence name our model variational GANs (VGANs). VGAN provides a practical solution to training deep EBMs in high dimensional space, by eliminating the need of MCMC sampling. From this view, we are also able to identify causes to the difficulty of training GANs and propose viable solutions.", "pdf": "/pdf/dd97f6c4ef644bca2ec96f6e3eafa0bd8151a839.pdf", "paperhash": "zhai|generative_adversarial_networks_as_variational_training_of_energy_based_models", "conflicts": ["binghamton.edu"], "keywords": [], "authors": ["Shuangfei Zhai", "Yu Cheng", "Rogerio Feris", "Zhongfei Zhang"], "authorids": ["szhai2@binghamton.edu", "chengyu@us.ibm.com", "rsferis@us.ibm.com", "zhongfei@cs.binghamton.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481983553310, "id": "ICLR.cc/2017/conference/-/paper567/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper567/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper567/AnonReviewer2", "ICLR.cc/2017/conference/paper567/AnonReviewer3", "ICLR.cc/2017/conference/paper567/AnonReviewer1"], "reply": {"forum": "ry7O1ssex", "replyto": "ry7O1ssex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper567/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper567/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481983553310}}}, {"tddate": null, "tmdate": 1480691388692, "tcdate": 1480691388688, "number": 1, "id": "ryB6CWyXg", "invitation": "ICLR.cc/2017/conference/-/paper567/pre-review/question", "forum": "ry7O1ssex", "replyto": "ry7O1ssex", "signatures": ["ICLR.cc/2017/conference/paper567/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper567/AnonReviewer2"], "content": {"title": "Learning curve", "question": "Do you have learning the curves from multiple runs? That could help comparing the stability of the models."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Generative Adversarial Networks as Variational Training of Energy Based Models", "abstract": "In this paper, we study deep generative models for effective unsupervised learning. We propose VGAN, which works by minimizing a variational lower bound of the negative log likelihood (NLL) of an energy based model (EBM), where the model density $p(\\mathbf{x})$ is approximated by a variational distribution $q(\\mathbf{x})$ that is easy to sample from. The training of VGAN takes a two step procedure: given $p(\\mathbf{x})$, $q(\\mathbf{x})$ is updated to maximize the lower bound; $p(\\mathbf{x})$ is then updated one step with samples drawn from $q(\\mathbf{x})$ to decrease the lower bound. VGAN is inspired by the generative adversarial networks (GANs), where $p(\\mathbf{x})$ corresponds to the discriminator and $q(\\mathbf{x})$ corresponds to the generator, but with several notable differences. We hence name our model variational GANs (VGANs). VGAN provides a practical solution to training deep EBMs in high dimensional space, by eliminating the need of MCMC sampling. From this view, we are also able to identify causes to the difficulty of training GANs and propose viable solutions.", "pdf": "/pdf/dd97f6c4ef644bca2ec96f6e3eafa0bd8151a839.pdf", "paperhash": "zhai|generative_adversarial_networks_as_variational_training_of_energy_based_models", "conflicts": ["binghamton.edu"], "keywords": [], "authors": ["Shuangfei Zhai", "Yu Cheng", "Rogerio Feris", "Zhongfei Zhang"], "authorids": ["szhai2@binghamton.edu", "chengyu@us.ibm.com", "rsferis@us.ibm.com", "zhongfei@cs.binghamton.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481983553310, "id": "ICLR.cc/2017/conference/-/paper567/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper567/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper567/AnonReviewer2", "ICLR.cc/2017/conference/paper567/AnonReviewer3", "ICLR.cc/2017/conference/paper567/AnonReviewer1"], "reply": {"forum": "ry7O1ssex", "replyto": "ry7O1ssex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper567/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper567/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481983553310}}}, {"tddate": null, "tmdate": 1480008863003, "tcdate": 1480008862982, "number": 3, "id": "rkPoNs4zx", "invitation": "ICLR.cc/2017/conference/-/paper567/public/comment", "forum": "ry7O1ssex", "replyto": "BJQQl9Vfe", "signatures": ["~Shuangfei_Zhai1"], "readers": ["everyone"], "writers": ["~Shuangfei_Zhai1"], "content": {"title": "Respond to Unboundedness of RBM free energy", "comment": "Hi Matt, \n\nThanks for the comments! \n\nIt is true that for an RBM with binary visible units the free energy only takes on a finite number of values. However, each of the values can be infinitely small (or large) by varying the parameters of the RBM. For example, given an arbitrary input $\\mathbf{x}$, the energy $E(\\mathbf{x})$ can grow to -Inf by letting each \\mathbf{b}_{h,j} grow to +Inf. And you can do the same thing to an RBM with real-valued inputs to verify that it's unbounded as well. This is bad for the variational objective in Equation (5), as it minimizes the energy of the data distribution w.r.t. the parameters of the EBM, which is unbounded from below by design. Doing so will eventually lead the model to assign infinitely low energy to both samples from the data distribution and samples from the variational distribution, which we have experienced in our experiments. \n\nHope this clarifies your questions,\nShuangfei"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Generative Adversarial Networks as Variational Training of Energy Based Models", "abstract": "In this paper, we study deep generative models for effective unsupervised learning. We propose VGAN, which works by minimizing a variational lower bound of the negative log likelihood (NLL) of an energy based model (EBM), where the model density $p(\\mathbf{x})$ is approximated by a variational distribution $q(\\mathbf{x})$ that is easy to sample from. The training of VGAN takes a two step procedure: given $p(\\mathbf{x})$, $q(\\mathbf{x})$ is updated to maximize the lower bound; $p(\\mathbf{x})$ is then updated one step with samples drawn from $q(\\mathbf{x})$ to decrease the lower bound. VGAN is inspired by the generative adversarial networks (GANs), where $p(\\mathbf{x})$ corresponds to the discriminator and $q(\\mathbf{x})$ corresponds to the generator, but with several notable differences. We hence name our model variational GANs (VGANs). VGAN provides a practical solution to training deep EBMs in high dimensional space, by eliminating the need of MCMC sampling. From this view, we are also able to identify causes to the difficulty of training GANs and propose viable solutions.", "pdf": "/pdf/dd97f6c4ef644bca2ec96f6e3eafa0bd8151a839.pdf", "paperhash": "zhai|generative_adversarial_networks_as_variational_training_of_energy_based_models", "conflicts": ["binghamton.edu"], "keywords": [], "authors": ["Shuangfei Zhai", "Yu Cheng", "Rogerio Feris", "Zhongfei Zhang"], "authorids": ["szhai2@binghamton.edu", "chengyu@us.ibm.com", "rsferis@us.ibm.com", "zhongfei@cs.binghamton.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287517819, "id": "ICLR.cc/2017/conference/-/paper567/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ry7O1ssex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper567/reviewers", "ICLR.cc/2017/conference/paper567/areachairs"], "cdate": 1485287517819}}}, {"tddate": null, "tmdate": 1480003611263, "tcdate": 1480003611258, "number": 2, "id": "BJQQl9Vfe", "invitation": "ICLR.cc/2017/conference/-/paper567/public/comment", "forum": "ry7O1ssex", "replyto": "ry7O1ssex", "signatures": ["~Matt_Graham1"], "readers": ["everyone"], "writers": ["~Matt_Graham1"], "content": {"title": "Unboundedness of RBM free energy", "comment": "In section 4 when comparing VGANs to GANs, in the 'parameterization of energy' point you claim\n\n> An RBM has free energy $E(\\mathbf{x}) = -\\mathbf{b}_{\\mathbf{v}} - \\sum_{j=1}^K \\log\\left(1 + e^{\\mathbf{W}^T_j\\mathbf{x} + \\mathbf{b}_{h,j}\\right)$ which is unbounded.\n\nIt is a bit unclear to me what you mean here. The expression you give is for the free energy of RBM with binary hidden and visible units i.e. $x_i \\in \\lbrace 0,\\,1 \\rbrace$. As a function of a binary vector $\\mathbf{x}$ the energy $E$ is bounded (assuming finite parameters) as it can only take on a finite number of values. For a real-valued vector $\\mathbf{x}$ the energy quoted would be unbounded but as this is defined as the free energy for a binary state this is not a very interpretable statement. It would seem more natural to use the free energy of a RBM with Gaussian visible units, which is defined on a real valued vector $\\mathbf{x}$ and due to the quadratic term in $\\mathbf{x}$ is bounded from below. Or am I missing something here?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Generative Adversarial Networks as Variational Training of Energy Based Models", "abstract": "In this paper, we study deep generative models for effective unsupervised learning. We propose VGAN, which works by minimizing a variational lower bound of the negative log likelihood (NLL) of an energy based model (EBM), where the model density $p(\\mathbf{x})$ is approximated by a variational distribution $q(\\mathbf{x})$ that is easy to sample from. The training of VGAN takes a two step procedure: given $p(\\mathbf{x})$, $q(\\mathbf{x})$ is updated to maximize the lower bound; $p(\\mathbf{x})$ is then updated one step with samples drawn from $q(\\mathbf{x})$ to decrease the lower bound. VGAN is inspired by the generative adversarial networks (GANs), where $p(\\mathbf{x})$ corresponds to the discriminator and $q(\\mathbf{x})$ corresponds to the generator, but with several notable differences. We hence name our model variational GANs (VGANs). VGAN provides a practical solution to training deep EBMs in high dimensional space, by eliminating the need of MCMC sampling. From this view, we are also able to identify causes to the difficulty of training GANs and propose viable solutions.", "pdf": "/pdf/dd97f6c4ef644bca2ec96f6e3eafa0bd8151a839.pdf", "paperhash": "zhai|generative_adversarial_networks_as_variational_training_of_energy_based_models", "conflicts": ["binghamton.edu"], "keywords": [], "authors": ["Shuangfei Zhai", "Yu Cheng", "Rogerio Feris", "Zhongfei Zhang"], "authorids": ["szhai2@binghamton.edu", "chengyu@us.ibm.com", "rsferis@us.ibm.com", "zhongfei@cs.binghamton.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287517819, "id": "ICLR.cc/2017/conference/-/paper567/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ry7O1ssex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper567/reviewers", "ICLR.cc/2017/conference/paper567/areachairs"], "cdate": 1485287517819}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1478371684683, "tcdate": 1478369130727, "number": 567, "id": "ry7O1ssex", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "ry7O1ssex", "signatures": ["~Shuangfei_Zhai1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Generative Adversarial Networks as Variational Training of Energy Based Models", "abstract": "In this paper, we study deep generative models for effective unsupervised learning. We propose VGAN, which works by minimizing a variational lower bound of the negative log likelihood (NLL) of an energy based model (EBM), where the model density $p(\\mathbf{x})$ is approximated by a variational distribution $q(\\mathbf{x})$ that is easy to sample from. The training of VGAN takes a two step procedure: given $p(\\mathbf{x})$, $q(\\mathbf{x})$ is updated to maximize the lower bound; $p(\\mathbf{x})$ is then updated one step with samples drawn from $q(\\mathbf{x})$ to decrease the lower bound. VGAN is inspired by the generative adversarial networks (GANs), where $p(\\mathbf{x})$ corresponds to the discriminator and $q(\\mathbf{x})$ corresponds to the generator, but with several notable differences. We hence name our model variational GANs (VGANs). VGAN provides a practical solution to training deep EBMs in high dimensional space, by eliminating the need of MCMC sampling. From this view, we are also able to identify causes to the difficulty of training GANs and propose viable solutions.", "pdf": "/pdf/dd97f6c4ef644bca2ec96f6e3eafa0bd8151a839.pdf", "paperhash": "zhai|generative_adversarial_networks_as_variational_training_of_energy_based_models", "conflicts": ["binghamton.edu"], "keywords": [], "authors": ["Shuangfei Zhai", "Yu Cheng", "Rogerio Feris", "Zhongfei Zhang"], "authorids": ["szhai2@binghamton.edu", "chengyu@us.ibm.com", "rsferis@us.ibm.com", "zhongfei@cs.binghamton.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 17, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}], "count": 18}