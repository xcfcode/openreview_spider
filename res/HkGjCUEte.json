{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1516003659552, "tcdate": 1487330970222, "number": 80, "id": "HkGjCUEte", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "HkGjCUEte", "signatures": ["~Zhenzhou_Wu1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "HiNet : Hierarchical Classification with Neural Network", "abstract": "Traditionally, classifying large hierarchical labels with more than 10000 distinct traces can only be achieved with flatten labels. Although flatten labels is feasible, it misses the hierarchical information in the labels. Hierarchical models like HSVM by \\cite{vural2004hierarchical} becomes impossible to train because of the sheer number of SVMs in the whole architecture. We developed a hierarchical architecture based on neural networks that is simple to train. Also, we derived an inference algorithm that can efficiently infer the MAP (maximum a posteriori) trace guaranteed by our theorems. Furthermore, the complexity of the model is only $O(n^2)$ compared to $O(n^h)$ in a flatten model, where $h$ is the height of the hierarchy.", "pdf": "/pdf/7dd7b83a8cb0a5723f2a90068e61caf5c3697f7c.pdf", "paperhash": "wu|hinet_hierarchical_classification_with_neural_network", "conflicts": ["sap.com"], "keywords": [], "authors": ["Zhenzhou Wu", "Sean Saito"], "authorids": ["zhenzhou.wu@sap.com", "sean.saito@u.yale-nus.edu.sg"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}, {"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028586203, "tcdate": 1490028586203, "number": 1, "id": "ryf4_F6sx", "invitation": "ICLR.cc/2017/workshop/-/paper80/acceptance", "forum": "HkGjCUEte", "replyto": "HkGjCUEte", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "HiNet : Hierarchical Classification with Neural Network", "abstract": "Traditionally, classifying large hierarchical labels with more than 10000 distinct traces can only be achieved with flatten labels. Although flatten labels is feasible, it misses the hierarchical information in the labels. Hierarchical models like HSVM by \\cite{vural2004hierarchical} becomes impossible to train because of the sheer number of SVMs in the whole architecture. We developed a hierarchical architecture based on neural networks that is simple to train. Also, we derived an inference algorithm that can efficiently infer the MAP (maximum a posteriori) trace guaranteed by our theorems. Furthermore, the complexity of the model is only $O(n^2)$ compared to $O(n^h)$ in a flatten model, where $h$ is the height of the hierarchy.", "pdf": "/pdf/7dd7b83a8cb0a5723f2a90068e61caf5c3697f7c.pdf", "paperhash": "wu|hinet_hierarchical_classification_with_neural_network", "conflicts": ["sap.com"], "keywords": [], "authors": ["Zhenzhou Wu", "Sean Saito"], "authorids": ["zhenzhou.wu@sap.com", "sean.saito@u.yale-nus.edu.sg"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028586755, "id": "ICLR.cc/2017/workshop/-/paper80/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "HkGjCUEte", "replyto": "HkGjCUEte", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028586755}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489374160992, "tcdate": 1489373948123, "number": 2, "id": "Sy4bjKQox", "invitation": "ICLR.cc/2017/workshop/-/paper80/public/comment", "forum": "HkGjCUEte", "replyto": "B1MxZImoe", "signatures": ["~Zhenzhou_Wu1"], "readers": ["everyone"], "writers": ["~Zhenzhou_Wu1"], "content": {"title": "Reply", "comment": "[Question] While this paper is trying to address an interesting subject, the paper is very confusing to read and needs a lot of improvement in explaining its content before it can get published.\n\n[Answer] Thanks for the feedback, I will definitely try to improve the presentation so that it's easier to understand. However I hope to re-emphasize the contributions in the paper in case it get missed out because of the presentation.\n\n1. By simply appending a stop neuron at each level, we have a very simple neural network architecture that can model any kind of hierarchy and traces of any length. \n2. It reduces space complexity from O(n^h) of a flatten classifier to O(h n^2) in HiNet where h is the maximum height of the hierarchy, and n is the average number of classes in one level. This may not be significant for small depth, but difference is huge if h is large. for h=10 and n=100, traditional flatten model will need 10^20 parameters while HiNet only requires 10^5. And compare HiNet to HSVM, the memory saving is even more significant, given that each SVM takes up O(number of support vectors ^2) which can be huge.\n3. It reduces the inference time (downpour algorithm) also from O(n^h) to O(hn^2). The traditional way of finding the trace with the maximum probability is to look at all traces which is of order O(n^h), however downpour algorithm significantly reduce the time complexity to O(hn^2) by greedily calculating the MAP at each level, thinking a time reduction of (10^20 to 10^5 for h=10 and n=100).\n4. In a few years time as we deal with large number of classes (typically hierarchical one), this neural network based model will have significant advantage over traditional node-based models.\n\n\n[Question] \"A combined cost allows travelling of information across levels which is equivalent to transfer learning between levels\". What happen to the results when the network architecture does allow shared cost function, but while keeping the downpour algorithm intact? \n\n[Answer] Not too clear about your question. The combined cost is used during training, while the dourpour algorithm is used for inference after the model is trained. Our results have shown that using a combined cost improves the prediction accuracy for each individual level compared to a separate cost for each level. We think this may be due to the transfer of knowledge of the dependences between levels by the cost function \n\n[Question] The experimental section should briefly mention the task description and put the results into context w.r.t alternative methods (other than just the flatten network)\n[Answer] Totally agree, we tried HSVM on a subset of features of DMOZ in order to speed up the training, but the results was pretty far off from both flatten and hiNet and that's why we didn't put it in. On the full feature which has about 80 dimensions, the training is impossible (thinking of the number of parameters in each svm which is proportional to the input dimension), takes a month and it's still running. We are also thinking of comparing them in a simpler hierarchy which has less than 100 unique traces. But since the workshop paper is more of presenting a very interesting idea rather than getting the full results, that's why we submitted here. I am humble to hear more feedbacks from you.\n\n\n "}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "HiNet : Hierarchical Classification with Neural Network", "abstract": "Traditionally, classifying large hierarchical labels with more than 10000 distinct traces can only be achieved with flatten labels. Although flatten labels is feasible, it misses the hierarchical information in the labels. Hierarchical models like HSVM by \\cite{vural2004hierarchical} becomes impossible to train because of the sheer number of SVMs in the whole architecture. We developed a hierarchical architecture based on neural networks that is simple to train. Also, we derived an inference algorithm that can efficiently infer the MAP (maximum a posteriori) trace guaranteed by our theorems. Furthermore, the complexity of the model is only $O(n^2)$ compared to $O(n^h)$ in a flatten model, where $h$ is the height of the hierarchy.", "pdf": "/pdf/7dd7b83a8cb0a5723f2a90068e61caf5c3697f7c.pdf", "paperhash": "wu|hinet_hierarchical_classification_with_neural_network", "conflicts": ["sap.com"], "keywords": [], "authors": ["Zhenzhou Wu", "Sean Saito"], "authorids": ["zhenzhou.wu@sap.com", "sean.saito@u.yale-nus.edu.sg"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487330971038, "tcdate": 1487330971038, "id": "ICLR.cc/2017/workshop/-/paper80/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper80/reviewers"], "reply": {"forum": "HkGjCUEte", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487330971038}}}, {"tddate": null, "tmdate": 1489359081776, "tcdate": 1489359081776, "number": 2, "id": "B1MxZImoe", "invitation": "ICLR.cc/2017/workshop/-/paper80/official/review", "forum": "HkGjCUEte", "replyto": "HkGjCUEte", "signatures": ["ICLR.cc/2017/workshop/paper80/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper80/AnonReviewer2"], "content": {"title": "hierarchical classification", "rating": "5: Marginally below acceptance threshold", "review": "- This paper investigates hierarchical classification using deep nets during learning and a greedy MAP procedure during inference. \n\n- While this paper is trying to address an interesting subject, the paper is very confusing to read and needs a lot of improvement in explaining its content before it can get published.\n\n- \"A combined cost allows travelling of information across levels which is equivalent to transfer learning between levels\". What happen to the results when the network architecture does allow shared cost function, but while keeping the downpour algorithm intact? \n\n-  The experimental section should briefly mention the task description and put the results into context w.r.t alternative methods (other than just the flatten network)\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "HiNet : Hierarchical Classification with Neural Network", "abstract": "Traditionally, classifying large hierarchical labels with more than 10000 distinct traces can only be achieved with flatten labels. Although flatten labels is feasible, it misses the hierarchical information in the labels. Hierarchical models like HSVM by \\cite{vural2004hierarchical} becomes impossible to train because of the sheer number of SVMs in the whole architecture. We developed a hierarchical architecture based on neural networks that is simple to train. Also, we derived an inference algorithm that can efficiently infer the MAP (maximum a posteriori) trace guaranteed by our theorems. Furthermore, the complexity of the model is only $O(n^2)$ compared to $O(n^h)$ in a flatten model, where $h$ is the height of the hierarchy.", "pdf": "/pdf/7dd7b83a8cb0a5723f2a90068e61caf5c3697f7c.pdf", "paperhash": "wu|hinet_hierarchical_classification_with_neural_network", "conflicts": ["sap.com"], "keywords": [], "authors": ["Zhenzhou Wu", "Sean Saito"], "authorids": ["zhenzhou.wu@sap.com", "sean.saito@u.yale-nus.edu.sg"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489359082606, "id": "ICLR.cc/2017/workshop/-/paper80/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper80/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper80/AnonReviewer1", "ICLR.cc/2017/workshop/paper80/AnonReviewer2"], "reply": {"forum": "HkGjCUEte", "replyto": "HkGjCUEte", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper80/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper80/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489359082606}}}, {"tddate": null, "tmdate": 1489338165696, "tcdate": 1489338165696, "number": 1, "id": "HJRNyZ7ie", "invitation": "ICLR.cc/2017/workshop/-/paper80/public/comment", "forum": "HkGjCUEte", "replyto": "B1dhCcgjl", "signatures": ["~Zhenzhou_Wu1"], "readers": ["everyone"], "writers": ["~Zhenzhou_Wu1"], "content": {"title": "Reply", "comment": "Hi Reviewer,\n\nI will like to make the explanation clearer, but due to the page limitation, we only presented the core algorithm. So to answer all your questions. I will like to illustrate the objective of the solution with an example. Typically in hierarchical labels, given an input X, we can have y1-y2-y3 of conditional labels whereby subsequent levels is conditioned on the previous level, for example, a department under a school, a school under a faculty, the length of the hierarchy can be different for different X. For large hierarchical labels with for example more than 10k possible traces. Traditionally, we  can flatten the labels which then treats similar trace like a-b-c and a-b-d as totally different, or we can put a classifier at each decision node, but it will require a sheer number of classifiers and takes a very long time to train. Here, we propose a much simpler solution with neural network that models the hierarchy of the label and at the same time is super quick to train. Below are the answers to your questions. Let me know if I can help you to understand the paper better, because I don't want it to be missed because my reader don't understand the paper.\n\n[Question] I did not completely understand how the neural network captures the hierarchy of the structure and what each depth is presenting?\n\n[Answer] It's neural network. Each level of the network predicts one level of the hierarchy. Given two similar inputs X1 with a-b-c and X2 with a-b-d, since the networks predict each level independently, so a-b-c and a-b-d will be treated to be closer to each other in a HiNet than if you treat them as two different classes in a flattened label. In the network, we append an additional stop label to a trace. So the actual label for X1 is a-b-c-stop and X2 is a-b-d-stop. This is necessary because the traces can be of different length. Each depth of the network represents each level of the hierarchy label\n  \n[Question] Does each nn depth correspond to a level in the hierarchy? How do the nodes indicate the predictions and which nodes are responsible for prediction?\n\n[Answer] Yes. Each node represent a label in the hierarchy. During inference, the label for the MAP trace return by the network will contain all the nodes that the MAP trace has passed through. For example, let's say we have a trace passes through the 2nd neuron in first level, 3rd neuron in second level, and 6th neuron in third level and finally it ends in stop neuron in the 4th level, then the returned label will be a2-b3-c6-stop.\n\n[Question] I did not understand how the neural network architecture is designed? Is it a feedforward as shown in the figures? How does it relate to the hierarchy of classes?\n\n[Answer] It is indeed a feedforward architecture. However one point that I'm afraid the reader may confuse is that they may treat the connection between layers as weights, no, the connections are not weights, the connections are just an indication that there is a relationship between a child neuron and a parent neuron.\n\nThere are two different paradigm for training and inference, during training as show in figure 2a, we want the model to predict the label at each level given the input X trained with a combined cost function. A combined cost function allows conditional information between levels to be transferred. \nDuring inference, as shown by figure2b, after the model gives a probabilistic score of labels at different levels, the objective of the inference algorithm is to find the MAP trace which is p(a, b, c .. stop) > p(every other trace) which is derived using the downpour algorithm and the MAP trace derived by the downpour algorithm is theoretically guaranteed by the three theorems.\n\n[Question] The authors mention that training hierarchical SVM's are difficult. Can they mention in what sense are these networks difficult to train?\n\n[Answer] The training takes a super long time, because there are just sheer number of svms to train (>10000), we run a hsvm for more than a month and it's still running. Also the amount of memory it consumes is also amazing when we want to train them in parallel.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "HiNet : Hierarchical Classification with Neural Network", "abstract": "Traditionally, classifying large hierarchical labels with more than 10000 distinct traces can only be achieved with flatten labels. Although flatten labels is feasible, it misses the hierarchical information in the labels. Hierarchical models like HSVM by \\cite{vural2004hierarchical} becomes impossible to train because of the sheer number of SVMs in the whole architecture. We developed a hierarchical architecture based on neural networks that is simple to train. Also, we derived an inference algorithm that can efficiently infer the MAP (maximum a posteriori) trace guaranteed by our theorems. Furthermore, the complexity of the model is only $O(n^2)$ compared to $O(n^h)$ in a flatten model, where $h$ is the height of the hierarchy.", "pdf": "/pdf/7dd7b83a8cb0a5723f2a90068e61caf5c3697f7c.pdf", "paperhash": "wu|hinet_hierarchical_classification_with_neural_network", "conflicts": ["sap.com"], "keywords": [], "authors": ["Zhenzhou Wu", "Sean Saito"], "authorids": ["zhenzhou.wu@sap.com", "sean.saito@u.yale-nus.edu.sg"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487330971038, "tcdate": 1487330971038, "id": "ICLR.cc/2017/workshop/-/paper80/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper80/reviewers"], "reply": {"forum": "HkGjCUEte", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487330971038}}}, {"tddate": null, "tmdate": 1489182384136, "tcdate": 1489182384136, "number": 1, "id": "B1dhCcgjl", "invitation": "ICLR.cc/2017/workshop/-/paper80/official/review", "forum": "HkGjCUEte", "replyto": "HkGjCUEte", "signatures": ["ICLR.cc/2017/workshop/paper80/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper80/AnonReviewer1"], "content": {"title": "This paper proposes a neural network based approach for hierarchical classification", "rating": "5: Marginally below acceptance threshold", "review": "In this paper, the authors propose a hierarchical classification method using neural networks. Traditional techniques that consider the hierarchy while training are inefficient to train. Other techniques aim to solve this inefficiency by flattening the labels, which results in ignoring the hierarchical relationships between the labels.\n\nThe main problem with the paper is with its presentation. I did not completely understand how the neural network captures the hierarchy of the structure and what each depth is presenting? Does each nn depth correspond to a level in the hierarchy? How do the nodes indicate the predictions and which nodes are responsible for prediction?\n\nI did not understand how the neural network architecture is designed? Is it a feedforward as shown in the figures? How does it relate to the hierarchy of classes?\n\nThe authors mention that training hierarchical SVM's are difficult. Can they mention in what sense are these networks difficult to train?\n\nThere are some typos in the paper. For example:\n  - Page 1, Models such as hierarchical SVM become ...\n  - Section 2.2, Line 3 - The sentence is incomplete \n  - Page 3, We compare HiNet with a Flatten Network which has ...\nand so on\n\nI suggest re-writing the paper to make it more clear and understandable.\n\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "HiNet : Hierarchical Classification with Neural Network", "abstract": "Traditionally, classifying large hierarchical labels with more than 10000 distinct traces can only be achieved with flatten labels. Although flatten labels is feasible, it misses the hierarchical information in the labels. Hierarchical models like HSVM by \\cite{vural2004hierarchical} becomes impossible to train because of the sheer number of SVMs in the whole architecture. We developed a hierarchical architecture based on neural networks that is simple to train. Also, we derived an inference algorithm that can efficiently infer the MAP (maximum a posteriori) trace guaranteed by our theorems. Furthermore, the complexity of the model is only $O(n^2)$ compared to $O(n^h)$ in a flatten model, where $h$ is the height of the hierarchy.", "pdf": "/pdf/7dd7b83a8cb0a5723f2a90068e61caf5c3697f7c.pdf", "paperhash": "wu|hinet_hierarchical_classification_with_neural_network", "conflicts": ["sap.com"], "keywords": [], "authors": ["Zhenzhou Wu", "Sean Saito"], "authorids": ["zhenzhou.wu@sap.com", "sean.saito@u.yale-nus.edu.sg"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489359082606, "id": "ICLR.cc/2017/workshop/-/paper80/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper80/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper80/AnonReviewer1", "ICLR.cc/2017/workshop/paper80/AnonReviewer2"], "reply": {"forum": "HkGjCUEte", "replyto": "HkGjCUEte", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper80/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper80/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489359082606}}}], "count": 6}