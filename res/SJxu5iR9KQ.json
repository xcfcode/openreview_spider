{"notes": [{"id": "SJxu5iR9KQ", "original": "S1gaBAo5tQ", "number": 545, "cdate": 1538087823582, "ddate": null, "tcdate": 1538087823582, "tmdate": 1550731157442, "tddate": null, "forum": "SJxu5iR9KQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Learning to Schedule Communication in Multi-agent Reinforcement Learning", "abstract": "Many real-world reinforcement learning tasks require multiple agents to make sequential decisions under the agents\u2019 interaction, where well-coordinated actions among the agents are crucial to achieve the target goal better at these tasks. One way to accelerate the coordination effect is to enable multiple agents to communicate with each other in a distributed manner and behave as a group. In this paper, we study a practical scenario when (i) the communication bandwidth is limited and (ii) the agents share the communication medium so that only a restricted number of agents are able to simultaneously use the medium, as in the state-of-the-art wireless networking standards. This calls for a certain form of communication scheduling. In that regard, we propose a multi-agent deep reinforcement learning framework, called SchedNet, in which agents learn how to schedule themselves, how to encode the messages, and how to select actions based on received messages. SchedNet is capable of deciding which agents should be entitled to broadcasting their (encoded) messages, by learning the importance of each agent\u2019s partially observed information. We evaluate SchedNet against multiple baselines under two different applications, namely, cooperative communication and navigation, and predator-prey. Our experiments show a non-negligible performance gap between SchedNet and other mechanisms such as the ones without communication and with vanilla scheduling methods, e.g., round robin, ranging from 32% to 43%.", "keywords": ["Multi agent reinforcement learning", "deep reinforcement learning", "Communication"], "authorids": ["kdw2139@gmail.com", "swmoon00@gmail.com", "ddhostallero@kaist.ac.kr", "soarhigh0714@gmail.com", "tylee0325@gmail.com", "khson@lanada.kaist.ac.kr", "yiyung@kaist.edu"], "authors": ["Daewoo Kim", "Sangwoo Moon", "David Hostallero", "Wan Ju Kang", "Taeyoung Lee", "Kyunghwan Son", "Yung Yi"], "pdf": "/pdf/13ff35ec770b2bb8d441f616f6de893bfcd3a01f.pdf", "paperhash": "kim|learning_to_schedule_communication_in_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nkim2018learning,\ntitle={Learning to Schedule Communication in Multi-agent Reinforcement Learning},\nauthor={Daewoo Kim and Sangwoo Moon and David Hostallero and Wan Ju Kang and Taeyoung Lee and Kyunghwan Son and Yung Yi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJxu5iR9KQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "Syx9matflE", "original": null, "number": 1, "cdate": 1544883489687, "ddate": null, "tcdate": 1544883489687, "tmdate": 1545354494718, "tddate": null, "forum": "SJxu5iR9KQ", "replyto": "SJxu5iR9KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper545/Meta_Review", "content": {"metareview": "The authors present a learnt scheduling mechanism for managing communications in bandwidth-constrained, contentious multi-agent RL domains. This is well-positioned in the rapidly advancing field of MARL and the contribution of the paper is both novel, interesting, and effective. The agents learn how to schedule themselves, how to encode messages, and how to select actions. The approach is evaluated against several other methods and achieves a good performance increase. The reviewers had concerns regarding the difficulty of evaluating the overall performance and also about how it would fare in more real-world scenarios, but all agree that this paper should be accepted.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Accept (Poster)", "title": "meta-review"}, "signatures": ["ICLR.cc/2019/Conference/Paper545/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper545/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Schedule Communication in Multi-agent Reinforcement Learning", "abstract": "Many real-world reinforcement learning tasks require multiple agents to make sequential decisions under the agents\u2019 interaction, where well-coordinated actions among the agents are crucial to achieve the target goal better at these tasks. One way to accelerate the coordination effect is to enable multiple agents to communicate with each other in a distributed manner and behave as a group. In this paper, we study a practical scenario when (i) the communication bandwidth is limited and (ii) the agents share the communication medium so that only a restricted number of agents are able to simultaneously use the medium, as in the state-of-the-art wireless networking standards. This calls for a certain form of communication scheduling. In that regard, we propose a multi-agent deep reinforcement learning framework, called SchedNet, in which agents learn how to schedule themselves, how to encode the messages, and how to select actions based on received messages. SchedNet is capable of deciding which agents should be entitled to broadcasting their (encoded) messages, by learning the importance of each agent\u2019s partially observed information. We evaluate SchedNet against multiple baselines under two different applications, namely, cooperative communication and navigation, and predator-prey. Our experiments show a non-negligible performance gap between SchedNet and other mechanisms such as the ones without communication and with vanilla scheduling methods, e.g., round robin, ranging from 32% to 43%.", "keywords": ["Multi agent reinforcement learning", "deep reinforcement learning", "Communication"], "authorids": ["kdw2139@gmail.com", "swmoon00@gmail.com", "ddhostallero@kaist.ac.kr", "soarhigh0714@gmail.com", "tylee0325@gmail.com", "khson@lanada.kaist.ac.kr", "yiyung@kaist.edu"], "authors": ["Daewoo Kim", "Sangwoo Moon", "David Hostallero", "Wan Ju Kang", "Taeyoung Lee", "Kyunghwan Son", "Yung Yi"], "pdf": "/pdf/13ff35ec770b2bb8d441f616f6de893bfcd3a01f.pdf", "paperhash": "kim|learning_to_schedule_communication_in_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nkim2018learning,\ntitle={Learning to Schedule Communication in Multi-agent Reinforcement Learning},\nauthor={Daewoo Kim and Sangwoo Moon and David Hostallero and Wan Ju Kang and Taeyoung Lee and Kyunghwan Son and Yung Yi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJxu5iR9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper545/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353177971, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJxu5iR9KQ", "replyto": "SJxu5iR9KQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper545/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper545/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper545/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353177971}}}, {"id": "r1ePGFoonQ", "original": null, "number": 3, "cdate": 1541286159404, "ddate": null, "tcdate": 1541286159404, "tmdate": 1544396981159, "tddate": null, "forum": "SJxu5iR9KQ", "replyto": "SJxu5iR9KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper545/Official_Review", "content": {"title": "Review", "review": "The authors present a study on scheduling multi-agent communication. Specifically, the authors look into cases where agents share the same reward and they are in a partially observable environment, each of them with different observations. The main contribution of this work is that authors provide a model for communication scheduling for dealing with cases where only a certain number of agents is allowed to communicate.\n\nThe paper is very clear, positions the work very well in the literature of MARL and communication. The authors perform experiments in two environments and include a number of reasonable baselines (e.g., adapted DIAL for top(k))  as well as the full-communication upper bound. \nThe authors moreover provide a nice analysis on the messages in the predator-pray experiment.\n\nMy only concern is that authors report \"DIAL(1) performs worse than SchedNet-Top(1)\".  However, Figure 3a clearly shows that Dial(1) to be within the variance of Sched-Top(1) -- from this it's not clear that the null hypothesis can be rejected. The authors should probably verify this with a statistical test cause at the moment their claim is unsupported. Moreover, why Figure 3c does not contain the same models as Figure 3a (e.g., DIAL appears to be missing)?\n\n\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper545/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Learning to Schedule Communication in Multi-agent Reinforcement Learning", "abstract": "Many real-world reinforcement learning tasks require multiple agents to make sequential decisions under the agents\u2019 interaction, where well-coordinated actions among the agents are crucial to achieve the target goal better at these tasks. One way to accelerate the coordination effect is to enable multiple agents to communicate with each other in a distributed manner and behave as a group. In this paper, we study a practical scenario when (i) the communication bandwidth is limited and (ii) the agents share the communication medium so that only a restricted number of agents are able to simultaneously use the medium, as in the state-of-the-art wireless networking standards. This calls for a certain form of communication scheduling. In that regard, we propose a multi-agent deep reinforcement learning framework, called SchedNet, in which agents learn how to schedule themselves, how to encode the messages, and how to select actions based on received messages. SchedNet is capable of deciding which agents should be entitled to broadcasting their (encoded) messages, by learning the importance of each agent\u2019s partially observed information. We evaluate SchedNet against multiple baselines under two different applications, namely, cooperative communication and navigation, and predator-prey. Our experiments show a non-negligible performance gap between SchedNet and other mechanisms such as the ones without communication and with vanilla scheduling methods, e.g., round robin, ranging from 32% to 43%.", "keywords": ["Multi agent reinforcement learning", "deep reinforcement learning", "Communication"], "authorids": ["kdw2139@gmail.com", "swmoon00@gmail.com", "ddhostallero@kaist.ac.kr", "soarhigh0714@gmail.com", "tylee0325@gmail.com", "khson@lanada.kaist.ac.kr", "yiyung@kaist.edu"], "authors": ["Daewoo Kim", "Sangwoo Moon", "David Hostallero", "Wan Ju Kang", "Taeyoung Lee", "Kyunghwan Son", "Yung Yi"], "pdf": "/pdf/13ff35ec770b2bb8d441f616f6de893bfcd3a01f.pdf", "paperhash": "kim|learning_to_schedule_communication_in_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nkim2018learning,\ntitle={Learning to Schedule Communication in Multi-agent Reinforcement Learning},\nauthor={Daewoo Kim and Sangwoo Moon and David Hostallero and Wan Ju Kang and Taeyoung Lee and Kyunghwan Son and Yung Yi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJxu5iR9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper545/Official_Review", "cdate": 1542234437064, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SJxu5iR9KQ", "replyto": "SJxu5iR9KQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper545/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335748451, "tmdate": 1552335748451, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper545/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SJxC36iFkE", "original": null, "number": 9, "cdate": 1544302005651, "ddate": null, "tcdate": 1544302005651, "tmdate": 1544302005651, "tddate": null, "forum": "SJxu5iR9KQ", "replyto": "r1l5FjajTm", "invitation": "ICLR.cc/2019/Conference/-/Paper545/Official_Comment", "content": {"title": "upward revision based on response", "comment": "Thank you for taking the time to write a detailed response further clarifying your contributions and outlining a couple of appropriate real-world evaluation directions for future work.  Taking this additional information into account I've adjust my review score from 6 to 7."}, "signatures": ["ICLR.cc/2019/Conference/Paper545/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper545/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper545/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Schedule Communication in Multi-agent Reinforcement Learning", "abstract": "Many real-world reinforcement learning tasks require multiple agents to make sequential decisions under the agents\u2019 interaction, where well-coordinated actions among the agents are crucial to achieve the target goal better at these tasks. One way to accelerate the coordination effect is to enable multiple agents to communicate with each other in a distributed manner and behave as a group. In this paper, we study a practical scenario when (i) the communication bandwidth is limited and (ii) the agents share the communication medium so that only a restricted number of agents are able to simultaneously use the medium, as in the state-of-the-art wireless networking standards. This calls for a certain form of communication scheduling. In that regard, we propose a multi-agent deep reinforcement learning framework, called SchedNet, in which agents learn how to schedule themselves, how to encode the messages, and how to select actions based on received messages. SchedNet is capable of deciding which agents should be entitled to broadcasting their (encoded) messages, by learning the importance of each agent\u2019s partially observed information. We evaluate SchedNet against multiple baselines under two different applications, namely, cooperative communication and navigation, and predator-prey. Our experiments show a non-negligible performance gap between SchedNet and other mechanisms such as the ones without communication and with vanilla scheduling methods, e.g., round robin, ranging from 32% to 43%.", "keywords": ["Multi agent reinforcement learning", "deep reinforcement learning", "Communication"], "authorids": ["kdw2139@gmail.com", "swmoon00@gmail.com", "ddhostallero@kaist.ac.kr", "soarhigh0714@gmail.com", "tylee0325@gmail.com", "khson@lanada.kaist.ac.kr", "yiyung@kaist.edu"], "authors": ["Daewoo Kim", "Sangwoo Moon", "David Hostallero", "Wan Ju Kang", "Taeyoung Lee", "Kyunghwan Son", "Yung Yi"], "pdf": "/pdf/13ff35ec770b2bb8d441f616f6de893bfcd3a01f.pdf", "paperhash": "kim|learning_to_schedule_communication_in_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nkim2018learning,\ntitle={Learning to Schedule Communication in Multi-agent Reinforcement Learning},\nauthor={Daewoo Kim and Sangwoo Moon and David Hostallero and Wan Ju Kang and Taeyoung Lee and Kyunghwan Son and Yung Yi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJxu5iR9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper545/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619191, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJxu5iR9KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper545/Authors", "ICLR.cc/2019/Conference/Paper545/Reviewers", "ICLR.cc/2019/Conference/Paper545/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper545/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper545/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper545/Authors|ICLR.cc/2019/Conference/Paper545/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper545/Reviewers", "ICLR.cc/2019/Conference/Paper545/Authors", "ICLR.cc/2019/Conference/Paper545/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619191}}}, {"id": "S1gQoIpqhX", "original": null, "number": 1, "cdate": 1541228187484, "ddate": null, "tcdate": 1541228187484, "tmdate": 1544301701507, "tddate": null, "forum": "SJxu5iR9KQ", "replyto": "SJxu5iR9KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper545/Official_Review", "content": {"title": "Generally ok, but hard to gauge significance of this work", "review": "# overview\nThis paper focuses on multi-agent reinforcement learning tasks that require communication between the agents, and further presupposes that the communication protocol is bandwidth constrained and contentious so that a scheduling mechanism is necessary.  To address this they introduce a new learned weighting scheme based scheduler and distributed actor, centralized critic based architecture which is evaluated on a couple of communication driven multi-agent tasks.\n\nThe two evaluation tasks had their bandwidth artificially constrained, and SchedNet time to convergence was shown to fall somewhere between having no communication and full communication, and somewhat better than a purely round-robin based scheduling scheme, which doesn't seem particularly informative.  From this it is difficult to assess the significance of the contributions.\n\n# pros\n* communication in multi-agent scenarios is an important aspect to consider, and this work shines a spotlight on scenarios in which bandwidth is constrained.\n* general presentation fairly clear and easy to read\n\n# cons\n* Would have been more impactful to focus experiments on real-world scenarios in which bandwidth is constrained and naturally contentious\n\n# other comments\n* pg. 2 related work, suspect you meant to call out Foerster et al 2017b in second reference not Foerster et al 2017a twice.", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper545/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Learning to Schedule Communication in Multi-agent Reinforcement Learning", "abstract": "Many real-world reinforcement learning tasks require multiple agents to make sequential decisions under the agents\u2019 interaction, where well-coordinated actions among the agents are crucial to achieve the target goal better at these tasks. One way to accelerate the coordination effect is to enable multiple agents to communicate with each other in a distributed manner and behave as a group. In this paper, we study a practical scenario when (i) the communication bandwidth is limited and (ii) the agents share the communication medium so that only a restricted number of agents are able to simultaneously use the medium, as in the state-of-the-art wireless networking standards. This calls for a certain form of communication scheduling. In that regard, we propose a multi-agent deep reinforcement learning framework, called SchedNet, in which agents learn how to schedule themselves, how to encode the messages, and how to select actions based on received messages. SchedNet is capable of deciding which agents should be entitled to broadcasting their (encoded) messages, by learning the importance of each agent\u2019s partially observed information. We evaluate SchedNet against multiple baselines under two different applications, namely, cooperative communication and navigation, and predator-prey. Our experiments show a non-negligible performance gap between SchedNet and other mechanisms such as the ones without communication and with vanilla scheduling methods, e.g., round robin, ranging from 32% to 43%.", "keywords": ["Multi agent reinforcement learning", "deep reinforcement learning", "Communication"], "authorids": ["kdw2139@gmail.com", "swmoon00@gmail.com", "ddhostallero@kaist.ac.kr", "soarhigh0714@gmail.com", "tylee0325@gmail.com", "khson@lanada.kaist.ac.kr", "yiyung@kaist.edu"], "authors": ["Daewoo Kim", "Sangwoo Moon", "David Hostallero", "Wan Ju Kang", "Taeyoung Lee", "Kyunghwan Son", "Yung Yi"], "pdf": "/pdf/13ff35ec770b2bb8d441f616f6de893bfcd3a01f.pdf", "paperhash": "kim|learning_to_schedule_communication_in_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nkim2018learning,\ntitle={Learning to Schedule Communication in Multi-agent Reinforcement Learning},\nauthor={Daewoo Kim and Sangwoo Moon and David Hostallero and Wan Ju Kang and Taeyoung Lee and Kyunghwan Son and Yung Yi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJxu5iR9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper545/Official_Review", "cdate": 1542234437064, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SJxu5iR9KQ", "replyto": "SJxu5iR9KQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper545/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335748451, "tmdate": 1552335748451, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper545/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJe2OZVW0X", "original": null, "number": 7, "cdate": 1542697331822, "ddate": null, "tcdate": 1542697331822, "tmdate": 1542697331822, "tddate": null, "forum": "SJxu5iR9KQ", "replyto": "ByxY2Jzg0Q", "invitation": "ICLR.cc/2019/Conference/-/Paper545/Official_Comment", "content": {"title": "Thanks for the comments", "comment": "We agree with the opinion of reviewer\u2019s about partial observability. Although we got the experiment result where RNN does not significantly improve the performance of SchedNet in PP environment, applying RNN to SchedNet can be pivotal in tackling partial observability issue in more complicated environments. We plan to examine SchedNet in more complex and realistic environments as a future topic.\n\nAs mentioned by the reviewer, in many protocols it is not difficult to get the identification information of sender. Even when agent does not use this information, we have seen the impact of scheduling communication. We expect that the usage of the information about sender is helpful for better performance, and that it is worth doing more experiments to gauge its helpfulness. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper545/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper545/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper545/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Schedule Communication in Multi-agent Reinforcement Learning", "abstract": "Many real-world reinforcement learning tasks require multiple agents to make sequential decisions under the agents\u2019 interaction, where well-coordinated actions among the agents are crucial to achieve the target goal better at these tasks. One way to accelerate the coordination effect is to enable multiple agents to communicate with each other in a distributed manner and behave as a group. In this paper, we study a practical scenario when (i) the communication bandwidth is limited and (ii) the agents share the communication medium so that only a restricted number of agents are able to simultaneously use the medium, as in the state-of-the-art wireless networking standards. This calls for a certain form of communication scheduling. In that regard, we propose a multi-agent deep reinforcement learning framework, called SchedNet, in which agents learn how to schedule themselves, how to encode the messages, and how to select actions based on received messages. SchedNet is capable of deciding which agents should be entitled to broadcasting their (encoded) messages, by learning the importance of each agent\u2019s partially observed information. We evaluate SchedNet against multiple baselines under two different applications, namely, cooperative communication and navigation, and predator-prey. Our experiments show a non-negligible performance gap between SchedNet and other mechanisms such as the ones without communication and with vanilla scheduling methods, e.g., round robin, ranging from 32% to 43%.", "keywords": ["Multi agent reinforcement learning", "deep reinforcement learning", "Communication"], "authorids": ["kdw2139@gmail.com", "swmoon00@gmail.com", "ddhostallero@kaist.ac.kr", "soarhigh0714@gmail.com", "tylee0325@gmail.com", "khson@lanada.kaist.ac.kr", "yiyung@kaist.edu"], "authors": ["Daewoo Kim", "Sangwoo Moon", "David Hostallero", "Wan Ju Kang", "Taeyoung Lee", "Kyunghwan Son", "Yung Yi"], "pdf": "/pdf/13ff35ec770b2bb8d441f616f6de893bfcd3a01f.pdf", "paperhash": "kim|learning_to_schedule_communication_in_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nkim2018learning,\ntitle={Learning to Schedule Communication in Multi-agent Reinforcement Learning},\nauthor={Daewoo Kim and Sangwoo Moon and David Hostallero and Wan Ju Kang and Taeyoung Lee and Kyunghwan Son and Yung Yi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJxu5iR9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper545/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619191, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJxu5iR9KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper545/Authors", "ICLR.cc/2019/Conference/Paper545/Reviewers", "ICLR.cc/2019/Conference/Paper545/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper545/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper545/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper545/Authors|ICLR.cc/2019/Conference/Paper545/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper545/Reviewers", "ICLR.cc/2019/Conference/Paper545/Authors", "ICLR.cc/2019/Conference/Paper545/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619191}}}, {"id": "ByxY2Jzg0Q", "original": null, "number": 6, "cdate": 1542623152531, "ddate": null, "tcdate": 1542623152531, "tmdate": 1542623152531, "tddate": null, "forum": "SJxu5iR9KQ", "replyto": "SyxSKqpsTX", "invitation": "ICLR.cc/2019/Conference/-/Paper545/Official_Comment", "content": {"title": "Response", "comment": "Q1. Thank you for the clarification and the update. The results may depend on the complexity of the setting, but in my opinion, anything without short / long term memory is that it's a weak architecture choice for tackling partial observability.\n\nQ2. Thanks.\n\nQ3. Although I find it quite far away, but since the 802.11 was referenced many times, I think that even if messages are broadcasted the identity is a common characteristic for most communication protocols. That coueld also help the performance of the proposed model."}, "signatures": ["ICLR.cc/2019/Conference/Paper545/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper545/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper545/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Schedule Communication in Multi-agent Reinforcement Learning", "abstract": "Many real-world reinforcement learning tasks require multiple agents to make sequential decisions under the agents\u2019 interaction, where well-coordinated actions among the agents are crucial to achieve the target goal better at these tasks. One way to accelerate the coordination effect is to enable multiple agents to communicate with each other in a distributed manner and behave as a group. In this paper, we study a practical scenario when (i) the communication bandwidth is limited and (ii) the agents share the communication medium so that only a restricted number of agents are able to simultaneously use the medium, as in the state-of-the-art wireless networking standards. This calls for a certain form of communication scheduling. In that regard, we propose a multi-agent deep reinforcement learning framework, called SchedNet, in which agents learn how to schedule themselves, how to encode the messages, and how to select actions based on received messages. SchedNet is capable of deciding which agents should be entitled to broadcasting their (encoded) messages, by learning the importance of each agent\u2019s partially observed information. We evaluate SchedNet against multiple baselines under two different applications, namely, cooperative communication and navigation, and predator-prey. Our experiments show a non-negligible performance gap between SchedNet and other mechanisms such as the ones without communication and with vanilla scheduling methods, e.g., round robin, ranging from 32% to 43%.", "keywords": ["Multi agent reinforcement learning", "deep reinforcement learning", "Communication"], "authorids": ["kdw2139@gmail.com", "swmoon00@gmail.com", "ddhostallero@kaist.ac.kr", "soarhigh0714@gmail.com", "tylee0325@gmail.com", "khson@lanada.kaist.ac.kr", "yiyung@kaist.edu"], "authors": ["Daewoo Kim", "Sangwoo Moon", "David Hostallero", "Wan Ju Kang", "Taeyoung Lee", "Kyunghwan Son", "Yung Yi"], "pdf": "/pdf/13ff35ec770b2bb8d441f616f6de893bfcd3a01f.pdf", "paperhash": "kim|learning_to_schedule_communication_in_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nkim2018learning,\ntitle={Learning to Schedule Communication in Multi-agent Reinforcement Learning},\nauthor={Daewoo Kim and Sangwoo Moon and David Hostallero and Wan Ju Kang and Taeyoung Lee and Kyunghwan Son and Yung Yi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJxu5iR9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper545/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619191, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJxu5iR9KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper545/Authors", "ICLR.cc/2019/Conference/Paper545/Reviewers", "ICLR.cc/2019/Conference/Paper545/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper545/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper545/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper545/Authors|ICLR.cc/2019/Conference/Paper545/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper545/Reviewers", "ICLR.cc/2019/Conference/Paper545/Authors", "ICLR.cc/2019/Conference/Paper545/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619191}}}, {"id": "r1l5FjajTm", "original": null, "number": 3, "cdate": 1542343554400, "ddate": null, "tcdate": 1542343554400, "tmdate": 1542343554400, "tddate": null, "forum": "SJxu5iR9KQ", "replyto": "S1gQoIpqhX", "invitation": "ICLR.cc/2019/Conference/-/Paper545/Official_Comment", "content": {"title": "Contribution of our work", "comment": "Apologies for the delay in response. We hope that our answers clarified our contribution and thank the reviewer again.\n\n1. Contribution of our work\n\nWe would like to highlight that our work\u2019s contribution is to provide a need of considering communication scheduling issues in MARL under shared medium contention constraint. To this end, first, we take a shared medium contention issue into consideration in MARL problem which could be a critical problem when deploying the algorithm in the real world, where the communication among the agents are typically done in wireless. Due to the contention, only a limited number of agents are able to simultaneously use the medium. Thus, scheduling is the major concern in this setting. Secondly, we propose the framework to learn how to schedule the communication to achieve the common goal of the application. We employ a weight based scheduling which can cover a various scheduling set, and train a neural network to learn how to set the weight to maximize the common shared reward in MARL. We think that our proposed neural network may not be the best one, which may require to include other smart modules for better performance (e.g., RNN structure and explicit consideration of message senders). However, raising this scheduling issue to the MARL community seems valuable, which, we hope, motivates further future work.   \n\nComparing SchedNet with a round robin scheme is meaningful, since we can see the power of jointly considering the effect of a given task and scheduling, which is actually why we propose SchedNet. The objective of conventional scheduling algorithms in wireless research community is often simple, e.g., fairness or throughput, which may not be closely related to the common reward depending on the application. Round robin based scheduling scheme is the typical example of scheduling algorithms for fairness. The experiment results show that if the agents learn how to schedule themselves by considering their common goal for better cooperation, then they can get a higher common reward.\n\nTo the best of our knowledge, SchedNet is the first work that considers the shared medium issues in MARL, and we expect that there will be other further works considering communication scheduling.\n\n\n2. Implications of our simulation environment\n\nWe think Predator and Prey (PP) environment can be an illustrative example of real-world multi-agent cooperation scenarios. One example is the cooperative task of UAVs (Unmanned Aerial Vehicles) where UAV matches to predator in PP environment. In general, UAVs are equipped with wireless communication, and their communication environment is as follows. \n\nUAVs are controlled by sequential command whose interval is very short (e.g., 0.02 sec). If there are a lot of UAVs, then it is difficult to allow all UAVs to have the chance to communicate, because only one agent can broadcast the message at a time. Given such short decision period, only a limited number of agents can communicate before making a decision of every command.\n\nAnother constraint is about bandwidth. If we use the unlicensed band radio (e.g., WiFi, ZigBee), the bandwidth is limited due to the regulation on the transmission power. This means that the bandwidth constraint has to be taken into account as well as contention constraint when designing the communication protocol. DIAL explicitly considered the bandwidth constraint into their design, but to the best of our knowledge, there is no prior work which tackles the contention constraint.\n\nAlthough our experiments were done with challenging MARL tasks, yet all evaluations were conducted in a simulated environment. We plan to extend our work into real-world scenario such as UAV and real-time robot control applications, which can be more impactful and challenging.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper545/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper545/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper545/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Schedule Communication in Multi-agent Reinforcement Learning", "abstract": "Many real-world reinforcement learning tasks require multiple agents to make sequential decisions under the agents\u2019 interaction, where well-coordinated actions among the agents are crucial to achieve the target goal better at these tasks. One way to accelerate the coordination effect is to enable multiple agents to communicate with each other in a distributed manner and behave as a group. In this paper, we study a practical scenario when (i) the communication bandwidth is limited and (ii) the agents share the communication medium so that only a restricted number of agents are able to simultaneously use the medium, as in the state-of-the-art wireless networking standards. This calls for a certain form of communication scheduling. In that regard, we propose a multi-agent deep reinforcement learning framework, called SchedNet, in which agents learn how to schedule themselves, how to encode the messages, and how to select actions based on received messages. SchedNet is capable of deciding which agents should be entitled to broadcasting their (encoded) messages, by learning the importance of each agent\u2019s partially observed information. We evaluate SchedNet against multiple baselines under two different applications, namely, cooperative communication and navigation, and predator-prey. Our experiments show a non-negligible performance gap between SchedNet and other mechanisms such as the ones without communication and with vanilla scheduling methods, e.g., round robin, ranging from 32% to 43%.", "keywords": ["Multi agent reinforcement learning", "deep reinforcement learning", "Communication"], "authorids": ["kdw2139@gmail.com", "swmoon00@gmail.com", "ddhostallero@kaist.ac.kr", "soarhigh0714@gmail.com", "tylee0325@gmail.com", "khson@lanada.kaist.ac.kr", "yiyung@kaist.edu"], "authors": ["Daewoo Kim", "Sangwoo Moon", "David Hostallero", "Wan Ju Kang", "Taeyoung Lee", "Kyunghwan Son", "Yung Yi"], "pdf": "/pdf/13ff35ec770b2bb8d441f616f6de893bfcd3a01f.pdf", "paperhash": "kim|learning_to_schedule_communication_in_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nkim2018learning,\ntitle={Learning to Schedule Communication in Multi-agent Reinforcement Learning},\nauthor={Daewoo Kim and Sangwoo Moon and David Hostallero and Wan Ju Kang and Taeyoung Lee and Kyunghwan Son and Yung Yi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJxu5iR9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper545/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619191, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJxu5iR9KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper545/Authors", "ICLR.cc/2019/Conference/Paper545/Reviewers", "ICLR.cc/2019/Conference/Paper545/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper545/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper545/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper545/Authors|ICLR.cc/2019/Conference/Paper545/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper545/Reviewers", "ICLR.cc/2019/Conference/Paper545/Authors", "ICLR.cc/2019/Conference/Paper545/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619191}}}, {"id": "SyxSKqpsTX", "original": null, "number": 2, "cdate": 1542343293511, "ddate": null, "tcdate": 1542343293511, "tmdate": 1542343349278, "tddate": null, "forum": "SJxu5iR9KQ", "replyto": "BJxKmdbi2Q", "invitation": "ICLR.cc/2019/Conference/-/Paper545/Official_Comment", "content": {"title": "Partial observability, schedule awareness, and sender identification", "comment": "Apologies for the delay in response. Thank you for your interest in our article\n\nQ1. It's not clear to me how do the authors tackle partial observability without the use of recurrent connections or time-steps? \n\nIn MARL, there are two ways to tackle partial observability issues. First, using RNN structure to indirectly remember the history can alleviate the partial observability issues. Another way is to use the observations of other agents through communication among them.\n \nThe goal of this paper is to show the importance of learning to schedule in a practical communication environment in which the shared medium contention is inevitable. Thus, we focused more on the latter in which efficient communication including scheduling helps to enlarge the observation of a single agent well and mitigates the partial observability issue. We comment that each agent uses its own scheduling history as the input of scheduling weight generator in our experiment. Although we did not use a typical RNN structure, we think that using history information of scheduling helps to address partial observability issues. \n\nEnlarging the observation through communication is somewhat orthogonal to considering temporal correlation, so merging both approaches is possible. \nSchedNet does not assume any specific neural network structure as an intrinsic/critical part of its algorithm, where feed-forward NNs is just one choice. \n\nWe have applied RNN to both IDQN \u2014 in which no communication is allowed \u2014 and SchedNet, and run more experiments in PP environment. We have updated the result in appendix C.2 of our modified manuscript. The results show that the average steps to complete tasks of IDQN with RNN is slightly smaller than that of IDQN with feed-forward network. In this case, RNN helps to improve the performance by tackling the partial observable issue. On the other hand, SchedNet-RNN and SchedNet achieve similar performance. We think that the communication in SchedNet somewhat resolves the partial observable issues. Although applying RNN to SchedNet is not really that helpful in this simple environment, we expect that in a more complex environment, using the recurrent connection is more helpful.\n\n\nQ2. Do the agents know if they were chosen to be broadcasted at the previous timestep? \n\nYes, the agents know whether they are scheduled in the previous timestep in our experiment. This is reasonable because the previous scheduling information of itself is easy to access in a real environment based on some practical a wireless scheduling protocol.  For example in 802.11 WiFi protocol, each agent has a decrementing counter initialized randomly, and check whether the channel is clean (clear channel assessment) when the counter turns zero. From this information, each agent can immediately know if the transmission has been done or it is waiting until the channel is clear. As we mentioned above, this information of scheduling histories is usefully used to determine the scheduling weight, which in turn decides whose observation is important or not.\n\n\nQ3. Many times it's important to know who sent the message, do the agents share this information?\n\nWe agree. In our experiments, we assume that the agents do not know explicitly who sent the message. However, agents are trained to generate different messages which might include the unique feature of each agent implicitly. Thus, if identifying the sender is important, the agent learns to encode the sender\u2019s identification information into a message and to decode the message to extract this information. \n\nAlso, although the agent does not have the explicit knowledge of who sent the message, the scheduler of SchedNet informs whose message is more important via learning weights. Thus, this helps the agents to consider the message from the agents who have the most important observation. \n\nThere can be some cases where the agents\u2019 identifying the sender might be helpful. SchedNet can also be easily extended to this case. The information about sender can be used as the input to the action selector (AS), which can be helpful for better performance. If it is not useful, then AS will be trained not to use this information. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper545/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper545/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper545/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Schedule Communication in Multi-agent Reinforcement Learning", "abstract": "Many real-world reinforcement learning tasks require multiple agents to make sequential decisions under the agents\u2019 interaction, where well-coordinated actions among the agents are crucial to achieve the target goal better at these tasks. One way to accelerate the coordination effect is to enable multiple agents to communicate with each other in a distributed manner and behave as a group. In this paper, we study a practical scenario when (i) the communication bandwidth is limited and (ii) the agents share the communication medium so that only a restricted number of agents are able to simultaneously use the medium, as in the state-of-the-art wireless networking standards. This calls for a certain form of communication scheduling. In that regard, we propose a multi-agent deep reinforcement learning framework, called SchedNet, in which agents learn how to schedule themselves, how to encode the messages, and how to select actions based on received messages. SchedNet is capable of deciding which agents should be entitled to broadcasting their (encoded) messages, by learning the importance of each agent\u2019s partially observed information. We evaluate SchedNet against multiple baselines under two different applications, namely, cooperative communication and navigation, and predator-prey. Our experiments show a non-negligible performance gap between SchedNet and other mechanisms such as the ones without communication and with vanilla scheduling methods, e.g., round robin, ranging from 32% to 43%.", "keywords": ["Multi agent reinforcement learning", "deep reinforcement learning", "Communication"], "authorids": ["kdw2139@gmail.com", "swmoon00@gmail.com", "ddhostallero@kaist.ac.kr", "soarhigh0714@gmail.com", "tylee0325@gmail.com", "khson@lanada.kaist.ac.kr", "yiyung@kaist.edu"], "authors": ["Daewoo Kim", "Sangwoo Moon", "David Hostallero", "Wan Ju Kang", "Taeyoung Lee", "Kyunghwan Son", "Yung Yi"], "pdf": "/pdf/13ff35ec770b2bb8d441f616f6de893bfcd3a01f.pdf", "paperhash": "kim|learning_to_schedule_communication_in_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nkim2018learning,\ntitle={Learning to Schedule Communication in Multi-agent Reinforcement Learning},\nauthor={Daewoo Kim and Sangwoo Moon and David Hostallero and Wan Ju Kang and Taeyoung Lee and Kyunghwan Son and Yung Yi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJxu5iR9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper545/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619191, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJxu5iR9KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper545/Authors", "ICLR.cc/2019/Conference/Paper545/Reviewers", "ICLR.cc/2019/Conference/Paper545/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper545/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper545/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper545/Authors|ICLR.cc/2019/Conference/Paper545/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper545/Reviewers", "ICLR.cc/2019/Conference/Paper545/Authors", "ICLR.cc/2019/Conference/Paper545/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619191}}}, {"id": "HklktK6i6X", "original": null, "number": 1, "cdate": 1542343030633, "ddate": null, "tcdate": 1542343030633, "tmdate": 1542343030633, "tddate": null, "forum": "SJxu5iR9KQ", "replyto": "r1ePGFoonQ", "invitation": "ICLR.cc/2019/Conference/-/Paper545/Official_Comment", "content": {"title": "Statistical testing and baseline comparison", "comment": "Apologies for the delay in response. We would like to thank the reviewers for evaluating our manuscript.\n\n1. Statistical testing\n\nAs the reviewer commented, the experiment results in our submitted version of the paper does not clearly support our claim which is \u201cSchedNet-Top(1) outperforms DIAL(1)\u201d due to the large variance. We have toned down the claim as \u201cthe average number of steps to capture the prey in DIAL(1) is larger than that of SchedNet-Top(1)\u201d in Section 4.1.\n\nFor the reviewer\u2019s convenience, we report a p-value of 0.075 with just 25 samples collected from a comparative simulation analysis of DIAL(1) and Sched-Top(1). The p-value shows a decreasing tendency with even just 25 samples, thereby tending towards a non-negligible performance gap that is not attributable to random factors alone. \n\nWe comment that the performance comparison with DIAL(1) is not for showing that SchedNet is better than DIAL(1), but mainly for highlighting that scheduling should be considered to get a higher reward. We comment that SchedNet is not intended for competing with DIAL but a complementary one. We believe that adding our idea of agent scheduling makes prior works more practical and valuable.\n\n\n2. Baseline comparison\n\nWe have updated the results that show the comparison with other baselines in appendix C.3 of our modified manuscript. The result shows a similar tendency with the result in PP depicted in Figure 3a. IDQN and COMA in which no communication is allowed, take a longer time to complete the task compared to other baselines with communication. DIAL works better than SchedNet-Top(1), however, under the contention constraint the average number of steps to complete the task in DIAL(1) is larger than that of SchedNet-Top(1)."}, "signatures": ["ICLR.cc/2019/Conference/Paper545/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper545/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper545/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Schedule Communication in Multi-agent Reinforcement Learning", "abstract": "Many real-world reinforcement learning tasks require multiple agents to make sequential decisions under the agents\u2019 interaction, where well-coordinated actions among the agents are crucial to achieve the target goal better at these tasks. One way to accelerate the coordination effect is to enable multiple agents to communicate with each other in a distributed manner and behave as a group. In this paper, we study a practical scenario when (i) the communication bandwidth is limited and (ii) the agents share the communication medium so that only a restricted number of agents are able to simultaneously use the medium, as in the state-of-the-art wireless networking standards. This calls for a certain form of communication scheduling. In that regard, we propose a multi-agent deep reinforcement learning framework, called SchedNet, in which agents learn how to schedule themselves, how to encode the messages, and how to select actions based on received messages. SchedNet is capable of deciding which agents should be entitled to broadcasting their (encoded) messages, by learning the importance of each agent\u2019s partially observed information. We evaluate SchedNet against multiple baselines under two different applications, namely, cooperative communication and navigation, and predator-prey. Our experiments show a non-negligible performance gap between SchedNet and other mechanisms such as the ones without communication and with vanilla scheduling methods, e.g., round robin, ranging from 32% to 43%.", "keywords": ["Multi agent reinforcement learning", "deep reinforcement learning", "Communication"], "authorids": ["kdw2139@gmail.com", "swmoon00@gmail.com", "ddhostallero@kaist.ac.kr", "soarhigh0714@gmail.com", "tylee0325@gmail.com", "khson@lanada.kaist.ac.kr", "yiyung@kaist.edu"], "authors": ["Daewoo Kim", "Sangwoo Moon", "David Hostallero", "Wan Ju Kang", "Taeyoung Lee", "Kyunghwan Son", "Yung Yi"], "pdf": "/pdf/13ff35ec770b2bb8d441f616f6de893bfcd3a01f.pdf", "paperhash": "kim|learning_to_schedule_communication_in_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nkim2018learning,\ntitle={Learning to Schedule Communication in Multi-agent Reinforcement Learning},\nauthor={Daewoo Kim and Sangwoo Moon and David Hostallero and Wan Ju Kang and Taeyoung Lee and Kyunghwan Son and Yung Yi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJxu5iR9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper545/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619191, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJxu5iR9KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper545/Authors", "ICLR.cc/2019/Conference/Paper545/Reviewers", "ICLR.cc/2019/Conference/Paper545/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper545/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper545/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper545/Authors|ICLR.cc/2019/Conference/Paper545/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper545/Reviewers", "ICLR.cc/2019/Conference/Paper545/Authors", "ICLR.cc/2019/Conference/Paper545/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619191}}}, {"id": "BJxKmdbi2Q", "original": null, "number": 2, "cdate": 1541244960643, "ddate": null, "tcdate": 1541244960643, "tmdate": 1541533902731, "tddate": null, "forum": "SJxu5iR9KQ", "replyto": "SJxu5iR9KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper545/Official_Review", "content": {"title": "Well written, easy to follow.", "review": "The authors present a setting of MARL communication where only a number of agents can broadcast messages in a shared and limited bandwidth channel. The paper is well written and easy to follow, and the authors run an extensive number of baselines to illustrate the contributions.\n\nComments:\n\n1) It's not clear to me how do the authors tackle partial observability without the use of recurrent connections or time-steps?\n\n2) Do the agents know if they were chosen to be broadcasted at the previous timestep?\n\n3) Many times it's important to know who sent the message, do the agents share this information?", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper545/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Schedule Communication in Multi-agent Reinforcement Learning", "abstract": "Many real-world reinforcement learning tasks require multiple agents to make sequential decisions under the agents\u2019 interaction, where well-coordinated actions among the agents are crucial to achieve the target goal better at these tasks. One way to accelerate the coordination effect is to enable multiple agents to communicate with each other in a distributed manner and behave as a group. In this paper, we study a practical scenario when (i) the communication bandwidth is limited and (ii) the agents share the communication medium so that only a restricted number of agents are able to simultaneously use the medium, as in the state-of-the-art wireless networking standards. This calls for a certain form of communication scheduling. In that regard, we propose a multi-agent deep reinforcement learning framework, called SchedNet, in which agents learn how to schedule themselves, how to encode the messages, and how to select actions based on received messages. SchedNet is capable of deciding which agents should be entitled to broadcasting their (encoded) messages, by learning the importance of each agent\u2019s partially observed information. We evaluate SchedNet against multiple baselines under two different applications, namely, cooperative communication and navigation, and predator-prey. Our experiments show a non-negligible performance gap between SchedNet and other mechanisms such as the ones without communication and with vanilla scheduling methods, e.g., round robin, ranging from 32% to 43%.", "keywords": ["Multi agent reinforcement learning", "deep reinforcement learning", "Communication"], "authorids": ["kdw2139@gmail.com", "swmoon00@gmail.com", "ddhostallero@kaist.ac.kr", "soarhigh0714@gmail.com", "tylee0325@gmail.com", "khson@lanada.kaist.ac.kr", "yiyung@kaist.edu"], "authors": ["Daewoo Kim", "Sangwoo Moon", "David Hostallero", "Wan Ju Kang", "Taeyoung Lee", "Kyunghwan Son", "Yung Yi"], "pdf": "/pdf/13ff35ec770b2bb8d441f616f6de893bfcd3a01f.pdf", "paperhash": "kim|learning_to_schedule_communication_in_multiagent_reinforcement_learning", "_bibtex": "@inproceedings{\nkim2018learning,\ntitle={Learning to Schedule Communication in Multi-agent Reinforcement Learning},\nauthor={Daewoo Kim and Sangwoo Moon and David Hostallero and Wan Ju Kang and Taeyoung Lee and Kyunghwan Son and Yung Yi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJxu5iR9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper545/Official_Review", "cdate": 1542234437064, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SJxu5iR9KQ", "replyto": "SJxu5iR9KQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper545/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335748451, "tmdate": 1552335748451, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper545/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 11}