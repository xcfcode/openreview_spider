{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488665182450, "tcdate": 1478297071285, "number": 492, "id": "ryHlUtqge", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "ryHlUtqge", "signatures": ["~Chelsea_Finn1"], "readers": ["everyone"], "content": {"title": "Generalizing Skills with Semi-Supervised Reinforcement Learning", "abstract": "Deep reinforcement learning (RL) can acquire complex behaviors from low-level inputs, such as images. However, real-world applications of such methods require generalizing to the vast variability of the real world. Deep networks are known to achieve remarkable generalization when provided with massive amounts of labeled data, but can we provide this breadth of experience to an RL agent, such as a robot? The robot might continuously learn as it explores the world around it, even while it is deployed and performing useful tasks. However, this learning requires access to a reward function, to tell the agent whether it is succeeding or failing at its task. Such reward functions are often hard to measure in the real world, especially in domains such as robotics and dialog systems, where the reward could depend on the unknown positions of objects or the emotional state of the user. On the other hand, it is often quite practical to provide the agent with reward functions in a limited set of situations, such as when a human supervisor is present, or in a controlled laboratory setting. Can we make use of this limited supervision, and still benefit from the breadth of experience an agent might collect in the unstructured real world? In this paper, we formalize this problem setting as semi-supervised reinforcement learning (SSRL), where the reward function can only be evaluated in a set of \u201clabeled\u201d MDPs, and the agent must generalize its behavior to the wide range of states it might encounter in a set of \u201cunlabeled\u201d MDPs, by using experience from both settings. Our proposed method infers the task objective in the unlabeled MDPs through an algorithm that resembles inverse RL, using the agent\u2019s own prior experience in the labeled MDPs as a kind of demonstration of optimal behavior. We evaluate our method on challenging, continuous control tasks that require control directly from images, and show that our approach can improve the generalization of a learned deep neural network policy by using experience for which no reward function is available. We also show that our method outperforms direct supervised learning of the reward.", "pdf": "/pdf/96383516c545c926b2bc3bbd36554fe4bd0cb3b6.pdf", "TL;DR": "We propose an algorithm for generalizing a deep neural network policy using \"unlabeled\" experience collected in MDPs where rewards are not available.", "paperhash": "finn|generalizing_skills_with_semisupervised_reinforcement_learning", "keywords": ["Reinforcement Learning"], "conflicts": ["berkeley.edu", "stanford.edu", "google.com", "washington.edu", "openai.com"], "authors": ["Chelsea Finn", "Tianhe Yu", "Justin Fu", "Pieter Abbeel", "Sergey Levine"], "authorids": ["cbfinn@eecs.berkeley.edu", "tianhe.yu@berkeley.edu", "justinfu@eecs.berkeley.edu", "pabbeel@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396615428, "tcdate": 1486396615428, "number": 1, "id": "BJe1R3G8dl", "invitation": "ICLR.cc/2017/conference/-/paper492/acceptance", "forum": "ryHlUtqge", "replyto": "ryHlUtqge", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "This paper provides an interesting framework for handling semi-supervised RL problems, settings were one can interact with many MDPs drawn from some class, but where only a few have observable rewards; the agent then uses a policy derived from the labeled MDPs to estimate a reward function for the unlabeled MDPs. The approach is straightforward, and one reviewer raises the reasonable concern that this seems to be a fairly narrow definition and approach to the universe of things that could be thought of as semi-supervised reinforcement learning (elements like Laplacian value functions, e.g. proto value functions, for instance, present a very different take on could what also be considered semi-supervised RL).\n \n Overall, however, the main benefit of this paper is that the overall idea here is quite compelling. Even if it's a narrow view on semisupervised RL, it nonetheless is clearly thinking more broadly about skill and data efficiency than what is common in many current RL papers. Given this impressive scope, plus good performance (if only on a relatively small set of benchmarks), it seems like this paper is certainly above bar for acceptance.\n \n Pros:\n + Nice introduction of a new/modified semisupervised reinforce setting\n + Results on benchmarks look compelling (if still fairly small scale)\n \n Cons:\n - Rather limited view of the space of all possible semisupervised Rl\n - Results on hardest task (half-cheetah) aren't _that_ much better than much simpler approaches", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalizing Skills with Semi-Supervised Reinforcement Learning", "abstract": "Deep reinforcement learning (RL) can acquire complex behaviors from low-level inputs, such as images. However, real-world applications of such methods require generalizing to the vast variability of the real world. Deep networks are known to achieve remarkable generalization when provided with massive amounts of labeled data, but can we provide this breadth of experience to an RL agent, such as a robot? The robot might continuously learn as it explores the world around it, even while it is deployed and performing useful tasks. However, this learning requires access to a reward function, to tell the agent whether it is succeeding or failing at its task. Such reward functions are often hard to measure in the real world, especially in domains such as robotics and dialog systems, where the reward could depend on the unknown positions of objects or the emotional state of the user. On the other hand, it is often quite practical to provide the agent with reward functions in a limited set of situations, such as when a human supervisor is present, or in a controlled laboratory setting. Can we make use of this limited supervision, and still benefit from the breadth of experience an agent might collect in the unstructured real world? In this paper, we formalize this problem setting as semi-supervised reinforcement learning (SSRL), where the reward function can only be evaluated in a set of \u201clabeled\u201d MDPs, and the agent must generalize its behavior to the wide range of states it might encounter in a set of \u201cunlabeled\u201d MDPs, by using experience from both settings. Our proposed method infers the task objective in the unlabeled MDPs through an algorithm that resembles inverse RL, using the agent\u2019s own prior experience in the labeled MDPs as a kind of demonstration of optimal behavior. We evaluate our method on challenging, continuous control tasks that require control directly from images, and show that our approach can improve the generalization of a learned deep neural network policy by using experience for which no reward function is available. We also show that our method outperforms direct supervised learning of the reward.", "pdf": "/pdf/96383516c545c926b2bc3bbd36554fe4bd0cb3b6.pdf", "TL;DR": "We propose an algorithm for generalizing a deep neural network policy using \"unlabeled\" experience collected in MDPs where rewards are not available.", "paperhash": "finn|generalizing_skills_with_semisupervised_reinforcement_learning", "keywords": ["Reinforcement Learning"], "conflicts": ["berkeley.edu", "stanford.edu", "google.com", "washington.edu", "openai.com"], "authors": ["Chelsea Finn", "Tianhe Yu", "Justin Fu", "Pieter Abbeel", "Sergey Levine"], "authorids": ["cbfinn@eecs.berkeley.edu", "tianhe.yu@berkeley.edu", "justinfu@eecs.berkeley.edu", "pabbeel@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396615998, "id": "ICLR.cc/2017/conference/-/paper492/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "ryHlUtqge", "replyto": "ryHlUtqge", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396615998}}}, {"tddate": null, "tmdate": 1481979780716, "tcdate": 1481979780716, "number": 3, "id": "BknFD3GVg", "invitation": "ICLR.cc/2017/conference/-/paper492/official/review", "forum": "ryHlUtqge", "replyto": "ryHlUtqge", "signatures": ["ICLR.cc/2017/conference/paper492/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper492/AnonReviewer3"], "content": {"title": "An approach to semi supervised RL using inverse RL ", "rating": "6: Marginally above acceptance threshold", "review": "In supervised learning, a significant advance occurred when the framework of semi-supervised learning was  adopted, which used the weaker approach of unsupervised learning to infer some property, such as a distance measure or a smoothness regularizer, which could then be used with a small number of labeled examples. The approach rested on the assumption of smoothness on the manifold, typically. \n\nThis paper attempts to stretch this analogy to reinforcement learning, although the analogy is somewhat incoherent. Labels are not equivalent to reward functions, and positive or negative rewards do not mean the same as positive and negative labels. Still, the paper makes a worthwhile attempt to explore this notion of semi-supervised RL, which is clearly an important area that deserves more attention. The authors use the term \"labeled MDP\" to mean the typical MDP framework where the reward function is unknown. They use the confusing term \"unlabeled MDP\" to mean the situation where the reward is unknown, which is technically not an MDP (but a controlled Markov process). \n\nIn the classical RL transfer learning setup, the agent is attempting to transfer learning from a source \"labeled\" MDP to a target \"labeled\" MDP (where both reward functions are known, but the learned policy is known only in the source MDP). In the semi-supervised RL setting, the target is an \"unlabeled\" CMP, and the source is both a \"labeled\" MDP and an \"unlabeled\" CMP. The basic approach is to use inverse RL to infer the unknown \"labels\" and then attempt to construct transfer. A further restriction is made to linearly solvable MDPs for technical reasons. Experiments are reported using three relatively complex domains using the Mujoco physics simulator. \n\nThe work is interesting, but in the opinion of this reviewer, the work fails to provide a simple sufficiently general notion of semi-supervised RL that will be of sufficiently wide interest to the RL community. That remains to be done by a future paper, but in the interim, the work here is sufficiently interesting and the problem is certainly a worthwhile one to study. ", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalizing Skills with Semi-Supervised Reinforcement Learning", "abstract": "Deep reinforcement learning (RL) can acquire complex behaviors from low-level inputs, such as images. However, real-world applications of such methods require generalizing to the vast variability of the real world. Deep networks are known to achieve remarkable generalization when provided with massive amounts of labeled data, but can we provide this breadth of experience to an RL agent, such as a robot? The robot might continuously learn as it explores the world around it, even while it is deployed and performing useful tasks. However, this learning requires access to a reward function, to tell the agent whether it is succeeding or failing at its task. Such reward functions are often hard to measure in the real world, especially in domains such as robotics and dialog systems, where the reward could depend on the unknown positions of objects or the emotional state of the user. On the other hand, it is often quite practical to provide the agent with reward functions in a limited set of situations, such as when a human supervisor is present, or in a controlled laboratory setting. Can we make use of this limited supervision, and still benefit from the breadth of experience an agent might collect in the unstructured real world? In this paper, we formalize this problem setting as semi-supervised reinforcement learning (SSRL), where the reward function can only be evaluated in a set of \u201clabeled\u201d MDPs, and the agent must generalize its behavior to the wide range of states it might encounter in a set of \u201cunlabeled\u201d MDPs, by using experience from both settings. Our proposed method infers the task objective in the unlabeled MDPs through an algorithm that resembles inverse RL, using the agent\u2019s own prior experience in the labeled MDPs as a kind of demonstration of optimal behavior. We evaluate our method on challenging, continuous control tasks that require control directly from images, and show that our approach can improve the generalization of a learned deep neural network policy by using experience for which no reward function is available. We also show that our method outperforms direct supervised learning of the reward.", "pdf": "/pdf/96383516c545c926b2bc3bbd36554fe4bd0cb3b6.pdf", "TL;DR": "We propose an algorithm for generalizing a deep neural network policy using \"unlabeled\" experience collected in MDPs where rewards are not available.", "paperhash": "finn|generalizing_skills_with_semisupervised_reinforcement_learning", "keywords": ["Reinforcement Learning"], "conflicts": ["berkeley.edu", "stanford.edu", "google.com", "washington.edu", "openai.com"], "authors": ["Chelsea Finn", "Tianhe Yu", "Justin Fu", "Pieter Abbeel", "Sergey Levine"], "authorids": ["cbfinn@eecs.berkeley.edu", "tianhe.yu@berkeley.edu", "justinfu@eecs.berkeley.edu", "pabbeel@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512566735, "id": "ICLR.cc/2017/conference/-/paper492/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper492/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper492/AnonReviewer2", "ICLR.cc/2017/conference/paper492/AnonReviewer4", "ICLR.cc/2017/conference/paper492/AnonReviewer3"], "reply": {"forum": "ryHlUtqge", "replyto": "ryHlUtqge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper492/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper492/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512566735}}}, {"tddate": null, "tmdate": 1481972820507, "tcdate": 1481972820507, "number": 2, "id": "B13U25zEx", "invitation": "ICLR.cc/2017/conference/-/paper492/official/review", "forum": "ryHlUtqge", "replyto": "ryHlUtqge", "signatures": ["ICLR.cc/2017/conference/paper492/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper492/AnonReviewer4"], "content": {"title": "Review", "rating": "7: Good paper, accept", "review": "This paper formalizes the problem setting of having only a subset of available MDPs for which one has access to a reward. The authors name this setting \"semi-supervised reinforcement learning\" (SSRL), as a reference to semi-supervised learning (where one only has access to labels for a subset of the dataset). They provide an approach for solving SSRL named semi-supervised skill generalization (S3G), which builds on the framework of maximum entropy control. The whole approach is straightforward and amounts to an EM algorithm with partial labels (: they alternate iteratively between estimating a reward function (parametrized) and fitting a control policy using this reward function. They provide experiments on 4 tasks (obstacle, 2-link reacher, 2-link reacher with vision, half-cheetah) in MuJoCo.\n\nThe paper is well-written, and is overall clear. The appendix provides some more context, I think a few implementation details are missing to be able to fully reproduce the experiments from the paper, but they will provide the code.\n\nThe link to inverse reinforcement learning seems to be done correctly. However, there is no reference to off-policy policy learning, and, for instance, it seems to me that the \\tau \\in D_{samp} term of equation (3) could benefit from variance reduction as in e.g. TB(\\lambda) [Precup et al. 2000] or Retrace(\\lambda) [Munos et al. 2016].\n\nThe experimental section is convincing, but I would appreciate a precision (and small discussion) of this sentence \"To extensively test the generalization capabilities of the policies learned with each method, we measure performance on a wide range of settings that is a superset of the unlabeled and labeled MDPs\" with numbers for the different scenarios (or the replacement of superset by \"union\" if this is the case). It may explain better the poor results of \"oracle\" on \"obstacle\" and \"2-link reacher\", and reinforce* the further sentences \"In the obstacle task, the true reward function is not sufficiently shaped for learning in the unlabeled MDPs. Hence, the reward regression and oracle methods perform poorly\".\n\nCorrection on page 4: \"5-tuple M_i = (S, A, T, R)\" is a 4-tuple.\n\nOverall, I think that this is a good and sound paper. I am personally unsure as to if all the parallels and/or references to previous work are complete, thus my confidence score of 3.\n\n(* pun intended)", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalizing Skills with Semi-Supervised Reinforcement Learning", "abstract": "Deep reinforcement learning (RL) can acquire complex behaviors from low-level inputs, such as images. However, real-world applications of such methods require generalizing to the vast variability of the real world. Deep networks are known to achieve remarkable generalization when provided with massive amounts of labeled data, but can we provide this breadth of experience to an RL agent, such as a robot? The robot might continuously learn as it explores the world around it, even while it is deployed and performing useful tasks. However, this learning requires access to a reward function, to tell the agent whether it is succeeding or failing at its task. Such reward functions are often hard to measure in the real world, especially in domains such as robotics and dialog systems, where the reward could depend on the unknown positions of objects or the emotional state of the user. On the other hand, it is often quite practical to provide the agent with reward functions in a limited set of situations, such as when a human supervisor is present, or in a controlled laboratory setting. Can we make use of this limited supervision, and still benefit from the breadth of experience an agent might collect in the unstructured real world? In this paper, we formalize this problem setting as semi-supervised reinforcement learning (SSRL), where the reward function can only be evaluated in a set of \u201clabeled\u201d MDPs, and the agent must generalize its behavior to the wide range of states it might encounter in a set of \u201cunlabeled\u201d MDPs, by using experience from both settings. Our proposed method infers the task objective in the unlabeled MDPs through an algorithm that resembles inverse RL, using the agent\u2019s own prior experience in the labeled MDPs as a kind of demonstration of optimal behavior. We evaluate our method on challenging, continuous control tasks that require control directly from images, and show that our approach can improve the generalization of a learned deep neural network policy by using experience for which no reward function is available. We also show that our method outperforms direct supervised learning of the reward.", "pdf": "/pdf/96383516c545c926b2bc3bbd36554fe4bd0cb3b6.pdf", "TL;DR": "We propose an algorithm for generalizing a deep neural network policy using \"unlabeled\" experience collected in MDPs where rewards are not available.", "paperhash": "finn|generalizing_skills_with_semisupervised_reinforcement_learning", "keywords": ["Reinforcement Learning"], "conflicts": ["berkeley.edu", "stanford.edu", "google.com", "washington.edu", "openai.com"], "authors": ["Chelsea Finn", "Tianhe Yu", "Justin Fu", "Pieter Abbeel", "Sergey Levine"], "authorids": ["cbfinn@eecs.berkeley.edu", "tianhe.yu@berkeley.edu", "justinfu@eecs.berkeley.edu", "pabbeel@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512566735, "id": "ICLR.cc/2017/conference/-/paper492/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper492/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper492/AnonReviewer2", "ICLR.cc/2017/conference/paper492/AnonReviewer4", "ICLR.cc/2017/conference/paper492/AnonReviewer3"], "reply": {"forum": "ryHlUtqge", "replyto": "ryHlUtqge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper492/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper492/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512566735}}}, {"tddate": null, "tmdate": 1481910077316, "tcdate": 1481910077316, "number": 1, "id": "rkrSDoZ4e", "invitation": "ICLR.cc/2017/conference/-/paper492/official/review", "forum": "ryHlUtqge", "replyto": "ryHlUtqge", "signatures": ["ICLR.cc/2017/conference/paper492/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper492/AnonReviewer2"], "content": {"title": "Review", "rating": "8: Top 50% of accepted papers, clear accept", "review": "The paper proposes to study the problem of semi-supervised RL where one has to distinguish between labelled MDPs that provide rewards, and unlabelled MDPs that are not associated with any reward signal. The underlying is very simple since it aims at simultaneously learning a policy based on the REINFORCE+entropy regularization technique, and also a model of the reward that will be used (as in inverse reinforcement learning) as a feedback over unlabelled MDPs. The experiments are made on different continous domains and show interesting results\n\nThe paper is well written, and easy to understand. It is based on a simple but efficient idea of simultaneously learning the policy and a model of the reward and the resulting algorithm exhibit interesting properties. The proposed idea is quite obvious, but the authors are the first ones to propose to test such a model. The experiments could be made stronger by mixing continuous and discrete problems but are convincing. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalizing Skills with Semi-Supervised Reinforcement Learning", "abstract": "Deep reinforcement learning (RL) can acquire complex behaviors from low-level inputs, such as images. However, real-world applications of such methods require generalizing to the vast variability of the real world. Deep networks are known to achieve remarkable generalization when provided with massive amounts of labeled data, but can we provide this breadth of experience to an RL agent, such as a robot? The robot might continuously learn as it explores the world around it, even while it is deployed and performing useful tasks. However, this learning requires access to a reward function, to tell the agent whether it is succeeding or failing at its task. Such reward functions are often hard to measure in the real world, especially in domains such as robotics and dialog systems, where the reward could depend on the unknown positions of objects or the emotional state of the user. On the other hand, it is often quite practical to provide the agent with reward functions in a limited set of situations, such as when a human supervisor is present, or in a controlled laboratory setting. Can we make use of this limited supervision, and still benefit from the breadth of experience an agent might collect in the unstructured real world? In this paper, we formalize this problem setting as semi-supervised reinforcement learning (SSRL), where the reward function can only be evaluated in a set of \u201clabeled\u201d MDPs, and the agent must generalize its behavior to the wide range of states it might encounter in a set of \u201cunlabeled\u201d MDPs, by using experience from both settings. Our proposed method infers the task objective in the unlabeled MDPs through an algorithm that resembles inverse RL, using the agent\u2019s own prior experience in the labeled MDPs as a kind of demonstration of optimal behavior. We evaluate our method on challenging, continuous control tasks that require control directly from images, and show that our approach can improve the generalization of a learned deep neural network policy by using experience for which no reward function is available. We also show that our method outperforms direct supervised learning of the reward.", "pdf": "/pdf/96383516c545c926b2bc3bbd36554fe4bd0cb3b6.pdf", "TL;DR": "We propose an algorithm for generalizing a deep neural network policy using \"unlabeled\" experience collected in MDPs where rewards are not available.", "paperhash": "finn|generalizing_skills_with_semisupervised_reinforcement_learning", "keywords": ["Reinforcement Learning"], "conflicts": ["berkeley.edu", "stanford.edu", "google.com", "washington.edu", "openai.com"], "authors": ["Chelsea Finn", "Tianhe Yu", "Justin Fu", "Pieter Abbeel", "Sergey Levine"], "authorids": ["cbfinn@eecs.berkeley.edu", "tianhe.yu@berkeley.edu", "justinfu@eecs.berkeley.edu", "pabbeel@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512566735, "id": "ICLR.cc/2017/conference/-/paper492/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper492/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper492/AnonReviewer2", "ICLR.cc/2017/conference/paper492/AnonReviewer4", "ICLR.cc/2017/conference/paper492/AnonReviewer3"], "reply": {"forum": "ryHlUtqge", "replyto": "ryHlUtqge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper492/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper492/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512566735}}}, {"tddate": null, "tmdate": 1481617086397, "tcdate": 1481617086391, "number": 2, "id": "rJ8aR7pXg", "invitation": "ICLR.cc/2017/conference/-/paper492/public/comment", "forum": "ryHlUtqge", "replyto": "r1B_zt17e", "signatures": ["~Chelsea_Finn1"], "readers": ["everyone"], "writers": ["~Chelsea_Finn1"], "content": {"title": "responses to questions", "comment": "Thank you for your questions.\n\nThe reward function is factorized over time R_\\psi(\\tau) = \\sum_t r_\\psi(s_t, a_t), where s and a denote the state and action [defined just below equation 2]. The form of r_\\psi(s_t, a_t) is a neural network with 2 hidden layers, and a quadratic norm on the output to force the function to be lower bounded. This form follows prior work (Finn et al. ICML \u201816).\n\nThe evaluation is done on both labeled and unlabeled MDPs [see second paragraph of Section 5.2]. Note that all methods reached perfect or near perfect performance on the labeled MDPs for all tasks.\n\nWhile it is true that the cheetah and obstacle tasks have a particular structure where solving the unlabeled MDP would also be successful in the labeled MDP, the two reaching tasks have a different structure, where the agent must execute different actions for each scenario. We designed the experimental tasks to reflect a variety of scenarios where detailed reward supervision is hard to obtain.\n\nOur experiments involved continuous control tasks with a range of difficulty, from planar reaching to cheetah jumping and tasks with vision. In principle, both the problem statement and the proposed method are general to other RL domains and other algorithms for policy optimization and cost learning. Of course, the number of experiments we can fit into a conference submission is limited, so we chose to focus on continuous control domains because those have been most experimented with in cost learning with neural networks (Finn et al. \u201816, Ho et al. \u201816). We have edited the abstract and the last paragraph of the introduction to indicate the focus on continuous control tasks."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalizing Skills with Semi-Supervised Reinforcement Learning", "abstract": "Deep reinforcement learning (RL) can acquire complex behaviors from low-level inputs, such as images. However, real-world applications of such methods require generalizing to the vast variability of the real world. Deep networks are known to achieve remarkable generalization when provided with massive amounts of labeled data, but can we provide this breadth of experience to an RL agent, such as a robot? The robot might continuously learn as it explores the world around it, even while it is deployed and performing useful tasks. However, this learning requires access to a reward function, to tell the agent whether it is succeeding or failing at its task. Such reward functions are often hard to measure in the real world, especially in domains such as robotics and dialog systems, where the reward could depend on the unknown positions of objects or the emotional state of the user. On the other hand, it is often quite practical to provide the agent with reward functions in a limited set of situations, such as when a human supervisor is present, or in a controlled laboratory setting. Can we make use of this limited supervision, and still benefit from the breadth of experience an agent might collect in the unstructured real world? In this paper, we formalize this problem setting as semi-supervised reinforcement learning (SSRL), where the reward function can only be evaluated in a set of \u201clabeled\u201d MDPs, and the agent must generalize its behavior to the wide range of states it might encounter in a set of \u201cunlabeled\u201d MDPs, by using experience from both settings. Our proposed method infers the task objective in the unlabeled MDPs through an algorithm that resembles inverse RL, using the agent\u2019s own prior experience in the labeled MDPs as a kind of demonstration of optimal behavior. We evaluate our method on challenging, continuous control tasks that require control directly from images, and show that our approach can improve the generalization of a learned deep neural network policy by using experience for which no reward function is available. We also show that our method outperforms direct supervised learning of the reward.", "pdf": "/pdf/96383516c545c926b2bc3bbd36554fe4bd0cb3b6.pdf", "TL;DR": "We propose an algorithm for generalizing a deep neural network policy using \"unlabeled\" experience collected in MDPs where rewards are not available.", "paperhash": "finn|generalizing_skills_with_semisupervised_reinforcement_learning", "keywords": ["Reinforcement Learning"], "conflicts": ["berkeley.edu", "stanford.edu", "google.com", "washington.edu", "openai.com"], "authors": ["Chelsea Finn", "Tianhe Yu", "Justin Fu", "Pieter Abbeel", "Sergey Levine"], "authorids": ["cbfinn@eecs.berkeley.edu", "tianhe.yu@berkeley.edu", "justinfu@eecs.berkeley.edu", "pabbeel@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287554865, "id": "ICLR.cc/2017/conference/-/paper492/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryHlUtqge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper492/reviewers", "ICLR.cc/2017/conference/paper492/areachairs"], "cdate": 1485287554865}}}, {"tddate": null, "tmdate": 1481616735100, "tcdate": 1480719428134, "number": 1, "id": "Hy3H3_yml", "invitation": "ICLR.cc/2017/conference/-/paper492/public/comment", "forum": "ryHlUtqge", "replyto": "r1qtIh6ze", "signatures": ["~Chelsea_Finn1"], "readers": ["everyone"], "writers": ["~Chelsea_Finn1"], "content": {"title": "response", "comment": "Thank you for your feedback.\n\nRegarding simplicity, we already compare to the simplest possible approach -- fitting the reward function through supervised learning. Our overall method is not complex, as it just amounts to using an inverse RL algorithm to transfer knowledge about the reward from the labeled to the unlabeled MDPs. The particular inverse RL method we use for this is based on MaxEnt IRL, hence corresponding to linearly solvable MDPs and entropic policies (which are just other words for the same framework).\n\nWe agree that the terms \u201clabeled\u201d and \u201cunlabeled\u201d MDP are not as precise as they could be, but we believe that this strikes the right balance between readability and precision. The terms are used by analogy to semi-supervised learning, where there are labeled and unlabeled datapoints. We added this discussion to the paper, in the first paragraph of Section 3, and clarified that the reward exists in the unlabeled MDPs, but just cannot be observed.\n\nAlthough an exploration of SSRL in the context of bandits would be interesting, we believe it would be out of scope of the current paper, which already presents an extensive experimental evaluation with three separate complex domains. We designed a range of experiments to reflect a variety of scenarios where detailed reward supervision is hard to obtain, and we believe that these adequately evaluate the approach."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalizing Skills with Semi-Supervised Reinforcement Learning", "abstract": "Deep reinforcement learning (RL) can acquire complex behaviors from low-level inputs, such as images. However, real-world applications of such methods require generalizing to the vast variability of the real world. Deep networks are known to achieve remarkable generalization when provided with massive amounts of labeled data, but can we provide this breadth of experience to an RL agent, such as a robot? The robot might continuously learn as it explores the world around it, even while it is deployed and performing useful tasks. However, this learning requires access to a reward function, to tell the agent whether it is succeeding or failing at its task. Such reward functions are often hard to measure in the real world, especially in domains such as robotics and dialog systems, where the reward could depend on the unknown positions of objects or the emotional state of the user. On the other hand, it is often quite practical to provide the agent with reward functions in a limited set of situations, such as when a human supervisor is present, or in a controlled laboratory setting. Can we make use of this limited supervision, and still benefit from the breadth of experience an agent might collect in the unstructured real world? In this paper, we formalize this problem setting as semi-supervised reinforcement learning (SSRL), where the reward function can only be evaluated in a set of \u201clabeled\u201d MDPs, and the agent must generalize its behavior to the wide range of states it might encounter in a set of \u201cunlabeled\u201d MDPs, by using experience from both settings. Our proposed method infers the task objective in the unlabeled MDPs through an algorithm that resembles inverse RL, using the agent\u2019s own prior experience in the labeled MDPs as a kind of demonstration of optimal behavior. We evaluate our method on challenging, continuous control tasks that require control directly from images, and show that our approach can improve the generalization of a learned deep neural network policy by using experience for which no reward function is available. We also show that our method outperforms direct supervised learning of the reward.", "pdf": "/pdf/96383516c545c926b2bc3bbd36554fe4bd0cb3b6.pdf", "TL;DR": "We propose an algorithm for generalizing a deep neural network policy using \"unlabeled\" experience collected in MDPs where rewards are not available.", "paperhash": "finn|generalizing_skills_with_semisupervised_reinforcement_learning", "keywords": ["Reinforcement Learning"], "conflicts": ["berkeley.edu", "stanford.edu", "google.com", "washington.edu", "openai.com"], "authors": ["Chelsea Finn", "Tianhe Yu", "Justin Fu", "Pieter Abbeel", "Sergey Levine"], "authorids": ["cbfinn@eecs.berkeley.edu", "tianhe.yu@berkeley.edu", "justinfu@eecs.berkeley.edu", "pabbeel@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287554865, "id": "ICLR.cc/2017/conference/-/paper492/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryHlUtqge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper492/reviewers", "ICLR.cc/2017/conference/paper492/areachairs"], "cdate": 1485287554865}}}, {"tddate": null, "tmdate": 1480721005460, "tcdate": 1480721005454, "number": 2, "id": "r1B_zt17e", "invitation": "ICLR.cc/2017/conference/-/paper492/pre-review/question", "forum": "ryHlUtqge", "replyto": "ryHlUtqge", "signatures": ["ICLR.cc/2017/conference/paper492/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper492/AnonReviewer2"], "content": {"title": "Pre-review question", "question": "Dear authors, first of all, I find the paper really interesting. Here are some preliminary questions to help me to better understand what you propose:\n\nConcerning the function R_\\psi(t) which aims at evaluating the reward over a trajectory, could you please provide details about the shape of this function since it takes as an input a complete trajectory ?\n\nConcerning the evaluation, it is not clear how it is done (evaluation made only on the unlabeled MDPs, or both the labeled and unlabeled ones ? )\n\nIt seems that the experimental tasks have a particular structure. For example, in the jumping experiment (the agent does not have information concerning the size of the wall) it seems that being able to solve the unsupervised MDPs (wall with a heigh of 0.5) is a good solution for the labeled MDPs (with a smaller wall). Could you please comment on this since it seems (but maybe I am wrong) to bias a little bit the interpretation of the experimental results ? Why not providing the height of the wall as an observation for example and to train with multiple heights ? \n\nThe algorithm is used with MDGPS which supposes the reward to be differentiable. Do you have results on \"more classic\" tasks with classical rewards ? (and discrete actions spaces for example) ? \n\n Thank you"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalizing Skills with Semi-Supervised Reinforcement Learning", "abstract": "Deep reinforcement learning (RL) can acquire complex behaviors from low-level inputs, such as images. However, real-world applications of such methods require generalizing to the vast variability of the real world. Deep networks are known to achieve remarkable generalization when provided with massive amounts of labeled data, but can we provide this breadth of experience to an RL agent, such as a robot? The robot might continuously learn as it explores the world around it, even while it is deployed and performing useful tasks. However, this learning requires access to a reward function, to tell the agent whether it is succeeding or failing at its task. Such reward functions are often hard to measure in the real world, especially in domains such as robotics and dialog systems, where the reward could depend on the unknown positions of objects or the emotional state of the user. On the other hand, it is often quite practical to provide the agent with reward functions in a limited set of situations, such as when a human supervisor is present, or in a controlled laboratory setting. Can we make use of this limited supervision, and still benefit from the breadth of experience an agent might collect in the unstructured real world? In this paper, we formalize this problem setting as semi-supervised reinforcement learning (SSRL), where the reward function can only be evaluated in a set of \u201clabeled\u201d MDPs, and the agent must generalize its behavior to the wide range of states it might encounter in a set of \u201cunlabeled\u201d MDPs, by using experience from both settings. Our proposed method infers the task objective in the unlabeled MDPs through an algorithm that resembles inverse RL, using the agent\u2019s own prior experience in the labeled MDPs as a kind of demonstration of optimal behavior. We evaluate our method on challenging, continuous control tasks that require control directly from images, and show that our approach can improve the generalization of a learned deep neural network policy by using experience for which no reward function is available. We also show that our method outperforms direct supervised learning of the reward.", "pdf": "/pdf/96383516c545c926b2bc3bbd36554fe4bd0cb3b6.pdf", "TL;DR": "We propose an algorithm for generalizing a deep neural network policy using \"unlabeled\" experience collected in MDPs where rewards are not available.", "paperhash": "finn|generalizing_skills_with_semisupervised_reinforcement_learning", "keywords": ["Reinforcement Learning"], "conflicts": ["berkeley.edu", "stanford.edu", "google.com", "washington.edu", "openai.com"], "authors": ["Chelsea Finn", "Tianhe Yu", "Justin Fu", "Pieter Abbeel", "Sergey Levine"], "authorids": ["cbfinn@eecs.berkeley.edu", "tianhe.yu@berkeley.edu", "justinfu@eecs.berkeley.edu", "pabbeel@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959251542, "id": "ICLR.cc/2017/conference/-/paper492/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper492/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper492/AnonReviewer3", "ICLR.cc/2017/conference/paper492/AnonReviewer2"], "reply": {"forum": "ryHlUtqge", "replyto": "ryHlUtqge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper492/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper492/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959251542}}}, {"tddate": null, "tmdate": 1480603265732, "tcdate": 1480603265726, "number": 1, "id": "r1qtIh6ze", "invitation": "ICLR.cc/2017/conference/-/paper492/pre-review/question", "forum": "ryHlUtqge", "replyto": "ryHlUtqge", "signatures": ["ICLR.cc/2017/conference/paper492/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper492/AnonReviewer3"], "content": {"title": "Simpler framework for SSRL ", "question": "The problem of semi-supervised RL is a good one to formalize, although the approach taken by the paper seems to add perhaps excessive layers of complexity to the simplest possible approach (i.e., linearly solvable MDPs, entropic policies, ...). \n\nFirst, the terminology of \"unlabeled MDP\" seems to grate on the ear. A reward function is not  analogous to a \"label\"  in supervised learning. It does not tell the agent what action to do, since RL is not associative learning. It might hint as to whether an action taken is appropriate or not, but even that is often misleading, since whether the reward is positive, negative or zero might not reveal anything about whether the current action is desirable or not. The reward function is simply a highly noisy indicator of the true value function, and the correct \"label\" for the state is the return. The reward is just the first term in the return. \n\nIt might be helpful to develop an SSRL model based on this idea, where \"unlabeled MDPs\" are simply those where the returns are identically zero (always), and \"labeled\" MDPs are those where the return is the exact value (this corresponds more to the regression setting in the paper). By expanding the return and progressively neutralizing successive terms, one is lead from a fully supervised MDP to a weakly supervised MDP (or even an MDP without  a reward function, which is technically not an MDP at all, but a controlled Markov process). \n\nThe connection to bandit problems might be helpful to discuss as well. What is the equivalent \"unlabeled\" bandit problem, and how does the present approach relate to the huge literature on bandits (which are special cases of MDPs)?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalizing Skills with Semi-Supervised Reinforcement Learning", "abstract": "Deep reinforcement learning (RL) can acquire complex behaviors from low-level inputs, such as images. However, real-world applications of such methods require generalizing to the vast variability of the real world. Deep networks are known to achieve remarkable generalization when provided with massive amounts of labeled data, but can we provide this breadth of experience to an RL agent, such as a robot? The robot might continuously learn as it explores the world around it, even while it is deployed and performing useful tasks. However, this learning requires access to a reward function, to tell the agent whether it is succeeding or failing at its task. Such reward functions are often hard to measure in the real world, especially in domains such as robotics and dialog systems, where the reward could depend on the unknown positions of objects or the emotional state of the user. On the other hand, it is often quite practical to provide the agent with reward functions in a limited set of situations, such as when a human supervisor is present, or in a controlled laboratory setting. Can we make use of this limited supervision, and still benefit from the breadth of experience an agent might collect in the unstructured real world? In this paper, we formalize this problem setting as semi-supervised reinforcement learning (SSRL), where the reward function can only be evaluated in a set of \u201clabeled\u201d MDPs, and the agent must generalize its behavior to the wide range of states it might encounter in a set of \u201cunlabeled\u201d MDPs, by using experience from both settings. Our proposed method infers the task objective in the unlabeled MDPs through an algorithm that resembles inverse RL, using the agent\u2019s own prior experience in the labeled MDPs as a kind of demonstration of optimal behavior. We evaluate our method on challenging, continuous control tasks that require control directly from images, and show that our approach can improve the generalization of a learned deep neural network policy by using experience for which no reward function is available. We also show that our method outperforms direct supervised learning of the reward.", "pdf": "/pdf/96383516c545c926b2bc3bbd36554fe4bd0cb3b6.pdf", "TL;DR": "We propose an algorithm for generalizing a deep neural network policy using \"unlabeled\" experience collected in MDPs where rewards are not available.", "paperhash": "finn|generalizing_skills_with_semisupervised_reinforcement_learning", "keywords": ["Reinforcement Learning"], "conflicts": ["berkeley.edu", "stanford.edu", "google.com", "washington.edu", "openai.com"], "authors": ["Chelsea Finn", "Tianhe Yu", "Justin Fu", "Pieter Abbeel", "Sergey Levine"], "authorids": ["cbfinn@eecs.berkeley.edu", "tianhe.yu@berkeley.edu", "justinfu@eecs.berkeley.edu", "pabbeel@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959251542, "id": "ICLR.cc/2017/conference/-/paper492/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper492/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper492/AnonReviewer3", "ICLR.cc/2017/conference/paper492/AnonReviewer2"], "reply": {"forum": "ryHlUtqge", "replyto": "ryHlUtqge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper492/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper492/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959251542}}}], "count": 9}