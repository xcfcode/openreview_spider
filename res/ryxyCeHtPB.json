{"notes": [{"id": "ryxyCeHtPB", "original": "SkltbEzFwS", "number": 2594, "cdate": 1569439942705, "ddate": null, "tcdate": 1569439942705, "tmdate": 1583912019895, "tddate": null, "forum": "ryxyCeHtPB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["kf.wang@siat.ac.cn", "xt.gao@siat.ac.cn", "yiren.zhao@cl.cam.ac.uk", "lixingjian@baidu.com", "doudejing@baidu.com", "czxu@um.edu.mo"], "title": "Pay Attention to Features, Transfer Learn Faster CNNs", "authors": ["Kafeng Wang", "Xitong Gao", "Yiren Zhao", "Xingjian Li", "Dejing Dou", "Cheng-Zhong Xu"], "pdf": "/pdf/6e6abb55b83a1b41eaa6f0c9d191b89c2bfeb7c4.pdf", "TL;DR": "We introduce attentive feature distillation and selection, to fine-tune a large model and produce a faster one.", "abstract": "Deep convolutional neural networks are now widely deployed in vision applications, but a limited size of training data can restrict their task performance. Transfer learning offers the chance for CNNs to learn with limited data samples by transferring knowledge from models pretrained on large datasets. Blindly transferring all learned features from the source dataset, however, brings unnecessary computation to CNNs on the target task. In this paper, we propose attentive feature distillation and selection (AFDS), which not only adjusts the strength of transfer learning regularization but also dynamically determines the important features to transfer. By deploying AFDS on ResNet-101, we achieved a state-of-the-art computation reduction at the same accuracy budget, outperforming all existing transfer learning methods. With a 10x MACs reduction budget, a ResNet-101 equipped with AFDS transfer learned from ImageNet to Stanford Dogs 120, can achieve an accuracy 11.07% higher than its best competitor.", "keywords": ["transfer learning", "pruning", "faster CNNs"], "paperhash": "wang|pay_attention_to_features_transfer_learn_faster_cnns", "_bibtex": "@inproceedings{\nWang2020Pay,\ntitle={Pay Attention to Features, Transfer Learn Faster CNNs},\nauthor={Kafeng Wang and Xitong Gao and Yiren Zhao and Xingjian Li and Dejing Dou and Cheng-Zhong Xu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxyCeHtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/84d13359f1520c521f106b4f3bc6265a787c22e0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "EE4Ml5hZtI", "original": null, "number": 1, "cdate": 1576798753000, "ddate": null, "tcdate": 1576798753000, "tmdate": 1576800882379, "tddate": null, "forum": "ryxyCeHtPB", "replyto": "ryxyCeHtPB", "invitation": "ICLR.cc/2020/Conference/Paper2594/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper presents an attention-based approach to transfer faster CNNs, which tackles the problem of jointly transferring source knowledge and pruning target CNNs.\n\nReviewers are unanimously positive on the paper, in terms of a well-written paper with a reasonable approach that yields strong empirical performance under the resource constraint.\n\nAC feels that the paper studies an important problem of making transfer learning faster for CNNs, however, the proposed model is a relatively straightforward combination of fine-tuning and filter-pruning, each having very extensive prior works. Also, AC has very critical comments for improving this paper:\n\n- The Attentive Feature Distillation (AFD) module is very similar to DELTA (Li et al. ICLR 2019) and L2T (Jang et al. ICML 2019), significantly weakening the novelty. The empirical evaluation should consider DELTA as baselines, e.g. AFS+DELTA.\n\nI accept this paper, assuming that all comments will be well addressed in the revision.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kf.wang@siat.ac.cn", "xt.gao@siat.ac.cn", "yiren.zhao@cl.cam.ac.uk", "lixingjian@baidu.com", "doudejing@baidu.com", "czxu@um.edu.mo"], "title": "Pay Attention to Features, Transfer Learn Faster CNNs", "authors": ["Kafeng Wang", "Xitong Gao", "Yiren Zhao", "Xingjian Li", "Dejing Dou", "Cheng-Zhong Xu"], "pdf": "/pdf/6e6abb55b83a1b41eaa6f0c9d191b89c2bfeb7c4.pdf", "TL;DR": "We introduce attentive feature distillation and selection, to fine-tune a large model and produce a faster one.", "abstract": "Deep convolutional neural networks are now widely deployed in vision applications, but a limited size of training data can restrict their task performance. Transfer learning offers the chance for CNNs to learn with limited data samples by transferring knowledge from models pretrained on large datasets. Blindly transferring all learned features from the source dataset, however, brings unnecessary computation to CNNs on the target task. In this paper, we propose attentive feature distillation and selection (AFDS), which not only adjusts the strength of transfer learning regularization but also dynamically determines the important features to transfer. By deploying AFDS on ResNet-101, we achieved a state-of-the-art computation reduction at the same accuracy budget, outperforming all existing transfer learning methods. With a 10x MACs reduction budget, a ResNet-101 equipped with AFDS transfer learned from ImageNet to Stanford Dogs 120, can achieve an accuracy 11.07% higher than its best competitor.", "keywords": ["transfer learning", "pruning", "faster CNNs"], "paperhash": "wang|pay_attention_to_features_transfer_learn_faster_cnns", "_bibtex": "@inproceedings{\nWang2020Pay,\ntitle={Pay Attention to Features, Transfer Learn Faster CNNs},\nauthor={Kafeng Wang and Xitong Gao and Yiren Zhao and Xingjian Li and Dejing Dou and Cheng-Zhong Xu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxyCeHtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/84d13359f1520c521f106b4f3bc6265a787c22e0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "ryxyCeHtPB", "replyto": "ryxyCeHtPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795724385, "tmdate": 1576800276024, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2594/-/Decision"}}}, {"id": "BJxy8nSUiS", "original": null, "number": 4, "cdate": 1573440582977, "ddate": null, "tcdate": 1573440582977, "tmdate": 1573440867870, "tddate": null, "forum": "ryxyCeHtPB", "replyto": "BkeROrlU5B", "invitation": "ICLR.cc/2020/Conference/Paper2594/-/Official_Comment", "content": {"title": "Thank you for your detailed reviews.", "comment": "Thank you for your comments. We would like to answer your questions:\n1. Consider a convolution operation with a \u201ck * k\u201d kernel, which takes input features with \u201cCi\u201d channels, and computes feature maps of shape \u201cCo * Ho * Wo\u201d. To evaluate the convolution thus requires \u201ck^2 * Ci * Co * Ho * Wo\u201d multiply-accumulate operations (MACs). AFS can reduce the number of MACs required in a coarse-grained manner: before computing the convolution, AFS can predict the importance of each output channel, request the convolution to evaluate only \u201cceil(d * Co)\u201d channels, and skip the remaining channels by setting them to zeros. Note that if the preceding layer is also a convolution that produce sparse outputs with only \u201cd * Ci\u201d non-zero channels, the input channels can also be skipped, reducing the number of MACs required to \u201ck^2 * d^2 * Ci * Co * Ho * Wo\u201d, a quadratic reduction in terms of \u201cd\u201d. We will update Section 3.4 to explain this in greater detail.\n2. As previous work did not examine the opportunity of pruning and transfer learning jointly, In Table 2, we re-implemented L2, L2-SP [1] and LwF [2]. We then used the best they can achieve with any one of the pruning methods, and compared the results against AFDS under 2x, 5x or 10x speedup constraints. We have additionally compared to existing smaller transfer learned models from related works [3, 4] in Table 3.\n3. We suspect the primary reason for the challenge is with the initial weights used in AFS. As the network depth gets larger, small changes in the variance used in initialization would result in highly sensitive changes in gradient magnitudes. Thanks for pointing out this to us and we will look into this in greater detail and update the paper accordingly.\n4. The code and accompanying models will be made available soon.\n\n[1]: Xuhong Li, et al., Explicit Inductive Bias for Transfer Learning with Convolutional Networks, ICML 2018.\n[2]: Zhizhong Li, et al., Learning without Forgetting, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018.\n[3]: Sergey Zagoruyko, Nikos Komodakis, Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer, ICLR 2017.\n[4]: Yunhun Jang, et al., Learning What and Where to Transfer, ICML 2019."}, "signatures": ["ICLR.cc/2020/Conference/Paper2594/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2594/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kf.wang@siat.ac.cn", "xt.gao@siat.ac.cn", "yiren.zhao@cl.cam.ac.uk", "lixingjian@baidu.com", "doudejing@baidu.com", "czxu@um.edu.mo"], "title": "Pay Attention to Features, Transfer Learn Faster CNNs", "authors": ["Kafeng Wang", "Xitong Gao", "Yiren Zhao", "Xingjian Li", "Dejing Dou", "Cheng-Zhong Xu"], "pdf": "/pdf/6e6abb55b83a1b41eaa6f0c9d191b89c2bfeb7c4.pdf", "TL;DR": "We introduce attentive feature distillation and selection, to fine-tune a large model and produce a faster one.", "abstract": "Deep convolutional neural networks are now widely deployed in vision applications, but a limited size of training data can restrict their task performance. Transfer learning offers the chance for CNNs to learn with limited data samples by transferring knowledge from models pretrained on large datasets. Blindly transferring all learned features from the source dataset, however, brings unnecessary computation to CNNs on the target task. In this paper, we propose attentive feature distillation and selection (AFDS), which not only adjusts the strength of transfer learning regularization but also dynamically determines the important features to transfer. By deploying AFDS on ResNet-101, we achieved a state-of-the-art computation reduction at the same accuracy budget, outperforming all existing transfer learning methods. With a 10x MACs reduction budget, a ResNet-101 equipped with AFDS transfer learned from ImageNet to Stanford Dogs 120, can achieve an accuracy 11.07% higher than its best competitor.", "keywords": ["transfer learning", "pruning", "faster CNNs"], "paperhash": "wang|pay_attention_to_features_transfer_learn_faster_cnns", "_bibtex": "@inproceedings{\nWang2020Pay,\ntitle={Pay Attention to Features, Transfer Learn Faster CNNs},\nauthor={Kafeng Wang and Xitong Gao and Yiren Zhao and Xingjian Li and Dejing Dou and Cheng-Zhong Xu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxyCeHtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/84d13359f1520c521f106b4f3bc6265a787c22e0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryxyCeHtPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2594/Authors", "ICLR.cc/2020/Conference/Paper2594/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2594/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2594/Reviewers", "ICLR.cc/2020/Conference/Paper2594/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2594/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2594/Authors|ICLR.cc/2020/Conference/Paper2594/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504138961, "tmdate": 1576860550445, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2594/Authors", "ICLR.cc/2020/Conference/Paper2594/Reviewers", "ICLR.cc/2020/Conference/Paper2594/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2594/-/Official_Comment"}}}, {"id": "r1e9TnrIsH", "original": null, "number": 5, "cdate": 1573440706488, "ddate": null, "tcdate": 1573440706488, "tmdate": 1573440706488, "tddate": null, "forum": "ryxyCeHtPB", "replyto": "Ske1UQAX9B", "invitation": "ICLR.cc/2020/Conference/Paper2594/-/Official_Comment", "content": {"title": "Thank you for your detailed reviews.", "comment": "Thank you for your comments. We would like to respond to the issues kindly raised by the reviewer:\n1. In the last sentence of Section 3.5, we mentioned that \u2018delta_s\u2019 is set to a value such that 50% of the channel neurons use the predictor function \u2018h_l\u2019\u201d. For this we mean that we first compute the variances of  \u201ch_l(x_{l-1})\u201d for each channel, and use the median of the channel variances as the value of the threshold \u201cdelta_s\u201d. As kindly suggested by the reviewer, we will be updating this section accordingly.\n2. Thanks for pointing out this to us, it will be fixed in the next revision."}, "signatures": ["ICLR.cc/2020/Conference/Paper2594/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2594/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kf.wang@siat.ac.cn", "xt.gao@siat.ac.cn", "yiren.zhao@cl.cam.ac.uk", "lixingjian@baidu.com", "doudejing@baidu.com", "czxu@um.edu.mo"], "title": "Pay Attention to Features, Transfer Learn Faster CNNs", "authors": ["Kafeng Wang", "Xitong Gao", "Yiren Zhao", "Xingjian Li", "Dejing Dou", "Cheng-Zhong Xu"], "pdf": "/pdf/6e6abb55b83a1b41eaa6f0c9d191b89c2bfeb7c4.pdf", "TL;DR": "We introduce attentive feature distillation and selection, to fine-tune a large model and produce a faster one.", "abstract": "Deep convolutional neural networks are now widely deployed in vision applications, but a limited size of training data can restrict their task performance. Transfer learning offers the chance for CNNs to learn with limited data samples by transferring knowledge from models pretrained on large datasets. Blindly transferring all learned features from the source dataset, however, brings unnecessary computation to CNNs on the target task. In this paper, we propose attentive feature distillation and selection (AFDS), which not only adjusts the strength of transfer learning regularization but also dynamically determines the important features to transfer. By deploying AFDS on ResNet-101, we achieved a state-of-the-art computation reduction at the same accuracy budget, outperforming all existing transfer learning methods. With a 10x MACs reduction budget, a ResNet-101 equipped with AFDS transfer learned from ImageNet to Stanford Dogs 120, can achieve an accuracy 11.07% higher than its best competitor.", "keywords": ["transfer learning", "pruning", "faster CNNs"], "paperhash": "wang|pay_attention_to_features_transfer_learn_faster_cnns", "_bibtex": "@inproceedings{\nWang2020Pay,\ntitle={Pay Attention to Features, Transfer Learn Faster CNNs},\nauthor={Kafeng Wang and Xitong Gao and Yiren Zhao and Xingjian Li and Dejing Dou and Cheng-Zhong Xu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxyCeHtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/84d13359f1520c521f106b4f3bc6265a787c22e0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryxyCeHtPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2594/Authors", "ICLR.cc/2020/Conference/Paper2594/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2594/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2594/Reviewers", "ICLR.cc/2020/Conference/Paper2594/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2594/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2594/Authors|ICLR.cc/2020/Conference/Paper2594/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504138961, "tmdate": 1576860550445, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2594/Authors", "ICLR.cc/2020/Conference/Paper2594/Reviewers", "ICLR.cc/2020/Conference/Paper2594/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2594/-/Official_Comment"}}}, {"id": "BJlAoqSIsH", "original": null, "number": 3, "cdate": 1573440166129, "ddate": null, "tcdate": 1573440166129, "tmdate": 1573440166129, "tddate": null, "forum": "ryxyCeHtPB", "replyto": "BklJwjhd5B", "invitation": "ICLR.cc/2020/Conference/Paper2594/-/Official_Comment", "content": {"title": "Thank you.", "comment": "We would like to thank the reviewer for the positive comments."}, "signatures": ["ICLR.cc/2020/Conference/Paper2594/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2594/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kf.wang@siat.ac.cn", "xt.gao@siat.ac.cn", "yiren.zhao@cl.cam.ac.uk", "lixingjian@baidu.com", "doudejing@baidu.com", "czxu@um.edu.mo"], "title": "Pay Attention to Features, Transfer Learn Faster CNNs", "authors": ["Kafeng Wang", "Xitong Gao", "Yiren Zhao", "Xingjian Li", "Dejing Dou", "Cheng-Zhong Xu"], "pdf": "/pdf/6e6abb55b83a1b41eaa6f0c9d191b89c2bfeb7c4.pdf", "TL;DR": "We introduce attentive feature distillation and selection, to fine-tune a large model and produce a faster one.", "abstract": "Deep convolutional neural networks are now widely deployed in vision applications, but a limited size of training data can restrict their task performance. Transfer learning offers the chance for CNNs to learn with limited data samples by transferring knowledge from models pretrained on large datasets. Blindly transferring all learned features from the source dataset, however, brings unnecessary computation to CNNs on the target task. In this paper, we propose attentive feature distillation and selection (AFDS), which not only adjusts the strength of transfer learning regularization but also dynamically determines the important features to transfer. By deploying AFDS on ResNet-101, we achieved a state-of-the-art computation reduction at the same accuracy budget, outperforming all existing transfer learning methods. With a 10x MACs reduction budget, a ResNet-101 equipped with AFDS transfer learned from ImageNet to Stanford Dogs 120, can achieve an accuracy 11.07% higher than its best competitor.", "keywords": ["transfer learning", "pruning", "faster CNNs"], "paperhash": "wang|pay_attention_to_features_transfer_learn_faster_cnns", "_bibtex": "@inproceedings{\nWang2020Pay,\ntitle={Pay Attention to Features, Transfer Learn Faster CNNs},\nauthor={Kafeng Wang and Xitong Gao and Yiren Zhao and Xingjian Li and Dejing Dou and Cheng-Zhong Xu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxyCeHtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/84d13359f1520c521f106b4f3bc6265a787c22e0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryxyCeHtPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2594/Authors", "ICLR.cc/2020/Conference/Paper2594/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2594/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2594/Reviewers", "ICLR.cc/2020/Conference/Paper2594/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2594/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2594/Authors|ICLR.cc/2020/Conference/Paper2594/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504138961, "tmdate": 1576860550445, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2594/Authors", "ICLR.cc/2020/Conference/Paper2594/Reviewers", "ICLR.cc/2020/Conference/Paper2594/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2594/-/Official_Comment"}}}, {"id": "Ske1UQAX9B", "original": null, "number": 1, "cdate": 1572229959099, "ddate": null, "tcdate": 1572229959099, "tmdate": 1572972317734, "tddate": null, "forum": "ryxyCeHtPB", "replyto": "ryxyCeHtPB", "invitation": "ICLR.cc/2020/Conference/Paper2594/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": " This paper proposes a method called attentive feature distillation and selection (AFDS) to improve the performance of transfer learning for CNNs. The authors argue that the regularization should constrain the proximity of feature maps, instead of pre-trained model weights. Specifically, the authors proposes two modifications of loss functions: 1) Attentive feature distillation (AFD), which modifies the regularization term to learn different weights for each channel and 2) Attentive feature selection (AFS), which modifies the ConvBN layers by predicts unimportant channels and suppress them. \n\nOverall, this is a good work in terms of theory and experimentation, thus I would recommend to accept it. The approach is well motivated, and the literature is complete and relevant. The author's argument is validated by experiments comparing the proposed AFDS method and other existing transfer learning methods. \n\nTo improve this paper, the authors are suggested to address the following issues:\n1.  Section 3.5 is not well organized. Besides, it is not mentioned what value the threshold hyper-parameter delta_m is set. \n2. In page 9, \"MACs\" is missing in the sentence \"In Figure 3, ...the number of vs. the target...\"\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2594/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2594/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kf.wang@siat.ac.cn", "xt.gao@siat.ac.cn", "yiren.zhao@cl.cam.ac.uk", "lixingjian@baidu.com", "doudejing@baidu.com", "czxu@um.edu.mo"], "title": "Pay Attention to Features, Transfer Learn Faster CNNs", "authors": ["Kafeng Wang", "Xitong Gao", "Yiren Zhao", "Xingjian Li", "Dejing Dou", "Cheng-Zhong Xu"], "pdf": "/pdf/6e6abb55b83a1b41eaa6f0c9d191b89c2bfeb7c4.pdf", "TL;DR": "We introduce attentive feature distillation and selection, to fine-tune a large model and produce a faster one.", "abstract": "Deep convolutional neural networks are now widely deployed in vision applications, but a limited size of training data can restrict their task performance. Transfer learning offers the chance for CNNs to learn with limited data samples by transferring knowledge from models pretrained on large datasets. Blindly transferring all learned features from the source dataset, however, brings unnecessary computation to CNNs on the target task. In this paper, we propose attentive feature distillation and selection (AFDS), which not only adjusts the strength of transfer learning regularization but also dynamically determines the important features to transfer. By deploying AFDS on ResNet-101, we achieved a state-of-the-art computation reduction at the same accuracy budget, outperforming all existing transfer learning methods. With a 10x MACs reduction budget, a ResNet-101 equipped with AFDS transfer learned from ImageNet to Stanford Dogs 120, can achieve an accuracy 11.07% higher than its best competitor.", "keywords": ["transfer learning", "pruning", "faster CNNs"], "paperhash": "wang|pay_attention_to_features_transfer_learn_faster_cnns", "_bibtex": "@inproceedings{\nWang2020Pay,\ntitle={Pay Attention to Features, Transfer Learn Faster CNNs},\nauthor={Kafeng Wang and Xitong Gao and Yiren Zhao and Xingjian Li and Dejing Dou and Cheng-Zhong Xu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxyCeHtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/84d13359f1520c521f106b4f3bc6265a787c22e0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ryxyCeHtPB", "replyto": "ryxyCeHtPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2594/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2594/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575832247824, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2594/Reviewers"], "noninvitees": [], "tcdate": 1570237720595, "tmdate": 1575832247835, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2594/-/Official_Review"}}}, {"id": "BkeROrlU5B", "original": null, "number": 2, "cdate": 1572369782324, "ddate": null, "tcdate": 1572369782324, "tmdate": 1572972317688, "tddate": null, "forum": "ryxyCeHtPB", "replyto": "ryxyCeHtPB", "invitation": "ICLR.cc/2020/Conference/Paper2594/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In general, I think it is a good paper and I like the contribution of the author. I think they explain in detail the methodology. The results compare the new methodologies with different databases which increase the credibility of the results. However, there is a couple of additional question that is important to manage:  \n\n1) The paper presents three different contributions. However, it is so clear how this work helps for \"By changing the fraction of channel neurons to skip for each convolution, AFDS can further accelerate the transfer learned models while minimizing the impact on task accuracy\" I think a better explanation of this part it would be necessary. \n\n2) The comparison of the results are very focused on AFDS, Did you compare the results with different transfer learning approach? \n\n3) During the training procedure. We need a better explanation of why \"we found that in residual networks with greater depths, AFS could become notably challenging to train to high accuracies\". Also, the results of the empirical test it would be useful to understand the challenges to train the network. \n\n4) I think it would be useful to have the code available for the final version. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2594/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2594/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kf.wang@siat.ac.cn", "xt.gao@siat.ac.cn", "yiren.zhao@cl.cam.ac.uk", "lixingjian@baidu.com", "doudejing@baidu.com", "czxu@um.edu.mo"], "title": "Pay Attention to Features, Transfer Learn Faster CNNs", "authors": ["Kafeng Wang", "Xitong Gao", "Yiren Zhao", "Xingjian Li", "Dejing Dou", "Cheng-Zhong Xu"], "pdf": "/pdf/6e6abb55b83a1b41eaa6f0c9d191b89c2bfeb7c4.pdf", "TL;DR": "We introduce attentive feature distillation and selection, to fine-tune a large model and produce a faster one.", "abstract": "Deep convolutional neural networks are now widely deployed in vision applications, but a limited size of training data can restrict their task performance. Transfer learning offers the chance for CNNs to learn with limited data samples by transferring knowledge from models pretrained on large datasets. Blindly transferring all learned features from the source dataset, however, brings unnecessary computation to CNNs on the target task. In this paper, we propose attentive feature distillation and selection (AFDS), which not only adjusts the strength of transfer learning regularization but also dynamically determines the important features to transfer. By deploying AFDS on ResNet-101, we achieved a state-of-the-art computation reduction at the same accuracy budget, outperforming all existing transfer learning methods. With a 10x MACs reduction budget, a ResNet-101 equipped with AFDS transfer learned from ImageNet to Stanford Dogs 120, can achieve an accuracy 11.07% higher than its best competitor.", "keywords": ["transfer learning", "pruning", "faster CNNs"], "paperhash": "wang|pay_attention_to_features_transfer_learn_faster_cnns", "_bibtex": "@inproceedings{\nWang2020Pay,\ntitle={Pay Attention to Features, Transfer Learn Faster CNNs},\nauthor={Kafeng Wang and Xitong Gao and Yiren Zhao and Xingjian Li and Dejing Dou and Cheng-Zhong Xu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxyCeHtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/84d13359f1520c521f106b4f3bc6265a787c22e0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ryxyCeHtPB", "replyto": "ryxyCeHtPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2594/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2594/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575832247824, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2594/Reviewers"], "noninvitees": [], "tcdate": 1570237720595, "tmdate": 1575832247835, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2594/-/Official_Review"}}}, {"id": "BklJwjhd5B", "original": null, "number": 4, "cdate": 1572551510593, "ddate": null, "tcdate": 1572551510593, "tmdate": 1572972317644, "tddate": null, "forum": "ryxyCeHtPB", "replyto": "ryxyCeHtPB", "invitation": "ICLR.cc/2020/Conference/Paper2594/-/Official_Review", "content": {"rating": "8: Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper presents an improvement to the task of transfer learning by being deliberate about which channels from the base model are most relevant to the new task at hand. It does this by apply attentive feature selection (AFS) to select channels or features that align well with the down stream task and attentive feature distillation (AFD) to pass on these features to the student network. In the process they do channel pruning there by decreasing the size of the network and enabling faster inference speeds. Their major argument is that plain transfer learning is redundant and wasteful and careful attention applied to selection of the features and channels to be transfered can lead to smaller faster models which in several cases presented in the paper provide superior performance.\n\nPaper is clear and concise and experimentally sound showing a real contribution to the body of knowledge in transfer learning and pruning."}, "signatures": ["ICLR.cc/2020/Conference/Paper2594/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2594/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kf.wang@siat.ac.cn", "xt.gao@siat.ac.cn", "yiren.zhao@cl.cam.ac.uk", "lixingjian@baidu.com", "doudejing@baidu.com", "czxu@um.edu.mo"], "title": "Pay Attention to Features, Transfer Learn Faster CNNs", "authors": ["Kafeng Wang", "Xitong Gao", "Yiren Zhao", "Xingjian Li", "Dejing Dou", "Cheng-Zhong Xu"], "pdf": "/pdf/6e6abb55b83a1b41eaa6f0c9d191b89c2bfeb7c4.pdf", "TL;DR": "We introduce attentive feature distillation and selection, to fine-tune a large model and produce a faster one.", "abstract": "Deep convolutional neural networks are now widely deployed in vision applications, but a limited size of training data can restrict their task performance. Transfer learning offers the chance for CNNs to learn with limited data samples by transferring knowledge from models pretrained on large datasets. Blindly transferring all learned features from the source dataset, however, brings unnecessary computation to CNNs on the target task. In this paper, we propose attentive feature distillation and selection (AFDS), which not only adjusts the strength of transfer learning regularization but also dynamically determines the important features to transfer. By deploying AFDS on ResNet-101, we achieved a state-of-the-art computation reduction at the same accuracy budget, outperforming all existing transfer learning methods. With a 10x MACs reduction budget, a ResNet-101 equipped with AFDS transfer learned from ImageNet to Stanford Dogs 120, can achieve an accuracy 11.07% higher than its best competitor.", "keywords": ["transfer learning", "pruning", "faster CNNs"], "paperhash": "wang|pay_attention_to_features_transfer_learn_faster_cnns", "_bibtex": "@inproceedings{\nWang2020Pay,\ntitle={Pay Attention to Features, Transfer Learn Faster CNNs},\nauthor={Kafeng Wang and Xitong Gao and Yiren Zhao and Xingjian Li and Dejing Dou and Cheng-Zhong Xu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxyCeHtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/84d13359f1520c521f106b4f3bc6265a787c22e0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ryxyCeHtPB", "replyto": "ryxyCeHtPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2594/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2594/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575832247824, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2594/Reviewers"], "noninvitees": [], "tcdate": 1570237720595, "tmdate": 1575832247835, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2594/-/Official_Review"}}}, {"id": "Skg8EJfGuH", "original": null, "number": 1, "cdate": 1570017070423, "ddate": null, "tcdate": 1570017070423, "tmdate": 1570017080129, "tddate": null, "forum": "ryxyCeHtPB", "replyto": "ryxyCeHtPB", "invitation": "ICLR.cc/2020/Conference/Paper2594/-/Public_Comment", "content": {"comment": "Hello, \n\nNice work that learns to select which features from a source model will help improve the fine-tuning performance on a target task! Indeed many times only few features contribute to the recognition in the target domain and others can even act as a confuser. \n\nIn structured sparsity section, you mention a few works that try to select which inference path to evaluate conditioned on the input. I thought (Veit and Belongie, ECCV 2018, http://openaccess.thecvf.com/content_ECCV_2018/papers/Andreas_Veit_Convolutional_Networks_with_ECCV_2018_paper.pdf) is also relevant for this list since they are learning to skip or process a layer (not channels) in Residual Networks without introducing architectural changes, and is tested on ImageNet. In this manner this can also be a coarser-grained alternative to \"channel skipping\" network proposed in this work. \n\nMy question is: Did you also try to do layer-wise skipping? It could be intuitive that some classes in the source domain may not even exist in the target domain (esp. for Birds-only and Dogs-only datasets) so than deciding to not process a layer on its entirety may yield even bigger drop in number of FLOPS, while maybe even increasing the accuracy? \n\n  ", "title": "Relevant work of Adaptive Inference Graphs ~ AIG (Veit and Belongie, ECCV 2018) ? "}, "signatures": ["~Mert_Kilickaya1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Mert_Kilickaya1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kf.wang@siat.ac.cn", "xt.gao@siat.ac.cn", "yiren.zhao@cl.cam.ac.uk", "lixingjian@baidu.com", "doudejing@baidu.com", "czxu@um.edu.mo"], "title": "Pay Attention to Features, Transfer Learn Faster CNNs", "authors": ["Kafeng Wang", "Xitong Gao", "Yiren Zhao", "Xingjian Li", "Dejing Dou", "Cheng-Zhong Xu"], "pdf": "/pdf/6e6abb55b83a1b41eaa6f0c9d191b89c2bfeb7c4.pdf", "TL;DR": "We introduce attentive feature distillation and selection, to fine-tune a large model and produce a faster one.", "abstract": "Deep convolutional neural networks are now widely deployed in vision applications, but a limited size of training data can restrict their task performance. Transfer learning offers the chance for CNNs to learn with limited data samples by transferring knowledge from models pretrained on large datasets. Blindly transferring all learned features from the source dataset, however, brings unnecessary computation to CNNs on the target task. In this paper, we propose attentive feature distillation and selection (AFDS), which not only adjusts the strength of transfer learning regularization but also dynamically determines the important features to transfer. By deploying AFDS on ResNet-101, we achieved a state-of-the-art computation reduction at the same accuracy budget, outperforming all existing transfer learning methods. With a 10x MACs reduction budget, a ResNet-101 equipped with AFDS transfer learned from ImageNet to Stanford Dogs 120, can achieve an accuracy 11.07% higher than its best competitor.", "keywords": ["transfer learning", "pruning", "faster CNNs"], "paperhash": "wang|pay_attention_to_features_transfer_learn_faster_cnns", "_bibtex": "@inproceedings{\nWang2020Pay,\ntitle={Pay Attention to Features, Transfer Learn Faster CNNs},\nauthor={Kafeng Wang and Xitong Gao and Yiren Zhao and Xingjian Li and Dejing Dou and Cheng-Zhong Xu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxyCeHtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/84d13359f1520c521f106b4f3bc6265a787c22e0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryxyCeHtPB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504177960, "tmdate": 1576860583712, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2594/Authors", "ICLR.cc/2020/Conference/Paper2594/Reviewers", "ICLR.cc/2020/Conference/Paper2594/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2594/-/Public_Comment"}}}], "count": 9}