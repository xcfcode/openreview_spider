{"notes": [{"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457650374184, "tcdate": 1457650374184, "id": "VAVwE51RPix0Wk76TAQK", "invitation": "ICLR.cc/2016/workshop/-/paper/135/review/12", "forum": "ROVmGqlgmhvnM0J1IpNq", "replyto": "ROVmGqlgmhvnM0J1IpNq", "signatures": ["ICLR.cc/2016/workshop/paper/135/reviewer/12"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/135/reviewer/12"], "content": {"title": "Authors propose using an extension to LSTMs using bilinear tensor products and evaluate it on character-level language modeling task. The results don't seem to be convincing that the method is really useful.", "rating": "3: Clear rejection", "review": "Quick Googling of related methods pointed me to \"Modeling Compositionality with Multiplicative Recurrent Neural Networks\" by Irsoy & Cardie, which was published last year at ICLR. They also use bilinear tensor products in the context of recurrent neural networks. Authors of this paper extend the idea for LSTMs where each matrix product is replaced by bilinear tensor products.\n\nThe experiments were run on the w 100M bytes of English Wikipedia. The results seem to imply that the proposed method (called GRTN) is significantly better than regular LSTM. It appears, however, that the GRTN uses significantly more parameters than the other approaches which makes the comparisons not necessarily valid. What is more, the log likelihoods of all methods seem to be significantly worse than in \"Generating Text with Recurrent Neural Networks\" by Sutskever et al, which was published in 2011.\n\n\nPros:\n- I haven't seen other people trying to use LSTMs with bilinear tensor products, which might be an interesting extension\nCons\n- the experimental section is very lacking\n- the paper misses references to important related work", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Sequence Modeling with Recurrent Tensor Networks", "abstract": "We introduce the recurrent tensor network, a recurrent neural network model that replaces the matrix-vector multiplications of a standard recurrent neural network with bilinear tensor products. We compare its performance against networks that employ long short-term memory (LSTM) networks. Our results demonstrate that using tensors to capture the interactions between network inputs and history can lead to substantial improvement in predictive performance on the language modeling task.\n", "pdf": "/pdf/ROVmGqlgmhvnM0J1IpNq.pdf", "paperhash": "kelley|sequence_modeling_with_recurrent_tensor_networks", "authors": ["Richard Kelley"], "authorids": ["richard@eigenmancy.com"], "conflicts": ["eigenmancy.com", "unr.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580038699, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580038699, "id": "ICLR.cc/2016/workshop/-/paper/135/review/12", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "ROVmGqlgmhvnM0J1IpNq", "replyto": "ROVmGqlgmhvnM0J1IpNq", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/135/reviewer/12", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457629014139, "tcdate": 1457629014139, "id": "k80YxLokNIOYKX7ji491", "invitation": "ICLR.cc/2016/workshop/-/paper/135/review/11", "forum": "ROVmGqlgmhvnM0J1IpNq", "replyto": "ROVmGqlgmhvnM0J1IpNq", "signatures": ["ICLR.cc/2016/workshop/paper/135/reviewer/11"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/135/reviewer/11"], "content": {"title": "", "rating": "2: Strong rejection", "review": "This paper introduces the multiplicative (or tensor) recurrent neural networks for sequence modeling and does a preliminary evaluation on the task of language modeling.\n\nMain issue of the paper is the core idea (tensor RNN) is not novel, and there are no citations to the papers employing similar ideas:\n\n(1) Generating Text with Recurrent Neural Networks by Sutskever et al (ICML 2011)\n(2) Modeling Compositionality with Multiplicative Recurrent Neural Networks by Irsoy & Cardie (ICLR 2015)\n\nCombination of bilinear product with the gated RNNs is, to my knowledge, novel, however the paper is not structured around that idea as its main contribution.\n\nAnother issue is the weakness of experimentation, as well as it being unfair in terms of parameter size (which is already addressed by the author). To my understanding, the author compares models with a single set of hyperparameters, therefore the results are also prone to the randomness due to this choice. There should be some degrees of freedom for hyperparameter tuning to reduce this randomness and make a fairer comparison.\n\nPros: (1) Gated tensor RNN idea is novel.\nCons: (1) The main contribution (tensor RNN) idea is not novel. Lack of citations to relevant papers that used tensor RNNs. (2) Experimentation is weak and unfair in terms of sizes of the models being compared. Thus the results are not conclusive or convincing.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Sequence Modeling with Recurrent Tensor Networks", "abstract": "We introduce the recurrent tensor network, a recurrent neural network model that replaces the matrix-vector multiplications of a standard recurrent neural network with bilinear tensor products. We compare its performance against networks that employ long short-term memory (LSTM) networks. Our results demonstrate that using tensors to capture the interactions between network inputs and history can lead to substantial improvement in predictive performance on the language modeling task.\n", "pdf": "/pdf/ROVmGqlgmhvnM0J1IpNq.pdf", "paperhash": "kelley|sequence_modeling_with_recurrent_tensor_networks", "authors": ["Richard Kelley"], "authorids": ["richard@eigenmancy.com"], "conflicts": ["eigenmancy.com", "unr.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580038936, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580038936, "id": "ICLR.cc/2016/workshop/-/paper/135/review/11", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "ROVmGqlgmhvnM0J1IpNq", "replyto": "ROVmGqlgmhvnM0J1IpNq", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/135/reviewer/11", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1455826598980, "tcdate": 1455826598980, "id": "ROVmGqlgmhvnM0J1IpNq", "invitation": "ICLR.cc/2016/workshop/-/submission", "forum": "ROVmGqlgmhvnM0J1IpNq", "signatures": ["~Richard_Kelley1"], "readers": ["everyone"], "writers": ["~Richard_Kelley1"], "content": {"CMT_id": "", "title": "Sequence Modeling with Recurrent Tensor Networks", "abstract": "We introduce the recurrent tensor network, a recurrent neural network model that replaces the matrix-vector multiplications of a standard recurrent neural network with bilinear tensor products. We compare its performance against networks that employ long short-term memory (LSTM) networks. Our results demonstrate that using tensors to capture the interactions between network inputs and history can lead to substantial improvement in predictive performance on the language modeling task.\n", "pdf": "/pdf/ROVmGqlgmhvnM0J1IpNq.pdf", "paperhash": "kelley|sequence_modeling_with_recurrent_tensor_networks", "authors": ["Richard Kelley"], "authorids": ["richard@eigenmancy.com"], "conflicts": ["eigenmancy.com", "unr.edu"]}, "nonreaders": [], "details": {"replyCount": 2, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1454464564200, "ddate": null, "super": null, "final": null, "duedate": 1455833700000, "tcdate": 1454464564200, "id": "ICLR.cc/2016/workshop/-/submission", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"order": 4, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv.", "value-regex": "upload|http://arxiv.org/pdf/.+"}, "title": {"order": 3, "description": "Title of paper.", "value-regex": ".{0,500}"}, "abstract": {"order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"order": 1, "description": "Comma separated list of author names, as they appear in the paper.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "author_emails": {"order": 2, "description": "Comma separated list of author email addresses, in the same order as above.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "conflicts": {"order": 100, "description": "Semi-colon separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.).", "value-regex": "^([a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))+(;[a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))*$"}, "CMT_id": {"order": 5, "value-regex": ".*", "description": "If the paper is a resubmission from the ICLR 2016 Conference Track, enter its CMT ID; otherwise, leave blank."}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "expdate": 1463609700000}}}], "count": 3}